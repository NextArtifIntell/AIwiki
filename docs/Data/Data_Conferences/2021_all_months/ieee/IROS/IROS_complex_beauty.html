<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IROS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="iros---1032">IROS - 1032</h2>
<ul>
<li><details>
<summary>
(2021). Convex approximation for LTL-based planning. <em>IROS</em>,
9863–9869. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a formulation for linear temporal logic (LTL)-based task planning of nonlinear dynamical systems. We consider pick-and-place task planning as a typical example of the planning task that can be modeled as a hybrid system that includes the states of robots and objects. LTL-based planning for hybrid systems is solved as a mixed-integer problem (MIP), especially a mixed-integer linear programming problem (MILP). Due to the formulation by the MILP, we could only deal with linear systems and linear constraints. In our proposed method, we apply a convex approximation to systems that have bilinear terms and quadratic terms in their dynamics. And we incorporate nonlinear systems into existing LTL-based planning as an MILP. We demonstrate the effectiveness through numerical simulations of a simple robot arm system and drone system.},
  archive   = {C_IROS},
  author    = {Shumpei Tokuda and Masaki Yamakita and Hiroyuki Oyama and Rin Takano},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636801},
  pages     = {9863-9869},
  title     = {Convex approximation for LTL-based planning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Formalizing trajectories in human-robot encounters via
probabilistic STL inference. <em>IROS</em>, 9857–9862. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we are interested in formalizing human trajectories in human-robot encounters. We consider a particular case where a human and a robot walk towards each other. A question that arises is whether, when, and how humans will deviate from their trajectory to avoid a collision. These human trajectories can then be used to generate socially acceptable robot trajectories. To model these trajectories, we propose a data-driven algorithm to extract a formal specification expressed in Signal Temporal Logic with probabilistic predicates. We evaluated our method on trajectories collected through an online study where participants had to avoid colliding with a robot in a shared environment. Further, we demonstrate that probabilistic STL is a suitable formalism to depict human behavior, choices and preferences in specific scenarios of social navigation.},
  archive   = {C_IROS},
  author    = {Alexis Linard and Ilaria Torre and Anders Steen and Iolanda Leite and Jana Tumova},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635951},
  pages     = {9857-9862},
  title     = {Formalizing trajectories in human-robot encounters via probabilistic STL inference},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Probabilistic trajectory prediction with structural
constraints. <em>IROS</em>, 9849–9856. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work addresses the problem of predicting the motion trajectories of dynamic objects in the environment. Recent advances in predicting motion patterns often rely on machine learning techniques to extrapolate motion patterns from observed trajectories, with no mechanism to directly incorporate known rules. We propose a novel framework, which combines probabilistic learning and constrained trajectory optimisation. The learning component of our framework provides a distribution over future motion trajectories conditioned on observed past coordinates. This distribution is then used as a prior to a constrained optimisation problem which enforces chance constraints on the trajectory distribution. This results in constraint-compliant trajectory distributions which closely resemble the prior. In particular, we focus our investigation on collision constraints, such that extrapolated future trajectory distributions conform to the environment structure. We empirically demonstrate on real-world and simulated datasets the ability of our framework to learn complex probabilistic motion trajectories for motion data, while directly enforcing constraints to improve generalisability, producing more robust and higher quality trajectory distributions.},
  archive   = {C_IROS},
  author    = {Weiming Zhi and Lionel Ott and Fabio Ramos},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636003},
  pages     = {9849-9856},
  title     = {Probabilistic trajectory prediction with structural constraints},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Formalizing the execution context of behavior trees for
runtime verification of deliberative policies. <em>IROS</em>, 9841–9848.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we enable automated property verification of deliberative components in robot control architectures. We focus on formalizing the execution context of Behavior Trees (BTs) to provide a scalable, yet formally grounded, methodology to enable runtime verification and prevent unexpected robot behaviors. To this end, we consider a message-passing model that accommodates both synchronous and asynchronous composition of parallel components, in which BTs and other components execute and interact according to the communication patterns commonly adopted in robotic software architectures. We introduce a formal property specification language to encode requirements and build runtime monitors. We performed a set of experiments, both on simulations and on the real robot, demonstrating the feasibility of our approach in a realistic application and its integration in a typical robot software architecture. We also provide an OS-level virtualization environment to reproduce the experiments in the simulated scenario.},
  archive   = {C_IROS},
  author    = {Michele Colledanchise and Giuseppe Cicala and Daniele E. Domenichelli and Lorenzo Natale and Armando Tacchella},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636129},
  pages     = {9841-9848},
  title     = {Formalizing the execution context of behavior trees for runtime verification of deliberative policies},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Wasserstein-splitting gaussian process regression for
heterogeneous online bayesian inference. <em>IROS</em>, 9833–9840. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Gaussian processes (GPs) are a well-known nonparametric Bayesian inference technique, but they suffer from scalability problems for large sample sizes, and their performance can degrade for non-stationary or spatially heterogeneous data. In this work, we seek to overcome these issues through (i) employing variational free energy approximations of GPs operating in tandem with online expectation propagation steps; and (ii) introducing a local splitting step which instantiates a new GP whenever the posterior distribution changes significantly as quantified by the Wasserstein metric over posterior distributions. Over time, then, this yields an ensemble of sparse GPs which may be updated incrementally, and adapts to locality, heterogeneity, and non-stationarity in training data. We provide a 1-dimensional example to illustrate the motivation behind our approach, and compare the performance of our approach to other Gaussian process methods across various data sets, which often achieves competitive, if not superior predictive performance, relative to other locality-based GP regression methods in which hyperparameters are learned in an online manner.},
  archive   = {C_IROS},
  author    = {Michael E. Kepler and Alec Koppel and Amrit Singh Bedi and Daniel J. Stilwell},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636357},
  pages     = {9833-9840},
  title     = {Wasserstein-splitting gaussian process regression for heterogeneous online bayesian inference},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Decentralized classification with assume-guarantee planning.
<em>IROS</em>, 9826–9832. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of decentralized classification conducted over a network of mobile sensors. We model the multiagent classification task as a hypothesis testing problem where each sensor has to almost surely find the true hypothesis from a finite set of candidate hypotheses. Each sensor makes noisy local observations and can also share information on their observations with other mobile sensors in communication range. In order to address the state-space explosion in the multiagent system, we propose a decentralized synthesis procedure that guarantees that each sensor will almost surely converge to the true hypothesis even in the presence of faulty or malicious agents. Additionally, we employ a contract-based synthesis approach that produces trajectories designed to empirically increase information-sharing between mobile sensors in order to converge faster to the true hypothesis. We implement and test the approach on experiments with both physical and simulated hardware to showcase the approach’s scalability and viability in real-world systems. Finally, we run a Gazebo/ROS simulated experiment with 12 agents to demonstrate the scalability of our approach in large environments with many agents.},
  archive   = {C_IROS},
  author    = {Steven Carr and Jesse Quattrociocchi and Suda Bharadwaj and Steven J. Spencer and Anup Parikh and Carol C. Young and Stephen P. Buerger and Bo Wu and Ufuk Topcu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636177},
  pages     = {9826-9832},
  title     = {Decentralized classification with assume-guarantee planning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Safe linear temporal logic motion planning in dynamic
environments. <em>IROS</em>, 9818–9825. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes an online control framework for mobile robots to satisfy a complex mission given in the form of linear temporal logic (LTL) without colliding with moving obstacles in the environment. The proposed framework consists of three modules named the static planner, the local collision avoidance, and the patcher. The static planner is synthesized by solving a parity game for a finite abstraction of the robot model based on a world map with static obstacles to fulfill the LTL task. The local collision avoidance module computes a set of safe controls that guarantees a safe distance between the moving objects. Both of the modules can be rigorously computed offline only once via formal methods. The patcher is activated whenever a moving obstacle is detected and modifies the static plan online for a short horizon by using only provably safe controls. The resulting modified strategy can guarantee collision-free motion without losing the ability to satisfy the LTL task. As opposed to using assume-guarantee type of LTL tasks, the proposed framework can handle the situations where obstacle movement is unpredictable.},
  archive   = {C_IROS},
  author    = {Yinan Li and Ebrahim Moradi Shahrivar and Jun Liu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636433},
  pages     = {9818-9825},
  title     = {Safe linear temporal logic motion planning in dynamic environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A large-scale dataset for water segmentation of SAR
satellite. <em>IROS</em>, 9796–9801. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Not only on earth, but also in space, robot systems are increasingly becoming essential elements in our lives such as a mobile exploration robot on Mars. Satellites are also an indispensable field in space robot systems, for example, from low-orbit satellites for self-driving vehicles to small satellites launched for various purposes. A lot of research is being conducted on an automated system using more and more satellites. In particular, earth observation using satellite images is being used in various fields such as disaster prediction, damage analysis, and land cover classification. There are three main types of satellite imagery used in automation systems: optical, Synthetic Aperture Radar (SAR), and infrared. Unlike optical satellite, which is heavily influenced by weather and light, SAR satellite can acquire images in all-weather conditions. Thanks to this advantage, SAR satellite images are used in many fields, in particular, water segmentation. There are various traditional SAR image-based water segmentation methods based on thresholding technique. However, these methods are not suitable for rapidly processing a large amount of SAR images because they require a manual operation to set different thresholds for each image. In this paper, we create a large-scale dataset for water segmentation of KOrean Multi-Purpose SATellite (KOMPSAT-5) containing more than 3,000 images. We perform water segmentation using representative deep learning-based segmentation models such as Fully Convolutional Networks (FCN), U-Net, DeepUNet, and High Resolution Network (HRNet). Experimental results show that high performance of water segmentation can be obtained when a large number of training images are used for all five segmentation models. In addition, we confirm the possibility of the automatic water segmentation system from a large amount of SAR images, away from traditional manual work.},
  archive   = {C_IROS},
  author    = {Myeung Un Kim and Han Oh and Seung-Jae Lee and Yeonju Choi and Sanghyuck Han},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635863},
  pages     = {9796-9801},
  title     = {A large-scale dataset for water segmentation of SAR satellite},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ROBI: A multi-view dataset for reflective objects in robotic
bin-picking. <em>IROS</em>, 9788–9795. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In robotic bin-picking applications, the perception of texture-less, highly reflective parts is a valuable but challenging task. The high glossiness can introduce fake edges in RGB images and inaccurate depth measurements, especially in heavily cluttered bin scenarios. In this paper, we present the ROBI (Reflective Objects in BIns) dataset, a public dataset for 6D object pose estimation and multi-view depth fusion in robotic bin-picking scenarios. The ROBI dataset includes a total of 63 bin-picking scenes captured with two active stereo cameras: a high-cost Ensenso sensor and a low-cost RealSense sensor. For each scene, the monochrome/RGB images and depth maps are captured from sampled view spheres around the scene, and are annotated with accurate 6D poses of visible objects and an associated visibility score. For evaluating the performance of depth fusion, we captured the ground truth depth maps by high-cost Ensenso camera with objects coated in anti-reflective scanning spray. To show the utility of the dataset, we evaluated the representative algorithms of 6D object pose estimation and multi-view depth fusion on the full dataset. Evaluation results demonstrate the difficulty of highly reflective objects, especially in difficult cases due to the degradation of depth data quality, severe occlusions, and cluttered scenes. The ROBI dataset is available online at https://www.trailab.utias.utoronto.ca/robi.},
  archive   = {C_IROS},
  author    = {Jun Yang and Yizhou Gao and Dong Li and Steven L. Waslander},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635871},
  pages     = {9788-9795},
  title     = {ROBI: A multi-view dataset for reflective objects in robotic bin-picking},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NYU-VPR: Long-term visual place recognition benchmark with
view direction and data anonymization influences. <em>IROS</em>,
9773–9779. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual place recognition (VPR) is critical in not only localization and mapping for autonomous driving vehicles, but also assistive navigation for the visually impaired population. To enable a long-term VPR system on a large scale, several challenges need to be addressed. First, different applications could require different image view directions, such as front views for self-driving cars while side views for the low vision people. Second, VPR in metropolitan scenes can often cause privacy concerns due to the imaging of pedestrian and vehicle identity information, calling for the need for data anonymization before VPR queries and database construction. Both factors could lead to VPR performance variations that are not well understood yet. To study their influences, we present the NYU-VPR dataset that contains more than 200,000 images over a 2km×2km area near the New York University campus, taken within the whole year of 2016. We present benchmark results on several popular VPR algorithms showing that side views are significantly more challenging for current VPR methods while the influence of data anonymization is almost negligible, together with our hypothetical explanations and in-depth analysis.},
  archive   = {C_IROS},
  author    = {Diwei Sheng and Yuxiang Chai and Xinru Li and Chen Feng and Jianzhe Lin and Claudio Silva and John-Ross Rizzo},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636640},
  pages     = {9773-9779},
  title     = {NYU-VPR: Long-term visual place recognition benchmark with view direction and data anonymization influences},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A photorealistic terrain simulation pipeline for
unstructured outdoor environments. <em>IROS</em>, 9765–9772. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Suitable datasets are an integral part of robotics research, especially for training neural networks in robot perception. However, in many domains, suitable real-world data are scarce and cannot be easily obtained. This problem is especially prevalent for unstructured outdoor environments, in particular, planetary ones. Recent advances in photorealistic simulations help researchers to simulate close-to-real data in many domains. Yet, there exists no high-quality synthetic data for planetary exploration tasks. Also, existing simulators lack the fidelity required for generating planetary data, which is inherently less structured than human environments. Synthetic planetary data requires careful modeling and annotation of many different terrain aspect and details, such as textures and distributions of rocks, to become a valuable test-bed for robotics. To fill this gap, we present a novel simulator specifically designed for the needs of planetary robotics visual tasks, but also applicable for other outdoor environments. Our simulator is capable of generating large varieties of (planetary) outdoor scenes with rich generation of meta data, such as multilevel semantic and instance annotations. To demonstrate the wide applicability of this new simulator, we evaluate its performance on typical robotics applications, i.e. semantic segmentation, instance segmentation, and visual SLAM. Our simulator is accessible under https://github.com/DLR-RM/oaisys.},
  archive   = {C_IROS},
  author    = {M. G. Müller and M. Durner and A. Gawel and W. Stürzl and R. Triebel and R. Siegwart},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636644},
  pages     = {9765-9772},
  title     = {A photorealistic terrain simulation pipeline for unstructured outdoor environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stereo hybrid event-frame (SHEF) cameras for 3D perception.
<em>IROS</em>, 9758–9764. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Stereo camera systems play an important role in robotics applications to perceive the 3D world. However, conventional cameras have drawbacks such as low dynamic range, motion blur and latency due to the underlying frame- based mechanism. Event cameras address these limitations as they report the brightness changes of each pixel independently with a fine temporal resolution, but they are unable to acquire absolute intensity information directly. Although integrated hybrid event-frame sensors (e.g., DAVIS) are available, the quality of data is compromised by coupling at the pixel level in the circuit fabrication of such cameras. This paper proposes a stereo hybrid event-frame (SHEF) camera system that offers a sensor modality with separate high-quality pure event and pure frame cameras, overcoming the limitations of each separate sensor and allowing for stereo depth estimation. We provide a SHEF dataset targeted at evaluating disparity estimation algorithms and introduce a stereo disparity estimation algorithm that uses edge information extracted from the event stream correlated with the edge detected in the frame data. Our disparity estimation outperforms the state-of-the-art stereo matching algorithm on the SHEF dataset.},
  archive   = {C_IROS},
  author    = {Ziwei Wang and Liyuan Pan and Yonhon Ng and Zheyu Zhuang and Robert Mahony},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636312},
  pages     = {9758-9764},
  title     = {Stereo hybrid event-frame (SHEF) cameras for 3D perception},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A dataset for provident vehicle detection at night.
<em>IROS</em>, 9750–9757. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In current object detection, algorithms require the object to be directly visible in order to be detected. As humans, however, we intuitively use visual cues caused by the respective object to already make assumptions about its appearance. In the context of driving, such cues can be shadows during the day and often light reflections at night. In this paper, we study the problem of how to map this intuitive human behavior to computer vision algorithms to detect oncoming vehicles at night just from the light reflections they cause by their headlights. For that, we present an extensive open-source dataset containing 59 746 annotated grayscale images out of 346 different scenes in a rural environment at night. In these images, all oncoming vehicles, their corresponding light objects (e. g., headlamps), and their respective light reflections (e. g., light reflections on guardrails) are labeled. In this context, we discuss the characteristics of the dataset and the challenges in objectively describing visual cues such as light reflections. We provide different metrics for different ways to approach the task and report the results we achieved using state-of-the-art and custom object detection models as a first benchmark. With that, we want to bring attention to a new and so far neglected field in computer vision research, encourage more researchers to tackle the problem, and thereby further close the gap between human performance and computer vision systems.},
  archive   = {C_IROS},
  author    = {Sascha Saralajew and Lars Ohnemus and Lukas Ewecker and Ebubekir Asan and Simon Isele and Stefan Roos},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636162},
  pages     = {9750-9757},
  title     = {A dataset for provident vehicle detection at night},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PlanSys2: A planning system framework for ROS2.
<em>IROS</em>, 9742–9749. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous robots need to plan the tasks they carry out to fulfill their missions. The missions’ increasing complexity does not let human designers anticipate all the possible situations, so traditional control systems based on state machines are not enough. This paper contains a description of the ROS2 Planning System (PlanSys2 in short), a framework for symbolic planning that incorporates novel approaches for execution on robots working in demanding environments. PlanSys2 aims to be the reference task planning framework in ROS2, the latest version of the de facto standard in robotics software development. Among its main features, it can be highlighted the optimized execution, based on Behavior Trees, of plans through a new actions auction protocol and its multi-robot planning capabilities. It already has a small but growing community of users and developers, and this document is a summary of the design and capabilities of this project.},
  archive   = {C_IROS},
  author    = {Francisco Martín and Jonatan Ginés Clavero and Vicente Matellán and Francisco J. Rodríguez},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636544},
  pages     = {9742-9749},
  title     = {PlanSys2: A planning system framework for ROS2},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rapid recovery from robot failures in multi-robot
visibility-based pursuit-evasion. <em>IROS</em>, 9734–9741. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the visibility-based pursuit-evasion problem where a team of pursuer robots operating in a two-dimensional polygonal space seek to establish visibility of an arbitrarily fast evader. This is a computationally challenging task for which the best known complete algorithm takes time doubly exponential in the number of robots. However, recent advances that utilize sampling-based methods have shown progress in generating feasible solutions. An aspect of this problem that has yet to be explored concerns how to ensure that the robots can recover from catastrophic failures which leave one or more robots unexpectedly incapable of continuing to contribute to the pursuit of the evader. To address this issue, we propose an algorithm that can rapidly recover from catastrophic failures. When such failures occur, a replanning occurs, leveraging both the information retained from the previous iteration and the partial progress of the search completed before the failure to generate a new motion strategy for the reduced team of pursuers. We describe an implementation of this algorithm and provide quantitative results that show that the proposed method is able to recover from robot failures more rapidly than a baseline approach that plans from scratch.},
  archive   = {C_IROS},
  author    = {Trevor Olsen and Nicholas M. Stiffler and Jason M. O’Kane},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636141},
  pages     = {9734-9741},
  title     = {Rapid recovery from robot failures in multi-robot visibility-based pursuit-evasion},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Team orienteering coverage planning with uncertain reward.
<em>IROS</em>, 9728–9733. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many municipalities and large organizations have fleets of vehicles that need to be coordinated for tasks such as garbage collection or infrastructure inspection. Motivated by this need, this paper focuses on the common subproblem in which a team of vehicles needs to plan coordinated routes to patrol an area over iterations while minimizing temporally and spatially dependent costs. In particular, at a specific location (e.g., a vertex on a graph), we assume the cost accumulates over time and its growth rate is a random variable with a fixed but unknown mean, and the cost is reset to zero whenever any vehicle visits the vertex (representing the robot &quot;servicing&quot; the vertex). We formulate this problem in graph terminology and call it Team Orienteering Coverage Planning with Uncertain Reward (TOCPUR). We propose to solve TOCPUR by simultaneously estimating the accumulated cost at every vertex on the graph and solving a novel variant of the Team Orienteering Problem (TOP) iteratively, which we call the Team Orienteering Coverage Problem (TOCP). We provide the first mixed integer programming formulation for the TOCP, as a significant adaptation of the original TOP. We introduce a new benchmark consisting of hundreds of randomly generated graphs for comparing different methods. We show the proposed solution outperforms both the exact TOP solution and a greedy algorithm. In addition, we provide a demo of our method on a team of three physical robots in a real-world environment. The code is publicly available at https://github.com/Cranial-XIX/TOCPUR.git.},
  archive   = {C_IROS},
  author    = {Bo Liu and Xuesu Xiao and Peter Stone},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636288},
  pages     = {9728-9733},
  title     = {Team orienteering coverage planning with uncertain reward},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On connected deployment of delay-critical FANETs.
<em>IROS</em>, 9720–9727. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many safety critical scenarios, including post-disaster areas, or military fields, require prompt area monitoring and fast detection of events of interest. Flying Ad-hoc Networks (FANETs) provide a powerful tool to search the area, and locate anomalies. Nevertheless, wide-area deployment of FANETs poses a number of challenges. Existing long range communication technologies are inadequate to meet the data rate and delay requirements of a safety critical application. To face this challenge, we formulate the connected deployment problem, where we require the FANET to create connected formations to ensure multi-hop low-latency communications while performing the monitoring task. We show that addressing the above problem with the aim of maximizing event coverage is NP-hard. We propose a polynomial time solution, called Greedy Connected Deployment (GCD), based on a two phase approximation of the problem. By means of extensive simulations and real field experiments, we show that our approach outperforms existing solutions to related problems, both in terms of monitoring accuracy and system responsiveness.},
  archive   = {C_IROS},
  author    = {Novella Bartolini and Andrea Coletta and Matteo Prata and Camilla Serino},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636826},
  pages     = {9720-9727},
  title     = {On connected deployment of delay-critical FANETs},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Loosely synchronized search for multi-agent path finding
with asynchronous actions. <em>IROS</em>, 9714–9719. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent path finding (MAPF) determines an ensemble of collision-free paths for multiple agents between their respective start and goal locations. Among the available MAPF planners for workspace modeled as a graph, A*-based approaches have been widely investigated due to their guarantees on completeness and solution optimality, and have demonstrated their efficiency in many scenarios. However, almost all of these A*-based methods assume that each agent executes an action concurrently in that all agents start and stop together. This article presents a natural generalization of MAPF with asynchronous actions (MAPF-AA) where agents do not necessarily start and stop concurrently. The main contribution of the work is a proposed approach called Loosely Synchronized Search (LSS) that extends A*-based MAPF planners to handle asynchronous actions. We show LSS is complete and finds an optimal solution if one exists. We also combine LSS with other existing MAPF methods that aims to trade-off optimality for computational efficiency. Numerical results are presented to corroborate the performance of LSS and the applicability of the proposed method is verified in the Robotarium, a remotely accessible swarm robotics research platform.},
  archive   = {C_IROS},
  author    = {Zhongqiang Ren and Sivakumar Rathinam and Howie Choset},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636683},
  pages     = {9714-9719},
  title     = {Loosely synchronized search for multi-agent path finding with asynchronous actions},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Iterative refinement for real-time multi-robot path
planning. <em>IROS</em>, 9690–9697. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the iterative refinement of path planning for multiple robots, known as multi-agent pathfinding (MAPF). Given a graph, agents, their initial locations, and destinations, a solution of MAPF is a set of paths without collisions. Iterative refinement for MAPF is desirable for three reasons: 1) optimization is intractable, 2) sub-optimal solutions can be obtained instantly, and 3) it is anytime planning, desired in online scenarios where time for deliberation is limited. Despite the high demand, this is under-explored in MAPF because finding good neighborhoods has been unclear so far. Our proposal uses a sub-optimal MAPF solver to obtain an initial solution quickly, then iterates the two procedures: 1) select a subset of agents, 2) use an optimal MAPF solver to refine paths of selected agents while keeping other paths unchanged. Since the optimal solvers are used on small instances of the problem, this scheme yields efficient-enough solutions rapidly while providing high scalability. We also present reasonable candidates on how to select a subset of agents. Evaluations in various scenarios show that the proposal is promising; the convergence is fast, scalable, and with reasonable quality.},
  archive   = {C_IROS},
  author    = {Keisuke Okumura and Yasumasa Tamura and Xavier Défago},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636071},
  pages     = {9690-9697},
  title     = {Iterative refinement for real-time multi-robot path planning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A morphing quadrotor that can optimize morphology for
transportation. <em>IROS</em>, 9683–9689. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multirotors can be effectively applied to various tasks, such as transportation, investigation, exploration, and lifesaving, depending on the type of payload. However, due to the nature of multirotors, the payload loaded on the multirotor is limited in its position and weight, which presents a major disadvantage when the multirotor is used in various fields. In this paper, we propose a novel method that greatly improves the restrictions on payload position and weight using a morphing quadrotor system. Our method can estimate the drone’s weight, center of gravity position, and inertia tensor in real-time, which change depending on payload, and determine the optimal morphology for efficient and stable flight. An adaptive control method that can reflect the change in flight dynamics by payload and morphing is also presented. Experiments were conducted to confirm that the proposed morphing quadrotor improves the stability and efficiency in various situations of transporting payloads compared with the conventional quadrotor systems.},
  archive   = {C_IROS},
  author    = {Chanyoung Kim and Hyungyu Lee and Myeongwoo Jeong and Hyun Myung},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636558},
  pages     = {9683-9689},
  title     = {A morphing quadrotor that can optimize morphology for transportation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The pursuit and evasion of drones attacking an automated
turret. <em>IROS</em>, 9677–9682. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper investigates the pursuit-evasion problem of a defensive gun turret and one or more attacking drones. The turret must &quot;visit&quot; each attacking drone once, as quickly as possible, to defeat the threat. This constitutes a Shortest Hamiltonian Path (SHP) through the drones. The investigation considers situations with increasing fidelity, starting with a 2D kinematic model and progressing to a 3D dynamic model. In 2D we determine the region from which one or more drones can always reach a turret, or the region close enough to it where they can evade the turret. This provides optimal starting angles for n drones around a turret and the maximum starting radius for one and two drones.We show that safety regions also exist in 3D and provide a controller so that a drone in this region can evade the pan-tilt turret. Through simulations we explore the maximum range n drones can start and still have at least one reach the turret, and analyze the effect of turret behavior and the drones’ number, starting configuration, and behaviors.},
  archive   = {C_IROS},
  author    = {Daniel Biediger and Luben Popov and Aaron T. Becker},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636731},
  pages     = {9677-9682},
  title     = {The pursuit and evasion of drones attacking an automated turret},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Low-level pose control of tilting multirotor for wall
perching tasks using reinforcement learning. <em>IROS</em>, 9669–9676.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, needs for unmanned aerial vehicles (UAVs) that are attachable to the wall have been highlighted. As one of the ways to address the need, researches on various tilting multirotors that can increase maneuverability has been employed. Unfortunately, existing studies on the tilting multirotors require considerable amounts of prior information on the complex dynamic model. Meanwhile, reinforcement learning on quadrotors has been studied to mitigate this issue. Yet, these are only been applied to standard quadrotors, whose systems are less complex than those of tilting multirotors. In this paper, a novel reinforcement learning-based method is proposed to control a tilting multirotor on real-world applications, which is the first attempt to apply reinforcement learning to a tilting multirotor. To do so, we propose a novel reward function for a neural network model that takes power efficiency into account. The model is initially trained over a simulated environment and then fine-tuned using real-world data in order to overcome the sim-to-real gap issue. Furthermore, a novel, efficient state representation with respect to the goal frame that helps the network learn optimal policy better is proposed. As verified on real-world experiments, our proposed method shows robust controllability by overcoming the complex dynamics of tilting multirotors.},
  archive   = {C_IROS},
  author    = {Hyungyu Lee and Myeongwoo Jeong and Chanyoung Kim and Hyungtae Lim and Changgue Park and Sungwon Hwang and Hyun Myung},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636372},
  pages     = {9669-9676},
  title     = {Low-level pose control of tilting multirotor for wall perching tasks using reinforcement learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Aerial manipulator suspended from a cable-driven parallel
robot: Preliminary experimental results. <em>IROS</em>, 9662–9668. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Since omnidirectional aerial vehicles can generate a six degrees of freedom wrench, they could be used for dexterous manipulation tasks without the need for an additional robotic arm. However, they suffer from a reduced efficiency and dynamics range due to the huge amount of energy lost in gravity compensation.In this work, we introduce an omnidirectional aerial manipulator suspended from a cable-driven parallel robot (CDPR) by a spring, combining the advantages of the CDPR large workspace with the high dynamics of aerial vehicles, while reducing energy consumption thanks to gravity compensation.A partitioned control scheme is implemented to regulate both systems separately. A preliminary control strategy is proposed for the CDPR motion that minimizes the total energy consumption. Experiments are carried out to assess the added value of the CDPR carrier.},
  archive   = {C_IROS},
  author    = {Arda Yiğit and Miguel Arpa Perozo and Mandela Ouafo and Loïc Cuvillon and Sylvain Durand and Jacques Gangloff},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635933},
  pages     = {9662-9668},
  title     = {Aerial manipulator suspended from a cable-driven parallel robot: Preliminary experimental results},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Aerodynamic modeling of fully-actuated multirotor UAVs with
nonparallel actuators. <em>IROS</em>, 9639–9645. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The beneficial aspects of fully-actuated multirotor UAVs, provided by nonparallel rotor configuration, are increasingly being recognized and utilized to great benefit in high-precision applications. Full six-degree-of-freedom force control, higher control bandwidth and improved disturbance rejection prove valuable. However, the cant angle will cause great multirotor dihedral effect and significantly affects blade flapping, which decreases the flight performance of nonparallel actuated UAVs. Therefore, this paper presents a novel aerodynamic model for fully-actuated hexrotor UAVs while considering the aerodynamic effects caused due to tilt angled propeller configurations. In the proposed aerodynamic model, the significance of multirotor dihedral effect, defined as an aerodynamic coefficient proportional to the relative linear velocity of the UAV, is modeled for nonparallel actuators. Additionally, the modeling for blade flapping effect for cant angled propellers is provided to accurately model the aerodynamics. Wind tunnel experiments were conducted to characterize the aerodynamic constants for multirotor dihedral effect, blade flapping effect and air friction. Experimental results are presented to validate the proposed aerodynamic model on a fully-actuated hexrotor UAV (Purdue’s Dexterous Hexrotor). Lastly, the multirotor dihedral effect and blade flapping effect at different cant angles and at different wind speeds are analyzed.},
  archive   = {C_IROS},
  author    = {Praveen Abbaraju and Xin Ma and Guangying Jiang and Mo Rastgaar and Richard M. Voyles},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636572},
  pages     = {9639-9645},
  title     = {Aerodynamic modeling of fully-actuated multirotor UAVs with nonparallel actuators},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vision-encoder-based payload state estimation for autonomous
MAV with a suspended payload. <em>IROS</em>, 9632–9638. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous delivery of suspended payloads with MAVs has many applications in rescue and logistics transportation. Robust and online estimation of the payload status is important but challenging especially in outdoor environments. The paper develops a novel real-time system for estimating the payload position; the system consists of a monocular fisheye camera and a novel encoder-based device. A Gaussian fusion-based estimation algorithm is developed to obtain the payload state estimation. Based on the robust payload position estimation, a payload controller is presented to ensure the re-liable tracking performance on aggressive trajectories. Several experiments are performed to validate the high performance of the proposed method.},
  archive   = {C_IROS},
  author    = {Yunfan Ren and Jianheng Liu and Haoyao Chen and Yunhui Liu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636466},
  pages     = {9632-9638},
  title     = {Vision-encoder-based payload state estimation for autonomous MAV with a suspended payload},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Insect-inspired odor intake method for chemical plume
tracing in an outdoor environment. <em>IROS</em>, 9577–9583. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this study, we propose an insect-inspired odor intake method for carrying out chemical plume tracing (CPT) in an outdoor environment and experimentally investigated the relationship between the intake state and behavioral states of search. Tracking chemical plumes is important because it facilitates the identification of an odor source. Previous studies have focused on the development of CPT algorithms to improve CPT performance. However, there is still inadequate discussion on the effect of the amount of odor acquisition, which is an important factor in determining the state of searching behavior, on CPT performance. To address this issue, we first designed and developed an insect-inspired odor-intake device. To evaluate the performance of the device, we conducted CPT experiments in an outdoor environment. An insect-inspired CPT algorithm that relies only on odor information was adopted because the purpose of this study was to evaluate the performance of the odor intake device. As a result of the outdoor CPT experiment, it was found that by setting the appropriate combination of the odor intake state and the behavioral state, certain search performances can be maintained even in highly unpredictable environments, such as outdoors. This suggests that in order to improve the CPT performance, it is important not only to improve the algorithm but also to devise an odor intake method and to combine it with the behavioral state appropriately.},
  archive   = {C_IROS},
  author    = {Shunsuke Shigaki and Kei Okajima and Kazushi Sanada and Daisuke Kurabayashi},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636125},
  pages     = {9577-9583},
  title     = {Insect-inspired odor intake method for chemical plume tracing in an outdoor environment},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SENSORIMOTOR GRAPH: Action-conditioned graph neural network
for learning robotic soft hand dynamics. <em>IROS</em>, 9569–9576. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft robotics is a thriving branch of robotics which takes inspiration from nature and uses affordable flexible materials to design adaptable non-rigid robots. However, their flexible behavior makes these robots hard to model, which is essential for a precise actuation and for optimal control. For system modelling, learning-based approaches have demonstrated good results, yet they fail to consider the physical structure underlying the system as an inductive prior. In this work, we take inspiration from sensorimotor learning, and apply a Graph Neural Network to the problem of modelling a non-rigid kinematic chain (i.e. a robotic soft hand) taking advantage of two key properties: 1) the system is compositional, that is, it is composed of simple interacting parts connected by edges, 2) it is order invariant, i.e. only the structure of the system is relevant for predicting future trajectories. We denote our model as the &quot;Sensorimotor Graph&quot; since it learns the system connectivity from observation and uses it for dynamics prediction. We validate our model in different scenarios and show that it outperforms the non-structured baselines in dynamics prediction while being more robust to configurational variations, tracking errors or node failures.},
  archive   = {C_IROS},
  author    = {João Damião Almeida and Paul Schydlo and Atabak Dehban and José Santos-Victor},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636377},
  pages     = {9569-9576},
  title     = {SENSORIMOTOR GRAPH: Action-conditioned graph neural network for learning robotic soft hand dynamics},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Design and experimental learning of swimming gaits for a
magnetic, modular, undulatory robot. <em>IROS</em>, 9562–9568. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Here we developed an experimental platform with a magnetic, modular, undulatory robot (μBot) for studying fish-inspired underwater locomotion. This platform will enable us to systematically explore the relationship between body morphology, swimming gaits, and swimming performance via reinforcement learning methods. The μBot was designed to be easily modifiable in morphology, compact in size, easy to be controlled and inexpensive. The experimental platform also included a towing tank and a motion tracking system for real-time measurement of the μBot kinematics. The swimming gaits of μBot were generated by a central pattern generator (CPG), which outputs voltage signals to μBot&#39;s magnetic actuators. The CPG parameters were learned experimentally using the parameter exploring policy gradient (PGPE) method to maximize swimming speed. In the experiments, two μBot designs with the same body morphology but different caudal-fin shapes were tested. Results showed that swimming gaits with back-propagating traveling waves can be learned experimentally via PGPE, while the shape of the caudal fins had moderate influences on the learned gaits and the swimming speed. Furthermore, robot swimming speed was sensitive to the undulating frequency and the voltage magnitude of the last three posterior actuators. In contrast, swimming gaits and speed were relatively invariant to the variances within the inter-module connection weights of CPG and the voltage applied to the anterior actuator.},
  archive   = {C_IROS},
  author    = {Hankun Deng and Patrick Burke and Donghao Li and Bo Cheng},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636100},
  pages     = {9562-9568},
  title     = {Design and experimental learning of swimming gaits for a magnetic, modular, undulatory robot},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gaussian process regression for COP trajectory estimation in
healthy and pathological gait using instrumented insoles. <em>IROS</em>,
9548–9553. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Research in powered prostheses and orthoses has relied on COP measurements to inform a device’s controller about the body’s progression through the gait cycle, and to provide sensory substitution for prosthesis users, thereby helping them maintain balance during locomotion. Obtaining accurate COP measurements in out-of-the-lab contexts currently requires pressure sensitive insoles with dense arrays of sensing elements, which are expensive and bulky, limiting the accessibility and scalability of this technology. In this paper, we present a new method to reconstruct COP trajectories in over-ground walking tasks, using an affordable sensor array with eight sensing elements embedded in shoe insoles. The method leverages Gaussian Process Regression (GPR) to perform predictions from raw sensor data using Bayesian inference. A preliminary validation was carried out with a convenience sample of healthy individuals and patients with neuromuscular disorders. Combined mediolateral (ML) and anteroposterior (AP) errors where 2\% and 3\% for healthy individuals and patients, respectively. The analysis evidenced larger stride-to-stride variability in the ML COP excursion for the patient group, suggesting higher levels of motor noise associated with selective muscle weakness. These promising results indicate the potential of the proposed method to accurately estimate COP trajectories for future applications in wearable robotics and out-of-the-lab clinical gait assessments.},
  archive   = {C_IROS},
  author    = {Ton T. H. Duong and David Uher and Sally Dunaway Young and Tina Duong and Monica Sangco and Kayla Cornett and Jacqueline Montes and Damiano Zanotto},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636562},
  pages     = {9548-9553},
  title     = {Gaussian process regression for COP trajectory estimation in healthy and pathological gait using instrumented insoles},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A soft assistive device for elbow effort-compensation.
<em>IROS</em>, 9540–9547. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The use of assistive technologies in industrial environments to improve human ergonomics and comfort in repetitive and high effort tasks have increased considerably in the last decade. Predominantly, the goal is to provide additional physical support through lightweight and wearable devices, without posing major constraints to the human body movements. Towards achieving this objective, in this work we present a novel actuation mechanism for a soft assistive device, by taking into account the human elbow torque-angle profile. The proposed design integrates a single motor coupled with an elastic bungee and a cam-spool mechanism to enable energy exchange during the elbow flexion movement, while allowing for free-motions during the extension of the joint. A cable-driven transmission with passive elastic attachments is employed to implement compliant couplings with the wearer and to achieve easy donning/doffing. Experiments are conducted on two 3D printed functional prototypes. Results suggest that the assistive elbow torque is effectively transmitted with an average 90\% success for balancing a 5N payload, and the free-motion range of 108° is measured for both flexion and extension.},
  archive   = {C_IROS},
  author    = {Emir Mobedi and Wansoo Kim and Elena De Momi and Nikos G. Tsagarakis and Arash Ajoudani},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636771},
  pages     = {9540-9547},
  title     = {A soft assistive device for elbow effort-compensation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating the center of mass of human-exoskeleton systems
with physically coupled serial chain. <em>IROS</em>, 9532–9539. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Estimating the center of mass (CoM) is essential for both gait planning and controlling of lower limb exoskeletons. Different from CoM estimation in human and humanoid robots, a critical issue in human-exoskeleton systems pis how to describe the effect of physical human-exoskeleton interactions in estimating the CoM of lower limb exoskeletons. This paper presents a novel center of mass estimation method Physically Coupled Serial Chain (PCSC) for human-coupled lower limb exoskeleton systems. Different from traditional serial chain methods, the proposed PCSC involves physical human-exoskeleton models to describe physical interactions between the pilot and the lower limb exoskeleton. We demonstrated the effectiveness of proposed PCSC model in the AIDER lower limb exoskeleton system. Experimental results indicate that the proposed PCSC model is more accuracy than traditional serial chain methods.},
  archive   = {C_IROS},
  author    = {Rui Huang and Zhinan Peng and Siying Guo and Kecheng Shi and Chaobin Zou and Jing Qiu and Hong Cheng},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636514},
  pages     = {9532-9539},
  title     = {Estimating the center of mass of human-exoskeleton systems with physically coupled serial chain},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluation of lumbar burdens for endoskeleton-type assist
suit based on musculoskeletal model and its improvement of the utility.
<em>IROS</em>, 9518–9523. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In endoskeleton-type assist suits, since the compressive force is added to the spine, a lumbar disc may be badly affected. However, in general, the lumbar burdens while lifting a heavy object under wearing the endoskeleton-type assist suit have not been fully investigated. In this paper, first, lumbar burdens while lifting the heavy object under wearing the endoskeleton-type assist suit ‘Sustainable’ were investigated through dynamic analysis based on the musculoskeletal model software AnyBody Modeling System. The results of analysis showed that the lumbar burdens can be decreased under wearing the Sustainable as compared with not wearing the Sustainable. Second, in order to improve the utility of the Sustainable, suitable number of artificial muscles of the Sustainable was investigated. Under the condition that the CO 2 gas cylinder is used as an air source, Sustainable with two artificial muscles and that with one artificial muscle were compared in terms of assist effect, lumbar burdens and cost performance through measurements of practical assistable times and experiments of lifting the heavy object. Then, suitable number of artificial muscles of the Sustainable was determined.},
  archive   = {C_IROS},
  author    = {Chiharu Ishii and Kouhei Takahashi},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636063},
  pages     = {9518-9523},
  title     = {Evaluation of lumbar burdens for endoskeleton-type assist suit based on musculoskeletal model and its improvement of the utility},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Method for the determination of relative joint axes for
wearable inertial sensor applications. <em>IROS</em>, 9512–9517. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Wearable IMU sensing systems have widely been used in the study of human motion. For example, gait analysis using wearable inertial sensor systems is a tool used by clinicians to discriminate between typical and pathological walking. Similarly, key descriptors can be identified in spontaneous kicking to distinguish between typical and atypical motor development in infants. Oftentimes in human applications, precise placement of inertial sensors is difficult due to the irregular shape of human limbs. Without precise placement and alignment of the inertial sensors, meaningful joint kinematic data are difficult to extract as the orientation of the joint axes are unknown in the sensor&#39;s local frame. So, for applications where precise alignment may not be possible, a necessary first step is to identify the joint axes with respect to the local frame.In this work, we propose a method for the identification of joint axes for multiple degree of freedom (multi-DOF) joints in a kinematic chain using acceleration and angular rate data. This method couples a thresholding activity detection algorithm with a principal component analysis (PCA) dimensionality reduction technique. Furthermore, this method is validated on mimicked kicking data from a NAO robot. This method can determine joint axes of a kinematic chain from simultaneous movement data within an error ratio of 0.09.},
  archive   = {C_IROS},
  author    = {Katelyn E. Fry and Yu-Ping Chen and Ayanna Howard},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636686},
  pages     = {9512-9517},
  title     = {Method for the determination of relative joint axes for wearable inertial sensor applications},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Wearable tactile sensor suit for natural body dynamics
extraction: Case study on posture prediction based on physical reservoir
computing. <em>IROS</em>, 9504–9511. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a wearable tactile sensor suit, which can be regarded as tactile sensor networks, for monitoring natural body dynamics to be exploited as a computational resource for estimating the posture of a human or robot that wears it. We emulated the periodic motions of a wearer (a human and an android robot) using a novel sensor suit with a 9-channel fabric tactile sensor on the left arm. The emulation was conducted by using a linear regression (LR) model of sensor states as readout modules that predict the next wearer&#39;s movement using the current sensor data. Our result shows that the LR performance is comparable with other recurrent neural network approaches, suggesting that a fabric tactile sensor network can monitor the natural body motions, and further, this natural body dynamics itself can be used as an effective computational resource.},
  archive   = {C_IROS},
  author    = {Hidenobu Sumioka and Kohei Nakajima and Kurima Sakai and Takashi Minato and Masahiro Shiomi},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636194},
  pages     = {9504-9511},
  title     = {Wearable tactile sensor suit for natural body dynamics extraction: Case study on posture prediction based on physical reservoir computing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time 3D navigation-based semi-automatic surgical
robotic system for pelvic fracture reduction. <em>IROS</em>, 9498–9503.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pelvic fracture is a serious high-energy injury with highest disability and mortality rate among all fractures. At present, the reduction of pelvic fracture is still completely dependent on surgeons&#39; experience, which may lead to poor effect of pelvic reduction, thus seriously affecting surgical treatment and postoperative rehabilitation of patients. Based on this, a new robotic system for pelvic fracture reduction was developed and tested. Withdrawing on the optical tracking system, the real-time 3D navigation of pelvic position during operation was realized through Nonrigid ICP method. The target position for fracture reduction was obtained through pelvic symmetry reduction method based on structural symmetry of the pelvis. The shortest reduction path was planned automatically, which could be adjusted by surgeons manually. Finally, the pelvic fracture reduction operation was completed through the robot. System accuracy and effectiveness were demonstrated through laboratory trials and preliminary cadaveric trials. The system resulted in high fracture reduction reliability with the registration accuracy of 1.3749 ± 0.6311mm, and the robot reduction accuracy of 2.8925 ± 0.8647mm. Preliminary cadaveric trials also provided a positive and favorable outcome pointing to the usability of the system in the operating theatre, potentially enhancing the capacity of pelvic fracture surgeries.},
  archive   = {C_IROS},
  author    = {Chao Shi and Xiangrui Zhao and Xinbao Wu and Chunpeng Zhao and Gang Zhu and Shuchang Shi and Yu Wang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636647},
  pages     = {9498-9503},
  title     = {Real-time 3D navigation-based semi-automatic surgical robotic system for pelvic fracture reduction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semi-supervised vein segmentation of ultrasound images for
autonomous venipuncture. <em>IROS</em>, 9475–9481. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Venipuncture is an indispensable procedure for both diagnosis and treatment. In this paper, unlike existing solutions that fully or partially rely on professional assistance, a compact robotic system integrating both novel hardware and software developments is introduced. The hardware consists of a set of units to facilitate the supporting, positioning, puncturing, and imaging functionalities. To achieve full automation, a novel deep learning framework — semi-ResNeXt-Unet for semi-supervised vein segmentation from ultrasound images is proposed. The depth information of vein is calculated and enables the automated navigation for the puncturing unit. The algorithm is validated on 40 volunteers, and the proposed semi-ResNeXt-Unet improves the dice similarity coefficient (DSC) by 5.36\%, decreases the centroid error by 1.38 pixels and decreases the failure rate by 5.60\%, compared to fully-supervised ResNeXt-Unet.},
  archive   = {C_IROS},
  author    = {Yu Chen and Yuxuan Wang and Bolin Lai and Zijie Chen and Xu Cao and Nanyang Ye and Zhongyuan Ren and Junbo Zhao and Xiao-Yun Zhou and Peng Qi},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636149},
  pages     = {9475-9481},
  title     = {Semi-supervised vein segmentation of ultrasound images for autonomous venipuncture},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Autonomous scanning target localization for robotic lung
ultrasound imaging. <em>IROS</em>, 9467–9474. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Under the ceaseless global COVID-19 pandemic, lung ultrasound (LUS) is the emerging way for effective diagnosis and severeness evaluation of respiratory diseases. However, close physical contact is unavoidable in conventional clinical ultrasound, increasing the infection risk for health-care workers. Hence, a scanning approach involving minimal physical contact between an operator and a patient is vital to maximize the safety of clinical ultrasound procedures. A robotic ultrasound platform can satisfy this need by remotely manipulating the ultrasound probe with a robotic arm. This paper proposes a robotic LUS system that incorporates the automatic identification and execution of the ultrasound probe placement pose without manual input. An RGB-D camera is utilized to recognize the scanning targets on the patient through a learning-based human pose estimation algorithm and solve for the landing pose to attach the probe vertically to the tissue surface; A position/force controller is designed to handle intraoperative probe pose adjustment for maintaining the contact force. We evaluated the scanning area localization accuracy, motion execution accuracy, and ultrasound image acquisition capability using an upper torso mannequin and a realistic lung ultrasound phantom with healthy and COVID-19-infected lung anatomy. Results demonstrated the overall scanning target localization accuracy of 19.67 ± 4.92 mm and the probe landing pose estimation accuracy of 6.92 ± 2.75 mm in translation, 10.35 ± 2.97 deg in rotation. The contact force-controlled robotic scanning allowed the successful ultrasound image collection, capturing pathological landmarks.},
  archive   = {C_IROS},
  author    = {Xihan Ma and Ziming Zhang and Haichong K. Zhang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635902},
  pages     = {9467-9474},
  title     = {Autonomous scanning target localization for robotic lung ultrasound imaging},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards a compact vision-based auto-focusing system for
endoscopic laser surgery. <em>IROS</em>, 9461–9466. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Endoscopic laser tools have been recently proposed in order to overcome the limitations of state-of-the-art laser tools, by integrating fiber-coupled lasers into flexible endoscopic systems. One of the main challenges in designing such endoscopic tools consists in the focusing of the laser, that requires to be frequently adjusted reducing the reliability of the system and increasing surgeons’ mental workload. To avoid these problems, compact auto-focusing tools have been recently developed, taking advantage of MEMS varifocal mirrors (VM) to allow integration with endoscopic tools. In this paper, we integrate such VM-based tool with a distance sensing algorithm based on 3D surface reconstruction to achieve a complete autofocusing system. We evaluate the performance of the proposed integrated system by ablating lines on plaster block targets at variable distance and comparing the obtained ablation depth and width with that of a fixed focus system. Preliminary results show that the proposed system is able to keep the laser in focus resulting in uniform ablation lines for distance ranges from 14mm to 22mm.},
  archive   = {C_IROS},
  author    = {Andre Geraldes and Veronica Penza and Leonardo S. Mattos},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636740},
  pages     = {9461-9466},
  title     = {Towards a compact vision-based auto-focusing system for endoscopic laser surgery},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Domain adaptive robotic gesture recognition with
unsupervised kinematic-visual data alignment. <em>IROS</em>, 9453–9460.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automated surgical gesture recognition is of great importance in robot-assisted minimally invasive surgery. However, existing methods assume that training and testing data are from the same domain, which suffers from severe performance degradation when a domain gap exists, such as the simulator and real robot. In this paper, we propose a novel unsupervised domain adaptation framework which can simultaneously transfer multi-modality knowledge, i.e., both kinematic and visual data, from simulator to real robot. It remedies the domain gap with enhanced transferable features by using temporal cues in videos, and inherent correlations in multi-modal towards recognizing gesture. Specifically, we first propose a Motion Direction Oriented Kinematics feature alignment (MDO-K) to align kinematics, which exploits temporal continuity to transfer motion directions with smaller gap rather than position values, relieving the adaptation burden. Moreover, we propose a Kinematic and Visual Relation Attention (KV-Relation-ATT) to transfer the co-occurrence signals of kinematics and vision. Such features attended by correlation similarity are more informative for enhancing domain-irreverent of the model. Two feature alignment strategies benefit the model mutually during the end-to-end learning process. We extensively evaluate our method for gesture recognition using DESK dataset with peg transfer procedure. Results show that our approach recovers the performance with great improvement gains, up to 12.91\% in Accuracy and 20.16\% in F1score without using any annotations in real robot.},
  archive   = {C_IROS},
  author    = {Xueying Shi and Yueming Jin and Qi Dou and Jing Qin and Pheng-Ann Heng},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636578},
  pages     = {9453-9460},
  title     = {Domain adaptive robotic gesture recognition with unsupervised kinematic-visual data alignment},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MBAPose: Mask and bounding-box aware pose estimation of
surgical instruments with photorealistic domain randomization.
<em>IROS</em>, 9445–9452. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Surgical robots are usually controlled using a priori models based on the robots’ geometric parameters, which are calibrated before the surgical procedure. One of the challenges in using robots in real surgical settings is that those parameters can change over time, consequently deteriorating control accuracy. In this context, our group has been investigating online calibration strategies without added sensors. In one step toward that goal, we have developed an algorithm to estimate the pose of the instruments’ shafts in endoscopic images. In this study, we build upon that earlier work and propose a new framework to more precisely estimate the pose of a rigid surgical instrument. Our strategy is based on a novel pose estimation model called MBAPose and the use of synthetic training data. Our experiments demonstrated an improvement of 21\% for translation error and 26\% for orientation error on synthetic test data with respect to our previous work. Results with real test data provide a baseline for further research.},
  archive   = {C_IROS},
  author    = {Masakazu Yoshimura and Murilo M. Marinho and Kanako Harada and Mamoru Mitsuishi},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636404},
  pages     = {9445-9452},
  title     = {MBAPose: Mask and bounding-box aware pose estimation of surgical instruments with photorealistic domain randomization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Understanding and segmenting human demonstrations into
reusable compliant primitives. <em>IROS</em>, 9437–9444. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hard coded robotic manipulation skills work well in known, predictable and repeatable situations. Human environments, however, are better described as dynamic, chaotic, uncertain or unstructured. Therefore, plans relying on preprogrammed trajectories are bound to fail in these settings. In order to increase robustness to uncertainty and avoid coding new skills from scratch, we can make flexible plans that execute existing autonomous primitives based on the sensed state of the environment. A key challenge of this approach is finding the sequence of primitives required to perform the desired task. This work uses a variation of a Hidden Markov Model (HMM) with an augmented particle filter to find the primitive sequence using only a reduced number of human demonstrations. The algorithm was tested on 40 demonstrations of two different manipulation tasks involving six primitives. It was seeded with a single manually labelled demonstration of each task and was able to automatically label the other 38 demonstration sequences with an average success of 81.5\%. The results show improved convergence and a 9\% increase in accuracy over other versions of the algorithm.},
  archive   = {C_IROS},
  author    = {Elena Galbally Herrero and Jonathan Ho and Oussama Khatib},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636523},
  pages     = {9437-9444},
  title     = {Understanding and segmenting human demonstrations into reusable compliant primitives},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning initial trajectory using sequence-to-sequence
approach to warm start an optimization-based motion planner.
<em>IROS</em>, 9430–9436. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, optimization-based motion planners have shown that they can provide a fast, smooth, and locally optimal trajectory even for a higher dimension planning problem. Their convergence rate depends on the given initial trajectory. The proper selection of an initial trajectory is crucially important: if it is not within the basin of attraction of the optimum, it will take longer to convergence or even get stuck in local minima. This paper presents a neural network-based initial trajectory predictor, which utilizes the power of the sequence-to-sequence (Seq2Seq) learning method to predict a good initial trajectory for an optimization-based motion planner even in an unseen environment. The proposed model learns the mapping between the tasks and the optimal trajectories from a database. Given a start and a goal configuration of a manipulator along with the environment information in the form of a voxel grid, the proposed model predicts a good initial trajectory, which was learned from previously seen situations. The learned model is evaluated in a 6 degree of freedom (DOF) manipulator planning in two different environments. The results show that by using the predicted initial trajectory, there is a significant improvement in the convergence rate and the planning time of an optimization-based motion planner, even in an unseen environment.},
  archive   = {C_IROS},
  author    = {Sankaranarayanan Natarajan},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636456},
  pages     = {9430-9436},
  title     = {Learning initial trajectory using sequence-to-sequence approach to warm start an optimization-based motion planner},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic grasping with reachability and motion awareness.
<em>IROS</em>, 9422–9429. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Grasping in dynamic environments presents a unique set of challenges. A stable and reachable grasp can become unreachable and unstable as the target object moves, motion planning needs to be adaptive and in real time, the delay in computation makes prediction necessary. In this paper, we present a dynamic grasping framework that is reachability-aware and motion-aware. Specifically, we model the reachability space of the robot using a signed distance field which enables us to quickly screen unreachable grasps. Also, we train a neural network to predict the grasp quality conditioned on the current motion of the target. Using these as ranking functions, we quickly filter a large grasp database to a few grasps in real time. In addition, we present a seeding approach for arm motion generation that utilizes solution from previous time step. This quickly generates a new arm trajectory that is close to the previous plan and prevents fluctuation. We implement a recurrent neural network (RNN) for modelling and predicting the object motion. Our extensive experiments demonstrate the importance of each of these components and we validate our pipeline on a real robot.},
  archive   = {C_IROS},
  author    = {Iretiayo Akinola and Jingxi Xu and Shuran Song and Peter K. Allen},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636057},
  pages     = {9422-9429},
  title     = {Dynamic grasping with reachability and motion awareness},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to hit: A statistical dynamical system based
approach. <em>IROS</em>, 9415–9421. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a manipulation scheme based on learning the motion of objects after being hit by a robotic end-effector. This allows for the object to be positioned at a desired location outside the physical workspace of the robot. An estimate of the object dynamics under friction and collisions is learnt and used to predict the desired hitting parameters (speed and direction), given the initial and desired location of the object. Based on the obtained hitting parameters, the desired pre-impact velocity of the end-effector is generated using a stable dynamical system. The performance of the proposed DS is validated in simulation and and is used to learn a model for hitting using real robot. The approach is tested on real robot with a KUKA LBR IIWA robot.},
  archive   = {C_IROS},
  author    = {Harshit Khurana and Michael Bombile and Aude Billard},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635976},
  pages     = {9415-9421},
  title     = {Learning to hit: A statistical dynamical system based approach},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic pre-grasp planning when tracing a moving object
through a multi-agent perspective. <em>IROS</em>, 9408–9414. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While a human is tracking a moving object to prepare for later grasping, we naturally change our hand pose to generate optimal pre-grasp to avoid post-grasp adjustment. Robot hand controllers need dynamic pre-grasp planning capability, so they are not limited in dynamic tracking and catching tasks. To fill this gap, we explore the feasibility of using a two-stage optimization method to enable dynamic pre-grasp planning of individual fingers while tracking a moving object to ensure a later successful grasp. The first stage adopts multi-agent pursuit to partition the search space on the object surface. The method allows each finger to consider its immediate surroundings in a local view instead of globally determining the best location for all fingers. The search space for each finger is dramatically reduced since sensible alternatives are the ones left after pruning. Each finger goal location acts independently yet coordinates with others to achieve the goal of covering the object. In the second stage, four different goal point movement strategies are presented to impact the finger goal location in their respective search space to demonstrate the ability to facilitate different needs of the task and requirements of the designer. Dynamic finger goal adaption is obtained by iteratively updating these two stages. The approach is consistent in different scenarios for the object.},
  archive   = {C_IROS},
  author    = {Michael Bowman and Xiaoli Zhang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636666},
  pages     = {9408-9414},
  title     = {Dynamic pre-grasp planning when tracing a moving object through a multi-agent perspective},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Coordinated motion generation and object placement: A
reactive planning and landing approach. <em>IROS</em>, 9401–9407. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Similar to human work, robotic tasks sometimes require two hands to be accomplished. This requires coordinated motion planning and control. While fulfilling the task in a coordinated manner is already a big challenge, the task at hand becomes even harder when obstacles are introduced in the environment that need to be avoided. Furthermore in the case of dynamic environments, contacts cannot be avoided all the time, even with robust planning. In addition to geometric constraints, bimanual systems need to be able to detect and react to contacts during task execution. To this aim, we integrate a vector-field based planning scheme, that is able to avoid obstacles, with contact detection and reactive control methods based on contact wrench estimation such as admittance control. We also fuse the real contact forces into the planner directly together with the circular repulsive fields. The resulting planner-controller combination is capable of obstacle avoidance planning as well as reaction control in the case of unforeseen contacts that can also be used in situations where the manipulation needs to be guided by the environment such as landing control in only roughly known environments. We evaluate our approach on the torque-controlled Kobo bimanual set-up and also perform rigorous simulation studies.},
  archive   = {C_IROS},
  author    = {Riddhiman Laha and Jonathan Vorndamme and Luis F.C. Figueredo and Zheng Qu and Abdalla Swikir and Christoph Jähne and Sami Haddadin},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636652},
  pages     = {9401-9407},
  title     = {Coordinated motion generation and object placement: A reactive planning and landing approach},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Motion and force planning for manipulating heavy objects by
pivoting. <em>IROS</em>, 9393–9400. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Manipulation of objects by exploiting their contact with the environment can enhance both the dexterity and payload capability of robotic manipulators. A common way to manipulate heavy objects beyond the payload capability of a robot is to use a sequence of pivoting motions, wherein, an object is moved while some contact points between the object and a support surface are kept fixed. The goal of this paper is to develop an algorithmic approach for automated plan generation for object manipulation with a sequence of pivoting motions. A plan for manipulating a heavy object consists of a sequence of joint angles of the manipulator, the corresponding object poses, as well as the joint torques required to move the object. The constraint of maintaining object contact with the ground during manipulation results in nonlinear constraints in the configuration space of the robot, which is challenging for motion planning algorithms. Exploiting the fact that pivoting motion corresponds to movements in a subgroup of the group of rigid body motions, SE(3), we present a novel task-space based planning approach for computing a motion plan for both the manipulator and the object while satisfying contact constraints. We also combine our motion planning algorithm with a grasping force synthesis algorithm to ensure that friction constraints at the contacts and actuator torque constraints are satisfied. We present simulation results with a dual-armed Baxter robot to demonstrate our approach.},
  archive   = {C_IROS},
  author    = {Amin Fakhari and Aditya Patankar and Nilanjan Chakraborty},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636103},
  pages     = {9393-9400},
  title     = {Motion and force planning for manipulating heavy objects by pivoting},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Planning robotic manipulation with tight environment
constraints. <em>IROS</em>, 9385–9392. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many real-world manipulation problems, the constraints imposed by the environment on an object are tight. In these cases, most state-of-the-art planners struggle to fit satisfactorily in low dimensional sub-manifolds, while still ensuring geometric and force feasibility. On the other hand, humans are at ease with such situations and indeed exploit constraints to manipulate objects proficiently.To face this challenge, we propose to merge state-of-art randomized grasp planning methods with model-based grasp analysis. We use the partial form-closure analysis framework to find the geometrically feasible motions of the object. Then, to ensure that the desired motions are physically realizable by the robot, we resort to an extension of the force-closure analysis framework accounting also for dynamic friction. We use these instruments to construct a random tree in a simplified planning space containing only object-robot configurations that are reachable through effectively applicable contact forces. The algorithm, validated in simulation and in preliminary experiments with a collaborative robot, features the ability to compute solutions for heavily constrained real-world manipulation problems.},
  archive   = {C_IROS},
  author    = {George Jose Pollayil and Giorgio Grioli and M. Bonilla and Antonio Bicchi},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636782},
  pages     = {9385-9392},
  title     = {Planning robotic manipulation with tight environment constraints},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Slip modeling and simulation of spiral zipper
friction-driven prismatic actuator. <em>IROS</em>, 9347–9352. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this study, based on an analysis of experimental results, we propose a method for slip modeling of Spiral Zipper. The Spiral Zipper is a prismatic actuator that extracts a length-changeable cylindrical tube from a flexible ABS band. The band tube is driven by a friction wheel; the slip results due to the lack of synchronization between the motion of the band and that of the friction wheel. Through experiments, we verify that the flexibility of the band causes an impact force between the band and the friction wheel. Additionally, we verify that the tube extraction process causes resistance, which contributes to the increase in the relative angular velocity. Considering the factors mentioned above, we develop a slip model by employing different numbers of contact patches that express the flexibility of the band. Subsequently, we validate the model’s performance through simulations. In addition, we propose a coefficients standardization method for obtaining similar results with different number of contact patches We expect that this study will be a basis for design optimization and control system development of the Spiral Zipper.},
  archive   = {C_IROS},
  author    = {Seohyeon Lee and Sahoon Ahn and Devin Carroll and Mark Yim and TaeWon Seo},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636843},
  pages     = {9347-9352},
  title     = {Slip modeling and simulation of spiral zipper friction-driven prismatic actuator},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gear ratio optimization of a multifunctional walker robot
using dual-motor actuation. <em>IROS</em>, 9339–9346. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Optimization of gear ratios for dual-motor actuators is presented for the development of a walker-type assist robot. The robot is reconfigurable to provide an elderly user with multiple physical support functions; one is to assist sitto-stand transitions and the other is to serve as a walker to aid the user in walking. To avoid falling while walking, the robot must react quickly and reconfigure its footprint for supporting the user. This requires high speed actuators. In contrast, for assisting sit-to-stand transitions, high torque actuators are required. To meet the bimodal, conflicting load conditions, i.e. high-torque, low-speed v.s. high-speed, low-torque, this paper presents a dual-motor actuation solution, where two motors with diverse gear ratios are combined. The system is characterized with two key parameters: an internal gear ratio between a high-speed motor and a torque-booster, and an external gear ratio for connecting the dual motor actuator to the load. The internal and external gear ratios are optimized for performing both sit-to-stand assist and rapid foot reconfiguration.},
  archive   = {C_IROS},
  author    = {John Bell and Emily Kamienski and Seiichi Teshigawara and Hirofumi Itagaki and H. Harry Asada},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636482},
  pages     = {9339-9346},
  title     = {Gear ratio optimization of a multifunctional walker robot using dual-motor actuation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A safe and rapidly switchable stiffness hydrostatic actuator
through valve-controlled air springs. <em>IROS</em>, 9333–9338. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hydrostatic transmission has shown promising results for enabling the manipulator to achieve low effective inertia, high stiffness, and high torque density. However, the incompressibility of fluid causes the lack of compliance, so that it could not provide intrinsic safety. Thus, it would be advantageous to introduce series compliance on the hydrostatic manipulator for adjusting stiffness depending on the situation. Here, we developed a safe and high-performance hydrostatic actuator based on the switchable stiffness mechanism implemented with an air spring and solenoid valve. The hydrostatic transmission is implemented with rolling diaphragms to attain zero fluid leakage and low seal friction. Air spring is serially connected to hydraulic lines for achieving compliance. Its modes (i.e., stiff and compliant modes) can be rapidly switched by modulating water flow via the solenoid valve. Block stiffness experiment shows that the stiffness of stiff mode is 9.63 times stiffer than one of compliant mode at 100 kPa. We experimentally demonstrated that compliant mode could mitigate the impact force to the level that a tangerine would not be crushed. The stiffness was switched within 12 ms; hence it is fast enough to be used for feedback application with vision or tactile sensors. As a result, the developed actuator can ensure safety without sacrificing the dynamic performance, owing to the simple and rapidly switchable stiffness mechanism.},
  archive   = {C_IROS},
  author    = {Sungbin Park and Kyungseo Park and Hwayeong Jeong and Wonseok Shin and Jung Kim},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636766},
  pages     = {9333-9338},
  title     = {A safe and rapidly switchable stiffness hydrostatic actuator through valve-controlled air springs},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Convex optimization for spring design in series elastic
actuators: From theory to practice. <em>IROS</em>, 9327–9332. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Natural dynamics, nonlinear optimization, and, more recently, convex optimization are available methods for stiffness design of energy-efficient series elastic actuators. Natural dynamics and general nonlinear optimization only work for a limited set of load kinetics and kinematics, cannot guarantee convergence to a global optimum, or depend on initial conditions to the numerical solver. Convex programs alleviate these limitations and allow a global solution in polynomial time, which is useful when the space of optimization variables grows (e.g., when designing optimal nonlinear springs or co-designing spring, controller, and reference trajectories). Our previous work introduced the stiffness design of series elastic actuators via convex optimization when the transmission dynamics are negligible, which is an assumption that applies mostly in theory or when the actuator uses a direct or quasi-direct drive. In this work, we extend our analysis to include friction at the transmission. Coulomb friction at the transmission results in a non-convex expression for the energy dissipated as heat, but we illustrate a convex approximation for stiffness design. We experimentally validated our framework using a series elastic actuator with specifications similar to the knee joint of the Open Source Leg, an open-source robotic knee-ankle prosthesis.},
  archive   = {C_IROS},
  author    = {Edgar A. Bolívar-Nieto and Gray C. Thomas and Elliott Rouse and Robert D. Gregg},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636427},
  pages     = {9327-9332},
  title     = {Convex optimization for spring design in series elastic actuators: From theory to practice},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online impedance adaptation facilitates manipulating a whip.
<em>IROS</em>, 9297–9302. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Manipulation of flexible objects is one of the major challenges in robotics as the nonlinear dynamics of the high-dimensional object structure makes it difficult to apply current control methods. A previous simulation study showed that control with few pre-structured joint trajectories coupled with joint impedance (dynamic primitives) could control a 25-dimensional whip to hit a target. This was possible even though the impedance values were constant. This paper explores whether time-varying impedance throughout the movement may further enhance performance. We present an online impedance adaptation (OIA) controller that modulates the joint impedances of a two-joint actuator in real time for the same task. Results showed that the OIA control method increased the speed of optimization and resulted in smaller deviation from the zero-torque joint trajectories compared to the controller with constant joint impedances. This novel way to modulate both motion and impedance of a manipulator may facilitate the control of flexible objects with significant dynamics.},
  archive   = {C_IROS},
  author    = {Xiaofeng Xiong and Moses C. Nah and Aleksei Krotov and Dagmar Sternad},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636663},
  pages     = {9297-9302},
  title     = {Online impedance adaptation facilitates manipulating a whip},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-stage energy-aware motion control with
exteroception-defined dynamic safety metric. <em>IROS</em>, 9290–9296.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the problem of motion control for safe physical interaction, and in particular finding new ways for impedance controller parameters’ adaptation to ensure better safety with minimum possible lose in robot performance. We propose an exteroception-based dynamically updated safety metric that takes into account current robot state and inertia as well as external objects’ mass, shape, material properties, velocity, sensor confidence and existing sampling rates. We also present how this metric can be applied to design a finite state machine of the multi-stage controller, which allows us to prioritize either safety of performance by setting different energy and power constraints with smooth transition in between free motion and interaction modes.},
  archive   = {C_IROS},
  author    = {Kirill Artemov and Sergey Kolyubin and Stefano Stramigioli},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636561},
  pages     = {9290-9296},
  title     = {Multi-stage energy-aware motion control with exteroception-defined dynamic safety metric},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Impedance control for a flexible robot enhanced with energy
tanks in the port-hamiltonian framework. <em>IROS</em>, 9283–9289. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In modern robotics, the manipulators are no longer isolated under fully controlled conditions but rather conceived to work in unconstrained environments. Under these operations, compliant control and passivity properties of the robot are of great importance, and thus the system’s energy function plays a crucial role in the control design. In this work, we propose a new design of cartesian impedance control for a flexible robot whose dynamics is represented within the port-Hamiltonian framework. To improve the performance of the system and maximize the capabilities of the robot, the robotic control system is enhanced with energy tanks that allow for temporarily non-passive operations, but ensure the passivity of the extended system. In addition, a secondary controller is designed using the port-Hamiltonian approach to cover the case of redundant robotic manipulators. The performance of the full control system is tested via simulations of the Kuka iiwa manipulator in closed loop with the proposed passivity-based controller. The results show a satisfactory performance of the control system for set-point regulation, external forces, time-varying reference trajectories, and parametric uncertainty.},
  archive   = {C_IROS},
  author    = {Martin Mujica and Alejandro Donaire and Mourad Benoussaad and Jean-Yves Fourquet},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636472},
  pages     = {9283-9289},
  title     = {Impedance control for a flexible robot enhanced with energy tanks in the port-hamiltonian framework},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Human guided trajectory and impedance adaptation for
tele-operated physical assistance. <em>IROS</em>, 9276–9282. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human physical assistance requires the assistant to tune both his trajectory and impedance in order to assist an individual as well as be guided by him. In this study we propose a controller for teleoperated human assistance that allows the assistant to guide the assisting robot in both trajectory and impedance. We propose to use the inherent perturbations in the task, induced by the elderly or stroke patient, for impedance estimation, while a simple neuroscience based filter allows the reference estimation of the operator. We tested our impedance estimation and the controller as a whole in two experiments in which a human operator guided a robot suffering force perturbations that simulated a human patient.},
  archive   = {C_IROS},
  author    = {Guillaume Gourmelen and Benjamin Navarro and Andrea Cherubini and Gowrishankar Ganesh},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636418},
  pages     = {9276-9282},
  title     = {Human guided trajectory and impedance adaptation for tele-operated physical assistance},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Circumventing conceptual flaws in classical interaction
control strategies. <em>IROS</em>, 9270–9275. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies the problem of interaction control based on force and velocity measurements. Without restricting ourselves to fixed control structures such as admittance or impedance control, we consider the general topology of the problem. Adopting the idea of Youla-based 2DOF control, we reveal a generic control architecture that splits the problem into two independent parts: nominal admittance shaping and disturbance rejection. This pinpoints conceptual flaws in classical interaction control architectures and suggests a way to circumvent them. As a by-product, a complete and compact parameterization of all achievable admittances is derived. The potential of the proposed architecture for analysis and controller design is demonstrated and validated experimentally.},
  archive   = {C_IROS},
  author    = {Maxim Kristalny and Jang Ho Cho},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636476},
  pages     = {9270-9275},
  title     = {Circumventing conceptual flaws in classical interaction control strategies},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A vision-based irregular obstacle avoidance framework via
deep reinforcement learning. <em>IROS</em>, 9262–9269. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep reinforcement learning has achieved great success in laser-based collision avoidance work because the laser can sense accurate depth information without too much redundant data, which can maintain the robustness of the algorithm when it is migrated from the simulation environment to the real world. However, high-cost laser devices are not only difficult to apply on a large scale but also have poor robustness to irregular objects, e.g., tables, chairs, shelves, etc. In this paper, we propose a vision-based collision avoidance framework to solve the challenging problem. Our method attempts to estimate the depth and incorporate the semantic information from RGB data to obtain a new form of data, pseudo-laser data, which combines the advantages of visual information and laser information. Compared to traditional laser data that only contains the one-dimensional distance information captured at a certain height, our proposed pseudo-laser data encodes the depth information and semantic information within the image, which makes our method more effective for irregular obstacles. Besides, we adaptively add noise to the laser data during the training stage to increase the robustness of our model in the real world, due to the estimated depth information is not accurate. Experimental results show that our framework achieves state-of-the-art performance in several unseen virtual and real-world scenarios.},
  archive   = {C_IROS},
  author    = {Lingping Gao and Jianchuan Ding and Wenxi Liu and Haiyin Piao and Yuxin Wang and Xin Yang and Baocai Yin},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636512},
  pages     = {9262-9269},
  title     = {A vision-based irregular obstacle avoidance framework via deep reinforcement learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to navigate in a VUCA environment: Hierarchical
multi-expert approach. <em>IROS</em>, 9254–9261. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite decades of efforts, robot navigation in a real scenario with volatility, uncertainty, complexity, and ambiguity (VUCA for short), remains a challenging topic. Inspired by the central nervous system (CNS), we propose a hierarchical multi-expert learning framework for autonomous navigation in a VUCA environment. With a heuristic exploration mechanism considering target location, path cost, and safety level, the upper layer performs simultaneous map exploration and route-planning to avoid trapping in a blind alley, similar to the cerebrum in the CNS. Using a local adaptive model fusing multiple discrepant strategies, the lower layer pursuits a balance between collision-avoidance and go-straight strategies, acting as the cerebellum in the CNS. We conduct simulation and real-world experiments on multiple platforms, including legged and wheeled robots. Experimental results demonstrate our algorithm outperforms the existing methods in terms of task achievement, time efficiency, and security. A video of our results is available at https://youtu.be/lAnW4QIWDoU.},
  archive   = {C_IROS},
  author    = {Wenqi Zhang and Kai Zhao and Peng Li and Xiao Zhu and Faping Ye and Weijie Jiang and Huiqiao Fu and Tao Wang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636370},
  pages     = {9254-9261},
  title     = {Learning to navigate in a VUCA environment: Hierarchical multi-expert approach},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Continuous-time gaussian process trajectory generation for
multi-robot formation via probabilistic inference. <em>IROS</em>,
9247–9253. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we extend a famous motion planning approach, GPMP2, to multi-robot cases, yielding a novel centralized trajectory generation method for the multi-robot formation. A sparse Gaussian Process model is employed to represent the continuous-time trajectories of all robots as a limited number of states, which improves computational efficiency due to the sparsity. We add constraints to guarantee collision avoidance between individuals as well as formation maintenance, then all constraints and kinematics are formulated on a factor graph. By introducing a global planner, our proposed method can generate trajectories efficiently for a team of robots which have to get through a width-varying area by adaptive formation change. Finally, we provide the implementation of an incremental replanning algorithm to demonstrate the online operation potential of our proposed framework. The experiments in simulation and real world illustrate the feasibility, efficiency and scalability of our approach.},
  archive   = {C_IROS},
  author    = {Shuang Guo and Bo Liu and Shen Zhang and Jifeng Guo and Changhong Wang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636573},
  pages     = {9247-9253},
  title     = {Continuous-time gaussian process trajectory generation for multi-robot formation via probabilistic inference},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image-based online command adaptation and guidance to
arbitrarily shaped objects for robot-assisted medical procedures.
<em>IROS</em>, 9241–9246. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Imaging techniques are established aids in surgical procedures and have become indispensable in modern medicine. Combined with robot-assisted systems, significantly higher precision can already be achieved today and work steps can be simplified. We hypothesize that algorithms can make the operations of the future safer and easier. The paper introduces an approach for the control of a robotic manipulator that, based on standardized imaging techniques, monitors and adapts externally given movement commands. The physician controls the system, benefits from the robot’s precision, and movements will be intervened in if unwanted contact with the environment would occur. This is important, e.g., when operating near sensitive tissue, nerves or bone that must not be injured. Objects from the image data are considered in the control scheme and may have arbitrary shape and size. Furthermore, since the approach can be in principle adapted to any shape it is shown how robot operations in confined spaces, that are challenging to achieve using an external controller, can be simplified by the introduced approach. The online evaluation takes place in Cartesian space and is then transformed into joint space using the inverse kinematics. The approach is implemented using GAZEBO and a simulation study is performed for the 6-DOF industrial manipulator Stäubli TX2-60.},
  archive   = {C_IROS},
  author    = {Jan Reinhold and Jonas Olschewski and Sebastian Lippross and Thomas Meurer},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636303},
  pages     = {9241-9246},
  title     = {Image-based online command adaptation and guidance to arbitrarily shaped objects for robot-assisted medical procedures},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Trust your supervisor: Quadrotor obstacle avoidance using
controlled invariant sets. <em>IROS</em>, 9219–9224. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Supervision of a nominal controller, to enforce safety, is concerned with appropriately modifying the generated control inputs, if needed, in order to keep a control system within a set of safe states. An integral component in supervision is a controlled invariant set contained in the set of safe states. In this paper, we build on recent results on the computation of polytopic controlled invariant sets to present a supervision framework that computes the corrected inputs analytically and, hence, suitable for real-time control. The framework is validated on the task of quadrotor obstacle avoidance by forcing the vehicle to navigate within controlled invariant sets of the obstacle-free space. The results are experimentally demonstrated on a Crazyflie 2.0 quadrotor.},
  archive   = {C_IROS},
  author    = {Luigi Pannocchi and Tzanis Anevlavis and Paulo Tabuada},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636485},
  pages     = {9219-9224},
  title     = {Trust your supervisor: Quadrotor obstacle avoidance using controlled invariant sets},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A scalable distributed collision avoidance scheme for
multi-agent UAV systems. <em>IROS</em>, 9212–9218. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this article we propose a distributed collision avoidance scheme for multi-agent unmanned aerial vehicles (UAVs) based on nonlinear model predictive control (NMPC), where other agents in the system are considered as dynamic obstacles with respect to the ego agent. Our control scheme operates at a low level and commands roll, pitch and thrust signals at a high frequency, each agent broadcasts its predicted trajectory to the other ones, and we propose an obstacle prioritization scheme based on the shared trajectories to allow up-scaling of the system. The NMPC problem is solved using an embedded solver generated by Optimization Engine (OpEn) where PANOC is combined with an augmented Lagrangian method to compute collision-free trajectories. We evaluate the proposed scheme in several challenging laboratory experiments for up to ten aerial agents, in dense aerial swarms.},
  archive   = {C_IROS},
  author    = {Björn Lindqvist and Pantelis Sopasakis and George Nikolakopoulos},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636293},
  pages     = {9212-9218},
  title     = {A scalable distributed collision avoidance scheme for multi-agent UAV systems},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A visual inertial odometry framework for 3D points, lines
and planes. <em>IROS</em>, 9206–9211. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recovering rigid registration between successive camera poses lies at the heart of 3D reconstruction, SLAM and visual odometry. Registration relies on the ability to compute discriminative 2D features in successive camera images for determining feature correspondences, which is very challenging in feature-poor environments, i.e. low-texture and/or low-light environments. In this paper, we aim to address the challenge of recovering rigid registration between successive camera poses in feature-poor environments in a Visual Inertial Odometry (VIO) setting. In addition to inertial sensing, we instrument a small aerial robot with an RGBD camera and propose a framework that unifies the incorporation of 3D geometric entities: points, lines, and planes. The tracked 3D geometric entities provide constraints in an Extended Kalman Filtering framework. We show that by directly exploiting 3D geometric entities, we can achieve improved registration. We demonstrate our approach on different texture-poor environments, with some containing only flat texture-less surfaces providing essentially no 2D features for tracking. In addition, we evaluate how the addition of different 3D geometric entities contributes to improved pose estimation by comparing an estimated pose trajectory to a ground truth pose trajectory obtained from a motion capture system. We consider computationally efficient methods for detecting 3D points, lines and planes, since our goal is to implement our approach on small mobile robots, such as drones.},
  archive   = {C_IROS},
  author    = {Shenbagaraj Kannapiran and Jeroen van Baar and Spring Berman},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636526},
  pages     = {9206-9211},
  title     = {A visual inertial odometry framework for 3D points, lines and planes},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RP-VIO: Robust plane-based visual-inertial odometry for
dynamic environments. <em>IROS</em>, 9198–9205. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern visual-inertial navigation systems (VINS) are faced with a critical challenge in real-world deployment: they need to operate reliably and robustly in highly dynamic environments. Current best solutions merely filter dynamic objects as outliers based on the semantics of the object category. Such an approach does not scale as it requires semantic classifiers to encompass all possibly-moving object classes; this is hard to define, let alone deploy. On the other hand, many realworld environments exhibit strong structural regularities in the form of planes such as walls and ground surfaces, which are also crucially static. We present RP-VIO, a monocular visual-inertial odometry system that leverages the simple geometry of these planes for improved robustness and accuracy in challenging dynamic environments. Since existing datasets have a limited number of dynamic elements, we also present a highly-dynamic, photorealistic synthetic dataset for a more effective evaluation of the capabilities of modern VINS systems. We evaluate our approach on this dataset, and three diverse sequences from standard datasets including two real-world dynamic sequences and show a significant improvement in robustness and accuracy over a state-of-the-art monocular visual-inertial odometry system. We also show in simulation an improvement over a simple dynamic-features masking approach. Our code and dataset are publicly available † .},
  archive   = {C_IROS},
  author    = {Karnik Ram and Chaitanya Kharyal and Sudarshan S. Harithas and K. Madhava Krishna},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636522},
  pages     = {9198-9205},
  title     = {RP-VIO: Robust plane-based visual-inertial odometry for dynamic environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A comparison of modern general-purpose visual SLAM
approaches. <em>IROS</em>, 9190–9197. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Advancing maturity in mobile and legged robotics technologies is changing the landscapes where robots are being deployed and found. This innovation calls for a transformation in simultaneous localization and mapping (SLAM) systems to support this new generation of service and consumer robots. No longer can traditionally robust 2D lidar systems dominate while robots are being deployed in multi-story indoor, outdoor unstructured, and urban domains with increasingly inexpensive stereo and RGB-D cameras. Visual SLAM (VSLAM) systems have been a topic of study for decades and a small number of openly available implementations have stood out: ORB-SLAM3, OpenVSLAM and RTABMap.This paper presents a comparison of these 3 modern, feature rich, and uniquely robust VSLAM techniques that have yet to be benchmarked against each other, using several different datasets spanning multiple domains negotiated by service robots. ORB-SLAM3 and OpenVSLAM each were not compared against at least one of these datasets previously in literature and we provide insight through this lens. This analysis is motivated to find general purpose, feature complete, and multi-domain VSLAM options to support a broad class of robot applications for integration into the new and improved ROS 2 Nav2 System as suitable alternatives to traditional 2D lidar solutions.},
  archive   = {C_IROS},
  author    = {Alexey Merzlyakov and Steve Macenski},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636615},
  pages     = {9190-9197},
  title     = {A comparison of modern general-purpose visual SLAM approaches},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sampson distance: A new approach to improving
visual-inertial odometry’s accuracy. <em>IROS</em>, 9184–9189. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a new scheme based on the Sampson distance (SD) to describe visual feature residuals for visual-inertial odometry (VIO). Unlike the epipolar-constraint-based SD for visual odometry (VO), the proposed SD is formulated based on the perspective projection constraint. We proved in theory that the proposed SD retains the good properties of those earlier SD criteria in the literature of VO and it represents a visual feature residual more accurately than the prevailing transfer distance (TD) in existing VIO methods. We formulate three distance criteria, including TD, reprojection error (RE), and SD, and compared their performances by simulation. The results show that the SD is much more accurate than the TD and it is a very accurate estimate of the gold standard criteria—RE. Based on the SD, we modified VINS-Mono by replacing its TD-based visual residuals with the SD-based residuals and study the SD&#39;s efficacy in pose estimation by experiments with several public datasets. The results reveal that the SD-based VINS-Mono has a substantial improvement over the original VINS-Mono in pose estimation accuracy. This indicates that the SD is a better distance criterion than the TD for representing visual feature residuals. The proposed SD may find its applications to broader areas in computer vision and robotics.},
  archive   = {C_IROS},
  author    = {He Zhang and Cang Ye},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636115},
  pages     = {9184-9189},
  title     = {Sampson distance: A new approach to improving visual-inertial odometry&#39;s accuracy},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accurate visual-inertial SLAM by feature re-identification.
<em>IROS</em>, 9168–9175. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most of the state-of-the-art visual inertial SLAM methods pay less attention to 2D-2D and 3D-2D matching with more reliable features in a long time span, which easily results in continuous estimation drift. In this paper, we propose an efficient drift-free visual-inertial SLAM method by a pose guided feature matching method to re-identify existing features from a spatial-temporal sensitive sub-global map. The re-identified features serve as augmented visual measurements to anchor the current frame and gradually decrease the accumulated error in the long run. When incorporating the measurements into the optimization module, it benefits to build a drift-free global map in the system. Extensive experiments show that our feature re-identification method is both effective and efficient. Specifically, when combining the feature re-identification with the state-of-the-art SLAM method [1], our method achieves 67.3\% and 87.5\% absolute trajectory error reduction with only a small additional computational cost on two public SLAM benchmark DBs: EuRoC and TUM-VI respectively.},
  archive   = {C_IROS},
  author    = {Xiongfeng Peng and Zhihua Liu and Qiang Wang and Yun-Tae Kim and Myungjae Jeon and Hong-Seok Lee},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636186},
  pages     = {9168-9175},
  title     = {Accurate visual-inertial SLAM by feature re-identification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Feature-aided bundle adjustment learning framework for
self-supervised monocular visual odometry. <em>IROS</em>, 9160–9167. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bundle adjustment refines scene geometry and relative camera poses simultaneously via reprojection error, computed by a set of images from different viewpoints, which is the gold standard for visual odometry. However, deep learning methods have not been well exploited within this area of study. This paper introduces a self-supervised learning framework for monocular visual odometry, inside which depth maps, relative camera poses, and dense feature maps (with the same resolution as images) are estimated and used for photometric, geometric, and feature-metric losses in bundle adjustment. In this manner, we consider that the learning of neural networks can be geometrically constrained by multi-view geometry. Furthermore, bundle adjustment is only required during the training time, allowing the networks to benefit from bundle adjustment without any additional computation burden during the inference time. To stabilize the training process, we apply a two-stage strategy that yields promising results. Finally, we carefully select the neural network architectures to ensure efficiency, and experimental results demonstrate the success of our proposed approach in terms of visual odometry accuracy and high speed.},
  archive   = {C_IROS},
  author    = {Weijun Mai and Yoshihiro Watanabe},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636756},
  pages     = {9160-9167},
  title     = {Feature-aided bundle adjustment learning framework for self-supervised monocular visual odometry},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stereo visual inertial odometry for robots with limited
computational resources. <em>IROS</em>, 9154–9159. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current existing stereo visual odometry algorithms are computationally too expensive for robots with restricted resources. Executing these algorithms on such robots leads to a low frame rate and unacceptable decay in accuracy. We modify S-MSCKF, one of the most computationally efficient stereo Visual Inertial Odometry (VIO) algorithm, to improve its speed and accuracy when tracking low numbers of features. Specifically, we implement the Inverse Lucas-Kanade (ILK) algorithm for feature tracking and stereo matching. An outlier detector based on the average sum square difference of the template and matching warp in the ILK ensures higher robustness, e.g., in the presence of brightness changes. We restrict stereo matching to slide the window only in the x-direction to further decrease the computational costs. Moreover, we limit detection of new features to the regions of interest that have too few features. The modified S-MSCKF uses half of the processing time while obtaining competitive accuracy. This allows the algorithm to run in real-time on the extremely limited Raspberry Pi Zero single-board computer.},
  archive   = {C_IROS},
  author    = {Stavrow Bahnam and Sven Pfeiffer and Guido C.H.E. de Croon},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636807},
  pages     = {9154-9159},
  title     = {Stereo visual inertial odometry for robots with limited computational resources},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MicROS.BT: An event-driven behavior tree framework for swarm
robots. <em>IROS</em>, 9146–9153. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose micROS.BT, an event-driven behavior tree (BT) framework aiming at supporting swarm-robot coordination. Compared with other BT frame-works, micROS.BT implements the event-driven way under the multi-thread mode, which can effectively save computing resources. Moreover, in order to ensure swarm-robot coordination, we optimize the implementation of the traditional blackboard and propose the multi-mode blackboard, which supports inner-tree, inter-tree, and inter-robot data sharing. Furthermore, considering the limited modularity of a single tree, micROS.BT realizes a mechanism called hierarchical tree management which involves inter-tree notifying and waiting functionalities, while ensuring that each tree is independent and self-scheduled. The effectiveness of micROS.BT is verified by simulation and real-robot experiments for different system settings, showing that a substantial improvement is achieved in comparison with the traditional BT implementations.},
  archive   = {C_IROS},
  author    = {Yunlong Wu and Jinghua Li and Huadong Dai and Xiaodong Yi and Yanzhen Wang and Xuejun Yang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636460},
  pages     = {9146-9153},
  title     = {MicROS.BT: An event-driven behavior tree framework for swarm robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extension of flocking models to environments with obstacles
and degraded communications. <em>IROS</em>, 9139–9145. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we study existing flocking models and propose extensions to improve their abilities to deal with environments having obstacles impacting the communication quality. Often depicted as robust systems, there is yet a lack of understanding how flocking models compare and how they are impacted by the communication quality when they exchange control data. We extend two standard models to improve their ability to stay connected while evolving in environments with different obstacles distributions. By taking into account the radio propagation, we model the obstacles impact on communications in a simulator that we use to optimize flocking parameters. The simulation results show the efficiency of the proposed models and how they adapt to different environmental constraints.},
  archive   = {C_IROS},
  author    = {Alexandre Bonnefond and Olivier Simonin and Isabelle Guérin-Lassous},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635944},
  pages     = {9139-9145},
  title     = {Extension of flocking models to environments with obstacles and degraded communications},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cooperative object transportation using gibbs random fields.
<em>IROS</em>, 9131–9138. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel methodology that allows a swarm of robots to perform a cooperative transportation task. Our approach consists of modeling the swarm as a Gibbs Random Field (GRF), taking advantage of this framework’s locality properties. By setting appropriate potential functions, robots can dynamically navigate, form groups, and perform co- operative transportation in a completely decentralized fashion. Moreover, these behaviors emerge from the local interactions without the need for explicit communication or coordination. To evaluate our methodology, we perform a series of simulations and proof-of-concept experiments in different scenarios. Our results show that the method is scalable, adaptable, and robust to failures and changes in the environment.},
  archive   = {C_IROS},
  author    = {Paulo Rezeck and Renato M. Assunção and Luiz Chaimowicz},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635928},
  pages     = {9131-9138},
  title     = {Cooperative object transportation using gibbs random fields},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sniffy bug: A fully autonomous swarm of gas-seeking nano
quadcopters in cluttered environments. <em>IROS</em>, 9099–9106. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Nano quadcopters are ideal for gas source localization (GSL) as they are safe, agile and inexpensive. However, their extremely restricted sensors and computational resources make GSL a daunting challenge. We propose a novel bug algorithm named ‘Sniffy Bug&#39;, which allows a fully autonomous swarm of gas-seeking nano quadcopters to localize a gas source in unknown, cluttered, and GPS-denied environments. The computationally efficient, mapless algorithm foresees in the avoidance of obstacles and other swarm members, while pursuing desired waypoints. The waypoints are first set for exploration, and, when a single swarm member has sensed the gas, by a particle swarm optimization-based (PSO) procedure. We evolve all the parameters of the bug (and PSO) algorithm using our novel simulation pipeline, ‘AutoGDM&#39;. It builds on and expands open source tools in order to enable fully automated end-to-end environment generation and gas dispersion modeling, allowing for learning in simulation. Flight tests show that Sniffy Bug with evolved parameters outperforms manually selected parameters in cluttered, real-world environments. Videos: https://bit.ly/37MmtdL},
  archive   = {C_IROS},
  author    = {Bardienus P. Duisterhof and Shushuai Li and Javier Burgués and Vijay Janapa Reddi and Guido C. H. E. de Croon},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636217},
  pages     = {9099-9106},
  title     = {Sniffy bug: A fully autonomous swarm of gas-seeking nano quadcopters in cluttered environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multitask and transfer learning of geometric robot motion.
<em>IROS</em>, 9071–9078. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When a learning solution is needed for different robots, a model is often trained for each robot geometry, even if the robotic task is the same and the robots are structurally similar. In this paper, we address the problem of transfer learning of swept volume predictors for the motion of articulated robots with similar geometric structure. The swept volume is a scalar value corresponding to the space occupied by an entire motion of the robot. Swept volume has many applications, including being an ideal distance measure for sampling based motion planners, but it is expensive to compute. We address this learning problem through a multitask network where a common input is used to learn multiple related tasks. In this work a single network learns the kinematic-geometric information common among robots. In order to identify the properties of our multitask network favorable for transfer, we evaluate transfer properties of several shared layers, number of robots in multitask training, and feature layers. We demonstrate positive transfer results with a training set that is a fraction of the data size used in the multitask and baseline training. All the robots considered are 7-DOF manipulators with links with a variety of lengths and shapes. We also present a study of the weights and activations of the trained networks that show high correlation with the transferability patterns we observed.},
  archive   = {C_IROS},
  author    = {Satomi Sugaya and Mohammad R. Yousefi and Andrew R. Ferdinand and Marco Morales and Lydia Tapia},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636052},
  pages     = {9071-9078},
  title     = {Multitask and transfer learning of geometric robot motion},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time volumetric-semantic exploration and mapping: An
uncertainty-aware approach. <em>IROS</em>, 9064–9070. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work we propose a holistic framework for autonomous aerial inspection tasks, using semantically-aware, yet, computationally efficient planning and mapping algorithms. The system leverages state-of-the-art receding horizon exploration techniques for next-best-view (NBV) planning with geometric and semantic segmentation information provided by state-of-the-art deep convolutional neural networks (DCNNs), with the goal of enriching environment representations. The contributions of this article are threefold, first we propose an efficient sensor observation model, and a reward function that encodes the expected information gains from the observations taken from specific view points. Second, we extend the reward function to incorporate not only geometric but also semantic probabilistic information, provided by a DCNN for semantic segmentation that operates in real-time. The incorporation of semantic information in the environment representation allows biasing exploration towards specific objects, while ignoring task-irrelevant ones during planning. Finally, we employ our approaches in an autonomous drone shipyard inspection task. A set of simulations in realistic scenarios demonstrate the efficacy and efficiency of the proposed framework when compared with the state-of-the-art.},
  archive   = {C_IROS},
  author    = {Rui Pimentel de Figueiredo and Jonas le Fevre Sejersen and Jakob Grimm Hansen and Martim Brandão and Erdal Kayacan},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635986},
  pages     = {9064-9070},
  title     = {Real-time volumetric-semantic exploration and mapping: An uncertainty-aware approach},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Risk conditioned neural motion planning. <em>IROS</em>,
9057–9063. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Risk-bounded motion planning is an important yet difficult problem for safety-critical tasks. While existing mathematical programming methods offer theoretical guarantees in the context of constrained Markov decision processes, they either lack scalability in solving larger problems or produce conservative plans. Recent advances in deep reinforcement learning improve scalability by learning policy networks as function approximators. In this paper, we propose an extension of soft actor critic model to estimate the execution risk of a plan through a risk critic and produce risk-bounded policies efficiently by adding an extra risk term in the loss function of the policy network. We define the execution risk in an accurate form, as opposed to approximating it through a summation of immediate risks at each time step that leads to conservative plans. Our proposed model is conditioned on a continuous spectrum of risk bounds, allowing the user to adjust the risk-averse level of the agent on the fly. Through a set of experiments, we show the advantage of our model in terms of both computational time and plan quality, compared to a state-of-the-art mathematical programming baseline, and validate its performance in more complicated scenarios, including nonlinear dynamics and larger state space.},
  archive   = {C_IROS},
  author    = {Xin Huang and Meng Feng and Ashkan Jasour and Guy Rosman and Brian Williams},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636201},
  pages     = {9057-9063},
  title     = {Risk conditioned neural motion planning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving kinodynamic planners for vehicular navigation with
learned goal-reaching controllers. <em>IROS</em>, 9038–9043. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper aims to improve the path quality and computational efficiency of sampling-based kinodynamic planners for vehicular navigation. It proposes a learning framework for identifying promising controls during the expansion process of sampling-based planners. Given a dynamics model, a reinforcement learning process is trained offline to return a low-cost control that reaches a local goal state (i.e., a waypoint) in the absence of obstacles. By focusing on the system’s dynamics and not knowing the environment, this process is data-efficient and takes place once for a robotic system. In this way, it can be reused in different environments. The planner generates online local goal states for the learned controller in an informed manner to bias towards the goal and consecutively in an exploratory, random manner. For the informed expansion, local goal states are generated either via (a) medial axis information in environments with obstacles, or (b) wavefront information for setups with traversability costs. The learning process and the resulting planning framework are evaluated for a first and second-order differential drive system, as well as a physically simulated Segway robot. The results show that the proposed integration of learning and planning can produce higher quality paths than sampling-based kinodynamic planning with random controls in fewer iterations and computation time.},
  archive   = {C_IROS},
  author    = {Aravind Sivaramakrishnan and Edgar Granados and Seth Karten and Troy McMahon and Kostas E. Bekris},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636657},
  pages     = {9038-9043},
  title     = {Improving kinodynamic planners for vehicular navigation with learned goal-reaching controllers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cognitive navigation for indoor environment using floorplan.
<em>IROS</em>, 9030–9037. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The recent years have seen the increasing importance of cognitive models for improved robot navigation. In this paper, a novel cognitive navigation package, which consists of topometric map representation and a three-level path planner, is proposed. The topometric maps are built from architectural floor plans with additional features within a limited number of selected regions. The inherent discrepancies between floor plans and the robot’s actual sensory readings are handled by the three-level path planner. The unique feature of this approach is that accurate localization is only required at the selected regions. At the other regions the robot will rely on the guiding directions towards the goal rather than on its accurate position on the map for navigation. Experiments show that our approach can endow a robot with capability for semantic interpretation and localization in unseen and dynamic environments.},
  archive   = {C_IROS},
  author    = {Jun Li and Chee Leong Chan and Jian Le Chan and Zhengguo Li and Kong Wah Wan and Wei Yun Yau},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635850},
  pages     = {9030-9037},
  title     = {Cognitive navigation for indoor environment using floorplan},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comprehension of spatial constraints by neural logic
learning from a single RGB-d scan. <em>IROS</em>, 9008–9013. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous industrial assembly relies on the precise measurement of spatial constraints as designed by computer-aided design (CAD) software such as SolidWorks. This paper proposes a framework for an intelligent industrial robot to understand the spatial constraints for model assembly. An extended generative adversary network (GAN) with a 3D long short-term memory (LSTM) network was designed to composite 3D point clouds from a single RGB-D scan. The spatial constraints of the segmented point clouds are identified by a neural-logic network that incorporates general knowledge of spatial constraints in terms of first-order logic. The model was designed to comprehend a complete set of spatial constraints that are consistent with industrial CAD software, including left, right, above, below, front, behind, parallel, perpendicular, concentric, and coincident relations. The accuracy of 3D model composition and spatial constraint identification was evaluated by the RGB-D scans and 3D models in the ABC dataset. The proposed model achieved 57.23\% intersection over union (IoU) in 3D model composition, and over 99\% in comprehending all spatial constraints.},
  archive   = {C_IROS},
  author    = {Fujian Yan and Dali Wang and Hongsheng He},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635939},
  pages     = {9008-9013},
  title     = {Comprehension of spatial constraints by neural logic learning from a single RGB-D scan},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Annotation cost reduction of stream-based active learning by
automated weak labeling using a robot arm. <em>IROS</em>, 9000–9007. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Stream-based active learning (AL) is an efficient training data collection method, and it is used to reduce human annotation cost required in machine learning. However, it is difficult to say that the human cost is low enough because most previous studies have assumed that an oracle is a human with domain knowledge. In this study, we propose a method to replace a part of the oracle’s work in stream-based AL by self-training with weak labeling using a robot arm. A camera attached to a robot arm takes a series of image data related to a streamed object, which should have the same label. We use this information as a weak label to connect a pseudo-label (estimated class label) and a target instance. Our method selects two data from a series of image data; high confidence data for correcting pseudo-labels and low confidence data for improving the performance of the classifier. We paired a pseudo-label provided to high confidence data with a target instance (low confidence data). By using this technique, we mitigate the inefficiency in self-training, that is, difficulty in creating pseudo-labeled training data with a high impact on the target classifier. In the experiments, we employed the proposed method in the classification task of objects on a belt conveyor. We evaluated the performance against human cost on multiple scenarios considering the temporal variation of data. The proposed method achieves the same or better performance as the conventional methods while reducing human cost.},
  archive   = {C_IROS},
  author    = {Kanata Suzuki and Taro Sunagawa and Tomotake Sasaki and Takashi Katoh},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636355},
  pages     = {9000-9007},
  title     = {Annotation cost reduction of stream-based active learning by automated weak labeling using a robot arm},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). StyleLess layer: Improving robustness for real-world
driving. <em>IROS</em>, 8992–8999. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep Neural Networks (DNNs) are a critical component for self-driving vehicles. They achieve impressive performance by reaping information from high amounts of labeled data. Yet, the full complexity of the real world cannot be encapsulated in the training data, no matter how big the dataset, and DNNs can hardly generalize to unseen conditions. Robustness to various image corruptions, caused by changing weather conditions or sensor degradation and aging, is crucial for safety when such vehicles are deployed in the real world. We address this problem through a novel type of layer, dubbed StyleLess, which enables DNNs to learn robust and informative features that can cope with varying external conditions. We propose multiple variations of this layer that can be integrated in most of the architectures and trained jointly with the main task. We validate our contribution on typical autonomous-driving tasks (detection, semantic segmentation), showing that in most cases, this approach improves predictive performance on unseen conditions (fog, rain), while preserving performance on seen conditions and objects.},
  archive   = {C_IROS},
  author    = {Julien Rebut and Andrei Bursuc and Patrick Pérez},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636204},
  pages     = {8992-8999},
  title     = {StyleLess layer: Improving robustness for real-world driving},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A general approach to state refinement. <em>IROS</em>,
8985–8991. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep learning algorithms such as Convolutional Neural Networks (CNNs) are currently used to solve a range of robotics and computer vision problems. These networks typically estimate the desired representation in a single forward pass and must therefore learn to converge from a wide range of initial conditions to a precise result. This is challenging, and has led to increased interest in the development of separate refinement modules which learn to improve a given initial estimate, thus reducing the required search space. Such modules are usually developed ad-hoc for each given application, often requiring significant engineering investment. In this work we propose a generic innovation-based CNN. Our CNN is implemented along with a stochastic gradient descent (SGD) algorithm to iteratively refine a given initial estimate. The proposed approach provides a general framework for the development of refinement modules applicable to a wide range of robotics problems. We apply this framework to object pose estimation and depth estimation and demonstrate significant improvement over the initial estimates, in the range of 4.2 - 8.1\%, for both applications.},
  archive   = {C_IROS},
  author    = {Gerard Kennedy and Jin Gao and Zheyu Zhuang and Xin Yu and Robert Mahony},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636400},
  pages     = {8985-8991},
  title     = {A general approach to state refinement},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DiGNet: Learning scalable self-driving policies for generic
traffic scenarios with graph neural networks. <em>IROS</em>, 8979–8984.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traditional decision and planning frameworks for self-driving vehicles (SDVs) scale poorly in new scenarios, thus they require tedious hand-tuning of rules and parameters to maintain acceptable performance in all foreseeable cases. Recently, self-driving methods based on deep learning have shown promising results with better generalization capability but less hand engineering effort. However, most of the previous learning-based methods are trained and evaluated in limited driving scenarios with scattered tasks, such as lane-following, autonomous braking, and conditional driving. In this paper, we propose a graph-based deep network to achieve scalable self-driving that can handle massive traffic scenarios. Specifically, more than 7,000 km of evaluation is conducted in a high-fidelity driving simulator, in which our method can obey the traffic rules and safely navigate the vehicle in a large variety of urban, rural, and highway environments, including unprotected left turns, narrow roads, roundabouts, and pedestrian-rich intersections. Demonstration videos are available at https: //caipeide.github.io/dignet/.},
  archive   = {C_IROS},
  author    = {Peide Cai and Hengli Wang and Yuxiang Sun and Ming Liu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636376},
  pages     = {8979-8984},
  title     = {DiGNet: Learning scalable self-driving policies for generic traffic scenarios with graph neural networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Identifying valid robot configurations via a deep learning
approach. <em>IROS</em>, 8973–8978. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many state-of-art robotics applications require fast and efficient motion planning algorithms. Existing motion planning methods become less effective as the dimensionality of the robot and its workspace increases, especially the computational cost of collision detection routines. In this work, we present a framework to address the cost of expensive primitive operations in sampling-based motion planning. This framework determines the validity of a sample robot configuration through a novel combination of a Contractive AutoEncoder (CAE), which captures an occupancy grids representation of the robot&#39;s workspace, and a Multilayer Perceptron (MLP), which efficiently predicts the collision state of the robot using the output from the CAE. We evaluate our framework on multiple planning problems with a variety of robots in 2D and 3D workspaces. The results show that (1) the framework is computationally efficient in all investigated problems, and (2) the framework generalizes well to new workspaces.},
  archive   = {C_IROS},
  author    = {Tuan Tran and Chinwe Ekenna},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636742},
  pages     = {8973-8978},
  title     = {Identifying valid robot configurations via a deep learning approach},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Transformer-based deep imitation learning for dual-arm robot
manipulation. <em>IROS</em>, 8965–8972. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep imitation learning is promising for solving dexterous manipulation tasks because it does not require an environment model and pre-programmed robot behavior. However, its application to dual-arm manipulation tasks remains challenging. In a dual-arm manipulation setup, the increased number of state dimensions caused by the additional robot manipulators causes distractions and results in poor performance of the neural networks. We address this issue using a self-attention mechanism that computes dependencies between elements in a sequential input and focuses on important elements. A Transformer, a variant of self-attention architecture, is applied to deep imitation learning to solve dual-arm manipulation tasks in the real world. The proposed method has been tested on dual-arm manipulation tasks using a real robot. The experimental results demonstrated that the Transformer-based deep imitation learning architecture can attend to the important features among the sensory inputs, therefore reducing distractions and improving manipulation performance when compared with the baseline architecture without the self-attention mechanisms.},
  archive   = {C_IROS},
  author    = {Heecheol Kim and Yoshiyuki Ohmura and Yasuo Kuniyoshi},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636301},
  pages     = {8965-8972},
  title     = {Transformer-based deep imitation learning for dual-arm robot manipulation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised temporal segmentation using models that
discriminate between demonstrations and unintentional actions.
<em>IROS</em>, 8951–8956. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Segmentation of a compound task with multiple subtasks is crucial for imitation learning. Conventional unsupervised segmentation methods focused on only reproducibility of demonstrations and did not use the property that goal-directed actions rarely occur without intention. In this paper, we propose a novel method to segment demonstrations into goal-directed actions by self-supervised learning. We use the discriminator between demonstrations and self-generated unintentional actions performed by the same body in behavioral cloning paradigm because goal-directed actions rarely occur without intention, and thus can be separated from unintentional actions. And we consider the states that cannot be reached by unintentional actions as subtask changepoints. We evaluated our method on manipulation tasks with multiple subtasks. The results indicate that our method can detect subtask changepoints more accurately than an existing unsupervised segmentation method.},
  archive   = {C_IROS},
  author    = {Takayuki Komatsu and Yoshiyuki Ohmura and Yasuo Kuniyoshi},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636688},
  pages     = {8951-8956},
  title     = {Unsupervised temporal segmentation using models that discriminate between demonstrations and unintentional actions},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-robot coverage and exploration using spatial graph
neural networks. <em>IROS</em>, 8944–8950. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The multi-robot coverage problem is an essential building block for systems that perform tasks like inspection, exploration, or search and rescue. We discretize the coverage problem to induce a spatial graph of locations and represent robots as nodes in the graph. Then, we train a Graph Neural Network controller that leverages the spatial equivariance of the task to imitate an expert open-loop routing solution. This approach generalizes well to much larger maps and larger teams that are intractable for the expert. In particular, the model generalizes effectively to a simulation of ten quadrotors and dozens of buildings in an urban setting. We also demonstrate the GNN controller can surpass planning-based approaches in an exploration task.},
  archive   = {C_IROS},
  author    = {Ekaterina Tolstaya and James Paulos and Vijay Kumar and Alejandro Ribeiro},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636675},
  pages     = {8944-8950},
  title     = {Multi-robot coverage and exploration using spatial graph neural networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Decentralized, unlabeled multi-agent navigation in
obstacle-rich environments using graph neural networks. <em>IROS</em>,
8936–8943. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a decentralized, learning-based solution to the challenging problem of unlabeled multi-agent navigation among obstacles, where robots need to simultaneously tackle the problems of goal assignment, local collision avoidance, and navigation. Our method has each robot infer their desired action by communicating with each other as well as a set of position-fixed routers. The inference is carried out on a graph neural network (GNN) with both robot and router nodes. We train our GNN using imitation learning on a small group of robots, where we modify the centralized version of the concurrent goal assignment and planning algorithm (CAPT) as our expert. By sharing weights among all robots and routers, our model can scale to unseen environments with any number of possibly kinodynamic agents during test time. We have achieved a success rate of 91.2\% and 85.6\% for point and car-like robots, respectively. Source code will be publicly available upon the publication of the work.},
  archive   = {C_IROS},
  author    = {Xuebo Ji and He Li and Zherong Pan and Xifeng Gao and Changhe Tu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636088},
  pages     = {8936-8943},
  title     = {Decentralized, unlabeled multi-agent navigation in obstacle-rich environments using graph neural networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Risk averse bayesian reward learning for autonomous
navigation from human demonstration. <em>IROS</em>, 8928–8935. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traditional imitation learning provides a set of methods and algorithms to learn a reward function or policy from expert demonstrations. Learning from demonstration has been shown to be advantageous for navigation tasks as it allows for machine learning non-experts to quickly provide information needed to learn complex traversal behaviors. However, a minimal set of demonstrations is unlikely to capture all relevant information needed to achieve the desired behavior in every possible future operational environment. Due to distributional shift among environments, a robot may encounter features that were rarely or never observed during training for which the appropriate reward value is uncertain, leading to undesired outcomes. This paper proposes a Bayesian technique which quantifies uncertainty over the weights of a linear reward function given a dataset of minimal human demonstrations to operate safely in dynamic environments. This uncertainty is quantified and incorporated into a risk averse set of weights used to generate cost maps for planning. Experiments in a 3-D environment with a simulated robot show that our proposed algorithm enables a robot to avoid dangerous terrain completely in two out of three test scenarios and accumulates a lower amount of risk than related approaches in all scenarios without requiring any additional demonstrations.},
  archive   = {C_IROS},
  author    = {Christian Ellis and Maggie Wigness and John Rogers and Craig Lennon and Lance Fiondella},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635835},
  pages     = {8928-8935},
  title     = {Risk averse bayesian reward learning for autonomous navigation from human demonstration},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Imitation learning with approximated behavior cloning loss.
<em>IROS</em>, 8921–8927. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent Imitation Learning (IL) techniques focus on adversarial imitation learning algorithms to learn from a fixed set of expert demonstrations. While these approaches are theoretically sound, they suffer from a number of problems such as poor sample efficiency, poor stability, and a host of issues that Generative Adversarial Networks (GANs) suffer from. In this paper we introduce a generalization of Behavior Cloning (BC) that is applicable in any IL setting. Our algorithm first approximates behavior cloning loss using a neural network and then uses that loss network to generate a loss signal which is minimized using standard supervised learning. We call the resulting algorithm family Approximated Behavior Cloning (ABC), introduce variants for each IL setting, and demonstrate an order of magnitude improvement in sample efficiency and increased stability in standard imitation learning environments.},
  archive   = {C_IROS},
  author    = {Corey A. Lowman and Joshua S. McClellan and Galen E. Mullins},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636797},
  pages     = {8921-8927},
  title     = {Imitation learning with approximated behavior cloning loss},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Generalization through hand-eye coordination: An action
space for learning spatially-invariant visuomotor control.
<em>IROS</em>, 8913–8920. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Imitation Learning (IL) is an effective framework to learn visuomotor skills from offline demonstration data. However, IL methods often fail to generalize to new scene configurations not covered by training data. On the other hand, humans can manipulate objects in varying conditions. Key to such capability is hand-eye coordination, a cognitive ability that enables humans to adaptively direct their movements at task-relevant objects and be invariant to the objects’ absolute spatial location. In this work, we present a learnable action space, Hand-eye Action Networks (HAN) that learns coordinated hand-eye movements from human teleoperated demonstrations. Through a set of challenging multi-stage manipulation tasks, we show that a visuomotor policy equipped with HAN is able to inherit the key spatial invariance property of handeye coordination and achieve generalization to new scene configurations. Additional materials available at https://sites.google.com/stanford.edu/han},
  archive   = {C_IROS},
  author    = {Chen Wang and Rui Wang and Ajay Mandlekar and Li Fei-Fei and Silvio Savarese and Danfei Xu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636023},
  pages     = {8913-8920},
  title     = {Generalization through hand-eye coordination: An action space for learning spatially-invariant visuomotor control},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Single-shot is enough: Panoramic infrastructure based
calibration of multiple cameras and 3D LiDARs. <em>IROS</em>, 8890–8897.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The integration of multiple cameras and 3D Li-DARs has become basic configuration of augmented reality devices, robotics, and autonomous vehicles. The calibration of multi-modal sensors is crucial for a system to properly function, but it remains tedious and impractical for mass production. Moreover, most devices require re-calibration after usage for certain period of time. In this paper, we propose a single-shot solution for calibrating extrinsic transformations among multiple cameras and 3D LiDARs. We establish a panoramic infrastructure, in which a camera or LiDAR can be robustly localized using data from single frame. Experiments are conducted on three devices with different camera-LiDAR configurations, showing that our approach achieved comparable calibration accuracy with the state-of-the-art approaches but with much greater efficiency.},
  archive   = {C_IROS},
  author    = {Chuan Fang and Shuai Ding and Zilong Dong and Honghua Li and Siyu Zhu and Ping Tan},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636767},
  pages     = {8890-8897},
  title     = {Single-shot is enough: Panoramic infrastructure based calibration of multiple cameras and 3D LiDARs},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Odometry model calibration for self-driving vehicles with
noise correction. <em>IROS</em>, 8860–8865. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the era of self-driving vehicles, state estimation has 3 main contradictory requirements, such as accuracy, robustness, and cost-effectiveness. To satisfy all of them, the integration of the wheel encoder measurements is a proper choice besides the generally applied GNSS, inertial and visual-odometry methods. The wheel odometry is a robust and cost-effective method, but the accuracy of the estimation is limited by the proper knowledge of the vehicle parameter values. However, the calibration of the nonlinear odometry model in the presence of noise remains an open problem in the context of autonomous vehicles yet. This paper presents an algorithm that takes advantage of the assumption that more measurements are available in a self-driving vehicle for accurate parameter estimation. With the proposed architecture, the measurements with distortion effects are detected, and also the noise is corrected to reach unbiased model calibration. The performance of the developed algorithm and the accuracy of the parameter estimation are demonstrated with detailed validation and test with a real vehicle.},
  archive   = {C_IROS},
  author    = {Máté Fazekas and Péter Gáspár and Balázs Németh},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635980},
  pages     = {8860-8865},
  title     = {Odometry model calibration for self-driving vehicles with noise correction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Kalibrot: A simple-to-use matlab package for robot kinematic
calibration. <em>IROS</em>, 8852–8859. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot modelling is an essential part to properly understand how a robotic system moves and how to control it. The kinematic model of a robot is usually obtained by using Denavit-Hartenberg convention, which relies on a set of parameters to describe the end-effector pose in a Cartesian space. These parameters are assigned based on geometrical considerations of the robotic structure, however, the assigned values may be inaccurate. The purpose of robot kinematic calibration is therefore to find optimal parameters which improve the accuracy of the robot model. In this work we present Kalibrot, an open source Matlab package for robot kinematic calibration. Kalibrot has been designed to simplify robot calibration and easily assess the calibration results. Beside computing the optimal parameters, Kalibrot provides a visualization layer showing the values of the calibrated parameters, what parameters can be identified, and the calibrated robotic structure. The capabilities of the package are here shown through simulated and real world experiments.},
  archive   = {C_IROS},
  author    = {Francesco Cursi and Weibang Bai and Petar Kormushev},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635859},
  pages     = {8852-8859},
  title     = {Kalibrot: A simple-to-use matlab package for robot kinematic calibration},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SemAlign: Annotation-free camera-LiDAR calibration with
semantic alignment loss. <em>IROS</em>, 8845–8851. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-sensor solution has been widely adopted in real-world robotics systems (e.g., self-driving vehicles) due to its better robustness. However, its performance is highly dependent on the accurate calibration between different sensors, which is very time-consuming (i.e., hours of human efforts) to acquire. Recent learning-based solutions partially address this yet still require costly ground-truth annotations as supervision. In this paper, we introduce a novel self-supervised semantic alignment loss to quantitatively measure the quality of a given calibration. It is well correlated with conventional evaluation metrics while it does not require ground-truth calibration annotations as the reference. Based on this loss, we further propose an annotation-free optimization-based calibration algorithm (SemAlign) that first estimates a coarse calibration with loss-guided initialization and then refines it with gradient-based optimization. SemAlign reduces the calibration time from hours of human efforts to only seconds of GPU computation. It not only achieves comparable performance with existing supervised learning frameworks but also demonstrates a much better generalization capability when transferred to a different dataset.},
  archive   = {C_IROS},
  author    = {Zhijian Liu and Haotian Tang and Sibo Zhu and Song Han},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635964},
  pages     = {8845-8851},
  title     = {SemAlign: Annotation-free camera-LiDAR calibration with semantic alignment loss},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). QoE-driven delay-adaptive control scheme switching for
time-delayed bilateral teleoperation with haptic data reduction.
<em>IROS</em>, 8838–8844. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Teleoperation systems with haptic feedback allow a human user to remotely interact with a dangerous or inac-cessible environment, perform various tasks, and perceive the haptic feedback. To ensure system stability while maintaining the best possible quality of experience (QoE), different teleoperation control schemes and haptic communication strategies need to be selected to adapt to varying network conditions and teleoperation tasks. In this paper, we propose a QoE-driven control scheme switching approach, which adaptively selects the control scheme that provides the best possible QoE for varying communication delay. A transition period is designed to moderate the artifacts during the switching phase. Haptic data reduction approaches are developed for the switching strategy to match the characteristics of each control scheme. Our experiments verify the feasibility of the proposed scheme. Subjective tests confirm that the proposed adaptive switching scheme is able to achieve a superior user QoE in contrast to a fixed control scheme in the presence of varying communication delay up to 200 ms.},
  archive   = {C_IROS},
  author    = {Xiao Xu and Siyuan Zhang and Qian Liu and Eckehard Steinbach},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636716},
  pages     = {8838-8844},
  title     = {QoE-driven delay-adaptive control scheme switching for time-delayed bilateral teleoperation with haptic data reduction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep neural skill assessment and transfer: Application to
robotic surgery training. <em>IROS</em>, 8822–8829. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Due to the high sensitivity and complexity of robotic surgery tasks, acquiring appropriate skill levels by trainee surgeons through an effective training process is very important and affects the patient’s safety and the quality of surgical outcomes. With the advanced deep learning technology and the recent availability of surgical procedures data, intelligent methods can be deployed to assess and transfer the skills of an experienced surgeon (mentor) to a novice surgeon (trainee). In this paper, we introduce a novel deep-learning-based skill transfer scheme consisting of a deep convolutional model, SkillNet, and a skill transfer algorithm for robotic surgery training. The proposed SkillNet extracts skill-related features of the mentor from different layers of the network. Then, trainee’s maneuver is enhanced by the proposed skill transfer algorithm while minimizing deviations from the trainee’s original intended trajectory. For validation, the JIGSAWS dataset and also our own experimental data were used to prove the generalizability of SkillNet in capturing skill-related features. The capability of the skill transfer algorithm in enhancing trainee trajectories in terms of predictability, hand tremor reduction, and noise cancellation were investigated separately. The obtained results indicate that this approach can be used as a high-performance filter that makes minor corrections to the input trajectory and improves the skill level of the trainee’s trajectory in practice.},
  archive   = {C_IROS},
  author    = {Abed Soleymani and Xingyu Li and Mahdi Tavakoli},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636627},
  pages     = {8822-8829},
  title     = {Deep neural skill assessment and transfer: Application to robotic surgery training},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Surface sliding behavior analysis of space probes in
simulated extraterrestrial environments. <em>IROS</em>, 8774–8781. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study analyzes the surface sliding behavior of space probes in simulated extraterrestrial environments. When a space probe lands on an extraterrestrial body, its landing gear (footpad, landing legs) contacts and slides along the ground surface. The influence of various parameters (i.e., footpad size, velocity, ground condition, atmospheric pressure, and gravity) on the friction behavior of footpads was experimentally evaluated herein. First, we developed an experimental system that can perform footpad sliding tests repeatedly. Subsequently, the system was used to perform tests under various conditions: 1) on ground under normal atmospheric conditions, 2) in a vacuum, and 3) in reduced gravity. The tests performed in a vacuum and in reduced gravity indicated that the friction behavior of the footpad is largely unaffected by atmospheric pressure and gravity. The findings obtained herein offer useful design and control guidelines for space probes landing on extraterrestrial bodies.},
  archive   = {C_IROS},
  author    = {Masataku Sutoh and Yuta Sakakieda and Masatsugu Otsuki and Taizo Kobayashi},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635958},
  pages     = {8774-8781},
  title     = {Surface sliding behavior analysis of space probes in simulated extraterrestrial environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online information-aware motion planning with inertial
parameter learning for robotic free-flyers. <em>IROS</em>, 8766–8773.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Space free-flyers like the Astrobee robots currently operating aboard the International Space Station must operate with inherent system uncertainties. Parametric uncertainties like mass and moment of inertia are especially important to quantify in these safety-critical space systems and can change in scenarios such as on-orbit cargo movement, where unknown grappled payloads significantly change the system dynamics. Cautiously learning these uncertainties en route can potentially avoid time- and fuel-consuming pure system identification maneuvers. Recognizing this, this work proposes RATTLE, an online information-aware motion planning algorithm that explicitly weights parametric model-learning coupled with real-time replanning capability that can take advantage of improved system models. The method consists of a two-tiered (global and local) planner, a low-level model predictive controller, and an online parameter estimator that produces estimates of the robot’s inertial properties for more informed control and replanning on-the-fly; all levels of the planning and control feature online update-able models. Simulation results of RAT-TLE for the Astrobee free-flyer grappling an uncertain payload are presented alongside results of a hardware demonstration showcasing the ability to explicitly encourage model parametric learning while achieving otherwise useful motion.},
  archive   = {C_IROS},
  author    = {Monica Ekal and Keenan Albee and Brian Coltin and Rodrigo Ventura and Richard Linares and David W. Miller},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636325},
  pages     = {8766-8773},
  title     = {Online information-aware motion planning with inertial parameter learning for robotic free-flyers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-modal loop closing in unstructured planetary
environments with visually enriched submaps. <em>IROS</em>, 8758–8765.
(<a href="https://doi.org/10.1109/IROS51168.2021.9635915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Future planetary missions will rely on rovers that can autonomously explore and navigate in unstructured environments. An essential element is the ability to recognize places that were already visited or mapped. In this work, we leverage the ability of stereo cameras to provide both visual and depth information, guiding the search and validation of loop closures from a multi-modal perspective. We propose to augment submaps that are created by aggregating stereo point clouds, with visual keyframes. Point clouds matches are found by comparing CSHOT descriptors and validated by clustering, while visual matches are established by comparing keyframes using Bag-of-Words (BoW) and ORB descriptors. The relative transformations resulting from both keyframe and point cloud matches are then fused to provide pose constraints between submaps in our graph-based SLAM framework. Using the LRU rover, we performed several tests in both an indoor laboratory environment as well as a challenging planetary analog environment on Mount Etna, Italy, consisting of areas where either keyframes or point clouds alone failed to provide adequate matches demonstrating the benefit of the proposed multi-modal approach.},
  archive   = {C_IROS},
  author    = {Riccardo Giubilato and Mallikarjuna Vayugundla and Wolfgang Stürzl and Martin J. Schuster and Armin Wedler and Rudolph Triebel},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635915},
  pages     = {8758-8765},
  title     = {Multi-modal loop closing in unstructured planetary environments with visually enriched submaps},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stereo perception in the dark using uncalibrated line laser.
<em>IROS</em>, 8745–8750. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Perception using stereo requires external light. In the absence of natural light, active, structured light provides light where it is needed. In this work, we demonstrate how a free moving line striping laser can be used to perceive and model terrains. In this formulation, we do not need to know the position of the laser with respect to the stereo pair which precludes the need for calibrating the laser position. This also saves an actuator as the laser can be passively articulated using a spring. In this paper, we present the algorithms to efficiently extract the laser pixels from the stereo image pair. We also present techniques to use the structure of the light and overcome degenerate cases to build cleaner maps. This perception method benefits micro-rovers specifically those trying to operate in the extreme lighting conditions.},
  archive   = {C_IROS},
  author    = {Srinivasan Vijayarangan and David Wettergreen},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636822},
  pages     = {8745-8750},
  title     = {Stereo perception in the dark using uncalibrated line laser},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards robust monocular visual odometry for flying robots
on planetary missions. <em>IROS</em>, 8737–8744. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the future, extraterrestrial expeditions will not only be conducted by rovers but also by flying robots. The technical demonstration drone Ingenuity, that just landed on Mars, will mark the beginning of a new era of exploration unhindered by terrain traversability. Robust self-localization is crucial for that. Cameras that are lightweight, cheap and information-rich sensors are already used to estimate the ego-motion of vehicles. However, methods proven to work in man-made environments cannot simply be deployed on other planets. The highly repetitive textures present in the wastelands of Mars pose a huge challenge to descriptor matching based approaches.In this paper, we present an advanced robust monocular odometry algorithm that uses efficient optical flow tracking to obtain feature correspondences between images and a refined keyframe selection criterion. In contrast to most other approaches, our framework can also handle rotation-only motions that are particularly challenging for monocular odometry systems. Furthermore, we present a novel approach to estimate the current risk of scale drift based on a principal component analysis of the relative translation information matrix. This way we obtain an implicit measure of uncertainty. We evaluate the validity of our approach on all sequences of a challenging real-world dataset captured in a Mars-like environment and show that it outperforms state-of-the-art approaches. The source code is publicly available at: https://github.com/DLR-RM/granit.},
  archive   = {C_IROS},
  author    = {M. Wudenka and M. G. Müller and N. Demmel and A. Wedler and R. Triebel and D. Cremers and W. Stürzl},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636844},
  pages     = {8737-8744},
  title     = {Towards robust monocular visual odometry for flying robots on planetary missions},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Super odometry: IMU-centric LiDAR-visual-inertial estimator
for challenging environments. <em>IROS</em>, 8729–8736. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose Super Odometry, a high-precision multi-modal sensor fusion framework, providing a simple but effective way to fuse multiple sensors such as LiDAR, camera, and IMU sensors and achieve robust state estimation in perceptually-degraded environments. Different from traditional sensor-fusion methods, Super Odometry employs an IMU-centric data processing pipeline, which combines the advantages of loosely coupled methods with tightly coupled methods and recovers motion in a coarse-to-fine manner. The proposed framework is composed of three parts: IMU odometry, Visual-inertial odometry, and LiDAR-inertial odometry. The Visual-inertial odometry and LiDAR-inertial odometry provide the pose prior to constrain the IMU bias and receive the motion prediction from IMU odometry. To ensure high performance in real-time, we apply a dynamic octree that only consumes 10\% of the running time compared with a static KD-tree. The proposed system was deployed on drones and ground robots, as part of Team Explorer’s effort to the DARPA Subterranean Challenge where the team won 1 st and 2 nd place in the Tunnel and Urban Circuits 1 , respectively.},
  archive   = {C_IROS},
  author    = {Shibo Zhao and Hengrui Zhang and Peng Wang and Lucas Nogueira and Sebastian Scherer},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635862},
  pages     = {8729-8736},
  title     = {Super odometry: IMU-centric LiDAR-visual-inertial estimator for challenging environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Coxgraph: Multi-robot collaborative, globally consistent,
online dense reconstruction system. <em>IROS</em>, 8722–8728. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-time dense reconstruction has been extensively studied for its wide applications in computer vision and robotics, meanwhile much effort has been made for the multi-robot system which plays an irreplaceable role in complicated but time-critical scenarios, e.g., search and rescue tasks. In this paper, we propose an efficient system named Coxgraph for multi-robot collaborative dense reconstruction in real-time. In our system, each client performs volumetric mapping in a producer-consumer manner. To facilitate transmission, we propose a compact 3D representation which transforms the SDF submap to mesh packs. During the recovery of submaps from mesh packs, the system can perform loop closure outlier rejection based on geometry consistency, trajectory collision and fitness check. Then we develop a robust map fusion method through joint optimization of trajectories and submaps. Extensive experiments demonstrate that our system can produce a globally consistent dense map in real-time with less transmission load, which is available as open-source software 1 .},
  archive   = {C_IROS},
  author    = {Xiangyu Liu and Weicai Ye and Chaoran Tian and Zhaopeng Cui and Hujun Bao and Guofeng Zhang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636645},
  pages     = {8722-8728},
  title     = {Coxgraph: Multi-robot collaborative, globally consistent, online dense reconstruction system},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed visual-inertial cooperative localization.
<em>IROS</em>, 8714–8721. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we present a consistent and distributed state estimator for multi-robot cooperative localization (CL) which efficiently fuses environmental features and loop-closure constraints across time and robots. In particular, we leverage covariance intersection (CI) to allow each robot to only estimate its own state and autocovariance and compensate for the unknown correlations between robots. Two novel multi-robot methods for utilizing common environmental SLAM features are introduced and evaluated in terms of accuracy and efficiency. Moreover, we adapt CI to enable drift-free estimation through the use of loop-closure measurement constraints to other robots’ historical poses without a significant increase in computational cost. The proposed distributed CL estimator is validated against its non-realtime centralized counterpart extensively in both simulations and real-world experiments.},
  archive   = {C_IROS},
  author    = {Pengxiang Zhu and Patrick Geneva and Wei Ren and Guoquan Huang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636031},
  pages     = {8714-8721},
  title     = {Distributed visual-inertial cooperative localization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ORBBuf: A robust buffering method for remote visual SLAM.
<em>IROS</em>, 8706–8713. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The data loss caused by unreliable network seriously impacts the results of remote visual SLAM systems. From our experiment, a loss of less than 1 second of data can cause a visual SLAM algorithm to lose tracking. We present a novel buffering method, ORBBuf, to reduce the impact of data loss on remote visual SLAM systems. We model the buffering problem as an optimization problem by introducing a similarity metric between frames. To solve the buffering problem, we present an efficient greedy algorithm to discard the frames that have the least impact on the quality of SLAM results. We implement our ORBBuf method on ROS, a widely used middleware framework. Through an extensive evaluation on real-world scenarios and tens of gigabytes of datasets, we demonstrate that our ORBBuf method can be applied to different state-estimation algorithms (DSO and VINS-Fusion), different sensor data (both monocular images and stereo images), different scenes (both indoor and outdoor), and different network environments (both WiFi networks and 4G networks). Our experimental results indicate that the network losses indeed affect the SLAM results, and our ORBBuf method can reduce the RMSE up to 50 times comparing with the Drop-Oldest and Random buffering methods.},
  archive   = {C_IROS},
  author    = {Yu-Ping Wang and Zi-Xin Zou and Cong Wang and Yue-Jiang Dong and Lei Qiao and Dinesh Manocha},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635950},
  pages     = {8706-8713},
  title     = {ORBBuf: A robust buffering method for remote visual SLAM},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A collaborative visual SLAM framework for service robots.
<em>IROS</em>, 8679–8685. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a collaborative visual simultaneous localization and mapping (SLAM) framework for service robots. With an edge server maintaining a map database and performing global optimization, each robot can register to an existing map, update the map, or build new maps, all with a unified interface and low computation and memory cost. We design an elegant communication pipeline to enable real-time information sharing between robots. With a novel landmark organization and retrieval method on the server, each robot can acquire landmarks predicted to be in its view, to augment its local map. The framework is general enough to support both RGB-D and monocular cameras, as well as robots with multiple cameras, taking the rigid constraints between cameras into consideration. The proposed framework has been fully implemented and verified with public datasets and live experiments.},
  archive   = {C_IROS},
  author    = {Ming Ouyang and Xuesong Shi and Yujie Wang and Yuxin Tian and Yingzhe Shen and Dawei Wang and Peng Wang and Zhiqiang Cao},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636798},
  pages     = {8679-8685},
  title     = {A collaborative visual SLAM framework for service robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). MR-iSAM2: Incremental smoothing and mapping with multi-root
bayes tree for multi-robot SLAM. <em>IROS</em>, 8671–8678. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present multi-robot iSAM2 (MR-iSAM2), an efficient incremental smoothing and mapping (iSAM) algorithm to solve multi-robot simultaneous localization and mapping (SLAM) inference problems. MR-iSAM2 is based on a novel data structure multi-root Bayes tree (MRBT), which packs multiple Bayes trees with the same undirected clique structure. In multi-robot scenarios, the MRBT enables new measurements from different robots to be updated in different root branches, while all updates are performed around the single root of the Bayes tree in the original iSAM2 algorithm. As a result, the MRBT better reveals the underlying sparsity and information flow in multi-robot SLAM inference problems than the Bayes tree. Based on this insight, we further develop MR-iSAM2 to incrementally update and maintain the sparsity structure of the MRBT and enable efficient information propagation among the roots for inter-robot inference. We analyze the properties of the MR-iSAM2 algorithm, and show with both synthetic and real world datasets that it significantly outperforms iSAM2 in efficiency when solving multi-robot SLAM problems.},
  archive   = {C_IROS},
  author    = {Yetong Zhang and Ming Hsiao and Jing Dong and Jakob Engel and Frank Dellaert},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636687},
  pages     = {8671-8678},
  title     = {MR-iSAM2: Incremental smoothing and mapping with multi-root bayes tree for multi-robot SLAM},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Thrust direction control of an underactuated oscillating
swimming robot. <em>IROS</em>, 8665–8670. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Modboat is an autonomous surface robot that turns the oscillation of a single motor into a controlled paddling motion through passive flippers. Inertial control methods developed in prior work can successfully drive the Modboat along trajectories and enable docking to neighboring modules, but have a non-constant cycle time and cannot react to dynamic environments. In this work we present a thrust direction control method for the Modboat that significantly improves the time-response of the system and increases the accuracy with which it can be controlled. We experimentally demonstrate that this method can be used to perform more compact maneuvers than prior methods or comparable robots can. We also present an extension to the controller that solves the reaction wheel problem of unbounded actuator velocity, and show that it further improves performance.},
  archive   = {C_IROS},
  author    = {Gedaliah Knizhnik and Mark Yim},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636778},
  pages     = {8665-8670},
  title     = {Thrust direction control of an underactuated oscillating swimming robot},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stochastic guidance of buoyancy controlled vehicles under
ice shelves using ocean currents. <em>IROS</em>, 8657–8664. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel technique for guidance of buoyancy-controlled vehicles in uncertain under-ice ocean flows. In-situ melt rate measurements collected at the grounding zone of Antarctic ice shelves, where the ice shelf meets the underlying bedrock, are essential to constrain models of future sea level rise. Buoyancy-controlled vehicles, which control their vertical position in the water column through internal actuation but have no means of horizontal propulsion, offer an affordable and reliable platform for such in-situ data collection. However, reaching the grounding zone requires vehicles to traverse tens of kilometers under the ice shelf, with approximate position knowledge and no means of communication, in highly variable and uncertain ocean currents. To address this challenge, we propose a partially observable MDP approach that exploits model-based knowledge of the under-ice currents and, critically, of their uncertainty, to synthesize effective guidance policies. The approach uses approximate dynamic programming to model uncertainty in the currents, and QMDP to address localization uncertainty. Numerical experiments show that the policy can deliver up to 88.8\% of underwater vehicles to the grounding zone – a 33\% improvement compared to state-of-the-art guidance techniques, and a 262\% improvement over uncontrolled drifters. Collectively, these results show that model-based under-ice guidance is a highly promising technique for exploration of under-ice cavities, and has the potential to enable cost-effective and scalable access to these challenging and rarely observed environments.},
  archive   = {C_IROS},
  author    = {Federico Rossi and Andrew Branch and Michael P. Schodlok and Timothy Stanton and Ian G. Fenty and Joshua Vander Hook and Evan B. Clark},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635987},
  pages     = {8657-8664},
  title     = {Stochastic guidance of buoyancy controlled vehicles under ice shelves using ocean currents},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Embedded stochastic field exploration with micro diving
agents using bayesian optimization-guided tree-search and GMRFs.
<em>IROS</em>, 8649–8656. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Exploration and monitoring of hazardous fields in marine environments is one of the most promising tasks to be performed by fleets of low-cost micro autonomous underwater vehicles (μAUVs). In contrast to vehicles in other domains, underwater robots are forced to perform all computations onboard as no powerful communication links are available underwater. This puts the focus on computationally efficient field exploration algorithms. We propose CBTS-GMRF – an extremely light-weight tree-search exploration framework suitable for embedded computing. With our framework we build on recent work in POMDP-exploration and field belief representations based on efficient Gaussian Markov random fields (GMRF). We propose a reward function for energy-efficient field exploration together with a sparse trajectory parameterization. By reducing both, energy consumption and computational complexity, we enable underwater field exploration with μAUVs. We benchmark the performance of our exploration framework in simulation against state-of-the-art exploratory planning schemes and provide an experimental study using a low-cost micro diving agent. In order to support community-wide algorithm benchmarking, our code and robot design can be accessed online.},
  archive   = {C_IROS},
  author    = {Daniel A Duecker and Benedikt Mersch and Rene C Hochdahl and Edwin Kreuzer},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635962},
  pages     = {8649-8656},
  title     = {Embedded stochastic field exploration with micro diving agents using bayesian optimization-guided tree-search and GMRFs},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A predictive control method for stabilizing a
manipulator-based UAV landing platform on fluctuating marine surface.
<em>IROS</em>, 8625–8632. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the process of landing unmanned aerial vehicles (UAVs) on an unmanned surface vehicle (USV), a manipulator can be applied to help the UAV land safely and accurately. However, it is a challenge to control the manipulator on a disturbed USV due to joint velocity constraints and bandwidth limitations. To solve this problem, a predictive control framework is proposed in this paper. We leverage a first-order delay system to describe the kinematics of each joint, and control joint velocities by the model predictive controller (MPC). To generate references for MPC, the motion of the floating base needs to be predicted. We apply the recent approach for motion prediction based on the wavelet network (WN) and modify the network to get smooth trajectories. The accuracy of the modified wavelet network (MWN) for motion prediction is tested on four-hour motion data from the real ocean environment and the smoothness of the generated trajectories is also evaluated. Simulations and experiments are implemented to verify the proposed method, the results show that the average control accuracies are improved by more than 30\% and 50\% in position and rotation compared with the traditional inverse kinematics (IK) controller for 1 Hz base fluctuation.},
  archive   = {C_IROS},
  author    = {Ruoyu Xu and Xiaoqiang Ji and Jiafan Hou and Hengli Liu and Huihuan Qian},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636055},
  pages     = {8625-8632},
  title     = {A predictive control method for stabilizing a manipulator-based UAV landing platform on fluctuating marine surface},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). From aerobatics to hydrobatics: Agile trajectory planning
and tracking for micro underwater robots. <em>IROS</em>, 8617–8624. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Aerobatic quadrotors have been a very active field of research for the last two decades. Their huge community boosted the development of computational light-weight planning and control algorithms. In contrast and despite recent progress, research on agile micro autonomous underwater vehicles (µAUV) is still in its infancy. Both vehicle classes share a close relationship. They achieve high speeds of multiple bodylengths per second. At the same time they are subject to limited onboard resources such as sensors and computing power.In this work, we explore and exploit the potential synergies between aerobatic drones and hydrobatic µAUVs. In order to demonstrate the possible transfer of concepts we build on a state-of-the-art quadrotor trajectory planning framework and extend it to incorporate hydrodynamic effects. Furthermore, we study in a series of experiments the performance of the transferred concepts and show that various quadrotor simplifications match well for hydrobatic µAUVs.},
  archive   = {C_IROS},
  author    = {Daniel A Duecker and Christian Horst and Edwin Kreuzer},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636154},
  pages     = {8617-8624},
  title     = {From aerobatics to hydrobatics: Agile trajectory planning and tracking for micro underwater robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Diverse complexity measures for dataset curation in
self-driving. <em>IROS</em>, 8609–8616. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern self-driving systems heavily rely on deep learning. As a consequence, their performance is influenced significantly by the quality and richness of the training data. Data collection platforms can generate many hours of raw data on a daily basis, however, it is not feasible to label everything. Therefore, it is critical to have a mechanism to identify &quot;what to label&quot;. Active learning approaches identify examples to label, but their interestingness is tied to a fixed model performing a particular task. These assumptions are not valid in self-driving, where we must solve a diverse set of tasks (i.e., perception, motion forecasting, and planning) and models frequently evolve over time. In this paper, we introduce a novel approach to dataset selection that exploits a diverse set of criteria that quantize interestingness of traffic scenes. Our experiments on a wide range of tasks and models demonstrate that the proposed curation pipeline is able to select datasets that lead to better generalization and improved performance.},
  archive   = {C_IROS},
  author    = {Abbas Sadat and Sean Segal and Sergio Casas and James Tu and Bin Yang and Raquel Urtasun and Ersin Yumer},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636869},
  pages     = {8609-8616},
  title     = {Diverse complexity measures for dataset curation in self-driving},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TUM-VIE: The TUM stereo visual-inertial event dataset.
<em>IROS</em>, 8601–8608. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Event cameras are bio-inspired vision sensors which measure per pixel brightness changes. They offer numerous benefits over traditional, frame-based cameras, including low latency, high dynamic range, high temporal resolution and low power consumption. Thus, these sensors are suited for robotics and virtual reality applications. To foster the development of 3D perception and navigation algorithms with event cameras, we present the TUM-VIE dataset. It consists of a large variety of handheld and head-mounted sequences in indoor and outdoor environments, including rapid motion during sports and high dynamic range scenarios. The dataset contains stereo event data, stereo grayscale frames at 20Hz as well as IMU data at 200Hz. Timestamps between all sensors are synchronized in hardware. The event cameras contain a large sensor of 1280x720 pixels, which is significantly larger than the sensors used in existing stereo event datasets (at least by a factor of ten). We provide ground truth poses from a motion capture system at 120Hz during the beginning and end of each sequence, which can be used for trajectory evaluation. TUM-VIE includes challenging sequences where state-of-the art visual SLAM algorithms either fail or result in large drift. Hence, our dataset can help to push the boundary of future research on event-based visual-inertial perception algorithms.},
  archive   = {C_IROS},
  author    = {Simon Klenk and Jason Chui and Nikolaus Demmel and Daniel Cremers},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636728},
  pages     = {8601-8608},
  title     = {TUM-VIE: The TUM stereo visual-inertial event dataset},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Grounding linguistic commands to navigable regions.
<em>IROS</em>, 8593–8600. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans have a natural ability to effortlessly comprehend linguistic commands such as “park next to the yellow sedan” and instinctively know which region of the road the vehicle should navigate. Extending this ability to autonomous vehicles is the next step towards creating fully autonomous agents that respond and act according to human commands. To this end, we propose the novel task of Referring Navigable Regions (RNR), i.e., grounding regions of interest for navigation based on the linguistic command. RNR is different from Referring Image Segmentation (RIS), which focuses on grounding an object referred to by the natural language expression instead of grounding a navigable region. For example, for a command “park next to the yellow sedan,” RIS will aim to segment the referred sedan, and RNR aims to segment the suggested parking region on the road. We introduce a new dataset, Talk2Car-RegSeg, which extends the existing Talk2car [1] dataset with segmentation masks for the regions described by the linguistic commands. A separate test split with concise manoeuvre-oriented commands is provided to assess the practicality of our dataset. We benchmark the proposed dataset using a novel transformer-based architecture. We present extensive ablations and show superior performance over baselines on multiple evaluation metrics. A downstream path planner generating trajectories based on RNR outputs confirms the efficacy of the proposed framework.},
  archive   = {C_IROS},
  author    = {Nivedita Rufus and Kanishk Jain and Unni Krishnan R Nair and Vineet Gandhi and K Madhava Krishna},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636172},
  pages     = {8593-8600},
  title     = {Grounding linguistic commands to navigable regions},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Indoor future person localization from an egocentric
wearable camera. <em>IROS</em>, 8586–8592. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate prediction of future person location and movement trajectory from an egocentric wearable camera can benefit a wide range of applications, such as assisting visually impaired people in navigation, and the development of mobility assistance for people with disability. In this work, a new egocentric dataset was constructed using a wearable camera, with 8,250 short clips of a targeted person either walking 1) toward, 2) away, or 3) across the camera wearer in indoor environments, or 4) staying still in the scene, and 13,817 person bounding boxes were manually labelled. Apart from the bounding boxes, the dataset also contains the estimated pose of the targeted person as well as the IMU signal of the wearable camera at each time point. An LSTM-based encoder-decoder framework was designed to predict the future location and movement trajectory of the targeted person in this egocentric setting. Extensive experiments have been conducted on the new dataset, and have shown that the proposed method is able to reliably and better predict future person location and trajectory in egocentric videos captured by the wearable camera compared to three baselines.},
  archive   = {C_IROS},
  author    = {Jianing Qiu and Frank P.-W. Lo and Xiao Gu and Yingnan Sun and Shuo Jiang and Benny Lo},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635868},
  pages     = {8586-8592},
  title     = {Indoor future person localization from an egocentric wearable camera},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ChangeSim: Towards end-to-end online scene change detection
in industrial indoor environments. <em>IROS</em>, 8578–8585. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a challenging dataset, ChangeSim, aimed at online scene change detection (SCD) and more. The data is collected in photo-realistic simulation environments with the presence of environmental non-targeted variations, such as air turbidity and light condition changes, as well as targeted object changes in industrial indoor environments. By collecting data in simulations, multi-modal sensor data and precise ground truth labels are obtainable such as the RGB image, depth image, semantic segmentation, change segmentation, camera poses, and 3D reconstructions. While the previous online SCD datasets evaluate models given well-aligned image pairs, ChangeSim also provides raw unpaired sequences that present an opportunity to develop an online SCD model in an end-to-end manner, considering both pairing and detection. Experiments show that even the latest pair-based SCD models suffer from the bottleneck of the pairing process, and it gets worse when the environment contains the non-targeted variations. Our dataset is available at https://sammica.github.io/ChangeSim/.},
  archive   = {C_IROS},
  author    = {Jin-Man Park and Jae-Hyuk Jang and Sahng-Min Yoo and Sun-Kyung Lee and Ue-Hwan Kim and Jong-Hwan Kim},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636350},
  pages     = {8578-8585},
  title     = {ChangeSim: Towards end-to-end online scene change detection in industrial indoor environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The radar ghost dataset – an evaluation of ghost objects in
automotive radar data. <em>IROS</em>, 8570–8577. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Radar sensors have a long tradition in advanced driver assistance systems (ADAS) and also play a major role in current concepts for autonomous vehicles. Their importance is reasoned by their high robustness against meteorological effects, such as rain, snow, or fog, and the radar’s ability to measure relative radial velocity differences via the Doppler effect. The cause for these advantages, namely the large wavelength, is also one of the drawbacks of radar sensors. Compared to camera or lidar sensor, a lot more surfaces in a typical traffic scenario appear flat relative to the radar’s emitted signal. This results in multi-path reflections or so called ghost detections in the radar signal. Ghost objects pose a major source for potential false positive detections in a vehicle’s perception pipeline. Therefore, it is important to be able to segregate multipath reflections from direct ones. In this article, we present a dataset with detailed manual annotations for different kinds of ghost detections. Moreover, two different approaches for identifying these kinds of objects are evaluated. We hope that our dataset encourages more researchers to engage in the fields of multi-path object suppression or exploitation.},
  archive   = {C_IROS},
  author    = {Florian Kraus and Nicolas Scheiner and Werner Ritter and Klaus Dietmayer},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636338},
  pages     = {8570-8577},
  title     = {The radar ghost dataset – an evaluation of ghost objects in automotive radar data},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Let’s play for action: Recognizing activities of daily
living by learning from life simulation video games. <em>IROS</em>,
8563–8569. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recognizing Activities of Daily Living (ADL) is a vital process for intelligent assistive robots, but collecting large annotated datasets requires time-consuming temporal labeling and raises privacy concerns, e.g., if the data is collected in a real household. In this work, we explore the concept of constructing training examples for ADL recognition by playing life simulation video games and introduce the SIMS4ACTION dataset created with the popular commercial game THE SIMS 4. We build SIMS4ACTION by specifically executing actions-of-interest in a &quot;top-down&quot; manner, while the gaming circumstances allow us to freely switch between environments, camera angles and subject appearances. While ADL recognition on gaming data is interesting from the theoretical perspective, the key challenge arises from transferring it to the real-world applications, such as smart-homes or assistive robotics. To meet this requirement, SIMS4ACTION is accompanied with a GAMING→REAL benchmark, where the models are evaluated on real videos derived from an existing ADL dataset. We integrate two modern algorithms for video-based activity recognition in our framework, revealing the value of life simulation video games as an inexpensive and far less intrusive source of training data. However, our results also indicate that tasks involving a mixture of gaming and real data are challenging, opening a new research direction. We will make our dataset publicly available at https://github.com/aroitberg/sims4action.},
  archive   = {C_IROS},
  author    = {Alina Roitberg and David Schneider and Aulia Djamal and Constantin Seibold and Simon Reiß and Rainer Stiefelhagen},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636381},
  pages     = {8563-8569},
  title     = {Let’s play for action: Recognizing activities of daily living by learning from life simulation video games},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A portable remote optoelectronic tweezer system for
microobjects manipulation. <em>IROS</em>, 8557–8562. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Non-contact manipulation technology has extensive application in the manipulation and fabrication of micro/nanomaterials. However, the manipulation devices are often precise and complex, operated only by professionals and subject to site constraints. We propose a simple optoelectronic tweezer platform, which can be controlled remotely and simply for the manipulation of microparticles at different scales, based on the novel manipulation technique called optically-induced dielectrophoresis. In this work, we design and set up the optoelectronic tweezer manipulation platform and develop the full-function human-computer interactive control interface and graphics rendering system to simplify the micro-operation process. Using Qt5.0 development environment, an experimental image processing system with multi-thread characteristics is developed, and the information interaction requirements needed in the experimental operation of optoelectronic tweezers are integrated into a control system to achieve unified information management and data analysis. Combined with cloud computing technology, the system realizes local/remote synchronous linkage operation, with cross-platform operation capability of various portable operating terminals like laptop and iPad.},
  archive   = {C_IROS},
  author    = {Yuqing Cao and Shuzhang Liang and Hanlong Chen and Chunyuan Gan and Li Song and Chaonan Zhang and Fumihito Arai and Lin Feng},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636265},
  pages     = {8557-8562},
  title     = {A portable remote optoelectronic tweezer system for microobjects manipulation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analysis of the effect of clearance in spherical joints on
the rotation accuracy of parallel type micro-robotic systems.
<em>IROS</em>, 8551–8556. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The spherical joint is an effective solution to design parallel micro-robotic systems with rotation capabilities in the three-dimensional space. This type of joint has however some non-linear characteristics, such as the clearance, which affect the positioning accuracy in micro-robotic tasks. The starting point of this study lies in experimental observations of rotation errors from a 3-PPPS 6-DOF parallel micro-robotic systems operating inside a scanning electron microscope. The objective of the paper is to assess the role of the spherical joints in the rotation errors and to evaluate whether the joints non-linearities can cause errors with the same order of magnitude as those observed experimentally. To this end, the first part of the study addresses the modeling of 3-PPPS 6-DOF parallel micro-robotic systems with spherical joints including the clearance. This model allows for analysing the effect of the clearance on position and rotation accuracies of the micro-robotic system. It is found by simulations that the same positioning behavior as in the experiments occurs when the clearance of the spherical joint is included in the model, supporting the hypothesis. Therefore, it is concluded that clearance in spherical joints has a significant effect on the precision of parallel type micro-robotic systems which opens new challenges in the control of poly-articulated micro-robotic systems with clearance compensation.},
  archive   = {C_IROS},
  author    = {Michael Pumphrey and Mahmoud Al-Tamimi and Aylar Abouzarkhanifard and Mohammad Al Janaideh and Stéphane Régnier and Mokrane Boudaoud},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636853},
  pages     = {8551-8556},
  title     = {Analysis of the effect of clearance in spherical joints on the rotation accuracy of parallel type micro-robotic systems},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Open-loop magnetic actuation of helical robots using
position-constrained rotating dipole field. <em>IROS</em>, 8545–8550.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Control of tetherless magnetically actuated helical robots using rotating dipole fields has a wide variety of medical applications. The most promising technique in manipulation of these robots involves a rotating permanent magnet controlled by a robotic manipulator. In this work, we study the open-loop response of helical robots (in viscous fluids characterized by low Reynolds numbers) in the presence of position constraints on the actuating rotating permanent magnet. We first derive a mapping between the space of the manipulator’s joints, the produced magnetic fields in three-dimensional space, and the translational and rotational velocities of the helical robot. Then, we constrain the 3D position of the rotating dipole field and predict the response of the helical robot by controlling its angular velocity using the constrained mapping. We demonstrate open-loop control and gravity compensation of the robot using the angular velocities of the actuating permanent magnet while enforcing constraints on the end-effector position.},
  archive   = {C_IROS},
  author    = {Ritwik Avaneesh and Roberto Venezian and Chang-Sei Kim and Jong-Oh Park and Sarthak Misra and Islam S. M. Khalil},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636564},
  pages     = {8545-8550},
  title     = {Open-loop magnetic actuation of helical robots using position-constrained rotating dipole field},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling of bilayer hydrogel springs for microrobots with
adaptive locomotion. <em>IROS</em>, 8530–8537. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Adaptive locomotion of microrobots can be achieved by using a smart polymer such as a hydrogel. For hydrogel-based bilayer helical microrobots, the change of environment such as temperature and pH can result in shape deformation into helical shapes differing from their initial state and hence swimming performance. In this work, we proposed a model for studying the parameters that affect the shape deformation of a hydrogel-based bilayer helical microrobot. Moreover, the dynamics of some examples of responsive helical swimming are compared before and after stimulation.},
  archive   = {C_IROS},
  author    = {Liyuan Tan and David J. Cappelleri},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636693},
  pages     = {8530-8537},
  title     = {Modeling of bilayer hydrogel springs for microrobots with adaptive locomotion},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simultaneous actuation and localization of magnetic robots
using mobile coils and eye-in-hand hall-effect sensors. <em>IROS</em>,
8515–8521. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large workspace localization of magnetic robots is important for medical applications. This paper presents a novel localization strategy to achieve simultaneous localization and actuation of magnetic robots using hall-effect sensors. We integrate 25 sensors into a sensing probe and mount it on to the mobile-coil system, which realizes accurate sensing and actuation of magnetic devices within a cylindrical workspace of ϕ500 mm×150 mm. Simulation results show the average localization error using the proposed method is 1.7 mm. A verification experiment is conducted to prove the design advantages; Another two experiments are conducted to demonstrate the simultaneous actuation and localization of a torque-driven robot and a force-driven floating robot respectively. For the force-driven floating robot, the average variation between the localization results and the desired trajectory is less than 2 mm.},
  archive   = {C_IROS},
  author    = {Moqiu Zhang and Lidong Yang and Chong Zhang and Zhengxin Yang and Li Zhang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635953},
  pages     = {8515-8521},
  title     = {Simultaneous actuation and localization of magnetic robots using mobile coils and eye-in-hand hall-effect sensors},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Run like a dog: Learning based whole-body control framework
for quadruped gait style transfer. <em>IROS</em>, 8508–8514. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, a learning-based whole-body loco-motion controller is proposed, which enables quadruped robots to perform running in the style of real animals. We use a low-level controller based on multi-rigid body dynamics to calculate desired torques for each joint, while the high-level neural network policy planning the expected gait and foothold. The policy is trained with reinforcement learning, so that the robot can track a variety of trajectories according to the gait patterns recorded from real-world dogs. We transfer the walking and running gait style to quadrupeds in simulation, involving pace, trot, high-speed gallop and natural transitions. The performance is evaluated by the synchronization rate of contact state between the policy result and the recorded sequence. In the experiments, the robot runs steadily at a speed of 2 m/s and showcases a notable synchronization rate of about 80\%. Without prior knowledge, the policy demonstrates a realistic foothold distribution that covers the central area of the torso, which is prevalent in running animals.},
  archive   = {C_IROS},
  author    = {Fulong Yin and Annan Tang and Liangwei Xu and Yue Cao and Yu Zheng and Zhengyou Zhang and Xiangyu Chen},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636805},
  pages     = {8508-8514},
  title     = {Run like a dog: Learning based whole-body control framework for quadruped gait style transfer},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Animal gaits on quadrupedal robots using motion matching and
model-based control. <em>IROS</em>, 8500–8507. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we explore the challenge of generating animal-like walking motions for legged robots. To this end, we propose a versatile and robust control pipeline that combines a state-of-the-art model-based controller with a data-driven technique that is commonly used in computer animation. We demonstrate the efficacy of our control framework on a variety of quadrupedal robots in simulation. We show, in particular, that our approach can automatically reproduce key characteristics of animal motions, including speed-specific gaits, unscripted footfall patterns for nonperiodic motions, and natural small variations in overall body movements.},
  archive   = {C_IROS},
  author    = {Dongho Kang and Simon Zimmermann and Stelian Coros},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635838},
  pages     = {8500-8507},
  title     = {Animal gaits on quadrupedal robots using motion matching and model-based control},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GPU-accelerated rapid planar region extraction for dynamic
behaviors on legged robots. <em>IROS</em>, 8493–8499. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Legged robots require fast and accurate representation of their surrounding terrain to achieve behaviors such as running, push recovery, continuous walking, backflips, while also utilizing on-board computational resources efficiently. The desired tasks can be achieved efficiently by representing the environment using planar regions. However, existing methods for planar region extraction are either too slow or require significant compute time on the Central Processing Unit (CPU). In this work we exploit key properties of depth images and Graphical Processing Unit (GPU) to estimate planar regions around the robot at very high frame rates of 150-200 Hz. The proposed algorithm uses a set of fully customizable and interchangeable set of kernel layers on the GPU to process the depth map in parallel and generate a locally connected graph structure, which is later separated into planar components using a basic depth-first search. We test the proposed algorithm on the Atlas robot while performing different walking behaviors on oriented cinder blocks, as well as in simulation with simulated sensor and robot. The algorithm is open-sourced for research on legged robots and other fields.},
  archive   = {C_IROS},
  author    = {Bhavyansh Mishra and Duncan Calvert and Sylvain Bertrand and Stephen McCrory and Robert Griffin and Hakki Erhan Sevil},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636009},
  pages     = {8493-8499},
  title     = {GPU-accelerated rapid planar region extraction for dynamic behaviors on legged robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rapid stability margin estimation for contact-rich
locomotion. <em>IROS</em>, 8485–8492. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The efficient evaluation the dynamic stability of legged robots on non-coplanar terrains is important when developing motion planning and control policies. The inference time of this measure has a strong influence on how fast a robot can react to unexpected events, plan its future footsteps or its body trajectory. Existing approaches suitable for real-time decision making are either limited to flat ground or to quasi-static locomotion. Furthermore, joint-space feasibility constraints are usually not considered in receding-horizon planning as their high dimensionality prohibits this. In this paper we propose the usage of a stability criterion for dynamic locomotion on rough terrain based on the Feasible Region (FR) and the Instantaneous Capture Point (ICP) and we leverage a Neural Network (NN) to quickly estimate it. We show that our network achieves satisfactory accuracy with respect to its analytical counterpart with a speed up of three orders-of-magnitude. It also enables the evaluation of the stability margin&#39;s gradient. We demonstrate this learned stability margin in two diverse applications - Reinforcement Learning (RL) and nonlinear Trajectory Optimization (TO) for legged robots. We demonstrate on a full-sized quadruped robot that the network enables the computation of physically-realizable Center of Mass (CoM) trajectories and foothold locations satisfying friction constraints and joint-torque limits in a receding-horizon fashion and on non-coplanar terrains.},
  archive   = {C_IROS},
  author    = {Romeo Orsolino and Siddhant Gangapurwala and Oliwier Melon and Mathieu Geisert and Ioannis Havoutis and Maurice Fallon},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636474},
  pages     = {8485-8492},
  title     = {Rapid stability margin estimation for contact-rich locomotion},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Verifying safe transitions between dynamic motion primitives
on legged robots. <em>IROS</em>, 8477–8484. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Functional autonomous systems often realize complex tasks by utilizing state machines comprised of discrete primitive behaviors and transitions between these behaviors. This architecture has been widely studied in the context of quasi-static and dynamics-independent systems. However, applications of this concept to dynamical systems are relatively sparse, despite extensive research on individual dynamic primitive behaviors, which we refer to as &quot;motion primitives.&quot; This paper formalizes a process to determine dynamic-state aware conditions for transitions between motion primitives in the context of safety. The result is framed as a &quot;motion primitive graph&quot; that can be traversed by standard graph search and planning algorithms to realize functional autonomy. To demonstrate this framework, dynamic motion primitives— including standing up, walking, and jumping—and the transitions between these behaviors are experimentally realized on a quadrupedal robot.},
  archive   = {C_IROS},
  author    = {Wyatt Ubellacker and Noel Csomay-Shanklin and Tamas G. Molnar and Aaron D. Ames},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636537},
  pages     = {8477-8484},
  title     = {Verifying safe transitions between dynamic motion primitives on legged robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Linear policies are sufficient to enable low-cost
quadrupedal robots to traverse rough terrain. <em>IROS</em>, 8469–8476.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The availability of inexpensive 3D-printed quadrupedal robots motivates the development of learning-based methods compatible with low-cost embedded processors and position-controlled hobby servos. In this work, we show that a linear policy is sufficient to modulate an open-loop trajectory generator, enabling a quadruped to walk over rough, unknown terrain, with limited sensing. The policy is trained in simulation using randomized terrain and dynamics and directly deployed on the robot. We show that the resulting controller can be implemented on resource-constrained systems. We demonstrate the results by deploying the policy on the OpenQuadruped, an open-source 3D-printed robot equipped with hobby servos and an embedded microprocessor.},
  archive   = {C_IROS},
  author    = {Maurice Rahme and Ian Abraham and Matthew L. Elwin and Todd D. Murphey},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636011},
  pages     = {8469-8476},
  title     = {Linear policies are sufficient to enable low-cost quadrupedal robots to traverse rough terrain},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hierarchical framework for quadruped locomotion based on
reinforcement learning. <em>IROS</em>, 8462–8468. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Quadruped locomotion is a challenging task for learning-based algorithms. It requires tedious manual tuning and is difficult to deploy in reality due to the reality gap. In this paper, we propose a quadruped robot learning system for agile locomotion which does not require any pre-training and works well in various real-world terrains. We introduce a hierarchical learning framework that uses reinforcement learning as the high-level policy to adjust the low-level trajectory generator for better adaptability to the terrain. We compact the observation and action space of the reinforcement learning to deploy it on a host computer in reality. Besides, we design a trajectory generator guided by robot posture, which can generate adaptive foot trajectory to interact with the environment. Experimental results show that our system can be easily deployed in reality while only trained in simulation, and also has the advantages of fast convergence and good terrain adaptability. The supplementary video demonstration is available at https://vsislab.github.io/hfql/.},
  archive   = {C_IROS},
  author    = {Wenhao Tan and Xing Fang and Wei Zhang and Ran Song and Teng Chen and Yu Zheng and Yibin Li},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636757},
  pages     = {8462-8468},
  title     = {A hierarchical framework for quadruped locomotion based on reinforcement learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Autonomous decision making in a bioinspired adaptive robotic
anchoring module. <em>IROS</em>, 8456–8461. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a bioinspired adaptive anchoring module that can be integrated into robots to enhance their mobility and manipulation abilities. The design of the module is inspired by the structure of the mouth in Chilean lamprey (Mordacia lapicida) where a combination of suction and several arrays of teeth with different sizes around the mouth opening is used for catching preys and anchoring onto them. The module can deploy a suitable mode of attachment, via teeth or vacuum suction, to different contact surfaces in response to the textural properties of those surfaces. In order to make a decision on the suitable mode of attachment, an original dataset of 500 images of outdoor and indoor surfaces was used to train a visual surface examination model using YOLOv3; a virtually real-time object detection algorithm. The mean average precision of the trained model was calculated to be 91\%. We have conducted a series of pull-out tests to characterize the module&#39;s strength of attachments. The results of the experiments indicate that the anchoring module can withstand an applied detachment force of up to 70N and 30N when attached using teeth and vacuum suction, respectively.},
  archive   = {C_IROS},
  author    = {Rasoul Sadeghian and Pooya Sareh and Shahrooz Shahin and Sina Sareh},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636378},
  pages     = {8456-8461},
  title     = {Autonomous decision making in a bioinspired adaptive robotic anchoring module},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Design of galloping robots with elastic spine: Tracking
relations between dynamic model parameters based on motion analysis of a
real cheetah. <em>IROS</em>, 8450–8455. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One way to create a quadruped galloping robot from scratch is to design a brick-shaped body and utilize relatively simple open-chain leg mechanisms controlled with relatively complex control algorithms. Alternatively, we can look at how nature solved the same task designing fast mammals such as cheetah, and by means of morphological computation, we can design a complex mechanical system that has much of the desired behavior within inherent dynamics and only a little control effort is needed to stabilize or augment the motion.In this paper, we have analyzed a real cheetah motion using video tracking and looked for a way to match the dynamic model parameters of the real cheetah with a galloping robot with an elastic spine. We believe the elastic spine is the essential feature for a fast-running energy-efficient galloping robot. Within this paper, we are focused on the flying stage when the elastic spine affects the motion of the robot’s front and rear bodies. We have found how to optimize mass distribution and elasticity in the spine in order to get the cheetah-like galloping motion of a quadruped robot.},
  archive   = {C_IROS},
  author    = {Olga Borisova and Ivan Borisov and Sergey Kolyubin and Stefano Stramigioli},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636859},
  pages     = {8450-8455},
  title     = {Design of galloping robots with elastic spine: Tracking relations between dynamic model parameters based on motion analysis of a real cheetah},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust top-down and bottom-up visual saliency for mobile
robots using bio-inspired design principles. <em>IROS</em>, 8444–8449.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern camera systems in robotics tend to pro-duce overwhelming amounts of visual information due to their high resolutions and high frame rates. This raises a fundamental question of how robots should focus attention on a region of the visual scene, and how they should process information in the periphery. This is particularly an issue for mobile robots, where the computational resources of low-power embedded computing boards tend to be much less than for workstations. In this paper, we look to biological design in the primate brain for inspiration on how to solve this problem. We develop a novel computational fusion of bottom-up and top-down visual saliency information. The bottom-up saliency is produced using standard colour, intensity, and motion image processing methods. The top-down saliency is produced using a deep convolutional neural network for object detection and recognition, with foveated images for computational efficiency. Regions of attention are obtained using a computational model of the basal ganglia, thought to be involved in optimal decision making. The model of the basal ganglia is based on the multi-hypothesis sequential probability ratio test (MSPRT). The visual saliency scheme is evaluated on omnidirectional video feed highlighting a proximity to human behaviour.},
  archive   = {C_IROS},
  author    = {Uziel Jaramillo-Avila and Jonathan M. Aitken and Kevin Gurney and Sean R. Anderson},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636800},
  pages     = {8444-8449},
  title     = {Robust top-down and bottom-up visual saliency for mobile robots using bio-inspired design principles},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A method to use nonlinear dynamics in a whisker sensor for
terrain identification by mobile robots. <em>IROS</em>, 8437–8443. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper shows analytical and experimental evidence of using the vibration dynamics of a compliant whisker for accurate terrain classification during steady state motion of a mobile robot. A Hall effect sensor was used to measure whisker vibrations due to perturbations from the ground. Analytical results predict that the whisker vibrations will have one dominant frequency at the vertical perturbation frequency of the mobile robot and one with distinct frequency components. These frequency components may come from bifurcation of vibration frequency due to nonlinear interaction dynamics at steady state. Experimental results also exhibit distinct dominant frequency components unique to the speed of the robot and the terrain roughness. This nonlinear dynamic feature is used in a deep multi-layer perceptron neural network to classify terrains. We achieved 85.6\% prediction success rate for seven flat terrain surfaces with different textures.},
  archive   = {C_IROS},
  author    = {Zhenhua Yu and S.M. Hadi Sadati and Hasitha Wegiriya and Peter Childs and Thrishantha Nanayakkara},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636571},
  pages     = {8437-8443},
  title     = {A method to use nonlinear dynamics in a whisker sensor for terrain identification by mobile robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evolving infotaxis for meandering environments.
<em>IROS</em>, 8431–8436. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Locating odour sources with mobile robots is a difficult task with many real world applications. Over the years, researchers have devised bio-inspired and cognitive methods to enable mobile robots to fulfil this task. One of the most popular cognitive approaches is Infotaxis, which computes a probability map for the location of the chemical source and, on each time step, moves the robot in the direction that minimises the entropy of that probability map. The main difficulty for applying Infotaxis in the real world is selecting proper values for the parameters of its internal gas dispersion model, as it has been shown that its performance is greatly influenced by the accuracy of said model. This work proposes a Genetic Algorithm for optimising those parameters for specific environments. The proposed method is applied to environments with distinct wind and odour dispersion characteristics and the resulting parameters are compared. Moreover, the performance of Infotaxis is compared to that of reactive search strategies evolved by Geometric Syntactic Genetic Programming. The statistically validated results show that the evolved reactive strategies achieve equivalent success rates to Infotaxis, while being significantly faster. Real world experiments conducted in a controlled wind tunnel validated the simulation results.},
  archive   = {C_IROS},
  author    = {João Macedo and Lino Marques and Ernesto Costa},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636779},
  pages     = {8431-8436},
  title     = {Evolving infotaxis for meandering environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bat bot 2.0: Bio-inspired anisotropic skin, passive wrist
joints, and redesigned flapping mechanism. <em>IROS</em>, 8424–8430. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bat flight has been an underdeveloped area of bio-inspired robotics because of the vast complexities of biological bat flight and the over 40 degrees of freedom present in their bodies. The robotic flapping system Bat Bot (B2) has been shown to exhibit fundamental properties of biological bat flight with its articulated wings, its deformable membrane, and its controllable hindlimbs. However, the system is limited in performance by its relatively large mass for the thrust it produces. In an effort to further pursue this important area of flapping flight, we have made several important hardware improvements to the system based on biological inspiration. These include passive wrist joints to reduce negative lift in the upstroke and a novel elastic fiber membrane to mimic the anistropic nature of bat skin for performance and durability. The redesigned flapping mechanism and structure have reduced the weight by 22\%, increased the flapping amplitude, lowered mechanical slackness, and improved mass distribution. These hardware improvements are functional together in free-flight tests. This new system Bat Bot 2.0 (B2.0) provides insights into the important elements of design of bat robots, and it brings the goal of complex bat flight maneuvers closer to reality.},
  archive   = {C_IROS},
  author    = {Jonathan Hoff and Nicole Jeon and Patrick Li and Joohyung Kim},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636496},
  pages     = {8424-8430},
  title     = {Bat bot 2.0: Bio-inspired anisotropic skin, passive wrist joints, and redesigned flapping mechanism},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Marine autonomous navigation for biomimetic underwater
robots based on deep stereo attention network. <em>IROS</em>, 8418–8423.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a multi-objective visionbased navigation network for biomimetic underwater robots to cope with scientific observation, target selection, and obstacle avoidance in marine missions. Structurally, a stereo block attention module is first constructed to serially extract the channel and spatial attention portion of the real-time visual feedback. Next, the parallax attention mechanism is introduced to enable the network to excavate implicit parallax information in stereo pairs, effectively eliminating the oscillation of the network output in the presence of ambiguous visual input. Further, with the assistance of other low-cost sensors, the proposed navigation network can be expanded in some largescale application scenarios, such as sparse coral observation. Finally, underwater simulations reveal that the proposed method obtains significantly improved control effect and real-time ability, compared with other related works. In particular, based on a self-developed biomimetic robotic dolphin, collision-free simulations with a cumulative distance beyond 1000 m were carried out and validated the effectiveness and the superiority of the navigation network, where both dense and sparse targets were fully tested. The robotic dolphin can not only successfully conduct accurate coral observation without collision, but also quest the observation targets as much as possible in the area where the observation targets are concentrated. The proposed network provides an intelligent and efficient navigation scheme for autonomous underwater operation of small-size underwater robots.},
  archive   = {C_IROS},
  author    = {Shuaizheng Yan and Zhengxing Wu and Jian Wang and Min Tan and Junzhi Yu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636159},
  pages     = {8418-8423},
  title     = {Marine autonomous navigation for biomimetic underwater robots based on deep stereo attention network},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Traversability-based trajectory planning with quasi-dynamic
vehicle model in loose soil. <em>IROS</em>, 8411–8417. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a framework for trajectory planning that explicitly considers robotic traversability based on a quasi-dynamic vehicle model of a mobile robot in loose soil. The quasi-dynamic model estimates the slip effect due to wheel-terrain interaction forces regardless of solving complicated multibody dynamics. Therefore, our proposed model is computationally efficient for quantifying how the robot safely traverses each trajectory segment generated by a planning algorithm. The trajectory planning in our framework exploits a sampling-based incremental search algorithm, i.e., Closed-Loop Rapidly-Exploring Random Trees (CL-RRT). In the tree extension process of the CL-RRT, the traversability assessment based on the quasi-dynamic vehicle model excludes the trajectory segment associated with a hazardous wheel slip ratio. As a result, a trajectory generated from the proposed framework is safely traversable for the robot even in high slip terrain. Simulation results show that the proposed vehicle model can run 57K times faster than the dynamic model and predict the robot motion 3 times more accurately than the kinematic model. Multiple trials of the trajectory planning simulation show that our proposed framework incorporated with the quasi-dynamic model reduces a wheel slip ratio by about 40\% as compared with the kinematic model.},
  archive   = {C_IROS},
  author    = {Reiya Takemura and Genya Ishigami},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635891},
  pages     = {8411-8417},
  title     = {Traversability-based trajectory planning with quasi-dynamic vehicle model in loose soil},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Environmentally adaptive control including variance
minimization using stochastic predictive network with parametric bias:
Application to mobile robots. <em>IROS</em>, 8404–8410. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this study, we propose a predictive model composed of a recurrent neural network including parametric bias and stochastic elements, and an environmentally adaptive robot control method including variance minimization using the model. Robots which have flexible bodies or whose states can only be partially observed are difficult to modelize, and their predictive models often have stochastic behaviors. In addition, the physical state of the robot and the surrounding environment change sequentially, and so the predictive model can change online. Therefore, in this study, we construct a learning-based stochastic predictive model implemented in a neural network embedded with such information from the experience of the robot, and develop a control method for the robot to avoid unstable motion with large variance while adapting to the current environment. This method is verified through a mobile robot in simulation and to the actual robot Fetch.},
  archive   = {C_IROS},
  author    = {Kento Kawaharazuka and Koki Shinjo and Yoichiro Kawamura and Kei Okada and Masayuki Inaba},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636416},
  pages     = {8404-8410},
  title     = {Environmentally adaptive control including variance minimization using stochastic predictive network with parametric bias: Application to mobile robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Design and analysis of a bi-directional transformable wheel
robot trimode. <em>IROS</em>, 8396–8403. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This article presents a novel transformable wheeled robot with three motion modes. Based on the four-bar mechanism design, the transformable wheel can transform into CW (clockwise) legged wheel mode and CCW (counterclock-wise) legged wheel mode from the circular wheel. Both legged wheel modes achieve good obstacle-climbing performance when the robot overcomes obstacles such as steps in the forward and backward directions. Mathematical expression of shape of the transformable wheel is established for design variables selection of the wheel to improve the obstacle-overcoming ability of the legged wheel modes. Dynamics model of the robot in the step-overcoming scene is analyzed to determine and reduce the torque load of the motor. Finally, two prototypes of the robot are developed. The early-stage prototype made by 3D printing verifies the step-overcoming ability of the transformable wheel. Then, structural strength of the robot chassis and the robustness of the hardware are improved in the second prototype, which can play a role in SAR and indoor service tasks. In general, the prototype can overcome steps with the height of more than 2 times the radius of the wheel in CW/CCW legged wheel mode and move at a maximum speed of 3.15 m/s in circular wheel mode (about 5.83 times the length of the robot per second).},
  archive   = {C_IROS},
  author    = {Qiwei Xu and Hao Xu and Kun Xiong and Qinqin Zhou and Weizhong Guo},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636421},
  pages     = {8396-8403},
  title     = {Design and analysis of a bi-directional transformable wheel robot trimode},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Whole-body MPC and online gait sequence generation for
wheeled-legged robots. <em>IROS</em>, 8388–8395. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Our paper proposes a model predictive controller as a single-task formulation that simultaneously optimizes wheel and torso motions. This online joint velocity and ground reaction force optimization integrates a kinodynamic model of a wheeled quadrupedal robot. It defines the single rigid body dynamics along with the robot’s kinematics while treating the wheels as moving ground contacts. With this approach, we can accurately capture the robot’s rolling constraint and dynamics, enabling automatic discovery of hybrid maneuvers without needless motion heuristics. The formulation’s generality through the simultaneous optimization over the robot’s whole-body variables allows for a single set of parameters and makes online gait sequence adaptation possible. Aperiodic gait sequences are automatically found through kinematic leg utilities without the need for predefined contact and lift-off timings, reducing the cost of transport by up to 85\%. Our experiments demonstrate dynamic motions on a quadrupedal robot with non-steerable wheels in challenging indoor and outdoor environments. The paper’s findings contribute to evaluating a decomposed, i.e., sequential optimization of wheel and torso motion, and single-task motion planner with a novel quantity, the prediction error, which describes how well a receding horizon planner can predict the robot’s future state. To this end, we report an improvement of up to 71\% using our proposed single-task approach, making fast locomotion feasible and revealing wheeled-legged robots’ full potential.},
  archive   = {C_IROS},
  author    = {Marko Bjelonic and Ruben Grandia and Oliver Harley and Cla Galliard and Samuel Zimmermann and Marco Hutter},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636371},
  pages     = {8388-8395},
  title     = {Whole-body MPC and online gait sequence generation for wheeled-legged robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sub-optimal and robust path tracking: A geometric approach.
<em>IROS</em>, 8381–8387. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Path tracking controllers for non-holonomic vehicles are commonly designed with primary focus on robustness to different kinds of disturbances, vehicle dynamics and other effects. Subsequently, the path tracking behavior is improved by parameter optimization or controller extensions. The possible improvement, however, is already limited by the control design itself. To overcome this drawback, this paper presents an alternative approach: In a first step, we formulate the geometrically optimal solution for a Dubins car reaching a straight target path as a globally stabilizing sliding-mode feedback law. In a second step, we introduce a single tuning parameter that achieves robustness with respect to exemplary matched disturbances. The result is a sub-optimal but robust controller. We investigate the impact of tuning this parameter towards optimality or robustness with respect to the transient and stationary tracking behavior in simulation scenarios. Following the presented approach, it is possible to adapt the control law with respect to various disturbances facing and utilizing the trade-off between optimality and robustness. This is a significant advantage in the design of path tracking controllers due to the variety of vehicle actuation and measurement systems and operational design domains.},
  archive   = {C_IROS},
  author    = {Karin Tieber and Johannes Rumetshofer and Michael Stolz and Daniel Watzenig},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636166},
  pages     = {8381-8387},
  title     = {Sub-optimal and robust path tracking: A geometric approach},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Continuous robust trajectory tracking control for autonomous
ground vehicles considering lateral and longitudinal kinematics and
dynamics via recursive backstepping. <em>IROS</em>, 8373–8380. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Maintaining lateral and longitudinal trajectory tracking accuracy is challenging for autonomous ground vehicles (AGVs). This paper considers kinematics and dynamics of longitudinal and lateral motion to form a novel composite structure considering the cross-impacts of acceleration and steering commands on tracking errors in the lateral and longitudinal directions, respectively. The multi-tiered structure uses backstepping with smooth robust control to iteratively map kinematics-based velocity and yaw rate commands to slip-yaw dynamics-based acceleration and steering commands. In kinematics, longitudinal tracking error is stabilized by sliding mode control (SMC) while variable structure control (VSC) stabilizes lateral tracking error and balances tracking accuracy and steering gracefulness. Backstepping extends these commands through vehicle dynamics to provide robust steering and acceleration commands. Cross impacts between lateral and longitudinal motion is addressed by vehicle modeling and controller designs. A robust observer is applied for sideslip estimation to reject uncertainties. Peaking from the high gain observer and robust control is addressed. Stability analysis is provided and field experiments on an open road demonstrate and validate effectiveness of the controllers.},
  archive   = {C_IROS},
  author    = {Ming Xin and Yue Yin and Kai Zhang and David Lackner and Zhongchao Ren and Mark Minor},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635920},
  pages     = {8373-8380},
  title     = {Continuous robust trajectory tracking control for autonomous ground vehicles considering lateral and longitudinal kinematics and dynamics via recursive backstepping},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel testbed for investigating the impact of teleoperator
dynamics on perceived environment dynamics. <em>IROS</em>, 8358–8364.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human-in-the-loop telerobotic systems (HiLTS) are robotic tools designed to extend and in some circumstances improve the dexterous capabilities of the human operator in virtual and remote environments. Dexterous manipulation, however, depends on how well the telerobot is incorporated into the operator’s sensorimotor control scheme. Empirical evidence suggests that haptic feedback can lead to improved dexterity. Unfortunately, haptic feedback can also introduce dynamics between the leader and follower of the telerobot that affect both stability and device performance. While concerted research effort has focused on masking these device dynamics or bypassing them altogether, it is not well understood how human operators incorporate these dynamics into their control scheme. We believe that to advance dexterous telerobotic manipulation, it is crucial to understand the process by which human operators incorporate teleoperator dynamics and distinguish them from the dynamics of the environment. Key to this knowledge is an understanding of how advanced telerobotic architectures compare to the gold standard, the rigid mechanical teleoperators first introduced in the 1950’s. In this manuscript, we present a teleoperator testbed that has reconfigurable transmissions between the leader and follower to change its dynamic behavior. The intent of this testbed is to investigate the effect of the teleoperator’s dynamics on perception of and task performance in the remote/virtual environment. We describe the hardware and software components of the testbed and then demonstrate how the different teleoperator transmissions can lead to differences, sometimes significant, in the dynamics that would be felt by the operator when exploring the same environment.},
  archive   = {C_IROS},
  author    = {Mohit Singhala and Jeremy D. Brown},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636829},
  pages     = {8358-8364},
  title     = {A novel testbed for investigating the impact of teleoperator dynamics on perceived environment dynamics},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stable haptic teleoperation of UAVs via small l2 gain and
control barrier functions. <em>IROS</em>, 8352–8357. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel haptic teleoperation approach that considers not only the safety but also the stability of a teleoperation system. Specifically, we build upon previous work on haptic shared control, which generates a reference haptic feedback that helps the human operator to safely navigate the robot but without taking away their control authority. Crucially, in this approach the force rendered to the user is not directly reflected in the motion of the robot (which is still directly controlled by the user); however, previous work in the area neglected to consider the possible instabilities in feedback loop generated by a user that over-responds to the haptic force. In this paper we introduce a differential constraint on the rendered force that makes the system finite-gain ${{\mathcal{L}}_2}$ stable; the constraint results in a Quadratically Constrained Quadratic Program (QCQP), for which we provide a closed-form solution. Our constraint is related to, but less restrictive than, the typical passivity constraint used in previous literature. We conducted an experimental simulation in which a human operator flies a UAV near an obstacle to evaluate the proposed method.},
  archive   = {C_IROS},
  author    = {Dawei Zhang and Roberto Tron},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635952},
  pages     = {8352-8357},
  title     = {Stable haptic teleoperation of UAVs via small l2 gain and control barrier functions},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Two-stage optimization of a reconfigurable asymmetric 6-DOF
haptic robot for task-specific workspace. <em>IROS</em>, 8345–8351. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Parallel mechanisms (PMs) are commonly used for developing haptic devices due to low inertia, high rigidity and precision. However, limited workspace impedes their application for task-oriented robotic therapy which generally requires large motion ranges. To solve this problem, first, a PM- based reconfigurable asymmetric 6-DOF haptic interface was presented, and then a two-stage optimization method was proposed to make the robot implement two kinds of task-specific workspaces including gross motor tasks (GMTs) and fine motor tasks (FMTs). Optimization of this robot was conducted to pursue a compact size and high accuracy. The global conditioning index (GCI) and the occupied area of the robot were selected as the evaluation indices, where the GCI was derived using a dimensionally homogeneous Jacobian matrix. A multi-objective optimization method based on the genetic algorithm (GA) was utilized. The actual design parameters were finally defined from solutions of the Pareto front. The proposed two-stage optimization method provides a feasible solution for determining task-specific robotic workspace of the reconfigurable mechanism.},
  archive   = {C_IROS},
  author    = {Changqi Zhang and Congzhe Wang and Qing Miao and Mingming Zhang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636251},
  pages     = {8345-8351},
  title     = {Two-stage optimization of a reconfigurable asymmetric 6-DOF haptic robot for task-specific workspace},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Can a vibrotactile stimulation on fingertips make an
illusion of elbow joint movement? <em>IROS</em>, 8339–8344. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traditionally vibrotactile feedback delivered by haptic interfaces is used to provide additional support for visual interaction via prehensile object manipulation using fingers. Nevertheless, haptic stimuli can be also applied for non-prehensile interaction that involves movements of the elbow joint. In this paper, we have designed a table-top haptic device that provides vibrotactile stimulation to the user’s fingertips. This stimulation creates a kinesthetic illusion of the user’s forearm pivot movement with respect to the elbow joint. The vibrotactile stimulation is amplified by the pseudo-haptic effect delivered through a head-mounted display (HMD). The efficacy of the approach was validated in experiments with human subjects. The results show that the combination of pseudo-haptic and haptic illusion effects can be used to render various soft and rigid virtual objects.},
  archive   = {C_IROS},
  author    = {Dinmukhammed Mukashev and Adilzhan Adilkhanov and Zhanat Kappassov},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636500},
  pages     = {8339-8344},
  title     = {Can a vibrotactile stimulation on fingertips make an illusion of elbow joint movement?},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Variable stiffness folding joints for haptic feedback.
<em>IROS</em>, 8332–8338. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Origami robots composed of rigid parts with flexible joints have inherent compliance that enables deployment and reconfiguration for various shape adaptations. The major drawback of such mechanical compliance is its intrinsic softness and lack of controllability of this stiffness. In this work, we propose a design of variable stiffness origami joints to be integrated into large scale origami systems with an inner controllable joint stiffness. This novel way of controlling compliance in origami structures demonstrates that the embedded variable stiffness joint in the prototype can provide rich haptic feedback to the user through its compact, collapsible interactive platform.},
  archive   = {C_IROS},
  author    = {Fabio Zuliani and Jamie Paik},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636656},
  pages     = {8332-8338},
  title     = {Variable stiffness folding joints for haptic feedback},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robotic guidance system for visually impaired users running
outdoors using haptic feedback. <em>IROS</em>, 8325–8331. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For the visually impaired people, some outdoor activities like running or soccer are difficult, due to not being able to clearly see the environment. Recently, multiple researchers have contributed to help the visually impaired people run outdoors using robotic systems with different types of feedback, such as auditory feedback and haptic feedback. They discovered that using robotic systems can be an effective way to guide visually impaired people while exercising outdoors. In this paper, we propose a method to guide the visually impaired people to do sports outside using a robotic system with haptic feedback, and we evaluate the feasibility of the proposed system through experiments with blindfolded users running outdoors. In the running guidance task, the position of the runner is determined from the visual feed of a drone, and haptic feedback produced on the users’ left lower leg is used to convey to the runner the directions in which to move to remain on a specific path. Additionally, we compared the performance of users under different haptic feedback modalities in the running task. The three compared modalities are: producing vibration only during the swing phase, only during stance phase or producing vibration continuously. The experimental results for a running task showed that our system enabled users to remain inside a specified track 93\% of the total running time, while the ratio decreased to 79\%, 77\%, and 61\% when receiving vibrations during only swing phase, during only stance phase, and without using any feedback respectively. We also observed users felt safer while running blindfolded by using the proposed method.},
  archive   = {C_IROS},
  author    = {Zhenyu Liao and Jose Salazar and Yasuhisa Hirata},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636567},
  pages     = {8325-8331},
  title     = {Robotic guidance system for visually impaired users running outdoors using haptic feedback},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NMPC-MP: Real-time nonlinear model predictive control for
safe motion planning in manipulator teleoperation. <em>IROS</em>,
8309–8316. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion control and planning for the manipulator are critical components in manipulator teleoperation. Online (real-time) motion control is challenging for active obstacle avoidance and often results in fluctuating and unsafe motion. Offline motion planning, on the other hand, generates precise and secure trajectories for complex manipulation. In this paper, a real-time nonlinear model predictive control based motion planner (NMPC-MP) is designed for teleoperated manipulation. In contrast to traditional NMPC-based approaches, our model considers a complex environment with dynamic obstacles. Our multi-threaded NMPC-MP allows for real-time planning, including dynamic objects. We evaluate our approach both in a simulated environment and with real-world experiments using the Kinova ® Movo platform. The comparison to state-of-the-art approaches (e.g., RRT-Connect, CHOMP, and STOMP) shows a significant improvement in real-time motion planning using NMPC-MP. In real-world tests, the proposed planner was applied on a human-shaped dual manipulator setup. Our results show that the NMPC-MP runs in real-time and generates smooth and reliable trajectories. The experiments validate that the planner is able to precisely track active goals from the teleoperator while avoiding self-collision and obstacles.},
  archive   = {C_IROS},
  author    = {Siqi Hu and Edwin Babaians and Mojtaba Karimi and Eckehard Steinbach},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636802},
  pages     = {8309-8316},
  title     = {NMPC-MP: Real-time nonlinear model predictive control for safe motion planning in manipulator teleoperation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Search-based path planning for a high dimensional
manipulator in cluttered environments using optimization-based
primitives. <em>IROS</em>, 8301–8308. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work we tackle the path planning problem for a 21-dimensional snake robot-like manipulator, navigating a cluttered gas turbine for the purposes of inspection. Heuristic search based approaches are effective planning strategies for common manipulation domains. However, their performance on high dimensional systems is heavily reliant on the effectiveness of the action space and the heuristics chosen. The complex nature of our system, reachability constraints, and highly cluttered turbine environment renders naive choices of action spaces and heuristics ineffective. To this extent we have developed i) a methodology for dynamically generating actions based on online optimization that help the robot navigate narrow spaces, ii) a technique for lazily generating these computationally expensive optimization actions to effectively utilize resources, and iii) heuristics that reason about the homotopy classes induced by the blades of the turbine in the robot workspace and a Multi-Heuristic framework which guides the search along the relevant classes. The impact of our contributions is presented through an experimental study in simulation, where the 21 DOF manipulator navigates towards regions of inspection within a turbine.},
  archive   = {C_IROS},
  author    = {Muhammad Suhail Saleem and Raghav Sood and Sho Onodera and Rohit Arora and Hiroyuki Kanazawa and Maxim Likhachev},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636387},
  pages     = {8301-8308},
  title     = {Search-based path planning for a high dimensional manipulator in cluttered environments using optimization-based primitives},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient picking by considering simultaneous two-object
grasping. <em>IROS</em>, 8295–8300. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a motion planning algorithm that enables robots to efficiently pick up objects by considering simultaneous multi-object grasping. At the center of the algorithm is a cost function that helps to determine one of the following three grasping policies considering distance and friction constraints – Grasping a single object; Grasping two objects simultaneously; Grasping two object simultaneously after pushing one of the objects close to the other. After recognizing the object distributions on a table by using a depth camera and Mask R-CNN, our algorithm will select grasp policies from the three candidates considering the cost function, and plan a policy sequence that can most quickly finish picking all the objects using dynamic programming. Both simulation and real-world experiments are carried out to examine the performance of the proposed planner. Results show that the proposed method significantly improves the efficiency of robotic picking compared to conventional single-object-based methods.},
  archive   = {C_IROS},
  author    = {Takumi Sakamoto and Weiwei Wan and Takao Nishi and Kensuke Harada},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636727},
  pages     = {8295-8300},
  title     = {Efficient picking by considering simultaneous two-object grasping},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient task planning for mobile manipulation: A virtual
kinematic chain perspective. <em>IROS</em>, 8288–8294. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a Virtual Kinematic Chain (VKC) perspective, a simple yet effective method, to improve task planning efficacy for mobile manipulation. By consolidating the kinematics of the mobile base, the arm, and the object being manipulated collectively as a whole, this novel VKC perspective naturally defines abstract actions and eliminates unnecessary predicates in describing intermediate poses. As a result, these advantages simplify the design of the planning domain and significantly reduce the search space and branching factors in solving planning problems. In experiments, we implement a task planner using Planning Domain Definition Language (PDDL) with VKC. Compared with conventional domain definition, our VKC-based domain definition is more efficient in both planning time and memory. In addition, abstract actions perform better in producing feasible motion plans and trajectories. We further scale up the VKC-based task planner in complex mobile manipulation tasks. Taken together, these results demonstrate that task planning using VKC for mobile manipulation is not only natural and effective but also introduces new capabilities.},
  archive   = {C_IROS},
  author    = {Ziyuan Jiao and Zeyu Zhang and Weiqi Wang and David Han and Song-Chun Zhu and Yixin Zhu and Hangxin Liu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636554},
  pages     = {8288-8294},
  title     = {Efficient task planning for mobile manipulation: A virtual kinematic chain perspective},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Geometry-based two-contact inverse kinematic solution for
whole arm manipulation. <em>IROS</em>, 8269–8274. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Whole-arm manipulation (WAM) is often used to manipulate large and bulky objects. Contact point-based methods for generating the robot configurations for WAM mostly search for suitable contact points and configurations simultaneously. However, in order to learn good contact points, or allow an operator to select them, inverse kinematics (IK) solvers are needed which take such points along with a surface normal into account. Therefore, we propose a geometry-based IK method for WAM generate robot configurations in contact with two specific points. The method is compared against two other methods, one numerical method and a commercial IK solver. We show our method to be the fastest.},
  archive   = {C_IROS},
  author    = {Pascal Gliesche and Christian Kowalski and Max Pfingsthorn and Andreas Hein},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636664},
  pages     = {8269-8274},
  title     = {Geometry-based two-contact inverse kinematic solution for whole arm manipulation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TrajectoTree: Trajectory optimization meets tree search for
planning multi-contact dexterous manipulation. <em>IROS</em>, 8262–8268.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dexterous manipulation tasks often require contact switching, where fingers make and break contact with the object. We propose a method that plans trajectories for dexterous manipulation tasks involving contact switching using contact-implicit trajectory optimization (CITO) augmented with a high-level discrete contact sequence planner. We first use the high-level planner to find a sequence of finger contact switches given a desired object trajectory. With this contact sequence plan, we impose additional constraints in the CITO problem. We show that our method finds trajectories approximately 7 times faster than a general CITO baseline for a four-finger planar manipulation scenario. Furthermore, when executing the planned trajectories in a full dynamics simulator, we are able to more closely track the object pose trajectories planned by our method than those planned by the baselines.},
  archive   = {C_IROS},
  author    = {Claire Chen and Preston Culbertson and Marion Lepert and Mac Schwager and Jeannette Bohg},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636346},
  pages     = {8262-8268},
  title     = {TrajectoTree: Trajectory optimization meets tree search for planning multi-contact dexterous manipulation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An analysis on the modeling accuracy of industrial
manipulators with inherent joint elasticity. <em>IROS</em>, 8246–8253.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {High precision industrial applications call for equally precise functioning of industrial manipulators, which in turn requires accurate modeling of the manipulators. This paper carries out a detailed study on the modeling of industrial manipulators with elastic joints to improve their accuracy. In particular, the effect of adopting a simple harmonic drive (HD) model and ignoring a dynamic effect called low inertia coupling between the actuators and links on the model accuracy has been analyzed from a parameter estimation perspective. Since the aforementioned model characteristics have been generally ignored for high gear reduction ratios, this study is carried out with five different reduction ratios ranging from low to high, where three different models of a three-joints elastic manipulator are considered. The accuracy of the models is compared using the torque performance metrics of a predefined joint motion of the robot. Furthermore, the impact of the models with different accuracy is assessed by carrying out a state-of-the-art dynamic parameter estimation, and the resulting errors are compared to ascertain the merits of adopting a detailed elastic dynamic model of a manipulator.},
  archive   = {C_IROS},
  author    = {Rajesh Subburaman and Mariapaola D’Imperio and Jinoh Lee and Ferdinando Cannella},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636291},
  pages     = {8246-8253},
  title     = {An analysis on the modeling accuracy of industrial manipulators with inherent joint elasticity},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Novel variable stiffness spring mechanism: Modulating
stiffness independent of the energy stored by the spring. <em>IROS</em>,
8232–8237. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Theory suggests a linear relation between stiffness and the energy stored by a linear helical spring at constant deformation. This relation implies that increasing the stiffness of a helical spring upon deformation requires more energy at larger deformations. State-of-the-art variable stiffness spring actuators, used to drive robots and human assistive and augmentation devices, are characterized by a similar relation: increasing stiffness as the spring is deformed costs more energy as more energy is stored by the spring. This feature imposes an apparently fundamental limitation on variable stiffness spring actuation in demanding tasks, such as lifting more, jumping higher, or running faster, because, in all these tasks, the variable stiffness spring should store a considerable amount of energy and provide different stiffness to accommodate different weights in lifting, heights in jumping, and speeds in running. Here, we present an innovative variable stiffness spring design, where the energy cost of changing stiffness is independent of the energy stored by the spring. The key element of the new design is a novel floating spring which changes stiffness without changing the energy stored by the spring. Springs possessing the aforementioned feature could pave the way towards variable stiffness robot actuation and human augmentation using smaller motors and smaller battery packs.},
  archive   = {C_IROS},
  author    = {Sung Y. Kim and David J. Braun},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636339},
  pages     = {8232-8237},
  title     = {Novel variable stiffness spring mechanism: Modulating stiffness independent of the energy stored by the spring},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parallel variable stiffness actuators. <em>IROS</em>,
8225–8231. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce a new type of compliant actuator named the Parallel Variable Stiffness Actuator (PVSA) which consists of a variable stiffness spring placed in parallel with a direct-drive motor. Parallel variable stiffness actuators provide (i) high-fidelity force control and (ii) controllable energy storage, as they inherit the benefits of direct-drive motors and variable stiffness springs. We present a compact design of the PVSA using a flat motor connected to an adjustable mechanical advantage torsional spring. We show that this PVSA is (1) not subject to the fundamental force control bandwidth limitation of series elastic and variable stiffness actuators, and most notably, (2) enables resonant energy accumulation despite the limited deformation of the spring and the constrained motion of the load attached to the actuator. The latter differentiates parallel variable stiffness actuators from fixed-stiffness parallel elastic actuators. PVSAs may be used with smaller direct-drive motors to match the peak power of larger motors without compromising force control fidelity. PVSAs may be used to implement resonant forcing under joint angle limitations in walking, jumping, running, swimming robots, or robotic exoskeletons used to augmented human motion in the aforementioned tasks.},
  archive   = {C_IROS},
  author    = {Chase W. Mathews and David J. Braun},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636249},
  pages     = {8225-8231},
  title     = {Parallel variable stiffness actuators},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A compliant five-bar legged mechanism for heavy-load legged
robots by using magneto-rheological actuators. <em>IROS</em>, 8217–8224.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, a compliant five-bar leg mechanism is proposed, designed and manufactured for heavy-load legged robots, by using two magneto-rheological actuators (MRAs) that are capable of offering a maximal torque of 78Nm. To address the rate-dependent hysteresis of the MRA, a hybrid rate-dependent hysteresis model is derived based on the idea of mappings between different hysteresis loops. With integrating the classical Preisach model and the NARX neural network, the hybrid model is able to model hysteresis nonlinearity of the magneto-rheological clutch (MRC). It is then used to estimate and control the output torque of the MRA at the absent of external force/torque sensors. High fidelity force control and variable compliance of the leg mechanism are realized and validated in various experiments with using the MRAs.},
  archive   = {C_IROS},
  author    = {Guangzeng Chen and Jiangtao Ran and Chenguang Bai and Pengyu Jie and Yunjiang Lou},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636256},
  pages     = {8217-8224},
  title     = {A compliant five-bar legged mechanism for heavy-load legged robots by using magneto-rheological actuators},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A self-biasing shape memory alloy gripper for lightweight
applications. <em>IROS</em>, 8211–8216. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Shape Memory Alloys (SMAs) are a type of smart material that are commonly used in compact and lightweight linear actuators. Paired with compliant motion conversion mechanisms, these alloys can be transformed into lightweight grippers ideal for applications such as drone deliveries. This work proposes to use the inherent stiffness of a compliant mechanism with an SMA coil, to design and size a self-biasing SMA gripper. In this paper, with the help of 2.5D design, a functional radial 4-prong gripper is designed and fabricated. While only weighing 17g, it has a gripping force and stroke of 1.8N and 4mm, respectively. Along with this functional prototype, an analytical model and design framework is presented allowing for the design and sizing of similar self-biasing SMA actuators for other different light-weight applications.},
  archive   = {C_IROS},
  author    = {Sean Thomas and Gabriel Maquignaz and Adrien Thabuis and Yves Perriard},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636155},
  pages     = {8211-8216},
  title     = {A self-biasing shape memory alloy gripper for lightweight applications},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust feedback motion primitives for exploration of unknown
terrains. <em>IROS</em>, 8173–8179. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unknown properties of a robot’s environment are one of the sources of uncertainty in autonomous navigation. This uncertainty has to be accounted for when modelling robot dynamics. For ground vehicles in particular, terrain structure is one of the main environmental factors that can strongly influence the dynamics. Therefore, to ensure the ability of a robot to safely and efficiently navigate new environments, robust motion planning and control systems are needed. This paper investigates a data-driven approach to planning and control based on construction of robust motion primitives (MPs) and corresponding feedback rules that ensure a bounded error along the planned trajectory. The approach is tested in an exploration scenario in which a robot systematically inspects an area consisting of several terrain types with the aim of recognizing changes in dynamical properties, learning new dynamics models when such changes are detected and recording that information for future use. The advantage of incorporating the collected data into motion planning in multi-terrain environments is illustrated via simulation.},
  archive   = {C_IROS},
  author    = {Charles Chernik and Pouria Tajvar and Jana Tumova},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636521},
  pages     = {8173-8179},
  title     = {Robust feedback motion primitives for exploration of unknown terrains},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Area defense and surveillance on rectangular regions using
control barrier functions. <em>IROS</em>, 8166–8172. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A formulation of the area defense and surveillance problem for one intruder and one defense and surveillance robot and its corresponding solution using control barrier functions is presented. The defense robot must follow the intruder as it moves through a rectangular region in the plane, ensuring that the position of the intruder is also within a rectangular region attached to the surveillance robot. The proposed reactive and closed-form control laws depend on the positions of the robots, their maximum speeds, and the size of the rectangular regions. We show the application and effectiveness of our results in experiments with real robots.},
  archive   = {C_IROS},
  author    = {Luis Guerrero-Bonilla and Magnus Egerstedt and Dimos V. Dimarogonas},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636379},
  pages     = {8166-8172},
  title     = {Area defense and surveillance on rectangular regions using control barrier functions},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Control of spherical robots on uneven terrains.
<em>IROS</em>, 8159–8165. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hybrid robots incorporate the advantages of both aerial-only and terrestrial-only vehicles to achieve enhanced mobility and better energy efficiency. Among hybrid vehicles, spherical robots offer the best maneuverability. While operating on uneven surfaces is one of the main benefits of spherical robots, the current literature only covers control of these robots on flat surfaces. This work presents two control algorithms to track a desired trajectory and angular velocity of spherical robots on uneven terrains. The proposed control algorithms can be used when the terrain is known analytically or empirically (i.e., point cloud). By allowing the controller to use empirical information about the terrain profile, this work broadens the implementation of spherical robots in real applications.},
  archive   = {C_IROS},
  author    = {Sahand Sabet and Mohammad Poursina and Parviz E. Nikravesh},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636543},
  pages     = {8159-8165},
  title     = {Control of spherical robots on uneven terrains},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exponential stability of trajectory tracking control in the
orientation space utilizing unit quaternions. <em>IROS</em>, 8151–8158.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trajectory tracking in the orientation space utilizing unit quaternions yields non linear error dynamics as opposed to Cartesian position. In this work, we study trajectory tracking in the orientation space utilizing the most popular quaternion error representations and angular velocity errors. By selecting error functions carefully we show exponential convergence in a region of attraction containing large initial errors. We further show that under certain conditions frequently en-countered in practice, the formulation respecting the geometric characteristics of the quaternion manifold and its tangent space yields linear tracking dynamics allowing us to guarantee a desired tracking performance by gain selection without tuning. Simulation and experimental results are provided.},
  archive   = {C_IROS},
  author    = {Leonidas Koutras and Zoe Doulgeri},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636171},
  pages     = {8151-8158},
  title     = {Exponential stability of trajectory tracking control in the orientation space utilizing unit quaternions},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Crowd-aware robot navigation for pedestrians with multiple
collision avoidance strategies via map-based deep reinforcement
learning. <em>IROS</em>, 8144–8150. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It is challenging for a mobile robot to navigate through human crowds. Existing approaches usually assume that pedestrians follow a predefined collision avoidance strategy, like social force model (SFM) or optimal reciprocal collision avoidance (ORCA). However, their performances commonly need to be further improved for practical applications, where pedestrians follow multiple different collision avoidance strategies. In this paper, we propose a map-based deep reinforcement learning approach for crowd-aware robot navigation with various pedestrians. We use the sensor map to represent the environmental information around the robot, including its shape and observable appearances of obstacles. We also introduce the pedestrian map that specifies the movements of pedestrians around the robot. By applying both maps as inputs of the neural network, we show that a navigation policy can be trained to better interact with pedestrians following different collision avoidance strategies. We evaluate our approach under multiple scenarios both in the simulator and on an actual robot. The results show that our approach allows the robot to successfully interact with various pedestrians and outperforms compared methods in terms of the success rate.},
  archive   = {C_IROS},
  author    = {Shunyi Yao and Guangda Chen and Quecheng Qiu and Jun Ma and Xiaoping Chen and Jianmin Ji},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636579},
  pages     = {8144-8150},
  title     = {Crowd-aware robot navigation for pedestrians with multiple collision avoidance strategies via map-based deep reinforcement learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DRQN-based 3D obstacle avoidance with a limited field of
view. <em>IROS</em>, 8137–8143. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a map-based end-to-end DRL approach for three-dimensional (3D) obstacle avoidance in a partially observed environment, which is applied to achieve autonomous navigation for an indoor mobile robot using a depth camera with a narrow field of view. We first train a neural network with LSTM units in a 3D simulator of mobile robots to approximate the Q-value function in double DRQN. We also use a curriculum learning strategy to accelerate and stabilize the training process. Then we deploy the trained model to a real robot to perform 3D obstacle avoidance in its navigation. We evaluate the proposed approach both in the simulated environment and on a robot in the real world. The experimental results show that the approach is efficient and easy to be deployed, and it performs well for 3D obstacle avoidance with a narrow observation angle, which outperforms other existing DRL-based models by 15.5\% on success rate.},
  archive   = {C_IROS},
  author    = {Yu’an Chen and Guangda Chen and Lifan Pan and Jun Ma and Yu Zhang and Yanyong Zhang and Jianmin Ji},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635949},
  pages     = {8137-8143},
  title     = {DRQN-based 3D obstacle avoidance with a limited field of view},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comparative analysis of control barrier functions and
artificial potential fields for obstacle avoidance. <em>IROS</em>,
8129–8136. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Artificial potential fields (APFs) and their variants have been a staple for collision avoidance of mobile robots and manipulators for almost 40 years. Its model-independent nature, ease of implementation, and real-time performance have played a large role in its continued success over the years. Control barrier functions (CBFs), on the other hand, are a more recent development, commonly used to guarantee safety for nonlinear systems in real-time in the form of a filter on a nominal controller. In this paper, we address the connections between APFs and CBFs. At a theoretic level, we show that given a broad class of APFs, one can construct a CBF that guarantees safety. Additionally, we prove that CBFs obtained from these APFs have additional beneficial properties and can be applied to nonlinear systems. Practically, we compare the performance of APFs and CBFs in the context of obstacle avoidance on simple illustrative examples and for a quadrotor with unknown dynamics, both in simulation and on hardware using onboard sensing.},
  archive   = {C_IROS},
  author    = {Andrew Singletary and Karl Klingebiel and Joseph Bourne and Andrew Browning and Phil Tokumaru and Aaron Ames},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636670},
  pages     = {8129-8136},
  title     = {Comparative analysis of control barrier functions and artificial potential fields for obstacle avoidance},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Trajectory splitting: A distributed formulation for
collision avoiding trajectory optimization. <em>IROS</em>, 8113–8120.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Efficient trajectory optimization is essential for avoiding collisions in unstructured environments, but it remains challenging to have both speed and quality in the solutions. One reason is that second-order optimality requires calculating Hessian matrices that can grow with O(N 2 ) with the number of waypoints. Decreasing the waypoints can quadratically decrease computation time. Unfortunately, fewer waypoints result in lower quality trajectories that may not avoid the collision. To have both, dense waypoints and reduced computation time, we took inspiration from recent studies on consensus optimization and propose a distributed formulation of collocated trajectory optimization. It breaks a long trajectory into several segments, where each segment becomes a subproblem of a few waypoints. These subproblems are solved classically, but in parallel, and the solutions are fused into a single trajectory with a consensus constraint that enforces continuity of the segments through a consensus update. With this scheme, the quadratic complexity is distributed to each segment and enables solving for higher-quality trajectories with denser waypoints. Furthermore, the proposed formulation is amenable to using any existing trajectory optimizer for solving the subproblems. We compare the performance of our implementation of trajectory splitting against leading motion planning algorithms and demonstrate the improved computational efficiency of our method.},
  archive   = {C_IROS},
  author    = {Changhao Wang and Jeffrey Bingham and Masayoshi Tomizuka},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636846},
  pages     = {8113-8120},
  title     = {Trajectory splitting: A distributed formulation for collision avoiding trajectory optimization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Human-inspired multi-agent navigation using knowledge
distillation. <em>IROS</em>, 8105–8112. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite significant advancements in the field of multi-agent navigation, agents still lack the sophistication and intelligence that humans exhibit in multi-agent settings. In this paper, we propose a framework for learning a human-like general collision avoidance policy for agent-agent interactions in fully decentralized, multi-agent environments. Our approach uses knowledge distillation with reinforcement learning to shape the reward function based on expert policies extracted from human trajectory demonstrations through behavior cloning. We show that agents trained with our approach can take human-like trajectories in collision avoidance and goal-directed steering tasks not provided by the demonstrations, outperforming the experts as well as learning-based agents trained without knowledge distillation.},
  archive   = {C_IROS},
  author    = {Pei Xu and Ioannis Karamouzas},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636463},
  pages     = {8105-8112},
  title     = {Human-inspired multi-agent navigation using knowledge distillation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). V-RVO: Decentralized multi-agent collision avoidance using
voronoi diagrams and reciprocal velocity obstacles. <em>IROS</em>,
8097–8104. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a decentralized collision avoidance method for dense environments based on buffered Voronoi cells (BVC) and reciprocal velocity obstacles (RVO). Our approach is designed for scenarios with a large number of agents in close proximity and provides passive-friendly collision avoidance guarantees. The Voronoi cells are superimposed with RVO cones to compute a suitable direction for each agent, and we use that direction to compute a local collision-free path. Our approach can also satisfy double-integrator dynamics, and we use the properties of the BVC to formulate a simple, decentralized deadlock resolution strategy. We demonstrate the benefits of V-RVO in complex scenarios with tens of agents in close proximity. In practice, V-RVO’s performance is comparable to prior velocity-obstacle methods, and the collision avoidance behavior is significantly less conservative than ORCA.},
  archive   = {C_IROS},
  author    = {Senthil Hariharan Arul and Dinesh Manocha},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636618},
  pages     = {8097-8104},
  title     = {V-RVO: Decentralized multi-agent collision avoidance using voronoi diagrams and reciprocal velocity obstacles},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A high-accuracy fiducial marker with parallel lenticular
angle gauges. <em>IROS</em>, 8091–8096. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Lenticular angle gauge (LEAG) is a planar pattern that visualizes relative attitude by the position of the black line, which moves according to the viewing angle. The authors developed a new LEAG in which the direction of movement of the black line is 90 ◦ different from the previous LEAG, and used it to develop a non-square high-accuracy fiducial marker. The new marker realized accurate pose estimation with a position error of 0.15\% of the distance and an attitude error of 0.5°. This research contributes to the development of high-accuracy markers with a more flexible design. This paper describes the principle and behavior of the new LEAG, the design of the new fiducial marker, and the pose estimation algorithm, followed by the results of performance verification experiments.},
  archive   = {C_IROS},
  author    = {Hideyuki Tanaka and Kunihiro Ogata},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635988},
  pages     = {8091-8096},
  title     = {A high-accuracy fiducial marker with parallel lenticular angle gauges},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Score refinement for confidence-based 3D multi-object
tracking. <em>IROS</em>, 8083–8090. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-object tracking is a critical component in autonomous navigation, as it provides valuable information for decision-making. Many researchers tackled the 3D multi-object tracking task by filtering out the frame-by-frame 3D detections; however, their focus was mainly on finding useful features or proper matching metrics. Our work focuses on a neglected part of the tracking system: score refinement and tracklet termination. We show that manipulating the scores depending on time consistency while terminating the tracklets depending on the tracklet score improves tracking results. We do this by increasing the matched tracklets’ score with score update functions and decreasing the unmatched tracklets’ score. Compared to count-based methods, our method consistently produces better AMOTA and MOTA scores when utilizing various detectors and filtering algorithms on different datasets. The improvements in AMOTA score went up to 1.83 and 2.96 in MOTA. We also used our method as a late-fusion ensembling method, and it performed better than voting-based ensemble methods by a solid margin. It achieved an AMOTA score of 67.6 on nuScenes test evaluation, which is comparable to other state-of-the-art trackers. Code is publicly available at: https://github.com/cogsys-tuebingen/CBMOT.},
  archive   = {C_IROS},
  author    = {Nuri Benbarka and Jona Schröder and Andreas Zell},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636032},
  pages     = {8083-8090},
  title     = {Score refinement for confidence-based 3D multi-object tracking},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Model-free vehicle tracking and state estimation in point
cloud sequences. <em>IROS</em>, 8075–8082. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Estimating the states of surrounding traffic participants stays at the core of autonomous driving. In this paper, we study a novel setting of this problem: model-free single-object tracking (SOT), which takes the object state in the first frame as input, and jointly solves state estimation and tracking in subsequent frames. The main purpose for this new setting is to break the strong limitation of the popular &quot;detection and tracking&quot; scheme in multi-object tracking. Moreover, we notice that shape completion by overlaying the point clouds, which is a by-product of our proposed task, not only improves the performance of state estimation but also has numerous applications. As no benchmark for this task is available so far, we construct a new dataset LiDAR-SOT and corresponding evaluation protocols based on the Waymo Open dataset [29]. We then propose an optimization-based algorithm called SOTracker involving point cloud registration, vehicle shapes, correspondence, and motion priors. Our quantitative and qualitative results prove the effectiveness of our SOTracker and reveal the challenging cases for SOT in point clouds, including the sparsity of LiDAR data, abrupt motion variation, etc. Finally, we also explore how the proposed task and algorithm may benefit other autonomous driving applications, including simulating LiDAR scans, generating motion data, and annotating optical flow. The code and protocols for our benchmark and algorithm are available at https://github.com/TuSimple/LiDAR_SOT/. A video demonstration is at https://www.youtube.com/watch?v=BpHixKs91i8.},
  archive   = {C_IROS},
  author    = {Ziqi Pang and Zhichao Li and Naiyan Wang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636202},
  pages     = {8075-8082},
  title     = {Model-free vehicle tracking and state estimation in point cloud sequences},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BundleTrack: 6D pose tracking for novel objects without
instance or category-level 3D models. <em>IROS</em>, 8067–8074. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tracking the 6D pose of objects in video sequences is important for robot manipulation. Most prior efforts, however, often assume that the target object&#39;s CAD model, at least at a category-level, is available for offline training or during online template matching. This work proposes BundleTrack, a general framework for 6D pose tracking of novel objects, which does not depend upon 3D models, either at the instance or category-level. It leverages the complementary attributes of recent advances in deep learning for segmentation and robust feature extraction, as well as memory-augmented pose graph optimization for spatiotemporal consistency. This enables long-term, low-drift tracking under various challenging scenarios, including significant occlusions and object motions. Comprehensive experiments given two public benchmarks demonstrate that the proposed approach significantly outperforms state-of-art, category-level 6D tracking or dynamic SLAM methods. When compared against state-of-art methods that rely on an object instance CAD model, comparable performance is achieved, despite the proposed method’s reduced information requirements. An efficient implementation in CUDA provides a real-time performance of 10Hz for the entire framework. Code is available at: https://github.com/wenbowen123/BundleTrack},
  archive   = {C_IROS},
  author    = {Bowen Wen and Kostas Bekris},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635991},
  pages     = {8067-8074},
  title     = {BundleTrack: 6D pose tracking for novel objects without instance or category-level 3D models},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards robust human trajectory prediction in raw videos.
<em>IROS</em>, 8059–8066. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human trajectory prediction has received increased attention lately due to its importance in applications such as autonomous vehicles and indoor robots. However, most existing methods make predictions based on human-labeled trajectories and ignore the errors and noises in detection and tracking. In this paper, we study the problem of human trajectory forecasting in raw videos, and show that the prediction accuracy can be severely affected by various types of tracking errors. Accordingly, we propose a simple yet effective strategy to correct the tracking failures by enforcing prediction consistency over time. The proposed &quot;re-tracking&quot; algorithm can be applied to any existing tracking and prediction pipelines. Experiments on public benchmark datasets demonstrate that the proposed method can improve both tracking and prediction performance in challenging real-world scenarios. The code and data are available at https://git.io/retracking-prediction.},
  archive   = {C_IROS},
  author    = {Rui Yu and Zihan Zhou},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636831},
  pages     = {8059-8066},
  title     = {Towards robust human trajectory prediction in raw videos},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-variable state prediction: HMM based approach for
real-time trajectory prediction. <em>IROS</em>, 8052–8058. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Predicting the motion of observed entities benefits humans almost seamlessly. The same benefits can be proliferated to mobile autonomous systems if we have a reliable, real-time solution to predict the motion of any object of interest, be it the host’s own motion or that of an observed foreign object. In this work, a novel Multi-Variable State Prediction (MVSP) methodology is devised for real-time trajectory prediction. MVSP incorporates cascaded stages of HMM with Viterbi algorithm and probabilistic quantization for accurately predicting the motion characteristics of the moving object. The overall scheme is employed to predict the motion of moving objects in a 3D space. The proposed approach is verified on both synthetically generated data sequences and data-sets captured from real-life experiments. For a practical scenario, the experiments resulted in an RMS error of 0.6m for a predicted distance of ~18m demonstrating the effectiveness and accuracy of the proposed methodology.},
  archive   = {C_IROS},
  author    = {Ankit and Karthik Narayanan and Dibyendu Ghosh and Vinayak Honkote and Ganeshram Nandakumar},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636556},
  pages     = {8052-8058},
  title     = {Multi-variable state prediction: HMM based approach for real-time trajectory prediction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving object permanence using agent actions and
reasoning. <em>IROS</em>, 8044–8051. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Object permanence in psychology means knowing that objects still exist even if they are no longer visible. It is a crucial concept for robots to operate autonomously in uncontrolled environments. Existing approaches learn object permanence from low-level perception, but perform poorly on more complex scenarios, like when objects are contained and carried by others. Knowledge about manipulation actions performed on an object prior to its disappearance allows us to reason about its location, e.g., that the object has been placed in a carrier. In this paper we argue that object permanence can be improved when the robot uses knowledge about executed actions and describe an approach to infer hidden object states from agent actions. We show that considering agent actions not only improves rule-based reasoning models but also purely neural approaches, showing its general applicability. Then, we conduct quantitative experiments on a snitch localization task using a dataset of 1,371 synthesized videos, where we compare the performance of different object permanence models with and without action annotations. We demonstrate that models with action annotations can significantly increase performance of both neural and rule-based approaches. Finally, we evaluate the usability of our approach in real-world applications by conducting qualitative experiments with two Universal Robots (UR5 and UR16e) in both lab and industrial settings. The robots complete benchmark tasks for a gearbox assembly and demonstrate the object permanence capabilities with real sensor data in an industrial environment.},
  archive   = {C_IROS},
  author    = {Ying Siu Liang and Chen Zhang and Dongkyu Choi and Kenneth Kwok},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636502},
  pages     = {8044-8051},
  title     = {Improving object permanence using agent actions and reasoning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Moving-platform pose estimation for cable-driven parallel
robots. <em>IROS</em>, 8036–8043. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cable-Driven Parallel Robots (CDPRs) are parallel robots with rigid links replaced by cables. As for most parallel robots the determination of the analytical solutions to the direct geometrico-static model (DGSM) is a difficult task that is often not feasible online. However, the knowledge of the moving-platform (MP) pose is necessary in order to control the CDPR, e.g. with visual servoing. When the MP pose measurement is not available, an estimation can be sufficient. This paper compares three estimation methods: (a) control-based; (b) image-based; and (c) model-based. The three methods are implemented experimentally with an open-loop velocity controller and a closed-loop visual servoing controller. Overall, very good results are shown with model-based and control-based methods for both controllers. Finally, it is shown that the visual servoing controller leads to a better accuracy of the robot than the velocity controller.},
  archive   = {C_IROS},
  author    = {Zane Zake and François Chaumette and Nicolò Pedemonte and Stéphane Caro},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636284},
  pages     = {8036-8043},
  title     = {Moving-platform pose estimation for cable-driven parallel robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel model-based robust super-twisting sliding mode
control of PKMs: Design and real-time experiments. <em>IROS</em>,
8029–8035. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, a new robust model-based super-twisting algorithm is proposed as a control solution for parallel kinematic manipulators (PKMs). The conventional super-twisting algorithm for robot manipulators has the structure of a computed-torque control which can be sensitive to measurement noise. This issue may deteriorate the dynamic performance of the manipulator and reduce its robustness towards changes in the operating conditions. The proposed approach, relying on the desired trajectory, is more computationally efficient and more robust. It includes a feedforward dynamic compensator, the super-twisting feedback control, and a feedback stabilizing term. As a validation, real-time experiments have been conducted on a 5-DOF redundantly actuated PKM. Several scenarios have been tested including nominal case and the robustness towards speed variations. The relevance of the proposed control solution is proved through the improvement of the tracking performance at different dynamic operating conditions.},
  archive   = {C_IROS},
  author    = {Hussein Saied and Ahmed Chemori and Maher El Rafei and Clovis Francis},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636168},
  pages     = {8029-8035},
  title     = {A novel model-based robust super-twisting sliding mode control of PKMs: Design and real-time experiments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel 2-SUR 6-DOF parallel manipulator actuated by
spherical motion generators. <em>IROS</em>, 8022–8028. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A novel 6-DOF parallel manipulator with two spherical-universal-revolute limbs is proposed in this work. Compared with general 6-DOF parallel manipulators of six kinematic limbs, this new manipulator actuated by spherical motion generators has only two limbs, which brings kinematic advantages such as small footprint and large workspace. The inverse position problem of the manipulator is solved by an analytical approach, upon which velocity equations are formulated. Kinematic performance including workspace and also manipulability are calculated to show the advantages of the new design.},
  archive   = {C_IROS},
  author    = {Kun Wang and Xiaoyong Wu and Yujin Wang and Bo Li and Bo Yuan and Shaoping Bai},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636689},
  pages     = {8022-8028},
  title     = {A novel 2-SUR 6-DOF parallel manipulator actuated by spherical motion generators},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FloMo: Tractable motion prediction with normalizing flows.
<em>IROS</em>, 7977–7984. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The future motion of traffic participants is inherently uncertain. To plan safely, therefore, an autonomous agent must take into account multiple possible trajectory outcomes and prioritize them. Recently, this problem has been addressed with generative neural networks. However, most generative models either do not learn the true underlying trajectory distribution reliably, or do not allow predictions to be associated with likelihoods. In our work, we model motion prediction directly as a density estimation problem with a normalizing flow between a noise distribution and the future motion distribution. Our model, named FloMo, allows likelihoods to be computed in a single network pass and can be trained directly with maximum likelihood estimation. Furthermore, we propose a method to stabilize training flows on trajectory datasets and a new data augmentation transformation that improves the performance and generalization of our model. Our method achieves state-of-the-art performance on three popular prediction datasets, with a significant gap to most competing models.},
  archive   = {C_IROS},
  author    = {Christoph Schöller and Alois Knoll},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636445},
  pages     = {7977-7984},
  title     = {FloMo: Tractable motion prediction with normalizing flows},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Kohonen self-organizing map based route planning: A revisit.
<em>IROS</em>, 7969–7976. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we revisit the long-standing Traveling Salesman Problem (TSP) and focus on the challenging, yet practical route planning problem with limited computational resources. We make contributions to TSP, one of the most famous NP-hard problems by providing a new improved approximate solution, which we term TOpology Preserving Self-Organizing Map (TOPSOM). TOPSOM well preserves the topology of the node map to be traversed by maintaining the continuity of nodes and the distances between them. In addition, to satisfy the requirements of convex hull, we design an elastic competitive Hebbian learning rule. TOPSOM can solve large-scale TSPs with high precision and high efficiency with limited computational costs. Extensive experimental results on mainstream route planning benchmarks including TSPLIB and National TSP’s show that our method consistently outperforms baseline methods, by up to 7.7\% in terms of the Percent Deviation of Mean solution to best known solution.},
  archive   = {C_IROS},
  author    = {Qingshu Guan and Xiaopeng Hong and Wei Ke and Liangfei Zhang and Guanghui Sun and Yihong Gong},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636025},
  pages     = {7969-7976},
  title     = {Kohonen self-organizing map based route planning: A revisit},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). A fast algorithm for stochastic orienteering with chance
constraints. <em>IROS</em>, 7961–7968. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the Stochastic Orienteering Problem with random traversal time for edges. In this scenario the length of the path is a random variable and we consider a formulation with chance constraints, i.e., a bound on the probability that the length of the path exceeds the allotted budget. Our proposed solution casts the problem as an instance of a suitably defined Constrained Markov Decision Process and uses a Lagrangian formulation to solve it. In particular, exploiting some structural properties of the associated decision process we can solve the Markov Decision Process using a Lagrangian approach and efficiently determine the optimal Lagrange multiplier. Our method is experimentally evaluated and demonstrated to be significantly faster than previous solutions using a linear programming approach to solve the Stochastic Orienteering Problem with chance constraints.},
  archive   = {C_IROS},
  author    = {Thomas C. Thayer and Stefano Carpin},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636669},
  pages     = {7961-7968},
  title     = {A fast algorithm for stochastic orienteering with chance constraints},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Search-based planning with learned behaviors for navigation
among pedestrians. <em>IROS</em>, 7953–7960. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Agent control among pedestrians is often approached in one of the three following ways: using predefined behaviors for agent navigation, learning navigation behaviors from data, or search-based planning on a graph where each edge is a feasible action chosen from a set of predefined actions. While the first approach often produces natural looking motions and the second learns and utilizes complex interactions with pedestrians, both lack global reasoning about how to sequence these behaviors to achieve the overall goal. The third approach, namely search-based planning, does incorporate global reasoning but relies on predefined actions that do not involve any interactions with pedestrians or assume predefined interactions that cannot model complex interactions. This is a significant drawback since many situations such as going through a doorway blocked by other people require complex interactions in order to avoid highly suboptimal behaviors or not being able to get to the goal at all. To this end, we propose a search-based planning framework that constructs and searches a graph wherein each edge can be either a predefined action or a learned behavior. We further extend it to deal with the uncertainty arising from introducing learned behaviors. We present the algorithm, go over its theoretical analysis, and present experimental results.},
  archive   = {C_IROS},
  author    = {Ishani Chatterjee and Yash Oza and Maxim Likhachev and Manuela Veloso},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636405},
  pages     = {7953-7960},
  title     = {Search-based planning with learned behaviors for navigation among pedestrians},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparsification for fast optimal multi-robot path planning in
lazy compilation schemes. <em>IROS</em>, 7931–7938. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Path planning for multiple robots (MRPP) represents a task of finding non-colliding paths for robots via which they can navigate from their initial positions to specified goal positions. The problem is often modeled using undirected graphs where robots move between vertices across edges while no two robots can simultaneously occupy the same vertex nor can traverse an edge in opposite directions. Contemporary optimal solving algorithms include dedicated search-based methods, that solve the problem directly, and compilation-based algorithms that reduce MRPP to a different formalism for which an efficient solver exists, such as constraint programming (CP), mixed integer linear programming (MILP), or Boolean satisfiability (SAT). In this paper, we enhance existing SAT-based algorithm for MRPP via sparsification of the set of candidate paths for each robot from which the target Boolean encoding is derived. Suggested sparsification of the set of paths led to a smaller target Boolean formulae that can be constructed and solved faster while optimality guarantees of the approach have been kept.},
  archive   = {C_IROS},
  author    = {Pavel Surynek},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636296},
  pages     = {7931-7938},
  title     = {Sparsification for fast optimal multi-robot path planning in lazy compilation schemes},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Capacitated vehicle routing with target geometric
constraints. <em>IROS</em>, 7925–7930. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate the capacitated vehicle routing problem (CVRP) under a robotics context, where a vehicle with limited payload must complete delivery (or pickup) tasks to serve a set of geographically distributed customers with varying demands. In classical CVRP, a customer location is modeled as a point. In many robotics applications, however, it is more appropriate to model such &quot;customer locations&quot; as 2D regions. For example, in aerial delivery, a drone may drop a package anywhere in a customer’s lot. This yields the problem of CVRG (Capacitated Vehicle Routing with Target Geometric Constraints). Computationally, CVRP is already strongly NPhard; CVRG is therefore more challenging. Nevertheless, we develop fast algorithms for CVRG, capable of computing high quality solutions for hundreds of regions. Our algorithmic solution is guaranteed to be optimal when customer regions are convex. Numerical evaluations show that our proposed methods significantly outperform greedy best-first approaches. Comprehensive simulation studies confirm the effectiveness of our methods.},
  archive   = {C_IROS},
  author    = {Kai Gao and Jingjin Yu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636123},
  pages     = {7925-7930},
  title     = {Capacitated vehicle routing with target geometric constraints},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DistillPose: Lightweight camera localization using auxiliary
learning. <em>IROS</em>, 7919–7924. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a lightweight retrieval-based pipeline to predict 6DOF camera poses from RGB images. Our pipeline uses a convolutional neural network (CNN) to encode a query image as a feature vector. A nearest neighbor lookup finds the pose-wise nearest database image. A siamese convolutional neural network regresses the relative pose from the nearest neighboring database image to the query image. The relative pose is then applied to the nearest neighboring absolute pose to obtain the query image’s final absolute pose prediction. Our model is a distilled version of NN-Net [1] that reduces its parameters by 98.87\%, information retrieval feature vector size by 87.5\%, and inference time by 89.18\% without a significant decrease in localization accuracy.},
  archive   = {C_IROS},
  author    = {Yehya Abouelnaga and Mai Bui and Slobodan Ilic},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635870},
  pages     = {7919-7924},
  title     = {DistillPose: Lightweight camera localization using auxiliary learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Trajectory generation in new environments from past
experiences. <em>IROS</em>, 7911–7918. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Being able to safely operate for extended periods of time in dynamic environments is a critical capability for autonomous systems. This generally involves the prediction and understanding of motion patterns of dynamic entities, such as vehicles and people, in the surroundings. Many motion prediction methods in the literature implicitly account for environmental factors by learning on observed motion in a fixed environment, and are designed to make predictions in the same environment. In this paper, we address the problem of generating likely motion trajectories for novel environments, represented as occupancy grid maps, where motion has not been observed. We introduce the Occupancy-Conditional Trajectory Network (OTNet) framework, capable of transferring the previously observed motion patterns in known environments to new environments. OTNet provides a functional representation for motion trajectories and utilises neural networks to learn occupancy-conditional distributions over the function parameters. We empirically demonstrate our method’s ability to generate complex multi-modal trajectory patterns in both simulated and real-world environments.},
  archive   = {C_IROS},
  author    = {Weiming Zhi and Tin Lai and Lionel Ott and Fabio Ramos},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636231},
  pages     = {7911-7918},
  title     = {Trajectory generation in new environments from past experiences},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-critical learning of influencing factors for trajectory
prediction using gated graph convolutional network. <em>IROS</em>,
7904–7910. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Forecasting future trajectories of multiple pedestrians in a crowded environment is a challenging problem due to the complex interactions among the pedestrians. The interactions can be asymmetric and their influences may vary over time. Moreover, each pedestrian can exhibit different behavior at any given time and context and thus they may have multiple future possible trajectories. In this work, we present a Gated Graph Convolutional Network (GatedGCN) based trajectory prediction model that explicitly deal with the asymmetric influences among the adjacent pedestrians through edge-wise gating mechanism. Through GatedGCN only, an overall average improvement of 16\% and 18\% was achieved on the two performance metrics over the state-of-the-art trajectory forecasting methods. Next, we tackle the problem of learning multi-modal distributions of each pedestrian trajectory using variational auto-encoders (VAEs). However, trajectories sampled from the learned distribution usually ignore the factors affecting the pedestrian motion such as collision avoidance and the target destination. While many of the existing approaches focus on learning such factors during the trajectory encoding process, we proposed a novel self-critical learning approach based on Actor-Critic framework to learn such factors during the trajectory generation process. We empirically show that our method creates fewer number of collisions than the existing methods on popular trajectory forecasting benchmarks.},
  archive   = {C_IROS},
  author    = {Niraj Bhujel and Yau Wei Yun and Han Wang and Vijay Prakash Dwivedi},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636641},
  pages     = {7904-7910},
  title     = {Self-critical learning of influencing factors for trajectory prediction using gated graph convolutional network},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). State-only imitation learning for dexterous manipulation.
<em>IROS</em>, 7865–7871. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern model-free reinforcement learning methods have recently demonstrated impressive results on a number of problems. However, complex domains like dexterous manipulation remain a challenge due to the high sample complexity. To address this, current approaches employ expert demonstrations in the form of state-action pairs, which are difficult to obtain for real-world settings such as learning from videos. In this paper, we move toward a more realistic setting and explore state-only imitation learning. To tackle this setting, we train an inverse dynamics model and use it to predict actions for state-only demonstrations. The inverse dynamics model and the policy are trained jointly. Our method performs on par with state-action approaches and considerably outperforms RL alone. By not relying on expert actions, we are able to learn from demonstrations with different dynamics, morphologies, and objects. Videos available on the ${\text{project page}}$.},
  archive   = {C_IROS},
  author    = {Ilija Radosavovic and Xiaolong Wang and Lerrel Pinto and Jitendra Malik},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636557},
  pages     = {7865-7871},
  title     = {State-only imitation learning for dexterous manipulation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust behavior cloning with adversarial demonstration
detection. <em>IROS</em>, 7858–7864. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Imitation learning (IL) frameworks in robotics typically assume that a domain expert&#39;s demonstration always contains a correct way of doing the task. Despite its theoretical convenience, this assumption has limited practical values for an IL-powered robot in real world. There are many reasons for an expert in the real world to provide demonstrations that may contain incorrect or potentially unsafe way of doing a task. In order for IL-powered robots to work in the real world, IL frameworks need to detect such adversarial demonstrations and not learn from them. This paper proposes an IL framework that can autonomously detect and remove adversarial demonstrations, if they exist in the demonstration set, as it directly learns a task policy from the expert. The proposed framework that we term Robust Maximum Entropy behavior cloning (R-MaxEnt) learns a stochastic model that maps states to actions. In doing so, R-MaxEnt solves a minmax problem that leverages the entropy of the model to assign weights to different demonstrations while assigning poor weights to adversarial samples. Our empirical results show that R-MaxEnt outperforms the existing IL approaches in both real and simulated robotics tasks.},
  archive   = {C_IROS},
  author    = {Mostafa Hussein and Brendan Crowe and Madison Clark-Turner and Paul Gesel and Marek Petrik and Momotaz Begum},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636203},
  pages     = {7858-7864},
  title     = {Robust behavior cloning with adversarial demonstration detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive t-momentum-based optimization for unknown ratio of
outliers in amateur data in imitation learning. <em>IROS</em>,
7851–7857. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Behavioral cloning (BC) bears a high potential for safe and direct transfer of human skills to robots. However, demonstrations performed by human operators often contain noise or imperfect behaviors that can affect the efficiency of the imitator if left unchecked. In order to allow the imitators to effectively learn from imperfect demonstrations, we propose to employ the robust t-momentum optimization algorithm. This algorithm builds on the Student&#39;s t-distribution in order to deal with heavy-tailed data and reduce the effect of outlying observations. We extend the t-momentum algorithm to allow for an adaptive and automatic robustness and show empirically how the algorithm can be used to produce robust BC imitators against datasets with unknown heaviness. Indeed, the imitators trained with the t-momentum-based Adam optimizers displayed robustness to imperfect demonstrations on two different manipulation tasks with different robots and revealed the capability to take advantage of the additional data while reducing the adverse effect of non-optimal behaviors.},
  archive   = {C_IROS},
  author    = {Wendyam Eric Lionel Ilboudo and Taisuke Kobayashi and Kenji Sugimoto},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636413},
  pages     = {7851-7857},
  title     = {Adaptive t-momentum-based optimization for unknown ratio of outliers in amateur data in imitation learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Seeing all the angles: Learning multiview manipulation
policies for contact-rich tasks from demonstrations. <em>IROS</em>,
7843–7850. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learned visuomotor policies have shown considerable success as an alternative to traditional, hand-crafted frameworks for robotic manipulation. Surprisingly, an extension of these methods to the multiview domain is relatively unexplored. A successful multiview policy could be deployed on a mobile manipulation platform, allowing the robot to complete a task regardless of its view of the scene. In this work, we demonstrate that a multiview policy can be found through imitation learning by collecting data from a variety of viewpoints. We illustrate the general applicability of the method by learning to complete several challenging multi-stage and contact-rich tasks, from numerous viewpoints, both in a simulated environment and on a real mobile manipulation platform. Furthermore, we analyze our policies to determine the benefits of learning from multiview data compared to learning with data collected from a fixed perspective. We show that learning from multiview data results in little, if any, penalty to performance for a fixed-view task compared to learning with an equivalent amount of fixed-view data. Finally, we examine the visual features learned by the multiview and fixed-view policies. Our results indicate that multiview policies implicitly learn to identify spatially correlated features.},
  archive   = {C_IROS},
  author    = {Trevor Ablett and Yifan Zhai and Jonathan Kelly},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636440},
  pages     = {7843-7850},
  title     = {Seeing all the angles: Learning multiview manipulation policies for contact-rich tasks from demonstrations},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Contrastively learning visual attention as affordance cues
from demonstrations for robotic grasping. <em>IROS</em>, 7835–7842. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Conventional works that learn grasping affordance from demonstrations need to explicitly predict grasping configurations, such as gripper approaching angles or grasping preshapes. Classic motion planners could then sample trajectories by using such predicted configurations. In this work, our goal is instead to fill the gap between affordance discovery and affordance-based policy learning by integrating the two objectives in an end-to-end imitation learning framework based on deep neural networks. From a psychological perspective, there is a close association between attention and affordance. Therefore, with an end-to-end neural network, we propose to learn affordance cues as visual attention that serves as a useful indicating signal of how a demonstrator accomplishes tasks, instead of explicitly modeling affordances. To achieve this, we propose a contrastive learning framework that consists of a Siamese encoder and a trajectory decoder. We further introduce a coupled triplet loss to encourage the discovered affordance cues to be more affordance-relevant. Our experimental results demonstrate that our model with the coupled triplet loss achieves the highest grasping success rate in a simulated robot environment. Our project website can be accessed at 1 .},
  archive   = {C_IROS},
  author    = {Yantian Zha and Siddhant Bhambri and Lin Guan},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636760},
  pages     = {7835-7842},
  title     = {Contrastively learning visual attention as affordance cues from demonstrations for robotic grasping},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning by watching: Physical imitation of manipulation
skills from human videos. <em>IROS</em>, 7827–7834. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning from visual data opens the potential to accrue a large range of manipulation behaviors by leveraging human demonstrations without specifying each of them mathe-matically, but rather through natural task specification. In this paper, we present Learning by Watching (LbW), an algorithmic framework for policy learning through imitation from a single video specifying the task. The key insights of our method are two-fold. First, since the human arms may not have the same morphology as robot arms, our framework learns unsupervised human to robot translation to overcome the morphology mis-match issue. Second, to capture the details in salient regions that are crucial for learning state representations, our model performs unsupervised keypoint detection on the translated robot videos. The detected keypoints form a structured representation that contains semantically meaningful information and can be used directly for computing reward and policy learning. We evaluate the effectiveness of our LbW framework on five robot manipulation tasks, including reaching, pushing, sliding, coffee making, and drawer closing. Extensive experimental evaluations demonstrate that our method performs favorably against the state-of-the-art approaches. More results and analysis are available at pair.toronto.edu/lbw-kp/.},
  archive   = {C_IROS},
  author    = {Haoyu Xiong and Quanzhou Li and Yun-Chun Chen and Homanga Bharadhwaj and Samarth Sinha and Animesh Garg},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636080},
  pages     = {7827-7834},
  title     = {Learning by watching: Physical imitation of manipulation skills from human videos},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel curved gaussian mixture model and its application in
motion skill encoding. <em>IROS</em>, 7813–7818. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The purpose of this paper is to present a novel curved Gaussian Mixture Model (CGMM) and to study the application of it in motion skill encoding. Primarily, Gaussian mixture model (GMM) has been widely applied on many occasions when a probability density function is needed to approximate a complex probability distribution. However, GMM cannot efficiently approach highly non-linear distributions. Thus, the proposed novel CGMM, as a weighted mixture of curved Gaussian models (CGM), is structured with non-linear transfers, which reshapes the flat GMM into a geo-metrically curved one. As a consequence, CGMM has more freedoms and flexibilities than the flat GMM so a CGMM requires fewer number of components in fitting highly non-linear motion trajectories. Moreover, we derive a dedicated iterative parameter estimation algorithm for the CGMM based on maximum likelihood estimation (MLE) theory. To evaluate the performance of the CGMM and its parameter estimation algorithm, a series of quantitative experiments are carried out. We first test the model performance in the data fitting task with the generated synthetic data. Then a motion skill encoding test is carried out on a human motion trajectory dataset built by a Virtual Reality (VR) based motion tracking system. The empirical results support that CGMM outperforms state-of-the-arts in the model performance test. Meanwhile, CGMM has a significant improvement in encoding high dimensional non-linear trajectory data compared to the GMM in motion skill encoding test with its dedicated parameter estimation algorithm.},
  archive   = {C_IROS},
  author    = {Disi Chen and Gongfa Li and Dalin Zhou and Zhaojie Ju},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636121},
  pages     = {7813-7818},
  title     = {A novel curved gaussian mixture model and its application in motion skill encoding},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning from successful and failed demonstrations via
optimization. <em>IROS</em>, 7807–7812. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning from Demonstration (LfD) is a popular approach that allows humans to teach robots new skills by showing the correct way(s) of performing the desired skill. Human-provided demonstrations, however, are not always optimal and the teacher usually addresses this issue by discarding or replacing sub-optimal (noisy or faulty) demonstrations. We propose a novel LfD representation that learns from both successful and failed demonstrations of a skill. Our approach encodes the two subsets of captured demonstrations (labeled by the teacher) into a statistical skill model, constructs a set of quadratic costs, and finds an optimal reproduction of the skill under novel problem conditions (i.e. constraints). The optimal reproduction balances convergence towards successful examples and divergence from failed examples. We evaluate our approach through several 2D and 3D experiments in real-world using a UR5e manipulator arm and also show that it can reproduce a skill from only failed demonstrations. The benefits of exploiting both failed and successful demonstrations are shown through comparison with two existing LfD approaches. We also compare our approach against an existing skill refinement method and show its capabilities in a multi-coordinate setting.},
  archive   = {C_IROS},
  author    = {Brendan Hertel and S. Reza Ahmadzadeh},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636679},
  pages     = {7807-7812},
  title     = {Learning from successful and failed demonstrations via optimization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to optimize control policies and evaluate
reproduction performance from human demonstrations. <em>IROS</em>,
7800–7806. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We are interested in learning from demonstration (LfD) that can both learn and execute a trajectory and evaluate the quality of a previously unseen trajectory in the domain of assistive robotics. To this end, we propose a novel continuous inverse optimal control (IOC) formulation that simultaneously learns an optimal time-invariant controller and an evaluation metric from human demonstrations. We assume that the expert’s objective function is a weighted combination of physically meaningful basis objective functions. The evaluation metric is derived from the learned expert’s objective function. The benefit of this approach is twofold: 1) the controller can be optimized with respect to the learned evaluation metric and subject to the robot’s dynamic limitations and 2) the evaluation metric can evaluate the quality of a demonstrated trajectory. We validate our approach with two experiments in a robot guided therapy setting: 1) evaluating demonstrated exercises with the learned metric and 2) reproducing both unconstrained trajectories and trajectories subject to the robot’s dynamic constraints.},
  archive   = {C_IROS},
  author    = {Paul Gesel and Dain LaRoche and Sajay Arthanat and Momotaz Begum},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636454},
  pages     = {7800-7806},
  title     = {Learning to optimize control policies and evaluate reproduction performance from human demonstrations},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards coordinated robot motions: End-to-end learning of
motion policies on transform trees. <em>IROS</em>, 7792–7799. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generating robot motion that fulfills multiple tasks simultaneously is challenging due to the geometric constraints imposed on the robot. In this paper, we propose to solve multi-task problems through learning structured policies from human demonstrations. Our structured policy is inspired by RMPflow, a framework for combining subtask policies on different spaces. The policy structure provides the user an interface to 1) specifying the spaces that are directly relevant to the completion of the tasks, and 2) designing policies for certain tasks that do not need to be learned. We derive an end-to-end learning objective that is suitable for the multi-task problem, emphasizing the distance between generated motions and demonstrations measured on task spaces. Furthermore, the motion generated from the learned policy class is guaranteed to be stable. We validate the effectiveness of our proposed learning framework through qualitative and quantitative evaluations on three robotic tasks on a 7-DOF Rethink Sawyer robot.},
  archive   = {C_IROS},
  author    = {M. Asif Rana and Anqi Li and Dieter Fox and Sonia Chernova and Byron Boots and Nathan Ratliff},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636097},
  pages     = {7792-7799},
  title     = {Towards coordinated robot motions: End-to-end learning of motion policies on transform trees},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A marginal log-likelihood approach for the estimation of
discount factors of multiple experts in inverse reinforcement learning.
<em>IROS</em>, 7786–7791. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We focus on multiple experts performing a task in a Markov decision process (MDP) environment. A probabilistic assignment of trajectories to clusters and a mathematical framework which leverages the utility function are employed to jointly estimate the discount factor and reward. We treat the number of clusters as a hyperparameter which can be &quot;freely&quot; selected by the problem designer. In this work, we specifically treat the cluster of trajectories as a latent variable in the adapted maximum entropy inverse reinforcement learning (IRL) formulation; the introduction of this latent variable adds to the complexity of the IRL problem. To manage such complexity, we optimize a marginal log-likelihood function via Expectation Maximization. To test our approach, we have utilized behavioral data generated from three MDP environments. Experimental works show that our approach is promising towards the estimation of discount factors in IRL for non-interacting multiple experts.},
  archive   = {C_IROS},
  author    = {Babatunde H. Giwa and Chi-Guhn Lee},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636479},
  pages     = {7786-7791},
  title     = {A marginal log-likelihood approach for the estimation of discount factors of multiple experts in inverse reinforcement learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ILoSA: Interactive learning of stiffness and attractors.
<em>IROS</em>, 7778–7785. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Teaching robots how to apply forces according to our preferences is still an open challenge that has to be tackled from multiple engineering perspectives. This paper studies how to learn variable impedance policies where both the Cartesian stiffness and the attractor can be learned from human demonstrations and corrections with a user-friendly interface. The presented framework, named ILoSA, uses Gaussian Processes for policy learning, identifying regions of uncertainty and allowing interactive corrections, stiffness modulation and active disturbance rejection. The experimental evaluation of the framework is carried out on a Franka-Emika Panda in four separate cases with unique force interaction properties: 1) pulling a plug wherein a sudden force discontinuity occurs upon successful removal of the plug, 2) pushing a box where a sustained force is required to keep the robot in motion, 3) wiping a whiteboard in which the force is applied perpendicular to the direction of movement, and 4) inserting a plug to verify the usability for precision-critical tasks in an experimental validation performed with non-expert users.},
  archive   = {C_IROS},
  author    = {Giovanni Franzese and Anna Mészáros and Luka Peternel and Jens Kober},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636710},
  pages     = {7778-7785},
  title     = {ILoSA: Interactive learning of stiffness and attractors},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning forceful manipulation skills from multi-modal human
demonstrations. <em>IROS</em>, 7770–7777. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning from Demonstration (LfD) provides an intuitive and fast approach to program robotic manipulators. Task parameterized representations allow easy adaptation to new scenes and online observations. However, this approach has been limited to pose-only demonstrations and thus only skills with spatial and temporal features. In this work, we extend the LfD framework to address forceful manipulation skills, which are of great importance for industrial processes such as assembly. For such skills, multi-modal demonstrations including robot end-effector poses, force and torque readings, and operation scene are essential. Our objective is to reproduce such skills reliably according to the demonstrated pose and force profiles within different scenes. The proposed method combines our previous work on task-parameterized optimization and attractor-based impedance control. The learned skill model consists of (i) the attractor model that unifies the pose and force features, and (ii) the stiffness model that optimizes the stiffness for different stages of the skill. Furthermore, an online execution algorithm is proposed to adapt the skill execution to real-time observations of robot poses, measured forces, and changed scenes. We validate this method rigorously on a 7-DoF robot arm over several steps of an E-bike motor assembly process, which require different types of forceful interaction such as insertion, sliding and twisting.},
  archive   = {C_IROS},
  author    = {An T. Le and Meng Guo and Niels van Duijkeren and Leonel Rozo and Robert Krug and Andras G. Kupcsik and Mathias Bürger},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636828},
  pages     = {7770-7777},
  title     = {Learning forceful manipulation skills from multi-modal human demonstrations},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive hyperparameter tuning for black-box LiDAR odometry.
<em>IROS</em>, 7708–7714. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study proposes an adaptive data-driven hy-perparameter tuning framework for black-box 3D LiDAR odometry algorithms. The proposed framework comprises offline parameter-error function modeling and online adaptive parameter selection. In the offline step, we run the odometry estimation algorithm for tuning with different parameters and environments and evaluate the accuracy of the estimated trajectories to build a surrogate function that predicts the trajectory estimation error for the given parameters and environments. Subsequently, we select the parameter set that is expected to result in good accuracy in the given environment based on trajectory error prediction with the surrogate function. The proposed framework does not require detailed information on the inner working of the algorithm to be tuned, and improves its accuracy by adaptively optimizing the parameter set. We first demonstrate the role of the proposed framework in improving the accuracy of odometry estimation across different environments with a simulation-based toy example. Further, an evaluation on the public dataset KITTI shows that the proposed framework can improve the accuracy of several odometry estimation algorithms in practical situations.},
  archive   = {C_IROS},
  author    = {Kenji Koide and Masashi Yokozuka and Shuji Oishi and Atsuhiko Banno},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636038},
  pages     = {7708-7714},
  title     = {Adaptive hyperparameter tuning for black-box LiDAR odometry},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Some research questions for SLAM in deformable environments.
<em>IROS</em>, 7653–7660. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {SLAM in deformable environments is a very challenging research topic. Some research works have been presented by different research groups in the past few years. However, there are still some challenging research questions remaining unanswered. This paper discusses some of these research questions focusing on the case when point features are used to describe the deformable environments. The SLAM problems are formulated as extensions of point feature based SLAM in static environments, including both optimisation based offline SLAM and filter based online SLAM. To illustrate the problems and questions more clearly, some concepts and results using simple 2D examples are presented. The MATLAB source codes of the results are made publicly available (https://github.com/cyb1212/DeformableSLAM2D.git) to help the readers understand the problems more clearly.},
  archive   = {C_IROS},
  author    = {Shoudong Huang and Yongbo Chen and Liang Zhao and Yanhao Zhang and Mengya Xu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635883},
  pages     = {7653-7660},
  title     = {Some research questions for SLAM in deformable environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Underwater visual acoustic SLAM with extrinsic calibration.
<em>IROS</em>, 7647–7652. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Underwater scenarios are challenging for visual Simultaneous Localization and Mapping (SLAM) due to limited visibility and intermittently losing structures in image views. In this paper, we propose a visual acoustic bundle adjustment system which fuses a camera and a Doppler Velocity Log (DVL) in a graph SLAM framework for reliable underwater localization and mapping. In order to fuse the vision with the acoustic measurements, an calibration algorithm is also designed to estimate extrinsic parameters between a camera and a DVL using features detected in scenes. Experimental results in a tank and an offshore wind farm show the proposed method can achieve better robustness and localization accuracy than pure visual SLAM, especially in visually challenging scenarios, and the extrinsic calibration parameters can be accurately estimated, even when initialized with a random guess.},
  archive   = {C_IROS},
  author    = {Shida Xu and Tomasz Luczynski and Jonatan Scharff Willners and Ziyang Hong and Kaicheng Zhang and Yvan R. Petillot and Sen Wang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636258},
  pages     = {7647-7652},
  title     = {Underwater visual acoustic SLAM with extrinsic calibration},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-hypothesis approach to pose ambiguity in
object-based SLAM. <em>IROS</em>, 7639–7646. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In object-based Simultaneous Localization and Mapping (SLAM), 6D object poses offer a compact representation of landmark geometry useful for downstream planning and manipulation tasks. However, measurement ambiguity then arises as objects may possess complete or partial object shape symmetries (e.g., due to occlusion), making it difficult or impossible to generate a single consistent object pose estimate. One idea is to generate multiple pose candidates to counteract measurement ambiguity. In this paper, we develop a novel approach that enables an object-based SLAM system to reason about multiple pose hypotheses for an object, and synthesize this locally ambiguous information into a globally consistent robot and landmark pose estimation formulation. In particular, we (1) present a learned pose estimation network that provides multiple hypotheses about the 6D pose of an object; (2) by treating the output of our network as components of a mixture model, we incorporate pose predictions into a SLAM system, which, over successive observations, recovers a globally consistent set of robot and object (landmark) pose estimates. We evaluate our approach on the popular YCB-Video Dataset and a simulated video featuring YCB objects. Experiments demonstrate that our approach is effective in improving the robustness of object-based SLAM in the face of object pose ambiguity. 1},
  archive   = {C_IROS},
  author    = {Jiahui Fu and Qiangqiang Huang and Kevin Doherty and Yue Wang and John J. Leonard},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635956},
  pages     = {7639-7646},
  title     = {A multi-hypothesis approach to pose ambiguity in object-based SLAM},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). PoseFusion2: Simultaneous background reconstruction and
human shape recovery in real-time. <em>IROS</em>, 7631–7638. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dynamic environments that include unstructured moving objects pose a hard problem for Simultaneous Localization and Mapping (SLAM) performance. The motion of rigid objects can be typically tracked by exploiting their texture and geometric features. However, humans moving in the scene are often one of the most important, interactive targets – they are very hard to track and reconstruct robustly due to non-rigid shapes. In this work, we present a fast, learning-based human object detector to isolate the dynamic human objects and realise a real-time dense background reconstruction framework. We go further by estimating and reconstructing the human pose and shape. The final output environment maps not only provide the dense static backgrounds but also contain the dynamic human meshes and their trajectories. Our Dynamic SLAM system runs at around 26 frames per second (fps) on GPUs, while additionally turning on accurate human pose estimation can be executed at up to 10 fps.},
  archive   = {C_IROS},
  author    = {Huayan Zhang and Tianwei Zhang and Tin Lun Lam and Sethu Vijayakumar},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636658},
  pages     = {7631-7638},
  title     = {PoseFusion2: Simultaneous background reconstruction and human shape recovery in real-time},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DSVP: Dual-stage viewpoint planner for rapid exploration by
dynamic expansion. <em>IROS</em>, 7623–7630. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a method for efficiently exploring highly convoluted environments. The method incorporates two planning stages - an exploration stage for extending the boundary of the map, and a relocation stage for explicitly transiting the robot to different sub-areas in the environment. The exploration stage develops a local Rapidly-exploring Random Tree (RRT) in the free space of the environment, and the relocation stage maintains a global graph through the mapped environment, both are dynamically expanded over replanning steps. The method is compared to existing state-of-the-art methods in various challenging simulation and real environments. Experiment comparisons show that our method is twice as efficient in exploring spaces using less processing than the existing methods. Further, we release a benchmark environment to evaluate exploration algorithms as well as facilitate development of autonomous navigation systems. The benchmark environment and our method are open-sourced.},
  archive   = {C_IROS},
  author    = {Hongbiao Zhu and Chao Cao and Yukun Xia and Sebastian Scherer and Ji Zhang and Weidong Wang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636473},
  pages     = {7623-7630},
  title     = {DSVP: Dual-stage viewpoint planner for rapid exploration by dynamic expansion},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust multi-camera SLAM with manhattan constraint toward
automated valet parking. <em>IROS</em>, 7615–7622. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a multi-camera simultaneous localization and mapping (SLAM) system using the Manhattan constraint to support automated valet parking. The proposed method uses multiple cameras to expand the system field of view, to improve the robustness of the SLAM system in textureless regions, where point features from different cameras are jointly optimized by a uniform cost function. To improve global map scale consistency, we utilize wheel odometer in the system initialization and the multi-camera cost function. In addition, we introduce the Manhattan world assumption, an abstraction of a man-made environment, into the proposed algorithm, to improve its estimation processes and make it suitable for the multi-camera SLAM system. The Manhattan world assumption is used to estimate the camera rotation by line features in the image and provide a global orientation constraint that increases the mapping accuracy. The proposed algorithm demonstrates stability in low-texture regions and achieves superior accuracy in experiments conducted in multistory parking lots, compared with other algorithms including monocular and multi-camera versions. Regarding efficiency, the proposed algorithm processes twice the number of measurements with 50\% additional computation time while maintaining SLAM stability under a textureless environment.},
  archive   = {C_IROS},
  author    = {Yifei Kang and Yu Song and Wuwei Ge and Tong Ling},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636304},
  pages     = {7615-7622},
  title     = {Robust multi-camera SLAM with manhattan constraint toward automated valet parking},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A new method for generating work piece surface
representations for robotic machining. <em>IROS</em>, 7607–7614. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Execution of automatically generated programs for accurate robotic machining requires the generated trajectories to be not only accurate with respect to the work piece, but also that the trajectories are continuous differentiable (C 1 ) while avoiding unnecessary large curvatures leading to large accelerations that could compromise machining quality or speed. A widely used work piece representation is 3D triangle meshes as they can be easily generated in any CAD representations and from surface scans, and they are also very suitable for robotics applications. However, they lack the C 1 property across the triangle edges.In this paper, a new method for generating C 1 surfaces based on 3D triangle meshes is presented. It will be shown by an example that the method is as good as existing methods with respect to the accuracy of the generated surface, and that the problem with large curvatures is much smaller than for existing methods. Moreover, the difficult input specification of derivatives at the vertices is avoided with this method.},
  archive   = {C_IROS},
  author    = {Nikolaj W. Leth and Henrik G. Petersen},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636774},
  pages     = {7607-7614},
  title     = {A new method for generating work piece surface representations for robotic machining},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Design of a new robot end-effector based on compliant
constant-force mechanism. <em>IROS</em>, 7601–7606. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes the design of a new robot end-effector based on compliant constant-force mechanism for robot-assisted manufacturing, such as polishing. One uniqueness of the proposed end-effector lies in that it offers a constant contact force without using a force sensor and controller. An industrial robot is adopted to position the end-effector and the end-effector regulates the contact force passively. When the end-effector contacts the workpiece, the constant-force motion range acts as a buffer to counteract the excessive displacement caused by inertia. As a result, there is no force overshoot, protecting the consistency of the workpiece. The analytical model of the constant-force mechanism is deduced and the structural parameters are optimized to maximize the constant-force motion range under other constraints. For experimental testing, a prototype of the constant-force end-effector has been fabricated. The mechanism exhibits constant-force tendency with the force varying from 3.4 to 4.2 N between 0.7 and 1.7 mm. Experimental results verify the effectiveness of the presented constant-force end-effector mechanism.},
  archive   = {C_IROS},
  author    = {Yuzhang Wei and Qingsong Xu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636597},
  pages     = {7601-7606},
  title     = {Design of a new robot end-effector based on compliant constant-force mechanism},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive optimization of autonomous vehicle computational
resources for performance and energy improvement. <em>IROS</em>,
7594–7600. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous vehicles usually consume a large amount of computational power for their operations, especially for the tasks of sensing and perception with artificial intelligence algorithms. Such a computation may not only cost a significant amount of energy but also cause performance issues when the onboard computational resources are limited. To address this issue, this paper proposes an adaptive optimization method to online allocate the onboard computational resources of an autonomous vehicle amongst multiple vehicular subsystems depending on the contexts of the situations that the vehicle is facing. Different autonomous driving scenarios were designed to validate the proposed approach and the results showed that it could help improve the overall performance and energy consumption of autonomous vehicles compared to existing computational arrangement.},
  archive   = {C_IROS},
  author    = {Saurabh Jambotkar and Longxiang Guo and Yunyi Jia},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635828},
  pages     = {7594-7600},
  title     = {Adaptive optimization of autonomous vehicle computational resources for performance and energy improvement},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Energy-efficient mobile robot control via run-time
monitoring of environmental complexity and computing workload.
<em>IROS</em>, 7587–7593. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose an energy-efficient controller to minimize the energy consumption of a mobile robot by dynamically manipulating the mechanical and computational actuators of the robot. The mobile robot performs real-time vision-based applications based on an event-based camera. The actuators of the controller are CPU voltage/frequency for the computation part and motor voltage for the mechanical part. We show that independently considering speed control of the robot and voltage/frequency control of the CPU does not necessarily result in an energy-efficient solution. In fact, to obtain the highest efficiency, the computation and mechanical parts should be controlled together in synergy. We propose a fast hill-climbing optimization algorithm to allow the controller to find the best CPU/motor configuration at run-time and whenever the mobile robot is facing a new environment during its travel. Experimental results on a robot with Brushless DC Motors, Jetson TX2 board as the computing unit, and a DAVIS-346 event-based camera show that the proposed control algorithm can save battery energy by an average of 50.5\%, 41\%, and 30\%, in low-complexity, medium-complexity, and high-complexity environments, over baselines.},
  archive   = {C_IROS},
  author    = {Sherif A.S. Mohamed and Mohammad-Hashem Haghbayan and Antonio Miele and Onur Mutlu and Juha Plosila},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635877},
  pages     = {7587-7593},
  title     = {Energy-efficient mobile robot control via run-time monitoring of environmental complexity and computing workload},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On step-and-scan trajectories used in wafer scanners in
semiconductor manufacturing. <em>IROS</em>, 7580–7586. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Adopting the ideal reliable machine model, the throughput of a lithography machine can be given as the reciprocal of the operation time. This time can be defined at the die level where the actual exposure process takes place as the time unit per die. A closer look at the motion profiles, namely step-and-scan trajectories, suggests that a multi-disciplinary design optimization should be involved when such profiles are selected or designed. Being the reference motion used, the step-and-scan trajectories not only affect the machine performance, but also affect its throughput and to an extent the die yield as well. Structural vibration, and thermal loading at the actuators due to friction and repetitive motion may build up because of following the reference motion. Moreover, since the exposure process and equipment are synchronized with the reference motion, deformation and thermal stress may affect the reticle, the wafer and the projection elements if the exposure high-energy duration and frequency are not taken into consideration while designing the reference motion. From dynamics point of view, reference motion with higher-order derivatives enhances the tracking performance of the machine, however, its operational cost is usually overlooked. In this paper, we present a case-study that outlines the aforementioned aspects using three step-and-scan profiles of the same order. We conclude by posing the following research question: what is the best combination of orders of the step and the scan trajectories that jointly meet the desired performance and operating conditions?},
  archive   = {C_IROS},
  author    = {Yazan M. Al-Rawashdeh and Mohammad Al Janaideh and Marcel Heertjes},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636016},
  pages     = {7580-7586},
  title     = {On step-and-scan trajectories used in wafer scanners in semiconductor manufacturing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning of parameters in behavior trees for movement
skills. <em>IROS</em>, 7572–7579. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement Learning (RL) is a powerful mathematical framework that allows robots to learn complex skills by trial-and-error. Despite numerous successes in many applications, RL algorithms still require thousands of trials to converge to high-performing policies, can produce dangerous behaviors while learning, and the optimized policies (usually modeled as neural networks) give almost zero explanation when they fail to perform the task. For these reasons, the adoption of RL in industrial settings is not common. Behavior Trees (BTs), on the other hand, can provide a policy representation that a) supports modular and composable skills, b) allows for easy interpretation of the robot actions, and c) provides an advantageous low-dimensional parameter space. In this paper, we present a novel algorithm that can learn the parameters of a BT policy in simulation and then generalize to the physical robot without any additional training. We leverage a physical simulator with a digital twin of our workstation, and optimize the relevant parameters with a black-box optimizer. We showcase the efficacy of our method with a 7-DOF KUKAiiwa manipulator in a task that includes obstacle avoidance and a contact-rich insertion (peg-in-hole), in which our method outperforms the baselines.},
  archive   = {C_IROS},
  author    = {Matthias Mayr and Konstantinos Chatzilygeroudis and Faseeh Ahmad and Luigi Nardi and Volker Krueger},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636292},
  pages     = {7572-7579},
  title     = {Learning of parameters in behavior trees for movement skills},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Textile taxonomy and classification using pulling and
twisting. <em>IROS</em>, 7564–7571. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Identification of textile properties is an important milestone toward advanced robotic manipulation tasks that consider interaction with clothing items such as assisted dressing, laundry folding, automated sewing, textile recycling and reusing. Despite the abundance of work considering this class of deformable objects, many open problems remain. These relate to the choice and modelling of the sensory feedback as well as the control and planning of the interaction and manipulation strategies. Most importantly, there is no structured approach for studying and assessing different approaches that may bridge the gap between the robotics community and textile production industry. To this end, we outline a textile taxonomy considering fiber types and production methods, commonly used in textile industry. We devise datasets according to the taxonomy, and study how robotic actions, such as pulling and twisting of the textile samples, can be used for the classification. We also provide important insights from the perspective of visualization and interpretability of the gathered data.},
  archive   = {C_IROS},
  author    = {Alberta Longhini and Michael C. Welle and Ioanna Mitsioni and Danica Kragic},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635992},
  pages     = {7564-7571},
  title     = {Textile taxonomy and classification using pulling and twisting},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mobile 3D printing robot simulation with viscoelastic
fluids. <em>IROS</em>, 7557–7563. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The system design and algorithm development of mobile 3D printing robots need a realistic simulation. They require a mobile robot simulation platform to interoperate with a physics-based material simulation for handling interactions between the time-variant deformable 3D printing materials and other simulated rigid bodies in the environment, which is not available for roboticists yet. To bridge this gap and enable the real-time simulation of mobile 3D printing processes, we develop a simulation framework that includes particle-based viscoelastic fluid simulation and particle-to-mesh conversion in the widely adopted Gazebo robotics simulator, avoiding the bottlenecks of traditional additive manufacturing simulation approaches. This framework is the first of its kind that enables the simulation of robot arms or mobile manipulators together with viscoelastic fluids. The method is tested using various material properties and multiple collaborating robots to demonstrate its simulation ability for the robots to plan and control the printhead trajectories and to visually sense at the same time the printed fluid materials as a free-form mesh. The scalability as a function of available material particles in the simulation was also studied. A simulation with an average of 5 FPS was achieved on a regular desktop computer.},
  archive   = {C_IROS},
  author    = {Uljad Berdica and Yuewei Fu and Yuchen Liu and Emmanouil Angelidis and Chen Feng},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636114},
  pages     = {7557-7563},
  title     = {Mobile 3D printing robot simulation with viscoelastic fluids},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Daß: Distributable and scalable simulation of robotic
applications. <em>IROS</em>, 7550–7556. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simulation is an essential tool for developing robotic systems; however it is computationally intensive and does not scale well, especially for multi-robot scenarios. In this paper we introduce daß – a system that facilitates distributable and scalable simulations of robotic applications. It overcomes many of the performance and quality limitations involved in common multi-robot simulation scenarios through horizontal scaling. Through extensive evaluation, we show that daß significantly outperforms the widely used Gazebo robotics simulator, and easily scales well beyond the current limitations with no significant loss in performance and simulation fidelity.},
  archive   = {C_IROS},
  author    = {Hans C. Woithe and Itai Segall},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636152},
  pages     = {7550-7556},
  title     = {Daß: Distributable and scalable simulation of robotic applications},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Muscle-reflex model of human locomotion entrains to
mechanical perturbations. <em>IROS</em>, 7544–7549. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Prior experiments have shown that human gait synchronizes to periodic torque pulses applied about the hip and ankle joints by robotic exoskeletons. Importantly, entrainment occurred even when the pulse period differed slightly from the user’s preferred stride period, making it a viable approach to increase gait speed. As gait speed is an important outcome of gait therapy, gait entrainment to mechanical perturbations may serve as a promising new method of robot-aided therapy. Still, an understanding of the underlying neuromechanical processes that give rise to gait entrainment is needed to fully evaluate its therapeutic potential. To gain such insight, the goal of this paper was to evaluate whether an existing neuromechanical model of human locomotion exhibited entrainment behavior similar to that observed in the prior human experiments. Simulation results showed that the model entrained to pulses applied at both the ankle and hip joints. The convergence of relative phase between model gait and hip perturbations was similar to that observed with the human gait, but differed slightly for ankle perturbations. Thus, models that can more accurately describe neuromechanical interactions between human gait and robotic exoskeletons are still needed. Nevertheless, the simulation results support the notion that the limit-cycle behavior observed during locomotion does not require supra-spinal control or a self-sustaining oscillatory neural network, which has important implications for improving gait therapy.},
  archive   = {C_IROS},
  author    = {Banu Abdikadirova and Jongwoo Lee and Neville Hogan and Meghan E. Huber},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636780},
  pages     = {7544-7549},
  title     = {Muscle-reflex model of human locomotion entrains to mechanical perturbations},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Co-design of embodied intelligence: A structured approach.
<em>IROS</em>, 7536–7543. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the problem of co-designing embodied intelligence as a whole in a structured way, from hardware components such as propulsion systems and sensors to software modules such as control and perception pipelines. We propose a principled approach to formulate and solve complex embodied intelligence co-design problems, leveraging a monotone co-design theory. The methods we propose are intuitive and integrate heterogeneous engineering disciplines, allowing analytical and simulation-based modeling techniques and enabling interdisciplinarity. We illustrate through a case study how, given a set of desired behaviors, our framework is able to compute Pareto efficient solutions for the entire hardware and software stack of a self-driving vehicle.},
  archive   = {C_IROS},
  author    = {Gioele Zardini and Dejan Milojevic and Andrea Censi and Emilio Frazzoli},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636513},
  pages     = {7536-7543},
  title     = {Co-design of embodied intelligence: A structured approach},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An approach to deploy interactive robotic simulators on the
web for HRI experiments: Results in social robot navigation.
<em>IROS</em>, 7528–7535. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Evaluation of social robot navigation inherently requires human input due to its qualitative nature. Motivated by the need to scale human evaluation, we propose a general method for deploying interactive, rich-client robotic simulations on the web. Prior approaches implement specific web- compatible simulators or provide tools to build a simulator for a specific study. Instead, our approach builds on standard Linux tools to share a graphical desktop with remote users. We leverage these tools to deploy simulators on the web that would typically be constrained to desktop computing environments. As an example implementation of our approach, we introduce the SEAN Experimental Platform (SEAN-EP). With SEAN- EP, remote users can virtually interact with a mobile robot in the Social Environment for Autonomous Navigation, without installing any software on their computer or needing specialized hardware. We validated that SEAN-EP could quickly scale the collection of human feedback and its usability through an online survey. In addition, we compared human feedback from participants that interacted with a robot using SEAN- EP with feedback obtained through a more traditional video survey. Our results suggest that human perceptions of robots may differ based on whether they interact with the robots in simulation or observe them in videos. Also, they suggest that people perceive the surveys with interactive simulations as less mentally demanding than video surveys.},
  archive   = {C_IROS},
  author    = {Nathan Tsoi and Mohamed Hussein and Olivia Fugikawa and J. D. Zhao and Marynel Vázquez},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636319},
  pages     = {7528-7535},
  title     = {An approach to deploy interactive robotic simulators on the web for HRI experiments: Results in social robot navigation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). IGibson 1.0: A simulation environment for interactive tasks
in large realistic scenes. <em>IROS</em>, 7520–7527. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present iGibson 1.0, a novel simulation environment to develop robotic solutions for interactive tasks in large-scale realistic scenes. Our environment contains 15 fully interactive home-sized scenes with 108 rooms populated with rigid and articulated objects. The scenes are replicas of real-world homes, with distribution and the layout of objects aligned to those of the real world. iGibson 1.0 integrates several key features to facilitate the study of interactive tasks: i) generation of high-quality virtual sensor signals (RGB, depth, segmentation, LiDAR, flow and so on), ii) domain randomization to change the materials of the objects (both visual and physical) and/or their shapes, iii) integrated sampling-based motion planners to generate collision-free trajectories for robot bases and arms, and iv) intuitive human-iGibson interface that enables efficient collection of human demonstrations. Through experiments, we show that the full interactivity of the scenes enables agents to learn useful visual representations that accelerate the training of downstream manipulation tasks. We also show that iGibson features enable the generalization of navigation agents, and that the human-iGibson interface and integrated motion planners facilitate efficient imitation learning of human demonstrated (mobile) manipulation behaviors. iGibson 1.0 is open-source, equipped with comprehensive examples and documentation. For more information, visit our project website: http://svl.stanford.edu/igibson/.},
  archive   = {C_IROS},
  author    = {Bokui Shen and Fei Xia and Chengshu Li and Roberto Martín-Martín and Linxi Fan and Guanzhi Wang and Claudia Pérez-D’Arpino and Shyamal Buch and Sanjana Srivastava and Lyne Tchapmi and Micael Tchapmi and Kent Vainio and Josiah Wong and Li Fei-Fei and Silvio Savarese},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636667},
  pages     = {7520-7527},
  title     = {IGibson 1.0: A simulation environment for interactive tasks in large realistic scenes},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to fly—a gym environment with PyBullet physics for
reinforcement learning of multi-agent quadcopter control. <em>IROS</em>,
7512–7519. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic simulators are crucial for academic research and education as well as the development of safety-critical applications. Reinforcement learning environments— simple simulations coupled with a problem specification in the form of a reward function—are also important to standardize the development (and benchmarking) of learning algorithms. Yet, full-scale simulators typically lack portability and paral-lelizability. Vice versa, many reinforcement learning environments trade-off realism for high sample throughputs in toy-like problems. While public data sets have greatly benefited deep learning and computer vision, we still lack the software tools to simultaneously develop—and fairly compare—control theory and reinforcement learning approaches. In this paper, we propose an open-source OpenAI Gym-like environment for multiple quadcopters based on the Bullet physics engine. Its multi-agent and vision-based reinforcement learning interfaces, as well as the support of realistic collisions and aerodynamic effects, make it, to the best of our knowledge, a first of its kind. We demonstrate its use through several examples, either for control (trajectory tracking with PID control, multi-robot flight with downwash, etc.) or reinforcement learning (single and multi-agent stabilization tasks), hoping to inspire future research that combines control theory and machine learning.},
  archive   = {C_IROS},
  author    = {Jacopo Panerati and Hehui Zheng and SiQi Zhou and James Xu and Amanda Prorok and Angela P. Schoellig},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635857},
  pages     = {7512-7519},
  title     = {Learning to fly—a gym environment with PyBullet physics for reinforcement learning of multi-agent quadcopter control},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Keeping it simple: Bio-inspired threshold-based strain
sensing for micro-aerial vehicles. <em>IROS</em>, 7505–7511. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Moths use hundreds of strain sensors (campaniform sensilla) on each wing to quickly respond to perturbations that may otherwise destabilize the moth during flight. A similar sensing approach could help stabilize micro-aerial vehicles (MAVs), but large sensor arrays are challenging due to the wiring and large latency that exists when capturing data from many traditional strain sensors. This work introduces a simplified bio-inspired strain sensor; the sensor interface and kinematics were inspired by campaniform sensilla that output a spike only in response to signals of interest. The engineered sensor outputs a discrete analog signal representing strain thresholds. A kinematic model of the sensor design is developed and describes the measured strain in terms of the sensor’s geometric parameters. This model is used to understand trade-offs between sensor resolution and range, and is validated using a finite element model (FEM) of the sensor. The sensor was designed with ease of fabrication in mind, using simple techniques and commercially available components. Fabricated sensors were tested in a four-point flexural test, and the data from the analytical and FEM model show good agreement with the experimental results. The sensors demonstrate resolutions of 83, 158, and 281 microstrain for the different designs tested. A sensor is placed on a model wing to illustrate future applications to MAVs as well as the sensor’s ability to sense both compressive and tensile strains.},
  archive   = {C_IROS},
  author    = {Regan Kubicek and Mahnoush Babaei and Sarah Bergbreiter},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636277},
  pages     = {7505-7511},
  title     = {Keeping it simple: Bio-inspired threshold-based strain sensing for micro-aerial vehicles},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Autonomous object harvesting using synchronized
optoelectronic microrobots. <em>IROS</em>, 7498–7504. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Optoelectronic tweezer-driven microrobots (OETdMs) are a versatile micromanipulation technology based on the application of light induced dielectrophoresis to move small dielectric structures (microrobots) across a photoconductive substrate. The microrobots in turn can be used to exert forces on secondary objects and carry out a wide range of micromanipulation operations, including collecting, transporting and depositing microscopic cargos. In contrast to alternative (direct) micromanipulation techniques, OETdMs are relatively gentle, making them particularly well suited to interacting with sensitive objects such as biological cells. However, at present such systems are used exclusively under manual control by a human operator. This limits the capacity for simultaneous control of multiple microrobots, reducing both experimental throughput and the possibility of cooperative multi-robot operations. In this article, we describe an approach to automated targeting and path planning to enable open-loop control of multiple microrobots. We demonstrate the performance of the method in practice, using microrobots to simultaneously collect, transport and deposit silica microspheres. Using computational simulations based on real microscopic image data, we investigate the capacity of microrobots to collect target cells from within a dissociated tissue culture. Our results indicate the feasibility of using OETdMs to autonomously carry out micromanipulation tasks within complex, unstructured environments.},
  archive   = {C_IROS},
  author    = {Christopher Bendkowski and Laurent Mennillo and Tao Xu and Mohamed Elsayed and Filip Stojic and Harrison Edwards and Shuailong Zhang and Cindi Morshead and Vijay Pawar and Aaron R. Wheeler and Danail Stoyanov and Michael Shaw},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636475},
  pages     = {7498-7504},
  title     = {Autonomous object harvesting using synchronized optoelectronic microrobots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive tracking controller for an alginate artificial
cell. <em>IROS</em>, 7483–7489. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an adaptive backstepping controller for the reference tracking of an alginate artificial cell. An adaptive controller was implemented to precisely manipulate a magnetic artificial cell actuated by rotating magnetic fields. The rolling motion of a small-scale robot in a fluidic environment is challenging, especially when the fluid imparts an unknown response at low Reynolds number. In order to compensate for this uncertainty, an unknown tuning parameter encapsulating these effects was added to the governing equations of motion. A controller with an update law was then designed to estimate the unknown parameter and force the artificial cell to produce the desired response. The stability of the proposed controller was established by a candidate Lyapunov function. Real-time experiments were conducted to demonstrate the effectiveness of the designed controller at guiding an artificial cell to an arbitrary target position. Alginate cells were guided through a maze using the controller and was later combined with wall constraints to allow multiple alginate cells to reach the same target location. This controller can be applied to both surface motion and swimming-based small-scale robots in future applications for micro-assembly and targeted drug delivery.},
  archive   = {C_IROS},
  author    = {Gokhan Kararsiz and Louis William Rogowski and Xiao Zhang and Anuruddha Bhattacharjee and Min Jun Kim},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636639},
  pages     = {7483-7489},
  title     = {Adaptive tracking controller for an alginate artificial cell},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hybrid magnetic force and torque actuation of miniature
helical robots using mobile coils to accelerate blood clot removal.
<em>IROS</em>, 7476–7482. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mechanical rubbing of blood clot using miniature magnetic helical robots is a potential way for thrombolysis. In this paper, we report a new strategy for this issue based on mobile coils. Previously, we proposed the concept of magnetic actuation with parallel mobile coils, in which multiple coils can move in 3D space. Enabled by mobility of the coils, additional degree-of-freedom (DOF) could be utilized for actuation performance optimization. Besides the primary helical propulsion by rotating magnetic fields, our strategy aims to optimize the coil motion to make the magnetic force contributes the most to the helical robot forward motion. For this goal, modeling of the magnetic field and force of multiple mobile coils are presented, based on which an optimization algorithm is formulated to output the best coil motion. For validation, an enhanced mobile coil system having a workspace of Φ500 mm ×150 mm is constructed based on the parallel mobile coil concept. Simulations show the effectiveness of the proposed strategy, whose effective workspace for a specific task can also be obtained. After implementing the proposed strategy, preliminary experiments using clot analog demonstrate that the removal speed is accelerated over 50\% compared to that without coil motion optimization.},
  archive   = {C_IROS},
  author    = {Lidong Yang and Moqiu Zhang and Haojin Yang and Zhengxin Yang and Li Zhang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636851},
  pages     = {7476-7482},
  title     = {Hybrid magnetic force and torque actuation of miniature helical robots using mobile coils to accelerate blood clot removal},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Development of a vision-based robotic manipulation system
for transferring of oocytes. <em>IROS</em>, 7470–7475. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Embryos/oocytes vitrification is an essential cryopreservation technique in IVF (in vitro fertilization) clinics. The reliable and effective transferring of embryos/oocytes is crucial to the subsequent steps in the whole procedure of vitrification. After each transferring, the straw needs to be replaced with a new one. Due to the uncertainties in the fabrication and installation, the exact knowledge of the kinematic model of the straw is usually unknown, and the relationship between the microscope and the straw is also unknown without calibration beforehand. In such situation, automatically transferring the oocytes from micropipette to the narrow tip of straw (0.7mm) is very challenging. In this paper, a new vision-guided robotic system is developed to automate the transferring of the oocyte without calibration. To this end, the unknown depth information is estimated then compensated by constructing a deep vision network through microscope image, and an approximate Jacobian control algorithm is also proposed to servo control the end tip of the uncalibrated straw to contact the micropipette with the vision feedback. After that, the oocyte is automatically transferred from the micropipette to the straw to finalize the task. The stability of the closed-loop control system is rigorously proved with Lyapunov methods, and the effectiveness of the developed robot is validated in experiments.},
  archive   = {C_IROS},
  author    = {Shu Miao and Dayuan Chen and Qiang Nie and Xin Jiang and Xulin Sun and Jianjun Dai and Yun-Hui Liu and Xiang Li},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636732},
  pages     = {7470-7475},
  title     = {Development of a vision-based robotic manipulation system for transferring of oocytes},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Precise control of magnetized macrophage cell robot for
targeted drug delivery. <em>IROS</em>, 7464–7469. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Micro-nano-robots are considered to be a promising platform for drug delivery in biological organisms, but there are still urgent technical problems in biocompatibility and degradability of 3D-printed-based micro-robots that need to be solved. Therefore, in this paper, we design a magnetized bio-hybrid robot, which uses mouse macrophages as carriers, and allowed it to swallow Fe 2 O 3 particles with a diameter of 10 nm. The robot takes advantage of macrophage’s natural biocompatibility and targeting characteristics to reach and function in complex environments such as: eye, knee, tumor, etc., and finally being able to be actively metabolized by the organism. More importantly, the cell robot can move precisely along a preplanned path under the control of a three-dimensional magnetic control system built in this study, and be delivered accurately to the vicinity of cancer cells in vitro environment. In future work, cellular robots could be allowed to carry anti-cancer drugs and release them in a targeted manner at the lesion. These microrobots have shown great potential for tumor reginal targeted drug delivery.},
  archive   = {C_IROS},
  author    = {Luyao Wang and Yuguo Dai and Hongyan Sun and Li Song and Lina Jia and Chiju Jiang and Fumihito Arai and Lin Feng},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635945},
  pages     = {7464-7469},
  title     = {Precise control of magnetized macrophage cell robot for targeted drug delivery},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Trotting and pacing locomotion of a position-controlled
quadruped robot. <em>IROS</em>, 7456–7463. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Compared with torque-control techniques, a position-controlled quadruped robot is lower cost, easier to build, and more direct to drive. However, the stiff actuation of position-controlled actuators makes it difficult for the quadruped to achieve dynamically stable locomotion. This paper presents an implementation of joint velocity programming technique to regulate the body’s moving speed and orientation for a position-controlled quadruped robot that performs trotting or pacing locomotion. The robot model is mapped to a new coordinate space in order to decouple the control of its body. In one plane of the new coordinate space, the robot is simplified to an inverted pendulum model to generate attitude and velocity tracking actions. In the other planes, body regulating problems are formulated in velocity forms and solved by designing support leg motions. The controllers in these planes are integrated to produce joint velocities that enable robust trotting and pacing locomotion at a variety of speeds and directions, despite lacking force control or feedback techniques. Physical test results as well as simulating results demonstrate control of the quadruped robot SmarQ to perform omni-directional locomotion, impact recovery, and adaptability to uneven terrains.},
  archive   = {C_IROS},
  author    = {Guoteng Zhang and Yibin Li and Shugen Ma},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636006},
  pages     = {7456-7463},
  title     = {Trotting and pacing locomotion of a position-controlled quadruped robot},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quadruped robot hopping on two legs. <em>IROS</em>,
7448–7455. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a control strategy for quadruped robots to hop on their rear legs in three-dimensional space. The proposed approach generates nominal center of mass (CoM) trajectories based on a template spring-loaded inverted pendulum (SLIP) model. Tracking this reference remains a challenge due to the underactauted nature of balance with point feet. To address this challenge, a control-Lyapunov function based quadratic programming (CLF-QP) controller is proposed, which modulates nominal ground reaction forces (GRFs) to balance the torso while considering friction limits. The CLF construction is guided by a variational-based linearization (VBL) applied to a reduced-order single-rigid-body (SRB) model, and treats underactuation via solving a Riccati equation to obtain the CLF. A new balance control approach is presented that effectively decouples sagittal plane control (via re-planning) with lateral and rotational control (via the CLF and VBL). The proposed approach shows more robust balancing performance than the conventional CLF-QP approach. Simulations of the Mini Cheetah demonstrate in-place hopping with up to a 0.71m apex height.},
  archive   = {C_IROS},
  author    = {Shenggao Li and Hua Chen and Wei Zhang and Patrick M. Wensing},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636120},
  pages     = {7448-7455},
  title     = {Quadruped robot hopping on two legs},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive force-based control for legged robots.
<em>IROS</em>, 7440–7447. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Adaptive control can address model uncertainty in control systems. However, it is preliminarily designed for tracking control. Recent advancements in the control of quadruped robots show that force control can effectively realize agile and robust locomotion. In this paper, we present a novel adaptive force-based control framework for legged robots. We introduce a new architecture in our proposed approach to incorporate adaptive control into quadratic programming (QP) force control. Since our approach is based on force control, it also retains the advantages of the baseline framework, such as robustness to uneven terrain, controllable friction constraints, or soft impacts. Our method is successfully validated in both simulation and hardware experiments. While the baseline QP control has shown a significant degradation in the body tracking error with a small load, our proposed adaptive force-based control can enable the 12-kg Unitree A1 robot to walk on rough terrains while carrying a heavy load of up to 6 kg (50\% of the robot weight). When standing with four legs, our proposed adaptive control can even allow the robot to carry up to 11 kg of load (92\% of the robot weight) with less than 5-cm tracking error in the robot height.},
  archive   = {C_IROS},
  author    = {Mohsen Sombolestan and Yiyu Chen and Quan Nguyen},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636393},
  pages     = {7440-7447},
  title     = {Adaptive force-based control for legged robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Force-feedback based whole-body stabilizer for
position-controlled humanoid robots. <em>IROS</em>, 7432–7439. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies stabilizer design for position-controlled humanoid robots. Stabilizers are an essential part for position-controlled humanoids, whose primary objective is to adjust the control input sent to the robot to assist the tracking controller to better follow the planned reference trajectory. To achieve this goal, this paper develops a novel force-feedback based whole-body stabilizer that fully exploits the six-dimensional force measurement information and the whole-body dynamics to improve tracking performance. Relying on rigorous analysis of whole-body dynamics of position-controlled humanoids under unknown contact, the developed stabilizer leverages quadratic-programming based technique that allows cooperative consideration of both the center-of-mass tracking and contact force tracking. The effectiveness of the proposed stabilizer is demonstrated on the UBTECH Walker robot in the MuJoCo simulator. Simulation validations show a significant improvement in various scenarios as compared to commonly adopted stabilizers based on the zero-moment-point feedback and the linear inverted pendulum model.},
  archive   = {C_IROS},
  author    = {Shunpeng Yang and Hua Chen and Zhen Fu and Wei Zhang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636634},
  pages     = {7432-7439},
  title     = {Force-feedback based whole-body stabilizer for position-controlled humanoid robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Design of a large-scale electrically-actuated quadruped
robot and locomotion control for the narrow passage. <em>IROS</em>,
7424–7431. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the gradual maturity of the software and hardware of quadruped robots, the application scenarios of quadruped robots are increasing, such as security, rescue, exploration and other tasks. Quadruped robots are flexible and adaptive to challenging or complex environment. This study presents a large-scale quadruped robot, Pegasus II, which is a new version upgraded from the previous quadruped robot, Pegasus [1]. System design of Pegasus II is introduced, including mechanical and electronic design. Locomotion control for a special scene, L-shaped narrow corner, in which a large-scale quadruped robot is not able to traverse in a common quadrupedal mode, is demonstrated. The long body length of a large-scale quadruped robot, such as Pegasus II, incurs difficulty in traversing freely in such a narrow passage. Motivated by this issue, this study proposes an experimental implementation to realize the transition from quadrupedal mode to bipedal mode. The control framework is presented, which mainly includes trajectory optimization, whole-body control, compliance control, and joint torque estimator. Simulations and experiments are conducted to validate the performance, including gait transition, compliance control.},
  archive   = {C_IROS},
  author    = {Shusheng Ye and Jianwen Luo and Caiming Sun and Bingchen Jin and Juntong Su and Aidong Zhang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636262},
  pages     = {7424-7431},
  title     = {Design of a large-scale electrically-actuated quadruped robot and locomotion control for the narrow passage},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Development of rotating workspace ground contact force
observer for legged robot. <em>IROS</em>, 7410–7415. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Legged robots have opened their way to more stable and practical mobile robot applications. However, their locomotion strategies are limited to similar patterns, and dynamic running at high speed still is not successfully realized. One of the key technology required for the realization of the dynamic running of the legged robot is to estimate the ground contact force and control it in real-time. This paper tackles this problem in two ways: the derivation of the observer algorithm based on the leg dynamics and the simplification of the observer design using the Rotating Workspace motion description. To this end, two novel coordinate systems are introduced to describe the joint space motion and the workspace differently, and the ground contact force observer is designed in the novel coordinate systems. The performance of the proposed observer is verified through experimental results.},
  archive   = {C_IROS},
  author    = {Woosong Kang and Chan Lee and Sehoon Oh},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636840},
  pages     = {7410-7415},
  title     = {Development of rotating workspace ground contact force observer for legged robot},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A real-time motion detection and object tracking framework
for future robot-rat interaction. <em>IROS</em>, 7404–7409. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose an automatic robot-rat interaction framework that enables a robotic rat to realize real-time localization, tracking and movement analysis of a laboratory rat. Specifically, we combine an object detector with stereo matching to achieve fast localization of the laboratory rat. Combined with the rat-like motion of the robot, one-step tracking of the rat is achieved, which enables the robot to eliminate last location error in one cycle of visual servo control. When positioning the rat, a unified quantitative description and analysis of rat motion is implemented by using a state vector composed of the centroids of head, body and tail. Preliminary robot-rat interaction tests show that the robot achieved a steady tracking of a fast-moving rat for a duration of 10 minutes. To the best of our knowledge, it is the first time that a rat-sized robot achieves a continuous tracking of actual rats by a built-in miniature stereo vision system. Experimental results show that the sequence of state vectors accurately represents the pitch movement of the rat. Thus, this work is a step toward more natural interaction between robots and animals.},
  archive   = {C_IROS},
  author    = {Chen Chen and Guanglu Jia and Zihang Gao and Xiaowen Guo and Qiang Huang and Toshio Fukuda and Qing Shi},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636403},
  pages     = {7404-7409},
  title     = {A real-time motion detection and object tracking framework for future robot-rat interaction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Development of a bio-inspired soft robotic gripper based on
tensegrity structures. <em>IROS</em>, 7398–7403. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The bones, muscles, tendons and connective tissues form a continuous tension network throughout human body. This heterogeneous mixture presents the characteristics of tensegrity, providing the body with structurally integrity, stability and flexibility. Inspired by this, this paper proposes a novel soft robotic gripper based on tensegrity structures. Firstly, the design and working principle of the tensegrity-based robotic gripper is introduced, which is composed of a series of discrete rigid segments connected with tensegrity joints by means of tensional cables. Then, the kinematics of the robotic gripper is analyzed using force density method to obtain the relationship between the pose of the gripper and the tension of cables. Finally experiments on the developed prototype demonstrates that the robotic gripper is able to grasp various objects of different sizes, shapes, and materials. Additional desirable properties are derived from using tensegrity structures in the robotic gripper: light weight, high compliance, inherent safety, low cost, and waterproof and dustproof performance. It is suggested that tensegrity structures have great potential to be an effective alternative to the development of soft robotic grippers.},
  archive   = {C_IROS},
  author    = {Yixiang Liu and Qing Bi and Yibin Li},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635929},
  pages     = {7398-7403},
  title     = {Development of a bio-inspired soft robotic gripper based on tensegrity structures},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Wing fold and twist greatly improves flight efficiency for
bat-scale flapping wing robots. <em>IROS</em>, 7391–7397. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inspired by bat flight performance, we explore the advantages of wing twist and fold for flapping wing robots. For this purpose, we develop a dynamical model that incorporates these two degrees of freedom to the wing. The twist is assumed to be linearly-increasing along the wing, while the wing fold is modeled as a relative rotation of the handwing with respect to the armwing. An optimization scheme parameterizes the wing kinematics for 2, 5 and 8 m/s forward flight velocities. The intricate interplay between wing orientation, effective angle of attack and the ensuing lift and thrust generation are discussed. The results show that wing twist and fold alleviate negative lift and thrust in the upstroke, and in some cases producing persistent positive thrust throughout cycle for handwing. As a result, power consumption drops precipitously compared to the base case of a rigid flat plate. Another crucial realization is the relative importance of wing twist and fold in achieving efficient flight strongly depends on speeds. At slow flight, twist is significantly more effective in minimizing the power, but becomes energetically inefficient for fast speeds. The results also show that a 45° wing fold during upstroke is energetically beneficial for all speeds. The synergy of wing twist and fold are most prominent at slow flight. These findings provides useful guidelines for designing flapping wing robots.},
  archive   = {C_IROS},
  author    = {Xiaozhou Fan and Kenneth Breuer and Hamid Vejdani},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636735},
  pages     = {7391-7397},
  title     = {Wing fold and twist greatly improves flight efficiency for bat-scale flapping wing robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Microspine-rubber composite for high friction on smooth,
rough, and wet surfaces. <em>IROS</em>, 7384–7390. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As robotic technologies advance and robots move out of factories and labs into the real world, grip on a variety of surfaces (e.g. smooth or rough) in a variety of conditions (e.g. dry or wet) becomes increasingly important. Bioinspired &quot;microspines&quot; have been previously explored, but primarily for vertical climbing applications or for small-scale robots applying low forces (less than 1 N). Further, these works primarily focused on rough surfaces. To advance this area of research, we present a composite material comprising high- friction rubber and compliant nitinol microspines which can passively retract below the surface of the rubber. We show that the composite can support large loads (greater than 75 N) with a high coefficient of friction on both smooth and rough surfaces (p &gt; 1.1), outperforming microspines alone on smooth surfaces and rubber alone on rough surfaces, especially when wet and oily. Further, due to the retraction of the microspines, the composite does not damage relatively soft, smooth surfaces, like wood flooring. We also test durability, and show that it is improved by microspine compliance, and test the effects of varying microspine diameter, angle, and tip shape. Finally, we demonstrate that a small RC car can climb steeper slopes and stop more quickly in wet conditions with microspines.},
  archive   = {C_IROS},
  author    = {Constance C. Berdan and Bryan G. Johnson and Elliot W. Hawkes},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636229},
  pages     = {7384-7390},
  title     = {Microspine-rubber composite for high friction on smooth, rough, and wet surfaces},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simulating ocean wave movement in a soft pneumatic surface.
<em>IROS</em>, 7378–7383. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It is well understood that nature has a calming effect on us. But in a physical space remote from nature, might the robotic embodiment of a natural phenomenon have the same effect? To address this question, we have simulated the soothing movement of ocean waves in a soft robotic surface, both as a simulation and in a physical prototype. In this paper, we report on our modeling methods of this natural phenomenon, we present the application of a selected model to both a simulation of the predicted inflation pattern of the soft robot surface and to the physical soft robot surface, and we compare the behavior of the physical robot to our simulation as a means to validate our design. We observed that the prototype’s frequency spectrum successfully shared the general trend and shape as our mathematical model. This research introduces a novel robot for a novel application: a soft robot to promote restoration and help regulate mental wellbeing in tight physical confines.},
  archive   = {C_IROS},
  author    = {Alexandra W. Steelman and Elena B. Sabinson and Isha Pradhan and Aratrika Ghatak and Keith E. Green},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636056},
  pages     = {7378-7383},
  title     = {Simulating ocean wave movement in a soft pneumatic surface},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quasi-static motion of a new serial snake-like robot on a
water surface: A geometrical approach. <em>IROS</em>, 7372–7377. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper reports methods to compute the equilibrium stances of a new snake-like robot designed to stabilize its head on a free water surface. To adjust rapidly the stability of the robot, this bio-inspired robot can rotate independently each body-shell, and modify the level of immersion of each module. To predict the stable stance accessible by this additional degree of freedom, a model is developed to compute the equilibrium configurations of the robot from a given parametrization of the body shape. Then, an algorithm is introduced to compute a sequence of controlled body deformations, such that the head configuration relatively to the water surface remains unchanged. Finally, we explore in simulation stances and quasi-static gaits, and investigate to what extent the buoyancy and the body deformations can be used to stabilize the head of the snake-like robot.},
  archive   = {C_IROS},
  author    = {Xiao Xie and Johann Herault and Étienne Clement and Vincent Lebastard and Frédéric Boyer},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636073},
  pages     = {7372-7377},
  title     = {Quasi-static motion of a new serial snake-like robot on a water surface: A geometrical approach},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Water surface stability prediction of amphibious
bio-inspired undulatory fin robot. <em>IROS</em>, 7365–7371. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To solve the interference problems of wind and wave action and load movement when switching under water surface conditions in the marine environment, a study on the water surface stability prediction of the bio-inspired undulatory fin robot is carried out. Based on the fin motion equation and fluid drag theory, a water surface stability calculation model of the robot is established. The study compares the effects of different loads and heel angles on the stability of the robot&#39;s water surface under different calculation methods and verifies the validity of the model through computational fluid dynamics methods. The simulation results show that the water surface stability of the robot exhibits sinusoidal-like changes over time, which is equal to the undulatory fin period. The stability decreases with the increase of the drainage volume. When the drainage volume is constant, the stability first increases and then decreases with the increase in heel angle. The theoretical calculation results are consistent with the numerical results, which verify the effectiveness of the water surface stability prediction model proposed in this paper. It can provide a theoretical basis for the optimization design of water surface stability of the undulatory fin robot.},
  archive   = {C_IROS},
  author    = {Zhenhan Chen and Qiao Hu and Yingliang Chen and Chang Wei and Shenglin Yin},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636182},
  pages     = {7365-7371},
  title     = {Water surface stability prediction of amphibious bio-inspired undulatory fin robot},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computationally affordable hierarchical framework for
humanoid robot control. <em>IROS</em>, 7349–7356. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a hierarchical control framework for generating versatile motions by a humanoid robot. The central feature of our framework is computational affordability: a large amount of computation time is allowable in the upper-level hierarchy. Consequently, whole-body trajectory optimization for a long time horizon becomes feasible. To ensure such affordability, a fast feedback loop is established in the lower-level hierarchy to increase the robustness against the large latency in the upper level. We experimentally examined the advantages of the achieved computational affordability. Our framework allowed a large computational time of 100 ms in each control cycle. This enables online trajectory optimization to predict 50 time steps ahead while taking full-body dynamics into account. Due to such a long prediction range, 20 motions were successfully generated in real time with our computation-ally affordable framework.},
  archive   = {C_IROS},
  author    = {Koji Ishihara and Jun Morimoto},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636013},
  pages     = {7349-7356},
  title     = {Computationally affordable hierarchical framework for humanoid robot control},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). State estimation of a partially observable multi-link system
with no joint encoders incorporating external dead-reckoning.
<em>IROS</em>, 7342–7348. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a technique for state estimation of a multi-link system having no joint encoders, which can only be partially observed by a camera. To fully observe the system without changing the current configuration, a gyroscope and an accelerometer are attached to each link as dead-reckoning sensors. Observations of the dead-reckoning sensors are associated with the states of the multi-link system such that the states are fully observable. The camera, which observes part of the system globally, is used as a global corrector in the framework of an extended Kalman filter to filter the dead-reckoning errors accumulated over time. Parametric studies in simulation have investigated and identified the efficacy of the proposed technique in estimating the state of the multilink system. Experimental validation using a two-link arm has demonstrated the applicability of the proposed technique to real-world multi-link systems.},
  archive   = {C_IROS},
  author    = {Tomonari Furukawa and J. Josiah Steckenrider and Gamini Dissanayake},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636132},
  pages     = {7342-7348},
  title     = {State estimation of a partially observable multi-link system with no joint encoders incorporating external dead-reckoning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards human haptic gesture interpretation for robotic
systems. <em>IROS</em>, 7334–7341. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Physical human-robot interactions (pHRI) are less efficient and communicative than human-human interactions, and a key reason is a lack of informative sense of touch in robotic systems. Interpreting human touch gestures is a nuanced, challenging task with extreme gaps between human and robot capability. Among prior works that demonstrate human touch recognition capability, differences in sensors, gesture classes, feature sets, and classification algorithms yield a conglomerate of non-transferable results and a glaring lack of a standard. To address this gap, this work presents 1) four proposed touch gesture classes that cover an important subset of the gesture characteristics identified in the literature, 2) the collection of an extensive force dataset on a common pHRI robotic arm with only its internal wrist force-torque sensor, and 3) an exhaustive performance comparison of combinations of feature sets and classification algorithms on this dataset. We demonstrate high classification accuracies among our proposed gesture definitions on a test set, emphasizing that neural network classifiers on the raw data outperform other combinations of feature sets and algorithms. Accompanying video is here. 1},
  archive   = {C_IROS},
  author    = {Bibit Bianchini and Prateek Verma and J. Kenneth Salisbury},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636015},
  pages     = {7334-7341},
  title     = {Towards human haptic gesture interpretation for robotic systems},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Organization and understanding of a tactile information
dataset TacAct for physical human-robot interaction. <em>IROS</em>,
7328–7333. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human touching the robot to convey intentions or emotions is an essential communication pathway during physical Human-Robot Interaction (pHRI). Therefore, advanced service robots require superior tactile intelligence to guarantee naturalness and safety when making physical contact with human subjects. Tactile intelligence is the capability to percept and recognize tactile information from touch behaviors, in which understanding the physical meaning of touching actions is crucial. For this purpose, this report introduces a recently collected and organized dataset &quot;TacAct&quot; that encloses real-time tactile information when human subjects touched the test device mimicking a robot forearm. The dataset contains 12 types of 24,000 touch actions from 50 subjects. The dataset details are described, the data are preliminarily analyzed, and the validity of the dataset is tested through a convolutional neural network LeNet-5 which classifying different types of touch actions. We believe that the TacAct dataset would be beneficial for the community to understand the touch intention under various circumstances and to develop learning-based intelligent algorithms for different applications.},
  archive   = {C_IROS},
  author    = {Peng Wang and Jixiao Liu and Funing Hou and Dicai Chen and Zihou Xia and Shijie Guo},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636389},
  pages     = {7328-7333},
  title     = {Organization and understanding of a tactile information dataset TacAct for physical human-robot interaction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multitask variational autoencoding of human-to-human object
handover. <em>IROS</em>, 7315–7320. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Assistive robots that operate alongside humans require the ability to understand and replicate human behaviours during a handover. A handover is defined as a joint action between two participants in which a giver hands an object over to the receiver. In this paper, we present a method for learning human-to-human handovers observed from motion capture data. Given the giver and receiver pose from a single timestep, and the object label in the form of a word embedding, our Multitask Variational Autoencoder jointly forecasts their pose as well as the orientation of the object held by the giver at handover. Our method is in large contrast to existing works for human pose forecasting that employ deep autoregressive models requiring a sequence of inputs. Furthermore, our method is novel in that it learns both the human pose and object orientation in a joint manner. Experimental results on the publicly available Handover Orientation and Motion Capture Dataset show that our proposed method outperforms the autoregressive baselines for handover pose forecasting by approximately 20\% while being on-par for object orientation prediction with a runtime that is 5x faster. a},
  archive   = {C_IROS},
  author    = {Haziq Razali and Yiannis Demiris},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636221},
  pages     = {7315-7320},
  title     = {Multitask variational autoencoding of human-to-human object handover},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inferring goals with gaze during teleoperated manipulation.
<em>IROS</em>, 7307–7314. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Assistive robot manipulators help people with upper motor impairments perform tasks by themselves. However, teleoperating a robot to perform complex tasks is difficult. Shared control algorithms make this easier: these algorithms predict the user’s goal, autonomously generate a plan to accomplish the goal, and fuse that plan with the user’s input. To accurately predict the user’s goal, these algorithms typically use the user’s input command (e.g., joystick input) directly. We use another sensing modality: the user’s natural eye gaze behavior, which is highly task-relevant and informative early in the task. We develop an algorithm using hidden Markov models to infer goals from natural eye gaze behavior that appears while users are teleoperating a robot. We show that gaze-based predictions outperform goal prediction based on the control input and that our sequence model improves the prediction quality relative to gaze-based aggregate models.},
  archive   = {C_IROS},
  author    = {Reuben M. Aronson and Nadia Almutlak and Henny Admoni},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636551},
  pages     = {7307-7314},
  title     = {Inferring goals with gaze during teleoperated manipulation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A conceptual approach of passive human-intention-orientated
variable admittance control using power envelope. <em>IROS</em>,
7300–7306. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Two main challenges that need to be addressed in physical human-robot interaction (pHRI) are efficient recognition of human intention and interaction safety. In this paper, a general human intention framework was summarized, firstly, according to the robot&#39;s roles: a passive follower and a compliant leader. Secondly, we proposed variable admittance control models governed by human intentions. Power envelope approaches were then proposed to impose constraints on the variable admittance parameters inferred from human intention for maintaining passivity conservatively. Our passivity preserving approaches were validated via simulation and shown to avoid mismatching of time-varying admittance parameters that restrain drastic changes of admittance controller dynamics, which usually result in instability. Finally, the relationship between the robot&#39;s passivity and stability when it interacts with the human was analyzed.},
  archive   = {C_IROS},
  author    = {Jingdong Chen and Paul I. Ro},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636487},
  pages     = {7300-7306},
  title     = {A conceptual approach of passive human-intention-orientated variable admittance control using power envelope},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Task geometry aware assistance for kinesthetic teaching of
redundant robots. <em>IROS</em>, 7285–7291. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Kinesthetic teaching allows the direct skill transfer from the human to the robot through physical human-robot interaction. However, it is heavily affected by the robot’s dynamics and the control scheme utilized for the physical interaction. In this work, we aim at assisting the human-teacher by reducing her/his physical and cognitive load. To this aim, we propose a controller with virtual fixtures and inertia optimization for assisting kinesthetic teaching, exploiting knowledge of the task geometry and the robot redundancy. Experimental results utilizing a KUKA LWR4+ robot for the teaching of a brush painting motion on a curved surface validate the method and demonstrate its performance in comparison with a gravity compensation scheme and the utilization of virtual fixtures alone. The system is proved to be passive under the exertion of a human force.},
  archive   = {C_IROS},
  author    = {Dimitrios Papageorgiou and Sotiris Stavridis and Christos Papakonstantinou and Zoe Doulgeri},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636209},
  pages     = {7285-7291},
  title     = {Task geometry aware assistance for kinesthetic teaching of redundant robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Contact anticipation for physical human–robot interaction
with robotic manipulators using onboard proximity sensors.
<em>IROS</em>, 7255–7262. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a framework that unites obstacle avoidance and deliberate physical interaction for robotic manipulators. As humans and robots begin to coexist in work and household environments, pure collision avoidance is insufficient, as human–robot contact is inevitable and, in some situations, desired. Our work enables manipulators to anticipate, detect, and act on contact. To achieve this, we allow limited deviation from the robot’s original trajectory through velocity reduction and motion restrictions. Then, if contact occurs, a robot can detect it and maneuver based on a novel dynamic contact thresholding algorithm. The core contribution of this work is dynamic contact thresholding, which allows a manipulator with onboard proximity sensors to track nearby objects and reduce contact forces in anticipation of a collision. Our framework elicits natural behavior during physical human–robot interaction. We evaluate our system on a variety of scenarios using the Franka Emika Panda robot arm; collectively, our results demonstrate that our contribution is not only able to avoid and react on contact, but also anticipate it.},
  archive   = {C_IROS},
  author    = {Caleb Escobedo and Matthew Strong and Mary West and Ander Aramburu and Alessandro Roncone},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636130},
  pages     = {7255-7262},
  title     = {Contact anticipation for physical Human–Robot interaction with robotic manipulators using onboard proximity sensors},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Force control with friction compensation in a pneumatic
gripper. <em>IROS</em>, 7231–7237. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots can grasp, even manipulate, objects with different shape, weight and size thanks to the their end-effectors. These are mostly constituted by two fingers, and are known as grippers. However, despite being quite simple for human beings, manipulation is not so straightforward to carry out on robotic systems. One of the main obstacles is the lack of reliable control methods: this is especially true for pneumatic grippers. Such devices are often mounted on industrial robots, though their behavior does not go beyond basic fully-open or fully-closed operations. This happens also as a consequence of the incapability of taking into account frictional effects limiting the pneumatic gripper performance. In this article, a new control strategy is delivered to solve this issue. The proposed strategy allows controlling the grasping force of a pneumatic gripper, without performance degradation due to friction. A pneumatic gripper was built and instrumented with several sensors to experimentally validate the proposed control strategy. The gripper was mechanically connected to a robotic arm and tested with different desired force profiles upon a wide range of force.},
  archive   = {C_IROS},
  author    = {Rocco A. Romeo and Agata Zocco and Luca Fiorio and Daniele Pucci and M. Maggiali},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636027},
  pages     = {7231-7237},
  title     = {Force control with friction compensation in a pneumatic gripper},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Synergetic gait prediction for stroke rehabilitation with
varying walking speeds. <em>IROS</em>, 7231–7327. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Lower Limb Exoskeletons (LLEs) are promising in gait rehabilitation for stroke survivors. In gait training of post-stroke patients with LLEs, one of the main challenges is how to generate appropriate gait patterns from the sound leg to the paretic leg for different patients with varying walking speeds. In this paper, we proposed a Synergetic Gait Prediction (SGP) model for rehabilitation LLEs with post-stroke patients, which can generate adaptive synergetic gait patterns for different patients with varying walking speeds. The proposed SGP model is based on Sequence-to-Sequence (Seq2Seq) neural networks with temporal attention mechanisms. In the training procedure of the proposed SGP model, a gait database with collected gait patterns from healthy subjects is employed to learn the parameters of SGP model. The SGP model takes current joint angles from the sound leg and a segment of observed history joint angles from both legs as input and predicts the future joint angles for the paretic leg. We compared the effectiveness of the SGP model with the Long Short Term Memory (LSTM) model, experimental results indicate that SGP model can generate synergetic gait patterns for different subjects via varying walking speeds with less prediction error.},
  archive   = {C_IROS},
  author    = {Chaobin Zou and Rui Huang and Zhinan Peng and Jing Qiu and Hong Cheng},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635860},
  pages     = {7231-7327},
  title     = {Synergetic gait prediction for stroke rehabilitation with varying walking speeds},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). A computational framework for robot hand design via
reinforcement learning. <em>IROS</em>, 7216–7222. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot hand is essential for a fully functional robot and designing a good robot hand is a sophisticated job that challenges the designer’s knowledge and experience. This paper presents a computational framework for automatic optimal robot hand design based on reinforcement learning (RL), which considers desired grasping tasks, grasp control strategies, and performance quality measures altogether. The RL-based framework intends to grow finger joints with different types and link lengths at different positions from null. Then, the reward function for such a growing action is defined in terms of quality indexes of the generated robot hand to perform desired grasping tasks under expected control strategies. To demonstrate the effectiveness of this framework, in this paper we set the desired task to simply grasping objects of three primitive shapes (i.e., box, cylinder, and sphere) with predefined hand positions and strategies to close fingers to achieve grasps for each object. The force closure condition, quantitative stability indexes, and energy consumption of grasps as well as some penalty terms are used to assemble the reward function. Through simulation and practical prototype experiments, we show that capable robot hands can be automatically generated by the proposed framework. Potential factors that affect the output of the framework and deserve further exploration are also discussed.},
  archive   = {C_IROS},
  author    = {Zhong Zhang and Yu Zheng and Zhe Hu and Lezhang Liu and Xuan Zhao and Xiong Li and Jia Pan},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636305},
  pages     = {7216-7222},
  title     = {A computational framework for robot hand design via reinforcement learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A dexterous, reconfigurable, adaptive robot hand combining
anthropomorphic and interdigitated configurations. <em>IROS</em>,
7209–7215. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot grasping and dexterous, in-hand manipulation allow robots to interact with their surroundings and execute a plethora of complex tasks such as pushing buttons, opening doors, and interacting with electrical appliances. In robotics, such complicated tasks are typically executed by multi-fingered end-effectors that are heavy, rigid, and expensive, employing numerous degrees of freedom and actuation. In this paper, we focus on the analysis, design, and development of a multi-grasp, reconfigurable, five fingered, anthropomorphic robot hand that can facilitate the execution of both robust grasping and dexterous manipulation tasks in service robotics and industrial automation applications. The robot hand is composed of eight actuators driving eighteen degrees of freedom with a telescoping mechanism and opposable thumb and pinky fingers to produce multiple anthropomorphic and non-anthropomorphic configurations for grasping and manipulation tasks. The reconfigurable finger base frames allow the hand to transform and utilize its degrees of actuation in an optimal manner to overcome its underactuated limitations. The underactuated robot hand is designed with a human hand structure that takes advantage of objects specifically designed for human operation (e.g., tool or handles with ergonomics for the human hand). This allows the system to better operate within a human-centered environment. The effectiveness of the proposed device is experimentally validated through three different tests: i) grasping experiments involving everyday-life objects, ii) force experiments that assess the force exertion capabilities of the hand in different finger base frame configurations, and iii) demonstration of in-hand object manipulation capabilities. The proposed hand weighs 1.28 kg and has a cost of approximately $1920 USD. The device is capable of exerting up to 14.3 N of contact force during pinch grasping and a maximum of 150.6 N power grasping.},
  archive   = {C_IROS},
  author    = {Geng Gao and Jayden Chapman and Saori Matsunaga and Toshisada Mariyama and Bruce MacDonald and Minas Liarokapis},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636538},
  pages     = {7209-7215},
  title     = {A dexterous, reconfigurable, adaptive robot hand combining anthropomorphic and interdigitated configurations},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The role of digit arrangement in soft robotic in-hand
manipulation. <em>IROS</em>, 7201–7208. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The need for robotic hands capable of gentle in-hand manipulation is growing rapidly as robots enter the real world. In this work, we show that the arrangement of digits in a soft robotic hand has a strong effect on in-hand manipulation capabilities. Introducing task-based performance metrics which quantify the range of motion, repeatability, and accuracy of in-hand manipulation tasks, we investigate hand designs with finger arrangements ranging from axisymmetric-circular to anthropomorphic. Using an open-source soft robot simulator, the effect of object size and aspect ratio on the in-hand manipulation performance is studied for a variety of finger arrangements, and findings are validated using a physical hardware platform. We found that the ideal finger arrangement is task-dependent; anthropomorphic arrangements excel at lateral translations, and axisymmetric arrangements are best suited for rotations. The aspect ratio of the object also has a strong effect on in-hand manipulation, with anthropomorphic designs performing best on objects of high aspect ratio, and axisymmetric arrangements doing well on objects of low aspect ratio. These findings are further confirmed in a real-world task with delicate pastries, where gentle in-hand manipulation is critical. Overall, our results suggest that active control of digit arrangement is necessary for soft robotic hands to maximize in-hand manipulation capabilities with arbitrary objects.},
  archive   = {C_IROS},
  author    = {Clark B. Teeple and Randall C. St. Louis and Moritz A. Graule and Robert J. Wood},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636188},
  pages     = {7201-7208},
  title     = {The role of digit arrangement in soft robotic in-hand manipulation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A caging inspired gripper using flexible fingers and a
movable palm. <em>IROS</em>, 7195–7200. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes the design of a robotic gripper motivated by the bin-picking problem, where a variety of objects need to be picked from cluttered bins. The presented gripper design focuses on an enveloping cage-like approach, which surrounds the object with three hooked fingers, and then presses into the object with a movable palm. The fingers are flexible and imbue grasps with some elasticity, helping to conform to objects and, crucially, adding friction to cases where an object cannot be caged. This approach proved effective on a set of basic shapes, such as cuboids and cylinders, in which every object could be grasped. In particular, flat bottom parts could be grasped in a very stable manner, as demonstrated by testing grasps with multiple 5N and 10N disturbances. A set of supermarket items were also tested, highlighting promising features such as effective grasping of fruits and vegetables, as well as some limitations in the current embodiment, which is not always able to slip the fingers underneath objects.},
  archive   = {C_IROS},
  author    = {Luke Beddow and Helge Wurdemann and Dimitrios Kanoulas},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635873},
  pages     = {7195-7200},
  title     = {A caging inspired gripper using flexible fingers and a movable palm},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating the shape of soft pneumatic actuators using
active vibroacoustic sensing. <em>IROS</em>, 7189–7194. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft robotic devices, including actuators fabricated from materials with a low modulus of elasticity, such as silicone elastomers, have gained significant interest in recent years. A flexible sensor is a vital component for estimating the conditions of soft actuators, such as shape, and deformation due to contact events. However, it is challenging to develop a flexible sensor with tolerability and versatility for soft actuators. Additionally, when an embedded sensor is employed, the fabrication process becomes complex. Therefore, to have tolerability and to increase versatility, we propose a method for estimating the shape of a soft pneumatic actuator based on vibroacoustic sensing. We employ a data-driven approach by utilizing several machine-learning techniques; hence, the proposed method could be applied to other types of actuators without employing any specific sensor or changing the fabrication process. The convolutional neural network is used as one of the dominant techniques, and huge annotated datasets are required. However, datasets can be obtained automatically using a robotic system, and estimation targets can be changed smoothly by switching datasets. We confirmed the feasibility, and versatility of the proposed method through several evaluation experiments. The bending angle can be estimated in real-time using active vibroacoustic, by emitting sweep signals. The mean error of bending angle estimation was between 5° to 10°. Furthermore, the proposed method is applied to a tensile actuator, and the mean error of the estimated length was approximately 0.21 mm.},
  archive   = {C_IROS},
  author    = {Kazumi Randika and Kentaro Takemura},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636527},
  pages     = {7189-7194},
  title     = {Estimating the shape of soft pneumatic actuators using active vibroacoustic sensing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic hand gesture recognition using a stretchable
multi-layer capacitive array, proximity sensing, and a SVM classifier.
<em>IROS</em>, 7183–7188. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hand Gesture Recognition (HGR) has application in Human Machine Interfaces (HMIs), to control robots, games, and machines. Here we demonstrate a soft-matter multi-layer printed electronic circuit, that can be used to detect the human gesture without the need for physical contact, except for unlocking the system. The film is able to detect touch and proximity of the hand at various nodes, and thus is able to construct an image of the hand. This is performed through fabrication of a thin, stretchable, high resolution, and multilayer circuit, that embeds a mesh of capacitive sensors for human gesture detection, through proximity sensing. The film is fabricated by superposing 6 layers: 3 insulating polymer layers, two EGaIn liquid metal layers patterned in shape of a mesh, and one flexible printed circuit board (FPCB) layer. The fabrication technique absed on spray deposition of EGaIn liquid metal is fast, low-cost, and easy, and can be used for scalable fabrication of large area multi-node sensing surfaces, with application in Robotic e-skins, and general Human Machine Interfaces. The system is trained with five gestures, including dynamic gestures. We demonstrate classification of these gestures using a support vector machine (SVM) based algorithm.},
  archive   = {C_IROS},
  author    = {Matteo Virone and Pedro Lopes and Rui Pedro Rocha and Anibal. T. de Almeida and Mahmoud Tavakoli},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636744},
  pages     = {7183-7188},
  title     = {Dynamic hand gesture recognition using a stretchable multi-layer capacitive array, proximity sensing, and a SVM classifier},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DMotion: Robotic visuomotor control with unsupervised
forward model learned from videos. <em>IROS</em>, 7135–7142. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning an accurate model of the environment is essential for model-based control tasks. Existing methods in robotic visuomotor control usually learn from data with heavily labelled actions, object entities or locations, which can be demanding in many cases. To cope with this limitation, we propose a method, dubbed DMotion, that trains a forward model from video data only, via disentangling the motion of controllable agent to model the transition dynamics. An object extractor and an interaction learner are trained in an end-to-end manner without supervision. The agent’s motions are explicitly represented using spatial transformation matrices containing physical meanings. In the experiments, DMotion achieves superior performance on learning an accurate forward model in a Grid World environment, as well as a more realistic robot control environment in simulation. With the accurate learned forward models, we further demonstrate their usage in model predictive control as an effective approach for robotic manipulations. Code, video and more materials are available at: https://hyperplane-lab.github.io/dmotion.},
  archive   = {C_IROS},
  author    = {Haoqi Yuan and Ruihai Wu and Andrew Zhao and Haipeng Zhang and Zihan Ding and Hao Dong},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636362},
  pages     = {7135-7142},
  title     = {DMotion: Robotic visuomotor control with unsupervised forward model learned from videos},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Particle MPC for uncertain and learning-based control.
<em>IROS</em>, 7127–7134. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As robotic systems move from highly structured environments to open worlds, incorporating uncertainty from dynamics learning or state estimation into the control pipeline is essential for robust performance. In this paper we present a nonlinear particle model predictive control (PMPC) approach to control under uncertainty, which directly incorporates any particle-based uncertainty representation, such as those common in robotics. Our approach builds on scenario methods for MPC, but in contrast to existing approaches, which either constrain all or only the first timestep to share actions across scenarios, we investigate the impact of a partial consensus horizon. Implementing this optimization for nonlinear dynamics by leveraging sequential convex optimization, our approach yields an efficient framework that can be tuned to the particular information gain dynamics of a system to mitigate both over-conservatism and over-optimism. We investigate our approach for two robotic systems across three problem settings: time-varying, partially observed dynamics; sensing uncertainty; and model-based reinforcement learning, and show that our approach improves performance over baselines in all settings.},
  archive   = {C_IROS},
  author    = {Robert Dyro and James Harrison and Apoorva Sharma and Marco Pavone},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635967},
  pages     = {7127-7134},
  title     = {Particle MPC for uncertain and learning-based control},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel quotient space approach to model-based fault
detection and isolation: Theory and preliminary simulation evaluation.
<em>IROS</em>, 7119–7126. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We report the development of novel fault detection and isolation (FDI) methods for model-based fault detection (MB-FD) and quotient-space fault isolation (QS-FI). This FDI approach performs MB-FD and QS-FI of single or multiple con-current faults in plants and actuators simultaneously, without a priori knowledge of fault form, type, or dynamics. To detect faults, MB-FD characterizes deviation from nominal behavior using the plant velocity and plant and actuator parameters estimated by nullspace-based adaptive identification. To isolate (i.e. identify) faults, the QS-FI algorithm compares the estimated parameters to a nominal parameter class in progressively decreasing-dimensional quotient spaces of the parameter space. A preliminary simulation study of these proposed FDI methods applied to a three degree-of-freedom uninhabited underwater vehicle plant model shows their ability to detect as well as isolate faults for the cases of both single and multiple simultaneous faults and suggests the generalizability of the MB-FD and QS-FI approaches to any well-defined second-order plant and actuator model whose parameters enter linearly: a broad class of systems which includes aerial vehicles, marine vehicles, spacecraft, and robot arms.},
  archive   = {C_IROS},
  author    = {Annie M. Mao and Louis L. Whitcomb},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636026},
  pages     = {7119-7126},
  title     = {A novel quotient space approach to model-based fault detection and isolation: Theory and preliminary simulation evaluation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guiding robot model construction with prior features.
<em>IROS</em>, 7112–7118. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Virtually all robot control methods benefit from the availability of an accurate mathematical model of the robot. However, obtaining a sufficient amount of informative data for constructing dynamic models can be difficult, especially when the models are to be learned during robot deployment. Under such circumstances, standard data-driven model learning techniques often yield models that do not comply with the physics of the robot. We extend a symbolic regression algorithm based on Single Node Genetic Programming by including the prior model information into the model construction process. In this way, symbolic regression automatically builds models that compensate for theoretical or empirical model deficiencies. We experimentally demonstrate the approach on two real-world systems: the TurtleBot 2 mobile robot and the Parrot Bebop 2 drone. The results show that the proposed model-learning algorithm produces realistic models that fit well the training data even when using small training sets. Passing the prior model information to the algorithm significantly improves the model accuracy while speeding up the search.},
  archive   = {C_IROS},
  author    = {Erik Derner and Jiří Kubalík and Robert Babuška},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635831},
  pages     = {7112-7118},
  title     = {Guiding robot model construction with prior features},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A robust data-driven approach for dynamics model
identification in trajectory planning. <em>IROS</em>, 7104–7111. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a data-driven modelling framework using a sparse regression technique to find the governing equations of dynamics systems. With this approach, the prior knowledge of features from simple structures can be used to deduce which on complex structures. The prior knowledge of single-pendulums, double-pendulums, and spherical pendulum enlightens the guess of the feature library for a 3-DOF manipulator. The feature library is sparsifled with a fully autonomous machine learning algorithm composited of the L1-regularization and proportional filter. The training dataset with non-zero-mean Gaussian noise simulates real-world conditions and proves the system&#39;s robustness to the noise. Compared with the neural-network-based system identification method, this paper&#39;s technique can be promptly applied in dynamic trajectory planning. A simulation of the optimal trajectory planning for the obstacle-avoidance on the Lynxmotion robot is accomplished by optimizing the objective function constructed with energy and penalty function. Results of the simulation support that the estimated model works correctly. Comparison between the data-driven and the closed-form model evidences the reliability and robustness of this identification technique.},
  archive   = {C_IROS},
  author    = {Jiangqiu Chen and Minyu Zhang and Zhifei Yang and Linqing Xia},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635979},
  pages     = {7104-7111},
  title     = {A robust data-driven approach for dynamics model identification in trajectory planning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-scale aggregation with self-attention network for
modeling electrical motor dynamics. <em>IROS</em>, 7097–7103. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modeling induction motor dynamics is a crucial problem in the industry. The previous works mainly model the dynamics based on the physical model assumption and state equation. However, due to the complex internal structure of motors, the traditional methods cannot estimate dynamics precisely. To address this issue, we adopt a deep learning-based approach that takes the time-series motor data measured from the sensor to estimate the dynamics without making any assumptions about the motor’s interior. In this paper, we propose a multi-scale feature aggregation with self-attention network (MASNet) to deal with modeling motor dynamics. First, our model extracts multi-scale features from motor signals by various convolutional kernel sizes. Then, the proposed adaptive feature aggregation module fuses different receptive signals effectively. To further refine these high-level motor features, two-stream components, containing Bidirectional LSTM and self-attention module, are applied to improve motor contextual information. Moreover, we present a novel temporal relative loss to enhance consecutive signal consistency, which improves the performance of modeling dynamics. To deploy the service in the real-world scenario, our network is very lightweight and reduces the number of parameters from 0.62M to 0.09M (around 85\% reduction) but outperforms state-of-the-art algorithms by extensive experiments on simulation and real-world motor datasets.},
  archive   = {C_IROS},
  author    = {Kuan-Chih Huang and Hao-Hsiang Yang and Wei-Ting Chen},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636717},
  pages     = {7097-7103},
  title     = {Multi-scale aggregation with self-attention network for modeling electrical motor dynamics},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GloCAL: Glocalized curriculum-aided learning of multiple
tasks with application to robotic grasping. <em>IROS</em>, 7089–7096.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The domain of robotics is challenging to apply deep reinforcement learning due to the need for large amounts of data and for ensuring safety during learning. Curriculum learning has shown good performance in terms of sample-efficient deep learning. In this paper, we propose an algorithm (named GloCAL) that creates a curriculum for an agent to learn multiple discrete tasks, based on clustering tasks according to their evaluation scores. From the highest-performing cluster, a global task representative of the cluster is identified for learning a global policy that transfers to subsequently formed new clusters, while remaining tasks in the cluster are learnt as local policies. The efficacy and efficiency of our GloCAL algorithm are compared with other approaches in the domain of grasp learning for 49 objects with varied object complexity and grasp difficulty from the EGAD! dataset. The results show that GloCAL is able to learn to grasp 100\% of the objects, whereas other approaches achieve at most 86\% despite being given 1.5× longer training time.},
  archive   = {C_IROS},
  author    = {Anil Kurkcu and Cihan Acar and Domenico Campolo and Keng Peng Tee},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636492},
  pages     = {7089-7096},
  title     = {GloCAL: Glocalized curriculum-aided learning of multiple tasks with application to robotic grasping},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint intention and trajectory prediction based on
transformer. <em>IROS</em>, 7082–7088. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although autonomous driving technology has made tremendous progress in recent years, it is still challenging to predict the intentions and trajectories of pedestrians. The state-of-the-art methods suffer from two problems. (1) Existing works consider these two tasks separately, ignoring the connection between them. (2) The selection and integration of inputs for these tasks are not well designed. In this paper, these two tasks are taken into consideration in a unified model. In this way, the information provided by the labels of each other is shared, improving the performance of both tasks. Besides, in addition to the bounding boxes and speeds, orientation and road semantic segmentation features are taken into consideration to show the potential intention and road context of the pedestrian. And all the inputs are weighted by an attention module before integration. Meanwhile, a Transformer encoder is applied in our method to extract the temporal information from the fused feature sequence. Our method outperforms all previous models for both trajectory prediction and intention prediction tasks on the JAAD dataset and PIE dataset.},
  archive   = {C_IROS},
  author    = {Ze Sui and Yue Zhou and Xu Zhao and Ao Chen and Yiyang Ni},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636241},
  pages     = {7082-7088},
  title     = {Joint intention and trajectory prediction based on transformer},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DeepSIL: A software-in-the-loop framework for evaluating
motion planning schemes using multiple trajectory prediction networks.
<em>IROS</em>, 7075–7081. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Testing and verification is still an open issue on the way to fully automated driving. Simulations can help to reduce the required testing efforts, however, classical simulators based on physical models and heuristics, such as the intelligent driver model (IDM), show limited model accuracy on a microscopic scenario level. In turn, learning-based driver models are often capable to predict human driver’s behavior accurately, but are difficult to tailor such that they follow an intended scenario description. In this work, we propose a software-in-the-loop framework to combine a learned model with a rule-based logic layer and a kinematic vehicle model of a classical traffic simulator. Thus, the merits of both, classical simulators and learning-based models are exploited. We demonstrate with a case study of evaluating a motion planning scheme that the simulator fits well with the needs of testing such methods. Furthermore, we show by experiments with real-world traffic data from a traffic surveillance system that the proposed simulator yields realistic behavior of the simulated road users.},
  archive   = {C_IROS},
  author    = {Jan Strohbeck and Johannes Müller and Adrian Holzbock and Michael Buchholz},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636423},
  pages     = {7075-7081},
  title     = {DeepSIL: A software-in-the-loop framework for evaluating motion planning schemes using multiple trajectory prediction networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A simple and efficient multi-task network for 3D object
detection and road understanding. <em>IROS</em>, 7067–7074. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Detecting dynamic objects and predicting static road information such as drivable areas and ground heights are crucial for safe autonomous driving. Previous works studied each perception task separately, and lacked a collective quantitative analysis. In this work, we show that it is possible to perform all perception tasks via a simple and efficient multi-task network. Our proposed network, LidarMTL, takes raw LiDAR point cloud as inputs, and predicts six perception outputs for 3D object detection and road understanding. The network is based on an encoder-decoder architecture with 3D sparse convolution and deconvolution operations. Extensive experiments verify the proposed method with competitive accuracies compared to state-of-the-art object detectors and other task-specific networks. LidarMTL is also leveraged for online localization. Code and pre-trained model have been made available at https://github.com/frankfengdi/LidarMTL.},
  archive   = {C_IROS},
  author    = {Di Feng and Yiyang Zhou and Chenfeng Xu and Masayoshi Tomizuka and Wei Zhan},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635858},
  pages     = {7067-7074},
  title     = {A simple and efficient multi-task network for 3D object detection and road understanding},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RV-FuseNet: Range view based fusion of time-series LiDAR
data for joint 3D object detection and motion forecasting.
<em>IROS</em>, 7060–7066. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robust real-time detection and motion forecasting of traffic participants is necessary for autonomous vehicles to safely navigate urban environments. In this paper, we present RV-FuseNet, a novel end-to-end approach for joint detection and trajectory estimation directly from time-series LiDAR data. Instead of the widely used bird’s eye view (BEV) representation, we utilize the native range view (RV) representation of LiDAR data. The RV preserves the full resolution of the sensor by avoiding the voxelization used in the BEV. Furthermore, RV can be processed efficiently due to its compactness. Previous approaches project time-series data to a common viewpoint for temporal fusion, and often this viewpoint is different from where it was captured. This is sufficient for BEV methods, but for RV methods, this can lead to loss of information and data distortion which has an adverse impact on performance. To address this challenge we propose a simple yet effective novel architecture, Incremental Fusion, that minimizes the information loss by sequentially projecting each RV sweep into the viewpoint of the next sweep in time. We show that our approach significantly improves motion forecasting performance over the existing state-of-the-art. Furthermore, we demonstrate that our sequential fusion approach is superior to alternative RV based fusion methods on multiple datasets.},
  archive   = {C_IROS},
  author    = {Ankit Laddha and Shivam Gautam and Gregory P. Meyer and Carlos Vallespi-Gonzalez and Carl K. Wellington},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636083},
  pages     = {7060-7066},
  title     = {RV-FuseNet: Range view based fusion of time-series LiDAR data for joint 3D object detection and motion forecasting},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semi-cooperative control for autonomous emergency vehicles.
<em>IROS</em>, 7052–7059. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous control of an emergency vehicle will save lives through faster transport and shorter response. Towards this goal, it must overcome the challenge of inter- acting with existing human drivers on the road. We present a game-theoretic approach for semi-cooperative control of an autonomous emergency vehicle that can interact efficiently with humans on the road. We model the interactions between autonomous and human driven cars with Social Value Orientation, a metric from social psychology, that allows the controller to leverage their influence on the trajectories of neighboring human drivers. In addition, by using a modified version of iterative best response, we direct the algorithm to converge to Nash equilibria that are cooperative. We demonstrate the efficacy of our algorithm in simulations of drivers in traffic, with a variety of traffic densities and driver personalities. In simulations of prosocial human drivers, our algorithm provides an 8\% improvement in distance-traveled compared to egoistic human drivers.},
  archive   = {C_IROS},
  author    = {Noam Buckman and Wilko Schwarting and Sertac Karaman and Daniela Rus},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636849},
  pages     = {7052-7059},
  title     = {Semi-cooperative control for autonomous emergency vehicles},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interpretable goal recognition in the presence of occluded
factors for autonomous vehicles. <em>IROS</em>, 7044–7051. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recognising the goals or intentions of observed vehicles is a key step towards predicting the long-term future behaviour of other agents in an autonomous driving scenario. When there are unseen obstacles or occluded vehicles in a scenario, goal recognition may be confounded by the effects of these unseen entities on the behaviour of observed vehicles. Existing prediction algorithms that assume rational behaviour with respect to inferred goals may fail to make accurate long-horizon predictions because they ignore the possibility that the behaviour is influenced by such unseen entities. We introduce the Goal and Occluded Factor Inference (GOFI) algorithm which bases inference on inverse-planning to jointly infer a probabilistic belief over goals and potential occluded factors. We then show how these beliefs can be integrated into Monte Carlo Tree Search (MCTS). We demonstrate that jointly inferring goals and occluded factors leads to more accurate beliefs with respect to the true world state and allows an agent to safely navigate several scenarios where other baselines take unsafe actions leading to collisions.},
  archive   = {C_IROS},
  author    = {Josiah P. Hanna and Arrasy Rahman and Elliot Fosong and Francisco Eiras and Mihai Dobre and John Redford and Subramanian Ramamoorthy and Stefano V. Albrecht},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635903},
  pages     = {7044-7051},
  title     = {Interpretable goal recognition in the presence of occluded factors for autonomous vehicles},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Diverse critical interaction generation for planning and
planner evaluation. <em>IROS</em>, 7036–7043. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generating diverse and comprehensive interacting agents to evaluate the decision-making modules is essential for the safe and robust planning of autonomous vehicles (AV). Due to efficiency and safety concerns, most researchers choose to train interactive adversary (competitive or weakly competitive) agents in simulators and generate test cases to interact with evaluated AVs. However, most existing methods fail to provide both natural and critical interaction behaviors in various traffic scenarios. To tackle this problem, we propose a styled generative model RouteGAN that generates diverse interactions by controlling the vehicles separately with desired styles. By altering its style coefficients, the model can generate trajectories with different safety levels serve as an online planner. Experiments show that our model can generate diverse interactions in various scenarios. We evaluate different planners with our model by testing their collision rate in interaction with RouteGAN planners of multiple critical levels.},
  archive   = {C_IROS},
  author    = {Zhao-Heng Yin and Lingfeng Sun and Liting Sun and Masayoshi Tomizuka and Wei Zhan},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636266},
  pages     = {7036-7043},
  title     = {Diverse critical interaction generation for planning and planner evaluation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PointSiamRCNN: Target-aware voxel-based siamese tracker for
point clouds. <em>IROS</em>, 7029–7035. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Currently, there have been many kinds of pointbased 3D trackers, while voxel-based methods are still underexplored. In this paper, we first propose a voxel-based tracker, named PointSiamRCNN, improving tracking performance by embedding target information into the search region. Our framework is composed of two parts for achieving proposal generation and proposal refinement, which fully releases the potential of the two-stage object tracking. Specifically, it takes advantage of efficient feature learning of the voxel-based Siamese network and high-quality proposal generation of the Siamese region proposal network head. In the search region, the groundtruth annotations are utilized to realize semantic segmentation, which leads to more discriminative feature learning with pointwise supervisions. Furthermore, we propose the Self and Cross Attention Module for embedding target information into the search region. Finally, the multi-scale RoI pooling module is proposed to obtain compact representations from target-aware features for proposal refinement. Exhaustive experiments on the KITTI tracking dataset demonstrate that our framework reaches the competitive performance with the state-of-the-art 3D tracking methods and achieves the state-of-the-art in terms of BEV tracking.},
  archive   = {C_IROS},
  author    = {Hao Zou and Chujuan Zhang and Yong Liu and Wanlong Li and Feng Wen and Hongbo Zhang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636863},
  pages     = {7029-7035},
  title     = {PointSiamRCNN: Target-aware voxel-based siamese tracker for point clouds},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic event camera calibration. <em>IROS</em>, 7021–7028.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Camera calibration is an important prerequisite towards the solution of 3D computer vision problems. Traditional methods rely on static images of a calibration pattern. This raises interesting challenges towards the practical usage of event cameras, which notably require image change to produce sufficient measurements. The current standard for event camera calibration therefore consists of using flashing patterns. They have the advantage of simultaneously triggering events in all reprojected pattern feature locations, but it is difficult to construct or use such patterns in the field. We present the first dynamic event camera calibration algorithm. It calibrates directly from events captured during relative motion between camera and calibration pattern. The method is propelled by a novel feature extraction mechanism for calibration patterns, and leverages existing calibration tools before optimizing all parameters through a multi-segment continuous-time formulation. As demonstrated through our results on real data, the obtained calibration method is highly convenient and reliably calibrates from data sequences spanning less than 10 seconds.},
  archive   = {C_IROS},
  author    = {Kun Huang and Yifu Wang and Laurent Kneip},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636398},
  pages     = {7021-7028},
  title     = {Dynamic event camera calibration},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CRACT: Cascaded regression-align-classification for robust
tracking. <em>IROS</em>, 7013–7020. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {High quality object proposals are crucial in visual tracking algorithms that utilize region proposal network (RPN). Refinement of these proposals, typically by box regression and classification in parallel, has been popularly adopted to boost tracking performance. However, it still meets problems when dealing with complex and dynamic background. Thus motivated, in this paper we introduce an improved proposal refinement module, Cascaded Regression-Align-Classification (CRAC), which yields new state-of-the-art performances on many benchmarks.First, having observed that the offsets from box regression can serve as guidance for proposal feature refinement, we design CRAC as a cascade of box regression, feature alignment and box classification. The key is to bridge box regression and classification via an alignment step, which leads to more accurate features for proposal classification with improved robustness. To address the variation in object appearance, we introduce an identification-discrimination component for box classification, which leverages offline reliable fine-grained template and online rich background information to distinguish the target from background. Moreover, we present pyramid RoIAlign that benefits CRAC by exploiting both the local and global cues of proposals. During inference, tracking proceeds by ranking all refined proposals and selecting the best one. In experiments on seven benchmarks including OTB-2015, UAV123, NfS, VOT-2018, TrackingNet, GOT-10k and LaSOT, our CRACT exhibits very promising results in comparison with state-of-the-art competitors and runs in real-time at 28 fps.},
  archive   = {C_IROS},
  author    = {Heng Fan and Haibin Ling},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636803},
  pages     = {7013-7020},
  title     = {CRACT: Cascaded regression-align-classification for robust tracking},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Powerline tracking with event cameras. <em>IROS</em>,
6990–6997. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous inspection of powerlines with quadrotors is challenging. Flights require persistent perception to keep a close look at the lines. We propose a method that uses event cameras to robustly track powerlines. Event cameras are inherently robust to motion blur, have low latency, and high dynamic range. Such properties are advantageous for autonomous inspection of powerlines with drones, where fast motions and challenging illumination conditions are ordinary. Our method identifies lines in the stream of events by detecting planes in the spatio-temporal signal, and tracks them through time. The implementation runs onboard and is capable of detecting multiple distinct lines in real time with rates of up to 320 thousand events per second. The performance is evaluated in real-world flights along a powerline. The tracker is able to persistently track the powerlines, with a mean lifetime of the line 10× longer than existing approaches.},
  archive   = {C_IROS},
  author    = {Alexander Dietsche and Giovanni Cioffi and Javier Hidalgo-Carrió and Davide Scaramuzza},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636824},
  pages     = {6990-6997},
  title     = {Powerline tracking with event cameras},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint multi-object detection and tracking with camera-LiDAR
fusion for autonomous driving. <em>IROS</em>, 6983–6989. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-object tracking (MOT) with camera-LiDAR fusion demands accurate results of object detection, affinity computation and data association in real time. This paper presents an efficient multi-modal MOT framework with online joint detection and tracking schemes and robust data association for autonomous driving applications. The novelty of this work includes: (1) development of an end-to-end deep neural network for joint object detection and correlation using 2D and 3D measurements; (2) development of a robust affinity computation module to compute occlusion-aware appearance and motion affinities in 3D space; (3) development of a comprehensive data association module for joint optimization among detection confidences, affinities and start-end probabilities. The experiment results on the KITTI tracking benchmark demonstrate the superior performance of the proposed method in terms of both tracking accuracy and processing speed.},
  archive   = {C_IROS},
  author    = {Kemiao Huang and Qi Hao},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636311},
  pages     = {6983-6989},
  title     = {Joint multi-object detection and tracking with camera-LiDAR fusion for autonomous driving},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enumeration of polyominoes &amp; polycubes composed of
magnetic cubes. <em>IROS</em>, 6977–6982. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper examines a family of designs for magnetic cubes and counts how many configurations are possible for each design as a function of the number of modules. Magnetic modular cubes are cubes with magnets arranged on their faces. The magnets are positioned so that each face has either magnetic south or north pole outward. Moreover, we require that the net magnetic moment of the cube passes through the center of opposing faces. These magnetic arrangements enable coupling when cube faces with opposite polarity are brought in close proximity and enable moving the cubes by controlling the orientation of a global magnetic field. This paper investigates the 2D and 3D shapes that can be constructed by magnetic modular cubes, and describes all possible magnet arrangements that obey these rules. We select ten magnetic arrangements and assign a &quot;color&quot; to each of them for ease of visualization and reference. We provide a method to enumerate the number of unique polyominoes and polycubes that can be constructed from a given set of colored cubes. We use this method to enumerate all arrangements for up to 20 modules in 2D and 16 modules in 3D. We provide a motion planner for 2D assembly and through simulations compare which arrangements require fewer movements to generate and which arrangements are more common. Hardware demonstrations explore the self-assembly and disassembly of these modules in 2D and 3D.},
  archive   = {C_IROS},
  author    = {Yitong Lu and Anuruddha Bhattacharjee and Daniel Biediger and MinJun Kim and Aaron T. Becker},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636784},
  pages     = {6977-6982},
  title     = {Enumeration of polyominoes &amp; polycubes composed of magnetic cubes},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Finding structure configurations for flying modular robots.
<em>IROS</em>, 6970–6976. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Flying Modular Structures offer a versatile mechanism that can change the arrangement of constituent actuators according to task requirements. In this work, we extend a modular aerial platform that can expand its actuation capabilities depending on the configuration. Each module is composed of a quadrotor in a cage that can rigidly connect with other modules. The quadrotor is connected with the cage by a revolute joint that allows it to rotate with respect to the cage. Modules located in the structure are either parallel or perpendicular to one another. The task specification defines forces and moments needed during the execution. We propose two search methods to find a configuration that can satisfy the specification. The first approach consists of an exhaustive search that yields optimal structure configurations by exploring the whole search space. The second approach proposes a heuristic based on subgroup search, reducing the problem complexity from exponential to linear. We validate our proposed algorithms with several simulations. Our results show that the proposed heuristic is computationally efficient and finds a near-optimal configuration even for flying modular structures composed of a large number of modules.},
  archive   = {C_IROS},
  author    = {Bruno Gabrich and David Saldaña and Mark Yim},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636086},
  pages     = {6970-6976},
  title     = {Finding structure configurations for flying modular robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-reconfiguration of modular robots using virtual forces.
<em>IROS</em>, 6948–6953. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Programmable matter is a material that can change its physical properties at will, whether it is its shape, density or conductivity. It can be implemented as an ensemble of micro-robots arranged in space to form a specific shape and having their own computing power. This technology behaves as a distributed system. Each micro-robot is called a module and the whole forms a modular robot. This paper tackles the self-reconfiguration problem by presenting a deterministic planning algorithm that can decide which positions can be filled over multiple iterations using virtual forces. The proposed algorithm implements the Hungarian method to optimize the planning by minimizing the total number of movements of the robots and preventing positions from being blocked. Each module embeds the same algorithm and coordinates with the others using neighbor-to-neighbor communications. Simulation results are conducted to show the effectiveness of the proposed approach.},
  archive   = {C_IROS},
  author    = {Edy Hourany and Christian Stephan and Abdallah Makhoul and Benoit Piranda and Bachir Habib and Julien Bourgeois},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635889},
  pages     = {6948-6953},
  title     = {Self-reconfiguration of modular robots using virtual forces},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Balloon animal robots: Reconfigurable isoperimetric inflated
soft robots. <em>IROS</em>, 6941–6947. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a new class of soft reconfigurable robot: balloon animal robots. The balloon animal robot consists of a closed volume inflatable tube which can be reconfigured into structures of varying topology by a collective of simple sub-unit robots. The robotic sub-units can (1) drive along the length of the tube to localize a joint, (2) create pinch points that locally reduce the bending stiffness of the tube to form a joint, and (3) selectively mechanically couple to one another through cable driven actuators to create nodes of the structure. In this work we introduce the hardware necessary to construct the robot, present experiments to guide the hardware design, and formulate an algorithm using graph theory to calculate the number of nodes and node connections needed to form different 2D shapes. Finally, we demonstrate the system with two active nodes and four passive nodes forming multiple 2D shapes from the same hardware.},
  archive   = {C_IROS},
  author    = {Anthony D. Stuart and Zachary M. Hammond and Sean Follmer},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635842},
  pages     = {6941-6947},
  title     = {Balloon animal robots: Reconfigurable isoperimetric inflated soft robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reconfiguring metamorphic robots via SMT: Is it a viable
way? <em>IROS</em>, 6935–6940. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a new approach to tackle the problem of lattice-type metamorphic robots reconfiguration. We base our approach on a reduction to satisfiability modulo theory (SMT). Unlike the current state-of-the-art solutions, we consider the spatial limitations of the modules themselves and produce collision-free plans. We give an in-depth description of the reduction and discuss several optimizations for our technique. We also show an experimental evaluation of our approach and list possible future improvements to our technique.},
  archive   = {C_IROS},
  author    = {Jan Mrázek and Martin Jonáš and Jiří Barnat},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636534},
  pages     = {6935-6940},
  title     = {Reconfiguring metamorphic robots via SMT: Is it a viable way?},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A sampling-based motion planning framework for complex motor
actions. <em>IROS</em>, 6928–6934. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a framework for planning complex motor actions such as pouring or scooping from arbitrary start states in cluttered real-world scenes. Traditional approaches to such tasks use dynamic motion primitives (DMPs) learned from human demonstrations. We enhance a recently proposed state-of-the-art DMP technique capable of obstacle avoidance by including them within a novel hybrid framework. This complements DMPs with sampling-based motion planning algorithms, using the latter to explore the scene and reach promising regions from which a DMP can successfully complete the task. Experiments indicate that even obstacle-aware DMPs suffer in task success when used in scenarios which largely differ from the trained demonstration in terms of the start, goal, and obstacles. Our hybrid approach significantly outperforms obstacle-aware DMPs by successfully completing tasks in cluttered scenes for a pouring task in simulation. We further demonstrate our method on a real robot for pouring and scooping tasks.},
  archive   = {C_IROS},
  author    = {Shlok Sobti and Rahul Shome and Swarat Chaudhuri and Lydia E. Kavraki},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636395},
  pages     = {6928-6934},
  title     = {A sampling-based motion planning framework for complex motor actions},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Using experience to improve constrained planning on
foliations for multi-modal problems. <em>IROS</em>, 6922–6927. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many robotic manipulation problems are multi-modal—they consist of a discrete set of mode families (e.g., whether an object is grasped or placed) each with a continuum of parameters (e.g., where exactly an object is grasped). Core to these problems is solving single-mode motion plans, i.e., given a mode from a mode family (e.g., a specific grasp), find a feasible motion to transition to the next desired mode. Many planners for such problems have been proposed, but complex manipulation plans may require prohibitively long computation times due to the difficulty of solving these underlying single-mode problems. It has been shown that using experience from similar planning queries can significantly improve the efficiency of motion planning. However, even though modes from the same family are similar, they impose different constraints on the planning problem, and thus experience gained in one mode cannot be directly applied to another. We present a new experience-based framework, ALEF, for such multi-modal planning problems. ALEF learns using paths from single-mode problems from a mode family, and applies this experience to novel modes from the same family. We evaluate ALEF on a variety of challenging problems and show a significant improvement in the efficiency of sampling-based planners both in isolation and within a multi-modal manipulation planner.},
  archive   = {C_IROS},
  author    = {Zachary Kingston and Constantinos Chamzas and Lydia E. Kavraki},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636236},
  pages     = {6922-6927},
  title     = {Using experience to improve constrained planning on foliations for multi-modal problems},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rough terrain navigation for legged robots using
reachability planning and template learning. <em>IROS</em>, 6914–6921.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Navigation planning for legged robots has distinct challenges compared to wheeled and tracked systems due to the ability to lift legs off the ground and step over obstacles. While most navigation planners assume a fixed traversability value for a single terrain patch, we overcome this limitation by proposing a reachability-based navigation planner for legged robots. We approximate the robot morphology by a set of reachability and body volumes, assuming that the reachability volumes need to always be in contact with the environment, while the body should be contact-free. We train a convolutional neural network to predict foothold scores which are used to restrict geometries which are considered suitable to step on. Using this representation, we propose a navigation planner based on probabilistic roadmaps. Through validation of only low-cost graph edges during graph expansion and an adaptive sampling scheme based on roadmap node density, we achieve real-time performance with fast update rates even in cluttered and narrow environments. We thoroughly validate the proposed navigation planner in simulation and demonstrate its performance in real-world experiments on the quadruped ANYmal.},
  archive   = {C_IROS},
  author    = {Lorenz Wellhausen and Marco Hutter},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636358},
  pages     = {6914-6921},
  title     = {Rough terrain navigation for legged robots using reachability planning and template learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Class-ordered LPA*: An incremental-search algorithm for
weighted colored graphs. <em>IROS</em>, 6907–6913. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Replanning is an essential problem for robots operating in a dynamic and complex environment for responsive and robust autonomy. Previous incremental-search algorithms efficiently reuse existing search results to facilitate a new plan when the environment changes. Yet, they rely solely on geometric information of the environment encoded in an edge-weighted graph. However, semantic information often provides valuable insights that cannot easily be captured quantitatively. We encode both semantic and geometric information of the environment in a weighted colored graph, in which the edges are partitioned into a finite set of ordered semantic classes (e.g., colors), and then we incrementally search for the shortest path among the set of paths with minimal inclusion of inferior classes, using information from the previous search using ideas similar to LPA*. The proposed Class-Ordered LPA* (COLPA*) algorithm inherits the strong theoretical properties of LPA*, namely, optimality and efficiency, but optimality now is with respect to the total path order. Numerical examples show that semantic information helps reduce the relevant search space in a dynamic environment.},
  archive   = {C_IROS},
  author    = {Jaein Lim and Oren Salzman and Panagiotis Tsiotras},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636736},
  pages     = {6907-6913},
  title     = {Class-ordered LPA*: An incremental-search algorithm for weighted colored graphs},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Orientation-aware planning for parallel task execution of
omni-directional mobile robot. <em>IROS</em>, 6891–6898. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Omni-directional mobile robot (OMR) systems have been very popular in academia and industry for their superb maneuverability and flexibility. Yet their potential has not been fully exploited, where the extra degree of freedom in OMR can potentially enable the robot to carry out extra tasks. For instance, gimbals or sensors on robots may suffer from a limited field of view or be constrained by the inherent mechanical design, which will require the chassis to be orientation-aware and respond in time. To solve this problem and further develop the OMR systems, in this paper, we categorize the tasks related to OMR chassis into orientation transition tasks and position transition tasks, where the two tasks can be carried out at the same time. By integrating the parallel task goals in a single planning problem, we proposed an orientation-aware planning architecture for OMR systems to execute the orientation transition and position transition in a unified and efficient way. A modified trajectory optimization method called orientation-aware timed-elastic-band (OATEB) is introduced to generate the trajectory that satisfies the requirements of both tasks. Experiments in both 2D simulated environments and real scenes are carried out. A four-wheeled OMR is deployed to conduct the real scene experiment and the results demonstrate that the proposed method is capable of simultaneously executing parallel tasks and is applicable to real-life scenarios.},
  archive   = {C_IROS},
  author    = {Cheng Gong and Zirui Li and Xingyu Zhou and Jiachen Li and Junhui Zhou and Jianwei Gong},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636051},
  pages     = {6891-6898},
  title     = {Orientation-aware planning for parallel task execution of omni-directional mobile robot},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A meta-learning-based trajectory tracking framework for UAVs
under degraded conditions. <em>IROS</em>, 6884–6890. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Due to changes in model dynamics or unexpected disturbances, an autonomous robotic system may experience unforeseen challenges during real-world operations which may affect its safety and intended behavior: in particular actuator and system failures and external disturbances are among the most common causes of degraded mode of operation. To deal with this problem, in this work, we present a meta-learning-based approach to improve the trajectory tracking performance of an unmanned aerial vehicle (UAV) under actuator faults and disturbances which have not been previously experienced. Our approach leverages meta-learning to train a model that is easily adaptable at runtime to make accurate predictions about the system’s future state. A runtime monitoring and validation technique is proposed to decide when the system needs to adapt its model by considering a data pruning procedure for efficient learning. Finally, the reference trajectory is adapted based on future predictions by borrowing feedback control logic to make the system track the original and desired path without needing to access the system’s controller. The proposed framework is applied and validated in both simulations and experiments on a faulty UAV navigation case study demonstrating a drastic increase in tracking performance.},
  archive   = {C_IROS},
  author    = {Esen Yel and Nicola Bezzo},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635918},
  pages     = {6884-6890},
  title     = {A meta-learning-based trajectory tracking framework for UAVs under degraded conditions},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data-fusion for robust off-road perception considering data
quality of uncertain sensors. <em>IROS</em>, 6876–6883. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robust off-road perception for autonomous navigation is hard to achieve. Versatile environments, different hardware, and numerous disturbances limit the perceptional portability in changing applications and cross-platform. This contribution proposes sensor-fusion considering the data quality of uncertain sensors to increase the classification and mapping components’ perceptual robustness. The resulting benefits on perception are demonstrated using the autonomous off-road robot U5023.},
  archive   = {C_IROS},
  author    = {Patrick Wolf and Karsten Berns},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636541},
  pages     = {6876-6883},
  title     = {Data-fusion for robust off-road perception considering data quality of uncertain sensors},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). AcousticFusion: Fusing sound source localization to visual
SLAM in dynamic environments. <em>IROS</em>, 6868–6875. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dynamic objects in the environment, such as people and other agents, lead to challenges for existing simultaneous localization and mapping (SLAM) approaches. To deal with dynamic environments, computer vision researchers usually apply some learning-based object detectors to remove these dynamic objects. However, these object detectors are computationally too expensive for mobile robot on-board processing. In practical applications, these objects output noisy sounds that can be effectively detected by on-board sound source localization. The directional information of the sound source object can be efficiently obtained by direction of sound arrival (DoA) estimation, but the depth estimation is difficult. Therefore, in this paper, we propose a novel audio-visual fusion approach that fuses sound source direction into the RGB-D image and thus removes the effect of dynamic obstacles on the multi-robot SLAM system. Experimental results of multirobot SLAM in different dynamic environments show that the proposed method uses very small computational resources to obtain very stable self-localization results.},
  archive   = {C_IROS},
  author    = {Tianwei Zhang and Huayan Zhang and Xiaofei Li and Junfeng Chen and Tin Lun Lam and Sethu Vijayakumar},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636585},
  pages     = {6868-6875},
  title     = {AcousticFusion: Fusing sound source localization to visual SLAM in dynamic environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reinforcement learning compensated extended kalman filter
for attitude estimation. <em>IROS</em>, 6854–6859. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inertial measurement units are widely used in different fields to estimate the attitude. Many algorithms have been proposed to improve estimation performance. However, most of them still suffer from 1) inaccurate initial estimation, 2) inaccurate initial filter gain, and 3) non-Gaussian process and/or measurement noise. This paper will leverage reinforcement learning to compensate for the classical extended Kalman filter estimation, i.e., to learn the filter gain from the sensor measurements. We also analyse the convergence of the estimate error. The effectiveness of the proposed algorithm is validated on both simulated data and real data.},
  archive   = {C_IROS},
  author    = {Yujie Tang and Liang Hu and Qingrui Zhang and Wei Pan},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635963},
  pages     = {6854-6859},
  title     = {Reinforcement learning compensated extended kalman filter for attitude estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3D-FFS: Faster 3D object detection with focused frustum
search in sensor fusion based networks. <em>IROS</em>, 6848–6853. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work we propose 3D-FFS, a novel approach to make sensor fusion based 3D object detection networks significantly faster using a class of computationally inexpensive heuristics. Existing sensor fusion based networks generate 3D region proposals by leveraging inferences from 2D object detectors. However, as images have no depth information, these networks rely on extracting semantic features of points from the entire scene to locate the object. By leveraging aggregated intrinsic properties (e.g. point density) of point cloud data, 3D-FFS can substantially constrain the 3D search space and thereby significantly reduce training time, inference time and memory consumption without sacrificing accuracy. To demonstrate the efficacy of 3D-FFS, we have integrated it with Frustum ConvNet (F-ConvNet), a prominent sensor fusion based 3D object detection model. We assess the performance of 3D-FFS on the KITTI dataset. Compared to F-ConvNet, we achieve improvements in training and inference times by up to 62.80\% and 58.96\%, respectively, while reducing the memory usage by up to 58.53\%. Additionally, we achieve 0.36\%, 0.59\% and 2.19\% improvements in accuracy for the Car, Pedestrian and Cyclist classes, respectively. 3D-FFS shows a lot of promise in domains with limited computing power, such as autonomous vehicles, drones and robotics where LiDAR-Camera based sensor fusion perception systems are widely used.},
  archive   = {C_IROS},
  author    = {Aniruddha Ganguly and Tasin Ishmam and Khandker Aftarul Islam and Md Zahidur Rahman and Md. Shamsuzzoha Bayzid},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636244},
  pages     = {6848-6853},
  title     = {3D-FFS: Faster 3D object detection with focused frustum search in sensor fusion based networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FINO-net: A deep multimodal sensor fusion framework for
manipulation failure detection. <em>IROS</em>, 6841–6847. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We need robots more aware of the unintended outcomes of their actions for ensuring safety. This can be achieved by an onboard failure detection system to monitor and detect such cases. Onboard failure detection is challenging with a limited set of onboard sensor setup due to the limitations of sensing capabilities of each sensor. To alleviate these challenges, we propose FINO-Net, a novel multimodal sensor fusion based deep neural network to detect and identify manipulation failures. We also introduce FAILURE, a multimodal dataset, containing 229 real-world manipulation data recorded with a Baxter robot. Our network combines RGB, depth and audio readings to effectively detect failures. Results indicate that fusing RGB with depth and audio modalities significantly improves the performance. FINO-Net achieves\%98.60 detection accuracy on our novel dataset. Code and data are publicly available at https://github.com/ardai/fino-net.},
  archive   = {C_IROS},
  author    = {Arda Inceoglu and Eren Erdal Aksoy and Abdullah Cihan Ak and Sanem Sariel},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636455},
  pages     = {6841-6847},
  title     = {FINO-net: A deep multimodal sensor fusion framework for manipulation failure detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accurate depth estimation from a hybrid event-RGB stereo
setup. <em>IROS</em>, 6833–6840. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Event-based visual perception is becoming increasingly popular owing to interesting sensor characteristics enabling the handling of difficult conditions such as highly dynamic motion or challenging illumination. The mostly complementary nature of event cameras however still means that best results are achieved if the sensor is paired with a regular frame-based sensor. The present work aims at answering a simple question: Assuming that both cameras do not share a common optical center, is it possible to exploit the hybrid stereo setup&#39;s baseline to perform accurate stereo depth estimation? We present a learning based solution to this problem leveraging modern spatio-temporal input representations as well as a novel hybrid pyramid attention module. Results on real data demonstrate competitive performance against pure frame-based stereo alternatives as well as the ability to maintain the advantageous properties of event-based sensors.},
  archive   = {C_IROS},
  author    = {Yi-Fan Zuo and Li Cui and Xin Peng and Yanyu Xu and Shenghua Gao and Xia Wang and Laurent Kneip},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635834},
  pages     = {6833-6840},
  title     = {Accurate depth estimation from a hybrid event-RGB stereo setup},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast reactive grasping with in-finger vision and in-hand
FPGA-accelerated CNNs. <em>IROS</em>, 6825–6832. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a soft humanoid hand with in-finger integrated cameras and in-hand real-time image processing system for fast reactive grasping. Specifically, we describe an FPGA-based, in-hand integrated, embedded system for processing visual data captured by the five in-finger cameras while avoiding high bandwidth raw data streaming via the robots real-time data bus. The hardware acceleration allows fast detection and localization of objects based on finger-camera images and provides input for a grasping controller. To this end, we implement a resource-aware encoder-decoder Convolutional Neural Network (CNN) for pixel-wise object segmentation and run inference on the in-hand embedded system at 3.58 GOPS. We evaluate the system, consisting of the soft hand with in-finger vision and the in-hand FPGA-accelerated CNN in several experiments on the humanoid robot ARMAR-6. Specifically, we evaluate the overall system response time, the ability to perform precision grasps and test reactivity and reliability that are required for handover actions. We obtain an overall system response time of 154 ms for catching a falling object and obtain a success rate of 90\% reliability for the power drill handover tasks. Further, we successfully demonstrate ability of dexterous grasping and manipulation of a pencil from a cup.},
  archive   = {C_IROS},
  author    = {Felix Hundhausen and Raphael Grimm and Leon Stieber and Tamim Asfour},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636043},
  pages     = {6825-6832},
  title     = {Fast reactive grasping with in-finger vision and in-hand FPGA-accelerated CNNs},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-view fusion for multi-level robotic scene
understanding. <em>IROS</em>, 6817–6824. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a system for multi-level scene awareness for robotic manipulation. Given a sequence of camera-inhand RGB images, the system calculates three types of information: 1) a point cloud representation of all the surfaces in the scene, for the purpose of obstacle avoidance. 2) the rough pose of unknown objects from categories corresponding to primitive shapes (e.g., cuboids and cylinders), and 3) full 6-DoF pose of known objects. By developing and fusing recent techniques in these domains, we provide a rich scene representation for robot awareness. We demonstrate the importance of each of these modules, their complementary nature, and the potential benefits of the system in the context of robotic manipulation.},
  archive   = {C_IROS},
  author    = {Yunzhi Lin and Jonathan Tremblay and Stephen Tyree and Patricio A. Vela and Stan Birchfield},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635994},
  pages     = {6817-6824},
  title     = {Multi-view fusion for multi-level robotic scene understanding},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving grasp stability with rotation measurement from
tactile sensing. <em>IROS</em>, 6809–6816. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Rotational displacement about the grasping point is a common grasp failure when an object is grasped at a location away from its center of gravity. Tactile sensors with soft surfaces, such as GelSight sensors, can detect the rotation patterns on the contacting surfaces when the object rotates. In this work, we propose a model-based algorithm that detects those rotational patterns and measures rotational displacement using the GelSight sensor. We also integrate the rotation detection feedback into a closed-loop regrasping framework, which detects the rotational failure of grasp in an early stage and drives the robot to a stable grasp pose. We validate our proposed rotation detection algorithm and grasp-regrasp system on self-collected dataset and online experiments to show how our approach accurately detects the rotation and increases grasp stability.},
  archive   = {C_IROS},
  author    = {Raj Kolamuri and Zilin Si and Yufan Zhang and Arpit Agarwal and Wenzhen Yuan},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636488},
  pages     = {6809-6816},
  title     = {Improving grasp stability with rotation measurement from tactile sensing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hybrid ICP. <em>IROS</em>, 6801–6808. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {ICP algorithms typically involve a fixed choice of data association method and a fixed choice of error metric. In this paper, we propose Hybrid ICP, a novel and flexible ICP variant which dynamically optimises both the data association method and error metric based on the live image of an object and the current ICP estimate. We show that when used for object pose estimation, Hybrid ICP is more accurate and more robust to noise than other commonly used ICP variants. We also consider the setting where ICP is applied sequentially with a moving camera, and we study the trade-off between the accuracy of each ICP estimate and the number of ICP estimates available within a fixed amount of time.},
  archive   = {C_IROS},
  author    = {Kamil Dreczkowski and Edward Johns},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636600},
  pages     = {6801-6808},
  title     = {Hybrid ICP},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Policy learning for visually conditioned tactile
manipulation. <em>IROS</em>, 6794–6800. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent work on robot learning with visual observations has shown great success in solving many manipulation tasks. While visual observations contain rich information about the environment and the robot, they can be unreliable in the presence of visual noise or occlusions. In these cases, we can leverage tactile observations generated by the interaction between the robot and the environment. In this paper, we propose a framework for learning manipulation policies that fuse visual and tactile feedback. The control problems considered in this work are to localize a gripper with respect to the environment image and navigate to desired states. Our method uses a learned Bayes filter to estimate the state of a gripper by conditioning the tactile observations on the environment image. We use deep reinforcement learning for solving the localization and navigation problems provided with the belief of the gripper’s state and the environment image. We compare our method against two baselines where the agent uses tactile observation directly with a recurrent neural network or uses a point estimate of the state instead of the full belief state. We also transfer the policies to the real world and validate them on a physical robot.},
  archive   = {C_IROS},
  author    = {Tarık Keleştemur and Taşkın Padır and Robert Platt},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636866},
  pages     = {6794-6800},
  title     = {Policy learning for visually conditioned tactile manipulation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual-tactile fusion for 3D objects reconstruction from a
single depth view and a single gripper touch for robotics tasks.
<em>IROS</em>, 6786–6793. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The planning of robotic manipulation and grasping tasks depends on the reconstruction of the 3D object’s shape. Most of the existing 3D object reconstruction methods are based on visual sensing that are limited due to the lack of the object’s occluded side information. The goal of this paper is to overcome these limitations and improve the 3D objects’ reconstruction by adding the tactile sensing to the visual data. In this paper, a novel multi-modal (visual and tactile) semi-supervised generative model is presented to reconstruct the complete 3D object’s shape using a single arbitrary depth-view and a single dexterous-hand’s touch. The presented approach takes the strength of the autoencoder and generative networks to provide an end-to-end trainable model with high generalization ability. The 3D voxel grids of the depth and tactile data are the only requirements of the proposed model to predict a high resolution voxel grids of 64 3 for the incomplete shape. This research generates its tactile dataset based on the kinematic model of the shadow dexterous hand. The developed dataset has aligned depth, tactile and ground truth voxel grids of different resolutions (40 3 , 64 3 and 128 3 ) from different camera views. Experimental results show that the proposed multi-modal model outperforms other state-of-the-art methods.},
  archive   = {C_IROS},
  author    = {Mohamed Tahoun and Omar Tahri and Juan Antonio Corrales Ramón and Youcef Mezouar},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636150},
  pages     = {6786-6793},
  title     = {Visual-tactile fusion for 3D objects reconstruction from a single depth view and a single gripper touch for robotics tasks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sim-to-real transfer for robotic manipulation with tactile
sensory. <em>IROS</em>, 6778–6785. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement Learning (RL) methods have been widely applied for robotic manipulations via sim-to-real transfer, typically with proprioceptive and visual information. However, the incorporation of tactile sensing into RL for contact-rich tasks lacks investigation. In this paper, we model a tactile sensor in simulation and study the effects of its feedback in RL-based robotic control via a zero-shot sim-to-real approach with domain randomization. We demonstrate that learning and controlling with feedback from tactile sensor arrays at the gripper, both in simulation and reality, can enhance grasping stability, which leads to a significant improvement in robotic manipulation performance for a door opening task. In real-world experiments, the door open angle was increased by 45\% on average for transferred policies with tactile sensing over those without it.},
  archive   = {C_IROS},
  author    = {Zihan Ding and Ya-Yen Tsai and Wang Wei Lee and Bidan Huang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636259},
  pages     = {6778-6785},
  title     = {Sim-to-real transfer for robotic manipulation with tactile sensory},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ontology-assisted generalisation of robot action execution
knowledge. <em>IROS</em>, 6763–6770. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When an autonomous robot learns how to execute actions, it is of interest to know if and when the execution policy can be generalised to variations of the learning scenarios. This can inform the robot about the necessity of additional learning, as using incomplete or unsuitable policies can lead to execution failures. Generalisation is particularly relevant when a robot has to deal with a large variety of objects and in different contexts. In this paper, we propose and analyse a strategy for generalising parameterised execution models of manipulation actions over different objects based on an object ontology. In particular, a robot transfers a known execution model to objects of related classes according to the ontology, but only if there is no other evidence that the model may be unsuitable. This allows using ontological knowledge as prior information that is then refined by the robot’s own experiences. We verify our algorithm for two actions - grasping and stowing everyday objects - such that we show that the robot can deduce cases in which an existing policy can generalise to other objects and when additional execution knowledge has to be acquired.},
  archive   = {C_IROS},
  author    = {Alex Mitrevsk and Paul G. Plöger and Gerhard Lakemeyer},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636791},
  pages     = {6763-6770},
  title     = {Ontology-assisted generalisation of robot action execution knowledge},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CRIL: Continual robot imitation learning via generative and
prediction model. <em>IROS</em>, 6747–5754. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Imitation learning (IL) algorithms have shown promising results for robots to learn skills from expert demonstrations. However, they need multi-task demonstrations to be provided at once for acquiring diverse skills, which is difficult in real world. In this work we study how to realize continual imitation learning ability that empowers robots to continually learn new tasks one by one, thus reducing the burden of multitask IL and accelerating the process of new task learning at the same time. We propose a novel trajectory generation model that employs both a generative adversarial network and a dynamics-aware prediction model to generate pseudo trajectories from all learned tasks in the new task learning process. Our experiments on both simulation and real-world manipulation tasks demonstrate the effectiveness of our method.},
  archive   = {C_IROS},
  author    = {Chongkai Gao and Haichuan Gao and Shangqi Guo and Tianren Zhang and Feng Chen},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636069},
  pages     = {6747-5754},
  title     = {CRIL: Continual robot imitation learning via generative and prediction model},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Behavior self-organization supports task inference for
continual robot learning. <em>IROS</em>, 6739–6746. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in robot learning have enabled robots to become increasingly better at mastering a predefined set of tasks. On the other hand, as humans, we have the ability to learn a growing set of tasks over our lifetime. Continual robot learning is an emerging research direction with the goal of endowing robots with this ability. In order to learn new tasks over time, the robot first needs to infer the task at hand. Task inference, however, has received little attention in the multi-task learning literature. In this paper, we propose a novel approach to continual learning of robotic control tasks. Our approach performs unsupervised learning of behavior embeddings by incrementally self-organizing demonstrated behaviors. Task inference is made by finding the nearest behavior embedding to a demonstrated behavior, which is used together with the environment state as input to a multi-task policy trained with reinforcement learning to optimize performance over tasks. Unlike previous approaches, our approach makes no assumptions about task distribution and requires no task exploration to infer tasks. We evaluate our approach in experiments with concurrently and sequentially presented tasks and show that it outperforms other multi-task learning approaches in terms of generalization performance and convergence speed, particularly in the continual learning setting.},
  archive   = {C_IROS},
  author    = {Muhammad Burhan Hafez and Stefan Wermter},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636297},
  pages     = {6739-6746},
  title     = {Behavior self-organization supports task inference for continual robot learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automated generation of robotic planning domains from
observations. <em>IROS</em>, 6732–6738. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automated planning enables robots to find plans to achieve complex, long-horizon tasks, given a planning domain. This planning domain consists of a list of actions, with their associated preconditions and effects, and is usually manually defined by a human expert, which is very time-consuming or even infeasible. In this paper, we introduce a novel method for generating this domain automatically from human demonstrations. First, we automatically segment and recognize the different observed actions from human demonstrations. From these demonstrations, the relevant preconditions and effects are obtained, and the associated planning operators are generated. Finally, a sequence of actions that satisfies a user-defined goal can be planned using a symbolic planner. The generated plan is executed in a simulated environment by the TIAGo robot. We tested our method on a dataset of 12 demonstrations collected from three different participants. The results show that our method is able to generate executable plans from using one single demonstration with a 92\% success rate, and 100\% when the information from all demonstrations are included, even for previously unseen stacking goals.},
  archive   = {C_IROS},
  author    = {Maximilian Diehl and Chris Paxton and Karinne Ramirez-Amaro},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636781},
  pages     = {6732-6738},
  title     = {Automated generation of robotic planning domains from observations},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). In-air knotting of rope using dual-arm robot based on deep
learning. <em>IROS</em>, 6724–6731. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this study, we report the successful execution of in-air knotting of rope using a dual-arm two-finger robot based on deep learning. Owing to its flexibility, the state of the rope was in constant flux during the operation of the robot. This required the robot control system to dynamically correspond to the state of the object at all times. However, a manual description of appropriate robot motions corresponding to all object states is difficult to be prepared in advance. To resolve this issue, we constructed a model that instructed the robot to perform bowknots and overhand knots based on two deep neural networks trained using the data gathered from its sensorimotor, including visual and proximity sensors. The resultant model was verified to be capable of predicting the appropriate robot motions based on the sensory information available online. In addition, we designed certain task motions based on the Ian knot method using the dual-arm two-fingers robot. The designed knotting motions do not require a dedicated workbench or robot hand, thereby enhancing the versatility of the proposed method. Finally, experiments were performed to estimate the knotting performance of the real robot while executing overhand knots and bowknots on rope and its success rate. The experimental results established the effectiveness and high performance of the proposed method.},
  archive   = {C_IROS},
  author    = {Kanata Suzuki and Momomi Kanamura and Yuki Suga and Hiroki Mori and Tetsuya Ogata},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635954},
  pages     = {6724-6731},
  title     = {In-air knotting of rope using dual-arm robot based on deep learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mobile manipulation–based deployment of micro aerial robot
scouts through constricted aperture-like ingress points. <em>IROS</em>,
6716–6723. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel strategy for the autonomous deployment of Micro Aerial Vehicle scouts through constricted aperture-like ingress points, by narrowly fitting and launching them with a high-precision Mobile Manipulation robot. A significant problem during exploration and reconnaissance into highly unstructured environments, such as indoor collapsed ones, is the encountering of impassable areas due to their constricted and rigid nature. We propose that a heterogeneous robotic system-of-systems armed with manipulation capabilities while also ferrying a fleet of microsized aerial agents, can deploy the latter through constricted apertures that marginally fit them in size, thus allowing them to act as scouts and resume the reconnaissance mission. This work’s contribution is twofold: first, it proposes active-vision based aperture detection to locate candidate ingress points and a hierarchical search-based aperture profile analysis to position a MAV’s body through them, and secondly it presents and experimentally demonstrates the novelty of a system-of-systems approach which leverages mobile manipulation to deploy other robots which are otherwise incapable of entering through extremely narrow openings.},
  archive   = {C_IROS},
  author    = {Prateek Arora and Christos Papachristos},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636178},
  pages     = {6716-6723},
  title     = {Mobile manipulation–based deployment of micro aerial robot scouts through constricted aperture-like ingress points},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The reasonable crowd: Towards evidence-based and
interpretable models of driving behavior. <em>IROS</em>, 6708–6715. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous vehicles must balance a complex set of objectives. There is no consensus on how they should do so, nor on a model for specifying a desired driving behavior. We created a dataset to help address some of these questions in a limited operating domain. The data consists of 92 traffic scenarios, with multiple ways of traversing each scenario. Multiple annotators expressed their preference between pairs of scenario traversals. We used the data to compare an instance of a rulebook [1], carefully hand-crafted independently of the dataset, with several interpretable machine learning models such as Bayesian networks, decision trees, and logistic regression trained on the dataset. To compare driving behavior, these models use scores indicating by how much different scenario traversals violate each of 14 driving rules. The rules are interpretable and designed by subject-matter experts. First, we found that these rules were enough for these models to achieve a high classification accuracy on the dataset. Second, we found that the rulebook provides high interpretability without excessively sacrificing performance. Third, the data pointed to possible improvements in the rulebook and the rules, and to potential new rules. Fourth, we explored the interpretability vs performance trade-off by also training non-interpretable models such as a random forest. Finally, we make the dataset publicly available to encourage a discussion from the wider community on behavior specification for AVs. Please find it at github.com/bassam-motional/Reasonable-Crowd.},
  archive   = {C_IROS},
  author    = {Bassam Helou and Aditya Dusi and Anne Collin and Noushin Mehdipour and Zhiliang Chen and Cristhian Lizarazo and Calin Belta and Tichakorn Wongpiromsarn and Radboud Duintjer Tebbens and Oscar Beijbom},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635960},
  pages     = {6708-6715},
  title     = {The reasonable crowd: Towards evidence-based and interpretable models of driving behavior},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Designing and deploying a mobile UVC disinfection robot.
<em>IROS</em>, 6700–6707. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a mobile UVC disinfection robot designed to mitigate the threat of airborne and surface pathogens. Our system comprises a mobile robot base, a custom UVC lamp assembly, and algorithms for autonomous navigation and path planning. We present a model of UVC disinfection and dosage of UVC light delivered by the mobile robot. We also discuss challenges and prototyping decisions for rapid deployment of the robot during the COVID-19 pandemic. Experimental results summarize a long-term deployment at The Greater Boston Food Bank, where the robot delivers (nightly) UVC dosages of at least 10 mJ/cm 2 to a 4000 ft 2 area in under 30 minutes. These dosages are capable of neutralizing 99\% of coronaviruses, including SARS-CoV-2, on surfaces and in airborne particles. Further simulations present how this mobile UVC disinfection robot may be extended to classic problems in robotic path planning and adaptive multi-robot coverage control.},
  archive   = {C_IROS},
  author    = {Alyssa Pierson and John W. Romanishin and Hunter Hansen and Leonardo Zamora Yañez and Daniela Rus},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636260},
  pages     = {6700-6707},
  title     = {Designing and deploying a mobile UVC disinfection robot},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Agent-aware state estimation in autonomous vehicles.
<em>IROS</em>, 6694–6699. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous systems often operate in environments where the behavior of multiple agents is coordinated by a shared global state. Reliable estimation of the global state is thus critical for successfully operating in a multi-agent setting. We introduce agent-aware state estimation—a framework for calculating indirect estimations of state given observations of the behavior of other agents in the environment. We also introduce transition-independent agent-aware state estimation—a tractable class of agent-aware state estimation—and show that it allows the speed of inference to scale linearly with the number of agents in the environment. As an example, we model traffic light classification in instances of complete loss of direct observation. By taking into account observations of vehicular behavior from multiple directions of traffic, our approach exhibits accuracy higher than that of existing traffic light-only HMM methods on a real-world autonomous vehicle data set under a variety of simulated occlusion scenarios.},
  archive   = {C_IROS},
  author    = {Shane Parr and Ishan Khatri and Justin Svegliato and Shlomo Zilberstein},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636210},
  pages     = {6694-6699},
  title     = {Agent-aware state estimation in autonomous vehicles},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Monitoring object detection abnormalities via data-label and
post-algorithm abstractions. <em>IROS</em>, 6688–6693. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While object detection modules are essential functionalities for any autonomous vehicle, the performance of such modules that are implemented using deep neural networks can be, in many cases, unreliable. In this paper, we develop abstraction-based monitoring as a logical framework for filtering potentially erroneous detection results. Concretely, we consider two types of abstraction, namely data-label abstraction and post-algorithm abstraction. Operated on the training dataset, the construction of data-label abstraction iterates each input, aggregates region-wise information over its associated labels, and stores the vector under a finite history length. Post-algorithm abstraction builds an abstract transformer for the tracking algorithm. Elements being associated together by the abstract transformer can be checked against consistency over their original values. We have implemented the overall framework to a research prototype and validated it using publicly available object detection datasets.},
  archive   = {C_IROS},
  author    = {Yuhang Chen and Chih-Hong Cheng and Jun Yan and Rongjie Yan},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636713},
  pages     = {6688-6693},
  title     = {Monitoring object detection abnormalities via data-label and post-algorithm abstractions},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A high-accuracy framework for vehicle dynamic modeling in
autonomous driving. <em>IROS</em>, 6680–6687. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vehicle dynamic models are the key to bridge the gap between simulation and real road test in autonomous driving. An accurate vehicle model allows control algorithms in simulation being transferred to real road test with same quality. In this paper, we present a dynamic model residual correction framework (DRF) for vehicle dynamic modeling. DRF provides a general accuracy improvement framework on existing vehicle dynamic models. On top of any existing open-loop dynamic model, this framework builds a Residual Correction Model (RCM) by integrating deep Neural Networks (NN) with Stochastic Variational Gaussian Process (SVGP) model. RCM takes a sequence of vehicle control commands and dynamic states for a certain time duration as modeling inputs, extracts underlying context from this sequence with deep encoder networks, and predicts open-loop dynamic model prediction errors. Five vehicle dynamic models are derived from DRF via encoder variations. Our contribution is consolidated with evaluation of the absolute trajectory error and the similarity between DRF outputs and the ground truth. Compared to classic rule-based and learning-based vehicle dynamic models, DRF accomplishes as high as 74.12\% to 85.02\% of the absolute trajectory error drop among all DRF variations.},
  archive   = {C_IROS},
  author    = {Shu Jiang and Yu Wang and Weiman Lin and Yu Cao and Longtao Lin and Jinghao Miao and Qi Luo},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636861},
  pages     = {6680-6687},
  title     = {A high-accuracy framework for vehicle dynamic modeling in autonomous driving},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast autonomous robotic exploration using the underlying
graph structure. <em>IROS</em>, 6672–6679. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we fully define the existing relationships between traditional optimality criteria and the connectivity of the underlying pose-graph in Active SLAM, characterizing, therefore, the connection between Graph Theory and the Theory Optimal Experimental Design. We validate the proposed relationships in 2D and 3D graph SLAM datasets, showing a remarkable relaxation of the computational load when using the graph structure. Furthermore, we present a novel Active SLAM framework which outperforms traditional methods by successfully leveraging the graphical facet of the problem so as to autonomously explore an unknown environment.},
  archive   = {C_IROS},
  author    = {Julio A. Placed and José A. Castellanos},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636148},
  pages     = {6672-6679},
  title     = {Fast autonomous robotic exploration using the underlying graph structure},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An efficient and continuous representation for occupancy
mapping with random mapping. <em>IROS</em>, 6664–6671. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generating meaningful spatial models of physical environments is a crucial ability for autonomous navigation of mobile robots. This paper considers the problem of building continuous occupancy maps from sparse and noisy sensor data. To this end, we propose a new method named random mapping maps that advances the popular methods in two aspects. Firstly, it can represent environment models in a memory-saving and time-saving manner by randomly mapping a low-dimensional feature space to a high-dimensional one where a linear model is learnt. Secondly, it can rapidly obtain accurate inferences of the occupancy states of the spatial locations. This technique is based on the random mapping that projects the measurement data into a random feature space in which a discriminative model is learnt by the available data. It can asymptotically represent the complexity of the real world as the mapping dimension increases. Evaluations of the proposed method were conducted on various environments to verify its availability to environment modeling. Its performances in terms of time and memory consumptions were evaluated quantitatively. Finally, as a practical application, experiments about path planning were conducted based on the gradients of the proposed representation of environment model.},
  archive   = {C_IROS},
  author    = {Xu Liu and Decai Li and Yuqing He},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635937},
  pages     = {6664-6671},
  title     = {An efficient and continuous representation for occupancy mapping with random mapping},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CLINS: Continuous-time trajectory estimation for
LiDAR-inertial system. <em>IROS</em>, 6657–6663. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a highly accurate continuous-time trajectory estimation framework dedicated to SLAM (Simultaneous Localization and Mapping) applications, which enables fuse high-frequency and asynchronous sensor data effectively. We apply the proposed framework in a 3D LiDAR-inertial system for evaluations. The proposed method adopts a non-rigid registration method for continuous-time trajectory estimation and simultaneously removing the motion distortion in LiDAR scans. Additionally, we propose a two-state continuous-time trajectory correction method to efficiently and efficiently tackle the computationally-intractable global optimization problem when loop closure happens. We examine the accuracy of the proposed approach on several publicly available datasets and the data we collected. The experimental results indicate that the proposed method outperforms the discrete-time methods regarding accuracy especially when aggressive motion occurs. Furthermore, we open source our code at https://github.com/APRIL-ZJU/clins to benefit research community.},
  archive   = {C_IROS},
  author    = {Jiajun Lv and Kewei Hu and Jinhong Xu and Yong Liu and Xiushui Ma and Xingxing Zuo},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636676},
  pages     = {6657-6663},
  title     = {CLINS: Continuous-time trajectory estimation for LiDAR-inertial system},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatic construction of lane-level HD maps for urban
scenes. <em>IROS</em>, 6649–6656. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {High definition (HD) maps have demonstrated their essential roles in enabling full autonomy, especially in complex urban scenarios. As a crucial layer of the HD map, lane-level maps are particularly useful: they contain geometrical and topological information for both lanes and intersections. However, large scale construction of HD maps is limited by tedious human labeling and high maintenance costs, especially for urban scenarios with complicated road structures and irregular markings. This paper proposes an approach based on semantic-particle filter to tackle the automatic lane-level mapping problem in urban scenes. The map skeleton is firstly structured as a directed cyclic graph from online mapping database OpenStreetMap. Our proposed method then performs semantic segmentation on 2D front-view images from ego vehicles and explores the lane semantics on a birds-eye-view domain with true topographical projection. Exploiting OpenStreetMap, we further infer lane topology and reference trajectory at intersections with the aforementioned lane semantics. The proposed algorithm has been tested in densely urbanized areas, and the results demonstrate accurate and robust reconstruction of the lane-level HD map.},
  archive   = {C_IROS},
  author    = {Yiyang Zhou and Yuichi Takeda and Masayoshi Tomizuka and Wei Zhan},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636205},
  pages     = {6649-6656},
  title     = {Automatic construction of lane-level HD maps for urban scenes},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DeepRelativeFusion: Dense monocular SLAM using single-image
relative depth prediction. <em>IROS</em>, 6641–6648. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traditional monocular visual simultaneous localization and mapping (SLAM) algorithms have been extensively studied and proven to reliably recover a sparse structure and camera motion. Nevertheless, the sparse structure is still insufficient for scene interaction, e.g., visual navigation and augmented reality applications. To densify the scene reconstruction, the use of single-image absolute depth prediction from convolutional neural networks (CNNs) for filling in the missing structure has been proposed. However, the prediction accuracy tends to not generalize well on scenes that are different from the training datasets.In this paper, we propose a dense monocular SLAM system, named DeepRelativeFusion, that is capable to recover a globally consistent 3D structure. To this end, we use a visual SLAM algorithm to reliably recover the camera poses and semi-dense depth maps of the keyframes, and then use relative depth prediction to densify the semi-dense depth maps and refine the keyframe pose-graph. To improve the semi-dense depth maps, we propose an adaptive filtering scheme, which is a structure- preserving weighted average smoothing filter that takes into account the pixel intensity and depth of the neighbouring pixels, yielding substantial reconstruction accuracy gain in densification. To perform densification, we introduce two incremental improvements upon the energy minimization framework proposed by DeepFusion: (1) an improved cost function, and(2) the use of single-image relative depth prediction. After densification, we update the keyframes with two-view consistent optimized semi-dense and dense depth maps to improve pose- graph optimization, providing a feedback loop to refine the keyframe poses for accurate scene reconstruction. Our system outperforms the state-of-the-art dense SLAM systems quantitatively in dense reconstruction accuracy by a large margin.For more information, see the demo video and supplementary material.},
  archive   = {C_IROS},
  author    = {Shing Yan Loo and Syamsiah Mashohor and Sai Hong Tang and Hong Zhang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636504},
  pages     = {6641-6648},
  title     = {DeepRelativeFusion: Dense monocular SLAM using single-image relative depth prediction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Smooth mesh estimation from depth data using non-smooth
convex optimization. <em>IROS</em>, 6633–6640. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Meshes are commonly used as 3D maps since they encode the topology of the scene while being lightweight. Unfortunately, 3D meshes are mathematically difficult to handle directly because of their combinatorial and discrete nature. Therefore, most approaches generate 3D meshes of a scene after fusing depth data using volumetric or other representations. Nevertheless, volumetric fusion remains computationally expensive both in terms of speed and memory. In this paper, we leapfrog these intermediate representations and build a 3D mesh directly from a depth map and the sparse landmarks triangulated with visual odometry. To this end, we formulate a non-smooth convex optimization problem that we solve using a primal-dual method. Our approach generates a smooth and accurate 3D mesh that substantially improves the state-of-the-art on direct mesh reconstruction while running in real-time.},
  archive   = {C_IROS},
  author    = {Antoni Rosinol and Luca Carlone},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636222},
  pages     = {6633-6640},
  title     = {Smooth mesh estimation from depth data using non-smooth convex optimization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Local to global plane regularity aggregation for dense
surfel mapping. <em>IROS</em>, 6625–6632. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel local to global plane regularity aggregation framework for dense surfel mapping, aiming for real-time reconstruction of high-quality 3D global models in both indoor and urban environments. Different from prior works that directly localize surfels globally, we investigate three interplanar geometric relations: {coplanarity, parallelism, orthogonality} from local to global scales as additional structural regularities in reconstruction, promoting the performance in plane-dominated scenes remarkably. Given a monocular RGB-D video as input, our framework extracts and utilizes the interplanar relations in three stages: local surfel creation, local to global relation propagation, and global plane-guided re-localization. In the first stage, surfels are created and refined within the current frame by aggregating temporal and spatial cues. The interplanar relations are adopted to regulate the normal and position of each surfel. Then in the second stage, we simultaneously establish correspondences between the created surfels and global model and propagate the interplanar relations from local to global. Finally, the positions of surfels are further relocated and optimized in a larger scale, based on the global interplanar relation priors aggregated across all local frames. Extensive experiments on datasets of different scales demonstrate that our framework achieves superior performance in terms of consistency and accuracy of the reconstructed global model. Meanwhile, the capability of our framework in the real-time 3D reconstruction on CPU opens the door to practical application.},
  archive   = {C_IROS},
  author    = {Jiexiang Tan and Xiangyang Ji},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636254},
  pages     = {6625-6632},
  title     = {Local to global plane regularity aggregation for dense surfel mapping},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). What’s best for my mesh? Convex or non-convex regularisation
for mesh optimisation. <em>IROS</em>, 6617–6624. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A 3D mesh offers a rich yet lightweight representation of geometry and topology for the metric and semantic understanding of a robot’s scene. Noisy features are often used to generate the mesh which furthers the need for accurate regularisation. Current approaches tightly couple front-end optimisation with regularisation making it difficult to evaluate the choice of discretisation and regularisation on mesh accuracy. In this work, we aim to explicitly query the performance of a set of well-known convex and non-convex regularisers on the mesh optimisation problem. We then apply these norms to dense depth estimation from a mesh representation and evaluate their performance in indoor and outdoor environments.While we show that the use of exotic, non-convex regularisers such as logTV and logTGV can result in more faithful structural reconstruction under noise, this comes at the cost of stronger outlier persistence that limits their performance when compared to their convex equivalents. This represents a significant departure from results achieved when the same regularisers are applied in denser &quot;every-pixel&quot; scenarios and suggests that current discretisation techniques adopted for this problem are more sensitive to triangulation. This sensitivity is often obscured in many practical robotic applications by a rigorous front-end that removes artefacts from the mesh to be optimised. By decoupling the front and back-ends we therefore show that further consideration must be taken to align current mesh discretisations with the classical definitions of variational regularisers to allow the full benefit of their well-documented properties to be realised.},
  archive   = {C_IROS},
  author    = {Jason Pilbrough and Paul Amayo},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636424},
  pages     = {6617-6624},
  title     = {What’s best for my mesh? convex or non-convex regularisation for mesh optimisation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust rank deficient SLAM. <em>IROS</em>, 6603–6608. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous mobile robots need maps for effective, safe navigation, and SLAM in general is still an unsolved problem. Nonetheless, certain combinations of environmental characteristics and sensors admit tractable solutions. In particular, detection and tracking of linear features such as line segments (2D) or planar facets (3D) has been proven robust in many man-made environments. However, these types of features produce rank-deficient constraints, which create challenges for graph-based SLAM optimizers. We present techniques for using rank-deficient features and constraints more robustly by analyzing the approximate null-space of the constraints for each node in the factor graph representing the trajectory. We also extend auxiliary methods for correspondence calculations and map update routines, the combination of which yields state-of-the-art performance for a rank-deficient SLAM system. We present results from quantitative experiments comparing memory use, compute load, accuracy, and robustness for several ablation tests on real and simulated data.},
  archive   = {C_IROS},
  author    = {Samer B. Nashed and Jong Jin Park and Roger Webster and Joseph W. Durham},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636443},
  pages     = {6603-6608},
  title     = {Robust rank deficient SLAM},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Random fourier features based SLAM. <em>IROS</em>,
6597–6602. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work is dedicated to simultaneous continuous-time trajectory estimation and mapping based on Gaussian Processes (GP). State-of-the-art GP-based models for Simultaneous Localization and Mapping (SLAM) are computationally efficient but can only be used with a restricted class of kernel functions. This paper provides the algorithm based on GP with Random Fourier Features (RFF) approximation for SLAM without any constraints. The advantages of RFF for continuous-time SLAM are that we can consider a broader class of kernels and, at the same time, maintain computational complexity at reasonably low level by operating in the Fourier space of features. The accuracy-speed trade-off can be controlled by the number of features. Our experimental results on synthetic and real-world benchmarks demonstrate the cases in which our approach provides better results compared to the current state-of-the-art.},
  archive   = {C_IROS},
  author    = {Yermek Kapushev and Anastasia Kishkun and Gonzalo Ferrer and Evgeny Burnaev},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636819},
  pages     = {6597-6602},
  title     = {Random fourier features based SLAM},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical segment-based optimization for SLAM.
<em>IROS</em>, 6573–6580. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a hierarchical segment-based optimization method for Simultaneous Localization and Mapping (SLAM) system. First we propose a reliable trajectory segmentation method that can be used to increase efficiency in the back-end optimization. Then we propose a buffer mechanism for the first time to improve the robustness of the segmentation. During the optimization, we use global information to optimize the frames with large error, and interpolation instead of optimization to update well-estimated frames to hierarchically allocate the amount of computation according to error of each frame. Comparative experiments on the benchmark show that our method greatly improves the efficiency of optimization with almost no drop in accuracy, and outperforms existing high-efficiency optimization method by a large margin.},
  archive   = {C_IROS},
  author    = {Yuxin Tian and Yujie Wang and Ming Ouyang and Xuesong Shi},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635913},
  pages     = {6573-6580},
  title     = {Hierarchical segment-based optimization for SLAM},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stereo plane SLAM based on intersecting lines.
<em>IROS</em>, 6566–6572. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Plane features can be used to reduce drift errors in SLAM systems, especially in indoor environments. It is easy and efficient to extract planes from a dense point cloud, which is commonly generated from a RGB-D camera or a 3D lidar. But when using a stereo camera, it is hard to compute dense point clouds accurately or efficiently. In this paper, we propose a novel method to compute plane parameters using intersecting lines, which are extracted from stereo images. Plane features are commonly extracted from the surface of man-made objects or structures, which have regular shapes and straight edge lines. In three dimensions, two intersecting lines determine a unique plane. Therefore, we extract line segments from both left and right images of a stereo camera. By stereo matching, we compute lines’ endpoints and direction vectors, and then a plane from two intersecting lines is calculated. We discard inaccurate plane features in the frame tracking. Adding such plane features in the stereo SLAM system reduces drift errors and refines the performance. Finally, we build a global map consisting of both points and planes, which can reflect real scene structures. We test our proposed system on public datasets and demonstrate its accurate estimation results, compared with state-of-the-art SLAM systems. To benefit the research of plane-based SLAM, we release our codes at https://github.com/fishmarch/Stereo-Plane-SLAM.},
  archive   = {C_IROS},
  author    = {Xiaoyu Zhang and Wei Wang and Xianyu Qi and Ziwei Liao},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635961},
  pages     = {6566-6572},
  title     = {Stereo plane SLAM based on intersecting lines},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Probabilistic specification learning for planning with
safety constraints. <em>IROS</em>, 6558–6565. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a framework for learning task specifications from demonstrations, while ensuring that the learned specifications do not violate safety constraints. Furthermore, we show how these specifications can be used in a planning problem to control the robot under environments that can be different from those encountered during the learning phase. We formulate the specification learning problem as a grammatical inference problem, using probabilistic automata to represent specifications. The edge probabilities of the resulting automata represent the demonstrator&#39;s preferences. The main novelty in our approach is to incorporate the safety property during the learning process. We prove that the resulting automaton always respects a pre-specified safety property, and furthermore, the proposed method can easily be included in any Evidence-Driven State Merging (EDSM)-based automaton learning scheme. Finally, we introduce a planning algorithm that produces the most desirable plan by maximizing the probability of an accepting trace of the automaton. Case studies show that our algorithm learns the true probability distribution most accurately while maintaining safety. Since, specification is detached from the robot&#39;s environment model, a satisfying plan can be synthesized for a variety of different robots and environments including both mobile robots and manipulators.},
  archive   = {C_IROS},
  author    = {Kandai Watanabe and Nicholas Renninger and Sriram Sankaranarayanan and Morteza Lahijanian},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636712},
  pages     = {6558-6565},
  title     = {Probabilistic specification learning for planning with safety constraints},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A topological approach to finding coarsely diverse paths.
<em>IROS</em>, 6552–6557. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a topological method for finding coarsely diverse pathways. The use of pre-computed paths for online planning in a dynamic context reduces the overhead of re-planning alternate routes. Our algorithm applied the notion of discrete Morse theory to identify critical points incident on the obstacles and used this information to identify and return a diverse set of coarse paths. Three sampling-based planning approaches are converted to topology-aware planners and compared to another that employs the SPARS2 path planning algorithm. We report on the number of coarse pathways found, computation time, and average path length and show that our approach outperformed previously published path diversity algorithms.},
  archive   = {C_IROS},
  author    = {Aakriti Upadhyay and Boris Goldfarb and Chinwe Ekenna},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636714},
  pages     = {6552-6557},
  title     = {A topological approach to finding coarsely diverse paths},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attainment regions in feature-parameter space for high-level
debugging in autonomous robots. <em>IROS</em>, 6546–6551. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Understanding a controller’s performance in different scenarios is crucial for robots that are going to be deployed in safety-critical tasks. If we do not have a model of the dynamics of the world, which is often the case in complex domains, we may need to approximate a performance function of the robot based on its interaction with the environment. Such a performance function gives us insights into the behaviour of the robot, allowing us to fine-tune the controller with manual interventions. In high-dimensionality systems, where the action-state space is large, fine-tuning a controller is non-trivial. To overcome this problem, we propose a performance function whose domain is defined by external features and parameters of the controller. Attainment regions are defined over such a domain defined by feature-parameter pairs, and serve the purpose of enabling prediction of successful execution of the task. The use of the feature-parameter space –in contrast to the action-state space– allows us to adapt, explain and fine-tune the controller over a simpler (i.e., lower dimensional) space. When the robot successfully executes the task, we use the attainment regions to gain insights into the limits of the controller, and its robustness. When the robot fails to execute the task, we use the regions to debug the controller and find adaptive and counterfactual changes to the solutions. Another advantage of this approach is that we can generalise through the use of Gaussian processes regression of the performance function in the high-dimensional space. To test our approach, we demonstrate learning an approximation to the performance function in simulation, with a mobile robot traversing different terrain conditions. Then, with a sample-efficient method, we propagate the attainment regions to a physical robot in a similar environment.},
  archive   = {C_IROS},
  author    = {Simón C. Smith and Subramanian Ramamoorthy},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636336},
  pages     = {6546-6551},
  title     = {Attainment regions in feature-parameter space for high-level debugging in autonomous robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Probabilistically guaranteed satisfaction of temporal logic
constraints during reinforcement learning. <em>IROS</em>, 6531–6537. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel constrained reinforcement learning method for finding optimal policies in Markov Decision Processes while satisfying temporal logic constraints with a desired probability throughout the learning process. An automata-theoretic approach is proposed to ensure the probabilistic satisfaction of the constraint in each episode, which is different from penalizing violations to achieve constraint satisfaction after a sufficiently large number of episodes. The proposed approach is based on computing a lower bound on the probability of constraint satisfaction and adjusting the exploration behavior as needed. We present theoretical results on the probabilistic constraint satisfaction achieved by the proposed approach. We also numerically demonstrate the proposed idea in a drone scenario, where the constraint is to perform periodically arriving pick-up and delivery tasks and the objective is to fly over high-reward zones to simultaneously perform aerial monitoring.},
  archive   = {C_IROS},
  author    = {Derya Aksaray and Yasin Yazıcıoğlu and Ahmet Semi Asarkaya},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636598},
  pages     = {6531-6537},
  title     = {Probabilistically guaranteed satisfaction of temporal logic constraints during reinforcement learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automata-based optimal planning with relaxed specifications.
<em>IROS</em>, 6525–6530. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce an automata-based framework for planning with relaxed specifications. User relaxation preferences are represented as weighted finite state edit systems that capture permissible operations on the specification, substitution and deletion of tasks, with complex constraints on ordering and grouping. We propose a three-way product automaton construction method that allows us to compute minimal relaxation policies for the robots using shortest path algorithms. The three-way product automaton captures the robot’s motion, specification satisfaction, and available relaxations at the same time. Additionally, we consider a bi-objective problem that balances temporal relaxation of deadlines within specifications with changing and deleting tasks. Finally, we present the runtime performance and a case study that highlights different modalities of our framework.},
  archive   = {C_IROS},
  author    = {Disha Kamale and Eleni Karyofylli and Cristian-Ioan Vasile},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635906},
  pages     = {6525-6530},
  title     = {Automata-based optimal planning with relaxed specifications},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sensor selection for detecting deviations from a planned
itinerary. <em>IROS</em>, 6511–6518. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Suppose an agent asserts that it will move through an environment in some way. When the agent executes its motion, how does one verify the claim? The problem arises in a range of contexts including validating safety claims about robot behavior, applications in security and surveillance, and for both the conception and the (physical) design and logistics of scientific experiments. Given a set of feasible sensors to select from, we ask how to choose sensors optimally in order to ensure that the agent’s execution does indeed fit its pre-disclosed itinerary. Our treatment is distinguished from prior work in sensor selection by two aspects: the form the itinerary takes (a regular language of transitions) and that families of sensor choices can be grouped as a single choice. Both are intimately tied together, permitting construction of a product automaton because the same physical sensors (i.e., the same choice) can appear multiple times. This paper establishes the hardness of sensor selection for itinerary validation within this treatment, and proposes an exact algorithm based on an integer linear programming (ILP) formulation that is capable of solving problem instances of moderate size. We demonstrate its efficacy on small-scale case studies, including one motivated by wildlife tracking.},
  archive   = {C_IROS},
  author    = {Hazhar Rahmani and Dylan A. Shell and Jason M. O’Kane},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636582},
  pages     = {6511-6518},
  title     = {Sensor selection for detecting deviations from a planned itinerary},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient computation of map-scale continuous mutual
information on chip in real time. <em>IROS</em>, 6464–6470. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Exploration tasks are essential to many emerging robotics applications, ranging from search and rescue to space exploration. The planning problem for exploration requires determining the best locations for future measurements that will enhance the fidelity of the map, for example, by reducing its total entropy. A widely-studied technique involves computing the Mutual Information (MI) between the current map and future measurements, and utilizing this MI metric to decide the locations for future measurements. However, computing MI for reasonably-sized maps is slow and power hungry, which has been a bottleneck towards fast and efficient robotic exploration. In this paper, we introduce a new hardware accelerator architecture for MI computation that features a low-latency, energy-efficient MI compute core and an optimized memory subsystem that provides sufficient bandwidth to keep the cores fully utilized. The core employs interleaving to counter the recursive algorithm, and workload balancing and numerical approximations to reduce latency and energy consumption. We demonstrate this optimized architecture with a Field-Programmable Gate Array (FPGA) implementation, which can compute MI for all cells in an entire 201-by-201 occupancy grid (e.g., representing a 20.1m-by-20.1m map at 0.1m resolution) in 1.55 ms while consuming 1.7 mJ of energy, thus finally rendering MI computation for the whole map real time and at a fraction of the energy cost of traditional compute platforms. For comparison, this particular FPGA implementation running on the Xilinx Zynq-7000 platform is two orders of magnitude faster and consumes three orders of magnitude less energy per MI map compute, when compared to a baseline GPU implementation running on an NVIDIA GeForce GTX 980 platform. The improvements are more pronounced when compared to CPU implementations of equivalent algorithms.},
  archive   = {C_IROS},
  author    = {Keshav Gupta and Peter Zhi Xuan Li and Sertac Karaman and Vivienne Sze},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636603},
  pages     = {6464-6470},
  title     = {Efficient computation of map-scale continuous mutual information on chip in real time},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Arena-rosnav: Towards deployment of
deep-reinforcement-learning-based obstacle avoidance into conventional
autonomous navigation systems. <em>IROS</em>, 6456–6463. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, mobile robots have become important tools in various industries, especially in logistics. Deep reinforcement learning emerged as an alternative planning method to replace overly conservative approaches and promises more efficient and flexible navigation. However, deep reinforcement learning approaches are not suitable for long-range navigation due to their proneness to local minima and lack of long term memory, which hinders its widespread integration into industrial applications of mobile robotics. In this paper, we propose a navigation system incorporating deep-reinforcement-learning- based local planners into conventional navigation stacks for long-range navigation. Therefore, a framework for training and testing the deep reinforcement learning algorithms along with classic approaches is presented. We evaluated our deep-reinforcement-learning-enhanced navigation system against various conventional planners and found that our system outperforms them in terms of safety, efficiency and robustness.},
  archive   = {C_IROS},
  author    = {Linh Kästner and Teham Buiyan and Lei Jiao and Tuan Anh Le and Xinlin Zhao and Zhengcheng Shen and Jens Lambrecht},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636226},
  pages     = {6456-6463},
  title     = {Arena-rosnav: Towards deployment of deep-reinforcement-learning-based obstacle avoidance into conventional autonomous navigation systems},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CompROS: A composable ROS2 based architecture for real-time
embedded robotic development. <em>IROS</em>, 6449–6455. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot Operating System (ROS) is a de-facto standard robot middleware in many academic and industrial use cases. However, utilizing ROS/ROS2 in safety-critical embedded applications with real-time requirement is challenging because of C1) Non-real-time underlying hardware, C2) No control on the host OS scheduler, C3) Unpredictable dynamic memory allocation, C4) High resource requirement, and C5) Unpredictable execution model for ROS nodes. In this paper, we address these limiting factors by proposing a hardwaresoftware architecture -CompROS- for ROS2 based robotic development in a Multi-Processor System on Chip (MPSoC) platform. The proposed hardware architecture consists of a Hard Real-Time (HRT) RISC-V based subsystem implemented in the Programmable Logic (PL) part of the MPSoC platform, a Soft Real-Time (SRT) ARM-based subsystem in the Processing System (PS) part of the MPSoC platform, and a Non-Real-Time (NRT) PC. While the proposed hardware architecture along with a partitioning layer overcomes the first two limiting factors, the rest are managed by the proposed multi-layer software architecture. We make a bare-metal implementation of XRCE-DDS standard for PL-PS communication, while peer-to-peer PL-PL communication is done through a proposed real-time publish-subscribe approach. The reliable communication for PS-PL communication is done through utilizing C-HEAP protocol. Further, we integrate ROS2 software layers on top of the proposed hardware and software layers. Finally, with respect to C5, we present a real-time execution model of ROS2 nodes by a mapping of ROS2 entities to CompROS entities, which is validated through experimental results. We run ROS2 middleware with an executable size of less than 200 KB on an MPSoC platform.},
  archive   = {C_IROS},
  author    = {Saeid Dehnavi and Martijn Koedam and Andrew Nelson and Dip Goswami and Kees Goossens},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636590},
  pages     = {6449-6455},
  title     = {CompROS: A composable ROS2 based architecture for real-time embedded robotic development},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Smart pointers and shared memory synchronisation for
efficient inter-process communication in ROS on an autonomous vehicle.
<em>IROS</em>, 6441–6448. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite the stringent requirements of a real-time system, the reliance of the Robot Operating System (ROS) on the loopback network interface imposes a considerable overhead on the transport of high bandwidth data, while the nodelet package, which is an efficient mechanism for intra-process communication, does not address the problem of efficient local inter-process communication (IPC). To remedy this, we propose a novel integration into ROS of smart pointers and synchronisation primitives stored in shared memory. These obey the same semantics and, more importantly, exhibit the same performance as their C++ standard library counterparts, making them preferable to other local IPC mechanisms. We present a series of benchmarks for our mechanism - which we call LOT (Low Overhead Transport) - and use them to assess its performance on realistic data loads based on Five’s Autonomous Vehicle (AV) system, and extend our analysis to the case where multiple ROS nodes are running in Docker containers. We find that our mechanism performs up to two orders of magnitude better than the standard IPC via local loopback. Finally, we apply industry-standard profiling techniques to explore the hotspots of code running in both user and kernel space, comparing our implementation against alternatives.},
  archive   = {C_IROS},
  author    = {Costin Iordache and Stephen M. Fendyke and Mike J. Jones and Robert A. Buckley},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636018},
  pages     = {6441-6448},
  title     = {Smart pointers and shared memory synchronisation for efficient inter-process communication in ROS on an autonomous vehicle},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimizing requests for support in context-restricted
autonomy. <em>IROS</em>, 6434–6440. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Adjustable Autonomy is gaining interest as it alleviates robot management costs, which often restrain non-routine applications. Whereas it seems straightforward to account for the availability of helpers when making plans that involve being granted for support in the future, no existing research covers this issue. As a solution, we formalize the first human-centric model that accounts for operator support dynamics when generating adjustable-autonomy plans. We formalize Restricted Autonomy Levels (RAL) within a Markov-based framework for representing when and what level of support the robot should ask for. This model is combined with a formalization of usual aspects of man-machine collaboration: operator availability, risk of denial and withdrawal, effect of teleoperation, risks and consequences for violating RAL restrictions and backup procedures, should violations occur. We empirically demonstrate, through a detailed example and the deployment on a professional-grade security robot, that the generated plans deeply combine the problem-solving activities of the robot with the management of requested human support, leading to improved performance and decreased operator effort. We also analyse the computational costs of computing policies that ensure a zero-chance of RAL violation.},
  archive   = {C_IROS},
  author    = {Loïs Vanhée and Laurent Jeanpierre and Abdel-Illah Mouaddib},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636240},
  pages     = {6434-6440},
  title     = {Optimizing requests for support in context-restricted autonomy},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hybrid path planning for UAV traffic management.
<em>IROS</em>, 6427–6433. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unmanned Aircraft System Traffic Management (UTM) becomes a highly relevant complex challenge, as the UAV activity is rapidly growing bringing more amateur and professional drones to the urban skies. The main concern of managing such a system is safely navigating and controlling hundreds or thousands of drones simultaneously, flying in a crowded dense environments. This paper introduces an innovative approach of hybrid path planning, which tries to make the best out of the commonly used centralized and decentralized planning approaches. The Hybrid Path Planner (HPP) defines two configuration spaces: the Local Zone, which represents the crowded city zone with many obstacles and constrains, and the Global Zone, which represents the outer suburban zone, mostly open space with predefined flight corridors. The HPP server communicates with each UAV, assigning it a close-to-optimal path in the global zone, while leaving the relatively heavy-duty local zone path planning task to be performed by the UAV, mostly using stochastic methods like RRT * . This approach reduces the complex path panning task of the centralized server to a simpler task of calculating only the entry and exit points to and from the global zone. This robust approach supports handling a high number of UAVs, while keeping close to optimal performance.},
  archive   = {C_IROS},
  author    = {Eyal Zehavi and Noa Agmon},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636390},
  pages     = {6427-6433},
  title     = {Hybrid path planning for UAV traffic management},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Force-based formation control of omnidirectional ground
vehicles. <em>IROS</em>, 6419–6426. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Formation control of multi-robot systems has been largely studied due to its wide application domain. Several methods in the literature rely on explicit communication among the robots, which in realistic scenarios may lead to reduced performance or even instability due to delays and packet loss or corruption. Nonetheless, multi-robot coordination based solely on implicit communication has been proposed in cooperative manipulation problems. Taking inspiration from this, we propose a method to solve the formation control problem for a group of ground robots not relying on direct communication among them. Instead, the robots are physically constrained to a common object through elastic cables in order to exploit forces as a means of indirect communication. After deriving the dynamic equations, the control and planning approaches are explained, and the stability of the controlled system is discussed using Lyapunov’s stability theory. Numerical simulations are presented to support the method.},
  archive   = {C_IROS},
  author    = {Chiara Gabellieri and Alessandro Palleschi and Lucia Pallottino},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636453},
  pages     = {6419-6426},
  title     = {Force-based formation control of omnidirectional ground vehicles},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). A resolution adaptive algorithm for the stochastic
orienteering problem with chance constraints. <em>IROS</em>, 6411–6418.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study a stochastic version of the classic orienteering problem where the time to traverse an edge is a continuous random variable. For a given temporal deadline B, our solution produces a policy, i.e., a function that, based on the current position along a solution path and the elapsed time, decides whether to continue along the path or take a shortcut to avoid missing the deadline. The solution is based on a formulation using constrained Markov decision processes to ensure that the deadline is met with a preassigned confidence level. To expedite the computation, a Monte Carlo simulation on an open loop policy is run to determine how to adaptively discretize the temporal dimension and therefore reduce the number of states and the number of optimization variables in the associated linear program. Our results show that the adaptive algorithm matches the performance of the non-adaptive one while taking significantly less time.},
  archive   = {C_IROS},
  author    = {Thomas C. Thayer and Stefano Carpin},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636104},
  pages     = {6411-6418},
  title     = {A resolution adaptive algorithm for the stochastic orienteering problem with chance constraints},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An augmented MDP approach for solving stochastic security
games. <em>IROS</em>, 6405–6410. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel theoretical approach for solving a Stochastic Security Game using augmented Markov Decison Processes and an experimental evaluation. Most of the previous works mentioned in the literature focus on Linear Programming techniques seeking Strong Stackelberg Equilibria through the defender and attacker’s strategy spaces. Although effective, these techniques are computationally expensive and tend to not scale well to very large problems. By fixing the set of the possible defense strategies, our approach is able to use the well-known augmented MDP formalism to compute an optimal policy for an attacker facing a defender patrolling. Experimental results on fully observable cases validate our approach and show good performances in comparison with optimistic and pessimistic approaches. However, these results also highlight the need of scalability improvements and of handling the partial observability cases.},
  archive   = {C_IROS},
  author    = {Romain Châtel and Abdel-Illah Mouaddib},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636495},
  pages     = {6405-6410},
  title     = {An augmented MDP approach for solving stochastic security games},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-robot scheduling for environmental monitoring as a
team orienteering problem. <em>IROS</em>, 6398–6404. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose an evolutionary algorithm for solving the multi-robot orienteering problem where a team of cooperative robots aims to maximize the total information collected by visiting a subset of given nodes within a fixed budget on travel costs. Multi-robot orienteering problems are relevant to applications such as logistic delivery services, precision agriculture, and environmental sampling and monitoring. We consider the case where the information gain at each node is related to the service time each robot spends at the node. As such, we address a variant of the Orienteering Problem where the collected rewards are a function of the time a robot spends at a given location. We present a genetic algorithm solver to this cooperative Team Orienteering Problem with service-time dependent rewards. We evaluate the approach over a diverse set of node configurations and for different team sizes. Lastly, we evaluate the effects of team heterogeneity on overall task performance through numerical simulations.},
  archive   = {C_IROS},
  author    = {Ariella Mansfield and Sandeep Manjanna and Douglas G. Macharet and M. Ani Hsieh},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636854},
  pages     = {6398-6404},
  title     = {Multi-robot scheduling for environmental monitoring as a team orienteering problem},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reduced state value iteration for multi-drone persistent
surveillance with charging constraints. <em>IROS</em>, 6390–6397. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents Reduced State Value Iteration (RSVI), an algorithm to compute policies for Markov Decision Processes (MDPs) that have natural checkpoints, allowing for a solution based on a reduced state space. The algorithm is applied to find policies for multiple drones to persistently surveil an environment subject to charging constraints. RSVI leverages the structure of the true MDP to build an MDP with a smaller state-action space. Monte Carlo simulations are used to estimate transitions between the states in the reduced MDP, which are used in value iteration to compute a policy for the reduced MDP. States in the true MDP are mapped to reduced states. Actions in the reduced space from the policy are then mapped to actions in the full space for execution on the true MDP. Performance of the RSVI policy improves as the state discretization becomes finer, but with increasing computational requirements, thus giving a natural trade-off between computational resources and policy suboptimality. Results of simulated persistent surveillance experiments show that our RSVI policy outperforms a baseline heuristic.},
  archive   = {C_IROS},
  author    = {Patrick H. Washington and Mac Schwager},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636160},
  pages     = {6390-6397},
  title     = {Reduced state value iteration for multi-drone persistent surveillance with charging constraints},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-robot task planning under individual and collaborative
temporal logic specifications. <em>IROS</em>, 6382–6389. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper investigates the task coordination of multi-robot where each robot has a private individual temporal logic task specification; and also has to jointly satisfy a globally given collaborative temporal logic task specification. To efficiently generate feasible and optimized task execution plans for the robots, we propose a hierarchical multi-robot temporal task planning framework, in which a central server allocates the collaborative tasks to the robots, and then individual robots can independently synthesize their task execution plans in a decentralized manner. Furthermore, we propose an execution plan adjusting mechanism that allows the robots to iteratively modify their execution plans via privacy-preserved inter-agent communication, to improve the expected actual execution performance by reducing waiting time in collaborations for the robots. The correctness and efficiency of the proposed method are analyzed and also verified by extensive simulation experiments.},
  archive   = {C_IROS},
  author    = {Ruofei Bai and Ronghao Zheng and Meiqin Liu and Senlin Zhang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636287},
  pages     = {6382-6389},
  title     = {Multi-robot task planning under individual and collaborative temporal logic specifications},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Design and comparison of tails for bird-scale flapping-wing
robots. <em>IROS</em>, 6358–6365. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Flapping-wing robots (so-called ornithopters) are a promising type of platform to perform efficient winged flight and interaction with the environment. However, the control of such vehicles is challenging due to their under-actuated morphology to meet lightweight requirements. Consequently, the flight control of flapping-wing robots is predominantly handled by the tail. Most ornithopters feature a tail with two degrees of freedom but the configuration choice is often arbitrary and without in-depth study. In this paper, we propose a thorough analysis of the design and in-flight performance for three tails. Their design and manufacturing methods are presented, with an emphasis on low weight, which is critical in ornithopters. The aerodynamics of the tails is analyzed through CFD simulations and their performance compared experimentally. The advantages and performance metrics of each configuration are discussed based on flight data. Two types of 3D flight tests were carried out: aggressive heading maneuvers and level turns. The results show that an inverted V-tail outperforms the others regarding maneuverability and stability. From the three configurations, only the inverted V-Tail can perform an aggressive stable banked level turn with a radius of 3.7 m at a turning rate of 1.6 rad/s. This research work describes the impact of the tail configuration choice on the performance of bird-scale flapping-wing robots.},
  archive   = {C_IROS},
  author    = {M.M. Guzmán and C. Ruiz Páez and F. J. Maldonado and R. Zufferey and J. Tormo-Barbero and J.Á Acosta and A. Ollero},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635990},
  pages     = {6358-6365},
  title     = {Design and comparison of tails for bird-scale flapping-wing robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient manoeuvring of quadrotor under constrained space
and predefined accuracy. <em>IROS</em>, 6352–6357. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent times, quadrotors have become immensely applicable in scenarios such as relief operations, infrastructure maintenance, search-and-rescue missions etc. A key control design challenge arises in these applications when the quadrotor has to manoeuvre through constrained spaces such as narrow windows, pipelines in the presence of external disturbances and parametric uncertainties: such conditions necessitate the controller to guarantee predefined tracking accuracy so as to not violate the constraints and simultaneously tackle uncertainties. However, state-of-the-art controllers dealing with constrained system motion are not applicable either for an underactuated system like quadrotor or for an uncertain system dynamics. This work proposes a robust controller that enables the quadrotor to follow a trajectory with predefined tracking accuracy in constrained space as well as to tackle uncertainties stemming from imprecise system modelling and external disturbances. The closed-loop system stability is analysed via the Barrier Lyapunov approach and the effectiveness of the proposed controller is validated via simulation with state of the art.},
  archive   = {C_IROS},
  author    = {Sourish Ganguly and Viswa N. Sankaranarayanan and B. V. S. G. Suraj and Rishabh Dev Yadav and Spandan Roy},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636323},
  pages     = {6352-6357},
  title     = {Efficient manoeuvring of quadrotor under constrained space and predefined accuracy},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Design, optimal guidance and control of a low-cost re-usable
electric model rocket. <em>IROS</em>, 6344–6351. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the last decade, autonomous vertical take-off and landing (VTOL) vehicles have become increasingly important as they lower mission costs thanks to their re-usability. However, their development is complex, rendering even the basic experimental validation of the required advanced guidance and control (G &amp; C) algorithms prohibitively time-consuming and costly. In this paper, we present the design of an inexpensive small-scale VTOL platform that can be built from off-the-shelf components for less than 1000 USD. The vehicle design mimics the first stage of a reusable launcher, making it a perfect test-bed for G &amp; C algorithms. To control the vehicle during ascent and descent, we propose a real-time optimization-based G &amp; C algorithm. The key features are a real-time minimum fuel and free-final-time optimal guidance combined with an offset-free tracking model predictive position controller. The vehicle hardware design and the G &amp; C algorithm are experimentally validated both indoors and outdoor, showing reliable operation in a fully autonomous fashion with all computations done on-board and in real-time.},
  archive   = {C_IROS},
  author    = {Lukas Spannagl and Elias Hampp and Andrea Carron and Jerome Sieber and Carlo Alberto Pascucci and Aldo U. Zgraggen and Alexander Domahidi and Melanie N. Zeilinger},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636430},
  pages     = {6344-6351},
  title     = {Design, optimal guidance and control of a low-cost re-usable electric model rocket},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The new dexterity omnirotor platform: Design, modeling, and
control of a modular, versatile, all-terrain vehicle. <em>IROS</em>,
6336–6343. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Micro Aerial Vehicles (MAV) with Vertical Takeoff and Landing (VTOL) capabilities, such as quadrotors, have offered significant value to many research fields and markets. However, only recently, MAV began to be explored as systems capable of interacting with the environment, performing manipulation tasks, and participating in more versatility-demanding operations. Pursuing the goal of turning flying machines into more versatile instruments, many researchers have resorted to using tilting rotor mechanisms to create new aerial vehicle concepts. Nevertheless, most such new concepts are bulky and lack the required versatility, and are restricted to particular applications. In this work, we address these issues by proposing a novel coaxial, versatile, modular tilt-rotor UAV concept. The Omnirotor platform can apply its full thrust in any direction, regardless of the frame’s orientation where it is mounted. The platform does not have any limitations regarding rotation’s range. It can change its thrust direction continuously without needing to unwind back to a specific configuration. With the addition of control surfaces between the coaxial rotors, the Omnirotor is turned into a functional VTOL MAV with hovering capabilities that can be used as a ground vehicle, a UAV, and an all-terrain vehicle.},
  archive   = {C_IROS},
  author    = {Joao Buzzatto and Pedro H. Mendes and Navin Perera and Karl Stol and Minas Liarokapis},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635995},
  pages     = {6336-6343},
  title     = {The new dexterity omnirotor platform: Design, modeling, and control of a modular, versatile, all-terrain vehicle},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Decentralized control and teleoperation of a multi-UAV
parallel robot based on intrinsic measurements. <em>IROS</em>,
6329–6335. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Aerial manipulators have great potential in accomplishing a variety of aerial tasks. One class of aerial manipulators, multi-UAV parallel robots, consists of multiple UAVs connected to a payload or an end-effector by passive kinematic chains. The primary limitation of such aerial manipulators is the dependence on motion capture (MOCAP) systems that provide precise and high-rate exteroceptive pose measurements of all bodies in a common inertial frame, but which are impractical in the majority of real applications.This paper proposes a novel methodology of controlling multi-UAV parallel robots, using a Flying Parallel Robot (FPR) as a case study, that could be deployed without a system of external localisation. Intrinsic measurements acquired onboard the UAVs are used to recover a set of robot states that avoid using coordinates derived from a global frame and allow control of the robot by teleoperation. Two decentralized control methods are proposed, based on inter-UAV communicating or non-communicating scenarios. Experiments with intrinsic measurements emulated by MOCAP are carried out to show the performance of the proposed method.},
  archive   = {C_IROS},
  author    = {Shiyu Liu and Julian Erskine and Abdelhamid Chriette and Isabelle Fantoni},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636195},
  pages     = {6329-6335},
  title     = {Decentralized control and teleoperation of a multi-UAV parallel robot based on intrinsic measurements},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Autonomous cooperative transportation system involving
multi-aerial robots with variable attachment mechanism. <em>IROS</em>,
6322–6328. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cooperative transportation by multi-aerial robots has the potential to support various payloads and improve fail- safe against dropping. Furthermore, changing the attachment positions of robots according payload characteristics increases the stability of transportation. However, there are almost no transportation systems capable of scaling to the payload weight and size and changing the optimal attachment positions. To address this issue, we propose a cooperative transportation sys- tem comprising autonomously executable software and suitable hardware, covering the entire process, from pre-takeoff setting to controlled flight. The proposed system decides the formation of the attachment positions by prioritizing controllability based on the center of gravity obtained from Bayesian estimations with robot pairs. We investigated the cooperative transportation of an unknown payload larger than that of whole carrier robots through numerical simulations. Furthermore, we performed cooperative transportation of an unknown payload (with a weight of about 3.2 kg and maximum length of 1.76 m) using eight robots. The proposed system was found to be versatile with regard to handling unknown payloads with different shapes and center-of-gravity positions.},
  archive   = {C_IROS},
  author    = {Koshi Oishi and Tomohiko Jimbo},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636145},
  pages     = {6322-6328},
  title     = {Autonomous cooperative transportation system involving multi-aerial robots with variable attachment mechanism},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). R-SNN: An analysis and design methodology for robustifying
spiking neural networks against adversarial attacks through noise
filters for dynamic vision sensors. <em>IROS</em>, 6315–6321. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Spiking Neural Networks (SNNs) aim at providing energy-efficient learning capabilities when implemented on neuromorphic chips with event-based Dynamic Vision Sensors (DVS). This paper studies the robustness of SNNs against adversarial attacks on such DVS-based systems, and proposes R-SNN, a novel methodology for robustifying SNNs through efficient DVS-noise filtering. We are the first to generate adversarial attacks on DVS signals (i.e., frames of events in the spatio-temporal domain) and to apply noise filters for DVS sensors in the quest for defending against adversarial attacks. Our results show that the noise filters effectively prevent the SNNs from being fooled. The SNNs in our experiments provide more than 90\% accuracy on the DVS-Gesture and NMNIST datasets under different adversarial threat models.},
  archive   = {C_IROS},
  author    = {Alberto Marchisio and Giacomo Pira and Maurizio Martina and Guido Masera and Muhammad Shafique},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636718},
  pages     = {6315-6321},
  title     = {R-SNN: An analysis and design methodology for robustifying spiking neural networks against adversarial attacks through noise filters for dynamic vision sensors},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rm-code: Proprioceptive real-time recursive multi-contact
detection, isolation and identification. <em>IROS</em>, 6307–6314. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humanoid robots in unknown environments need to be able to quickly react to contacts in order to ensure safety of humans and their own hardware. For showing useful reactions to contacts, the robot needs information about possibly multiple contacts such as their respective contact locations and wrenches. In this paper, we introduce our algorithm rm-Code, a real-time multi-contact detection, isolation and identification algorithm for tree-structured floating base robots based on generalized external forces and (optional) external wrenches measured by force/torque sensors within the kinematic chain. Those entities have been deduced in the literature using proprioceptive sensing only. Furthermore, the algorithm is fast enough for online computation. To the best of our knowledge, this is the first algorithm combining all of these properties. Rm-Code is quantitatively evaluated in simulation. The results show that our solution is able to accurately solve the problem when fed with perfect input data. In a second step, possible sources of error in the presence of noisy input data are analyzed. It is concluded that purely proprioception based contact isolation and identification in the multi-contact case has certain limitations under realistic conditions. However, these limitations could be overcome easily by integrating simple link contact detection, e.g. bumpers or other similarly simple means.},
  archive   = {C_IROS},
  author    = {Jonathan Vorndamme and Sami Haddadin},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636091},
  pages     = {6307-6314},
  title     = {Rm-code: Proprioceptive real-time recursive multi-contact detection, isolation and identification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reactive and safe road user simulations using neural barrier
certificates. <em>IROS</em>, 6299–6306. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reactive and safe agent modellings are important for nowadays traffic simulator designs and safe planning applications. In this work, we proposed a reactive agent model which can ensure safety without comprising the original purposes, by learning only high-level decisions from expert data and a low level decentralized controller guided by the jointly learned decentralized barrier certificates. Empirical results show that our learned road user simulation models can achieve a significant improvement in safety comparing to state-of-the-art imitation learning and pure control-based methods, while being similar to human agents by having smaller error to the expert data. Moreover, our learned reactive agents are shown to generalize better to unseen traffic conditions, and react better to other road users and therefore can help understand challenging planning problems pragmatically.},
  archive   = {C_IROS},
  author    = {Yue Meng and Zengyi Qin and Chuchu Fan},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636568},
  pages     = {6299-6306},
  title     = {Reactive and safe road user simulations using neural barrier certificates},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Group multi-object tracking for dynamic risk map and safe
path planning. <em>IROS</em>, 6292–6298. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies the group multi-object tracking (MOT) problem in dynamic pedestrian environments, with intended application to safe navigation for autonomous vehicles. We complete a full autonomous vehicle navigation pipeline from object detection, tracking, grouping, to risk map generation and safe path planning. Our main contribution is to instantiate a group multi-object tracking algorithm, which provides the crucial grouped activity information, i.e. group position, group velocity, group size, to the risk map generator, and therewith produce a stable and robust risk map for the downstream safe path planner. Experimental results with real world data show the socially acceptable, robust and stable performance of the proposed algorithm over its individual MOT counterpart.},
  archive   = {C_IROS},
  author    = {Lyuyu Shen and Hongliang Guo and Yechao Bai and Lei Qin and Marcelo Ang and Daniela Rus},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636702},
  pages     = {6292-6298},
  title     = {Group multi-object tracking for dynamic risk map and safe path planning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Measurement-robust control barrier functions: Certainty in
safety with uncertainty in state. <em>IROS</em>, 6286–6291. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The increasing complexity of modern robotic systems and the environments they operate in necessitates the formal consideration of safety in the presence of imperfect measurements. In this paper we propose a rigorous framework for safety-critical control of systems with erroneous state estimates. We develop this framework by leveraging Control Barrier Functions (CBFs) and unifying the method of Backup Sets for synthesizing control invariant sets with robustness requirements—the end result is the synthesis of Measurement-Robust Control Barrier Functions (MR-CBFs). This provides theoretical guarantees on safe behavior in the presence of imperfect measurements and improved robustness over standard CBF approaches. We demonstrate the efficacy of this framework both in simulation and experimentally on a Segway platform using an onboard stereo-vision camera for state estimation.},
  archive   = {C_IROS},
  author    = {Ryan K. Cosner and Andrew W. Singletary and Andrew J. Taylor and Tamas G. Molnar and Katherine L. Bouman and Aaron D. Ames},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636584},
  pages     = {6286-6291},
  title     = {Measurement-robust control barrier functions: Certainty in safety with uncertainty in state},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On compliance and safety with torque-control for robots with
high reduction gears and no joint-torque feedback. <em>IROS</em>,
6262–6269. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we report the safety-oriented framework for controlling the torque in the case of robots with high reduction gears and having no joint torque feedback. This kind of robots suffer from high joint friction and low backdrivability, requiring high gains and integral feedback, which can be dangerous. Our optimization-based framework includes feasibility and safety features borrowed from position control, and we introduce novel ones. We show how we limit the integral terms using a QP-based anti-windup which produces the optimal torque that maintains the best performances under safety limits. We show also a new controller for null-space compliance, providing strong guarantees of convergence in the task-space and ignoring the corresponding null-space where the robot can be moved freely. We validate these features with experiments on one 9 DoF arm of the robot HRP-5P performing a Cartesian task, and then a dual Cartesian / admittance task.},
  archive   = {C_IROS},
  author    = {Mehdi Benallegue and Rafael Cisneros and Abdelaziz Benallegue and Arnaud Tanguy and Adrien Escande and Mitsuharu Morisawa and Fumio Kanehiro},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636081},
  pages     = {6262-6269},
  title     = {On compliance and safety with torque-control for robots with high reduction gears and no joint-torque feedback},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Iterative program synthesis for adaptable social navigation.
<em>IROS</em>, 6256–6261. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot social navigation is influenced by human preferences and environment-specific scenarios such as elevators and doors, thus necessitating end-user adaptability. State-of-the-art approaches to social navigation fall into two categories: model-based social constraints and learning-based approaches. While effective, these approaches have fundamental limitations – model-based approaches require constraint and parameter tuning to adapt to preferences and new scenarios, while learning-based approaches require reward functions, significant training data, and are hard to adapt to new social scenarios or new domains with limited demonstrations.In this work, we propose Iterative Dimension Informed Program Synthesis (IDIPS) to address these limitations by learning and adapting social navigation in the form of human-readable symbolic programs. IDIPS works by combining pro-gram synthesis, parameter optimization, predicate repair, and iterative human demonstration to learn and adapt model-free action selection policies from orders of magnitude less data than learning-based approaches. We introduce a novel predicate repair technique that can accommodate previously unseen social scenarios or preferences by growing existing policies.We present experimental results showing that IDIPS: 1) synthesizes effective policies that model user preference, 2) can adapt existing policies to changing preferences, 3) can extend policies to handle novel social scenarios such as locked doors, and 4) generates policies that can be transferred from simulation to real-world robots with minimal effort.},
  archive   = {C_IROS},
  author    = {Jarrett Holtz and Simon Andrews and Arjun Guha and Joydeep Biswas},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636540},
  pages     = {6256-6261},
  title     = {Iterative program synthesis for adaptable social navigation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online recognition of bimanual coordination provides
important context for movement data in bimanual teleoperated robots.
<em>IROS</em>, 6248–6255. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An important problem in designing human-robot systems is the integration of human intent and performance in the robotic control loop, especially during complex tasks. Bimanual coordination is a complex human behavior that is critical in many fine motor tasks, including robot-assisted surgery. To fully leverage the capabilities of the robot as an intelligent and assistive agent, online recognition of bimanual coordination could be important. Robotic assistance for a suturing task, for example, will be fundamentally different during phases when the suture is wrapped around the instrument (i.e., making a c-loop), than when the ends of the suture are pulled apart. In this study, we develop an online recognition method of bimanual coordination modes (i.e., the directions and symmetries of right and left hand movements) using geometric descriptors of hand motion. We (1) develop this framework based on ideal trajectories obtained during virtual 2D bimanual path following tasks performed by human subjects operating Geomagic Touch haptic devices, (2) test the offline recognition accuracy of bimanual direction and symmetry from human subject movement trials, and (3) evalaute how the framework can be used to characterize 3D trajectories of the da Vinci Surgical System’s surgeon-side manipulators during bimanual surgical training tasks. In the human subject trials, our geometric bimanual movement classification accuracy was 92.3\% for movement direction (i.e., hands moving together, parallel, or away) and 86.0\% for symmetry (e.g., mirror or point symmetry). We also show that this approach can be used for online classification of different bimanual coordination modes during needle transfer, making a C loop, and suture pulling gestures on the da Vinci system, with results matching the expected modes. Finally, we discuss how these online estimates are sensitive to task environment factors and surgeon expertise, and thus inspire future work that could leverage adaptive control strategies to enhance user skill during robot-assisted surgery.},
  archive   = {C_IROS},
  author    = {Jacob R. Boehm and Nicholas P. Fey and Ann Majewicz Fey},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636272},
  pages     = {6248-6255},
  title     = {Online recognition of bimanual coordination provides important context for movement data in bimanual teleoperated robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Not all users are the same: Providing personalized
explanations for sequential decision making problems. <em>IROS</em>,
6240–6247. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There is a growing interest in designing robots that can work alongside humans. Such robots will undoubtedly be expected to explain their behavior and decisions. While generating explanations is an actively researched topic, most works tend to focus on methods that generate explanations that are one size fits all. As in the specifics of the user-model are completely ignored. The handful of works that look at tailoring their explanation to the user’s background rely on having specific models of the users (either analytic models or learned labeling models). The goal of this work is thus to propose an end-to-end adaptive explanation generation system that begins by learning the different types of users that the robot could interact with. Then during the interaction with the target user, it is tasked with identifying the type on the fly and adjust its explanations accordingly. The former is achieved by a data-driven clustering approach while for the latter, we compile our explanation generation problem into a POMDP. We demonstrate the usefulness of our system on two domains using state-of-the-art POMDP solvers. We also report the results of a user study that investigates the benefits of providing personalized explanations in a human-robot interaction setting.},
  archive   = {C_IROS},
  author    = {Utkarsh Soni and Sarath Sreedharan and Subbarao Kambhampati},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636331},
  pages     = {6240-6247},
  title     = {Not all users are the same: Providing personalized explanations for sequential decision making problems},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). What information should a robot convey? <em>IROS</em>,
6232–6239. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic technologies are becoming pervasive within industrial and domestic settings, resulting in more frequent interactions between humans and robots. To ensure these interactions are effective, Human-Robot Interaction (HRI) researchers have argued that robots and humans must establish a shared common ground by communicating fundamental pieces of information to each other, such as their intentions, goals, plans, status, etc. Although a large body of work has explored how robots might signal individual aspects of such information to users, we still know relatively little regarding the importance of such information overall (e.g., is communicating robot status more important than communicating robot goals?). Such information is necessary for robots acting in the wild to create prioritized lists of communicative goals as, at any given time, it is unlikely that a robot will be able to convey all possibly relevant or important aspects of information to users. Prioritizing information for users is a complex problem as many factors might influence information priority, including task context, user expertise, and robot capability. In this work, we first address the current state-of-the-art signaling methods for non-humanoid robots. Second, we take an initial step towards understanding prioritization by exploring what types of information users request, and how the rankings of informational importance that users assign change, in a prototypical shared-environment interaction with three different types of robots. Our results, collected from 150 participants on Amazon’s Mechanical Turk, generally show that users value information related to the robot’s battery, capabilities, task, safety, navigation, communication, and privacy, with user priorities of these items varying across a small ground robot, a large ground robot, and an aerial robot.},
  archive   = {C_IROS},
  author    = {Hooman Hedayati and Mark D. Gross and Daniel Szafir},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635999},
  pages     = {6232-6239},
  title     = {What information should a robot convey?},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Using bayesian optimization to identify optimal exoskeleton
parameters targeting propulsion mechanics: A simulation study.
<em>IROS</em>, 6225–6231. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The long-term goal of this research is to develop methods for training propulsion during walking using robotic exoskeletons that customize their intervention based on the response of an individual.In this study, we first determined the feasibility of modeling the relationship between propulsion mechanics and parameters of a robotic intervention applied at the hip and knee joints as a Gaussian process. Specifically, we used data obtained in a previous experiment that used pulses of torque applied at the hip and knee joint, at early and late stance, to establish the relationship between a 4D control parameter space and the resulting changes in hip extension and propulsive impulse at multiple strides following intervention. We estimated Gaussian models both at the group level and for each subject. Moreover, we used the estimated subject-specific models to simulate virtual human-in-the-loop optimization (HIL) experiments based on Bayesian optimization to establish the optimal settings of acquisition function and seed point selection methods.The estimated subject-specific optimal conditions have large between-subject variability in the kinetic component of propulsion mechanics (propulsive impulse), with only 31\% of subjects featuring a subject-specific optimal point in the surroundings (within a sphere of radius 20\% of each dimension’s range) of the group-level optimal point. Instead, variability of the effects on the kinematic component of propulsion (leg extension) were smaller (75\% of the subjects within the surroundings of the group-optimal point). Virtual HIL experiments indicate that expected improvement is the most effective acquisition method, while no significant effect of seed point selection method was observed. Our study suggests that individualized training may be necessary for inducing desired effects in propulsive force generation during walking.},
  archive   = {C_IROS},
  author    = {GilHwan Kim and Fabrizio Sergi},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635982},
  pages     = {6225-6231},
  title     = {Using bayesian optimization to identify optimal exoskeleton parameters targeting propulsion mechanics: A simulation study},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Asking the right questions: Facilitating semantic constraint
specification for robot skill learning and repair. <em>IROS</em>,
6217–6224. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Developments in human-robot teaming have given rise to significant interest in training methods that enable collaborative agents to safely and successfully execute tasks alongside human teammates. While effective, many existing methods are brittle to changes in the environment and do not account for the preferences of human collaborators. This ineffectiveness is typically due to the complexity of deployment environments and the unique personal preferences of human teammates. These complications lead to behavior that can cause task failure or user discomfort. In this work, we introduce Plan Augmentation and Repair through SEmantic Constraints (PARSEC): a novel algorithm that utilizes a semantic hierarchy to enable novice users to quickly and effectively select constraints using natural language that correct faulty behavior or adapt skills to their preferences. We show through a case study that our algorithm efficiently finds corrective constraints that match the user’s intent, providing a path for novice users to exploit the advantages of constrained motion planning combined with human-in-the-loop skill training.},
  archive   = {C_IROS},
  author    = {Aaquib Tabrez and Jack Kawell and Bradley Hayes},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636375},
  pages     = {6217-6224},
  title     = {Asking the right questions: Facilitating semantic constraint specification for robot skill learning and repair},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving driver situation awareness prediction using human
visual sensory and memory mechanism. <em>IROS</em>, 6210–6216. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Situation awareness (SA) is generally considered as the perception, understanding, and projection of objects’ properties and positions. We believe if the system can sense drivers’ SA, it can appropriately provide warnings for objects that drivers are not aware of. To investigate drivers’ awareness, in this study, a human-subject experiment of driving simulation was conducted for data collection. While a previous predictive model for drivers’ situation awareness utilized drivers’ gaze movement only, this work utilizes object properties, characteristics of human visual sensory and memory mechanism. As a result, the proposed driver SA prediction model achieves over 70\% accuracy and outperforms the baselines.},
  archive   = {C_IROS},
  author    = {Haibei Zhu and Teruhisa Misu and Sujitha Martin and Xingwei Wu and Kumar Akash},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636112},
  pages     = {6210-6216},
  title     = {Improving driver situation awareness prediction using human visual sensory and memory mechanism},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Effect of display response time on brain activity in
human–machine interface commander operation. <em>IROS</em>, 6204–6209.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the recent diversification of operating devices, the demand for input operations that require confirmation of the effect of differences in display response on operability has increased. Regarding display response, previous studies have investigated the threshold time and sense of agency for a delayed response during device operation. However, these studies only focused on subjective evaluations. Therefore, this study aims to clarify the human motor characteristics and activated brain regions based on the differences in display response time during device operation. The target motion is the rotational operation of the cylindrical rotary controller using the index finger and thumb. The experimental conditions involve four types of display response times (the duration from the operation to the indicated response). We measured the brain activity using near-infrared spectroscopy, the muscle activity from a surface myoelectric potential measurement device, and the force data of the index finger and thumb tip obtained from two independent six-axis force/torque sensors. Although the experimental results showed no significant difference in the muscle activity and gripping force, a significant difference was observed in the brain activity and the questionnaire survey by the difference in display response time. This investigation reveals that the difference in display response time affects brain activity and subjective information, clarifying the relationship between brain activity and subjective information.},
  archive   = {C_IROS},
  author    = {Kentaro Oshima and Toru Tsumugiwa and Ryuichi Yokogawa and Mitsuhiro Narusue and Hiroto Nishimura and Yusaku Takeda and Toshihiro Hara},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636452},
  pages     = {6204-6209},
  title     = {Effect of display response time on brain activity in Human–Machine interface commander operation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Design and implementation of a stumble recovery controller
for a knee exoskeleton. <em>IROS</em>, 6196–6203. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a stumble recovery controller for a knee exoskeleton that detects a stumble perturbation; selects an anticipated recovery strategy; and provides appropriate recovery assistance. In order to assess the efficacy of the controller in providing an assistive response to a stumble perturbation, the controller was implemented in a knee exoskeleton and evaluated in a single healthy adult participant against several other controller reactions, and against the participant’s response without an exoskeleton. Results show that the stumble recovery controller successfully detected the perturbation and correctly selected the strategy that matched the participant’s response for all 29 trials in which the exoskeleton was used. Further, results show improvements in stumble recovery metrics when using the exoskeleton with the stumble recovery controller, compared to the control cases of: 1) no change in the nominal controller when stumble is detected; 2) turning off exoskeleton torque when a stumble is detected; and 3) not wearing an exoskeleton.},
  archive   = {C_IROS},
  author    = {Maura Eveld and Shane King and Karl Zelik and Michael Goldfarb},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636879},
  pages     = {6196-6203},
  title     = {Design and implementation of a stumble recovery controller for a knee exoskeleton},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). User controlled interface for tuning robotic knee
prosthesis. <em>IROS</em>, 6190–6195. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The tuning process for a robotic prosthesis is a challenging and time-consuming task both for users and clinicians. An automatic tuning approach using reinforcement learning (RL) has been developed for a knee prosthesis to address the challenges of manual tuning methods. The algorithm tunes the optimal control parameters based on the provided knee joint profile that the prosthesis is expected to replicate during gait safely. This paper presents an intuitive interface designed for the prosthesis users and clinicians to choose the preferred knee joint profile during gait and use the autotuner to replicate in the prosthesis. The interface-based approach is validated by observing the ability of the tuning algorithm to successfully converge to various alternate knee profiles by testing on two able-bodied subjects walking with a robotic knee prosthesis. The algorithm was found to converge successfully in an average duration of 1.15 min for the first subject and 2.31 min for the second subject. Further, the subjects displayed different preferences for optimal profiles reinforcing the need to tune alternate profiles. The implications of the results in the tuning of robotic prosthetic devices are discussed.},
  archive   = {C_IROS},
  author    = {Abbas Alili and Varun Nalam and Minhan Li and Ming Liu and Jennie Si and He Helen Huang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636264},
  pages     = {6190-6195},
  title     = {User controlled interface for tuning robotic knee prosthesis},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Phase-variable control of a powered knee-ankle prosthesis
over continuously varying speeds and inclines. <em>IROS</em>, 6182–6189.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most controllers for lower-limb robotic prostheses require individually tuned parameter sets for every combination of speed and incline that the device is designed for. Because ambulation occurs over a continuum of speeds and inclines, this design paradigm requires tuning of a potentially prohibitively large number of parameters. This limitation motivates an alternative control framework that enables walking over a range of speeds and inclines while requiring only a limited number of tunable parameters. In this work, we present the implementation of a continuously varying kinematic controller on a custom powered knee-ankle prosthesis. The controller uses a phase variable derived from the residual thigh angle, along with real-time estimates of ground inclination and walking speed, to compute the appropriate knee and ankle joint angles from a continuous model of able-bodied kinematic data. We modify an existing phase variable architecture to allow for changes in speeds and inclines, quantify the closed-loop accuracy of the speed and incline estimation algorithms for various references, and experimentally validate the controller by observing that it replicates kinematic trends seen in able-bodied gait as speed and incline vary.},
  archive   = {C_IROS},
  author    = {T. Kevin Best and Kyle R. Embry and Elliott J. Rouse and Robert D. Gregg},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636180},
  pages     = {6182-6189},
  title     = {Phase-variable control of a powered knee-ankle prosthesis over continuously varying speeds and inclines},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sensorimotor-inspired tactile feedback and control improve
consistency of prosthesis manipulation in the absence of direct vision.
<em>IROS</em>, 6174–6181. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The lack of haptically aware upper-limb prostheses forces amputees to rely largely on visual cues to complete activities of daily living. In contrast, non-amputees inherently rely on conscious haptic perception and automatic tactile reflexes to govern volitional actions in situations that do not allow for constant visual attention. We therefore propose a myoelectric prosthesis system that reflects these concepts to aid manipulation performance without direct vision. To implement this design, we constructed two fabric-based tactile sensors that measure contact location along the palmar and dorsal sides of the prosthetic fingers and grasp pressure at the tip of the prosthetic thumb. Inspired by the natural sensorimotor system, we use the measurements from these sensors to provide vibrotactile feedback of contact location and implement a tactile grasp controller with reflexes that prevent over-grasping and object slip. We compare this tactile system to a standard myoelectric prosthesis in a challenging reach-to-pick-and-place task conducted without direct vision; 17 non-amputee adults took part in this single-session between-subjects study. Participants in the tactile group achieved more consistent high performance compared to participants in the standard group. These results show that adding contact-location feedback and reflex control increases the consistency with which objects can be grasped and moved without direct vision in upper-limb prosthetics.},
  archive   = {C_IROS},
  author    = {Neha Thomas and Farimah Fazlollahi and Jeremy D. Brown and Katherine J. Kuchenbecker},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635885},
  pages     = {6174-6181},
  title     = {Sensorimotor-inspired tactile feedback and control improve consistency of prosthesis manipulation in the absence of direct vision},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A powered prosthetic ankle designed for task variability – a
concept validation. <em>IROS</em>, 6153–6158. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ankle joints play key roles in everyday locomotion, such as walking, stair climbing, and sit-to-stand. Despite the achievement in designing powered prosthetic ankles, engineers still face challenges to duplicate the full mechanics of ankle joints, including high torque, large range of motion (ROM), low profile, backdrivability, and efficiency, using electric motors and related transmissions. In this study, our goal was to develop a new active prosthetic ankle, Variable Spring embedded Motor-ball screw (VSeM) ankle, to meet all these requirements at the same time. Using a manually adjustable elastic element, which is parallel with our motor actuator, we can readjust the ROM of VSeM to handle all normal locomotion tasks. VSeM’s capability to mimic human ankle was validated through both bench tests and human subject tests.},
  archive   = {C_IROS},
  author    = {Sameer Upadhye and Chinmay Shah and Ming Liu and Gregory Buckner and He Helen Huang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636324},
  pages     = {6153-6158},
  title     = {A powered prosthetic ankle designed for task variability – a concept validation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An anthropomorphic prosthetic hand with an active,
selectively lockable differential mechanism: Towards affordable
dexterity. <em>IROS</em>, 6147–6152. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Over the last decade, adaptive tendon driven devices have gained an increased interest from the research community for their lightweight, compact, and affordable design features attributed to the utilisation of underactuation, differential mechanisms, and structural compliance. Although adaptive tendon driven devices are capable of efficiently executing stable grasps under significant object pose uncertainties with simplistic control algorithms, they lack the controllability over individual fingers in comparison to traditional fully actuated designs. In this paper, we focus on the development of a selectively lockable differential mechanism that is powered through a small and low torque servo to provide increased autonomy to highly underactuated and adaptive prosthetic hands, without compromising the weight, cost, and compactness of the device. The proposed prosthetic hand is experimentally validated through four tests: i) grasping posture and gesture execution experiments, ii) grasping experiments with everyday life objects, iii) force exertion experiments, and iv) Electromyography (EMG) based control of the prosthetic hand.},
  archive   = {C_IROS},
  author    = {Geng Gao and Anany Dwivedi and Minas Liarokapis},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636621},
  pages     = {6147-6152},
  title     = {An anthropomorphic prosthetic hand with an active, selectively lockable differential mechanism: Towards affordable dexterity},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An under-actuated whippletree mechanism gripper based on
multi-objective design optimization with auto-tuned weights.
<em>IROS</em>, 6139–6146. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current rigid linkage grippers are limited in flexibility, and gripper design optimality relies on expertise, experiments, or arbitrary parameters. Our proposed rigid gripper can accommodate irregular and off-center objects through a whippletree mechanism, improving adaptability. We present a whippletree-based rigid under-actuated gripper and its parametric design multi-objective optimization for a one-wall climbing task. Our proposed objective function considers kinematics and grasping forces simultaneously with a mathematical metric based on a model of an object environment. Our multi-objective problem is formulated as a single kinematic objective function with auto-tuning force-based weight. Our results indicate that our proposed objective function determines optimal parameters and kinematic ranges for our under-actuated gripper in the task environment with sufficient grasping forces.},
  archive   = {C_IROS},
  author    = {Yusuke Tanaka and Yuki Shirai and Zachary Lacey and Xuan Lin and Jane Liu and Dennis Hong},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635872},
  pages     = {6139-6146},
  title     = {An under-actuated whippletree mechanism gripper based on multi-objective design optimization with auto-tuned weights},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Grasping with embedded synergies through a reconfigurable
electric actuation topology. <em>IROS</em>, 6131–6138. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Kinematic and force synergies can be used to reduce the complexity and dimensionality of the motion generation and control problem, as well as facilitate the mechanical implementation of robotic hands. In this paper we present a novel implementation of hardware synergies realized on the actuation level by leveraging a novel reconfigurable electric actuation topology principle. The proposed electric actuation topology enables different actuation synergies by changing the interconnections among the actuators at the electrical/motor driver level. We describe the synergies and their implementation in a port-based context, and elaborate how equivalent hard and soft synergies emerge from the electric power flow within different actuation topologies. We realize the reconfigurable electric actuation topology scheme in the HERI III hand, a novel robust and powerful robotic gripper, also introduced in this paper, resulting in easy to control behaviours like on industrial grippers or underactuated hands, but with the high grasping versatility of fully actuated hands. Finally we present grasping experiments performed on the HERI III hand that clearly show the desired behaviours and validate the innovative proposed scheme.},
  archive   = {C_IROS},
  author    = {Éamon Barrett and Zeyu Ren and Nikos Tsagarakis},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636074},
  pages     = {6131-6138},
  title     = {Grasping with embedded synergies through a reconfigurable electric actuation topology},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-modal robotic gripper with a reconfigurable base:
Improving dexterous manipulation without compromising grasping
efficiency. <em>IROS</em>, 6124–6130. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Design optimization can lead to the development of robotic end-effectors with optimal grasping and dexterous, in-hand manipulation capabilities. In particular, the finger link dimensions have been identified as one of the primary design parameters that affects the performance of a robotic gripper. The ability of a gripper to manipulate objects is mainly attributed to the interaction between a set of coordinated fingers. This coordination is primarily affected by the inter-finger distance. This paper presents a framework for finding an appropriate distance between the finger bases of a two-fingered robotic gripper so as to increase the dexterous manipulation workspace for a range of object sizes. To do that, a parallel multi-start search algorithm is employed to solve a multiparametric optimization problem. The results demonstrate that different distances lead to completely different workspace shapes and that the ratio defined by the area of the optimized workspace (nominator) and the union of all workspaces (denominator) is always significantly less than 1. This means that the area of the union of all workspaces is always larger than the area of the &quot;optimized&quot; workspace. Based on these results a multi-modal robotic gripper with movable finger bases was developed. The proposed gripper can vary the distance between the finger bases online and it offers an increased dexterous manipulation workspace without sacrificing grasping performance.},
  archive   = {C_IROS},
  author    = {Nathan Elangovan and Lucas Gerez and Geng Gao and Minas Liarokapis},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636392},
  pages     = {6124-6130},
  title     = {A multi-modal robotic gripper with a reconfigurable base: Improving dexterous manipulation without compromising grasping efficiency},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computational design of reconfigurable underactuated
linkages for adaptive grippers. <em>IROS</em>, 6117–6123. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an optimization-based structural-parametric synthesis method for reconfigurable closed-chain underactuated linkages for robotic systems that physically interact with the environment with an emphasis on adaptive grasping. The key idea is to implement morphological computation concepts to keep both necessary trajectory-specific holonomic constraints and mechanism adaptivity using variable length links (VLL), while we evolve from a fully actuated to an underactuated system satisfying imposed design requirements. It allows to minimize the number of actuators, weight, and cost but keep high payload and endurance that are not reachable by tendon-driven designs. Despite the method is general enough, for clarity, we demonstrate its use on a number of finger mechanisms for adaptive grippers.},
  archive   = {C_IROS},
  author    = {Ivan I. Borisov and Evgenii E. Khomutov and Sergey A. Kolyubin and Stefano Stramigioli},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636792},
  pages     = {6117-6123},
  title     = {Computational design of reconfigurable underactuated linkages for adaptive grippers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A series elastic, compact differential mechanism: On the
development of adaptive, lightweight robotic grippers and hands.
<em>IROS</em>, 6110–6116. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Differential mechanisms allow the designers of robotic and prosthetic grippers and hands to create devices that require a minimal number of motors in order to grasp a plethora of everyday life objects, leading to light-weight, compact, and low-cost implementations. The working principle of differential mechanisms is simple. They allow the distribution of the forces exerted by a single actuator to multiple outputs (e.g., fingers). This reduction in the number of motors leads to underactuation, which is the use of fewer motors than the available degrees of freedom. But differentials need also to be power-efficient, compact, adaptive, and lightweight. Most of the existing solutions lack at least one of these attributes. In this paper, we focus on the design, modeling, and development of a compact, adaptive, series elastic differential. The proposed mechanism consists of four elastic elements connected in series with the four output attachments. The compression of the elastic elements during grasping allows the gripper or hand to conform to the object’s shape. The efficiency of the differential mechanism is experimentally validated using two different types of experiments, measuring: i) the maximum achievable tension load at the outputs, and ii) the maximum achievable compliance of a single output when all other outputs are blocked. The proposed differential has been employed for the development of a gripper and its efficiency has been assessed by executing grasping tasks with several everyday life objects. The device can be easily replicated using additive manufacturing and off-the-shelf materials and is disseminated in an open-source manner.},
  archive   = {C_IROS},
  author    = {Mojtaba Shahmohammadi and Minas Liarokapis},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636462},
  pages     = {6110-6116},
  title     = {A series elastic, compact differential mechanism: On the development of adaptive, lightweight robotic grippers and hands},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dexterous textile manipulation using electroadhesive
fingers. <em>IROS</em>, 6104–6109. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Handling of fabric is a crucial step in the manufacturing of garments. This task is typically performed by trained workers who manipulate one sheet at a time, thus introducing a bottleneck in the automation of the textile industry. This paper seeks to address the challenge of picking fabric up by proposing a new method of achieving ply-separation. Our approach relies on a finger-tip sized (2 cm 2 ) electroadhesive skin to lift fabric up. A pinch-type grasp is then used to securely hold the separated sheet of fabric, enabling easy manipulation thereafter. The ability to successfully pick up and manipulate a variety of commercial fabrics with diverse materials, shapes, sizes and textures is demonstrated. The ability to handle fabrics 100s of times larger than the electroadhesive skin is unique to our approach. Additionally, we demonstrate the manipulation of non-flat fabrics, a challenge that has not been previously addressed by electroadhesive approaches. We believe that this method introduces a smarter way of handling flexible and limp materials, showing great potential towards automation of garment manufacturing.},
  archive   = {C_IROS},
  author    = {Krishna Manaswi Digumarti and Vito Cacucciolo and Herbert Shea},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636095},
  pages     = {6104-6109},
  title     = {Dexterous textile manipulation using electroadhesive fingers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A three-fingered adaptive gripper with multiple grasping
modes. <em>IROS</em>, 6097–6103. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an underactuated robotic gripper that consists of three fingers. This gripper is driven by seven actuators and capable of grasping a wide range of objects in different working scenarios. A combination of a four-bar mechanism and parallelograms ensures that each finger can provide the basic pinch grasp and power grasp. Detailed fingertip grasping force analysis shows the large payload of this gripper. To fulfill multiple challenging grasping tasks, the fingertip orientation of each finger was designed to be decoupled from the finger flexion motion. Particular emphasis is placed on the environmental contact-based grasp and active transition from the pinch grasp to the power grasp. Detailed analysis shows that the contact-based fingertip grasp could be used to grasp thin objects lying on a flat surface with widths less than 168 mm safely and stably. To prevent over-squeezing of the grasped object in the active transition, the fingertip orientation and finger flexion motion were controlled coordinately. Moreover, a combination of the contact-based grasp and the direct power grasp was also allowed to grasp objects lying on a flat surface and having smooth surfaces robustly. Experimental results demonstrate the effectiveness of the proposed gripper in real-world applications.},
  archive   = {C_IROS},
  author    = {Long Kang and Yang Yang and Jian Yang and Byung-Ju Yi},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636758},
  pages     = {6097-6103},
  title     = {A three-fingered adaptive gripper with multiple grasping modes},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SPHR: A soft pneumatic hybrid robot with extreme shape
changing and lifting abilities. <em>IROS</em>, 6090–6096. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many soft robots are capable of significantly changing their shape, an ability that can offer advantages in many applications. For instance, such a robot can flatten its body to fit under small gaps and expand to move over large obstacles. Further, because these shape changes are usually driven by a pressurized fluid, if they act over a large area, they have the potential to apply large forces to the world. However, when these same shape changes are used for the locomotion of an untethered robot, they tend to result in slow forward movement. Here we present a hybrid soft-rigid elongated-sphere robot that decouples shape change from locomotion. Pairing a compliant, inflatable outer skin, which changes volume by 15x to both fit under and roll over obstacles and can lift objects up to 30 kg, with a wheeled internal carriage, we obtain relatively fast locomotion. A new two-sided controllable adhesive between the internal carriage and the skin enables the carriage to climb vertically inside the skin, allowing the robot to climb external obstacles. We present the design of the robot, simple modeling of its behavior, and experimental testing. Our work advances the area of hybrid soft-rigid robotics by demonstrating how leveraging the strengths of both soft and rigid systems can have quantifiable performance benefits.},
  archive   = {C_IROS},
  author    = {Matthew R. Devlin and Myia M. Dickens and Charles Xiao and Elliot W. Hawkes},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636076},
  pages     = {6090-6096},
  title     = {SPHR: A soft pneumatic hybrid robot with extreme shape changing and lifting abilities},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Origami logic gates for printable robots. <em>IROS</em>,
6084–6089. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Origami robots–often called &quot;printable&quot; robots– created using folding processes have gained extensive attention due to their potential for rapid and accessible design and fabrication through simple structures with complex functionalities. However, almost all origami robots require conventional rigid electronics for control, which may hinder the integration and restrict the potential of these origami systems. Here we introduce origami logic gates that can be built through folding. The major enabling technology is a bistable switch that can switch between two different circuits to control the electrical flow. Based on the origami switch, we develop NOT, AND, and OR logic gates (showing functional completeness) and demonstrate these logic gates through sufficiently powering low-current LEDs. These logic gates are fabricated using cut-and-fold manufacturing and offer a potential way of integrating logic functions directly into origami machines without electronics.},
  archive   = {C_IROS},
  author    = {Wenzhong Yan and Chang Liu and Ankur Mehta},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636420},
  pages     = {6084-6089},
  title     = {Origami logic gates for printable robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Development of a permanent magnet elastomer (PME) infused
soft robot skin for tactile sensing. <em>IROS</em>, 6039–6046. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The skin is an important organ which enables humans to interact with the unstructured environment around. It is perfectly soft and covers the entire body providing immediate feedback even when that part is not directly in the field of vision. With the human skin as an inspiration, in this paper, we develop a novel completely soft robot skin for tactile sensing. The skin utilizes a new type of material called as Permanent Magnet Elastomer (PME) to replace the traditionally used hard permanent magnet for hall effect based tactile sensors. PME is formed by mixing Neodymium particles in a polymer base and using strong magnetization (up to 6 T) for anisotropy and to achieve strong and complete magnetization. The 6-axis soft PME is a perfect replacement for powerful hard magnets. We also do a thorough analysis of this material by infusing it in different types of silicone and as a result the most suitable combinations are selected. Performance tests show that the sensor can detect minute forces like 0.1 N. Moreover, the hysteresis test is carried out and the hysteresis error for our skin is found to be only 1.402\%. An overloading test is also performed by loading the skin up to 64 N to check the robustness. In conclusion, the skin can produce reliable Triaxial force measurements and we present two models of it for smaller and large force range measurements respectively.},
  archive   = {C_IROS},
  author    = {Sahil Shembekar and Mitsuhiro Kamezaki and Peizhi Zhang and Zhuoyi He and Yuhiro Iwamoto and Yasushi Ido and Hiroyuki Sakamoto and Shigeki Sugano},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636817},
  pages     = {6039-6046},
  title     = {Development of a permanent magnet elastomer (PME) infused soft robot skin for tactile sensing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning contact-rich assembly skills using residual
admittance policy. <em>IROS</em>, 6023–6030. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Contact-rich assembly tasks may result in large and unpredictable forces and torques when the locations of the contacting parts are uncertain. The ability to correct the trajectory in response to haptic feedback and accomplish the task despite location uncertainties is an important skill. We hypothesize that this skill would facilitate generalization and support direct transfer from simulations to real world. To reduce sample complexity, we propose to learn a residual admittance policy (RAP). RAP is learned to correct the movements generated by a baseline policy in the framework of dynamic movement primitives. Given the reference trajectories generated by the baseline policy, the action space of RAP is limited to the admittance parameters. Using deep reinforcement learning, a deep neural network is trained to map task specifications to proper admittance parameters. We demonstrate that RAP handles uncertainties in board location, generalizes well over space, size and shape, and facilitates quick transfer learning. Most impressively, we demonstrate that the policy learned in simulations achieves similar robustness to uncertainties, generalization and performance when deployed on an industrial robot (UR5e) without further training. See accompanying video for demonstrations.},
  archive   = {C_IROS},
  author    = {Oren Spector and Miriam Zacksenhouse},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636547},
  pages     = {6023-6030},
  title     = {Learning contact-rich assembly skills using residual admittance policy},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). STFP: Simultaneous traffic scene forecasting and planning
for autonomous driving. <em>IROS</em>, 6016–6022. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous vehicles must be able to understand the surrounding traffic flows and predict the future traffic conditions for planning a safe maneuver. During prediction, the action of autonomous vehicles should be considered, as it influences the interaction between vehicles sharing the same traffic scene and thus influences the future traffic flow. From this perspective, not only should the prediction be considered for planning, but also the action of autonomous vehicles generated by planning should be considered for traffic scene prediction. Therefore, prediction and planning must work interactively at every time step, considering results of each other. In this paper, we present a novel learning-based framework that simultaneously forecasts a nearby traffic scene and plans a maneuver of autonomous vehicle at every time step. Through experiments, we demonstrated that the proposed method exhibits better planning performance than baselines in complex traffic conditions involving various surrounding vehicles.},
  archive   = {C_IROS},
  author    = {Chan Kim and Hyung-Suk Yoon and Seung-Woo Seo and Seong-Woo Kim},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636255},
  pages     = {6016-6022},
  title     = {STFP: Simultaneous traffic scene forecasting and planning for autonomous driving},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Binary neural network in robotic manipulation: Flexible
object manipulation for humanoid robot using partially binarized
auto-encoder on FPGA. <em>IROS</em>, 6010–6015. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A neural network based flexible object manipulation system for a humanoid robot on FPGA is proposed. Although the manipulations of flexible objects using robots attract ever increasing attention since these tasks are the basic and essential activities in our daily life, it has been put into practice only recently with the help of deep neural networks. However such systems have relied on GPU accelerators, which cannot be implemented into the space limited robotic body. Although field programmable gate arrays (FPGAs) are known to be energy efficient and suitable for embedded systems, the model size should be drastically reduced since FPGAs have limited on-chip memory. To this end, we propose &quot;partially&quot; binarized deep convolutional auto-encoder technique, where only an encoder part is binarized to compress model size without degrading the inference accuracy. The model implemented on Xilinx ZCU102 achieves 41.1 frames per second with a power consumption of 3.1 W, which corresponds to 10× and 3.7× improvements from the systems implemented on Core i7 6700K and RTX 2080 Ti, respectively.},
  archive   = {C_IROS},
  author    = {Satoshi Ohara and Tetsuya Ogata and Hiromitsu Awano},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636825},
  pages     = {6010-6015},
  title     = {Binary neural network in robotic manipulation: Flexible object manipulation for humanoid robot using partially binarized auto-encoder on FPGA},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning-based contact status recognition for peg-in-hole
assembly. <em>IROS</em>, 6003–6009. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Opening a lock without vision sensors remains a challenge for robots. Inspired by the ability of a human to open a lock through touch and intuition, a peg-in-hole assembly method for recognizing the relative position and inclination angle of a hole is proposed. We use supervised learning to generate a contact-state model to judge the relative contact state and introduce force control strategies that ensure stable and safe interaction with the environment. Adaptive impedance control is adopted to ensure the stability of the alignment and insertion process. The proposed method is not restricted by the object shape. The system can learn an effective classification model with a small volume of force and torque data and predict the relative contact state of a peg and hole. The proposed method is verified in an experiment in which a bicycle lock is opened at different inclination angles. The proposed method has potential application in the field of industrial assembly.},
  archive   = {C_IROS},
  author    = {Chaojie Yan and Jun Wu and Qiuguo Zhu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636147},
  pages     = {6003-6009},
  title     = {Learning-based contact status recognition for peg-in-hole assembly},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hannes prosthesis control based on regression machine
learning algorithms. <em>IROS</em>, 5997–6002. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The quality of life for upper limb amputees can be greatly improved by the adoption of poly-articulated myoelectric prostheses. Typically, in these applications, a pattern recognition algorithm is used to control the system by converting the recorded electromyographic activity (EMG) into complex multi-degrees of freedom (DoFs) movements. However, there is currently a trade-off between the intuitiveness of the control and the number of active DoFs. We here address this challenge by performing simultaneous multi-joint control of the Hannes system and testing several state-of-the-art classifiers to decode hand and wrist movements. The algorithms discriminated multi-DoF movements from forearm EMG signals of 10 healthy subjects reproducing hand opening-closing, wrist flexion-extension and wrist pronation-supination. We first explored the effect of the number of employed EMG electrodes on device performance through the classifiers optimization in terms of F1Score. We further improved classifiers by tuning their respective hyperparameters in terms of the Embedding Optimization Factor. Finally, three mono-lateral amputees tested the optimized algorithms to intuitively and simultaneously control the Hannes system. We found that the algorithms performances were similar to that of healthy subjects, particularly identifying the Non-Linear Regression classifier as the ideal candidate for prosthetic applications.},
  archive   = {C_IROS},
  author    = {D. Di Domenico and A. Marinelli and N. Boccardo and M. Semprini and L. Lombardi and M. Canepa and S. Stedman and A. Dellacasa Bellingegni and M. Chiappalone and E. Gruppioni and M. Laffranchi and L. De Michieli},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636391},
  pages     = {5997-6002},
  title     = {Hannes prosthesis control based on regression machine learning algorithms},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Coarse-to-fine for sim-to-real: Sub-millimetre precision
across wide task spaces. <em>IROS</em>, 5989–5996. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we study the problem of zero-shot sim-to-real when the task requires both highly precise control with sub-millimetre error tolerance, and wide task space generalisation. Our framework involves a coarse-to-fine controller, where trajectories begin with classical motion planning using ICP-based pose estimation, and transition to a learned end-to-end controller which maps images to actions and is trained in simulation with domain randomisation. In this way, we achieve precise control whilst also generalising the controller across wide task spaces, and keeping the robustness of vision-based, end-to-end control. Real-world experiments on a range of different tasks show that, by exploiting the best of both worlds, our framework significantly outperforms purely motion planning methods, and purely learning-based methods. Furthermore, we answer a range of questions on best practices for precise sim-to-real transfer, such as how different image sensor modalities and image feature representations perform.},
  archive   = {C_IROS},
  author    = {Eugene Valassakis and Norman Di Palo and Edward Johns},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636388},
  pages     = {5989-5996},
  title     = {Coarse-to-fine for sim-to-real: Sub-millimetre precision across wide task spaces},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cooperative transportation robot system using risk-sensitive
stochastic control. <em>IROS</em>, 5981–5988. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a method to determine control input on the basis of minimizing the risk-sensitive cost function and show the results of an experiment in which the method was applied to a cooperative transportation robot system that we have developed. In the robot system, two robots hold a work object to transport without an external fixing device. The mechanism yields the force interaction between the robots and the object, which results in unexpected random errors in transportation. We add a stochastic term to the system model to describe such errors and solve the stochastic differential equation numerically to estimate the cost function. Utilizing the risk-sensitive cost function enables us to find the control input that balances efficiency and safety. The experimental results revealed that the chance of a robot colliding with an obstacle during transportation decreased from 90\% to 7\% compared with the optimal control.},
  archive   = {C_IROS},
  author    = {Shinya Yasuda and Taichi Kumagai and Hiroshi Yoshida},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636238},
  pages     = {5981-5988},
  title     = {Cooperative transportation robot system using risk-sensitive stochastic control},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BEV-net: A bird’s eye view object detection network for
LiDAR point cloud. <em>IROS</em>, 5973–5980. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {LiDAR-only object detection is essential for autonomous driving systems and is a challenging problem. For the representation of a bird’s eye view LiDAR point-cloud, this paper proposes a single-stage object detector. The detector can output classification information and accurate positioning information for multi-category objects. In this paper, the detector’s design methods are detailed from a bird’s eye view LiDAR point-cloud encoding, network design, data augmentation, etc. The detector was evaluated on three challenging datasets: KITTI, nuScenes and Waymo. The experimental results demonstrated that the proposed detector can accurately achieve object detection tasks and the detection speed can reach 26.9 FPS. Both the precision and the speed can meet the requirements of most autonomous driving scenarios.},
  archive   = {C_IROS},
  author    = {Meng Liu and Jianwei Niu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636810},
  pages     = {5973-5980},
  title     = {BEV-net: A bird’s eye view object detection network for LiDAR point cloud},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vehicle dispatch in on-demand ride-sharing with stochastic
travel times. <em>IROS</em>, 5966–5972. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {On-demand ride-sharing is a promising way to improve mobility efficiency and reliability. The quality of passenger experience and the profit achieved by these platforms are strongly affected by the vehicle dispatch policy. However, existing ride-sharing research seldom considers travel time uncertainty, which leads to inaccurate dispatch allocations. This paper proposes a framework for dynamic vehicle dispatch that leverages stochastic travel time models to improve the performance of a fleet of shared vehicles. The novelty of this work includes: (1) a stochastic on-demand ride-sharing scheme to maximize the service rate (percentage of requests served) and reliability (probability of on-time arrival); (2) a technique based on approximate stochastic shortest path algorithms to compute the reliability for a ride-sharing trip; (3) a method to maximize the profit when a penalty for late arrivals is introduced. Based on New York City taxi data, it is shown that by considering travel time uncertainty, ride-sharing service achieves higher service rate, reliability and profit.},
  archive   = {C_IROS},
  author    = {Cheng Li and David Parker and Qi Hao},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636499},
  pages     = {5966-5972},
  title     = {Vehicle dispatch in on-demand ride-sharing with stochastic travel times},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fine-grained off-road semantic segmentation and mapping via
contrastive learning. <em>IROS</em>, 5950–5957. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Road detection or traversability analysis has been a key technique for a mobile robot to traverse complex off-road scenes. The problem has been mainly formulated in early works as a binary classification one, e.g. associating pixels with road or non-road labels. Whereas understanding scenes with fine-grained labels are needed for off-road robots, as scenes are very diverse, and the various mechanical performance of off-road robots may lead to different definitions of safe regions to traverse. How to define and annotate fine-grained labels to achieve meaningful scene understanding for a robot to traverse off-road is still an open question. This research proposes a contrastive learning based method. With a set of human-annotated anchor patches, a feature representation is learned to discriminate regions with different traversability, a method of fine-grained semantic segmentation and mapping is subsequently developed for off-road scene understanding. Experiments are conducted on a dataset of three driving segments that represent very diverse off-road scenes. An anchor accuracy of 89.8\% is achieved by evaluating the matching with human-annotated image patches in cross-scene validation. Examined by associated 3D LiDAR data, the fine-grained segments of visual images are demonstrated to have different levels of toughness and terrain elevation, which represents their semantical meaningfulness. The resultant maps contain both fine-grained labels and confidence values, providing rich information to support a robot traversing complex off-road scenes.},
  archive   = {C_IROS},
  author    = {Biao Gao and Shaochi Hu and Xijun Zhao and Huijing Zhao},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636033},
  pages     = {5950-5957},
  title     = {Fine-grained off-road semantic segmentation and mapping via contrastive learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Finding failures in high-fidelity simulation using adaptive
stress testing and the backward algorithm. <em>IROS</em>, 5944–5949. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Validating the safety of autonomous systems generally requires the use of high-fidelity simulators that adequately capture the variability of real-world scenarios. However, it is generally not feasible to exhaustively search the space of simulation scenarios for failures. Adaptive stress testing (AST) is a method that uses reinforcement learning to find the most likely failure of a system. AST with a deep reinforcement learning solver has been shown to be effective in finding failures across a range of different systems. This approach generally involves running many simulations, which can be very expensive when using a high-fidelity simulator. To improve efficiency, we present a method that first finds failures in a low-fidelity simulator. It then uses the backward algorithm, which trains a deep neural network policy using a single expert demonstration, to adapt the low-fidelity failures to high-fidelity. We have created a series of autonomous vehicle validation case studies that represent some of the ways low-fidelity and high-fidelity simulators can differ, such as time discretization. We demonstrate in a variety of case studies that this new AST approach is able to find failures with significantly fewer high-fidelity simulation steps than are needed when just running AST directly in high-fidelity. As a proof of concept, we also demonstrate AST on NVIDIA’s DriveSim simulator, an industry state-of-the-art high-fidelity simulator for finding failures in autonomous vehicles.},
  archive   = {C_IROS},
  author    = {Mark Koren and Ahmed Nassar and Mykel J. Kochenderfer},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636072},
  pages     = {5944-5949},
  title     = {Finding failures in high-fidelity simulation using adaptive stress testing and the backward algorithm},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Decoder fusion RNN: Context and interaction aware decoders
for trajectory prediction. <em>IROS</em>, 5937–5943. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Forecasting the future behavior of all traffic agents in the vicinity is a key task to achieve safe and reliable autonomous driving systems. It is a challenging problem as agents adjust their behavior depending on their intentions, the others’ actions, and the road layout. In this paper, we propose Decoder Fusion RNN (DF-RNN), a recurrent, attention-based approach for motion forecasting. Our network is composed of a recurrent behavior encoder, an inter-agent multi-headed attention module, and a context-aware decoder. We design a map encoder that embeds polyline segments, combines them to create a graph structure, and merges their relevant parts with the agents’ embeddings. We fuse the encoded map information with further inter-agent interactions only inside the decoder and propose to use explicit training as a method to effectively utilize the information available. We demonstrate the efficacy of our method by testing it on the Argoverse motion forecasting dataset and show its state-of-the-art performance on the public benchmark.},
  archive   = {C_IROS},
  author    = {Edoardo Mello Rella and Jan-Nico Zaech and Alexander Liniger and Luc Van Gool},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636577},
  pages     = {5937-5943},
  title     = {Decoder fusion RNN: Context and interaction aware decoders for trajectory prediction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TemporalFusion: Temporal motion reasoning with multi-frame
fusion for 6D object pose estimation. <em>IROS</em>, 5930–5936. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {6D object pose estimation is an essential task in vision-based robotic grasping and manipulation. Prior works extract spatial features by fusing the RGB image and depth without considering the temporal motion information, limiting their performance in heavy occlusion robotic grasping scenarios. In this paper, we present an end-to-end model named TemporalFusion, which integrates the temporal motion information from RGB-D images for 6D object pose estimation. The core of proposed TemporalFusion model is to embed and fuse the temporal motion information from multi-frame RGB-D sequences, which could handle heavy occlusion in robotic grasping tasks. Furthermore, the proposed deep model can also obtain stable pose sequences, which is essential for real-time robotic grasping tasks. We evaluated the proposed method in the YCB-Video dataset, and experimental results show our model outperforms state-of-the-art approaches. Our code is available at https://github.com/mufengjun260/TemporalFusion21.},
  archive   = {C_IROS},
  author    = {Fengjun Mu and Rui Huang and Ao Luo and Xin Li and Jing Qiu and Hong Cheng},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636583},
  pages     = {5930-5936},
  title     = {TemporalFusion: Temporal motion reasoning with multi-frame fusion for 6D object pose estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving monocular depth estimation by semantic
pre-training. <em>IROS</em>, 5916–5923. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Knowing the distance to nearby objects is crucial for autonomous cars to navigate safely in everyday traffic. In this paper, we investigate monocular depth estimation, which advanced substantially within the last years and is providing increasingly more accurate results while only requiring a single camera image as input. In line with recent work, we use an encoder-decoder structure with so-called packing layers to estimate depth values in a self-supervised fashion. We propose integrating a joint pre-training of semantic segmentation plus depth estimation on a dataset providing semantic labels. By using a separate semantic decoder that is only needed for pre-training, we can keep the network comparatively small. Our extensive experimental evaluation shows that the addition of such pre-training improves the depth estimation performance substantially. Finally, we show that we achieve competitive performance on the KITTI dataset despite using a much smaller and more efficient network.},
  archive   = {C_IROS},
  author    = {Peter Rottmann and Thorbjörn Posewsky and Andres Milioto and Cyrill Stachniss and Jens Behley},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636546},
  pages     = {5916-5923},
  title     = {Improving monocular depth estimation by semantic pre-training},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accurate grid keypoint learning for efficient video
prediction. <em>IROS</em>, 5908–5915. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Video prediction methods generally consume substantial computing resources in training and deployment, among which keypoint-based approaches show promising improvement in efficiency by simplifying dense image prediction to light keypoint prediction. However, keypoint locations are often modeled only as continuous coordinates, so noise from semantically insignificant deviations in videos easily disrupt learning stability, leading to inaccurate keypoint modeling. In this paper, we design a new grid keypoint learning framework, aiming at a robust and explainable intermediate keypoint representation for long-term efficient video prediction. We have two major technical contributions. First, we detect keypoints by jumping among candidate locations in our raised grid space and formulate a condensation loss to encourage meaningful keypoints with strong representative capability. Second, we introduce a 2D binary map to represent the detected grid keypoints and then suggest propagating keypoint locations with stochasticity by selecting entries in the discrete grid space, thus preserving the spatial structure of keypoints in the long-term horizon for better future frame generation. Extensive experiments verify that our method outperforms the state-of-the-art stochastic video prediction methods while saves more than 98\% of computing resources. We also demonstrate our method on a robotic-assisted surgery dataset with promising results. Our code is available at https://github.com/xjgaocs/Grid-Keypoint-Learning.},
  archive   = {C_IROS},
  author    = {Xiaojie Gao and Yueming Jin and Qi Dou and Chi-Wing Fu and Pheng-Ann Heng},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636874},
  pages     = {5908-5915},
  title     = {Accurate grid keypoint learning for efficient video prediction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-supervised optical flow with spiking neural networks
and event based cameras. <em>IROS</em>, 5892–5899. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Optical flow can be leveraged in robotic systems for obstacle detection where low latency solutions are critical in highly dynamic settings. While event-based cameras have changed the dominant paradigm of sending by encoding stimuli into spike trails, offering low bandwidth and latency, events are still processed with traditional convolutional networks in GPUs defeating, thus, the promise of efficient low capacity low power processing that inspired the design of event sensors. In this work, we introduce a shallow spiking neural network for the computation of optical flow consisting of Leaky Integrate and Fire neurons.Optical flow is predicted as the synthesis of motion orientation selective channels. Learning is accomplished by Back-propapagation Through Time. We present promising results on events recorded in real &quot;in the wild&quot; scenes that has the capability to use only a small fraction of the energy consumed in CNNs deployed on GPUs.},
  archive   = {C_IROS},
  author    = {Kenneth Chaney and Artemis Panagopoulou and Chankyu Lee and Kaushik Roy and Kostas Daniilidis},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635975},
  pages     = {5892-5899},
  title     = {Self-supervised optical flow with spiking neural networks and event based cameras},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast and unsupervised non-local feature learning for direct
volume rendering of 3D medical images. <em>IROS</em>, 5886–5891. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To improve the efficiency of medical visualization for computer aided surgery, we propose a fast and unsupervised 3D-CNN based non-local feature learning network. The proposed network consists of an encoder structure and a decoder structure. The encoder of the network projects the cube into a high-dimensional feature space, and the decoder of the network reconstructs the cube from the feature space. The decoder of the network serves as a dictionary shared by the cube to enforce the features for similar parts to be similar although they may distribute at disjointed locations. With such structures, the network is able to extract non-local features of the entire data. Moreover, a sparse constraint is incorporated into the network to increase the discriminative of the non-local features. Then the extracted non-local features of each voxel are fused with the corresponding position matrix and Hessian matrix for the voxel classification using Random Forest. Finally, a multidimensional transfer function is designed to enable the volume rendering. Experimental results demonstrate that the proposed method outperforms the state-of-the-art methods with much less training time.},
  archive   = {C_IROS},
  author    = {Xinmei Fu and Zhenzhou Shao and Ying Qu and Yong Guan and Yibo Zou and Zhiping Shi and Jindong Tan},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636042},
  pages     = {5886-5891},
  title     = {Fast and unsupervised non-local feature learning for direct volume rendering of 3D medical images},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). OPEn: An open-ended physics environment for learning without
a task. <em>IROS</em>, 5878–5885. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans have mental models that allow them to plan, experiment, and reason in the physical world. How should an intelligent agent go about learning such models? In this paper, we will study if models of the world learned in an open-ended physics environment, without any specific tasks, can be reused for downstream physics reasoning tasks. To this end, we build a benchmark Open-ended Physics Environment (OPEn) and also design several tasks to test learning representations in this environment explicitly. This setting reflects the conditions in which real agents (i.e. rolling robots) find themselves, where they may be placed in a new kind of environment and must adapt without any teacher to tell them how this environment works. This setting is challenging because it requires solving an exploration problem in addition to a model building and representation learning problem. We test several existing RL-based exploration methods on this benchmark and find that an agent using unsupervised contrastive learning for representation learning, and impact-driven learning for exploration, achieved the best results. However, all models still fall short in sample efficiency when transferring to the downstream tasks. We expect that OPEn will encourage the development of novel rolling robot agents that can build reusable mental models of the world that facilitate many tasks.},
  archive   = {C_IROS},
  author    = {Chuang Gan and Abhishek Bhandwaldar and Antonio Torralba and Joshua B. Tenenbaum and Phillip Isola},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636830},
  pages     = {5878-5885},
  title     = {OPEn: An open-ended physics environment for learning without a task},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed sampling-based planning for non-myopic active
information gathering. <em>IROS</em>, 5872–5877. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the problem of active information gathering for multi-robot systems. Specifically, we consider scenarios where robots are tasked with reducing uncertainty of dynamical hidden states evolving in complex environments. The majority of existing information gathering approaches are centralized and, therefore, they cannot be applied to distributed robot teams where communication to a central user is not available. To address this challenge, we propose a novel distributed sampling-based planning algorithm that can significantly increase robot and target scalability while decreasing computational cost. In our non-myopic approach, all robots build in parallel local trees exploring the information space and their corresponding motion space. As the robots construct their respective local trees, they communicate with their neighbors to exchange and aggregate their local beliefs about the hidden state through a distributed Kalman filter. We show that the proposed algorithm is probabilistically complete and asymptotically optimal. We provide extensive simulation results that demonstrate the scalability of the proposed algorithm and that it can address large-scale, multi-robot information gathering tasks, that are computationally challenging for centralized methods.},
  archive   = {C_IROS},
  author    = {Mariliza Tzes and Yiannis Kantaros and George J. Pappas},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636101},
  pages     = {5872-5877},
  title     = {Distributed sampling-based planning for non-myopic active information gathering},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimization-based robot team exploration considering
attrition and communication constraints. <em>IROS</em>, 5864–5871. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Exploring robots may fail due to environmental hazards. Thus, robots need to account for the possibility of failure to plan the best exploration paths. Optimizing expected utility enables robots to find plans that balance achievable reward with the inherent risks of exploration. Moreover, when robots rendezvous and communicate to exchange observations, they increase the probability that at least one robot is able to return with the map. Optimal exploration is NP-hard, so we apply a constraint-based approach to enable highly-engineered solution techniques. We model exploration under the possibility of robot failure and communication constraints as an integer, linear program and a generalization of the Vehicle Routing Problem. Empirically, we show that for several scenarios, this formulation produces paths within 50\% of a theoretical optimum and achieves twice as much reward as a baseline greedy approach.},
  archive   = {C_IROS},
  author    = {Matthew A. Schack and John G. Rogers and Qi Han and Neil T. Dantam},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636029},
  pages     = {5864-5871},
  title     = {Optimization-based robot team exploration considering attrition and communication constraints},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Relative localization of mobile robots with multiple
ultra-WideBand ranging measurements. <em>IROS</em>, 5857–5863. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Relative localization between autonomous robots without infrastructure is crucial to achieve their navigation, path planning, and formation in many applications, such as emergency response, where acquiring a prior knowledge of the environment is not possible. The traditional Ultra-WideBand (UWB)-based approach provides a good estimation of the distance between the robots, but obtaining the relative pose (including the displacement and orientation) remains challenging. We propose an approach to estimate the relative pose between a group of robots by equipping each robot with multiple UWB ranging nodes. We determine the pose between two robots by minimizing the residual error of the ranging measurements from all UWB nodes. To improve the localization accuracy, we propose to utilize the odometry constraints through a sliding window-based optimization. The optimized pose is then fused with the odometry in a particle filtering for pose tracking among a group of mobile robots. We have conducted extensive experiments to validate the effectiveness of the proposed approach.},
  archive   = {C_IROS},
  author    = {Zhiqiang Cao and Ran Liu and Chau Yuen and Achala Athukorala and Benny Kai Kiat Ng and Muraleetharan Mathanraj and U-Xuan Tan},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636017},
  pages     = {5857-5863},
  title     = {Relative localization of mobile robots with multiple ultra-WideBand ranging measurements},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Meta preference learning for fast user adaptation in
human-supervisory multi-robot deployments. <em>IROS</em>, 5851–5856. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As multi-robot systems (MRS) are widely used in various tasks such as natural disaster response and social security, people enthusiastically expect an MRS to be ubiquitous that a general user without heavy training can easily operate. However, humans have various preferences on balancing between task performance and safety, imposing different requirements onto MRS control. Failing to comply with preferences makes people feel difficult in operation and decreases human willingness of using an MRS. Therefore, to improve social acceptance as well as performance, there is an urgent need to adjust MRS behaviors according to human preferences before triggering human corrections, which increases cognitive load. In this paper, a novel Meta Preference Learning (MPL) method was developed to enable an MRS to fast adapt to user preferences. MPL based on meta learning mechanism can quickly assess human preferences from limited instructions; then, a neural network based preference model adjusts MRS behaviors for preference adaption. To validate method effectiveness, a task scenario &quot;An MRS searches victims in an earthquake disaster site&quot; was designed; 20 human users were involved to identify preferences as {&quot;aggressive&quot;, &quot;medium&quot;, &quot;reserved&quot;}; based on user guidance and domain knowledge, about 20,000 preferences were simulated to cover different operations related to {&quot;task quality&quot;, &quot;task progress&quot;, &quot;robot safety&quot;}. The effectiveness of MPL in preference adaption was validated by the reduced duration and frequency of human interventions.},
  archive   = {C_IROS},
  author    = {Chao Huang and Wenhao Luo and Rui Liu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636515},
  pages     = {5851-5856},
  title     = {Meta preference learning for fast user adaptation in human-supervisory multi-robot deployments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detection and inference of randomness-based behavior for
resilient multi-vehicle coordinated operations. <em>IROS</em>,
5844–5850. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A resilient multi-vehicle system cooperatively performs tasks by exchanging information, detecting, and removing cyber attacks that have the intent of hijacking or diminishing performance of the entire system. In this paper, we propose a framework to: i) detect and isolate misbehaving vehicles in the network, and ii) securely encrypt information among the network to alert and attract nearby vehicles toward points of interest in the environment without explicitly broadcasting safety-critical information. To accomplish these goals, we lever-age a decentralized virtual spring-damper mesh physics model for formation control on each vehicle. To discover inconsistent behavior of any vehicle in the network, we consider an approach that monitors for changes in sign behavior of an inter-vehicle residual that does not match with an expectation. Similarly, to disguise important information and trigger vehicles to switch to different behaviors, we leverage side-channel information on the state of the vehicles and characterize a hidden spring-damper signature model detectable by neighbor vehicles. Our framework is demonstrated in simulation and experiments on formations of unmanned ground vehicles (UGVs) in the presence of malicious man-in-the-middle communication attacks.},
  archive   = {C_IROS},
  author    = {Paul J Bonczek and Nicola Bezzo},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635899},
  pages     = {5844-5850},
  title     = {Detection and inference of randomness-based behavior for resilient multi-vehicle coordinated operations},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Impedance-based collision reaction strategy via internal
stress loading in cooperative manipulation. <em>IROS</em>, 5837–5843.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cooperative manipulation systems inherently cause internal stress on the common object. Many works have proposed methods to eliminate this internal stress. However, in this paper, we show that this property can be cautiously leveraged to compensate for external disturbance on the cooperative system, particularly disturbances that occur due to collision along the links of one of the cooperating robots. We present an impedance-based scheme to control the level of compensation, thereby regulating the internal stress on the object due to the applied compensation wrenches. Previously, we introduced a method to compensate for collision with one arm of the system, but that approach sometimes caused untenable stress on the object. With the impedance-based compensation strategy presented in this paper, a suitable trade-off between maintaining the desired pose of the grasped object and limiting the permissible internal stress on the object, is achieved. We demonstrate our approach by using two kuka arms to cooperatively grasp and lift a rod in simulation.},
  archive   = {C_IROS},
  author    = {Victor Aladele and Seth Hutchinson},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636135},
  pages     = {5837-5843},
  title     = {Impedance-based collision reaction strategy via internal stress loading in cooperative manipulation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Event-triggered control for weight-unbalanced directed robot
networks. <em>IROS</em>, 5831–5836. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We develop an event-triggered control strategy for a weighted-unbalanced directed homogeneous robot network to reach a dynamic consensus in this work. We present some guarantees for synchronizing a robot network when all robots have access to the reference and when a limited number of robots have access. The proposed event-triggered control can reduce and avoid the periodic updating of the signals. Unlike some current control methods, we prove stability by making use of a logarithmic norm, which extends the possibilities of the control law to be applied to a wide range of directed graphs, in contrast to other works where the event-triggered control can be only implemented over strongly connected and weight-balanced digraphs. We test the performance of our algorithm by carrying out experiments both in simulation and in a real team of robots.},
  archive   = {C_IROS},
  author    = {Juan D. Pabon and Gustavo A. Cardona and Nestor I. Ospina and Juan Calderon and Eduardo Mojica-Nava},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636574},
  pages     = {5831-5836},
  title     = {Event-triggered control for weight-unbalanced directed robot networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Refuel scheduling for multirobot charging-on-demand.
<em>IROS</em>, 5825–5830. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we consider the refuel scheduling problem for a team of ground robots deployed in &quot;aislelike&quot; environments wherein the robots are constrained to move along rows. In order to maintain a minimum service rate or throughput for the ground robots, we investigate the problem of scheduling a team of mobile charging stations deployed to replace the batteries on-board the ground robots without any interruption in their task. We propose two scheduling schemes for the mobile chargers to serve the ground robots for long-term service, and derive the parameters associated with the system required for persistent uninterrupted operation.},
  archive   = {C_IROS},
  author    = {Tianshuang Gao and Yan Tian and Sourabh Bhattacharya},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636815},
  pages     = {5825-5830},
  title     = {Refuel scheduling for multirobot charging-on-demand},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Human-aware navigation planner for diverse human-robot
interaction contexts. <em>IROS</em>, 5817–5824. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As more robots are being deployed into human environments, a human-aware navigation planner needs to handle multiple contexts that occur in indoor and outdoor environments. In this paper, we propose a tunable human-aware robot navigation planner that can handle a variety of human-robot contexts. We present the architecture of the system and discuss the features along with some implementation details. Then we present a detailed analysis of various simulated human-robot contexts using the proposed planner. Further, we show that our system performs better when compared with an exiting human-aware planner in various contexts. Finally, we show the results in a real-world scenario after deploying our system on a real robot.},
  archive   = {C_IROS},
  author    = {Phani Teja Singamaneni and Anthony Favier and Rachid Alami},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636613},
  pages     = {5817-5824},
  title     = {Human-aware navigation planner for diverse human-robot interaction contexts},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robotic jigsaw: A non-holonomic cutting robot and path
planning algorithm. <em>IROS</em>, 5809–5816. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bladed tools such as jigsaws are common tools for wood workers on job-sites and in workshops, but do not currently have sufficient autonomous hardware or path planning algorithms to enable automation. Here we present a system of an autonomous robot and a path planning algorithm for automating jigsaw operations. The robot can drill holes, insert the jigsaw, and cut plywood. Our algorithm converts complex shapes into paths for the jigsaw, drill holes, and traversal movements for the robot. The algorithm decomposes input shapes into cuttable sections and determines possible locations for drilling entry holes for inserting the blade. We cast the drill hole problem as a set coverage problem with a trade-off between number of holes and cutting distance. We characterize the algorithm on a series of shapes and determined the algorithm found valid solutions. We executed an example on the robot to demonstrate the end-to-end system.},
  archive   = {C_IROS},
  author    = {Haisen Zhao and Yash Talwekar and Wenqing Lan and Chetan Sharma and Daniela Rus and Adriana Schulz and Jeffrey I Lipton},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636066},
  pages     = {5809-5816},
  title     = {Robotic jigsaw: A non-holonomic cutting robot and path planning algorithm},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Constrained iterative LQG for real-time chance-constrained
gaussian belief space planning. <em>IROS</em>, 5801–5808. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion planning under uncertainty is of significant importance for safety-critical systems such as autonomous vehicles. Such systems have to satisfy necessary constraints (e.g., collision avoidance) with potential uncertainties coming from either disturbed system dynamics or noisy sensor measurements. However, existing motion planning methods cannot efficiently find the robust optimal solutions under general nonlinear and non-convex settings. In this paper, we formulate such problem as chance-constrained Gaussian belief space planning and propose the constrained iterative Linear Quadratic Gaussian (CILQG) algorithm as a real-time solution. In this algorithm, we iteratively calculate a Gaussian approximation of the belief and transform the chance-constraints. We evaluate the effectiveness of our method in simulations of autonomous driving planning tasks with static and dynamic obstacles. Results show that CILQG can handle uncertainties more appropriately and has faster computation time than baseline methods.},
  archive   = {C_IROS},
  author    = {Jianyu Chen and Yutaka Shimizu and Liting Sun and Masayoshi Tomizuka and Wei Zhan},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636187},
  pages     = {5801-5808},
  title     = {Constrained iterative LQG for real-time chance-constrained gaussian belief space planning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Safe navigation in human occupied environments using
sampling and control barrier functions. <em>IROS</em>, 5794–5800. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sampling-based methods such as Rapidly-exploring Random Trees (RRTs) have been widely used for generating motion paths for autonomous mobile systems. In this work, we extend time-based RRTs with Control Barrier Functions (CBFs) to generate, safe motion plans in dynamic environments with many pedestrians. Our framework is based upon a human motion prediction model which is well suited for indoor narrow environments. We demonstrate our approach on a high-fidelity model of the Toyota Human Support Robot navigating in narrow corridors. We show in simulation results that our proposed online method can navigate safely in the presence of moving agents with unknown dynamics.},
  archive   = {C_IROS},
  author    = {Keyvan Majd and Shakiba Yaghoubi and Tomoya Yamaguchi and Bardh Hoxha and Danil Prokhorov and Georgios Fainekos},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636406},
  pages     = {5794-5800},
  title     = {Safe navigation in human occupied environments using sampling and control barrier functions},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Combined stochastic-deterministic predictive control using
local-minima free navigation. <em>IROS</em>, 5788–5793. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work proposes a model predictive control approach of a wheeled mobile robot based on a local-minima free navigation function. The constructed navigation function includes information on a goal location and obstacles. Novel conservative navigation is introduced that is simple to compute and yields convergent control behavior. To solve the optimization problem the combined optimization is proposed by a fixed candidate set and particle swarm optimization. The efficiency of the proposed approaches is validated on simulations, by experimental results on the Husky A200 mobile robot and comparisons to Stable Sparse RRT planner.},
  archive   = {C_IROS},
  author    = {Gregor Klančar and Marija Seder},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636306},
  pages     = {5788-5793},
  title     = {Combined stochastic-deterministic predictive control using local-minima free navigation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust sample-based output-feedback path planning.
<em>IROS</em>, 5780–5787. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel approach for sampling-based and control-based motion planning. We combine a representation of the environment obtained via a modified version of optimal Rapidly-exploring Random Trees (RRT * ), with landmark-based output-feedback controllers obtained via Control Lyapunov Functions, Control Barrier Functions, and robust Linear Programming. Our solution inherits many benefits of RRT * -like algorithms, such as the ability to implicitly handle arbitrarily complex obstacles. Additionally, it extends planning beyond the discrete nominal paths, as feedback controllers can correct deviations from such paths, and are robust to discrepancies between the planning and real environment maps. We test our algorithms first in simulations and then in experiments, evaluating the robustness of the approach to practical conditions, such as deformations of the environment, mismatches in the dynamical model of the robot, and measurements acquired with a camera with a limited field of view.},
  archive   = {C_IROS},
  author    = {Mahroo Bahreinian and Marc Mitjans and Roberto Tron},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636273},
  pages     = {5780-5787},
  title     = {Robust sample-based output-feedback path planning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning continuous cost-to-go functions for non-holonomic
systems. <em>IROS</em>, 5772–5779. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a supervised learning method to generate continuous cost-to-go functions of non-holonomic systems directly from the workspace description. Supervision from informative examples reduces training time and improves network performance. The manifold representing the optimal trajectories of a non-holonomic system has high-curvature regions which can not be efficiently captured with uniform sampling. To address this challenge, we present an adaptive sampling method which makes use of sampling based planners along with local, closed-form solutions to generate training samples. The cost-to-go function over a specific workspace is represented as a neural network whose weights are generated by a second, higher order network. The networks are trained in an end-to-end fashion. In our previous work, this architecture was shown to successfully learn to generate the cost-to-go functions of holonomic systems using uniform sampling. In this work, we show that uniform sampling fails for non-holonomic systems. However, with the proposed adaptive sampling methodology, our network can generate near-optimal trajectories for non-holonomic systems while avoiding obstacles. Experiments show that our method is two orders of magnitude faster compared to traditional approaches in cluttered environments.},
  archive   = {C_IROS},
  author    = {Jinwook Huh and Daniel D. Lee and Volkan Isler},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636139},
  pages     = {5772-5779},
  title     = {Learning continuous cost-to-go functions for non-holonomic systems},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discrete optimization of adaptive state lattices for
iterative motion planning on unmanned ground vehicles. <em>IROS</em>,
5764–5771. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robust motion planners for unmanned ground vehicles must minimize risk while obeying vehicle mobility constraints. Algorithms such as the State Lattice (SL) utilize offline computation to generate expressive control sets which form recombinant search spaces, enabling the use of heuristic search to efficiently produce feasible motion plans online. The Adaptive State Lattice (ASL) demonstrated that local optimizations of the continuous states explored by heuristic search can produce lower-cost solutions in less time than more densely sampled unadapted lattices in sufficiently complex environments. However, the computational cost of this online adaptation limits the application of ASL for mobile robot navigation. We present the Efficiently Adaptive State Lattice (EASL), a novel formalism for online discrete ASL adaptation to overcome this limitation. By discretizing the space of states considered during adaptation, EASL limits the set of feasible motions which could arise during search. This permits the precomputation of an approximation of all motions that could be expressed by an ASL. This approximation removes the online trajectory generation component of the ASL while retaining the benefits of lattice adaptation and enables the use of precomputed swaths for evaluating edge costs. Experimental results demonstrate how an EASL-based planner can generate lower-cost paths than a SL-based planner in roughly equal to or less than the same amount of time.},
  archive   = {C_IROS},
  author    = {Benned Hedegaard and Ethan Fahnestock and Jacob Arkin and Ashwin Menon and Thomas M. Howard},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636181},
  pages     = {5764-5771},
  title     = {Discrete optimization of adaptive state lattices for iterative motion planning on unmanned ground vehicles},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reinforcement learning for vision-based object manipulation
with non-parametric policy and action primitives. <em>IROS</em>,
5756–5763. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The object manipulation is a crucial ability for a service robot, but it is hard to solve with reinforcement learning due to some reasons such as sample efficiency. In this paper, to tackle this object manipulation, we propose a novel framework, AP-NPQL (Non-Parametric Q Learning with Action Primitives), that can efficiently solve the object manipulation with visual input and sparse reward, by utilizing a nonparametric policy for reinforcement learning and appropriate behavior prior for the object manipulation. We evaluate the efficiency and the performance of the proposed AP-NPQL for four object manipulation tasks on simulation (pushing plate, stacking box, flipping cup, and picking and placing plate), and it turns out that our AP-NPQL outperforms the state-of-the-art algorithms based on parametric policy and behavior prior in terms of learning time and task success rate. We also successfully transfer and validate the learned policy of the plate pick-and-place task to the real robot in a sim-to-real manner.},
  archive   = {C_IROS},
  author    = {Dongwon Son and Myungsin Kim and Jaecheol Sim and Wonsik Shin},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636563},
  pages     = {5756-5763},
  title     = {Reinforcement learning for vision-based object manipulation with non-parametric policy and action primitives},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GhostPose: Multi-view pose estimation of transparent objects
for robot hand grasping. <em>IROS</em>, 5749–5755. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pose estimation is a key challenge in robot manipulation and grasping task. Current object pose estimation approaches based on 3D models and depth sensor information have difficulties to handle transparent objects because of the limitation to capture the accurate depth information. To address these issues, we present a 6DoF pose estimation approach, called GhostPose, which utilizes a novel 3D bounding box prediction network and multi-view geometry with cameras on manipulator robot. Our 3D bounding box prediction network is simple and light-weight by adding a small branch to a one-stage object detector. The network detects 2D projections of 3D bounding box vertices. Then, 3D points are reconstructed from the 2D results of the multiple viewpoints with camera motion information, i.e. extrinsic parameters, calculated from the robot joint angles. We also present generalized pose definition to address pose ambiguity of symmetric objects and keep consistency of geometric properties around feature points across both of the symmetric and asymmetric objects. Comparing with the previous pose estimation approaches, GhostPose is more generalized to environments and object types, because it does not require 3D models, object specific key points, predefined stereo settings and depth map. In experiments, it outperforms a state-of-the-art approach and shows generalized properties by applying to a real manipulator robot grasping system.},
  archive   = {C_IROS},
  author    = {Jaesik Chang and Minju Kim and Seongmin Kang and Heungwoo Han and Sunpyo Hong and Kyunghun Jang and Sungchul Kang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636459},
  pages     = {5749-5755},
  title     = {GhostPose: Multi-view pose estimation of transparent objects for robot hand grasping},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph-based task-specific prediction models for interactions
between deformable and rigid objects. <em>IROS</em>, 5741–5748. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Capturing scene dynamics and predicting the future scene state is challenging but essential for robotic manipulation tasks, especially when the scene contains both rigid and deformable objects. In this work, we contribute a simulation environment and generate a novel dataset for task-specific manipulation, involving interactions between rigid objects and a deformable bag. The dataset incorporates a rich variety of scenarios including different object sizes, object numbers and manipulation actions. We approach dynamics learning by proposing an object-centric graph representation and two modules which are Active Prediction Module (APM) and Position Prediction Module (PPM) based on graph neural networks with an encode-process-decode architecture. At the inference stage, we build a two-stage model based on the learned modules for single time step prediction. We combine modules with different prediction horizons into a mixed-horizon model which addresses long-term prediction. In an ablation study, we show the benefits of the two-stage model for single time step prediction and the effectiveness of the mixed-horizon model for long-term prediction tasks. Supplementary material is available at https://github.com/wengzehang/deformable_rigid_interaction_prediction},
  archive   = {C_IROS},
  author    = {Zehang Weng and Fabian Paus and Anastasiia Varava and Hang Yin and Tamim Asfour and Danica Kragic},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636660},
  pages     = {5741-5748},
  title     = {Graph-based task-specific prediction models for interactions between deformable and rigid objects},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DemoGrasp: Few-shot learning for robotic grasping with human
demonstration. <em>IROS</em>, 5733–5740. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to successfully grasp objects is crucial in robotics, as it enables several interactive downstream applications. To this end, most approaches either compute the full 6D pose for the object of interest or learn to predict a set of grasping points. While the former approaches do not scale well to multiple object instances or classes yet, the latter require large annotated datasets and are hampered by their poor generalization capabilities to new geometries. To overcome these shortcomings, we propose to teach a robot how to grasp an object with a simple and short human demonstration. Hence, our approach neither requires many annotated images nor is it restricted to a specific geometry. We first present a small sequence of RGB-D images displaying a human-object interaction. This sequence is then leveraged to build associated hand and object meshes that represent the depicted interaction. Subsequently, we complete missing parts of the reconstructed object shape and estimate the relative transformation between the reconstruction and the visible object in the scene. Finally, we transfer the a-priori knowledge from the relative pose between object and human hand with the estimate of the current object pose in the scene into necessary grasping instructions for the robot. Exhaustive evaluations with Toyota’s Human Support Robot (HSR) in real and synthetic environments demonstrate the applicability of our proposed methodology and its advantage in comparison to previous approaches.},
  archive   = {C_IROS},
  author    = {Pengyuan Wang and Fabian Manhardt and Luca Minciullo and Lorenzo Garattoni and Sven Meier and Nassir Navab and Benjamin Busam},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636856},
  pages     = {5733-5740},
  title     = {DemoGrasp: Few-shot learning for robotic grasping with human demonstration},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient and accurate candidate generation for grasp pose
detection in SE(3). <em>IROS</em>, 5725–5732. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Grasp detection of novel objects in unstructured environments is a key capability in robotic manipulation. For 2D grasp detection problems where grasps are assumed to lie in the plane, it is common to design a fully convolutional neural network that predicts grasps over an entire image in one step. However, this is not possible for grasp pose detection where grasp poses are assumed to exist in SE(3). In this case, it is common to approach the problem in two steps: grasp candidate generation and candidate classification [1], [2], [3], [4]. Since grasp candidate classification is typically expensive, the problem becomes one of efficiently identifying high quality candidate grasps. This paper proposes a new grasp candidate generation method that significantly outperforms major 3D grasp detection baselines. Supplementary material is available at this website.},
  archive   = {C_IROS},
  author    = {Andreas ten Pas and Colin Keil and Robert Platt},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636215},
  pages     = {5725-5732},
  title     = {Efficient and accurate candidate generation for grasp pose detection in SE(3)},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reactive long horizon task execution via visual skill and
precondition models. <em>IROS</em>, 5717–5724. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Zero-shot execution of unseen robotic tasks is important to allowing robots to perform a wide variety of tasks in human environments, but collecting the amounts of data necessary to train end-to-end policies in the real-world is often infeasible. We describe an approach for sim-to-real training that can accomplish unseen robotic tasks using models learned in simulation to ground components of a simple task planner. We learn a library of parameterized skills, along with a set of predicates-based preconditions and termination conditions, entirely in simulation. We explore a block-stacking task because it has a clear structure, where multiple skills must be chained together, but our methods are applicable to a wide range of other problems and domains, and can transfer from simulation to the real-world with no fine tuning. The system is able to recognize failures and accomplish long-horizon tasks from perceptual input, which is critical for real-world execution. We evaluate our proposed approach in both simulation and in the real-world, showing an increase in success rate from 91.6\% to 98\% in simulation and from 10\% to 80\% success rate in the real-world as compared with naive baselines. For experiment videos including both real-world and simulation, see: https://www.youtube.com/playlist?list=PL-oD0xHUngeLfQmpngYkGFZarstfPOXqX},
  archive   = {C_IROS},
  author    = {Shohin Mukherjee and Chris Paxton and Arsalan Mousavian and Adam Fishman and Maxim Likhachev and Dieter Fox},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636037},
  pages     = {5717-5724},
  title     = {Reactive long horizon task execution via visual skill and precondition models},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DepthGrasp: Depth completion of transparent objects using
self-attentive adversarial network with spectral residual for grasping.
<em>IROS</em>, 5710–5716. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Transparent objects with unique visual properties often make depth cameras fail to scan their reflective and refractive surfaces. Recent studies on depth completion of transparent objects have leveraged a linear system based on the geometric constraints to predict the missing depth, which is hard to be employed in an end-to-end framework and achieve joint optimization. In this paper, we propose DepthGrasp - a deep learning approach for depth completion of transparent objects from a raw RGB-D image. More specifically, we use a generative adversarial network, which utilizes the generator to complete the depth maps by predicting the missing or inaccurate depth values, and use discriminator to guide the completed depth maps against the groundtruth. In the generator, we devise spectral residual blocks (SRB) with spectral normalization for network stability, and residual block to pass the attention map in order to capture the structure information and distinguish the geometric shape of transparent objects. In the discriminator, we use a patch-based convolutional network to adapt the data distributions of the predicted depth maps according to groundtruth. Extensive experiments conducted on ClearGrasp dataset show the effectiveness and generalization of the DepthGrasp for depth completion, and the deployed robotic picking system makes significant improvement on the performance of grasping on transparent objects.},
  archive   = {C_IROS},
  author    = {Yingjie Tang and Junhong Chen and Zhenguo Yang and Zehang Lin and Qing Li and Wenyin Liu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636382},
  pages     = {5710-5716},
  title     = {DepthGrasp: Depth completion of transparent objects using self-attentive adversarial network with spectral residual for grasping},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stereo matching by self-supervision of multiscopic vision.
<em>IROS</em>, 5702–5709. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-supervised learning for depth estimation possesses several advantages over supervised learning. The benefits of no need for ground-truth depth, online fine-tuning, and better generalization with unlimited data attract researchers to seek self-supervised solutions. In this work, we propose a new self-supervised framework for stereo matching utilizing multiple images captured at aligned camera positions. A cross photometric loss, an uncertainty-aware mutual-supervision loss, and a new smoothness loss are introduced to optimize the network in learning disparity maps end-to-end without ground-truth depth information. To train this framework, we build a new multiscopic dataset consisting of synthetic images rendered by 3D engines and real images captured by real cameras. After being trained with only the synthetic images, our network can perform well in unseen outdoor scenes. Our experiment shows that our model obtains better disparity maps than previous unsupervised methods on the KITTI dataset and is comparable to supervised methods when generalized to unseen data. Our source code and dataset are available at https://sites.google.com/view/multiscopic.},
  archive   = {C_IROS},
  author    = {Weihao Yuan and Yazhan Zhang and Bingkun Wu and Siyu Zhu and Ping Tan and Michael Yu Wang and Qifeng Chen},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636616},
  pages     = {5702-5709},
  title     = {Stereo matching by self-supervision of multiscopic vision},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VoluMon: Weakly-supervised volumetric monocular estimation
with ellipsoid representations. <em>IROS</em>, 5686–5693. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep learning approaches to estimating 3D object pose and geometry present an attractive alternative to online estimation techniques, which can suffer from significant estimation latency. However, a practical hurdle to training state-of-the-art deep 3D bounding box estimators is collecting a sufficiently large dataset of 3D bounding box labels. In this work, we present a novel framework for weakly supervised volumetric monocular estimation (VoluMon) that requires annotations in the image space only, i.e., associated object bounding box detections and instance segmentation. By approximating object geometry as ellipsoids, we can exploit the dual form of the ellipsoid to optimize with respect to bounding box annotations and the primal form of the ellipsoid to optimize with respect to a segmented pointcloud. For a simulated dataset with access to ground-truth, we show monocular object estimation performance similar to a naive online depth based estimation approach and after online refinement when depth images are available, we also approach the performance of a learned deep 6D pose estimator, which is supervised with projected 3D bounding box keypoints and assumes known model dimensions. Finally, we show promising qualitative results generated from a real-world dataset collected using a stereo pair.},
  archive   = {C_IROS},
  author    = {Katherine Liu and Kyel Ok and Nicholas Roy},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636725},
  pages     = {5686-5693},
  title     = {VoluMon: Weakly-supervised volumetric monocular estimation with ellipsoid representations},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Model adaptation through hypothesis transfer with gradual
knowledge distillation. <em>IROS</em>, 5679–5685. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to adapt their perception to changing environments is a core characterization of intelligent robots. At present, Unsupervised Domain Adaptation (UDA) methods are used to address this problem where the adaptation task is formulated as a transfer problem from a well-described scenario (source domain) to a new scenario (target domain). In order to implement the domain adaptation, these methods require access to the source data for achieving the distribution matching between both domains. However, in many real-world applications, the source data is inaccessible and only a source model pre-trained on the source domain is available during the transfer process. Therefore, the traditional UDA methods cannot support the challenging setting. This paper developed a new hypothesis transfer method to achieve model adaptation with gradual knowledge distillation. Specifically, we first prepare a source model through training a deep network on the labeled source domain by supervised learning. Then, we transfer the source model to the unlabeled target domain by self-training. To implement gradual knowledge distillation, we sliced the self-training into several epochs and then used the soft pseudo-labels from the latest epoch to guide the current epoch. In this process, the soft labels were generated by a semantic fusion on a proposed geometry of the neighborhood. To regulate the self-training, we developed a new objective constructed on the neighborhood. Experiments on three benchmarks have confirmed the state-of-the-art results of our method.},
  archive   = {C_IROS},
  author    = {Song Tang and Yuji Shi and Zhiyuan Ma and Jian Li and Jianzhi Lyu and Qingdu Li and Jianwei Zhang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636206},
  pages     = {5679-5685},
  title     = {Model adaptation through hypothesis transfer with gradual knowledge distillation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ORStereo: Occlusion-aware recurrent stereo matching for
4K-resolution images. <em>IROS</em>, 5671–5678. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Stereo reconstruction models trained on small images do not generalize well to high-resolution data. Training a model on high-resolution image size faces difficulties of data availability and is often infeasible due to limited computing resources. In this work, we present the Occlusion-aware Recurrent binocular Stereo matching (ORStereo), which deals with these issues by only training on available low disparity range stereo images. ORStereo generalizes to unseen high-resolution images with large disparity ranges by formulating the task as residual updates and refinements of an initial prediction. ORStereo is trained on images with disparity ranges limited to 256 pixels, yet it can operate 4K-resolution input with over 1000 disparities using limited GPU memory. We test the model’s capability on both synthetic and real-world high-resolution images. Experimental results demonstrate that ORStereo achieves comparable performance on 4K-resolution images compared to state-of-the-art methods trained on large disparity ranges. Compared to the baseline methods that are only trained on low-resolution images, our method has 60\% or less error on 4K-resolution images.},
  archive   = {C_IROS},
  author    = {Yaoyu Hu and Wenshan Wang and Huai Yu and Weikun Zhen and Sebastian Scherer},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635869},
  pages     = {5671-5678},
  title     = {ORStereo: Occlusion-aware recurrent stereo matching for 4K-resolution images},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Superline: A robust line segment feature for visual SLAM.
<em>IROS</em>, 5664–5670. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Along with point features, line features play an important role in achieving robust Simultaneous Localization and Mapping (SLAM) under complex environments. This paper proposes a fast and effective method, namely Superline, to simultaneously detect line segments and generate robust descriptors for matching. The entire model is composed of a convolutional backbone and two task heads, i.e., detection head and description head respectively. A line selecting mechanism and a spatial pyramid Line-of-Interest (LOI) pooling module is specially designed in the description head to aggregate multi-scale information into line feature descriptors. The entire model is implemented end-to-end and can be trained on a dataset with only line annotations and without the need of providing ground truth matching. Comparative experimental results on Wireframe and York Urban datasets as well as applying the Superline features on SLAM applications demonstrate the superior performance of our method.},
  archive   = {C_IROS},
  author    = {Chengyu Qiao and Tingming Bai and Zhiyu Xiang and Qi Qian and Yunfeng Bi},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636435},
  pages     = {5664-5670},
  title     = {Superline: A robust line segment feature for visual SLAM},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PCTMA-net: Point cloud transformer with morphing atlas-based
point generation network for dense point cloud completion.
<em>IROS</em>, 5657–5663. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inferring a complete 3D geometry given an in-complete point cloud is essential in many vision and robotics applications. Previous work mainly relies on a global feature extracted by a Multi-layer Perceptron (MLP) for predicting the shape geometry. This suffers from a loss of structural details, as its point generator fails to capture the detailed topology and structure of point clouds using only the global features. The irregular nature of point clouds makes this task more challenging. This paper presents a novel method for shape completion to address this problem. The Transformer structure is currently a standard approach for natural language processing tasks and its inherent nature of permutation invariance makes it well suited for learning point clouds. Furthermore, the Transformer’s attention mechanism can effectively capture the local context within a point cloud and efficiently exploit its incomplete local structure details. A morphing-atlas-based point generation network further fully utilizes the extracted point Transformer feature to predict the missing region using charts defined on the shape. Shape completion is achieved via the concatenation of all predicting charts on the surface. Extensive experiments on the Completion3D and KITTI data sets demonstrate that the proposed PCTMA-Net outperforms the state-of-the-art shape completion approaches and has a 10\% relative improvement over the next best-performing method.},
  archive   = {C_IROS},
  author    = {Jianjie Lin and Markus Rickert and Alexander Perzylo and Alois Knoll},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636483},
  pages     = {5657-5663},
  title     = {PCTMA-net: Point cloud transformer with morphing atlas-based point generation network for dense point cloud completion},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). All characteristics preservation: Single image dehazing
based on hierarchical detail reconstruction wavelet decomposition
network. <em>IROS</em>, 5649–5656. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Single image haze removal is crucial in computer vision. In open literatures, two kinds of dehazing strategies (prior-based and learning-based methods) have been developed. However, they have a trade-off between detail preservation and the image quality. Prior-based methods reconstruct the detail well but have lower image quality while learning-based methods achieve better recovered quality but lose the detail. In this paper, to mitigate this dilemma, a hierarchical architecture using the discrete wavelet transform (DWT) is proposed. It divides the dehazing problem into two parts: detail and background reconstruction. Based on investigating how haze affects the image in the wavelet domain, two networks for detail and background reconstruction are proposed. To avoid color distortion and the detail loss, the anti-vanish wavelet loss and the bound penalty are proposed. The multi-level wavelet component discriminator is proposed for further improvement. Experiments show that the proposed network can achieve superior performance in all metrics.},
  archive   = {C_IROS},
  author    = {Wei-Ting Chen and Hao-Yu Fang and Cheng-Che Tsai and Jian-Jiun Ding and Sy-Yen Kuo},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636373},
  pages     = {5649-5656},
  title     = {All characteristics preservation: Single image dehazing based on hierarchical detail reconstruction wavelet decomposition network},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Q-learning with long-term action-space shaping to model
complex behavior for autonomous lane changes. <em>IROS</em>, 5641–5648.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In autonomous driving applications, reinforcement learning agents often have to perform complex behavior, which can translate into optimizing multiple objectives while following certain rules. Encoding traffic rules and desires such as safety and comfort via classical methods based on reward shaping (i.e. a weighted combination of different objectives in the reward signal) or Lagrangian methods (including auxiliary losses in the optimization) can be very hard and cumbersome. In this work, we propose to instead shape the action-space at the maximization step of Q-learning. We further introduce a formulation for fixed-horizon estimation of auxiliary costs under the current target-policy based on truncated value- functions to encode the desire of comfortable driving ensuring interpretable behavior. We compare our algorithm to reward shaping and Lagrangian methods in the application of high- level decision making in autonomous driving, considering rules for safety, keeping right and comfort. We train and evaluate our agent in the open-source simulator SUMO on a variety of scenarios with different driver types and traffic situations. Additionally, we apply our method on the real HighD data set, showing the real-world applicability and simplicity of Q- learning with Action-space Shaping.},
  archive   = {C_IROS},
  author    = {Gabriel Kalweit and Maria Huegle and Moritz Werling and Joschka Boedecker},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636668},
  pages     = {5641-5648},
  title     = {Q-learning with long-term action-space shaping to model complex behavior for autonomous lane changes},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MAMBPO: Sample-efficient multi-robot reinforcement learning
using learned world models. <em>IROS</em>, 5635–5640. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-robot systems can benefit from reinforcement learning (RL) algorithms that learn behaviours in a small number of trials, a property known as sample efficiency. This research thus investigates the use of learned world models to improve sample efficiency. We present a novel multi-agent model-based RL algorithm: Multi-Agent Model-Based Policy Optimization (MAMBPO), utilizing the Centralized Learning for Decentralized Execution (CLDE) framework. CLDE algorithms allow a group of agents to act in a fully decentralized manner after training. This is a desirable property for many systems comprising of multiple robots. MAMBPO uses a learned world model to improve sample efficiency compared to model-free Multi-Agent Soft Actor-Critic (MASAC). We demonstrate this on two simulated multi-robot tasks, where MAMBPO achieves a similar performance to MASAC, but requires far fewer samples to do so. Through this, we take an important step towards making real-life learning for multi-robot systems possible.},
  archive   = {C_IROS},
  author    = {Daniël Willemsen and Mario Coppola and Guido C.H.E. de Croon},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635836},
  pages     = {5635-5640},
  title     = {MAMBPO: Sample-efficient multi-robot reinforcement learning using learned world models},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PNS: Population-guided novelty search for reinforcement
learning in hard exploration environments. <em>IROS</em>, 5627–5634. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement Learning (RL) has made remarkable achievements, but it still suffers from inadequate exploration strategies, sparse reward signals, and deceptive reward functions. To alleviate these problems, a Population-guided Novelty Search (PNS) parallel learning method is proposed in this paper. In PNS, the population is divided into multiple sub-populations, each of which has one chief agent and several exploring agents. The chief agent evaluates the policies learned by exploring agents and shares the optimal policy with all sub-populations. The exploring agents learn their policies in collaboration with the guidance of the optimal policy and, simultaneously, upload their policies to the chief agent. To balance exploration and exploitation, the Novelty Search (NS) is employed in every chief agent to encourage policies with high novelty while maximizing per-episode performance. We apply PNS to the twin delayed deep deterministic (TD3) policy gradient algorithm. The effectiveness of PNS to promote exploration and improve performance in continuous control domains is demonstrated in the experimental section. Notably, PNS-TD3 achieves rewards that far exceed the SOTA methods in environments with sparse or delayed reward signals. We also demonstrate that PNS enables robotic agents to learn control policies directly from pixels for sparse-reward manipulation in both simulated and real-world settings.},
  archive   = {C_IROS},
  author    = {Qihao Liu and Yujia Wang and Xiaofeng Liu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636234},
  pages     = {5627-5634},
  title     = {PNS: Population-guided novelty search for reinforcement learning in hard exploration environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Memory-based deep reinforcement learning for POMDPs.
<em>IROS</em>, 5619–5626. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A promising characteristic of Deep Reinforcement Learning (DRL) is its capability to learn optimal policy in an end-to-end manner without relying on feature engineering. However, most approaches assume a fully observable state space, i.e. fully observable Markov Decision Processes (MDPs). In real-world robotics, this assumption is unpractical, because of issues such as sensor sensitivity limitations and sensor noise, and the lack of knowledge about whether the observation design is complete or not. These scenarios lead to Partially Observable MDPs (POMDPs). In this paper, we propose Long-Short-Term-Memory-based Twin Delayed Deep Deterministic Policy Gradient (LSTM-TD3) by introducing a memory component to TD3, and compare its performance with other DRL algorithms in both MDPs and POMDPs. Our results demonstrate the significant advantages of the memory component in addressing POMDPs, including the ability to handle missing and noisy observation data.},
  archive   = {C_IROS},
  author    = {Lingheng Meng and Rob Gorbet and Dana Kulić},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636140},
  pages     = {5619-5626},
  title     = {Memory-based deep reinforcement learning for POMDPs},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian residual policy optimization: : Scalable bayesian
reinforcement learning with clairvoyant experts. <em>IROS</em>,
5611–5618. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Informed and robust decision making in the face of uncertainty is critical for robots operating in unstructured environments. We formulate this as Bayesian Reinforcement Learning over latent Markov Decision Processes (MDPs). While Bayes-optimality is theoretically the gold standard, existing algorithms scale poorly to continuous state and action spaces. We build on the following insight: in the absence of uncertainty, each latent MDP is easier to solve. We first obtain an ensemble of experts, one for each latent MDP, and fuse their advice to compute a baseline policy. Next, we train a Bayesian residual policy to improve upon the ensemble’s recommendation and learn to reduce uncertainty. Our algorithm, Bayesian Residual Policy Optimization (BRPO), imports the scalability of policy gradient methods and task-specific expert skills. BRPO significantly improves the ensemble of experts and drastically outperforms existing adaptive RL methods, both in simulated and physical robot experiments.},
  archive   = {C_IROS},
  author    = {Gilwoo Lee and Brian Hou and Sanjiban Choudhury and Siddhartha S. Srinivasa},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636163},
  pages     = {5611-5618},
  title     = {Bayesian residual policy optimization: : scalable bayesian reinforcement learning with clairvoyant experts},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semantic tracklets: An object-centric representation for
visual multi-agent reinforcement learning. <em>IROS</em>, 5603–5610. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Solving complex real-world tasks, e.g., autonomous fleet control, often involves a coordinated team of multiple agents which learn strategies from visual inputs via reinforcement learning. Many existing multi-agent reinforcement learning (MARL) algorithms however don’t scale to environments where agents operate on visual inputs. To address this issue, algorithmically, recent works have focused on non-stationarity and exploration. In contrast, we study whether scalability can also be achieved via a disentangled representation. For this, we explicitly construct an object-centric intermediate representation to characterize the states of an environment, which we refer to as ‘semantic tracklets.’ We evaluate ‘semantic tracklets’ on the visual multi-agent particle environment (VMPE) and on the challenging visual multi-agent GFootball environment. ‘Semantic tracklets’ consistently outperform baselines on VMPE, and achieve a +2.4 higher score difference than baselines on GFootball. Notably, this method is the first to successfully learn a strategy for five players in the GFootball environment using only visual data. For more, please see our project page: https://ioujenliu.github.io/SemanticTracklets},
  archive   = {C_IROS},
  author    = {Iou-Jen Liu and Zhongzheng Ren and Raymond A. Yeh and Alexander G. Schwing},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636592},
  pages     = {5603-5610},
  title     = {Semantic tracklets: An object-centric representation for visual multi-agent reinforcement learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-agent collaborative learning with relational graph
reasoning in adversarial environments. <em>IROS</em>, 5596–5602. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a collaborative policy framework via relational graph reasoning for multi-agent systems to accomplish adversarial tasks. A relational graph reasoning module consisting of an agent graph reasoning module and an opponent graph module, is designed to enable each agent to learn mixture state representation to enhance the effectiveness of the policy. In particular, for each agent, the agent graph reasoning module is designed to infer different underlying influences from different opponents and generate agent-level state representation. The opponent graph reasoning module is creatively designed for the opponents to reason relations from their surrounding objects including the agents and the opponents based on their latent features and then predict the future state of the opponents. It forms an opponent-level state representation. Besides, in order to effectively predict the state of the opponents, an intrinsic reward based on prediction error is designed to motivate the policy learning. Furthermore, interactions among agents are utilized to transmit messages and fuse information to promote the cooperative behaviors among the agents. Finally, various representative simulations on two multi-agent adversarial tasks are conducted to demonstrate the superiority and effectiveness of the proposed framework by comparison with existing methods.},
  archive   = {C_IROS},
  author    = {Shiguang Wu and Tenghai Qiu and Zhiqiang Pu and Jianqiang Yi},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636636},
  pages     = {5596-5602},
  title     = {Multi-agent collaborative learning with relational graph reasoning in adversarial environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Benchmarking safe deep reinforcement learning in aquatic
navigation. <em>IROS</em>, 5590–5595. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel benchmark environment for Safe Reinforcement Learning focusing on aquatic navigation. Aquatic navigation is an extremely challenging task due to the non-stationary environment and the uncertainties of the robotic platform, hence it is crucial to consider the safety aspect of the problem, by analyzing the behavior of the trained network to avoid dangerous situations (e.g., collisions). To this end, we consider a value-based and policy-gradient Deep Reinforcement Learning (DRL) and we propose a crossover-based strategy that combines gradient-based and gradient-free DRL to improve sample-efficiency. Moreover, we propose a verification strategy based on interval analysis that checks the behavior of the trained models over a set of desired properties. Our results show that the crossover-based training outperforms prior DRL approaches, while our verification allows us to quantify the number of configurations that violate the behaviors that are described by the properties. Crucially, this will serve as a benchmark for future research in this domain of applications.},
  archive   = {C_IROS},
  author    = {Enrico Marchesini and Davide Corsi and Alessandro Farinelli},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635925},
  pages     = {5590-5595},
  title     = {Benchmarking safe deep reinforcement learning in aquatic navigation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Context and orientation aware path tracking. <em>IROS</em>,
5583–5589. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous vehicles on city roads and especially in pedestrian environments require agility to navigate narrow passages and turn in tight spaces, leading to the need for a real-time, robust and adaptable controller. In this paper, we present orientation and context aware controllers for autonomous vehicles that can closely track the reference path wit alh respect to the current state of the vehicle, environmental properties, and the desired target orientation at the desired target location. Our proposed controllers are derived from the widely used pure pursuit controller. We validate our proposed controllers with respect to the baseline pure pursuit controller in simulation and on a full-size autonomous vehicle in a pedestrian environment. Our experimental results suggest significant improvements in adaptability and tracking performance compared to the pure pursuit controller.},
  archive   = {C_IROS},
  author    = {Nicholas Michael Bünger and Sahil Panjwani and Malika Meghjani and Zefan Huang and Marcelo H. Ang and Daniela Rus},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635922},
  pages     = {5583-5589},
  title     = {Context and orientation aware path tracking},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Disentangling and vectorization: A 3D visual perception
approach for autonomous driving based on surround-view fisheye cameras.
<em>IROS</em>, 5576–5582. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The 3D visual perception for vehicles with the surround-view fisheye camera system is a critical and challenging task for low-cost urban autonomous driving. While existing monocular 3D object detection methods perform not well enough on the fisheye images for mass production, partly due to the lack of 3D datasets of such images. In this paper, we manage to overcome and avoid the difficulty of acquiring the large scale of accurate 3D labeled truth data, by breaking down the 3D object detection task into some sub-tasks, such as vehicle’s contact point detection, type classification, re-identification and unit assembling, etc. Particularly, we propose the concept of Multidimensional Vector to include the utilizable information generated in different dimensions and stages, instead of the descriptive approach for the bird’s eye view (BEV) or a cube of eight points. The experiments of real fisheye images demonstrate that our solution achieves state-of-the-art accuracy while being real-time in practice.},
  archive   = {C_IROS},
  author    = {Zizhang Wu and Wenkai Zhang and Jizheng Wang and Man Wang and Yuanzhu Gan and Xinchao Gou and Muqing Fang and Jing Song},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636707},
  pages     = {5576-5582},
  title     = {Disentangling and vectorization: A 3D visual perception approach for autonomous driving based on surround-view fisheye cameras},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online high-level model estimation for efficient
hierarchical robot navigation. <em>IROS</em>, 5568–5575. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We would like to enable a robot to navigate efficiently and robustly in known, structured environments that are large enough to cause traditional planning approaches to incur considerable computational cost. Hierarchical planners are a promising way to increase planning efficiency in such environments because high-level abstract plans can be used to reduce the size of the search space over which detailed planning occurs. However, useful high-level representations of planning problems can be challenging to generate without prior domain knowledge. In this work, we propose a high-level planning representation which can be learned from previous plans considered in the environment and used online during hierarchical, multi-query robot navigation. We treat previous planning results as noisy measurements of high-level navigation properties, then update these properties over time using recursive estimation. We test our approach in standard and risk-aware hierarchical planning schemes, and demonstrate up to an 86\% decrease in the number of nodes expanded and a 66\% decrease in wallclock time as compared to a baseline A* planner while finding plans that are only 2-10\% more expensive.},
  archive   = {C_IROS},
  author    = {Martina Stadler and Katherine Liu and Nicholas Roy},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636565},
  pages     = {5568-5575},
  title     = {Online high-level model estimation for efficient hierarchical robot navigation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Map compressibility assessment for LiDAR registration.
<em>IROS</em>, 5560–5567. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We aim to assess the performance of LiDAR-to-map registration on compressive maps. Modern autonomous vehicles utilize pre-built HD (High-Definition) maps to perform sensor-to-map registration, which recovers pose estimation failures and reduces drift in a large-scale environment. However, sensor-to-map registration is usually realized by registering the sensor to a dense 3D model, which occupies massive storage space in the HD map and requires much data processing overhead. Although smaller 3D models are preferable, the optimal compressive map format for preservation of the best registration performance remains unclear.In this paper, we propose a novel and challenging benchmark to evaluate existing LiDAR-to-map registration methods from three perspectives: map compressibility, robustness, and precision. We compared various map formats, including raw points, hierarchical GMMs, and feature points, and show their performance trade-offs between compressibility and robustness on real-world LiDAR datasets: KITTI Odometry Dataset and Argoverse Tracking Dataset. Our benchmark reveals that state-of-the-art deep feature point based methods outperform traditional methods significantly when the map size budget is high. However, when map size budget is low, deep methods are outperformed by the methods using simpler models in Argoverse Tracking Dataset due to poor spatial coverage. In addition, we observe that the recently published TEASER++ significantly outperforms RANSAC for the feature point methods. Our analysis provides a valuable reference for the community to design budgeted real-world systems and find potential research opportunities. We will release the benchmark for public use.},
  archive   = {C_IROS},
  author    = {Ming-Fang Chang and Wei Dong and Joshua Mangelson and Michael Kaess and Simon Lucey},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636789},
  pages     = {5560-5567},
  title     = {Map compressibility assessment for LiDAR registration},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AVP-loc: Surround view localization and relocalization based
on HD vector map for automated valet parking. <em>IROS</em>, 5552–5559.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Localization is a crucial prerequisite for automated valet parking, in which a vehicle is required to navigate itself in a GPS-denied parking lot. Traditional visual localization methods usually build a feature map and use it for future localizations. However, the feature map is not robust to changes in illumination, appearance, and viewing perspective. To deal with this issue, we need a more stable map. In this paper, we propose to use the parking lot’s HD vector map directly for localization. The vector representation is ultimately stable but brings challenges in data association as well. To this end, we present a novel data association method to match the surround-view images with the vector map. In addition, we also propose a closed-form relocalization strategy by exploiting distinctive road mark combinations in the vector map. Experiments show that the proposed method is able to achieve centimeter-level localization accuracy in a multi-floor parking lot.},
  archive   = {C_IROS},
  author    = {Chi Zhang and Hao Liu and Zhijun Xie and Kuiyuan Yang and Kun Guo and Rui Cai and Zhiwei Li},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636746},
  pages     = {5552-5559},
  title     = {AVP-loc: Surround view localization and relocalization based on HD vector map for automated valet parking},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust LiDAR localization on an HD vector map without a
separate localization layer. <em>IROS</em>, 5536–5543. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many autonomous driving applications nowadays come along with a prebuilt vector map for routing and planning purposes. In order to localize on this map, traditional LiDAR localization methods usually require a separate localization layer to function. On one hand, the separate layer occupies large storage and is not convenient to update. On the other hand, the potential of the vector map itself has not been fully exploited by existing methods. In this paper, we present a LiDAR localization system that leverages the vector map directly as the localization layer. A semantic extraction module is developed to match the heterogeneous data between LiDAR measurements and the 3D vector elements. A local map maintenance module is introduced to keep the system function robustly when there are not enough vector matches. The system adopts an optimization-based framework and infers 6-DOF poses. Experiments show that the proposed system is able to achieve centimeter accuracy robustly in both highway and urban environments, without a separate localization layer.},
  archive   = {C_IROS},
  author    = {Chi Zhang and Liwen Liu and Zhoupeng Xue and Kun Guo and Kuiyuan Yang and Rui Cai and Zhiwei Li},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636227},
  pages     = {5536-5543},
  title     = {Robust LiDAR localization on an HD vector map without a separate localization layer},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interaction-based trajectory prediction over a hybrid
traffic graph. <em>IROS</em>, 5530–5535. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Behavior prediction of traffic actors is an essential component of any real-world self-driving system. Actors’ long-term behaviors tend to be governed by their interactions with other actors or traffic elements (traffic lights, stop signs) in the scene. To capture this highly complex structure of interactions, we propose to use a hybrid graph whose nodes represent both the traffic actors as well as the static and dynamic traffic elements present in the scene. The different modes of temporal interaction (e.g., stopping and going) among actors and traffic elements are explicitly modeled by graph edges. This explicit reasoning about discrete interaction types not only helps in predicting future motion, but also enhances the interpretability of the model, which is important for safety-critical applications such as autonomous driving. We predict actors’ trajectories and interaction types using a graph neural network, which is trained in a semi-supervised manner. We show that our proposed model, TrafficGraphNet, achieves state-of-the-art trajectory prediction accuracy while maintaining a high level of interpretability.},
  archive   = {C_IROS},
  author    = {Sumit Kumar and Yiming Gu and Jerrick Hoang and Galen Clark Haynes and Micol Marchetti-Bowick},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636143},
  pages     = {5530-5535},
  title     = {Interaction-based trajectory prediction over a hybrid traffic graph},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient localisation using images and OpenStreetMaps.
<em>IROS</em>, 5507–5513. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to localise is key for robot navigation. We describe an efficient method for vision-based localisation, which combines sequential Monte Carlo tracking with matching ground-level images to 2-D cartographic maps such as OpenStreetMaps. The matching is based on a learned embedded space representation linking images and map tiles, encoding the common semantic information present in both and providing potential for invariance to changing conditions. Moreover, the compactness of 2-D maps supports scalability. This contrasts with the majority of previous approaches based on matching with single-shot geo-referenced images or 3-D reconstructions. We present experiments using the StreetLearn and Oxford RobotCar datasets and demonstrate that the method is highly effective, giving high accuracy and fast convergence.},
  archive   = {C_IROS},
  author    = {Mengjie Zhou and Xieyuanli Chen and Noe Samano and Cyrill Stachniss and Andrew Calway},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635972},
  pages     = {5507-5513},
  title     = {Efficient localisation using images and OpenStreetMaps},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time multi-adaptive-resolution-surfel 6D LiDAR odometry
using continuous-time trajectory optimization. <em>IROS</em>, 5499–5506.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simultaneous Localization and Mapping (SLAM) is an essential capability for autonomous robots, but due to high data rates of 3D LiDARs real-time SLAM is challenging. We propose a real-time method for 6D LiDAR odometry. Our approach combines a continuous-time B-Spline trajectory representation with a Gaussian Mixture Model (GMM) formulation to jointly align local multi-resolution surfel maps. Sparse voxel grids and permutohedral lattices ensure fast access to map surfels, and an adaptive resolution selection scheme effectively speeds up registration. A thorough experimental evaluation shows the performance of our approach on multiple datasets and during real-robot experiments.},
  archive   = {C_IROS},
  author    = {Jan Quenzel and Sven Behnke},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636763},
  pages     = {5499-5506},
  title     = {Real-time multi-adaptive-resolution-surfel 6D LiDAR odometry using continuous-time trajectory optimization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DLL: Direct LIDAR localization. A map-based localization
approach for aerial robots. <em>IROS</em>, 5491–5498. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents DLL, a fast direct map-based localization technique using 3D LIDAR for its application to aerial robots. DLL implements a point cloud to map registration based on non-linear optimization of the distance of the points and the map, thus not requiring features, neither point correspondences. Given an initial pose, the method is able to track the pose of the robot by refining the predicted pose from odometry. Through benchmarks using real datasets and simulations, we show how the method performs much better than Monte-Carlo localization methods and achieves comparable precision to other optimization-based approaches but running one order of magnitude faster. The method is also robust under odometric errors. The approach has been implemented under the Robot Operating System (ROS), and it is publicly available.},
  archive   = {C_IROS},
  author    = {Fernando Caballero and Luis Merino},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636501},
  pages     = {5491-5498},
  title     = {DLL: Direct LIDAR localization. a map-based localization approach for aerial robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CLMM-net: Robust cascaded LiDAR map matching based on
multi-level intensity map. <em>IROS</em>, 5484–5490. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {LiDAR map matching(LMM) is a critical localization technique in autonomous driving while existing methods have problems in terms of both accuracy and robustness when driving in the scenes with poor structure information (e.g. highways). This paper put forward a multi-level intensity map based cascaded network for LiDAR map matching in autonomous driving. The network uses an effective multi-level intensity map representation to compactly encode the appearance and structure information of point clouds, which effectively reduce the position ambiguity in structure-less scenarios. Besides, this method leverages the multi-scale nature of deep neural networks and matches the online LiDAR observation with the offline map in a coarse-to-fine manner so as to balance the time-consuming and precision. Extensive experiments on diverse autonomous driving environments demonstrate the superiority of our proposed method over other existing state-of-the-art methods.},
  archive   = {C_IROS},
  author    = {Kai Chen and Lei He and Xiaofeng Wang and Yuqian Liu and Ming Zhao},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636332},
  pages     = {5484-5490},
  title     = {CLMM-net: Robust cascaded LiDAR map matching based on multi-level intensity map},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ground encoding: Learned factor graph-based models for
localizing ground penetrating radar. <em>IROS</em>, 5476–5483. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the problem of robot localization using ground penetrating radar (GPR) sensors. Current approaches for localization with GPR sensors require a priori maps of the system’s environment as well as access to approximate global positioning (GPS) during operation. In this paper, we propose a novel, real-time GPR-based localization system for unknown and GPS-denied environments. We model the localization problem as an inference over a factor graph. Our approach combines 1D single-channel GPR measurements to form 2D image submaps. To use these GPR images in the graph, we need sensor models that can map noisy, high-dimensional image measurements into the state space. These are challenging to obtain a priori since image generation has a complex dependency on subsurface composition and radar physics, which itself varies with sensors and variations in subsurface electromagnetic properties. Our key idea is to instead learn relative sensor models directly from GPR data that map non-sequential GPR image pairs to relative robot motion. These models are incorporated as factors within the factor graph with relative motion predictions correcting for accumulated drift in the position estimates. We demonstrate our approach over datasets collected across multiple locations using a custom designed experimental rig. We show reliable, real-time localization using only GPR and odometry measurements for varying trajectories in three distinct GPS-denied environments.},
  archive   = {C_IROS},
  author    = {Alexander Baikovitz and Paloma Sodhi and Michael Dille and Michael Kaess},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636764},
  pages     = {5476-5483},
  title     = {Ground encoding: Learned factor graph-based models for localizing ground penetrating radar},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BSP-MonoLoc: Basic semantic primitives based monocular
localization on roads. <em>IROS</em>, 5470–5475. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robust visual localization in traffic scenes is a fundamental problem for self-driving vehicles. However, it is still challenging to achieve accurate localization performance because of drastic viewpoint and illumination changes. To address the issues, we design a novel monocular localization framework based on a light-weight prior map, called BSP-MonoLoc, which leverages the 2D semantic primitives from the monocular images and the 3D basic semantic primitives from the prior map. These primitives are commonly available but lack of distinctive signature. To effectively make associations between the 2D and 3D primitives and refine the vehicle’s pose, we adopt an iterative optimization method, where an efficient hierarchical sample strategy is designed to give a good initial prediction for the associations and the pose. Experimental results on the KAIST dataset and our dataset demonstrate the proposed method can achieve high localization accuracy and run at a real-time performance.},
  archive   = {C_IROS},
  author    = {Heping Li and Changliang Xue and Feng Wen and Hongbo Zhang and Wei Gao},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636321},
  pages     = {5470-5475},
  title     = {BSP-MonoLoc: Basic semantic primitives based monocular localization on roads},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CFEAR radarodometry - conservative filtering for efficient
and accurate radar odometry. <em>IROS</em>, 5462–5469. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an accurate, highly efficient and learning free method for large-scale radar odometry estimation. By using a simple filtering technique that keeps the strongest returns, we produce a clean radar data representation and reconstruct surface normals for efficient and accurate scan matching. Registration is carried out by minimizing a point-to-line metric and robustness to outliers is achieved using a Huber loss. Drift is additionally reduced by jointly registering the latest scan to a history of keyframes. We found that our odometry pipeline generalize well to different sensor models and datasets without changing a single parameter. We evaluate our method in three widely different environments and demonstrate an improvement over spatially cross validated state-of-the-art with an overall translation error of 1.76\% in a public urban radar odometry benchmark, running merely on a single laptop CPU thread at 55 Hz.},
  archive   = {C_IROS},
  author    = {Daniel Adolfsson and Martin Magnusson and Anas Alhashimi and Achim J. Lilienthal and Henrik Andreasson},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636253},
  pages     = {5462-5469},
  title     = {CFEAR radarodometry - conservative filtering for efficient and accurate radar odometry},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Angular super-resolution radar SLAM. <em>IROS</em>,
5456–5461. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Radar SLAM has attracted wide attention due to its all-day and all-weather working characteristics in the last decade. The existing radar SLAM systems mainly adopt mechanically pivoting radar with simple principle and high resolution, but this kind of radar has disadvantages such as low frame rate, distortion of the radar image, and high cost. Although array snapshot radar has the advantages of high frame rate and low cost, its low azimuth resolution, multipath reflection, and angular glint limit its application in SLAM. This paper proposes a SLAM system developed on array snapshot radar. The system realizes angular super-resolution radar imaging through compressed sensing, which effectively solves the problems of poor azimuth resolution and multipath reflection of array snapshot radar. We also propose the corresponding point cloud extraction method and scan matching method, this method performs a centroid iterative closest point algorithm between the submaps, thereby effectively improving the interference of noise and angular glint. Experimental results show that our proposed array snapshot radar SLAM system can reduce the mean absolute trajectory error by more than 3 times compared with the existing system, and can show accuracy and robustness in various environments.},
  archive   = {C_IROS},
  author    = {Zhiyuan Zeng and Xiangwei Dang and Yanlei Li and Xiangxi Bu and Xingdong Liang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636438},
  pages     = {5456-5461},
  title     = {Angular super-resolution radar SLAM},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-layer VI-GNSS global positioning framework with
numerical solution aided MAP initialization. <em>IROS</em>, 5448–5455.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motivated by the goal of achieving long-term drift-free camera pose estimation in complex scenarios, we propose a global positioning framework fusing visual, inertial and Global Navigation Satellite System (GNSS) measurements in multiple layers. Different from previous loosely- and tightly-coupled methods, the proposed multi-layer fusion allows us to delicately correct the drift of visual odometry and keep reliable positioning while GNSS degrades. In particular, local motion estimation is conducted in the inner-layer, solving the problem of scale drift and inaccurate bias estimation in visual odometry by fusing the velocity of GNSS, pre-integration of Inertial Measurement Unit (IMU) and camera measurement in a tightly-coupled way. The global localization is achieved in the outer-layer, where the local motion is further fused with GNSS position and course in a long-term period in a loosely-coupled way. Furthermore, a dedicated initialization method is proposed to guarantee fast and accurate estimation for all state variables and parameters. We give exhaustive tests of the proposed framework on indoor and outdoor public datasets. The mean localization error is reduced up to 63\%, with a promotion of 69\% in initialization accuracy compared with state-of-the-art works. We have applied the algorithm to Augmented Reality (AR) navigation, crowd sourcing high-precision map update and other large-scale applications.},
  archive   = {C_IROS},
  author    = {Bing Han and Zhongyang Xiao and Shuai Huang and Tao Zhang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636871},
  pages     = {5448-5455},
  title     = {Multi-layer VI-GNSS global positioning framework with numerical solution aided MAP initialization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GR-fusion: Multi-sensor fusion SLAM for ground robots with
high robustness and low drift. <em>IROS</em>, 5440–5447. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a tightly coupled pipeline, which efficiently fuses measurements of LiDAR, camera, IMU, encoder, and GNSS to estimate the robot state and build a map even in challenging situations. The depth of visual features is extracted by projecting the LiDAR point cloud and ground plane into image. We select the tracked high-quality visual features and LiDAR features and tightly coupled the pre-integrated values of the IMU and the encoder to optimize the state increment of a robot. We use the estimated relative pose to re-evaluate the matching distance between features in the local window and remove dynamic objects and outliers. In the mapping node, we use refined features and tightly coupled the GNSS measurements, increment factors, and local ground constraints to further refine the robot’s global state by aligning LiDAR features with the global map. Furthermore, the method can detect sensor degradation and automatically reconfigure the optimization process. Based on a six-wheeled ground robot, we perform extensive experiments in both indoor and outdoor environments and demonstrated that the proposed GR-Fusion outperforms state-of-the-art SLAM methods in terms of accuracy and robustness.},
  archive   = {C_IROS},
  author    = {Ting Wang and Yun Su and Shiliang Shao and Chen Yao and Zhidong Wang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636232},
  pages     = {5440-5447},
  title     = {GR-fusion: Multi-sensor fusion SLAM for ground robots with high robustness and low drift},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Consensus-informed optimization over mixtures for
ambiguity-aware object SLAM. <em>IROS</em>, 5432–5439. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Building object-level maps can facilitate robot-environment interactions (e.g. planning and manipulation), but objects could often have multiple probable poses when viewed from a single vantage point, due to symmetry, occlusion or perceptual failures. A robust object-level simultaneous localization and mapping (object SLAM) algorithm needs to be aware of this pose ambiguity. We propose to maintain and subsequently disambiguate the multiple pose interpretations to gradually recover a globally consistent world representation. The max-mixtures model is applied to implicitly and efficiently track all pose hypotheses, but the resulting formulation is non-convex, and therefore subject to local optima. To mitigate this problem, temporally consistent hypotheses are extracted, guiding the optimization into the global optimum. This consensus-informed inference method is applied online via landmark variable re-initialization within an incremental SLAM framework, iSAM2, for robust real-time performance. We demonstrate that this approach improves SLAM performance on both simulated and real object SLAM problems with pose ambiguity.},
  archive   = {C_IROS},
  author    = {Ziqi Lu and Qiangqiang Huang and Kevin Doherty and John J. Leonard},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636213},
  pages     = {5432-5439},
  title     = {Consensus-informed optimization over mixtures for ambiguity-aware object SLAM},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SymbioLCD: Ensemble-based loop closure detection using
CNN-extracted objects and visual bag-of-words. <em>IROS</em>, 5425. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Loop closure detection is an essential tool of Simultaneous Localization and Mapping (SLAM) to minimize drift in its localization. Many state-of-the-art loop closure detection (LCD) algorithms use visual Bag-of-Words (vBoW), which is robust against partial occlusions in a scene but cannot perceive the semantics or spatial relationships between feature points. CNN object extraction can address those issues, by providing semantic labels and spatial relationships between objects in a scene. Previous work has mainly focused on replacing vBoW with CNN derived features. In this paper we propose SymbioLCD, a novel ensemble-based LCD that utilizes both CNN-extracted objects and vBoW features for LCD candidate prediction. When used in tandem, the added elements of object semantics and spatial-awareness creates a more robust and symbiotic loop closure detection system. The proposed SymbioLCD uses scale-invariant spatial and semantic matching, Hausdorff distance with temporal constraints, and a Random Forest that utilizes combined information from both CNN-extracted objects and vBoW features for predicting accurate loop closure candidates. Evaluation of the proposed method shows it outperforms other Machine Learning (ML) algorithms - such as SVM, Decision Tree and Neural Network, and demonstrates that there is a strong symbiosis between CNN-extracted object information and vBoW features which assists accurate LCD candidate prediction. Furthermore, it is able to perceive loop closure candidates earlier than state-of-the-art SLAM algorithms, utilizing added spatial and semantic information from CNN-extracted objects.},
  archive   = {C_IROS},
  author    = {Jonathan J.Y. Kim and Martin Urschler and Patricia J. Riddle and Jörg S. Wicker},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636622},
  pages     = {5425},
  title     = {SymbioLCD: Ensemble-based loop closure detection using CNN-extracted objects and visual bag-of-words},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accurate visual-inertial SLAM by manhattan frame
re-identification. <em>IROS</em>, 5418–5424. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most of the state-of-the-art visual-inertial SLAM methods pay less attention to the scene structure of man-made environments. In this paper, based on the assumption of multiple local Manhattan worlds (MWs), we propose a Manhattan frame (MF) re-identification method to build relative rotation constraints between MF matching pairs and tightly couple these constraints into global bundle adjust module. Specifically, a coarse-to-fine vanishing point (VP) estimation method and pose guided MF temporal consistency verification method are firstly proposed to improve the accuracy and robustness of MF estimation. Then unreliable MF matching pairs are filtered out by a spatial temporal consistency check. Finally, the relative rotation constraints of the remaining MF matching pairs are combined into global bundle adjustment energy function for further optimization. We have validated our proposed method on both synthetic and real-world datasets. When comparing with the baseline method [1], the real-time absolute trajectory error (ATE) of our proposed method has decreased by 29.1\%, 19.8\% on TartanAir hospital and EuRoC datasets respectively. Our method also exceeds existing state-of-the-art algorithms on both synthetic and real-world datasets.},
  archive   = {C_IROS},
  author    = {Xiongfeng Peng and Zhihua Liu and Qiang Wang and Yun-Tae Kim and Hong-Seok Lee},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636245},
  pages     = {5418-5424},
  title     = {Accurate visual-inertial SLAM by manhattan frame re-identification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AquaVis: A perception-aware autonomous navigation framework
for underwater vehicles. <em>IROS</em>, 5410–5417. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual monitoring operations underwater require both observing the objects of interest in close-proximity, and tracking the few feature-rich areas necessary for state estimation. This paper introduces the first navigation framework, called AquaVis, that produces on-line visibility-aware motion plans that enable Autonomous Underwater Vehicles (AUVs) to track multiple visual objectives with an arbitrary camera configuration in real-time. Using the proposed pipeline, AUVs can efficiently move in 3D, reach their goals while avoiding obstacles safely, and maximizing the visibility of multiple objectives along the path within a specified proximity. The method is sufficiently fast to be executed in real-time and is suitable for single or multiple camera configurations. Experimental results show the significant improvement on tracking multiple automatically-extracted points of interest, with low computational overhead and fast re-planning times.Accompanying short video: https://youtu.be/JKO bbrIZyU},
  archive   = {C_IROS},
  author    = {Marios Xanthidis and Michail Kalaitzakis and Nare Karapetyan and James Johnson and Nikolaos Vitzilaios and Jason M. O’Kane and Ioannis Rekleitis},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636124},
  pages     = {5410-5417},
  title     = {AquaVis: A perception-aware autonomous navigation framework for underwater vehicles},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ShorelineNet: An efficient deep learning approach for
shoreline semantic segmentation for unmanned surface vehicles.
<em>IROS</em>, 5403–5409. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a novel deep learning approach to semantic segmentation of the shoreline environments with a high frames-per-second (fps) performance, making the approach readily applicable to autonomous navigation for Unmanned Surface Vehicles (USV). The proposed ShorelineNet is an efficient deep neural network of high performance relying only on visual input. ShorelineNet uses monocular visual input to produce accurate shoreline separation and obstacle detection compared to the state-of-the-art, and achieves this with real-time performance. Experimental validation on a challenging multi-modal maritime obstacle detection dataset, the MODD2 dataset, achieves a much faster inference (25fps on an NVIDIA Tesla K80 and 6fps on a CPU) with respect to the recent state-of-the-art methods, while keeping the performance equally high (73.1\% F-score). This makes ShorelineNet a robust and effective model to be used for reliable USV navigation that require real-time and high-performance semantic segmentation of maritime environments.},
  archive   = {C_IROS},
  author    = {Linghong Yao and Dimitrios Kanoulas and Ze Ji and Yuanchang Liu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636614},
  pages     = {5403-5409},
  title     = {ShorelineNet: An efficient deep learning approach for shoreline semantic segmentation for unmanned surface vehicles},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient LiDAR-based in-water obstacle detection and
segmentation by autonomous surface vehicles in aquatic environments.
<em>IROS</em>, 5387–5394. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Identifying in-water obstacles is fundamental for safe navigation of Autonomous Surface Vehicles (ASVs). This paper presents a model-free method for segmenting individual in-water objects (e.g., swimmers, buoys, boats) and shorelines from LiDAR sensor data. To reduce the computational requirement, our method first converts the 3D point cloud into a 2D spherical projection image. Then, an algorithm based on the integration of a breadth-first search and a variant of a hierarchical agglomerative clustering segments the points according to different objects. Our method addresses the sparsity and instability of the point cloud in the aquatic domain – a characteristic that makes the methods developed for self-driving cars not directly applicable for in-water obstacle segmentation, as demonstrated in our experiments. Our method is compared with other state-of-the-art approaches and is validated both in simulation and in real-world ASV deployments, with different objects and encountering scenarios. The proposed method is effective in segmenting in-water obstacles not known a priori, in real-time, outperforming other state-of-the art methods.},
  archive   = {C_IROS},
  author    = {Mingi Jeong and Alberto Quattrini Li},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636028},
  pages     = {5387-5394},
  title     = {Efficient LiDAR-based in-water obstacle detection and segmentation by autonomous surface vehicles in aquatic environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predicting the future motion of divers for enhanced
underwater human-robot collaboration. <em>IROS</em>, 5379–5386. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous Underwater Vehicles (AUVs) can be effective collaborators to human scuba divers in many applications, such as environmental surveying, mapping, or infrastructure repair. However, for these applications to be realized in the real world, it is essential that robots are able to both lead and follow their human collaborators. Current algorithms for diver following are not robust to non-uniform changes in the motion of the diver, and no framework currently exists for robots to lead divers. One method to improve the robustness of diver following and enable the capability of diver leading is to predict the future motion of a diver. In this paper, we present a vision-based approach for AUVs to predict the future motion trajectory of divers, utilizing the Vanilla-LSTM and Social-LSTM temporal deep neural networks. We also present a dense optical flow-based method to stabilize the input annotations from the dataset and reduce the effects of camera ego-motion. We analyze the results of these models on scenarios ranging from swimming pools to the open ocean and present the model’s accuracy at varying prediction lengths. We find that our LSTM models can generate predictions with significant accuracy 1.5 seconds into the future and that stabilizing LSTM models significantly improves trajectory prediction performance.},
  archive   = {C_IROS},
  author    = {Tanmay Agarwal and Michael Fulton and Junaed Sattar},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636374},
  pages     = {5379-5386},
  title     = {Predicting the future motion of divers for enhanced underwater human-robot collaboration},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards robust visual diver detection onboard autonomous
underwater robots: Assessing the effects of models and data1.
<em>IROS</em>, 5372–5378. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep neural networks are the leading solution to the object detection problem. However, challenges arise when applying these networks to the kind of real-time, first-person video data that a robotic platform must process: specifically, detections may not be consistent from frame to frame, and objects may frequently appear at viewpoints that are particularly challenging for the model, resulting in inaccurate detections. In this paper, we present our approach for addressing these challenges for our particular vision problem: diver detection onboard autonomous underwater vehicles (AUVs). We begin by producing and releasing a dataset of approximately 105,000 annotated images of divers sourced from videos in order to address the challenge of learning a wide variety of object rotations and translations. This is one of the largest and most varied diver detection datasets ever created, and we compare models trained and tested on both our dataset and a previous dataset to demonstrate that our dataset improves the state-of-the-art in diver detection. Then, in order to choose an object detection model that produces detections that are consistent from frame to frame, we evaluate several state-of-the-art object detection models on the temporal stability of their detections in addition to the typical accuracy and efficiency metrics, mean average precision (mAP) and frames per second. Importantly, our results showed that models with the highest mAP do not also have the highest temporal stability.},
  archive   = {C_IROS},
  author    = {Karin de Langis and Michael Fulton and Junaed Sattar},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636099},
  pages     = {5372-5378},
  title     = {Towards robust visual diver detection onboard autonomous underwater robots: Assessing the effects of models and data1},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interpretable trade-offs between robot task accuracy and
compute efficiency. <em>IROS</em>, 5364–5371. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A robot can invoke heterogeneous computation resources such as CPUs, cloud GPU servers, or even human computation for achieving a high-level goal. The problem of invoking an appropriate computation model so that it will successfully complete a task while keeping its compute and energy costs within a budget is called a model selection problem. In this paper, we present an optimal solution to the model selection problem with two compute models, the first being fast but less accurate, and the second being slow but more accurate. The main insight behind our solution is that a robot should invoke the slower compute model only when the benefits from the gain in accuracy outweigh the computational costs. We show that such cost-benefit analysis can be performed by leveraging the statistical correlation between the accuracy of fast and slow compute models. We demonstrate the broad applicability of our approach to diverse problems such as perception using neural networks and safe navigation of a simulated Mars rover.},
  archive   = {C_IROS},
  author    = {Bineet Ghosh and Sandeep Chinchali and Parasara Sridhar Duggirala},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636580},
  pages     = {5364-5371},
  title     = {Interpretable trade-offs between robot task accuracy and compute efficiency},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). I3SA: The increased step size stability assessment benchmark
and its application to the humanoid robot REEM-c. <em>IROS</em>,
5357–5363. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The implementation of stable locomotion on humanoid robots is a difficult task. This is complicated by the fact that there is no uniform method for analyzing a robot and its control architecture and for calculating indicators to quantify performance of flat ground walking. Moreover, there is no widely accepted indicator do distinct between a stable and unstable state of the robot. We propose the Increased Step Size Stability Assessment (I3SA) as a testing protocol and standardized procedure for data collection and evaluation of stability for locomotion on flat terrain. We apply this test to the humanoid robot REEM-C. The biped must cover a set distance of four meters with predefined step sizes. The initial step size is defined as 20\% of the total leg length of the robot. After three successful trials, the step size is continuously increased until REEM-C’s last successful trial at 40\% of its total leg length, leading to REEM-C’s I3SA rating of 40. The recorded data are evaluated using metrics known from the literature, such as the capture point, foot placement estimator, and the angular momentum acting at the center of mass. We illustrate the experimental setup, data collection and processing, the calculation of performance indicators for several step sizes and the trial which resulted in a fall of the robot. The trend towards decreasing stability with increased step size and the available key assumptions are reported.},
  archive   = {C_IROS},
  author    = {Felix Aller and Monika Harant and Sebastian Sontag and Matthew Millard and Katja Mombaur},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636429},
  pages     = {5357-5363},
  title     = {I3SA: The increased step size stability assessment benchmark and its application to the humanoid robot REEM-C},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). New metrics for industrial depth sensors evaluation for
precise robotic applications. <em>IROS</em>, 5350–5356. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Precise perception is one of the key enablers of autonomous robotic operations. The right selection of sensors significantly influences the overall performance of the system. This paper provides a systematic approach for evaluation of various sensors available on the market. The main focus is to assess the performance in use cases of short to medium distance operations, especially relevant for precise manipulation and/or quality control. The evaluation is based solely on depth data (point clouds). We use six metrics to evaluate the sensors and propose a novel approach for low-cost fabrication of benchmark targets. The evaluation experiments are conducted on different materials to simulate various industrial environments. Our results provide a qualitative and quantitative comparison of different characteristics of various sensors and can be used to select an appropriate device for specific conditions.},
  archive   = {C_IROS},
  author    = {Konrad P Cop and Arne Peters and Bare L Žagar and Daniel Hettegger and Alois C Knoll},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636322},
  pages     = {5350-5356},
  title     = {New metrics for industrial depth sensors evaluation for precise robotic applications},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Error diagnosis of deep monocular depth estimation models.
<em>IROS</em>, 5344–5649. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Estimating depth from a monocular image is an ill-posed problem: when the camera projects a 3D scene onto a 2D plane, depth information is inherently and permanently lost. Nevertheless, recent work has shown impressive results in estimating 3D structure from 2D images using deep learning. In this paper, we put on an introspective hat and analyze state-of-the-art monocular depth estimation models in indoor scenes to understand these models’ limitations and error patterns. To address errors in depth estimation, we introduce a novel Depth Error Detection Network (DEDN) that spatially identifies erroneous depth predictions in the monocular depth estimation models. By experimenting with multiple state-of-the-art monocular indoor depth estimation models on multiple datasets, we show that our proposed depth error detection network can identify a significant number of errors in the predicted depth maps. Our module is flexible and can be readily plugged into any monocular depth prediction network to help diagnose its results. Additionally, we propose a simple yet effective Depth Error Correction Network (DECN) that iteratively corrects errors based on our initial error diagnosis.},
  archive   = {C_IROS},
  author    = {Jagpreet Chawla and Nikhil Thakurdesai and Anuj Godase and Md Reza and David Crandall and Soon-Heung Jung},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636673},
  pages     = {5344-5649},
  title     = {Error diagnosis of deep monocular depth estimation models},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Overlap displacement error: Are your SLAM poses
map-consistent? <em>IROS</em>, 5336–5343. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Localization is an essential module that supports many intelligent functions of a mobile robot such as transportation or inspection. However, justifying that a localization module is sufficiently accurate for supporting all downstream tasks is one of the most difficult questions to answer in practice. To overcome this problem, we move away from the traditional calculation of pose errors and propose a new approach that instead evaluates the potential map inconsistency introduced by those pose errors.For this purpose, we propose a new metric, which we call Overlap Displacement Error (ODE). This metric measures the relative displacements between multiple overlapping sensor frustums with respect to the ground truth. All you need to compute this metric are a query trajectory, a ground truth trajectory and the sensor frustum used for mapping. Having the sensor frustum and the map representation as part of the metric, the ODE is customized to the hardware configuration and the mapping strategy. This design allows the analysis of pose accuracy in a space that matters to map creation, and also allows the identification of problems sitting in the interplay between localization and mapping. We demonstrate the potential of this new analysis tool on synthetic and the real-world sequences.},
  archive   = {C_IROS},
  author    = {Christian Mostegel and Jianbo Ye and Yu Luo and Yang Liu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636833},
  pages     = {5336-5343},
  title     = {Overlap displacement error: Are your SLAM poses map-consistent?},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluating the impact of semantic segmentation and pose
estimation on dense semantic SLAM. <em>IROS</em>, 5328–5335. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent Semantic SLAM methods combine classical geometry-based estimation with deep learning-based object detection or semantic segmentation. In this paper we evaluate the quality of semantic maps generated by state-of-the-art class-and instance-aware dense semantic SLAM algorithms whose codes are publicly available and explore the impacts both semantic segmentation and pose estimation have on the quality of semantic maps. We obtain these results by providing algorithms with ground-truth pose and/or semantic segmentation data available from simulated environments. We establish that semantic segmentation is the largest source of error through our experiments, dropping mAP and OMQ performance by up to 74.3\% and 71.3\% respectively.},
  archive   = {C_IROS},
  author    = {Suman Raj Bista and David Hall and Ben Talbot and Haoyang Zhang and Feras Dayoub and Niko Sünderhauf},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636271},
  pages     = {5328-5335},
  title     = {Evaluating the impact of semantic segmentation and pose estimation on dense semantic SLAM},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust SLAM systems: Are we there yet? <em>IROS</em>,
5320–5327. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Progress in the last decade has brought about significant improvements in the accuracy and speed of SLAM systems, broadening their mapping capabilities. Despite these advancements, long-term operation remains a major challenge, primarily due to the wide spectrum of perturbations robotic systems may encounter.Increasing the robustness of SLAM algorithms is an ongoing effort, however it usually addresses a specific perturbation. Generalisation of robustness across a large variety of challenging scenarios is not well-studied nor understood. This paper presents a systematic evaluation of the robustness of open-source state-of-the-art SLAM algorithms with respect to challenging conditions such as fast motion, non-uniform illumination, and dynamic scenes. The experiments are performed with perturbations present both independently of each other, as well as in combination in long-term deployment settings in unconstrained environments (lifelong operation).The detailed results (approx. 20,000 experiments) along with comprehensive documentation of the benchmarking tool for integrating new datasets and evaluating SLAM algorithms not studied in this work are available at https://robustslam.github.io/evaluation.},
  archive   = {C_IROS},
  author    = {Mihai Bujanca and Xuesong Shi and Matthew Spear and Pengpeng Zhao and Barry Lennox and Mikel Luján},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636814},
  pages     = {5320-5327},
  title     = {Robust SLAM systems: Are we there yet?},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NimbRo avatar: Interactive immersive telepresence with
force-feedback telemanipulation. <em>IROS</em>, 5312–5319. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic avatars promise immersive teleoperation with human-like manipulation and communication capabilities. We present such an avatar system, based on the key components of immersive 3D visualization and transparent force-feedback telemanipulation. Our avatar robot features an anthropomorphic bimanual arm configuration with dexterous hands. The remote human operator drives the arms and fingers through an exoskeleton-based operator station, which provides force feedback both at the wrist and for each finger. The robot torso is mounted on a holonomic base, providing locomotion capability in typical indoor scenarios, controlled using a 3D rudder device. Finally, the robot features a 6D movable head with stereo cameras, which stream images to a VR HMD worn by the operator. Movement latency is hidden using spherical rendering. The head also carries a telepresence screen displaying a synthesized image of the operator with facial animation, which enables direct interaction with remote persons. We evaluate our system successfully both in a user study with untrained operators as well as a longer and more complex integrated mission. We discuss lessons learned from the trials and possible improvements.},
  archive   = {C_IROS},
  author    = {Max Schwarz and Christian Lenz and Andre Rochow and Michael Schreiber and Sven Behnke},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636191},
  pages     = {5312-5319},
  title     = {NimbRo avatar: Interactive immersive telepresence with force-feedback telemanipulation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to arbitrate human and robot control using
disagreement between sub-policies. <em>IROS</em>, 5305–5311. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the context of teleoperation, arbitration refers to deciding how to blend between human and autonomous robot commands. We present a reinforcement learning solution that learns an optimal arbitration strategy that allocates more control authority to the human when the robot comes across a decision point in the task. A decision point is where the robot encounters multiple options (sub-policies), such as having multiple paths to get around an obstacle or deciding between two candidate goals. By expressing each directional sub-policy as a von Mises distribution, we identify the decision points by observing the modality of the mixture distribution. Our reward function reasons on this modality and prioritizes to match its learned policy to either the user or the robot accordingly. We report teleoperation experiments on reach-and-grasping objects using a robot manipulator arm with different simulated human controllers. Results indicate that our shared control agent outperforms direct control and improves the teleoperation performance among different users. Using our reward term enables flexible blending between human and robot commands while maintaining safe and accurate teleoperation.},
  archive   = {C_IROS},
  author    = {Yoojin Oh and Marc Toussaint and Jim Mainprice},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636049},
  pages     = {5305-5311},
  title     = {Learning to arbitrate human and robot control using disagreement between sub-policies},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to guide human attention on mobile telepresence
robots with 360° vision. <em>IROS</em>, 5297–5304. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile telepresence robots (MTRs) allow people to navigate and interact with a remote environment that is in a place other than the person’s true location. Thanks to the recent advances in 360° vision, many MTRs are now equipped with an all-degree visual perception capability. However, people’s visual field horizontally spans only about 120° of the visual field captured by the robot. To bridge this observability gap toward human-MTR shared autonomy, we have developed a framework, called GHAL360, to enable the MTR to learn a goal-oriented policy from reinforcements for guiding human attention using visual indicators. Three telepresence environments were constructed using datasets that are extracted from Matterport3D and collected from a real robot respectively. Experimental results show that GHAL360 outperformed the baselines from the literature in the efficiency of a human-MTR team completing target search tasks. A demo video is available: https://youtu.be/aGbTxCGJSDM},
  archive   = {C_IROS},
  author    = {Kishan Chandan and Jack Albertson and Xiaohan Zhang and Xiaoyang Zhang and Yao Liu and Shiqi Zhang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636607},
  pages     = {5297-5304},
  title     = {Learning to guide human attention on mobile telepresence robots with 360° vision},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SnakeRaven: Teleoperation of a 3D printed snake-like
manipulator integrated to the RAVEN II surgical robot. <em>IROS</em>,
5282–5288. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Telerobotic systems combined with miniaturised snake-like or elephant-trunk robotic arms can improve the ergonomics and accessibility in minimally invasive surgical tasks such as knee arthroscopy. Such systems, however, are usually designed in a specific and integral approach, making it expensive to adapt to various procedures or patient anatomies. 3D printed instruments with a detachable design can bring the benefits of patient-specific customisation, affordability, and adaptability to new clinical scenarios. However, the integration of such snake-like instruments to standard telerobotic systems can be challenging in terms of design and control. In this study, a teleoperation system is developed to control and steer the pose of SnakeRaven: a 3D printed, customisable snake-like end-effector attached to the RAVEN II platform for the application of fibre-optic knee arthroscopy. Algorithms for the parametric inverse kinematics and mapping between the RAVEN II joint space to the coupled tendon-driven rolling joints are developed. The controller is tested and validated on the physical prototype interfacing with the RAVEN II platform in a teleoperation experiment. A video demonstrating the main results of this paper can be found via https://youtu.be/ApJjR853kIQ},
  archive   = {C_IROS},
  author    = {Andrew Razjigaev and Ajay K. Pandey and David Howard and Jonathan Roberts and Liao Wu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636878},
  pages     = {5282-5288},
  title     = {SnakeRaven: Teleoperation of a 3D printed snake-like manipulator integrated to the RAVEN II surgical robot},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Passivity-based control for haptic teleoperation of a legged
manipulator in presence of time-delays. <em>IROS</em>, 5276–5281. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When dealing with the haptic teleoperation of multi-limbed mobile manipulators, the problem of mitigating the destabilizing effects arising from the communication link between the haptic device and the remote robot has not been properly addressed. In this work, we propose a passive control architecture to haptically teleoperate a legged mobile manipulator, while remaining stable in the presence of time delays and frequency mismatches in the master and slave controllers. At the master side, a discrete-time energy modulation of the control input is proposed. At the slave side, passivity constraints are included in an optimization-based whole-body controller to satisfy the energy limitations. A hybrid teleoperation scheme allows the human operator to remotely operate the robot’s end- effector while in stance mode, and its base velocity in locomotion mode. The resulting control architecture is demonstrated on a quadrupedal robot with an artificial delay added to the network.},
  archive   = {C_IROS},
  author    = {Mattia Risiglione and Jean-Pierre Sleiman and Maria Vittoria Minniti and Burak Çizmeci and Douwe Dresscher and Marco Hutter},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636642},
  pages     = {5276-5281},
  title     = {Passivity-based control for haptic teleoperation of a legged manipulator in presence of time-delays},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Non-prehensile manipulation of cuboid objects using a
catenary robot. <em>IROS</em>, 5270–5275. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Transporting objects using quadrotors with cables has been widely studied in the literature. However, most of those approaches assume that the cables are previously attached to the load by human intervention. In tasks where multiple objects need to be moved, the efficiency of the robotic system is constrained by the requirement of manual labor. Our approach uses a non-stretchable cable connected to two quadrotors, which we call the catenary robot, that fully automates the transportation task. Using the cable, we can roll and drag the cuboid object (box) on planar surfaces. Depending on the surface type, we choose the proper action, dragging for low friction, and rolling for high friction. Therefore, the transportation process does not require any human intervention as we use the cable to interact with the box without requiring fastening. We validate our control design in simulation and with actual robots, where we show them rolling and dragging boxes to track desired trajectories.},
  archive   = {C_IROS},
  author    = {Gustavo A. Cardona and Diego S. D’Antonio and Cristian-Ioan Vasile and David Saldaña},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636820},
  pages     = {5270-5275},
  title     = {Non-prehensile manipulation of cuboid objects using a catenary robot},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3D human reconstruction in the wild with collaborative
aerial cameras. <em>IROS</em>, 5263–5269. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Aerial vehicles are revolutionizing applications that require capturing the 3D structure of dynamic targets in the wild, such as sports, medicine and entertainment. The core challenges in developing a motion-capture system that operates in outdoors environments are: (1) 3D inference requires multiple simultaneous viewpoints of the target, (2) occlusion caused by obstacles is frequent when tracking moving targets, and (3) the camera and vehicle state estimation is noisy. We present a real-time aerial system for multi-camera control that can reconstruct human motions in natural environments without the use of special-purpose markers. We develop a multi-robot coordination scheme that maintains the optimal flight formation for target reconstruction quality amongst obstacles. We provide studies evaluating system performance in simulation, and validate real-world performance using two drones while a target performs activities such as jogging and playing soccer. Supplementary video: https://youtu.be/jxt91vx0cns},
  archive   = {C_IROS},
  author    = {Cherie Ho and Andrew Jong and Harry Freeman and Rohan Rao and Rogerio Bonatti and Sebastian Scherer},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636745},
  pages     = {5263-5269},
  title     = {3D human reconstruction in the wild with collaborative aerial cameras},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gamma-ray imaging with spatially continuous intensity
statistics. <em>IROS</em>, 5257–5262. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Novel methods for the inference of radiation intensity functions defined over known surfaces are proposed, intended for use in surveying applications with mobile spectrometers. Previous approaches, based on the maximum likelihood expectation maximization (ML-EM) framework with Poisson likelihoods, are extended to better handle spatially continuous intensity statistics using ideas from Gaussian filtering. The resulting algorithm is evaluated against a classical ML-EM method, and a recently proposed sparse additive point source localization (APSL) algorithm in a Monte-Carlo simulation study. The new generalized ASPL (GASPL) is shown to compare favorably in terms of estimation accuracy when the true intensity is not well described by a set of point sources. Finally, the GASPL is used in an experiment where a detector is mounted to an unmanned aerial vehicle to estimate the intensity and location of radioactive sources placed in a meadow.},
  archive   = {C_IROS},
  author    = {Marcus Greiff and Emil Rofors and Anders Robertsson and Rolf Johansson and Rikard Tyllström},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635973},
  pages     = {5257-5262},
  title     = {Gamma-ray imaging with spatially continuous intensity statistics},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visibility-aware trajectory optimization with application to
aerial tracking. <em>IROS</em>, 5249–5256. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The visibility of targets determines performance and even success rate of various applications, such as active slam, exploration, and target tracking. Therefore, it is crucial to take the visibility of targets into explicit account in trajectory planning. In this paper, we propose a general metric for target visibility, considering observation distance and angle as well as occlusion effect. We formulate this metric into a differentiable visibility cost function, with which spatial trajectory and yaw can be jointly optimized. Furthermore, this visibility-aware trajectory optimization handles dynamic feasibility of position and yaw simultaneously. To validate that our method is practical and generic, we integrate it into a customized quadrotor tracking system. The experimental results show that our visibility-aware planner performs more robustly and observes targets better. In order to benefit related researches, we release our code to the public.},
  archive   = {C_IROS},
  author    = {Qianhao Wang and Yuman Gao and Jialin Ji and Chao Xu and Fei Gao},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636753},
  pages     = {5249-5256},
  title     = {Visibility-aware trajectory optimization with application to aerial tracking},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Aggressive visual perching with quadrotors on inclined
surfaces. <em>IROS</em>, 5242–5248. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous Micro Aerial Vehicles (MAVs) have the potential to be employed for surveillance and monitoring tasks. By perching and staring on one or multiple locations aerial robots can save energy while concurrently increasing their overall mission time without actively flying. In this paper, we address the estimation, planning, and control problems for autonomous perching on inclined surfaces with small quadrotors using visual and inertial sensing. We focus on planning and executing dynamically feasible trajectories to navigate and perch to a desired target location with on board sensing and computation. Our planner also supports certain classes of nonlinear global constraints by leveraging an efficient algorithm that we have mathematically verified. The on board cameras and IMU are concurrently used for state estimation and to infer the relative robot/target localization. The proposed solution runs in real-time on board a limited computational unit. Experimental results validate the proposed approach by tackling aggressive perching maneuvers with flight envelopes that include large excursions from the hover position on inclined surfaces up to 90°, angular rates up to 600 deg/s, and accelerations up to 10 m/s 2 .},
  archive   = {C_IROS},
  author    = {Jeffrey Mao and Guanrui Li and Stephen Nogar and Christopher Kroninger and Giuseppe Loianno},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636690},
  pages     = {5242-5248},
  title     = {Aggressive visual perching with quadrotors on inclined surfaces},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toward battery-free flight: Duty cycled recharging of small
drones. <em>IROS</em>, 5234–5241. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Constrained battery life on current Unmanned Aerial Vehicles (drones) limits the time they can operate and distance they can travel. We address this challenge by harvesting solar power to enable duty-cycled operation on a palm-sized drone. We present a scaling analysis that suggests that more solar power can be collected per unit mass of the drone as scale reduces, favoring small drones. By charging from the sun, the drone can operate for more than a single charging cycle, enabling extended mission time, and long-distance travel. To realize this, we design a high efficiency charging circuit and introduce two innovations. The first is a photovoltaic array that passively folds down while in flight to reduce air drag and automatically opens during landing due to the ground effect. The second is a sensor system and controller that autonomously finds suitable charging sites that are flat and well-lit. The drone can be fully charged in 3 hrs using the solar array and charging circuit with an average efficiency of 90.84\%. Each charge enables a 4.7 min flight, allowing the drone to travel up to 1.2 km in a day. We also discuss how this platform could be used to take periodic measurements for smart agriculture or wildlife tracking, rapidly deploy wireless networks, or deploy microrobots in the future.},
  archive   = {C_IROS},
  author    = {Nishant Elkunchwar and Suvesha Chandrasekaran and Vikram Iyer and Sawyer B. Fuller},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636087},
  pages     = {5234-5241},
  title     = {Toward battery-free flight: Duty cycled recharging of small drones},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards an online framework for changing-contact robot
manipulation tasks. <em>IROS</em>, 5203–5210. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We describe a framework for changing-contact robot manipulation tasks, which require the robot to make and break contacts with objects and surfaces. The discontinuous interaction dynamics of such tasks make it difficult to construct and use a single dynamics model or control strategy for such tasks. For any target motion trajectory, our framework incrementally improves its prediction of when contacts will occur. This prediction and a model relating approach velocity to impact force modify the velocity profile of the motion sequence such that it is C ∞ smooth, and help achieve a desired force on impact. We implement this framework by building on our hybrid force-motion variable impedance controller for continuous-contact tasks. We evaluate our framework in the illustrative context of a robot manipulator performing sliding tasks involving multiple contact changes with surfaces of different properties.},
  archive   = {C_IROS},
  author    = {Saif Sidhik and Mohan Sridharan and Dirk Ruiken},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636278},
  pages     = {5203-5210},
  title     = {Towards an online framework for changing-contact robot manipulation tasks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computationally efficient HQP-based whole-body control
exploiting the operational-space formulation. <em>IROS</em>, 5197–5202.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a novel and practical approach to enhance the computational efficiency of the hierarchical quadratic programming (HQP)-based whole-body control. The HQP method is known to offer control solutions satisfying strict priority with various constraints for multiple-tasks execution. However, it inherently comes at the price of high computation time to solve QP optimization problems in each hierarchical level which limits practicability in a real-time control system with fast sampling time. To mitigate this issue, we propose that the operational space formulation is incorporated into the HQP method, where the decision variables are intuitively defined at the task level and possess smaller dimensions. Indeed, it serves faster whole-body control solution for multiple tasks under equality and inequality constraints yet strictly fulfilling the task priority. The performance of the pro-posed method is experimentally verified on the actual floating-based humanoid, named TOCABI with 33 degrees-of-freedom. In addition, computation time is analyzed by comparison with conventional HQP and other advanced implementation forms.},
  archive   = {C_IROS},
  author    = {Yisoo Lee and Junewhee Ahn and Jinoh Lee and Jaeheung Park},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636867},
  pages     = {5197-5202},
  title     = {Computationally efficient HQP-based whole-body control exploiting the operational-space formulation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fundamental challenges in deep learning for stiff contact
dynamics. <em>IROS</em>, 5181–5188. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Frictional contact has been extensively studied as the core underlying behavior of legged locomotion and manipulation, and its nearly-discontinuous nature makes planning and control difficult even when an accurate model of the robot is available. Here, we present empirical evidence that learning an accurate model in the first place can be confounded by contact, as modern deep learning approaches are not designed to capture this non-smoothness. We isolate the effects of contact’s non-smoothness by varying the mechanical stiffness of a compliant contact simulator. Even for a simple system, we find that stiffness alone dramatically degrades training processes, generalization, and data-efficiency. Our results raise serious questions about simulated testing environments which do not accurately reflect the stiffness of rigid robotic hardware. Significant additional investigation will be necessary to fully understand and mitigate these effects, and we suggest several avenues for future study.},
  archive   = {C_IROS},
  author    = {Mihir Parmar and Mathew Halm and Michael Posa},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636383},
  pages     = {5181-5188},
  title     = {Fundamental challenges in deep learning for stiff contact dynamics},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time physically-accurate simulation of robotic snap
connection process. <em>IROS</em>, 5173–5180. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel real-time physically-accurate simulation framework for the snap connection process. For this, we first notice the peculiarities of the process, namely, small/smooth deformation, stiff connector and segmented contact. We then design our simulation to fully exploit these peculiarities by adopting the following strategies: 1) the technique of passive midpoint integration (PMI [1]), which allows for stable simulation of arbitrarily light/stiff system by enforcing discrete-time passivity; 2) linear finite element method (FEM [2]) modeling, which is adequate to deal with the small snap connector deformation while providing much faster speed as compared to nonlinear FEM; 3) segmentation of the snap connector FEM model and solving of each segment individually with their coupling analytically eliminated, thereby, further speeding up the simulation; 4) balanced model reduction (BMR [3]) to further reduce the dimension of each segment purely analytically without any prior experiment or simulation; and 5) parallelized data-driven collision detection, which turns out to further significantly speed up our simulation. Experimentally-verified simulations are also performed to show the efficacy of our proposed simulation framework.},
  archive   = {C_IROS},
  author    = {Minji Lee and Jeongmin Lee and Jaemin Yoon and Dongjun Lee},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636246},
  pages     = {5173-5180},
  title     = {Real-time physically-accurate simulation of robotic snap connection process},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Contact tracing: A low cost reconstruction framework for
surface contact interpolation. <em>IROS</em>, 5165–5172. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel, low cost framework for reconstructing surface contact movements during in-hand manipulations. Unlike many existing methods focused on hand pose tracking, ours models the behavior of contact patches, and by doing so is the first to obtain detailed contact tracking estimates for multi-contact manipulations. Our framework is highly accessible, requiring only low cost, readily available paint materials, a single RGBD camera, and a simple, deterministic interpolation algorithm. Despite its simplicity, we demonstrate the framework’s effectiveness over the course of several manipulations on three common household items. Finally, we demonstrate the use of a generated contact time series in manipulation learning for a simulated robot hand.},
  archive   = {C_IROS},
  author    = {Arjun Lakshmipathy and Dominik Bauer and Nancy S. Pollard},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636313},
  pages     = {5165-5172},
  title     = {Contact tracing: A low cost reconstruction framework for surface contact interpolation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning linear policies for robust bipedal locomotion on
terrains with varying slopes. <em>IROS</em>, 5159–5164. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, with a view toward deployment of light-weight control frameworks for bipedal walking robots, we realize end-foot trajectories that are shaped by a single linear feedback policy. We learn this policy via a model-free and a gradient free learning algorithm, Augmented Random Search (ARS), in the two robot platforms Rabbit and Digit. Our contributions are two-fold: a) By using torso and support plane orientation as inputs, we achieve robust walking on slopes of upto 20° in simulation. b) We demonstrate additional behaviors like walking backwards, stepping-in-place, and recovery from external pushes of upto 120 N. The end-result is a robust and a fast feedback control law for bipedal walking on terrains with varying slopes. Towards the end, we also provide preliminary results of hardware transfer to Digit.},
  archive   = {C_IROS},
  author    = {Lokesh Krishna and Utkarsh A. Mishra and Guillermo A. Castillo and Ayonga Hereid and Shishir Kolathaya},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636070},
  pages     = {5159-5164},
  title     = {Learning linear policies for robust bipedal locomotion on terrains with varying slopes},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Impact invariant control with applications to bipedal
locomotion. <em>IROS</em>, 5151–5158. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When legged robots impact their environment, they undergo large changes in their velocities in a small amount of time. Measuring and applying feedback to these velocities is challenging, and is further complicated due to uncertainty in the impact model and impact timing. This work proposes a general framework for adapting feedback control during impact by projecting the control objectives to a subspace that is invariant to the impact event. The resultant controller is robust to uncertainties in the impact event while maintaining maximum control authority over the impact invariant subspace. We demonstrate the utility of the projection on a walking controller for a planar five-link-biped and on a jumping controller for a compliant 3D bipedal robot, Cassie. The effectiveness of our method is shown to translate well on hardware.},
  archive   = {C_IROS},
  author    = {William Yang and Michael Posa},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636094},
  pages     = {5151-5158},
  title     = {Impact invariant control with applications to bipedal locomotion},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Learning when to switch: Composing controllers to traverse
a sequence of terrain artifacts. <em>IROS</em>, 5144–5150. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Legged robots often use separate control policies that are highly engineered for traversing difficult terrain such as stairs, gaps, and steps, where switching between policies is only possible when the robot is in a region that is common to adjacent controllers. Deep Reinforcement Learning (DRL) is a promising alternative to hand-crafted control design, though typically requires the full set of test conditions to be known before training. DRL policies can result in complex (often unrealistic) behaviours that have few or no overlapping regions between adjacent policies, making it difficult to switch behaviours. In this work we develop multiple DRL policies with Curriculum Learning (CL), each that can traverse a single respective terrain condition, while ensuring an overlap between policies. We then train a network for each destination policy that estimates the likelihood of successfully switching from any other policy. We evaluate our switching method on a previously unseen combination of terrain artifacts and show that it performs better than heuristic methods. While our method is trained on individual terrain types, it performs comparably to a Deep Q Network trained on the full set of terrain conditions. This approach allows the development of separate policies in constrained conditions with embedded prior knowledge about each behaviour, that is scalable to any number of behaviours, and prepares DRL methods for applications in the real world.},
  archive   = {C_IROS},
  author    = {Brendan Tidd and Akansel Cosgun and Jürgen Leitner and Nicolas Hudson},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636233},
  pages     = {5144-5150},
  title     = {Learning when to switch: Composing controllers to traverse a sequence of terrain artifacts},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust feedback motion policy design using reinforcement
learning on a 3D digit bipedal robot. <em>IROS</em>, 5136–5143. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, a hierarchical and robust framework for learning bipedal locomotion is presented and successfully implemented on the 3D biped robot Digit built by Agility Robotics. We propose a cascade-structure controller that combines the learning process with intuitive feedback regulations. This design allows the framework to realize robust and stable walking with a reduced-dimensional state and action spaces of the policy, significantly simplifying the design and increasing the sampling efficiency of the learning method. The inclusion of feedback regulation into the framework improves the robustness of the learned walking gait and ensures the success of the sim-to-real transfer of the proposed controller with minimal tuning. We specifically present a learning pipeline that considers hardware-feasible initial poses of the robot within the learning process to ensure the initial state of the learning is replicated as close as possible to the initial state of the robot in hardware experiments. Finally, we demonstrate the feasibility of our method by successfully transferring the learned policy in simulation to the Digit robot hardware, realizing sustained walking gaits under external force disturbances and challenging terrains not incurred during the training process. To the best of our knowledge, this is the first time a learning-based policy is transferred successfully to the Digit robot in hardware experiments.},
  archive   = {C_IROS},
  author    = {Guillermo A. Castillo and Bowen Weng and Wei Zhang and Ayonga Hereid},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636467},
  pages     = {5136-5143},
  title     = {Robust feedback motion policy design using reinforcement learning on a 3D digit bipedal robot},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Knee-stretched biped gait generation along spatially
quantized curves. <em>IROS</em>, 5120–5127. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a method for biped gait generation along a predefined curve with fully stretched knees. First, we design a spatial gait pattern as a function of the traveled distance on the path without considering dynamics. Then, a consistent dynamic walking motion is obtained by optimization that minimizes the zero-moment point and the speed errors while considering the trade-off between kinematics and dynamics. This method generalizes the spatially quantized dynamics-based gait generation, which is our former method restricted to straight paths, to walk on arbitrary curves. The generated gaits are validated by dynamic simulation.},
  archive   = {C_IROS},
  author    = {Yuki Onishi and Shuuji Kajita and Tatsuya Ibuki and Mitsuji Sampei},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636872},
  pages     = {5120-5127},
  title     = {Knee-stretched biped gait generation along spatially quantized curves},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). F-VESPA: A kinematic-based algorithm for real-time
heel-strike detection during walking. <em>IROS</em>, 5098–5103. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With over 10 million people currently suffering from significant long-term gait disability in the United States only, robot-assisted rehabilitation and wearable devices are increasingly gaining attention as a mean to regain functional mobility. Since these devices work collaborative and synchronously with the human gait, it is necessary to be able to detect gait events, such as heel-strikes, in real-time. Although many algorithms have been proposed for detecting heel-strikes with either wearable (e.g. Inertial Measurement Units (IMUs)) or non-wearable (e.g. force plates) sensors, there is a great need for employing less obtrusive and reliable sensors that rely only on recording the kinematics of the leg motion. This work proposes a novel and efficient kinematic algorithm, called the Foot VErtical &amp; Sagittal Position Algorithm (F-VESPA), which has several advantages over existing methods. First, it accurately estimates heel-strike events using kinematic data without requiring access to future data points, rendering it the first to our knowledge kinematic algorithm capable of real-time implementation during treadmill walking. Moreover, it does not require tuning of the utilized parameters, rendering it robust to different subjects, conditions and equipment. The algorithm is tested in a large set of subjects across various treadmill speeds, and it is shown to outperform online and offline implementations of existing prominent kinematic algorithms. Using a 150 Hz data collection system, the F-VESPA achieved a total true error of 33 ms (median) in detecting heel-strike. The F-VESPA is the first to our knowledge kinematic algorithm that can detect heel-strike events during treadmill walking in real-time, with high accuracy, robustness and fast response, enabling real-time control of a variety of assistive platforms and devices, among others.},
  archive   = {C_IROS},
  author    = {Chrysostomos Karakasis and Panagiotis Artemiadis},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636335},
  pages     = {5098-5103},
  title     = {F-VESPA: A kinematic-based algorithm for real-time heel-strike detection during walking},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Muscle synergies enable accurate joint moment prediction
using few electromyography sensors. <em>IROS</em>, 5090–5097. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There is an increasing demand for accurate prediction of joint moments using wearable sensors for robotic exoskeletons to achieve precise control and for rehabilitation care to remotely monitor users’ condition. In this study, we used electromyography (EMG) signals to first identify muscle synergies, then used them to train of a long short-term memory network to predict knee joint moments during walking. Kinematics, ground reaction forces and EMG from 10 muscles on the right limb were collected from 6 able-bodied subjects during normal gait. Between 4 and 6 muscle synergies were extracted from the EMG signals, generating two outputs - the muscle synergies weight matrix and the time-dependent muscle synergies action signals. The muscle synergies action signals and measured knee joint moments from inverse dynamics were then used as inputs to train the joint moment prediction model using a long short-term memory network. For testing, between 4 and 7 EMG signals were used to estimate the muscle synergies action signals with the extracted muscle synergies weights matrix. The estimated muscle synergies action signals were then used to predict knee joint moments. Knee joint moments were also predicted directly from all 10 EMGs, then from 4-7 EMG signals using another long short-term memory network. Prediction accuracy from the synergies-trained network vs. the EMG-trained network were compared, using the same number of EMG signals in each. Prediction error with respect to moments measured via inverse dynamics was computed for both networks. Knee moments predicted with as few as 4 EMGs was at least as accurate as moments predicted from all 10 EMGs when muscle synergies were exploited. Predicted knee moments from muscle synergies achieved an average of 4.63\% root mean square error from 4 EMG signals, which was lower than error when predicted directly from 4 EMG signals (5.63\%).},
  archive   = {C_IROS},
  author    = {Yi-Xing Liu and Elena M. Gutierrez-Farewik},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636696},
  pages     = {5090-5097},
  title     = {Muscle synergies enable accurate joint moment prediction using few electromyography sensors},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A soft robotic hip exosuit (SR-HExo) to assist hip flexion
and extension during human locomotion. <em>IROS</em>, 5060–5066. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents the design, fabrication, and preliminary results of a soft hip exosuit to assist hip flexion and extension during walking. The exosuit uses soft and compliant materials to create a wearable robot that has a low profile, low mass, and is highly flexible to freely move with the user’s natural range of motion. The Soft Robotic Hip Exosuit (SR-HExo) consists of flat fabric pneumatic artificial muscles (ff-PAM) that contract when pressurized. The ff-PAM actuators are oriented in an ‘X’ shape to allow for natural range of motion across the hip joint and can generate 190 N of force at 200 kPa in a 0.3 sec window. The ‘X’ configuration (X-ff-PAM) actuators were placed on the anterior and posterior sides of the hip joint with height adjustable Velcro straps. Extension assistance and flexion assistance was provided in 10-45\% and 50-90\% of the gait cycle, respectively. To evaluate the effectiveness of the SR-HExo with healthy participants, hip range of motion and muscle activity during walking were monitored using a motion capture system and surface electromyography sensors. The impact of the SR-HExo on the range of motion was minimal with only a 4.0 o reduction from the target range of motion of 30 o . The exosuit contributed to reducing hip muscle activity. Hip extensor muscles showed a reduction of 13.1\% for the gluteus maximus and 6.6\% for the biceps femoris. Hip flexor muscles showed a reduction of 10.7\% for the iliacus and 27.7\% for the rectus femoris.},
  archive   = {C_IROS},
  author    = {Carly M. Thalman and Lily Baye-Wallace and Hyunglae Lee},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636225},
  pages     = {5060-5066},
  title     = {A soft robotic hip exosuit (SR-HExo) to assist hip flexion and extension during human locomotion},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PackerBot: Variable-sized product packing with heuristic
deep reinforcement learning. <em>IROS</em>, 5002–5008. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Product packing is a typical application in ware-house automation that aims to pick objects from unstructured piles and place them into bins with optimized placing policy. However, it still remains a significant challenge to finish the product packing tasks in general logistics scenarios where the objects are variable-sized and the configurations are complex. In this work, we present the PackerBot, a complete robotic pipeline for performing variable-sized product packing in unstructured scenes. First, by leveraging the imperfect experience of human packer, we propose a heuristic DRL framework for learning optimal online 3D bin packing policy. Then we integrate it with a 6-DoF suction-based picking module and a product size estimation module, leading to a complete product packing system, namely the PackerBot. Extensive experimental results show that our method achieves the state-of-the-art performance in both simulated and real-world tests. The video demonstration is available at: https://vsislab.github.io/packerbot.},
  archive   = {C_IROS},
  author    = {Zifei Yang and Shuo Yang and Shuai Song and Wei Zhang and Ran Song and Jiyu Cheng and Yibin Li},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635914},
  pages     = {5002-5008},
  title     = {PackerBot: Variable-sized product packing with heuristic deep reinforcement learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-object grasping – estimating the number of objects in
a robotic grasp. <em>IROS</em>, 4995–5001. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A human hand can grasp a desired number of objects at once from a pile based solely on tactile sensing. To do so, a robot needs to make a grasp in a pile, sense the number of objects in the grasp before lifting, and predict how many will remain in the grasp after lifting. It is a very challenging problem because when making the prediction, the robotic hand is still in the pile and the objects in the grasp are not observable to vision systems. Moreover, some objects in the hand before lifting may fall out the grasp when the lifting starts because they were supported by other objects in the pile instead of the fingers. A robotic hand should sense how many objects are in a grasp using its tactile sensors before lifting. This paper presents novel multi-object grasping analyzing methods to solve this problem. They include a grasp volume calculation, tactile force analysis, and a data-driven deep learning approach. The methods have been implemented on a Barrett hand and then evaluated in simulations and a real setup with a robotic system. The evaluation results conclude that once the Barrett hand grasps multiple objects in the pile, the data-driven models can make a good prediction before lifting on how many objects will remain in the hand after lifting. The root-mean-square errors are 0.74 for balls and 0.58 for cubes in simulations, and 1.06 for balls and 1.45 for cubes in the real system.},
  archive   = {C_IROS},
  author    = {Tianze Chen and Adheesh Shenoy and Anzhelika Kolinko and Syed Shah and Yu Sun},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636777},
  pages     = {4995-5001},
  title     = {Multi-object grasping – estimating the number of objects in a robotic grasp},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Partial formation of hydroxyapatite on poly (vinyl alcohol)
hydrogel for intensive motions of biomimetic soft robots. <em>IROS</em>,
4989–4994. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The fabrication method to utilize poly (vinyl alcohol) hydrogels with additional stiff parts in a single structure for hydrogel-based soft robots to realize an intensive motion with elastic energy is proposed in this paper. An inorganic material which is often seen in the hard tissues of our body; hydroxyapatite, was partially formed on a hydrogel with a simple procedure of alternatingly soaking a hydrogel with a covering device into two salt liquids. With this bioinspired structure, the whole hydrogel was able to change its physical properties while maintaining its whole structure as one. We succeeded in flicking motion of this structure only by bias displacement. This structure will help soft robots to move more intensively in a simple fabrication.},
  archive   = {C_IROS},
  author    = {Towa Ueno and Haruka Oda and Yuya Morimoto and Shoji Takeuchi},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636368},
  pages     = {4989-4994},
  title     = {Partial formation of hydroxyapatite on poly (Vinyl alcohol) hydrogel for intensive motions of biomimetic soft robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Soft retraction device and internal camera mount for
everting vine robots. <em>IROS</em>, 4982–4988. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft, tip-extending, pneumatic &quot;vine robots&quot; that grow via eversion are well suited for navigating cluttered environments. Two key mechanisms that add to the robot’s functionality are a tip-mounted retraction device that allows the growth process to be reversed, and a tip-mounted camera that enables vision. However, previous designs used rigid, relatively heavy electromechanical retraction devices and external camera mounts, which reduce some advantages of these robots. These designs prevent the robot from squeezing through tight gaps, make it challenging to lift the robot tip against gravity, and require the robot to drag components against the environment. To address these limitations, we present a soft, pneumatically driven retraction device and an internal camera mount that are both lightweight and smaller than the diameter of the robot. The retraction device is composed of a soft, extending pneumatic actuator and a pair of soft clamping actuators that work together in an inch-worming motion. The camera mount sits inside the robot body and is kept at the tip of the robot by two low-friction interlocking components. We present characterizations of our retraction device and demonstrations that the robot can grow and retract through turns, tight gaps, and sticky environments while transmitting live video from the tip. Our designs advance the ability of everting vine robots to navigate difficult terrain while collecting data.},
  archive   = {C_IROS},
  author    = {William E. Heap and Nicholas D. Naclerio and Margaret M. Coad and Sang-Goo Jeong and Elliot W. Hawkes},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636697},
  pages     = {4982-4988},
  title     = {Soft retraction device and internal camera mount for everting vine robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Climbot-ω: A soft robot with novel grippers and
rigid-compliantly constrained body for climbing on various poles.
<em>IROS</em>, 4975–4981. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft climbing robots have been attracting increasing attention in soft robotics community, and a lot of prototypes been proposed with basic climbing function implemented. Climbing on poles is a challenge with soft robots, and the capability of current pole-climbing soft robots needs to be improved in terms of adaptability to various poles and deformation controllability or constraining of the soft body. In this paper, a rigid-compliant coupling or constraint method is proposed in design of pole climbing robots. Specifically, a novel gripper with inner expanding bubbles and a rigid-compliant belt is presented and designed, featured with excellent shape/size-adaptability and grip-reliability. To constrain undesired deformation of the soft body, rigid-compliant belts are also adopted. Three rigid-compliant constrained actuators are connected to implement an Ω-shaped deformation of the body for climbing motion. The kinematic feature of the body is established, and the two main features of the inner expanding gripper are analyzed. A series of experiments, where the robot climbs on poles with different shapes and sizes and in different poses, have verified the effectiveness of the presented rigid-compliant constraint and the shape/size adaptability and grip-reliability of the novel inner expanding grippers.},
  archive   = {C_IROS},
  author    = {Manjia Su and Yu Qiu and Yisheng Guan and Haifei Zhu and Zhi Liu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636566},
  pages     = {4975-4981},
  title     = {Climbot-Ω: A soft robot with novel grippers and rigid-compliantly constrained body for climbing on various poles},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multifunctional robotic glove with active-passive training
modes for hand rehabilitation and assistance. <em>IROS</em>, 4969–4974.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft robotic gloves have shown great advantages in assisting individuals with hand pathologies to perform continuous exercises to restore their hand functions, which could considerably accelerate the rehabilitation process and reduce the costs. However, single rehabilitation mode, difficulty in achieving multiple degrees-of-freedom (DoF) motion, and the lack of high-fidelity feedback still challenge the development of soft robotic gloves. In this paper, we propose a novel design of a robotic glove based on soft-rigid hybrid joint actuators and minimal clutches. We first introduce structures and working principles of the proposed bending joint actuator in detail and then characterize the single joint actuator. Furthermore, we present a performance evaluation of the whole robotic glove in both active and passive modes. Preliminary experimental results showed that (1) in the active training mode, the tested human hand’s muscle effort needed to conduct gross finger flexion increased from 11.16\% to 42.60\% of the maximum value when the air pressure inside the minimal clutches changed from 0 kPa to 200 kPa; (2) in the passive mode, the 10-DoF robotic glove could assist the tested hand to perform various training exercises and grasp various objects with different hand postures. This paper focuses on the integrated design of multi-DoF structures and variable stiffness mechanisms, which will have an impact on the development of multifunctional soft robots and wearable devices.},
  archive   = {C_IROS},
  author    = {Yongkang Jiang and Diansheng Chen and Junlin Ma and Zhe Liu and Yazhe Luo and Jian Li and Yingtian Li},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636437},
  pages     = {4969-4974},
  title     = {Multifunctional robotic glove with active-passive training modes for hand rehabilitation and assistance},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Turning an articulated 3-PPSR manipulator into a parallel
continuum robot. <em>IROS</em>, 4955–4960. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Parallel Continuum Robots (PCR) have received a lot of attention in recent years. This paper presents a new 6-degrees-of-freedom PCR derived from the conventional 3-PPSR parallel manipulator. This robot is driven by three limbs consisting of two flexible rods each and replacing the spherical and revolute joints of the original version. Each limb is mounted onto two linear axes arranged in series. To allow a direct comparison between the articulated and the continuum version, the parallel mechanism of an industrial manipulator has been replaced by an elastic structure of the same size. The simulations and the experiments show that the flexible counterpart of the manipulator is able to achieve a larger workspace, increasing the range of motion by 150\% for rotations and by 157\% in elevation. Moreover, the position repeatability is improved by 47\% (reaching 3.4 µm) and the orientation repeatability by 57\% (reaching 14.3 µrad). This can be explained by the removal of the spherical and revolute joints but also by the constant stress in the structure that acts as an anti backlash system on leadscrew actuators.},
  archive   = {C_IROS},
  author    = {Oscar F. Gallardo and Benjamin Mauzé and Redwan Dahmouche and Christian Duriez and Guillaume J. Laurent},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636596},
  pages     = {4955-4960},
  title     = {Turning an articulated 3-PPSR manipulator into a parallel continuum robot},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-balancing online dataset for incremental driving
intelligence. <em>IROS</em>, 4941–4946. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous driving with imitation learning is vulnerable to the quality of an expert dataset. Typical driving involves situations or online data that are biased toward specific scenarios such as lane following or stop. This property causes an imbalance in the driving dataset, and it is highly likely to deteriorate the performance of autonomous driving with imitation learning. In this paper, we propose a dataset self-balancing system with biased online data and an imbalanced dataset. By estimating the probability distribution of a dataset, we compute the probability and novelty of online data and then filter only qualified novel data. In addition, using the computed probability distribution, we determine the data that are non-informative in the current dataset and then exchange them with novel online data. At last, by retraining the driving neural network with high-entropy data batches, our method achieves incremental driving intelligence. We demonstrated the effectiveness of our method through open-loop evaluation and ablation studies in a CARLA simulator; the results show that our proposed system effectively balances the dataset with 100 scenarios and decreases test loss over time.},
  archive   = {C_IROS},
  author    = {Hyung-Suk Yoon and Chan Kim and Seong-Woo Kim and Seung-Woo Seo},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636525},
  pages     = {4941-4946},
  title     = {Self-balancing online dataset for incremental driving intelligence},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards safe navigation through crowded dynamic
environments. <em>IROS</em>, 4934–4940. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a novel neural network-based control policy to enable a mobile robot to navigate safety through environments filled with both static obstacles, such as tables and chairs, and dense crowds of pedestrians. The network architecture uses early fusion to combine a short history of lidar data with kinematic data about nearby pedestrians. This kinematic data is key to enable safe robot navigation in these uncontrolled, human-filled environments. The network is trained in a supervised setting, using expert demonstrations to learn safe navigation behaviors. A series of experiments in detailed simulated environments demonstrate the efficacy of this policy, which is able to achieve a higher success rate than either standard model-based planners or state-of-the-art neural network control policies that use only raw sensor data.},
  archive   = {C_IROS},
  author    = {Zhanteng Xie and Pujie Xin and Philip Dames},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636102},
  pages     = {4934-4940},
  title     = {Towards safe navigation through crowded dynamic environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bootstrapping motor skill learning with motion planning.
<em>IROS</em>, 4926–4933. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning a robot motor skill from scratch is impractically slow; so much so that in practice, learning must typically be bootstrapped using human demonstration. However, relying on human demonstration necessarily degrades the autonomy of robots that must learn a wide variety of skills over their operational lifetimes. We propose using kinematic motion planning as a completely autonomous, sample efficient way to bootstrap motor skill learning for object manipulation. We demonstrate the use of motion planners to bootstrap motor skills in two complex object manipulation scenarios with different policy representations: opening a drawer with a dynamic movement primitive representation, and closing a microwave door with a deep neural network policy. We also show how our method can bootstrap a motor skill for the challenging dynamic task of learning to hit a ball off a tee, where a kinematic plan based on treating the scene as static is insufficient to solve the task, but sufficient to bootstrap a more dynamic policy. In all three cases, our method is competitive with human-demonstrated initialization, and significantly out-performs starting with a random policy. This approach enables robots to to efficiently and autonomously learn motor policies for dynamic tasks without human demonstration.},
  archive   = {C_IROS},
  author    = {Ben Abbatematteo and Eric Rosen and Stefanie Tellex and George Konidaris},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636661},
  pages     = {4926-4933},
  title     = {Bootstrapping motor skill learning with motion planning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Many-joint robot arm control with recurrent spiking neural
networks. <em>IROS</em>, 4918–4925. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the paper, we show how scalable, low-cost trunk-like robotic arms can be constructed using only basic 3D-printing equipment and simple electronics. The design is based on uniform, stackable joint modules with three degrees of freedom each. Moreover, we present an approach for controlling these robots with recurrent spiking neural networks. At first, a spiking forward model learns motor-pose correlations from movement observations. After training, intentions can be projected back through unrolled spike trains of the forward model essentially routing the intention-driven motor gradients towards the respective joints, which unfolds goal-direction navigation. We demonstrate that spiking neural networks can thus effectively control trunk-like robotic arms with up to 75 articulated degrees of freedom with near millimeter accuracy.},
  archive   = {C_IROS},
  author    = {Manuel Traub and Robert Legenstein and Sebastian Otte},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636001},
  pages     = {4918-4925},
  title     = {Many-joint robot arm control with recurrent spiking neural networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ORCHID: Optimisation of robotic control and hardware in
design using reinforcement learning. <em>IROS</em>, 4911–4917. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The successful performance of any system is dependant on the hardware of the agent, which is typically immutable during RL training. In this work, we present ORCHID (Optimisation of Robotic Control and Hardware In Design) which allows for truly simultaneous optimisation of hardware and control parameters in an RL pipeline. We show that by forming a complex differential path through a trajectory rollout we can leverage a vast amount of information from the system that was previously lost in the ‘black-box’ environment. Combining this with a novel hardware-conditioned critic network minimises variance during training and ensures stable updates are made. This allows for refinements to be made to both the morphology and control parameters simultaneously. The result is an efficient and versatile approach to holistic robot design, that brings the final system nearer to true optimality. We show improvements in performance across 4 different test environments with two different control algorithms - in all experiments the maximum performance achieved with ORCHID is shown to be unattainable using only policy updates with the default design. We also show how re-designing a robot using ORCHID in simulation, transfers to a vast improvement in the performance of a real-world robot.},
  archive   = {C_IROS},
  author    = {Lucy Jackson and Celyn Walters and Steve Eckersley and Pete Senior and Simon Hadfield},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635865},
  pages     = {4911-4917},
  title     = {ORCHID: Optimisation of robotic control and hardware in design using reinforcement learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Talk the talk and walk the walk: Dialogue-driven navigation
in unknown indoor environments. <em>IROS</em>, 4903–4910. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Prior work in natural-language-driven navigation demonstrates success in systems deployed in synthetic environments or applied to large datasets, both real and synthetic. However, there is an absence of such frameworks being deployed and rigorously tested in real environments, unknown a priori. In this paper, we present a novel framework that uses spoken dialogue with a real person to interpret a set of navigational instructions into a plan and subsequently execute that plan in a novel, unknown, indoor environment. This framework is implemented on a real robot and its performance is evaluated in 39 trials across 3 novel test-building environments. We also demonstrate that our approach outperforms three prior vision-and-language navigation methods in this same environment.},
  archive   = {C_IROS},
  author    = {Thomas Victor Ilyevsky and Jared Sigurd Johansen and Jeffrey Mark Siskind},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636548},
  pages     = {4903-4910},
  title     = {Talk the talk and walk the walk: Dialogue-driven navigation in unknown indoor environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NaturalNets: Simplified biological neural networks for
learning complex tasks. <em>IROS</em>, 4896–4902. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a new neural network architecture, called NaturalNet, which uses a simplified biological neuron model and consists of a set of nonlinear ordinary differential equations. We model the membrane potential of each neuron by integrating the in-flowing currents, but we do not consider ion channels, nor individual spikes. To keep the membrane potential within a defined value range, we introduce a suitable clipping mechanism. With our approach, we aim to develop agents solving complex tasks by providing a higher biological plausibility than commonly used neural networks for deep learning applications, while also offering low computational complexity to enable fast training. To demonstrate the learning capabilities of NaturalNets, we use the virtual robotic environments of the OpenAI Gym framework, a widely-used toolkit for developing and comparing reinforcement learning algorithms. We compared a variety of different widespread neural network architectures, including long short-term memory (LSTM), gated recurrent units (GRUs), feedforward, and Elman networks. Our experiments show that NaturalNets were able to perform well for all considered virtual robotic control tasks, where we apply the covariance matrix adaptation evolutionary strategy (CMAES) for training.},
  archive   = {C_IROS},
  author    = {Daniel Zimmermann and Björn Jürgens and Patrick Deubel and Anne Koziolek},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636471},
  pages     = {4896-4902},
  title     = {NaturalNets: Simplified biological neural networks for learning complex tasks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Maneuver-based trajectory prediction for self-driving cars
using spatio-temporal convolutional networks. <em>IROS</em>, 4888–4895.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to predict the future movements of other vehicles is a subconscious and effortless skill for humans and key to safe autonomous driving. Therefore, trajectory prediction for autonomous cars has gained a lot of attention in recent years. It is, however, still a hard task to achieve human-level performance. Interdependencies between vehicle behaviors and the multimodal nature of future intentions in a dynamic and complex driving environment render trajectory prediction a challenging problem. In this work, we propose a new, datadriven approach for predicting the motion of vehicles in a road environment. The model allows for inferring future intentions from the past interaction among vehicles in highway driving scenarios. Using our neighborhood-based data representation, the proposed system jointly exploits correlations in the spatial and temporal domain using convolutional neural networks. Our system considers multiple possible maneuver intentions and their corresponding motion and predicts the trajectory for five seconds into the future. We implemented our approach and evaluated it on two highway datasets taken in different countries and are able to achieve a competitive prediction performance.},
  archive   = {C_IROS},
  author    = {Benedikt Mersch and Thomas Höllen and Kun Zhao and Cyrill Stachniss and Ribana Roscher},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636875},
  pages     = {4888-4895},
  title     = {Maneuver-based trajectory prediction for self-driving cars using spatio-temporal convolutional networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gridlock-free autonomous parking lots for autonomous
vehicles. <em>IROS</em>, 4881–4887. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many cities suffer from a shortage of parking spaces. Research in high density parking (HDP) focuses on how to increase the capacity of parking lots by allowing vehicles to block each other but temporarily give way to other vehicles by driving autonomously upon request. Previous works on HDP did not consider mixing different parking strategies and ignored the possibility of gridlock when multiple vehicles move simultaneously. In this paper, we describe the design of autonomous parking lots, which allows the deployment of different parking strategies in different regions in a parking lot. We present algorithms for checking whether adding a vehicle to an autonomous parking lot can lead to gridlock. Our simulation shows that autonomous parking lots can hold 60\% more vehicles given the same amount of space.},
  archive   = {C_IROS},
  author    = {Tsz-Chiu Au},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636591},
  pages     = {4881-4887},
  title     = {Gridlock-free autonomous parking lots for autonomous vehicles},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vision-based control of an unknown suspended payload with a
multirotor. <em>IROS</em>, 4875–4880. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a vision-based control strategy for a rotary-wing unmanned aerial vehicle (RUAV) transporting an unknown suspended payload. The suspended payload parameters, which include its mass and cable length, are unknown and direct measurements of its states are not available. A feedforward-feedback adaptive control strategy, that consists of a notch filter and linear quadratic Gaussian (LQG) controller, is proposed to simultaneously avoid the excitation and actively damp the payload swing oscillations. The unknown payload mass is estimated using recursive least squares and the unknown cable length is estimated using a dedicated sine wave estimator. The payload parameter estimates are then used to adapt the control strategy for the specific suspended payload. A vision-based state estimator is implemented to provide payload state estimates for the optimal full-state feedback controller. Simulation results show that the control strategy successfully adapts for different suspended payloads and effectively damps the unwanted payload swing oscillations.},
  archive   = {C_IROS},
  author    = {J. F. Slabber and H. W. Jordaan},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636648},
  pages     = {4875-4880},
  title     = {Vision-based control of an unknown suspended payload with a multirotor},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automated type-aware traffic speed prediction based on
sparse intelligent camera system. <em>IROS</em>, 4869–4874. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many essential services for autonomous vehicles, e.g., navigation on high-quality maps, are designed based on the understanding of traffic conditions, e.g., travel time/speed on road segments, traffic flow, etc. However, most existing traffic condition models lack the consideration of the differentiation for vehicles with different types (e.g., personal vehicles or trucks) and thus they cannot satisfy some type-specific services, e.g., traffic-condition-based routing for autonomous vehicles with different types. To address this challenge, we design a novel vehicular mobility based sensing model called mDrive to predict the travel speed on the road segments, which is targeted for different types of vehicles by utilizing the camera data obtained from the traffic cameras equipped in the road intersections only, without any in-vehicle GPS devices. mDrive addresses the type-aware traffic speed prediction problem with sparse sensors based on three correlations: (1) the spatial correlation of travel speed on the connected road segments; (2) the temporal correlation of travel speed on the consecutive time slots; (3) the type correlation of different vehicular types’ speed on the same road segment. We implement mDrive on traffic camera data from the Chinese city Suzhou and evaluate it by using the detailed GPS data from personal vehicles, taxis, and trucks, with road contextual data as ground truth. The experiment show mDrive outperforms state-of-the-art methods by reducing 6.2\% mean relative error on average for all types of vehicles.},
  archive   = {C_IROS},
  author    = {Xiaoyang Xie and Kangjia Shao and Yang Wang and Fei Miao and Desheng Zhang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636559},
  pages     = {4869-4874},
  title     = {Automated type-aware traffic speed prediction based on sparse intelligent camera system},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extended VINS-mono: A systematic approach for absolute and
relative vehicle localization in large-scale outdoor environments.
<em>IROS</em>, 4861–4868. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a systematic approach called Extended VINS-Mono to utilize VINS-Mono, a state-of-the-art monocular visual-inertial relative localization method, targeting practical vehicle localization in large-scale outdoor road environments. Our proposed fusion approach associates multiple independent localization methods and provides multiple (projected) state estimates in a desired coordinate system to satisfy different accuracy, rate and latency requirements. We extend VINS-Mono with absolute localization methods like GNSS and relative localization methods like Kalman-filter-based INS to provide global state estimation for navigation/routing and local state estimation for planning/control. Additionally, Extended VINS-Mono addresses two significant drawbacks in VINS-Mono for use in large-scale outdoor road environments. First, motion on an almost planar road surface will make scale unobservable in VINS-Mono. Secondly, moving objects in dynamic scenarios will degrade accuracy. We handle the scale estimation problem of VINS-Mono by extending its (re-)initialization process with speed readings and introducing a speed factor for use with graph optimization. A dynamic feature-point filter method with masks from DNN-based object detection handles dynamic environments and re-collects feature points on stationary objects like parked cars. Better global accuracy is obtained with Extended VINS-Mono, compared to VINS-Mono, in a 25 km-trip journey through highways, tunnels, urban areas and suburban areas in Pittsburgh. Thus, Extended VINS-Mono can be used for reliable and accurate absolute localization in dynamic road environments. We also evaluate the accuracy, localization rate and latency of multiple (projected) state estimates in the global coordinate system from multiple localization methods. Our fusion method is therefore able to satisfy different localization requirements of various tasks on an intelligent vehicle.},
  archive   = {C_IROS},
  author    = {Mengwen He and Ragunathan Raj Rajkumar},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636776},
  pages     = {4861-4868},
  title     = {Extended VINS-mono: A systematic approach for absolute and relative vehicle localization in large-scale outdoor environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3D radar velocity maps for uncertain dynamic environments.
<em>IROS</em>, 4854–4860. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Future urban transportation concepts include a mixture of ground and air vehicles with varying degrees of autonomy in a congested environment. In such dynamic environments, occupancy maps alone are not sufficient for safe path planning. Safe and efficient transportation requires reasoning about the 3D flow of traffic and properly modeling uncertainty. Several different approaches can be taken for developing 3D velocity maps. This paper explores a Bayesian approach that captures our uncertainty in the map given training data. The approach involves projecting spatial coordinates into a high-dimensional feature space and then applying Bayesian linear regression to make predictions and quantify uncertainty in our estimates. On a collection of air and ground datasets, we demonstrate that this approach is effective and more scalable than several alternative approaches.},
  archive   = {C_IROS},
  author    = {Ransalu Senanayake and Kyle Beltran Hatch and Jason Zheng and Mykel J. Kochenderfer},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636019},
  pages     = {4854-4860},
  title     = {3D radar velocity maps for uncertain dynamic environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic lambda-field: A counterpart of the bayesian
occupancy grid for risk assessment in dynamic environments.
<em>IROS</em>, 4846–4853. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the context of autonomous vehicles, one of the most crucial tasks is to estimate the risk of the undertaken action. While navigating in complex urban environments, the Bayesian occupancy grid is one of the most popular types of maps, where the information of occupancy is stored as the probability of collision. Although widely used, this kind of representation is not well suited for risk assessment: because of its discrete nature, the probability of collision becomes dependent on the tessellation size. Therefore, risk assessments on Bayesian occupancy grids cannot yield risks with meaningful physical units. In this article, we propose an alternative framework called Dynamic Lambda-Field that is able to assess generic physical risks in dynamic environments without being dependent on the tessellation size. Using our framework, we are able to plan safe trajectories where the risk function can be adjusted depending on the scenario. We validate our approach with quantitative experiments, showing the convergence speed of the grid and that the framework is suitable for real-world scenarios.},
  archive   = {C_IROS},
  author    = {Johann Laconte and Elie Randriamiarintsoa and Abderrahim Kasmi and François Pomerleau and Roland Chapuis and Christophe Debain and Romuald Aufrère},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636804},
  pages     = {4846-4853},
  title     = {Dynamic lambda-field: A counterpart of the bayesian occupancy grid for risk assessment in dynamic environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online monitoring of object detection performance during
deployment. <em>IROS</em>, 4839–4845. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {During deployment, an object detector is expected to operate at a similar performance level reported on its testing dataset. However, when deployed onboard mobile robots that operate under varying and complex environmental conditions, the detector’s performance can fluctuate and occasionally degrade severely without warning. Undetected, this can lead the robot to take unsafe and risky actions based on low-quality and unreliable object detections. We address this problem and introduce a cascaded neural network that monitors the performance of the object detector by predicting the quality of its mean average precision (mAP) on a sliding window of the input frames. The proposed cascaded network exploits the internal features from the deep neural network of the object detector. We evaluate our proposed approach using different combinations of autonomous driving datasets and object detectors.},
  archive   = {C_IROS},
  author    = {Quazi Marufur Rahman and Niko Sünderhauf and Feras Dayoub},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635940},
  pages     = {4839-4845},
  title     = {Online monitoring of object detection performance during deployment},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Object learning for 6D pose estimation and grasping from
RGB-d videos of in-hand manipulation. <em>IROS</em>, 4831–4838. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Object models are highly useful for robots as they enable tasks such as detection, pose estimation and manipulation. However, models are not always easily available, especially in real-world domains of operation such as peoples’ homes. This work presents a pipeline to generate high-quality object reconstructions from human in-hand manipulation to alleviate the necessity of specialised or expensive hardware. Missing data, due to occlusion or unseen sides, is explicitly handled by incorporating shape completion. We demonstrate the usability of the reconstructions by applying a model-based as well as a CNN-based object pose estimator that is trained on synthetic images by employing state-of-the-art texture synthesis. Using our pipeline to cheaply generate object models and synthetic RGB images for training, we achieve competitive performance compared to baselines that require an elaborate set-up to construct models or large amounts of annotated data. Object grasping is also enabled by learning with the reconstructions in simulation, then executing with a real robot. These evaluations show that our reconstructions are comparable to those made under near-perfect conditions and enable 6D object pose estimation as well as real-world grasping.},
  archive   = {C_IROS},
  author    = {Timothy Patten and Kiru Park and Markus Leitner and Kevin Wolfram and Markus Vincze},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635884},
  pages     = {4831-4838},
  title     = {Object learning for 6D pose estimation and grasping from RGB-D videos of in-hand manipulation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unknown object segmentation from stereo images.
<em>IROS</em>, 4823–4830. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although instance-aware perception is a key prerequisite for many autonomous robotic applications, most of the methods only partially solve the problem by focusing solely on known object categories. However, for robots interacting in dynamic and cluttered environments, this is not realistic and severely limits the range of potential applications. Therefore, we propose a novel object instance segmentation approach that does not require any semantic or geometric information of the objects beforehand. In contrast to existing works, we do not explicitly use depth data as input, but rely on the insight that slight viewpoint changes, which for example are provided by stereo image pairs, are often sufficient to determine object boundaries and thus to segment objects. Focusing on the versatility of stereo sensors, we employ a transformer-based architecture that maps directly from the pair of input images to the object instances. This has the major advantage that instead of a noisy, and potentially incomplete depth map as an input, on which the segmentation is computed, we use the original image pair to infer the object instances and a dense depth map. In experiments in several different application domains, we show that our Instance Stereo Transformer (INSTR) algorithm outperforms current state-of-the-art methods that are based on depth maps. Training code and pretrained models are available at https://github.com/DLR-RM/instr.},
  archive   = {C_IROS},
  author    = {Maximilian Durner and Wout Boerdijk and Martin Sundermeyer and Werner Friedl and Zoltán-Csaba Márton and Rudolph Triebel},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636281},
  pages     = {4823-4830},
  title     = {Unknown object segmentation from stereo images},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Category-level 6D object pose estimation via cascaded
relation and recurrent reconstruction networks. <em>IROS</em>,
4807–4814. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Category-level 6D pose estimation, aiming to predict the location and orientation of unseen object instances, is fundamental to many scenarios such as robotic manipulation and augmented reality, yet still remains unsolved. Precisely recovering instance 3D model in the canonical space and accurately matching it with the observation is an essential point when estimating 6D pose for unseen objects. In this paper, we achieve accurate category-level 6D pose estimation via cascaded relation and recurrent reconstruction networks. Specifically, a novel cascaded relation network is dedicated for advanced representation learning to explore the complex and informative relations among instance RGB image, instance point cloud and category shape prior. Furthermore, we design a recurrent reconstruction network for iterative residual refinement to progressively improve the reconstruction and correspondence estimations from coarse to fine. Finally, the instance 6D pose is obtained leveraging the estimated dense correspondences between the instance point cloud and the reconstructed 3D model in the canonical space. We have conducted extensive experiments on two well-acknowledged benchmarks of category-level 6D pose estimation, with significant performance improvement over existing approaches. On the representatively strict evaluation metrics of 3D 75 and 5°2cm, our method exceeds the latest state-of-the-art SPD [1] by 4.9\% and 17.7\% on the CAMERA25 dataset, and by 2.7\% and 8.5\% on the REAL275 dataset. Codes are avaliable at https://wangjiaze.cn/projects/6DPoseEstimation.html.},
  archive   = {C_IROS},
  author    = {Jiaze Wang and Kai Chen and Qi Dou},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636212},
  pages     = {4807-4814},
  title     = {Category-level 6D object pose estimation via cascaded relation and recurrent reconstruction networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). COINet: Adaptive segmentation with co-interactive network
for autonomous driving. <em>IROS</em>, 4800–4806. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Semantic segmentation serves as a cornerstone for safety autonomous driving and has been achieved remarkable progress at the price of dense annotations. Unsupervised domain adaptation was widely utilized to addresses this labor-intensive problem, which transfers the knowledge learned from labeled synthetic datset to real-world without any annotations. However, most existing adaptation works predict the segmentation results and domain identification results separately only with the last-layer feature, and ignore the intrinsic relationship among these two tasks. To address this issue, we present a CO-Interactive Network (COINet) for unsupervised adaptive segmentation. In particular, we propose a scale-aware distilled decoder to integrate multi-scale features dynamically through the designed inter-distilled module (IDM) and obtain fine-grained feature representations. A dual-task classifier is advanced with this decoder, to jointly predict the segmentation results and pixel-wise domain prediction results, which extracts shared complementary information for accurate segmentation. We further devise a co-interactive loss to explicitly model the intrinsic relationship among the segmentation and domain prediction, enabling the feature distribution alignment in pixel-level and an optimal segmentation decision boundary. We demonstrate the effectiveness of the proposed COINet on benchmark adaptation settings with extensive experimental and ablation results, and our model shows favorable performance against existing algorithms.},
  archive   = {C_IROS},
  author    = {Jie Liu and Xiaoqing Guo and Baopu Li and Yixuan Yuan},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636111},
  pages     = {4800-4806},
  title     = {COINet: Adaptive segmentation with co-interactive network for autonomous driving},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ADD: A fine-grained dynamic inference architecture for
semantic image segmentation. <em>IROS</em>, 4792–4799. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dynamic inference that adaptively skips parts of model execution based on the complexity of input data can effectively reduce the computation cost of deep learning models during the inference. However, current architectures for dynamic inference only consider the exits at the block level, whose results may not be suitable for different applications. In this paper, we present the Auto-Dynamic-DeepLab (ADD), a network architecture that enables the fine-grained dynamic inference for semantic image segmentation. To allow the exit points in the cell level, ADD utilizes Neural Architecture Search (NAS), supported by the framework of Auto-DeepLab, to seek the optimal network structure. In addition, ADD replaces the cells in Auto-DeepLab with the densely connected cells to ease the interference among multiple classifiers and employs the earlier decision maker (EDM) to further improve the performance. Experimental results show that ADD can achieve similar accuracy as Auto-DeepLab in terms of mIoU with a 1.6 times speedup. For the fast mode, ADD can achieve 2.15 times speedup with only a 1.3\% accuracy drop compared to those of Auto-DeepLab.},
  archive   = {C_IROS},
  author    = {Chi-Hsi Kung and Che-Rung Lee},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636650},
  pages     = {4792-4799},
  title     = {ADD: A fine-grained dynamic inference architecture for semantic image segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scalable reinforcement learning policies for multi-agent
control. <em>IROS</em>, 4785–4791. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We develop a Multi-Agent Reinforcement Learning (MARL) method to learn scalable control policies for target tracking. Our method can handle an arbitrary number of pursuers and targets; we show results for tasks consisting up to 1000 pursuers tracking 1000 targets. We use a decentralized, partially-observable Markov Decision Process framework to model pursuers as agents receiving partial observations (range and bearing) about targets which move using fixed, unknown policies. An attention mechanism is used to parameterize the value function of the agents; this mechanism allows us to handle an arbitrary number of targets. Entropy-regularized off-policy RL methods are used to train a stochastic policy, and we discuss how it enables a hedging behavior between pursuers that leads to a weak form of cooperation in spite of completely decentralized control execution. We further develop a masking heuristic that allows training on smaller problems with few pursuers-targets and execution on much larger problems. Thorough simulation experiments and comparisons to state of the art algorithms are performed to study the scalability of the approach and robustness of performance to varying numbers of agents and targets.},
  archive   = {C_IROS},
  author    = {Christopher D. Hsu and Heejin Jeong and George J. Pappas and Pratik Chaudhari},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636344},
  pages     = {4785-4791},
  title     = {Scalable reinforcement learning policies for multi-agent control},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Moving forward in formation: A decentralized hierarchical
learning approach to multi-agent moving together. <em>IROS</em>,
4777–4784. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent path finding in formation has many potential real-world applications like mobile warehouse robotics. However, previous multi-agent path finding (MAPF) methods hardly take formation into consideration. Further-more, they are usually centralized planners and require the whole state of the environment. Other decentralized partially observable approaches to MAPF are reinforcement learning (RL) methods. However, these RL methods encounter difficulties when learning path finding and formation problems at the same time. In this paper, we propose a novel decentralized partially observable RL algorithm that uses a hierarchical structure to decompose the multi-objective task into unrelated ones. It also calculates a theoretical weight that makes each tasks reward has equal influence on the final RL value function. Additionally, we introduce a communication method that helps agents cooperate with each other. Experiments in simulation show that our method outperforms other end-to-end RL methods and our method can naturally scale to large world sizes where centralized planner struggles. We also deploy and validate our method in a real-world scenario.},
  archive   = {C_IROS},
  author    = {Shanqi Liu and Licheng Wen and Jinhao Cui and Xuemeng Yang and Junjie Cao and Yong Liu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636224},
  pages     = {4777-4784},
  title     = {Moving forward in formation: A decentralized hierarchical learning approach to multi-agent moving together},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hiding leader’s identity in leader-follower navigation
through multi-agent reinforcement learning. <em>IROS</em>, 4769–4776.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Leader-follower navigation is a popular class of multi-robot algorithms where a leader robot leads the follower robots in a team. The leader has specialized capabilities or mission critical information (e.g. goal location) that the followers lack, and this makes the leader crucial for the mission’s success. However, this also makes the leader a vulnerability -an external adversary who wishes to sabotage the robot team’s mission can simply harm the leader and the whole robot team’s mission would be compromised. Since robot motion generated by traditional leader-follower navigation algorithms can reveal the identity of the leader, we propose a defense mechanism of hiding the leader’s identity by ensuring the leader moves in a way that behaviorally camouflages it with the followers, making it difficult for an adversary to identify the leader. To achieve this, we combine Multi-Agent Reinforcement Learning, Graph Neural Networks and adversarial training. Our approach enables the multi-robot team to optimize the primary task performance with leader motion similar to follower motion, behaviorally camouflaging it with the followers. Our algorithm outperforms existing work that tries to hide the leader’s identity in a multi-robot team by tuning traditional leader-follower control parameters with Classical Genetic Algorithms. We also evaluated human performance in inferring the leader’s identity and found that humans had lower accuracy when the robot team used our proposed navigation algorithm.},
  archive   = {C_IROS},
  author    = {Ankur Deka and Wenhao Luo and Huao Li and Michael Lewis and Katia Sycara},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636314},
  pages     = {4769-4776},
  title     = {Hiding leader’s identity in leader-follower navigation through multi-agent reinforcement learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to play soccer from scratch: Sample-efficient
emergent coordination through curriculum-learning and competition.
<em>IROS</em>, 4745–4752. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work proposes a scheme that allows learning complex multi-agent behaviors in a sample efficient manner, applied to 2v2 soccer. The problem is formulated as a Markov game, and solved using deep reinforcement learning. We propose a basic multi-agent extension of TD3 for learning the policy of each player, in a decentralized manner. To ease learning, the task of 2v2 soccer is divided in three stages: 1v0, 1v1 and 2v2. The process of learning in multi-agent stages (1v1 and 2v2) uses agents trained in a previous stage as fixed opponents. In addition, we propose using experience sharing, a method that shares experience from a fixed opponent, trained in a previous stage, for training the agent currently learning, and a form of frame-skipping, to raise performance significantly. Our results show that high quality soccer play can be obtained with our approach in just under 40M interactions. A summarized video of the resulting game play can be found in https://youtu.be/pScrKNqfELE.},
  archive   = {C_IROS},
  author    = {Pavan Samtani and Francisco Leiva and Javier Ruiz-del-Solar},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636046},
  pages     = {4745-4752},
  title     = {Learning to play soccer from scratch: Sample-efficient emergent coordination through curriculum-learning and competition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Flocking and collision avoidance for a dynamic squad of
fixed-wing UAVs using deep reinforcement learning. <em>IROS</em>,
4738–4744. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Developing the flocking behavior for a dynamic squad of fixed-wing UAVs is still a challenge due to kinematic complexity and environmental uncertainty. In this paper, we deal with the decentralized flocking and collision avoidance problem through deep reinforcement learning (DRL). Specifically, we formulate a decentralized DRL-based decision making framework from the perspective of every follower, where a collision avoidance mechanism is integrated into the flocking controller. Then, we propose a novel reinforcement learning algorithm PS-CACER for training a shared control policy for all the followers. Besides, we design a plug-n-play embedding module based on convolutional neural networks and the attention mechanism. As a result, the variable-length system state can be encoded into a fixed-length embedding vector, which makes the learned DRL policy independent with the number and the order of followers. Finally, numerical simulation results demonstrate the effectiveness of the proposed method, and the learned policies can be directly transferred to semi-physical simulation without any parameter finetuning.},
  archive   = {C_IROS},
  author    = {Chao Yan and Xiaojia Xiang and Chang Wang and Zhen Lan},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636183},
  pages     = {4738-4744},
  title     = {Flocking and collision avoidance for a dynamic squad of fixed-wing UAVs using deep reinforcement learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Roadmap for visibility-based target tracking: Iterative
construction and motion strategy. <em>IROS</em>, 4732–4737. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the problem of generating a fixed path for a mobile observer in a polygonal environment that can maintain a line-of-sight with an unpredictable target. In contrast to purely off-line or on-line techniques, we propose a hierarchical tracking strategy in which an off-line path generation technique based on a RRT is coupled with an online feedback-control technique to generate trajectories for the mobile observer.},
  archive   = {C_IROS},
  author    = {Guillermo Laguna and Shashwata Mandal and Sourabh Bhattacharya},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636581},
  pages     = {4732-4737},
  title     = {Roadmap for visibility-based target tracking: Iterative construction and motion strategy},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast generation of obstacle-avoiding motion primitives for
quadrotors. <em>IROS</em>, 4726–4731. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work considers the problem of generating computationally efficient quadrotor motion primitives between a given pose (position, velocity, and acceleration) and a goal plane in the presence of obstacles. A new motion primitive tool based on the logistic curve is proposed and a closed-form analytic approach is developed to satisfy constraints on starting pose, goal plane, velocity, acceleration, and jerk. The geometric obstacle avoidance problem is represented as a combinatorial set problem and a heuristic approach is proposed to accelerate the solution search. Numerical examples are presented to highlight the fast motion primitive generation in multi-obstacle pose-to-plane scenarios.},
  archive   = {C_IROS},
  author    = {Saurabh Upadhyay and Thomas Richardson and Arthur Richards},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636002},
  pages     = {4726-4731},
  title     = {Fast generation of obstacle-avoiding motion primitives for quadrotors},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Risk-aware submodular optimization for stochastic travelling
salesperson problem. <em>IROS</em>, 4720–4725. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a risk-aware variant of the Traveling Salesperson Problem (TSP), where the robot tour cost and reward have to be optimized simultaneously, while being subjected to uncertainty in both. We study the case where the rewards and the costs exhibit diminishing marginal gains, i.e., are submodular. Since the costs and the rewards are stochastic, we seek to maximize a risk metric known as Conditional-Value-at-Risk (CVaR) of the submodular function. We propose a Risk-Aware Greedy Algorithm (RAGA) to find an approximate solution for this problem. The approximation algorithm runs in polynomial time and is within a constant factor of the optimal and an additive term that depends on the value of optimal solution. We use the submodular function’s curvature to improve approximation results further and verify the algorithm’s performance through simulations.},
  archive   = {C_IROS},
  author    = {Rishab Balasubramanian and Lifeng Zhou and Pratap Tokekar and P. B. Sujit},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635957},
  pages     = {4720-4725},
  title     = {Risk-aware submodular optimization for stochastic travelling salesperson problem},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Variable-speed traveling salesman problem for vehicles with
curvature constrained trajectories. <em>IROS</em>, 4714–4719. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel approach to the multigoal trajectory planning for vehicles with curvature-constrained trajectories such as fixed-wing aircraft. In the existing formulation called the Dubins Traveling Salesman Problem (DTSP), the vehicle speed is assumed to be constant over the whole trajectory, and that does not allow adaptation of the turning radius of the trajectory between the target locations. It does not support optimization of the overall flight time of the multi-goal trajectory by exploiting higher speeds for longer turning radii. Therefore, we propose a novel problem formulation called the Variable-Speed Traveling Salesman Problem (VS-TSP) that employs time-efficient trajectories with variable speed based on a generalization of the Dubins vehicle model, allowing multiple turning radii and change of the forward speed of the vehicle. The VS-TSP allows the vehicle to slow down if high maneuverability is necessary and speed up if high-speed turns with a large radius are beneficial to the overall tour cost. Based on the evaluation results for Cessna 172 aircraft model, the proposed VNS-based algorithm with variable speed provides up to about 20\% faster trajectories than a solution of the DTSP with a single turning radius.},
  archive   = {C_IROS},
  author    = {Kristýna Kučerová and Petr Váňa and Jan Faigl},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636762},
  pages     = {4714-4719},
  title     = {Variable-speed traveling salesman problem for vehicles with curvature constrained trajectories},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Path planning for robotic manipulators in dynamic
environments using distance information. <em>IROS</em>, 4708–4713. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a novel algorithm – DRGBT (Dynamic Rapidly-exploring Generalized Bur Tree), intended for motion planning in dynamic environments. The main idea behind DRGBT lies in a so-called adaptive horizon, consisting of a set of prospective target nodes that belong to a predefined $\mathcal{C}$-space path, which originates from the current node. Each node is assigned a weight that depends on relative distances and captured changes in the environment. The algorithm continuously uses a suitable horizon assessment to decide when to trigger the replanning procedure. A comprehensive simulation study is performed, covering a variety of manipulators, where DRGBT is compared to a state-of-the-art algorithm. Results indicate some promising features of the proposed method.},
  archive   = {C_IROS},
  author    = {Nermin Covic and Bakir Lacevic and Dinko Osmankovic},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636730},
  pages     = {4708-4713},
  title     = {Path planning for robotic manipulators in dynamic environments using distance information},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint sampling and trajectory optimization over graphs for
online motion planning. <em>IROS</em>, 4700–4707. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Among the most prevalent motion planning techniques, sampling and trajectory optimization have emerged successful due to their ability to handle tight constraints and high-dimensional systems, respectively. However, limitations in sampling in higher dimensions and local minima issues in optimization have hindered their ability to excel beyond static scenes in offline settings. Here we consider highly dynamic environments with long horizons that necessitate a fast on-line solution. We present a unified approach that leverages the complementary strengths of sampling and optimization, and interleaves them both in a manner that is well suited to this challenging problem. With benchmarks in multiple synthetic and realistic simulated environments, we show that our approach performs significantly better on various metrics against baselines that employ either only sampling or only optimization. Project page: https://sites.google.com/view/jistplanner},
  archive   = {C_IROS},
  author    = {Kalyan Vasudev Alwala and Mustafa Mukadam},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636064},
  pages     = {4700-4707},
  title     = {Joint sampling and trajectory optimization over graphs for online motion planning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning when to quit: Meta-reasoning for motion planning.
<em>IROS</em>, 4692–4699. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Anytime motion planners are widely used in robotics. However, the relationship between their solution quality and computation time is not well understood, and thus, determining when to quit planning and start execution is unclear. In this paper, we address the problem of deciding when to stop deliberation under bounded computational capacity, so called meta-reasoning, for anytime motion planning. We propose data-driven learning methods, model-based and model-free meta-reasoning, that are applicable to different environment distributions and agnostic to the choice of anytime motion planners. As a part of the framework, we design a convolutional neural network-based optimal solution predictor that predicts the optimal path length from a given 2D workspace image. We empirically evaluate the performance of the proposed methods in simulation in comparison with baselines.},
  archive   = {C_IROS},
  author    = {Yoonchang Sung and Leslie Pack Kaelbling and Tomás Lozano-Pérez},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636864},
  pages     = {4692-4699},
  title     = {Learning when to quit: Meta-reasoning for motion planning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Grasp pose detection from a single RGB image. <em>IROS</em>,
4686–4691. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Grasp pose detection generates the position and orientation of the robot end-effector to grasp objects from the RGB or RGB-D image. In this paper, we propose a novel grasp pose detection network that generates 3-DOF grasp poses using the RGB image. The network follows the anchor-based object detection pipeline and incorporates the angle detection unit. Furthermore, we redesign the grasp angle predictor with a classification unit to increase the accuracy of grasp pose rotation estimation. Our method classifies the prediction angle densely in contrast with the previous regression method or sparse classification method. Moreover, an angle smooth label is designed to avoid the sudden change of the angle regression loss caused by the periodic property of the angle. We validate our algorithm on Cornell Grasp Dataset and obtain a higher detection accuracy than the state-of-the-art method. The real scenario experiment also proves the effectiveness of our method. The robot equipped with the parallel gripper achieves a 96.4\% grasp success rate.},
  archive   = {C_IROS},
  author    = {Hu Cheng and Yingying Wang and Max Q.-H. Meng},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636511},
  pages     = {4686-4691},
  title     = {Grasp pose detection from a single RGB image},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Occlusion-aware search for object retrieval in clutter.
<em>IROS</em>, 4678–4685. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the manipulation task of retrieving a target object from a cluttered shelf. When the target object is hidden, the robot must search through the clutter for retrieving it. Solving this task requires reasoning over the likely locations of the target object. It also requires physics reasoning over multi-object interactions and future occlusions. In this work, we present a data-driven hybrid planner for generating occlusion-aware actions in closed-loop. The hybrid planner explores likely locations of the occluded target object as predicted by a learned distribution from the observation stream. The search is guided by a heuristic trained with reinforcement learning to act on observations with occlusions. We evaluate our approach in different simulation and real-world settings (video available on https://youtu.be/dY7YQ3LUVQg). The results validate that our approach can search and retrieve a target object in near real time in the real world while only being trained in simulation.},
  archive   = {C_IROS},
  author    = {Wissam Bejjani and Wisdom C. Agboh and Mehmet R. Dogar and Matteo Leonetti},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636230},
  pages     = {4678-4685},
  title     = {Occlusion-aware search for object retrieval in clutter},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning a generative transition model for uncertainty-aware
robotic manipulation. <em>IROS</em>, 4670–4677. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot learning of real-world manipulation tasks remains challenging and time consuming, even though actions are often simplified by single-step manipulation primitives. In order to compensate the removed time dependency, we additionally learn an image-to-image transition model that is able to predict a next state including its uncertainty. We apply this approach to bin picking, the task of emptying a bin using grasping as well as pre-grasping manipulation as fast as possible. The transition model is trained with up to 42 000 pairs of real-world images before and after a manipulation action. Our approach enables two important skills: First, for applications with flange-mounted cameras, picks per hours (PPH) can be increased by around 15\% by skipping image measurements. Second, we use the model to plan action sequences ahead of time and optimize time-dependent rewards, e.g. to minimize the number of actions required to empty the bin. We evaluate both improvements with real-robot experiments and achieve over 700 PPH in the YCB Box and Blocks Test.},
  archive   = {C_IROS},
  author    = {Lars Berscheid and Pascal Meißner and Torsten Kröger},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636623},
  pages     = {4670-4677},
  title     = {Learning a generative transition model for uncertainty-aware robotic manipulation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural motion prediction for in-flight uneven object
catching. <em>IROS</em>, 4662–4669. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In-flight objects capture is extremely challenging. The robot is required to complete trajectory prediction, interception position calculation and motion planning within tens of milliseconds. As in-flight uneven objects are affected by various kinds of forces, which leads to the time-varying acceleration, motion prediction for them is difficult. In order to compensate the system’s non-linearity, we propose using a recurrent neural network model, which we call the Neural Acceleration Estimator (NAE), to estimate the varying acceleration by observing a small fragment of previous deflected trajectory without any prior information. Moreover, end-to-end training with Differantiable Filter (NAE-DF) gives a supervision for measurement uncertainty and further improves the prediction accuracy. Experimental results show that motion prediction with NAE and NAE-DF is superior to other methods and has a good generalization performance on unseen objects. We test our methods on a robot, performing velocity control in real world and respectively achieve 83.3\% and 86.7\% success rate on a ploy urethane banana and a gourd. We also release an object in-flight dataset containing 1,500 trajectorys for uneven objects, which can be found on the project website:https://sites.google.com/view/neural-motion-prediction.},
  archive   = {C_IROS},
  author    = {Hongxiang Yu and Dashun Guo and Huan Yin and Anzhe Chen and Kechun Xu and Zexi Chen and Minhang Wang and Qimeng Tan and Yue Wang and Rong Xiong},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635983},
  pages     = {4662-4669},
  title     = {Neural motion prediction for in-flight uneven object catching},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Double-dot network for antipodal grasp detection.
<em>IROS</em>, 4654–4661. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a new deep learning approach to antipodal grasp detection, named Double-Dot Network (DD-Net). It follows the recent anchor-free object detection framework, which does not depend on empirically pre-set anchors and thus allows more generalized and flexible prediction on unseen objects. Specifically, unlike the widely used 5-dimensional rectangle, the gripper configuration is defined as a pair of fingertips. An effective CNN architecture is introduced to localize such fingertips, and with the help of auxiliary centers for refinement, it accurately and robustly infers grasp candidates. Additionally, we design a specialized loss function to measure the quality of grasps, and in contrast to the IoU scores of bounding boxes adopted in object detection, it is more consistent to the grasp detection task. Both the simulation and robotic experiments are executed and state of the art accuracies are achieved, showing that DD-Net is superior to the counterparts in handling unseen objects.},
  archive   = {C_IROS},
  author    = {Yao Wang and Yangtao Zheng and Boyang Gao and Di Huang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636706},
  pages     = {4654-4661},
  title     = {Double-dot network for antipodal grasp detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to detect multi-modal grasps for dexterous grasping
in dense clutter. <em>IROS</em>, 4647–4653. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose an approach to multi-modal grasp detection that jointly predicts the probabilities that several types of grasps succeed at a given grasp pose. Given a partial point cloud of a scene, the algorithm proposes a set of feasible grasp candidates, then estimates the probabilities that a grasp of each type would succeed at each candidate pose. Predicting grasp success probabilities directly from point clouds makes our approach agnostic to the number and placement of depth sensors at execution time. We evaluate our system both in simulation and on a real robot with a Robotiq 3-Finger Adaptive Gripper and compare our network against several baselines that perform fewer types of grasps. Our experiments show that a system that explicitly models grasp type achieves an object retrieval rate 8.5\% higher in a complex cluttered environment than our highest-performing baseline.},
  archive   = {C_IROS},
  author    = {Matt Corsaro and Stefanie Tellex and George Konidaris},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636876},
  pages     = {4647-4653},
  title     = {Learning to detect multi-modal grasps for dexterous grasping in dense clutter},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Precise object placement with pose distance estimations for
different objects and grippers. <em>IROS</em>, 4639–4646. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a novel approach for the grasping and precise placement of various known rigid objects using multiple grippers within highly cluttered scenes. Using a single depth image of the scene, our method estimates multiple 6D object poses together with an object class, a pose distance for object pose estimation, and a pose distance from a target pose for object placement for each automatically obtained grasp pose with a single forward pass of a neural network.By incorporating model knowledge into the system, our approach has higher success rates for grasping than state-of-the-art model-free approaches. Furthermore, our method chooses grasps that result in significantly more precise object placements than prior model-based work.},
  archive   = {C_IROS},
  author    = {Kilian Kleeberger and Jonathan Schnitzler and Muhammad Usman Khalid and Richard Bormann and Werner Kraus and Marco F. Huber},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635926},
  pages     = {4639-4646},
  title     = {Precise object placement with pose distance estimations for different objects and grippers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). KDFNet: Learning keypoint distance field for 6D object pose
estimation. <em>IROS</em>, 4631–4638. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present KDFNet, a novel method for 6D object pose estimation from RGB images. To handle occlusion, many recent works have proposed to localize 2D keypoints through pixel-wise voting and solve a Perspective-n-Point (PnP) problem for pose estimation, which achieves leading performance. However, such voting process is direction-based and cannot handle long and thin objects where the direction intersections cannot be robustly found. To address this problem, we propose a novel continuous representation called Keypoint Distance Field (KDF) for projected 2D keypoint locations. Formulated as a 2D array, each element of the KDF stores the 2D Euclidean distance between the corresponding image pixel and a specified projected 2D keypoint. We use a fully convolutional neural network to regress the KDF for each keypoint. Using this KDF encoding of projected object keypoint locations, we propose to use a distance-based voting scheme to localize the keypoints by calculating circle intersections in a RANSAC fashion. We validate the design choices of our framework by extensive ablation experiments. Our proposed method achieves state-of-the-art performance on Occlusion LINEMOD dataset with an average ADD(-S) accuracy of 50.3\% and TOD dataset mug subset with an average ADD accuracy of 75.72\%. Extensive experiments and visualizations demonstrate that the proposed method is able to robustly estimate the 6D pose in challenging scenarios including occlusion.},
  archive   = {C_IROS},
  author    = {Xingyu Liu and Shun Iwase and Kris M. Kitani},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636489},
  pages     = {4631-4638},
  title     = {KDFNet: Learning keypoint distance field for 6D object pose estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Denoising 3D human poses from low-resolution video using
variational autoencoder. <em>IROS</em>, 4625–4630. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We tackle the problem of refining and denoising a series of 3D human poses estimated from a low-resolution video. Low-resolution often causes the wrong pose estimation, e.g., left-right switching and the absence of keypoints. We propose to use the variational autoencoder (VAE) to remove these challenging noises. The VAE model utilizes time-series information and motion priors in denoising. From our experiments, the VAE model can reduce the pose estimation error (MPJPE) for poor-quality images by 24.37mm, from the original 105.53mm. This improves about 6.5 times over the traditional DCT approach. In addition, it removes jitters and generates smooth movements, which is helpful in recognition of human behaviors.},
  archive   = {C_IROS},
  author    = {Chihiro Nakatsuka and Satoshi Komorita},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636144},
  pages     = {4625-4630},
  title     = {Denoising 3D human poses from low-resolution video using variational autoencoder},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pose estimation from RGB images of highly symmetric objects
using a novel multi-pose loss and differential rendering. <em>IROS</em>,
4618–4624. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel multi-pose loss function to train a neural network for 6D pose estimation, using synthetic data and evaluating it on real images. Our loss is inspired by the VSD (Visible Surface Discrepancy) metric and relies on a differentiable renderer and CAD models. This novel multi-pose approach produces multiple weighted pose estimates to avoid getting stuck in local minima. Our method resolves pose ambiguities without using predefined symmetries. It is trained only on synthetic data. We test on real-world RGB images from the T-LESS dataset, containing highly symmetric objects common in industrial settings. We show that our solution can be used to replace the codebook in a state-of-the-art approach. So far, the codebook approach has had the shortest inference time in the field. Our approach reduces inference time further while a) avoiding discretization, b) requiring a much smaller memory footprint and c) improving pose recall. 3},
  archive   = {C_IROS},
  author    = {Stefan Hein Bengtson and Hampus Åström and Thomas B. Moeslund and Elin A. Topp and Volker Krueger},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636839},
  pages     = {4618-4624},
  title     = {Pose estimation from RGB images of highly symmetric objects using a novel multi-pose loss and differential rendering},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Moving SLAM: Fully unsupervised deep learning in non-rigid
scenes. <em>IROS</em>, 4611–4617. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a new deep learning framework to decompose monocular videos into 3D geometry (camera pose and depth), moving objects, and their motions, with no supervision. We build upon the idea of view synthesis, which uses classical camera geometry to re-render a source image from a different point-of-view to obtain supervisory signals, specified by a predicted relative 6-degree-of-freedom pose and depth map. However, the typical view synthesis equations rely on a strong assumption: that objects in scenes do not move. This rigid-world assumption limits the predictive power, and rules out learning about objects automatically. We propose a simple solution: minimize the synthesis error on small local regions of the image instead. While the scene as a whole may be non-rigid, it is always possible to find small regions that are approximately rigid, such as inside a moving object. Our network can learn a dense pose map describing poses for each local region. This represents a significantly richer model, including 6D object motions, with little additional complexity. We establish very competitive results on unsupervised odometry and depth prediction on KITTI. We also demonstrate new capabilities on EPIC-Kitchens, a challenging dataset of indoor videos, where there is no ground truth information for depth, odometry, object segmentation or motion - yet all are recovered automatically by our approach.},
  archive   = {C_IROS},
  author    = {Dan Xu and Andrea Vedaldi and João F. Henriques},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636075},
  pages     = {4611-4617},
  title     = {Moving SLAM: Fully unsupervised deep learning in non-rigid scenes},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Using visual anomaly detection for task execution
monitoring. <em>IROS</em>, 4604–4610. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Execution monitoring is essential for robots to detect and respond to failures. Since it is impossible to enumerate all failures for a given task, we learn from successful executions of the task to detect visual anomalies during runtime. Our method learns to predict the motions that occur during the nominal execution of a task, including camera and robot body motion. A probabilistic U-Net architecture is used to learn to predict optical flow, and the robot’s kinematics and 3D model are used to model camera and body motion. The errors between the observed and predicted motion are used to calculate an anomaly score. We evaluate our method on a dataset of a robot placing a book on a shelf, which includes anomalies such as falling books, camera occlusions, and robot disturbances. We find that modeling camera and body motion, in addition to the learning-based optical flow prediction, results in an improvement of the area under the receiver operating characteristic curve from 0.752 to 0.804, and the area under the precision-recall curve from 0.467 to 0.549.},
  archive   = {C_IROS},
  author    = {Santosh Thoduka and Juergen Gall and Paul G. Plöger},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636133},
  pages     = {4604-4610},
  title     = {Using visual anomaly detection for task execution monitoring},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VIPose: Real-time visual-inertial 6D object pose tracking.
<em>IROS</em>, 4597–4603. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Estimating the 6D pose of objects is beneficial for robotics tasks such as transportation, autonomous navigation, manipulation as well as in scenarios beyond robotics like virtual and augmented reality. With respect to single image pose estimation, pose tracking takes into account the temporal information across multiple frames to overcome possible detection inconsistencies and to improve the pose estimation efficiency. In this work, we introduce a novel Deep Neural Network (DNN) called VIPose, that combines inertial and camera data to address the object pose tracking problem in real-time. The key contribution is the design of a novel DNN architecture which fuses visual and inertial features to predict the objects’ relative 6D pose between consecutive image frames. The overall 6D pose is then estimated by consecutively combining relative poses. Our approach shows remarkable pose estimation results for heavily occluded objects that are well known to be very challenging to handle by existing state-of-the-art solutions. The effectiveness of the proposed approach is validated on a new dataset called VIYCB with RGB image, IMU data, and accurate 6D pose annotations created by employing an automated labeling technique. The approach presents accuracy performances comparable to state-of-the-art techniques, but with the additional benefit of being real-time.},
  archive   = {C_IROS},
  author    = {Rundong Ge and Giuseppe Loianno},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636283},
  pages     = {4597-4603},
  title     = {VIPose: Real-time visual-inertial 6D object pose tracking},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). You only group once: Efficient point-cloud processing with
token representation and relation inference module. <em>IROS</em>,
4589–4596. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D perception on point-cloud is a challenging and crucial computer vision task. A point-cloud consists of a sparse, unstructured, and unordered set of points. To understand a point-cloud, previous point-based methods, such as PointNet++, extract visual features through the hierarchical aggregation of local features. However, such methods have several critical limitations: 1) They require considerable sampling and grouping operations, which leads to low inference speed. 2) Despite redundancy among adjacent points, they treat all points alike with an equal amount of computation. 3) They aggregate local features together through downsampling, which causes information loss and hurts perception capability. To overcome these challenges, we propose a novel, simple, and elegant deep learning model called YOGO (You Only Group Once). YOGO divides a point-cloud into a small number of parts and extracts a high-dimensional token to represent points within each sub-region. Next, we use self-attention to capture token-to-token relations, and project the token features back to the point features. We formulate such a series of operations as a relation inference module (RIM). Compared with previous methods, YOGO is very efficient because it only needs to sample and group a point-cloud once. Instead of operating on points, YOGO operates on a small number of tokens, each of which summarizes the point features in a sub-region. This allows us to avoid redundant computation and thus boosts efficiency. Moreover, YOGO preserves pointwise features by projecting token features to point features although the RIM computes on tokens. This avoids information loss and enhances point-wise perception capability. We conduct thorough experiments to demonstrate that YOGO achieves at least 3.0x speedup over point-based baselines while delivering competitive classification and segmentation performance on a classification dataset and a segmentation dataset based on 3D Wharehouse, and S3DIS datasets. The code is available at https://github.com/chenfengxu714/YOGO.git.},
  archive   = {C_IROS},
  author    = {Chenfeng Xu and Bohan Zhai and Bichen Wu and Tian Li and Wei Zhan and Peter Vajda and Kurt Keutzer and Masayoshi Tomizuka},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636858},
  pages     = {4589-4596},
  title     = {You only group once: Efficient point-cloud processing with token representation and relation inference module},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Centralizing state-values in dueling networks for
multi-robot reinforcement learning mapless navigation. <em>IROS</em>,
4583–4588. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of multi-robot mapless navigation in the popular Centralized Training and Decentralized Execution (CTDE) paradigm. This problem is challenging when each robot considers its path without explicitly sharing observations with other robots and can lead to non-stationary issues in Deep Reinforcement Learning (DRL). The typical CTDE algorithm factorizes the joint action-value function into individual ones, to favor cooperation and achieve decentralized execution. Such factorization involves constraints (e.g., monotonicity) that limit the emergence of novel behaviors in an individual as each agent is trained starting from a joint action-value. In contrast, we propose a novel architecture for CTDE that uses a centralized state-value network to compute a joint state-value, which is used to inject global state information in the value-based updates of the agents. Consequently, each model computes its gradient update for the weights, considering the overall state of the environment. Our idea follows the insights of Dueling Networks as a separate estimation of the joint state-value has both the advantage of improving sample efficiency, while providing each robot information whether the global state is (or is not) valuable. Experiments in a robotic navigation task with 2 4, and 8 robots, confirm the superior performance of our approach over prior CTDE methods (e.g., VDN, QMIX).},
  archive   = {C_IROS},
  author    = {Enrico Marchesini and Alessandro Farinelli},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636349},
  pages     = {4583-4588},
  title     = {Centralizing state-values in dueling networks for multi-robot reinforcement learning mapless navigation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Monolithic vs. Hybrid controller for multi-objective
sim-to-real learning. <em>IROS</em>, 4576–4582. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simulation to real (Sim-to-Real) is an attractive approach to construct controllers for robotic tasks that are easier to simulate than to analytically solve. Working Sim-to-Real solutions have been demonstrated for tasks with a clear single objective such as &quot;reach the target&quot;. Real world applications, however, often consist of multiple simultaneous objectives such as &quot;reach the target&quot; but &quot;avoid obstacles&quot;. A straightforward solution in the context of reinforcement learning (RL) is to combine multiple objectives into a multi-term reward function and train a single monolithic controller. Recently, a hybrid solution based on pre-trained single objective controllers and a switching rule between them was proposed. In this work, we compare these two approaches in the multi-objective setting of a robot manipulator to reach a target while avoiding an obstacle. Our findings show that the training of a hybrid controller is easier and obtains a better success-failure trade-off than a monolithic controller. The controllers trained in simulator were verified by a real set-up.},
  archive   = {C_IROS},
  author    = {Atakan Dag and Alexandre Angleraud and Wenyan Yang and Nataliya Strokina and Roel S. Pieters and Minna Lanz and Joni-Kristian Kämäräinen},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636426},
  pages     = {4576-4582},
  title     = {Monolithic vs. hybrid controller for multi-objective sim-to-real learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Meta-learning for fast adaptive locomotion with
uncertainties in environments and robot dynamics. <em>IROS</em>,
4568–4575. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work developed meta-learning control policies to achieve fast online adaptation to different changing conditions, which generate diverse and robust locomotion. The proposed method updates the interaction model constantly, samples feasible sequences of actions of estimated state-action trajectories, and then applies the optimal actions to maximize the reward. To achieve online model adaptation, our proposed method learns different latent vectors of each training condition, which is selected online based on newly collected data from the past 10 samples within 0.2s. Our work designs appropriate state space and reward functions, and optimizes feasible actions in an MPC fashion which are sampled directly in the joint space with constraints, hence requiring no prior design or training of specific gaits. We further demonstrated the robot’s capability of detecting unexpected changes during the interaction and adapting the control policy in less than 0.2s. The extensive validation on the SpotMicro robot in a physics simulation shows adaptive and robust locomotion skills under changing ground friction, external pushes, and different robot dynamics including motor failures and the whole leg amputation.},
  archive   = {C_IROS},
  author    = {Timothée Anne and Jack Wilkinson and Zhibin Li},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635840},
  pages     = {4568-4575},
  title     = {Meta-learning for fast adaptive locomotion with uncertainties in environments and robot dynamics},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning human rewards by inferring their latent
intelligence levels in multi-agent games: A theory-of-mind approach with
application to driving data. <em>IROS</em>, 4560–4567. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reward function, as an incentive representation that recognizes humans’ agency and rationalizes humans’ actions, is particularly appealing for modeling human behavior in human-robot interaction. Inverse Reinforcement Learning is an effective way to retrieve reward functions from demonstrations. However, it has always been challenging when applying it to multi-agent settings since the mutual influence between agents has to be appropriately modeled. To tackle this challenge, previous work either exploits equilibrium solution concepts by assuming humans as perfectly rational optimizers with unbounded intelligence or pre-assigns humans’ interaction strategies a priori. In this work, we advocate that humans are bounded rational and have different intelligence levels when reasoning about others’ decision-making process, and such an inherent and latent characteristic should be accounted for in reward learning algorithms. Hence, we exploit such insights from Theory-of-Mind and propose a new multi-agent Inverse Reinforcement Learning framework that reasons about humans’ latent intelligence levels during learning. We validate our approach in both zero-sum and general-sum games with synthetic agents, and illustrate a practical application to learning human drivers’ reward functions from real driving data. We compare our approach with two baseline algorithms. The results show that by reasoning about humans’ latent intelligence levels, the proposed approach has more flexibility and capability to retrieve reward functions that explain humans’ driving behaviors better.},
  archive   = {C_IROS},
  author    = {Ran Tian and Masayoshi Tomizuka and Liting Sun},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636653},
  pages     = {4560-4567},
  title     = {Learning human rewards by inferring their latent intelligence levels in multi-agent games: A theory-of-mind approach with application to driving data},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Model-based constrained reinforcement learning using
generalized control barrier function. <em>IROS</em>, 4552–4559. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Model information can be used to predict future trajectories, so it has huge potential to avoid dangerous regions when applying reinforcement learning (RL) on real-world tasks, like autonomous driving. However, existing studies mostly use model-free constrained RL, which causes inevitable constraint violations. This paper proposes a model-based feasibility enhancement technique of constrained RL, which enhances the feasibility of policy using generalized control barrier function (GCBF) defined on the distance to constraint boundary. By using the model information, the policy can be optimized safely without violating actual safety constraints, and the sample efficiency is increased. The infeasibility in solving the constrained policy gradient is handled by an adaptive coefficient mechanism. We evaluate the proposed method in both simulations and real vehicle experiments in a complex autonomous driving collision avoidance task. The proposed method achieves up to four times fewer constraint violations and converges 3.36 times faster than baseline constrained RL approaches.},
  archive   = {C_IROS},
  author    = {Haitong Ma and Jianyu Chen and Shengbo Eben and Ziyu Lin and Yang Guan and Yangang Ren and Sifa Zheng},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636468},
  pages     = {4552-4559},
  title     = {Model-based constrained reinforcement learning using generalized control barrier function},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical terrain-aware control for quadrupedal
locomotion by combining deep reinforcement learning and optimal control.
<em>IROS</em>, 4546–4551. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Quadruped robots possess advantages on different terrains over other types of mobile robots by virtue of their flexible choices of foothold points. It is crucial to integrate terrain perception with motion planning to exploit the potential of quadruped robots. We propose a novel hierarchical terrain-aware control (HTC) framework, which leverages deep reinforcement learning (DRL) for the high-level planner and optimal control for the low-level controller. In general, traditional control methods yield better stability by using an optimization algorithm. In addition, DRL is able to offer more adaptive behavior. Our approach makes full use of the advantages of these two methods and possesses better adaptability and stability in challenging natural environments. Furthermore, the global height map of the terrain serves as visual information for the DRL, which determines the desired footholds for the robot’s leg swings and body postures. Optimal control calculates the torque of the joints on the standing legs to maintain body balance. Our method is tested on various terrains both simulated and real environments. The experimental results show that HTC can effectively enhance the adaptability of the quadruped robot by coordinating body posture.},
  archive   = {C_IROS},
  author    = {Qingfeng Yao and Jilong Wang and Donglin Wang and Shuyu Yang and Hongyin Zhang and Yinuo Wang and Zhengqing Wu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636738},
  pages     = {4546-4551},
  title     = {Hierarchical terrain-aware control for quadrupedal locomotion by combining deep reinforcement learning and optimal control},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Terrain-aware risk-assessment-network-aided deep
reinforcement learning for quadrupedal locomotion in tough terrain.
<em>IROS</em>, 4538–4545. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When it comes to the control system of quadruped robots, deep reinforcement learning (DRL) is considered to be a promising solution. Despite years of development in this field, difficulties remain in guaranteeing the action stability of DRL-based quadruped robots’ locomotion, especially in tough terrain. In this paper, a terrain-aware teacher-student controller integrating a risk assessment network (RAN) is proposed to alleviate this problem. During the training phase, the RAN can evaluate the risk level of historical observation or current state and further guide the update of the policy, thereby assisting the policy in selecting better actions and avoid risky ones. Furthermore, the real-time elevation map is transmitted to the controller as visual information, so that it can perceive the terrain to produce higher performance locomotion. With the aforementioned configuration, we enable a robot to traverse various challenging terrain in simulation and bound or trot stably in the real environment.},
  archive   = {C_IROS},
  author    = {Hongyin Zhang and Jilong Wang and Zhengqing Wu and Yinuo Wang and Donglin Wang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636519},
  pages     = {4538-4545},
  title     = {Terrain-aware risk-assessment-network-aided deep reinforcement learning for quadrupedal locomotion in tough terrain},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reinforcement learning based negotiation-aware motion
planning of autonomous vehicles. <em>IROS</em>, 4532–4537. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For autonomous vehicles integrating onto road-ways with human traffic participants, it requires understanding and adapting to the participants’ intention by responding in predictable ways. This paper proposes a reinforcement learning based negotiation-aware motion planning framework, which adopts RL to adjust the driving style of the planner by dynamically modifying the prediction horizon length of the motion planner in real time adaptively. The framework models the interaction between the autonomous vehicle and other traffic participants as a Markov Decision Process. A temporal sequence of occupancy grid maps are taken as inputs for RL module to embed an implicit intention reasoning. Curriculum learning is employed to enhance the training efficiency and the robustness of the algorithm. We applied our method to narrow lane navigation in both simulation and real world to demonstrate that the proposed method outperforms the common alternative due to its advantage in alleviating the social dilemma problem with proper negotiation skills.},
  archive   = {C_IROS},
  author    = {Zhitao Wang and Yuzheng Zhuang and Qiang Gu and Dong Chen and Hongbo Zhang and Wulong Liu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635935},
  pages     = {4532-4537},
  title     = {Reinforcement learning based negotiation-aware motion planning of autonomous vehicles},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Shape estimation of negative obstacles for autonomous
navigation. <em>IROS</em>, 4525–4531. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Obstacle detection and avoidance plays a crucial role in autonomous navigation of unmanned ground vehicles. This becomes more challenging in off-road environments due to the higher probability of finding negative obstacles (e.g., holes, ditches, trenches, etc.) compared with on-road environments. One approach to solve this problem is to avoid the candidate path with a negative obstacle, but in off-road avoiding negative obstacles all the time is not possible. In such cases, the path planner may need to choose a candidate path with a negative obstacle that causes the least amount of damage to the vehicle. To deal better with these types of scenarios, this study introduces a novel approach to perform shape estimation of negative obstacles using LiDAR 3D point cloud data. The dimensions (width, diameter, and depth) and the location (center) of negative obstacles are calculated based on estimated shape. This approach is tested on different terrain types using the Mississippi Autonomous Vehicle Simulation (MAVS).},
  archive   = {C_IROS},
  author    = {Viswadeep Lebakula and Bo Tang and Christopher Goodin and Cindy L. Bethel},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636250},
  pages     = {4525-4531},
  title     = {Shape estimation of negative obstacles for autonomous navigation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cooperative autonomous vehicles that sympathize with human
drivers. <em>IROS</em>, 4517–4524. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Widespread adoption of autonomous vehicles will not become a reality until solutions are developed that enable these intelligent agents to co-exist with humans. This includes safely and efficiently interacting with human-driven vehicles, especially in both conflictive and competitive scenarios. We build up on the prior work on socially-aware navigation and borrow the concept of social value orientation from psychology —that formalizes how much importance a person allocates to the welfare of others— in order to induce altruistic behavior in autonomous driving. In contrast with existing works that explicitly model the behavior of human drivers and rely on their expected response to create opportunities for cooperation, our Sympathetic Cooperative Driving (SymCoDrive) paradigm trains altruistic agents that realize safe and smooth traffic flow in competitive driving scenarios only from experiential learning and without any explicit coordination. We demonstrate a significant improvement in both safety and traffic-level metrics as a result of this altruistic behavior and importantly conclude that the level of altruism in agents requires proper tuning as agents that are too altruistic also lead to sub-optimal traffic flow. The code and supplementary material are available at: https://symcodrive.toghi.net/},
  archive   = {C_IROS},
  author    = {Behrad Toghi and Rodolfo Valiente and Dorsa Sadigh and Ramtin Pedarsani and Yaser P. Fallah},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636151},
  pages     = {4517-4524},
  title     = {Cooperative autonomous vehicles that sympathize with human drivers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning-based 3D occupancy prediction for autonomous
navigation in occluded environments. <em>IROS</em>, 4509–4516. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In autonomous navigation, sensors suffer from massive occlusion in cluttered environments, leaving a significant amount of space unknown. In practice, treating the unknown space in optimistic or pessimistic ways both set limitations on planning performance. Therefore, aggressiveness and safety cannot be satisfied at the same time. Mimicking human behavior, in this paper, we propose a method based on deep neural network to predict occupancy distribution of unknown space. Specifically, the proposed method utilizes contextual information of environments and prior knowledge to predict obstacle distributions in the occluded space. Our self-supervised learning method use unlabeled and no-ground-truth data and augments the data by simulating navigation trajectories. Our Occupancy Prediction Network is faster than current SOTA scene completion models and is successfully applied to unseen test environments without any refinement. Results show that our predictor leverages the performance of a kinodynamic planner by improving security with no reduction of speed in cluttered environments.},
  archive   = {C_IROS},
  author    = {Lizi Wang and Hongkai Ye and Qianhao Wang and Yuman Gao and Chao Xu and Fei Gao},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636333},
  pages     = {4509-4516},
  title     = {Learning-based 3D occupancy prediction for autonomous navigation in occluded environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Autonomous mobile robot navigation independent of road
boundary using driving recommendation map. <em>IROS</em>, 4501–4508. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Numerous autonomous navigation systems have been proposed, so far, for use in walking environments. Of these, systems that do not rely on high-definition maps and precise localization are cheaper to maintain and easier to implement in unknown outdoor environments. In these systems, road-following navigation using road boundaries is commonly used. In outdoor environments, however, the road boundary is indistinct in some cases. Therefore, it is necessary to infer an area recommended by traffic rules and navigate based on this inference. This paper proposes an autonomous navigation system that is independent of the road boundaries by using a driving recommendation map. In this system, the driving recommendation degree is inferred by deep learning-based semantic segmentation using automatically labeled training data, and a map that probabilistically represents the driving recommendation degree is generated using this information. The proposed autonomous navigation system has demonstrated its usefulness in an environment with indistinct road boundaries. The demonstration shows the proposed autonomous navigation system can plan a path based on driving recommendation degree and properly drive in a scene where the road boundary is indistinct.},
  archive   = {C_IROS},
  author    = {Yuya Onozuka and Ryosuke Matsumi and Motoki Shino},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636635},
  pages     = {4501-4508},
  title     = {Autonomous mobile robot navigation independent of road boundary using driving recommendation map},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). KB-tree: Learnable and continuous monte-carlo tree search
for autonomous driving planning. <em>IROS</em>, 4493–4500. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a novel learnable and continuous Monte-Carlo Tree Search method, named as KB-Tree, for motion planning in autonomous driving. The proposed method utilizes an asymptotical PUCB based on Kernel Regression (KR-AUCB) as a novel UCB variant, to improve the exploitation and exploration performance. In addition, we further optimize the sampling in continuous space by adapting Bayesian Optimization (BO) in the selection process of MCTS. Moreover, we use a customized Graph Neural Network (GNN) as our feature extractor to improve the learning performance. To the best of our knowledge, we are the first to apply the continuous MCTS method in autonomous driving. To validate our method, we conduct extensive experiments under several weakly and strongly interactive scenarios. The results show that our proposed method performs well in all tasks, and outperforms the learning-based continuous MCTS method and the state-of-the-art Reinforcement Learning (RL) baseline.},
  archive   = {C_IROS},
  author    = {Lanxin Lei and Ruiming Luo and Renjie Zheng and Jingke Wang and JianWei Zhang and Cong Qiu and Liulong Ma and Liyang Jin and Ping Zhang and Junbo Chen},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636442},
  pages     = {4493-4500},
  title     = {KB-tree: Learnable and continuous monte-carlo tree search for autonomous driving planning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluation of long-term LiDAR place recognition.
<em>IROS</em>, 4487–4492. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We compare a state-of-the-art deep image retrieval and a deep place recognition method for place recognition using LiDAR data. Place recognition aims to detect previously visited locations and thus provides an important tool for navigation, mapping, and localisation. Experimental comparisons are conducted using challenging outdoor and indoor datasets, Oxford Radar RobotCar and COLD, in the &quot;long-term&quot; setting where the test conditions differ substantially from the training and gallery data. Based on our results the image retrieval methods using LiDAR depth images can achieve accurate localization (the single best match recall 80\%) within 5.00 m in urban outdoors. In office indoors the comparable accuracy is 50 cm but is more sensitive to changes in the environment.},
  archive   = {C_IROS},
  author    = {Jukka Peltomäki and Farid Alijani and Jussi Puura and Heikki Huttunen and Esa Rahtu and Joni-Kristian Kämäräinen},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636320},
  pages     = {4487-4492},
  title     = {Evaluation of long-term LiDAR place recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vessel classification using a regression neural network
approach. <em>IROS</em>, 4480–4486. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Marine vessels are subject to high wear and tear due to the conditions they operate in. To reduce risk of failure during operation, vessels are inspected periodically every five years. These inspections are prone to high subjectiveness that makes them hard to reproduce for the shipping owners. The purpose of this paper is to present a regressor to a Faster R-CNN network that can help alleviate some of the subjective assessment currently performed by human surveyors by estimating the severity of a corroded area, autonomously using drones. A feature pyramid backbone is shared between the Faster R-CNN and the added regression head. The goal of the regressor is to introduce a more objective assessment of the vessel that gives a consistent output for a consistent input. The system is evaluated on a real dataset, acquired in ballast tanks and the experimental results indicate that our deep learning approach can be used to detect and quantify corroded areas during the inspection process of marine vessels.},
  archive   = {C_IROS},
  author    = {Rasmus Eckholdt Andersen and Lazaros Nalpantidis and Evangelos Boukas},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636161},
  pages     = {4480-4486},
  title     = {Vessel classification using a regression neural network approach},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ODIP: Towards automatic adaptation for object detection by
interactive perception. <em>IROS</em>, 4474–4479. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Object detection plays a deep role in visual systems by identifying instances for downstream algorithms. In industrial scenarios, however, a slight change in manufacturing systems would lead to costly data re-collection and human annotation processes to re-train models. Existing solutions such as semi-supervised and few-shot methods either rely on numerous human annotations or suffer low performance. In this work, we explore a novel object detector based on interactive perception (ODIP), which can be adapted to novel domains in an automated manner. By interacting with a grasping system, ODIP accumulates visual observations of novel objects, learning to identify previously unseen instances without humanannotated data. Extensive experiments show ODIP outperforms both the generic object detector and state-of-the-art few-shot object detector fine-tuned in traditional manners. A demo video is provided to further illustrate the idea [1].},
  archive   = {C_IROS},
  author    = {Tung-I Chen and Jen-Wei Wang and Winston H. Hsu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635829},
  pages     = {4474-4479},
  title     = {ODIP: Towards automatic adaptation for object detection by interactive perception},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FEANet: Feature-enhanced attention network for RGB-thermal
real-time semantic segmentation. <em>IROS</em>, 4467–4473. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The RGB-Thermal (RGB-T) information for semantic segmentation has been extensively explored in recent years. However, most existing RGB-T semantic segmentation usually compromises spatial resolution to achieve real-time inference speed, which leads to poor performance. To better extract detail spatial information, we propose a two-stage Feature-Enhanced Attention Network (FEANet) for the RGB-T semantic segmentation task. Specifically, we introduce a Feature-Enhanced Attention Module (FEAM) to excavate and enhance multi-level features from both the channel and spatial views. Benefited from the proposed FEAM module, our FEANet can preserve the spatial information and shift more attention to high-resolution features from the fused RGB-T images. Extensive experiments on the urban scene dataset demonstrate that our FEANet outperforms other state-of-the-art (SOTA) RGB-T methods in terms of objective metrics and subjective visual comparison (+2.6\% in global mAcc and +0.8\% in global mIoU). For the 480 × 640 RGB-T test images, our FEANet can run with a real-time speed on an NVIDIA GeForce RTX 2080 Ti card.},
  archive   = {C_IROS},
  author    = {Fuqin Deng and Hua Feng and Mingjian Liang and Hongmin Wang and Yong Yang and Yuan Gao and Junfeng Chen and Junjie Hu and Xiyue Guo and Tin Lun Lam},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636084},
  pages     = {4467-4473},
  title     = {FEANet: Feature-enhanced attention network for RGB-thermal real-time semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Look before you act: Boosting pseudo-LiDAR with online
semantic embedding. <em>IROS</em>, 4459–4466. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vision-based 3D object detection is a research focus in the field of autonomous driving system. While recently proposed pseudo-LiDAR is a promising solution, its performance is severely restricted by the image-based depth estimator, leading to a considerable performance gap against the LiDAR-based counterparts. In this paper, substantial advances are developed along an orthogonal direction to the previous efforts in the pseudo-LiDAR pipeline. Concretely, we propose a plug- and-play module, called Online Semantic Embedding (OSE), aligning image semantics with the pseudo-LiDAR detection in an end-to-end manner. On the KITTI object detection benchmark, existing stereo-based baselines integrated with our approach show impressive improvements without bells and whistles. Furthermore, we emphasize that OSE works in retrieving the performance under geometric imperfection conditions.},
  archive   = {C_IROS},
  author    = {Liangjun Zhang and Tao Song and Tao Jiang and Di Xie and Shiliang Pu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636529},
  pages     = {4459-4466},
  title     = {Look before you act: Boosting pseudo-LiDAR with online semantic embedding},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FIDNet: LiDAR point cloud semantic segmentation with fully
interpolation decoding. <em>IROS</em>, 4453–4458. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Projecting the point cloud on the 2D spherical range image transforms the LiDAR semantic segmentation to a 2D segmentation task on the range image. However, the LiDAR range image is still naturally different from the regular 2D RGB image; for example, each position on the range image encodes the unique geometry information. In this paper, we propose a new projection-based LiDAR semantic segmentation pipeline that consists of a novel network structure and an efficient post-processing step. In our network structure, we design a FID (fully interpolation decoding) module that directly upsamples the multi-resolution feature maps using bilinear interpolation. Inspired by the 3D distance interpolation used in PointNet++, we argue this FID module is a 2D version distance interpolation on (θ, ϕ) space. As a parameter-free decoding module, the FID largely reduces the model complexity by maintaining good performance. Besides the network structure, we empirically find that our model predictions have clear boundaries between different semantic classes. This makes us rethink whether the widely used K-nearest-neighbor post-processing is still necessary for our pipeline. Then, we realize the many-to-one mapping causes the blurring effect that some points are mapped into the same pixel and share the same label. Therefore, we propose to process those occluded points by assigning the nearest predicted label to them. This NLA (nearest label assignment) post-processing step shows a better performance than KNN with faster inference speed in the ablation study. On SemanticKITTI dataset, our pipeline achieves the best performance among all projection-based methods with 64×2048 resolution and all point-wise solutions. With a ResNet-34 as the backbone, both the training and testing of our model can be finished on a single RTX 2080 Ti with 11G memory. The code is released here. 1},
  archive   = {C_IROS},
  author    = {Yiming Zhao and Lin Bai and Xinming Huang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636385},
  pages     = {4453-4458},
  title     = {FIDNet: LiDAR point cloud semantic segmentation with fully interpolation decoding},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Event-based motion segmentation by cascaded two-level
multi-model fitting. <em>IROS</em>, 4445–4452. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Among prerequisites for a synthetic agent to inter-act with dynamic scenes, the ability to identify independently moving objects is specifically important. From an application perspective, nevertheless, standard cameras may deteriorate remarkably under aggressive motion and challenging illumination conditions. In contrast, event-based cameras, as a category of novel biologically inspired sensors, deliver advantages to deal with these challenges. Its rapid response and asynchronous nature enables it to capture visual stimuli at exactly the same rate of the scene dynamics. In this paper, we present a cascaded two-level multi-model fitting method for identifying independently moving objects (i.e., the motion segmentation problem) with a monocular event camera. The first level leverages tracking of event features and solves the feature clustering problem under a progressive multi-model fitting scheme. Initialized with the resulting motion model instances, the second level further addresses the event clustering problem using a spatio-temporal graph-cut method. This combination leads to efficient and accurate event-wise motion segmentation that cannot be achieved by any of them alone. Experiments demonstrate the effectiveness and versatility of our method in real-world scenes with different motion patterns and an unknown number of independently moving objects.},
  archive   = {C_IROS},
  author    = {Xiuyuan Lu and Yi Zhou and Shaojie Shen},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636307},
  pages     = {4445-4452},
  title     = {Event-based motion segmentation by cascaded two-level multi-model fitting},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Active perception for ambiguous objects classification.
<em>IROS</em>, 4437–4444. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent visual pose estimation and tracking solutions provide notable results on popular datasets such as T-LESS and YCB. However, in the real world, we can find ambiguous objects that do not allow exact classification and detection from a single view. In this work, we propose a framework that, given a single view of an object, provides the coordinates of a next viewpoint to discriminate the object against similar ones, if any, and eliminates ambiguities. We also describe a complete pipeline from a real object’s scans to the viewpoint selection and classification. We validate our approach with a Franka Emika Panda robot and common household objects featured with ambiguities. We released the source code to reproduce our experiments.},
  archive   = {C_IROS},
  author    = {Evgenii Safronov and Nicola Piga and Michele Colledanchise and Lorenzo Natale},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636414},
  pages     = {4437-4444},
  title     = {Active perception for ambiguous objects classification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). What’s in my LiDAR odometry toolbox? <em>IROS</em>,
4429–4436. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the democratization of 3D LiDAR sensors, precise LiDAR odometries and SLAM are in high demand. New methods regularly appear, proposing solutions ranging from small variations in classical algorithms to radically new paradigms based on deep learning. Yet it is often difficult to compare these methods, notably due to the few datasets on which the methods can be evaluated and compared. Furthermore, their weaknesses are rarely examined, often letting the user discover the hard way whether a method would be appropriate for a use case.In this paper, we review and organize the main 3D LiDAR odometries into distinct categories. We implemented several approaches (geometric based, deep learning based, and hybrid methods) to conduct an in-depth analysis of their strengths and weaknesses on multiple datasets, guiding the reader through the different LiDAR odometries available. Implementation of the methods has been made publicly available at: https://github.com/Kitware/pyLiDAR-SLAM.},
  archive   = {C_IROS},
  author    = {Pierre Dellenbach and Jean-Emmanuel Deschaud and Bastien Jacquet and François Goulette},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636348},
  pages     = {4429-4436},
  title     = {What’s in my LiDAR odometry toolbox?},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LiDAR-based object-level SLAM for autonomous vehicles.
<em>IROS</em>, 4397–4404. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simultaneous localization and mapping (SLAM) is an essential technique for autonomous driving. Recently, combining image recognition technology to generate semantically meaningful maps has become a new trend in visual SLAM research. However, in the field of LiDAR SLAM, this potential has not been fully explored. We propose a novel object-level SLAM system using 3D LiDARs for autonomous vehicles. We detect and track poles, walls, and parked cars, which are common along urban roads. This paper presents how we process the measurement data of three different shapes of objects to build a graph-based optimization system and facilitate the geometric distribution of poles to detect loops. Experiments were carried out on datasets collected with a test vehicle in city traffic. The results show that our object-level SLAM system can build precise and semantically meaningful maps and produce more accurate pose estimations compared to the state-of-the-art systems on our datasets.},
  archive   = {C_IROS},
  author    = {Bingyi Cao and Ricardo Carrillo Mendoza and Andreas Philipp and Daniel Göhring},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636299},
  pages     = {4397-4404},
  title     = {LiDAR-based object-level SLAM for autonomous vehicles},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). F-LOAM: Fast LiDAR odometry and mapping. <em>IROS</em>,
4390–4396. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simultaneous Localization and Mapping (SLAM) has wide robotic applications such as autonomous driving and unmanned aerial vehicles. Both computational efficiency and localization accuracy are of great importance towards a good SLAM system. Existing works on LiDAR based SLAM often formulate the problem as two modules: scan-to-scan match and scan-to-map refinement. Both modules are solved by iterative calculation which are computationally expensive. In this paper, we propose a general solution that aims to provide a computationally efficient and accurate framework for LiDAR based SLAM. Specifically, we adopt a non-iterative two-stage distortion compensation method to reduce the computational cost. For each scan input, the edge and planar features are extracted and matched to a local edge map and a local plane map separately, where the local smoothness is also considered for iterative pose optimization. Thorough experiments are performed to evaluate its performance in challenging scenarios, including localization for a warehouse Automated Guided Vehicle (AGV) and a public dataset on autonomous driving. The proposed method achieves a competitive localization accuracy with a processing rate of more than 10 Hz in the public dataset evaluation, which provides a good trade-off between performance and computational cost for practical applications. It is one of the most accurate and fastest open-sourced SLAM systems 1 in KITTI dataset ranking.},
  archive   = {C_IROS},
  author    = {Han Wang and Chen Wang and Chun-Lin Chen and Lihua Xie},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636655},
  pages     = {4390-4396},
  title     = {F-LOAM: Fast LiDAR odometry and mapping},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual place recognition using LiDAR intensity information.
<em>IROS</em>, 4382–4389. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots and autonomous systems need to know where they are within a map to navigate effectively. Thus, simultaneous localization and mapping or SLAM is a common building block of robot navigation systems. When building a map via a SLAM system, robots need to re-recognize places to find loop closure and reduce the odometry drift. Image-based place recognition received a lot of attention in computer vision, and in this work, we investigate how such approaches can be used for 3D LiDAR data. Recent LiDAR sensors produce high-resolution 3D scans in combination with comparably stable intensity measurements. Through a cylindrical projection, we can turn this information into a 360° panoramic range image. As a result, we can apply techniques from visual place recognition to LiDAR intensity data. The question of how well this approach works in practice has only partially been investigated. This paper provides an analysis of how such visual techniques can be with LiDAR data, and we provide an evaluation on different datasets. Our results suggest that this form of place recognition is possible and an effective means for determining loop closures.},
  archive   = {C_IROS},
  author    = {Luca Di Giammarino and Irvin Aloise and Cyrill Stachniss and Giorgio Grisetti},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636649},
  pages     = {4382-4389},
  title     = {Visual place recognition using LiDAR intensity information},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Online kinematic and dynamic parameter estimation for
autonomous surface and underwater vehicles. <em>IROS</em>, 4374–4381.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One of the main challenges in underwater robot localization is the scarcity of external positioning references. Therefore, accurate inertial localization in between external position updates is crucial for applications such as underwater environmental sampling. In this paper, we present a framework for estimating kinematic and dynamic model parameters used for inertial navigation. Accurate values of these parameters result in better trajectory estimation. Our approach can run online as well as offline, with either choice providing different advantages. Further, our framework can correct errors in the past trajectory at each estimation step. By doing so, we are able to provide improved geo-references for past as well as future spatial measurements made by the robots. This has an impact on adaptive sampling methods, which use geo-tagged measurements for building local spatial distributions and choose future sampling points. We present results from field experiments and demonstrate improvement in trajectory estimation accuracy. We also experimentally show that with optimal parameter estimates, robots can tolerate longer intervals in external positioning updates for a specified acceptable level of estimation error.},
  archive   = {C_IROS},
  author    = {Anwar Quraishi and Alcherio Martinoli},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636659},
  pages     = {4374-4381},
  title     = {Online kinematic and dynamic parameter estimation for autonomous surface and underwater vehicles},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3D ensemble-based online oceanic flow field estimation for
underwater glider path planning. <em>IROS</em>, 4358–4365. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Estimating ocean flow fields in 3D is a critical step in enabling the reliable operation of underwater gliders and other small, low-powered autonomous marine vehicles. Existing methods produce depth-averaged 2D layers arranged at discrete vertical intervals, but this type of estimation can lead to severe navigation errors. Based on the observation that real-world ocean currents exhibit relatively low vertical velocity components, we propose an accurate 3D estimator that extends our previous work in estimating 2D flow fields as a linear combination of basis flows. The proposed algorithm uses data from ensemble forecasting to build a set of 3D basis flows, and then iteratively updates basis coefficients using point measurements of underwater currents. We report results from experiments using actual ensemble forecasts and synthetic measurements to compare the performance of our method to the direct 3D extension of the previous work. These results show that our method produces estimates with dramatically lower error metrics, with and without measurement noise.},
  archive   = {C_IROS},
  author    = {Felix H. Kong and K. Y. Cadmus To and Gary Brassington and Stuart Anstee and Robert Fitch},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636692},
  pages     = {4358-4365},
  title     = {3D ensemble-based online oceanic flow field estimation for underwater glider path planning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Shipborne sea-ice field mapping using a LiDAR.
<em>IROS</em>, 4350–4357. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The increasing interest for autonomous ships has motivated research in numerous areas. One such area is the safe navigation through ice infested waters, for which a sensor instrumentation and automated process are proposed for near-field, sea-ice 3D scanning and mapping using a ship mounted LiDAR, with attitude compensation from inertial and satellite positioning sensors. Data were collected both at the Aalto Ice Tank laboratory and on board the icebreaker S.A. Agulhas II during its voyage to the Antarctic waters. The implemented process enables automated acquisition of detailed 3D point cloud maps, containing highly valuable information for icy waters going ships currently operated by a human crew and, in the near future, supporting the development of autonomous ships. Compared to other methods using satellite, aerial or underwater data, the proposed method is a more cost-effective and easy to integrate solution into current and future icy waters going ships, thus enabling a higher level of situational awareness.},
  archive   = {C_IROS},
  author    = {Andrei Sandru and Arto Visala and Pentti Kujala},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636275},
  pages     = {4350-4357},
  title     = {Shipborne sea-ice field mapping using a LiDAR},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Coordinated path planning for surface acoustic beacons for
supporting underwater localization. <em>IROS</em>, 4343–4349. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate localization is one of the biggest challenges in underwater robotics. The primary reasons behind that are unavailability of satellite-based positioning below the surface, and lack of clear features in natural water bodies for visually aided localization. As such, the common method of choice for external position referencing in underwater robots is the use of acoustic signals for computing range or direction of arrival. To that end, we have developed an acoustic range based navigation system with floating, movable beacons. In this paper, we present an approach for planning the trajectory of acoustic beacons in a way that they provide the best possible navigation support for a group of underwater vehicles. We use an information theoretic approach to beacon path planning that minimizes the group’s position uncertainty. We evaluate our approach with realistic simulations calibrated using real-world data, and present results.},
  archive   = {C_IROS},
  author    = {Anwar Quraishi and Alcherio Martinoli},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636703},
  pages     = {4343-4349},
  title     = {Coordinated path planning for surface acoustic beacons for supporting underwater localization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cooperative ASV/AUV system exploiting active acoustic
localization. <em>IROS</em>, 4337–4342. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The lack of GPS signal in the underwater environment poses limitations in terms of localization and navigation of mobile robots. Strategies based on acoustic localization systems are employed to improve underwater navigation. In this paper we describe a first step towards the development of a marine system of systems involving autonomous mobile nodes. The approach relies on communication networking between an Autonomous Surface Vehicle (ASV), equipped with an Ultra Short BaseLine (USBL) device, and an Autonomous Underwater Vehicle (AUV). An active acoustic communication protocol prioritizes the AUV positioning rate, whereas a model-based navigation filter handles the delayed measurements caused by the acoustic communication latency. The system has been tested in a real marine environment to analyze its behavior and the quality of the navigation estimation. The experimental results show that the navigation algorithm on-board the AUV provides an estimate of its position with an error of a few meters with respect to the GPS ground-truth, over a total path of approximately 210m, exploiting acoustic positioning data provided by the ASV in order to limit drift problems.},
  archive   = {C_IROS},
  author    = {Matteo Bresciani and Giovanni Peralta and Francesco Ruscio and Lorenzo Bazzarello and Andrea Caiti and Riccardo Costanzi},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636326},
  pages     = {4337-4342},
  title     = {Cooperative ASV/AUV system exploiting active acoustic localization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An open-source, fiducial-based, underwater stereo
visual-inertial localization method with refraction correction.
<em>IROS</em>, 4331–4336. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Underwater visual localization is an essential technique for the autonomous operation of underwater robots. However, the unique underwater image characteristics, including refraction, sparse features, and severe noise, pose an enormous challenge to it. For addressing these issues, this paper proposes an open-source fiducial-based underwater stereo visual-inertial localization method under the extended Kalman filter (EKF) framework, which is called FBUS-EKF. First, the refraction is corrected by the refractive camera model and akin triangulation. Second, the fiducial marker and a novel marker pose estimation method are applied to alleviate the adverse effect of sparse features. Third, the EKF is utilized to fuse the inertial and visual information so as to reject the serious noise. Finally, extensive experiments on a test bench demonstrate the effectiveness of the FBUS-EKF method, where the typical localization error is less than 3\%, namely, the average error is lower than 3 cm within one meter. The obtained results reveal that the FBUS-EKF method has the prospect to be applied in the precise short-range operation and the localization for underwater robots, which offers a valuable insight for further autonomous underwater task.},
  archive   = {C_IROS},
  author    = {Pengfei Zhang and Zhengxing Wu and Jian Wang and Shihan Kong and Min Tan and Junzhi Yu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636198},
  pages     = {4331-4336},
  title     = {An open-source, fiducial-based, underwater stereo visual-inertial localization method with refraction correction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HOPPY: An open-source kit for education with dynamic legged
robots. <em>IROS</em>, 4312–4318. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces HOPPY, an open-source, low-cost, robust, and modular kit for robotics education. The robot dynamically hops around a rotating gantry with a fixed base. The kit is intended to lower the entry barrier for studying dynamic robots and legged locomotion with real systems. It bridges the theoretical content of fundamental robotic courses with real dynamic robots by facilitating and guiding the software and hardware integration. This paper describes the topics which can be studied using the kit, lists its components, discusses preferred practices for implementation, presents results from experiments with the simulator and the real system, and suggests further improvements. A simple heuristic-based controller is described to achieve velocities up to 1.7m/s, navigate small objects, and mitigate external disturbances when the robot is aided by a counterweight. HOPPY was utilized as the subject of a semester-long project for the Robot Dynamics and Control course at the University of Illinois at Urbana-Champaign. The positive feedback from the students and instructors about the hands-on activities during the course motivates us to share this kit and continue improving it in the future.},
  archive   = {C_IROS},
  author    = {Joao Ramos and Yanran Ding and Young-Woo Sim and Kevin Murphy and Daniel Block},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636108},
  pages     = {4312-4318},
  title     = {HOPPY: An open-source kit for education with dynamic legged robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards efficient learning-based model predictive control
via feedback linearization and gaussian process regression.
<em>IROS</em>, 4306–4311. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a learning-based Model Predictive Control (MPC) methodology incorporating nonlinear predictions with robotics applications in mind. In particular, MPC is combined with feedback linearization for computational efficiency and Gaussian Process Regression (GPR) is used to model unknown system dynamics and nonlinearities. In this method, MPC predicts future states by leveraging a GPR model and optimizes a sequence of inputs over feedback linearized states. The controller was tested in simulation by using a two-link planar robot in the presence of model uncertainty. With respect to trajectory-tracking error, the proposed controller outperformed a conventional Proportional-Derivative Inverse Dynamics controller and a GPR-augmented version. Although a fully nonlinear MPC formulation achieved slightly better performance, the proposed controller had an average control calculation time that was 82× faster.},
  archive   = {C_IROS},
  author    = {Jack Caldwell and Joshua A. Marshall},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636755},
  pages     = {4306-4311},
  title     = {Towards efficient learning-based model predictive control via feedback linearization and gaussian process regression},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On assessing the usefulness of proxy domains for developing
and evaluating embodied agents. <em>IROS</em>, 4298–4305. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many situations it is either impossible or impractical to develop and evaluate agents entirely on the target domain on which they will be deployed. This is particularly true in robotics, where doing experiments on hardware is much more arduous than in simulation. This has become arguably more so in the case of learning-based agents. To this end, considerable recent effort has been devoted to developing increasingly realistic and higher fidelity simulators. However, we lack any principled way to evaluate how good a &quot;proxy domain&quot; is, specifically in terms of how useful it is in helping us achieve our end objective of building an agent that performs well in the target domain. In this work, we investigate methods to address this need. We begin by clearly separating two uses of proxy domains that are often conflated: 1) their ability to be a faithful predictor of agent performance and 2) their ability to be a useful tool for learning. In this paper, we attempt to clarify the role of proxy domains and establish new proxy usefulness (PU) metrics to compare the usefulness of different proxy domains. We propose the relative predictive PU to assess the predictive ability of a proxy domain and the learning PU to quantify the usefulness of a proxy as a tool to generate learning data. Furthermore, we argue that the value of a proxy is conditioned on the task that it is being used to help solve. We demonstrate how these new metrics can be used to optimize parameters of the proxy domain for which obtaining ground truth via system identification is not trivial.},
  archive   = {C_IROS},
  author    = {Anthony Courchesne and Andrea Censi and Liam Paull},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635977},
  pages     = {4298-4305},
  title     = {On assessing the usefulness of proxy domains for developing and evaluating embodied agents},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards a reference framework for tactile robot performance
and safety benchmarking. <em>IROS</em>, 4290–4297. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Improving robot systems via newly-developed sensing devices, control algorithms, or state estimators in order to obtain safe and efficient human-robot interaction as well as tactile manipulation skills requires standardized performance measurement protocols for objective comparison. Common protocols to evaluate robot motion performance are currently defined in EN ISO 9283:1998. For tactile and safety performance, however, no common metrics were agreed on nor standardized yet. In this paper, we propose a set of quantifiable performance criteria for robot performance analysis, objectifying robot force sensing, force control, and collision detection/reaction performance. We introduce the corresponding measurement setups and protocols, demonstrate and experimentally validate each with a Universal Robot UR10e and UR5e as well as a Franka Emika Panda robot arm. The proposed performance criteria, metrics, and experimental setups constitute the basis of a fully tactile performance and safety benchmarking framework that allows to objectively evaluate tactile robot performance via reproducible reference tests.},
  archive   = {C_IROS},
  author    = {Robin Jeanne Kirschner and Alexander Kurdas and Kübra Karacan and Philipp Junge and Seyed Ali Baradaran Birjandi and Nico Mansfeld and Saeed Abdolshah and Sami Haddadin},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636329},
  pages     = {4290-4297},
  title     = {Towards a reference framework for tactile robot performance and safety benchmarking},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). JCopter: Reliable UAV software through managed languages.
<em>IROS</em>, 4282–4289. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {UAVs are deployed in various applications including disaster search-and-rescue, precision agriculture, law enforcement and first response. As UAV software systems grow more complex, the drawbacks of developing them in low-level languages become more pronounced. For example, the lack of memory safety in C implies poor isolation between the UAV autopilot and other concurrent tasks. As a result, the most crucial aspect of UAV reliability-timely control of the flight-could be adversely impacted by other tasks such as perception or planning. We introduce JCopter, an autopilot framework for UAVs developed in a managed language, i.e., a high-level language with built-in safe memory and timing management. Through detailed simulation as well as flight testing, we demonstrate how JCopter retains the timeliness of C-based autopilots while also providing the reliability of managed languages.},
  archive   = {C_IROS},
  author    = {Adam Czerniejewski and John Henry Burns and Farshad Ghanei and Karthik Dantu and Yu David Liu and Lukasz Ziarek},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636617},
  pages     = {4282-4289},
  title     = {JCopter: Reliable UAV software through managed languages},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Identifying performance regression conditions for testing
&amp; evaluation of autonomous systems. <em>IROS</em>, 4276–4281. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the problem of identifying whether/how a black-box autonomous system has regressed in performance when compared to previous versions. The approach analyzes performance datasets (typically gathered through simulation-based testing) and automatically extracts test parameter clusters of predicted performance regression. First, surrogate modeling with quantile random forests is used to predict regions of performance regression with high confidence. The predicted regression landscape is then clustered in both the output space and input space to produce groupings of test conditions ranked by performance regression severity. This approach is analyzed using randomized test functions as well as through a case study to detect performance regression in autonomous surface vessel software.},
  archive   = {C_IROS},
  author    = {Paul Stankiewicz and Marin Kobilarov},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636004},
  pages     = {4276-4281},
  title     = {Identifying performance regression conditions for testing &amp; evaluation of autonomous systems},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Safety-oriented teleoperation framework for contact-rich
tasks in hazardous workspaces. <em>IROS</em>, 4268–4275. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes an admittance controller-based teleoperation system for contact-rich tasks. Based on the analysis of the motivating task (deposited iron lump removal task in the steel mill), the system concept is focused on the practical aspects of the system, and various components are combined to enhance the safety of the teleoperation of the robot. To connect the large inertia difference between the teleoperated robot and the command device, the admittance control is utilized in the teleoperation system, and the virtual spring saturation is adapted with the damping injection to ensure safe motion during the task. Lastly, the inertia and damping adaptation rule based on the contact force frequency is developed so that the system can selectively dissipate energy when the system shows oscillatory behavior. The proposed techniques have shown their effectiveness through the experiments. Although this study has started from a specific target, it suggests a practical solution for various contact-rich teleoperated tasks in the hazardous industrial workspace.},
  archive   = {C_IROS},
  author    = {Donghyeon Lee and Wan Kyun Chung and Keehoon Kim},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636093},
  pages     = {4268-4275},
  title     = {Safety-oriented teleoperation framework for contact-rich tasks in hazardous workspaces},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A reconfigurable interface for ergonomic and dynamic
tele-locomanipulation. <em>IROS</em>, 4260–4267. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Prolonged remote tele-locomanipulation of multi degrees-of-freedom mobile manipulators requires a compromise between the system’s performance and the operator’s ergonomics. Neglecting this demand can significantly affect either the task completion or the level of comfort to achieve it. However, the simultaneous consideration of these key factors has received less attention in the literature. To respond to this demand, in this work, we introduce a new teleoperation setup, which integrates the features of an ergonomic and a highly maneuverable interface into a unified solution. The ergonomic part of the interface implements a 3D mouse-like functionality, enabling the execution of long navigation tasks for the floating base. The highly manoeuvrable interface instead, enables the operator to perform dynamic or more precise manipulation by moving his/her arm in space. The locomotion and manipulation modes of the follower robot are controlled separately, which can be easily and seamlessly switched by the operator by pressing a button at any moment. Furthermore, due to the follower manipulator’s redundancy, this robot is controlled by a hierarchical quadratic programming technique which enables the definition of a set of secondary tasks to be executed in the robot’s nullspace. Finally, to demonstrate the advantages and disadvantages of the proposed user interfaces, five participants are asked to perform two different experiments: (i) target selection task on a moving surface and (ii) remote path tracking on a fixed surface. The quantitative and qualitative analyses show the effectiveness of the proposed interface during the teleoperation tasks, especially when it comes to the precise and dynamic task execution.},
  archive   = {C_IROS},
  author    = {Soheil Gholami and Francesco Tassi and Elena De Momi and Arash Ajoudani},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636775},
  pages     = {4260-4267},
  title     = {A reconfigurable interface for ergonomic and dynamic tele-locomanipulation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analysis of user preferences for robot motions in immersive
telepresence. <em>IROS</em>, 4252–4259. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper considers how the motions of a telepresence robot moving autonomously affect a person immersed in the robot through a head-mounted display. In particular, we explore the preference, comfort, and naturalness of elements of piecewise linear paths compared to the same elements on a smooth path. In a user study, thirty-six subjects watched panoramic videos of three different paths through a simulated museum in virtual reality and responded to questionnaires regarding each path. Preference for a particular path was influenced the most by comfort, forward speed, and characteristics of the turns. Preference was also strongly associated with the users’ perceived naturalness, which was primarily determined by the ability to see salient objects, the distance to the walls and objects, as well as the turns. Participants favored the paths that had a one meter per second forward speed and rated the path with the least amount of turns as the most comfortable.},
  archive   = {C_IROS},
  author    = {Katherine J. Mimnaugh and Markku Suomalainen and Israel Becerra and Eliezer Lozano and Rafael Murrieta-Cid and Steven M. LaValle},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636852},
  pages     = {4252-4259},
  title     = {Analysis of user preferences for robot motions in immersive telepresence},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Drawing elon musk: A robot avatar for remote manipulation.
<em>IROS</em>, 4244–4251. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The fast growth of communication technologies such as 5G provides high bandwidth and low latency wireless internet access. This enables both high definition video stream and real-time robot commands transmitted between robots and operators in the context of telepresence and teleoperation. Although there has been substantial research to establish algorithms that convert images to robot motions and telerobotic systems, little effort was made in establishing a clear scheme that enable artists to draw portraits using telerobotic systems. In this paper, we provide an easy-to-follow structure and implementation of a robot avatar for portrait drawing by artists through remote manipulation. The proposed telerobotic system uses a digital tablet and motion capture suit as input devices, which provides accurate drawing and continuous motion data stream respectively. With sensor fusion of the input data on the robot side, the drawing process presented in this work uses a unified force and impedance controller to ensure smooth and uniform pen-strokes. The proposed scheme was used to synthesise a system that was used by an artist to successfully finish the portrait drawing of Elon Musk. Finally, we show the effectiveness of the introduced control framework through an experiment. In particular, we validate the benefit of combining unified force and impedance control with sensor fusion of the digital tablet and motion capture suit data.},
  archive   = {C_IROS},
  author    = {Lingyun Chen and Abdalla Swikir and Sami Haddadin},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635879},
  pages     = {4244-4251},
  title     = {Drawing elon musk: A robot avatar for remote manipulation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mobile teleoperation: Feasibility of wireless wearable
sensing of the operator’s arm motion. <em>IROS</em>, 4238–4243. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Teleoperation platforms often require the user to be situated at a fixed location to both visualize and control the movement of the robot and thus do not provide the operator with much mobility. One example is in existing robotic surgery solutions that require the surgeons to be away from the patient, attached to consoles where their heads must be fixed and their arms can only move in a limited space. This creates a barrier between physicians and patients that does not exist in normal surgery. To address this issue, we propose a mobile telesurgery solution where the surgeons are no longer mechanically limited to control consoles and are able to teleoperate the robots from the patient bedside, using their arms equipped with wireless sensors and viewing the endoscope video via optical see-through head-mounted displays (HMDs). We evaluate the feasibility and efficiency of our user interaction method compared to a standard surgical robotic manipulator via two tasks with different levels of required dexterity. The results indicate that with sufficient training our proposed platform can attain similar efficiency while providing added mobility for the operator.},
  archive   = {C_IROS},
  author    = {Guanhao Fu and Ehsan Azimi and Peter Kazanzides},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636838},
  pages     = {4238-4243},
  title     = {Mobile teleoperation: Feasibility of wireless wearable sensing of the operator’s arm motion},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cursor-based robot tele-manipulation through 2D-to-SE2
interfaces. <em>IROS</em>, 4230–4237. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cursor-based tele-operation interfaces for manipulators can enable widely available and accessible control of robots to make many near term applications possible. However, their efficiency is restricted by the challenge of controlling 6 Degrees-of-Freedom (DoF) with 2D input from the cursor. Existing interfaces make use of different strategies to tackle this challenge, including viewpoint constraints, mode switching, and visual overlays, but it is unclear how these strategies impact the efficiency and accessibility of the interface. In this paper we characterize the design space of cursor-based robot control interfaces and compare alternatives in two user studies. Study 1 (N=216) compares nine alternative interfaces focusing on control of 3 DoFs to understand the differences of the interfaces at the basic level and examine the impact of task parameters on efficiency. Study 2 (N=60) compares a subset of the interfaces integrated into a system that allows full control of a robot manipulator from three orthogonal views. We also present a framework for heuristically evaluating accessibility of these interfaces and discuss the efficiency and accessibility trade-off with recommendations.},
  archive   = {C_IROS},
  author    = {Maria E. Cabrera and Kavi Dey and Kavita Krishnaswamy and Tapomayukh Bhattacharjee and Maya Cakmak},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636008},
  pages     = {4230-4237},
  title     = {Cursor-based robot tele-manipulation through 2D-to-SE2 interfaces},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic grasping with a “soft” drone: From theory to
practice. <em>IROS</em>, 4214–4221. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Rigid grippers used in existing aerial manipulators require precise positioning to achieve successful grasps and transmit large contact forces that may destabilize the drone. This limits the speed during grasping and prevents &quot;dynamic grasping&quot;, where the drone attempts to grasp an object while moving. On the other hand, biological systems (e.g., birds) rely on compliant and soft parts to dampen contact forces and compensate for grasping inaccuracy, enabling impressive feats.This paper presents the first prototype of a soft drone — a quadrotor where traditional (i.e., rigid) landing gears are replaced with a soft tendon-actuated gripper to enable aggressive grasping. We provide three key contributions. First, we describe our soft drone prototype, including electro-mechanical design, software infrastructure, and fabrication. Second, we review the set of algorithms we use for trajectory optimization and control of the drone and the soft gripper; the algorithms combine state-of-the-art techniques for quadrotor control (i.e., an adaptive geometric controller) with advanced soft robotics models (i.e., a quasi-static finite element model). Finally, we evaluate our soft drone in physics simulations (using SOFA and Unity) and in real tests in a motion-capture room. Our drone is able to dynamically grasp objects of unknown shape where baseline approaches fail. Our physical prototype ensures consistent performance, achieving 91.7\% successful grasps across 23 trials. We showcase dynamic grasping results in the video attachment.},
  archive   = {C_IROS},
  author    = {Joshua Fishman and Samuel Ubellacker and Nathan Hughes and Luca Carlone},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635927},
  pages     = {4214-4221},
  title     = {Dynamic grasping with a &quot;Soft&quot; drone: From theory to practice},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A motion decoupled aerial robotic manipulator for better
inspection. <em>IROS</em>, 4207–4213. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For conventional aerial manipulators, the robotic arm is rigidly attached to the quadrotor. Consequently, the maneuver of the quadrotor will affect the motion of the robotic arm when it is used for tasks such as inspection. In this paper, we propose a novel aerial manipulator with a self-locking gimbal system which can switch between motion coupled and decoupled mode. Furthermore, a dynamic gravity compensation mechanism is designed, where the location of the battery and the number of teeth are optimized to minimize the weight imbalance of the robotic arm during its motions. To the best of the authors’ knowledge, this is the first aerial manipulator with a motion-decoupled mechanism. Experimental results demonstrate that the proposed manipulator design can significantly improve the performance of the manipulator for general inspection tasks.},
  archive   = {C_IROS},
  author    = {Rui Peng and Xianda Chen and Peng Lu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636765},
  pages     = {4207-4213},
  title     = {A motion decoupled aerial robotic manipulator for better inspection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stability and robustness analysis of plug-pulling using an
aerial manipulator. <em>IROS</em>, 4199–4206. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, an autonomous aerial manipulation task of pulling a plug out of an electric socket is conducted, where maintaining the stability and robustness is challenging due to sudden disappearance of a large interaction force. The abrupt change in the dynamical model before and after the separation of the plug can cause destabilization or mission failure. To accomplish aerial plug-pulling, we employ the concept of hybrid automata to divide the task into three operative modes, i.e, wire-pulling, stabilizing, and free-flight. Also, a strategy for trajectory generation and a design of disturbance-observer-based controllers for each operative mode are presented. Furthermore, the theory of hybrid automata is used to prove the stability and robustness during the mode transition. We validate the proposed trajectory generation and control method by an actual wire-pulling experiment with a multirotor-based aerial manipulator.},
  archive   = {C_IROS},
  author    = {Jeonghyun Byun and Dongjae Lee and Hoseong Seo and Inkyu Jang and Jeongjun Choi and H. Jin Kim},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636356},
  pages     = {4199-4206},
  title     = {Stability and robustness analysis of plug-pulling using an aerial manipulator},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). REAL: Rapid exploration with active loop-closing toward
large-scale 3D mapping using UAVs. <em>IROS</em>, 4194–4198. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Exploring an unknown environment without colliding with obstacles is one of the essentials of autonomous vehicles to perform diverse missions such as structural inspections, rescues, deliveries, and so forth. Therefore, unmanned aerial vehicles (UAVS), which are fast, agile, and have high degrees of freedom, have been widely used. However, previous approaches have two limitations: a) First, they may not be appropriate for exploring large-scale environments because they mainly depend on random sampling-based path planning that causes unnecessary movements. b) Second, they assume the pose estimation is accurate enough, which is the most critical factor in obtaining an accurate map. In this paper, to explore and map unknown large-scale environments rapidly and accurately, we propose a novel exploration method that combines the pre-calculated Peacock Trajectory with graph-based global exploration and active loop-closing. Because the two-step trajectory that considers the kinodynamics of UAVs is used, obstacle avoidance is guaranteed in the receding-horizon manner. In addition, local exploration that considers the frontier and global exploration based on the graph maximizes the speed of exploration by minimizing unnecessary revisiting. In addition, by actively closing the loop based on the likelihood, pose estimation performance is improved. The proposed method’s performance is verified by exploring 3D simulation environments in comparison with the state-of-the-art methods. Finally, the proposed approach is validated in a real-world experiment.},
  archive   = {C_IROS},
  author    = {Eungchang Mason Lee and Junho Choi and Hyungtae Lim and Hyun Myung},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636611},
  pages     = {4194-4198},
  title     = {REAL: Rapid exploration with active loop-closing toward large-scale 3D mapping using UAVs},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). CCRobot-IV-f: A ducted-fan-driven flying-type
bridge-stay-cable climbing robot. <em>IROS</em>, 4184–4190. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A Flying-type cable climbing robot, CCRobot-IV-F, is presented in this paper. It is a climbing precursor of the fourth version of CCRobot, designed to surpass the abilities of previous robots with high climbing speed and obstacle-crossing capability. CCRobot-IV-F weighs less than 10 kg and a no-load speed of up to 4.5 m/s, which significantly exceeds that of other climbing robots. A dynamic model integrated with a cable-fixed coordinate system is developed, and a cascaded controller designed for stabilizing hover and climb with grippers, when a Global Positioning System and magnetometer are unavailable, is shown to work reliably in practice. Experimental results show that CCRobot-IV-F significantly improves the locomotive performance of CCRobot-IV, exhibiting fast speed, good payload capacity, and excellent obstacle-crossing capability. Moreover, CCRobot-IV-F is applied to a cable-stayed bridge in the field.},
  archive   = {C_IROS},
  author    = {Wenchao Zhang and Zhenliang Zheng and Xueqi Fu and Sarsenbek Hazken and Huaping Chen and Min Zhao and Ning Ding},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636022},
  pages     = {4184-4190},
  title     = {CCRobot-IV-F: A ducted-fan-driven flying-type bridge-stay-cable climbing robot},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GateNet: An efficient deep neural network architecture for
gate perception using fish-eye camera in autonomous drone racing.
<em>IROS</em>, 4176–4183. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fast and robust gate perception is of great importance in autonomous drone racing. We propose a convolutional neural network-based gate detector (GateNet 1 ) that concurrently detects gate’s center, distance, and orientation with respect to the drone using only images from a single fish-eye RGB camera. GateNet achieves a high inference rate (up to 60 Hz) on an onboard processor (Jetson TX2). Moreover, GateNet is robust to gate pose changes and background disturbances. The proposed perception pipeline leverages a fish-eye lens with a wide field-of-view and thus can detect multiple gates in close range, allowing a longer planning horizon even in tight environments. For benchmarking, we propose a comprehensive dataset (AU-DR) that focuses on gate perception. Throughout the experiments, GateNet shows its superiority when compared to similar methods while being efficient for onboard computers in autonomous drone racing. The effectiveness of the proposed framework is tested on a fully-autonomous drone that flies on previously-unknown track with tight turns and varying gate positions and orientations in each lap.},
  archive   = {C_IROS},
  author    = {Huy Xuan Pham and Ilker Bozcan and Andriy Sarabakha and Sami Haddadin and Erdal Kayacan},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636207},
  pages     = {4176-4183},
  title     = {GateNet: An efficient deep neural network architecture for gate perception using fish-eye camera in autonomous drone racing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Design, integration and implementation of an intelligent and
self-recharging drone system for autonomous power line inspection.
<em>IROS</em>, 4168–4175. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Today, many inspection domains utilize the benefits of drones to monitor and inspect infrastructure in an efficient manner. The energy grid is challenged by frequent and thorough inspection to stay operational. So far, drones have already been introduced to solve this challenge. However, the inspection drone still requires manual control and subsequent human examination of the captured photos and videos. This inspection process comes with a high inspection cost and is susceptible to human errors in fault finding. The proposed system builds on top of the authors’ previous research to develop and verify an integrated drone system for autonomous power line inspection, enabling functioning mechanics through pneumatics, extension of operation time through energy harvesting, Artificial Intelligent (AI) fault detection, and system autonomy using navigational algorithms. An advanced drone system has been designed and manufactured for the mission, with the results demonstrating the capability to perform an autonomous inspection mission in conditions up to 30 kts wind speeds, being additionally able to detect faults in real-time at a high rate during drone motion. Furthermore, we demonstrate the ability to recharge the drone battery within 2.4 hours.},
  archive   = {C_IROS},
  author    = {Nicolai Iversen and Oscar Bowen Schofield and Linda Cousin and Naeem Ayoub and Gerd vom Bögel and Emad Ebeid},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635924},
  pages     = {4168-4175},
  title     = {Design, integration and implementation of an intelligent and self-recharging drone system for autonomous power line inspection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Affect-driven robot behavior learning system using EEG
signals for less negative feelings and more positive outcomes.
<em>IROS</em>, 4162–4167. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning from human feedback using event-related electroencephalography (EEG) signals has attracted extensive attention recently owing to their intuitive communication ability by decoding user intentions. However, this approach requires users to perform specified tasks and their success or failure. In addition, the amount of attention needed for decision-making increases with the task difficulty, decreasing human feedback quality over time because of fatigue. Consequently, this can reduce the interaction quality and can even cause interaction breakdowns. To overcome these limitations and enable the interaction of robots with higher complexity tasks, we propose a closed-loop control system that learns affective responses to robot behaviors and provides natural feedback to optimize robot parameters for smoothing the next action. Experimental results demonstrate our affect-driven closed-loop control system yielded better affective outcomes and task performance than an open-loop system with correlated neuroscientific characteristics of EEG signals, thus enhancing the quality of human-robot interaction.},
  archive   = {C_IROS},
  author    = {Byung Hyung Kim and Ji Ho Kwak and Minuk Kim and Sungho Jo},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636451},
  pages     = {4162-4167},
  title     = {Affect-driven robot behavior learning system using EEG signals for less negative feelings and more positive outcomes},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hybrid graph convolutional networks for skeleton-based and
EEG-based jumping action recognition. <em>IROS</em>, 4156–4161. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Kinematic information obtained directly from the skeletal model has been useful for jumping action recognition. Current research focuses on dynamic analysis based on the video stream. Although skeletal data can accurately capture the high-level information of human action, it ignores the brain’s pre-execution command information, which plays a crucial role in identifying jumping action. Therefore, we proposed a hybrid model based on brain network and dynamic skeleton. Specifically, we used a brain network graph convolutional network (BNGCN) to encode brain command information. Also, a dynamic skeleton convolutional network (DSGCN) using the angular velocity of skeleton nodes instead of video is proposed, which can break the fixed experimental area’s limitation. BNGCN and DSGCN are fused through three network nodes to construct an end-to-end Brain Network and Dynamic Skeleton Hybrid Model. Our contribution consists of three parts. First, we have created a data set that can be used for jumping action and its sub-phase recognition. Second, BNGCN is used to extract brain command information for jumping action recognition. Third, a hybrid model is proposed to incorporate brain command and skeleton kinematic information. The results show that our hybrid model can effectively capture the high-level features for jumping action recognition. The method outperforms compared methods for jumping action recognition.},
  archive   = {C_IROS},
  author    = {Naishi Feng and Fo Hu and Hong Wang and Ziqi Zhao},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636110},
  pages     = {4156-4161},
  title     = {Hybrid graph convolutional networks for skeleton-based and EEG-based jumping action recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neurointerface implemented with oscillator motifs.
<em>IROS</em>, 4150–4155. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we present a definition of a neurointerface architecture combined from two main parts (1) neuroport (a hardware device) that implements a neuro protocol, generated and managed by a (2) neuroterminal (a software). The proposed architecture was created by analogy with OSI network architecture. We also present the neuroterminal as an oscillator motif real-time neurosimulation and results of the comparison of a bio-plausible motor pattern generated by oscillator motifs with square pulses of 20 – 40 Hz used as the neuro protocol for the output neuroport and measured their discomfort rate and efficacy according to an angle of subject fingers deflection. We determined that the most effective is the five oscillator motifs generated pattern for a median nerve stimulation, whereas for a muscle simulation 20 and 40 Hz are more effective. We indicate that the oscillator motif generated pattern feels more natural than square pulses 20 – 40 Hz, which feel like a spasm.},
  archive   = {C_IROS},
  author    = {Max Talanov and Alina Suleimanova and Alexey Leukhin and Yulia Mikhailova and Alexander Toschev and Alena Militskova and Igor Lavrov and Evgeni Magid},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636089},
  pages     = {4150-4155},
  title     = {Neurointerface implemented with oscillator motifs},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Manifold trial selection to reduce negative transfer in
motor imagery-based brain–computer interface. <em>IROS</em>, 4144–4149.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A major challenge in electroencephalogram (EEG) signal classification is that the EEG signals recorded from different subjects are drawn from different distributions. When the unlabeled EEG data of the new subject arrive, called target domain, classifying them with a classifier trained on prerecorded EEG data of other subjects, called source domain, will greatly decrease the classification accuracy. Being able to use the classifiers trained on data of source domain to accurately classify the data of target domain could reduce the time of the calibration phase in the actual application of the brain-computer interface. This study considers an offline cross-subject classification scenario. We propose a novel manifold trial selection method, which reduces the distribution distance between the source and target domains by manifold transformation and domain adaptation. The proposed method provides a trial selection strategy to suppress negative transfer by removing some abnormal samples. The proposed method is applied to the motor imagery-based brain–computer interface and compared with several existing algorithms. Experimental results show that the proposed method outperforms the state-of-the-art methods.},
  archive   = {C_IROS},
  author    = {Zilin Liang and Zheng Zheng and Weihai Chen and Jianhua Wang and Jianbin Zhang and Jianer Chen and Zuobing Chen},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636137},
  pages     = {4144-4149},
  title     = {Manifold trial selection to reduce negative transfer in motor imagery-based Brain–Computer interface},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A wearable, open-source, lightweight forcemyography armband:
On intuitive, robust muscle-machine interfaces. <em>IROS</em>,
4138–4143. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With an increasing number of robotic and prosthetic devices, there is a need for intuitive interfaces which enable the user to efficiently interact with them. The conventional interfaces are generally bulky and unsuitable for dynamic and unstructured environments. An alternative to the traditional interfaces is the class of Muscle-Machine Interfaces (MuMIs) that allow the user to have an embodied interaction with the devices they are controlling. In this work, we present a wearable, lightweight, Forcemyography (FMG) based armband for Human-Machine Interaction fabricated entirely out of 3D-printed parts and silicone components. The armband uses six force sensing units, each housing an Force Sensitive Resistor (FSR) sensor. The capabilities of the armband are evaluated while decoding four different gestures (pinch, power, tripod, extension) and rest state and its performance is compared with a state-of-the-art Electromyography (EMG) bioamplifier. The decoding performance of the decoding models trained on the data acquired from the armband is significantly better than the performance of the models trained on raw EMG data. The hardware design and the related processing software, are disseminated in an open-source manner.},
  archive   = {C_IROS},
  author    = {Jayden Chapman and Anany Dwivedi and Minas Liarokapis},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636345},
  pages     = {4138-4143},
  title     = {A wearable, open-source, lightweight forcemyography armband: On intuitive, robust muscle-machine interfaces},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An assistive shared control architecture for a robotic arm
using EEG-based BCI with motor imagery. <em>IROS</em>, 4132–4137. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The paper presents a shared control architecture for robotic systems commanded through a motor imagery based Brain-Computer Interface (BCI). The overall system is aimed at assisting people to perform teleoperated manipulation tasks, and it is structured so as to leave different levels of autonomy to the user depending on the actual stage of the task execution. The low-level part of the shared control architecture is also in charge of taking into account safety and operational tasks, such as to avoid collisions or to manage robot joint limits. The overall architecture has been realized by integrating control and perception software modules developed within the ROS environment, with the OpenVibe framework used to operate the BCI device. The effectiveness of the proposed architecture has been validated through experiments where a healthy user, wearing a Unicorn g.tec BCI, performs an assisted task through motor imagery sessions, with a 7 Degrees of Freedoms Kinova Jaco2 robotic arm.},
  archive   = {C_IROS},
  author    = {Giuseppe Gillini and Paolo Di Lillo and Filippo Arrichiello},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636261},
  pages     = {4132-4137},
  title     = {An assistive shared control architecture for a robotic arm using EEG-based BCI with motor imagery},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Design of an SSVEP-based BCI stimuli system for
attention-based robot navigation in robotic telepresence. <em>IROS</em>,
4126–4131. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Brain-computer interface (BCI)-based robotic telepresence provides an opportunity for people with disabilities to control robots remotely without any actual physical movement. However, traditional BCI systems usually require the user to select the navigation direction from visual stimuli in a fixed background, which makes it difficult to control the robot in a dynamic environment during the locomotion. In this paper, a novel SSVEP-based BCI stimuli system is proposed for robotic telepresence. The novel system utilized the live video streamed from the robot onboard camera as the input. By altering and flickering the detected objects in the scene with different frequencies predefined based on their relative positions on the screen, the robot can be navigated based on the user’s attention in a dynamic manner. In order to better differentiate multiple objects (more than the number of frequencies predefined), the task-related component analysis (TRCA) model was trained with a priori offline experimental data to select the front objects with priority. Experiments were conducted to validate the proposed system. Using the system, four human subjects are able to control a humanoid robot to navigate through multiple objects to reach the desired goal. The success rate reaches 87.5\% in average.},
  archive   = {C_IROS},
  author    = {Xingchao Wang and Xiaopeng Huang and Yi Lin and Liguang Zhou and Zhenglong Sun and Yangsheng Xu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636720},
  pages     = {4126-4131},
  title     = {Design of an SSVEP-based BCI stimuli system for attention-based robot navigation in robotic telepresence},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Drop prevention control for humanoid robots carrying stacked
boxes. <em>IROS</em>, 4118–4125. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We developed a method to enable a humanoid robot to carry stacked boxes. In order to transport objects efficiently, it is necessary to carry multiple objects at the same time, but in previous studies, humanoid robots have only been able to carry a single object. When a humanoid robot carries stacked boxes, the robot drops boxes when the positional relationship between un-grasped boxes changes. The causes for dropping the boxes can be divided into sudden changes attributed to robot making turns or losing balance, and the accumulation of small changes that occur because of the impact of landing while walking. We propose a method that prevents sudden changes in the stacked boxes by smoothing the hand trajectory and modifying the misalignment by tilting or shaking the entire stack. We verify the effectiveness of proposed method for enabling a humanoid robot to carry stacked boxes through experiments using a simulator and an actual robot.},
  archive   = {C_IROS},
  author    = {Shimpei Sato and Yuta Kojio and Kunio Kojima and Fumihito Sugai and Yohei Kakiuchi and Kei Okada and Masayuki Inaba},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635892},
  pages     = {4118-4125},
  title     = {Drop prevention control for humanoid robots carrying stacked boxes},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic humanoid locomotion over rough terrain with
streamlined perception-control pipeline. <em>IROS</em>, 4111–4117. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vision aided dynamic exploration on bipedal robots poses an integrated challenge for perception and control. Rapid walking motions as well as the vibrations caused by the landing-foot contact-force introduce critical uncertainty in the visual-inertial system, which can cause the robot to misplace its feet placing on complex terrains and even fall over. In this paper, we present a streamlined integration of an efficient geometric footstep planner and the corresponding walking controller for a humanoid robot to dynamically walk across rough terrain at speeds up to 0.3 m/s. To handle perception uncertainty that arises during dynamic locomotion, we present a geometric safety scoring method in our footstep planner to optimally select feasible path candidates. In addition, the real-time performance of the perception pipeline allows for reactive locomotion such as generating a new corresponding swing leg trajectory in mid-gait if a sudden change in the terrain is detected. The proposed perception-control pipeline is evaluated and demonstrated with real experiments using a full-scale humanoid to traverse across various terrains.},
  archive   = {C_IROS},
  author    = {Moonyoung Lee and Youngsun Kwon and Sebin Lee and JongHun Choe and Junyong Park and Hyobin Jeong and Yujin Heo and Min-Su Kim and Jo Sungho and Sung-Eui Yoon and Jun-Ho Oh},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636218},
  pages     = {4111-4117},
  title     = {Dynamic humanoid locomotion over rough terrain with streamlined perception-control pipeline},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The ARoA platform: An autonomous robotic assistant with a
reconfigurable torso system and dexterous manipulation capabilities.
<em>IROS</em>, 4103–4110. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ongoing global healthcare crisis has amplified the need for automation of manual tasks in several industries and service sectors. Simple household tasks such as tidying and cleaning are in high demand, with only a few robotic platforms capable of performing them due to the mobility, workspace, and dexterity requirements. This work presents ARoA, an autonomous robotic assistant that can execute complex tasks in industrial, service, and home environments. It is equipped with two lightweight, compliant, 7 degree of freedom arms and a pair of adaptive end-effectors that enable efficient execution of a wide range of tasks. Due to the linear rail based torso system that supports the arms, the ARoA offers exceptional flexibility in terms of reachable workspace. A framework for vision-based execution of tidying and cleaning tasks is also proposed and integrated in the platform. The efficiency of the ARoA platform was experimentally validated through two everyday life applications: i) picking up and tidying randomly scattered household objects and ii) cleaning of common surfaces.},
  archive   = {C_IROS},
  author    = {Gal Gorjup and Che-Ming Chang and Geng Gao and Lucas Gerez and Anany Dwivedi and Ruobing Yu and Patrick Jarvis and Minas Liarokapis},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636604},
  pages     = {4103-4110},
  title     = {The ARoA platform: An autonomous robotic assistant with a reconfigurable torso system and dexterous manipulation capabilities},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Communicative learning with natural gestures for embodied
navigation agents with human-in-the-scene. <em>IROS</em>, 4095–4102. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human-robot collaboration is an essential re-search topic in artificial intelligence (AI), enabling researchers to devise cognitive AI systems and affords an intuitive means for users to interact with the robot. Of note, communication plays a central role. To date, prior studies in embodied agent navigation have only demonstrated that human languages facilitate communication by instructions in natural languages. Nevertheless, a plethora of other forms of communication is left unexplored. In fact, human communication originated in gestures and oftentimes is delivered through multimodal cues, e.g., “go there” with a pointing gesture. To bridge the gap and fill in the missing dimension of communication in embodied agent navigation, we propose investigating the effects of using gestures as the communicative interface instead of verbal cues. Specifically, we develop a VR-based 3D simulation environment, named Gesture-based THOR (Ges-THOR), based on AI2-THOR platform. In this virtual environment, a human player is placed in the same virtual scene and shepherds the artificial agent using only gestures. The agent is tasked to solve the navigation problem guided by natural gestures with unknown semantics; we do not use any predefined gestures due to the diversity and versatile nature of human gestures. We argue that learning the semantics of natural gestures is mutually beneficial to learning the navigation task—learn to communicate and communicate to learn. In a series of experiments, we demonstrate that human gesture cues, even without predefined semantics, improve the object-goal navigation for an embodied agent, outperforming various state-of-the-art methods.},
  archive   = {C_IROS},
  author    = {Qi Wu and Cheng-Ju Wu and Yixin Zhu and Jungseock Joo},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636208},
  pages     = {4095-4102},
  title     = {Communicative learning with natural gestures for embodied navigation agents with human-in-the-scene},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamical effect of elastically supported wobbling mass on
biped running. <em>IROS</em>, 4071–4078. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Our research team has been developing biped robots based on the nature of passive dynamics. We aim to both investigate the effect of wobbling mass and apply the findings to biped robots to achieve high-performance running. We used an elastically supported wobbling mass in the trunk of biped robots because humans utilize their elastic organs in the upper body and arms to improve running performance. To investigate the characteristics and mechanisms of passive dynamics of the wobbling mass focusing on the vertical ground reaction force, we used a simple model equipped with an elastically supported wobbling mass and analytically derived periodic solutions. The normal mode analysis of the obtained solutions explained the mechanism under which periodic solutions are achieved, and analytic solutions showed similar vertical locomotion to human running and suggested the mechanism under which high-performance locomotion is achieved from the viewpoint of the ground reaction force and energy efficiency. Based on these solutions, we conducted experiments using a prototype and a biped robot equipped with wobbling mass. Experimental results showed similar characteristics in the vertical movement to the simple model and human running. These findings will help improve our understanding of biped running and contribute to producing human-like running of biped robots.},
  archive   = {C_IROS},
  author    = {Tomoya Kamimura and Koudai Sato and Daiki Murayama and Nanako Kawase and Akihito Sano},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636036},
  pages     = {4071-4078},
  title     = {Dynamical effect of elastically supported wobbling mass on biped running},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pre-operative offline optimization of insertion point
location for safe and accurate surgical task execution. <em>IROS</em>,
4040–4047. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In robotically assisted surgical procedures the surgical tool is usually inserted in the patient’s body through a small incision, which acts as a constraint for the motion of the robot, known as remote center of Motion (RCM). The location of the insertion point on the patient’s body has huge effects on the performances of the surgical robot. In this work we present an offline pre-operative framework to identify the optimal insertion point location in order to guarantee accurate and safe surgical task execution. The approach is validated using a serial-link manipulator in conjunction with a surgical robotic tool to perform a tumor resection task, while avoiding nearby organs. Results show that the framework is capable of identifying the best insertion point ensuring high dexterity, high tracking accuracy, and safety in avoiding nearby organs.},
  archive   = {C_IROS},
  author    = {Francesco Cursi and Petar Kormushev},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636285},
  pages     = {4040-4047},
  title     = {Pre-operative offline optimization of insertion point location for safe and accurate surgical task execution},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fall detection for robotic endoscope holders in minimally
invasive surgery. <em>IROS</em>, 4032–4039. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Classic Minimally Invasive Surgery (MIS) is an ergonomic burden for assistants and surgeons. The former need to adopt uncomfortable positions for hours while holding a camera to track the latter’s gestures inside the patient. This incurs assistant’s muscle fatigue which can lead to tremor or drift of the video feedback. A backdrivable robotic holder can be attached to this device in order to compensate its weight. This allows the user to place the camera at a desired position which the robot will steadily keep once he/she releases it. However, endoscopic cameras present difficult-to-model accessories whose gravity parameters can change during the same surgery. If these changes are not foreseen by the gravity model of the robot this results in a fall of the endoscope each time it is released. Therefore, it is desired to firstly detect if there is a fall in order to be able to correct it. In this article a fall detection method for a comanipulated robotic endoscope holder is proposed. It evaluates smoothness of the robot end effector trajectory to identify whether the user manipulates the instrument or it has been released and poorly compensated. An experiment was carried out with 10 subjects where 240 releases of the endoscope were performed while it was poorly compensated. The algorithm succeeded to detect the falls with sensitivity up to 99.17\%.},
  archive   = {C_IROS},
  author    = {Jesus Mago and François Louveau and Marie-Aude Vitrani},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636678},
  pages     = {4032-4039},
  title     = {Fall detection for robotic endoscope holders in minimally invasive surgery},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Safe reinforcement learning using formal verification for
tissue retraction in autonomous robotic-assisted surgery. <em>IROS</em>,
4025–4031. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep Reinforcement Learning (DRL) is a viable solution for automating repetitive surgical subtasks due to its ability to learn complex behaviours in a dynamic environment. This task automation could lead to reduced surgeon’s cognitive workload, increased precision in critical aspects of the surgery, and fewer patient-related complications. However, current DRL methods do not guarantee any safety criteria as they maximise cumulative rewards without considering the risks associated with the actions performed. Due to this limitation, the application of DRL in the safety-critical paradigm of robot-assisted Minimally Invasive Surgery (MIS) has been constrained. In this work, we introduce a Safe-DRL framework that incorporates safety constraints for the automation of surgical subtasks via DRL training. We validate our approach in a virtual scene that replicates a tissue retraction task commonly occurring in multiple phases of an MIS. Furthermore, to evaluate the safe behaviour of the robotic arms, we formulate a formal verification tool for DRL methods that provides the probability of unsafe configurations. Our results indicate that a formal analysis guarantees safety with high confidence such that the robotic instruments operate within the safe workspace and avoid hazardous interaction with other anatomical structures.},
  archive   = {C_IROS},
  author    = {Ameya Pore and Davide Corsi and Enrico Marchesini and Diego Dall’Alba and Alicia Casals and Alessandro Farinelli and Paolo Fiorini},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636175},
  pages     = {4025-4031},
  title     = {Safe reinforcement learning using formal verification for tissue retraction in autonomous robotic-assisted surgery},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Autonomous bi-manual surgical suturing based on skills
learned from demonstration. <em>IROS</em>, 4017–4024. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel application of Learning from Demonstration to realize a fully autonomous bi-manual surgical suturing task, including needle pick up, insertion, re-grasping, extraction and hand-over. Surgical action primitives are learned from a single human demonstration and encoded into an action library from which they are pulled to compose more elaborate tasks at planning/execution time. The method is demonstrated in a non-clinical setting, using unmodified surgical instruments with a custom surgical robot system. We use stereo vision to automatically detect the suture needle and entry points to close the control loop and generalize tasks to different task conditions. The suturing task is shown to generalize well to differing initial conditions with a success rate of 17\% for the full task, a mean subtask success rate of 75\% and mean needle insertion error of 3.3 mm over the course of 46 trial task executions at human speed. Failures could all be attributed to erroneous vision-based detection, pose estimation and robot calibration.},
  archive   = {C_IROS},
  author    = {Kim L. Schwaner and Iñigo Iturrate and Jakob K. H. Andersen and Pernille T. Jensen and Thiusius R. Savarimuthu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636432},
  pages     = {4017-4024},
  title     = {Autonomous bi-manual surgical suturing based on skills learned from demonstration},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computing a task-dependent grasp metric using second-order
cone programs. <em>IROS</em>, 4009–4016. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Evaluating a grasp generated by a set of hand-object contact locations is a key component of many grasp planning algorithms. In this paper, we present a novel second-order cone program (SOCP) based optimization formulation for evaluating a grasps’ ability to apply wrenches to generate a linear motion along a given direction and/or an angular motion about the given direction. Our quality measure can be computed efficiently since the SOCP is a convex optimization problem, which can be solved optimally with interior point methods. A key feature of our approach is that we can consider the effect of contact wrenches from any contact of the object with the environment. This is different from the extant literature where only the effect of finger-object contacts is considered. Exploiting the environmental contact is useful in many manipulation scenarios either to enhance the dexterity of simple hands or improve the payload capability of the manipulator. In contrast to most existing approaches, our approach also takes into account the practical constraint that the maximum contact force that can be applied at a finger-object contact can be different for each contact. We can also include the effect of external forces like gravity, as well as the joint torque constraints of the fingers/manipulators. Furthermore, for a given motion path as a constant screw motion or a sequence of constant screw motions, we can discretize the path and compute a global grasp metric to accomplish the whole task with a chosen set of finger-object contact locations.},
  archive   = {C_IROS},
  author    = {Amin Fakhari and Aditya Patankar and Jiayin Xie and Nilanjan Chakraborty},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636197},
  pages     = {4009-4016},
  title     = {Computing a task-dependent grasp metric using second-order cone programs},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Geometry-based grasping pipeline for bi-modal pick and
place. <em>IROS</em>, 4002–4008. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose an autonomous grasping pipeline that relies on geometric information extracted from segmented point cloud data. This is in contrast to many recent approaches leveraging deep learning and thus relying on a rather large amount of training samples. We argue that the proposed geometric approach facilitates task-level planning as the shape, size, and symmetry of objects can be directly taken into account during the planning process that utilizes the new MoveIt! Task Constructor (MTC) framework to define and plan action sequences composed of several inter-related sub-tasks. The efficiency of the proposed grasping pipeline is illustrated in pick-and-place scenarios, including a long-distance pick-and-place requiring a hand-over between two hands.},
  archive   = {C_IROS},
  author    = {Robert Haschke and Guillaume Walck and Helge Ritter},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635981},
  pages     = {4002-4008},
  title     = {Geometry-based grasping pipeline for bi-modal pick and place},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Assessing grasp quality using local sensitivity analysis.
<em>IROS</em>, 3995–4001. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a new approach to investigate and quantify dynamic grasp performance. Oftentimes, existing approaches to grasp analysis assess a grasp’s quality in a static situation. We build upon such considerations to also account for the dynamic nature of most grasp operations. In particular, these typically do not, in practice, occur in a static setting. Robotic grasping is indeed commonly involved in, for instance, pick-and-place operations which involve movement and thus a dynamic aspect. We investigate grasp quality over such movements, affording consideration not only to the gripper’s and grasp configuration, but also to their trajectory. More specifically, we explore the relationship from the gripper’s base acceleration to the stability of the grasped object (assessed using the relative acceleration of the object with respect to that of the gripper), using linear approximations of the corresponding dynamics. From such relations, we construct a grasp’s robustness metric, which accounts for the movements involved in the considered scenario. Numerical simulations are used to compare achieved results with those obtained using alternate existing methods. We illustrate merit of the proposed metric by exploring robustness of a given grasp under different trajectories.},
  archive   = {C_IROS},
  author    = {Michael Zechmair and Yannick Morel},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636021},
  pages     = {3995-4001},
  title     = {Assessing grasp quality using local sensitivity analysis},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SpectGRASP: Robotic grasping by spectral correlation.
<em>IROS</em>, 3987–3994. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a spectral correlation-based method (SpectGRASP) for robotic grasping of arbitrarily shaped, unknown objects. Given a point cloud of an object, SpectGRASP extracts contact points on the object’s surface matching the hand configuration. It neither requires offline training nor a-priori object models. We propose a novel Binary Extended Gaussian Image (BEGI), which represents the point cloud surface normals of both object and robot fingers as signals on a 2-sphere. Spherical harmonics are then used to estimate the correlation between fingers and object BEGIs. The resulting spectral correlation density function provides a similarity measure of gripper and object surface normals. This is highly efficient in that it is simultaneously evaluated at all possible finger rotations in SO(3). A set of contact points are then extracted for each finger using rotations with high correlation values. We then use our previous work, Local Contact Moment (LoCoMo) similarity metric, to sequentially rank the generated grasps such that the one with maximum likelihood is executed. We evaluate the performance of SpectGRASP by conducting experiments with a 7-axis robot fitted with a parallel-jaw gripper, in a physics simulation environment. Obtained results indicate that the method not only can grasp individual objects, but also can successfully clear randomly organized groups of objects. The SpectGRASP method also outperforms the closest state-of-the-art method in terms of grasp generation time and grasp-efficiency.},
  archive   = {C_IROS},
  author    = {Maxime Adjigble and Cristiana de Farias and Rustam Stolkin and Naresh Marturi},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636235},
  pages     = {3987-3994},
  title     = {SpectGRASP: Robotic grasping by spectral correlation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detecting grasp phases and adaption of object-hand
interaction forces of a soft humanoid hand based on tactile feedback.
<em>IROS</em>, 3979–3986. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Engineering humanoid robot hands with the ability to dexterously grasp objects of different sizes, shapes, mate-rial properties and weights requires sophisticated tactile sensing and intelligent controllers able to interpret sensory information and adapt contact forces with the object to achieve a stable and safe grasp. In this paper, we present a new soft humanoid hand equipped with a multimodal sensor system in each finger and a human-inspired grasp-phases controller that is able to detect the different phases of a grasping and manipulation task, adapt interaction forces with the manipulated object and balance the force distribution in both precision and power grasps based on tactile feedback. To evaluate the controller, we conducted experiments with the hand on the humanoid robot ARMAR-6 and 31 different soft and rigid everyday objects and food items with weights ranging from 4.8 g of a paper cup to 1133.8 g of a bottle, different shapes and material properties. The results show that grasping force can be reduced by 65\% compared to a naive grasping approach using maximum force for grasping and manipulating both fragile objects without destruction as well as heavy objects.},
  archive   = {C_IROS},
  author    = {Pascal Weiner and Felix Hundhausen and Raphael Grimm and Tamim Asfour},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636484},
  pages     = {3979-3986},
  title     = {Detecting grasp phases and adaption of object-hand interaction forces of a soft humanoid hand based on tactile feedback},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Trajectory-based split hindsight reverse curriculum
learning. <em>IROS</em>, 3971–3978. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Grasping is one of the most fundamental problems in robotic manipulation. In recent years, with the development of data-driven methods, reinforcement learning has been used in solving robotic grasping problems. However, grasping is a long-horizon and sparse reward task, whose natural reward only appears when the task is successfully achieved. Therefore, it brings great challenges to the deployment of reinforcement learning methods. To tackle this difficulty, we propose a new method called Trajectory-based Split Hindsight Reverse Curriculum Learning. This method of reverse learning from the goal can greatly improve the learning efficiency and the final performance of the tasks. Specifically, based on referred trajectories, the agent starts to learn in a small state space near the goal and then gradually in larger state spaces until covering the entire state space. Through split hindsight experience replay, the sampled trajectory is divided into segments that match the current subspace&#39;s size; then, they are modified to successful trajectories to enable more efficient learning. In both simulation and real-world experiments, our method surpasses the existing methods and achieves the goal-oriented grasping tasks with higher success rates and better data efficiencies. The detailed experimental results can be viewed at https://youtu.be/7uNRzmRZhDk.},
  archive   = {C_IROS},
  author    = {Jiaxi Wu and Dianmin Zhang and Shanlin Zhong and Hong Qiao},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636842},
  pages     = {3971-3978},
  title     = {Trajectory-based split hindsight reverse curriculum learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Temporal force synergies in human grasping. <em>IROS</em>,
3963–3970. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans can intuitively grasp objects of different shape and weight. Throughout the grasp execution they control and coordinate the grasp forces at all contact points between the hand and the object to achieve a stable grasp. Dexterous grasping with humanoid hands relies on the perfect coordination between grasp posture and force balance at the contact points in a high dimensional space and remains a challenge. In this paper, we present temporal force synergies describing the change in human grasp forces during the grasp execution in a low-dimensional space based on two new grasp synergy models: 1) static force synergies that are derived by a Principal Component Analysis and represent temporal grasp forces as a sequence of time-independent synergy configurations and 2) dynamic force synergies that are learned by a recurrent neural network and encode the temporal change of grasp forces throughout grasp execution in a latent synergy space clustered by grasp types. We show that both synergy spaces encode human grasp forces with an error of less than 2\% and allow the generation of human-like grasp force patterns. Grasp forces for stable grasps described by the dynamic force synergies achieve a grasp quality comparable to demonstrated human grasps in simulation.},
  archive   = {C_IROS},
  author    = {Julia Starke and Marco Keller and Amim Asfour},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636223},
  pages     = {3963-3970},
  title     = {Temporal force synergies in human grasping},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deformable elasto-plastic object shaping using an elastic
hand and model-based reinforcement learning. <em>IROS</em>, 3955–3962.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deformable solid objects such as clay or dough are prevalent in industrial and home environments. However, robotic manipulation of such objects has largely remained unexplored in literature due to the high complexity involved in representing and modeling their deformation. This work addresses the problem of shaping elasto-plastic dough by proposing to use a novel elastic end-effector to roll dough in a reinforcement learning framework. The transition model for the end-effector-to-dough interactions is learned from one hour of robot exploration, and doughs of different hydration levels are rolled out into varying lengths. Experimental results are encouraging, with the proposed framework accomplishing the task of rolling out dough into a specified length with 60\% fewer actions than a heuristic method. Furthermore, we show that estimating stiffness using the soft end-effector can be used to effectively initialize models, improving robot performance by approximately 40\% over incorrect model initialization.},
  archive   = {C_IROS},
  author    = {Carolyn Matl and Ruzena Bajcsy},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636808},
  pages     = {3955-3962},
  title     = {Deformable elasto-plastic object shaping using an elastic hand and model-based reinforcement learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fuzzy-depth objects grasping based on FSG algorithm and a
soft robotic hand. <em>IROS</em>, 3948–3954. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous grasping is an important factor for robots physically interacting with the environment and executing versatile tasks. However, a universally applicable, cost-effective, and rapidly deployable autonomous grasping approach is still limited by those target objects with fuzzy-depth information. Examples are transparent, specular, flat, and small objects whose depth is difficult to be accurately sensed. In this work, we present a solution to those fuzzy-depth objects. The framework of our approach includes two major components: one is a soft robotic hand and the other one is a Fuzzy-depth Soft Grasping (FSG) algorithm. The soft hand is replaceable for most existing soft hands/grippers with body compliance. FSG algorithm exploits both RGB and depth images to predict grasps while not trying to reconstruct the whole scene. Two grasping primitives are designed to further increase robustness. The proposed method outperforms reference baselines in unseen fuzzy-depth objects grasping experiments (84\% success rate).},
  archive   = {C_IROS},
  author    = {Hanwen Cao and Junda Huang and Yichuan Li and Jianshu Zhou and Yunhui Liu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636173},
  pages     = {3948-3954},
  title     = {Fuzzy-depth objects grasping based on FSG algorithm and a soft robotic hand},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toward state-unsaturation guaranteed fault detection method
in visual servoing of soft robot manipulators. <em>IROS</em>, 3942–3947.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper puts forward a novel sensor-less fault detection method with only task errors feedback and applies it to visual servoing tasks of soft robot manipulators. The method is developed by introducing a suitably designed endogenous accessory signal (EAS). On the one hand, EAS transforms the change of jacobian matrix led by faults into the change of task errors, which enables the fault to be directly measured and detected; on the other hand, EAS adjusts the state trajectories according to the distance between states and their boundaries, so that state saturation is avoided. To enhance the robustness of the method, we introduce an artificial potential field that keeps the states from the undesired hyperplanes that lead to the loss of effectiveness of the method. Due to the uncalibrated feature point, its coordinates used in control laws and artificial potential filed are unknown. An adaptive algorithm is developed to guarantee the stability of the system and the convergence of the image errors. Experiments are conducted to validate the performance of the proposed method in both healthy and faulty systems.},
  archive   = {C_IROS},
  author    = {Haoyuan Gu and Hesheng Wang and Weidong Chen},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636138},
  pages     = {3942-3947},
  title     = {Toward state-unsaturation guaranteed fault detection method in visual servoing of soft robot manipulators},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SoMo: Fast and accurate simulations of continuum robots in
complex environments. <em>IROS</em>, 3934–3941. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Engineers and scientists often rely on their intuition and experience when designing soft robotic systems. The development of performant controllers and motion plans for these systems commonly requires time-consuming iterations on hardware. We present the SoMo (Soft Motion) toolkit, a software framework that makes it easy to instantiate and control typical continuum manipulators in an accurate physics simulator. SoMo introduces a standardized and human-readable description format for continuum manipulators. It leverages this description format and the Bullet physics engine to enable fast and accurate simulations of soft and soft-rigid hybrid robots in environments with complex contact interactions. This allows users to vary design and control parameters across simulations with minimal effort. We compare the capabilities of SoMo to other physics simulators and highlight the benefits and accuracy of SoMo by demonstrating the agreement between simulation and real-world experiments on several examples; these include an in-hand manipulation task with continuum fingers, an automated exploration of how to design soft fingers for precision grasping, and a brief snake locomotion study. Overall, SoMo provides an accessible way for designers of soft robotic hardware and control systems to gain access to a simulation-accelerated workflow.},
  archive   = {C_IROS},
  author    = {Moritz A. Graule and Clark B. Teeple and Thomas P. McCarthy and Grace R. Kim and Randall C. St. Louis and Robert J. Wood},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636059},
  pages     = {3934-3941},
  title     = {SoMo: Fast and accurate simulations of continuum robots in complex environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). OHPL: One-shot hand-eye policy learner. <em>IROS</em>,
3904–3910. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The control of a robot for manipulation tasks generally relies on object detection and pose estimation. An attractive alternative is to learn control policies directly from raw input data. However, this approach is time-consuming and expensive since learning the policy requires many trials with robot actions in the physical environment. To reduce the training cost, the policy can be learned in simulation with a large set of synthetic images. The limit of this approach is the domain gap between the simulation and the robot workspace. In this paper, we propose to learn a policy for robot reaching movements from a single image captured directly in the robot workspace from a camera placed on the end-effector (a hand-eye camera). The idea behind the proposed policy learner is that view changes seen from the hand-eye camera produced by actions in the robot workspace are analogous to locating a region-of-interest in a single image by performing sequential object localisation. This similar view change enables training of object reaching policies using reinforcement-learning-based sequential object localisation. To facilitate the adaptation of the policy to view changes in the robot workspace, we further present a dynamic filter that learns to bias an input state to remove irrelevant information for an action decision. The proposed policy learner can be used as a powerful representation for robotic tasks, and we validate it on static and moving object reaching tasks.},
  archive   = {C_IROS},
  author    = {Changjae Oh and Yik Lung Pang and Andrea Cavallaro},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636835},
  pages     = {3904-3910},
  title     = {OHPL: One-shot hand-eye policy learner},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to control an unstable system with one minute of
data: Leveraging gaussian process differentiation in predictive control.
<em>IROS</em>, 3896–3903. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a straightforward and efficient way to control unstable robotic systems using an estimated dynamics model. Specifically, we show how to exploit the differentiability of Gaussian Processes to create a state-dependent linearized approximation of the true continuous dynamics that can be integrated with model predictive control. Our approach is compatible with most Gaussian process approaches for system identification, and can learn an accurate model using modest amounts of training data. We validate our approach by learning the dynamics of an unstable system such as a segway with a 7-D state space and 2-D input space (using only one minute of data), and we show that the resulting controller is robust to unmodelled dynamics and disturbances, while state-of-the-art control methods based on nominal models can fail under small perturbations. Code is open sourced at https://github.com/learning-and-control/core.},
  archive   = {C_IROS},
  author    = {Ivan D. Jimenez Rodriguez and Ugo Rosolia and Aaron D. Ames and Yisong Yue},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636786},
  pages     = {3896-3903},
  title     = {Learning to control an unstable system with one minute of data: Leveraging gaussian process differentiation in predictive control},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Object picking using a two-fingered gripper measuring the
deformation and slip detection based on a 3-axis tactile sensing.
<em>IROS</em>, 3888–3895. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Object picking with two-fingered grippers is widely used in practice. However, the deformability and slipperiness of the target object still remain a challenge, and not resolving them might lead to breaking or dropping of the grasped objects. To prevent such instances, tactile sensing plays an important role because it can directly detect even the subtle changes that occur during grasping. Mechanoreceptors in the human skin detect such events by the change in the skin shape and/or vibration. Using a similar approach, a combined deformation and slip detection system using a distributed 3axis tactile information with various time-scales is proposed. Specifically, the tactile information includes the z-axis data, which denotes the deformation of the skin perpendicular to the finger’s surface and the x- and y-axes, which measure deformations tangential to the surface. The perpendicular and tangential tactile information are used to determine the deformation and slip, respectively. The system is based on a multilayer perceptron (MLP) that outputs detection results from a 3-axis tactile information. Results showed that, the perpendicular and tangential tactile information with an appropriate timescale were effective for deformation and slip detection with over 89\% and 95\% recognition rates, respectively, measured for 40 different objects. Moreover, 195 out of 200 real-time untrained grasping states were successful detected. Finally, 10 untrained objects were successfully picked.},
  archive   = {C_IROS},
  author    = {Satoshi Funabashi and Yuta Kage and Hiroyuki Oka and Yoshihiro Sakamoto and Shigeki Sugano},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636354},
  pages     = {3888-3895},
  title     = {Object picking using a two-fingered gripper measuring the deformation and slip detection based on a 3-axis tactile sensing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to play pursuit-evasion with visibility
constraints. <em>IROS</em>, 3858–3863. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of pursuit-evasion for a single pursuer and an evader in polygonal environments where the players have visibility constraints. The pursuer is tasked with catching the evader as quickly as possible while the evader tries to avoid being captured. We formalize this problem as a zero-sum game where the players have private observations and conflicting objectives.One of the challenging aspects of this game is due to limited visibility. When a player, for example, the pursuer does not see the evader, it needs to reason about all possible locations of the evader. This causes an exponential increase in the size of the state space as compared to the arena size. To overcome the challenges associated with large state spaces, we introduce a new learning-based method that compresses the game state and uses it to plan actions for the players. The results indicate that our method outperforms the existing reinforcement learning methods, and performs competitively against the current state-of-the-art randomized strategy in complex environments.},
  archive   = {C_IROS},
  author    = {Selim Engin and Qingyuan Jiang and Volkan Isler},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635959},
  pages     = {3858-3863},
  title     = {Learning to play pursuit-evasion with visibility constraints},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cross-modal 3D object detection and tracking for
auto-driving. <em>IROS</em>, 3850–3857. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Detecting and tracking objects in 3D scenes play crucial roles in autonomous driving. Successfully recognizing objects through space and time hinges on a strong detector and a reliable association scheme. Recent 3D detection and tracking approaches widely represent objects as points when associating detection results with trajectories. Despite the demonstrated success, these approaches do not fully exploit the rich appearance information of objects. In this paper, we present a conceptually simple yet effective algorithm, named AlphaTrack, which considers both the location and appearance changes to perform joint 3D object detection and tracking. To achieve this, we propose a cross-modal fusion scheme that fuses camera appearance feature with LiDAR feature to facilitate 3D detection and tracking. We further attach an additional branch to the 3D detector to output instance-aware appearance embedding, which significantly improves tracking performance with our designed association mechanisms. Extensive validations on large-scale autonomous driving dataset demonstrate the effectiveness of the proposed algorithm in comparison with state-of-the-art approaches. Notably, the proposed algorithm ranks first on the nuScenes tracking leaderboard to date.},
  archive   = {C_IROS},
  author    = {Yihan Zeng and Chao Ma and Ming Zhu and Zhiming Fan and Xiaokang Yang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636498},
  pages     = {3850-3857},
  title     = {Cross-modal 3D object detection and tracking for auto-driving},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Content disentanglement for semantically consistent
synthetic-to-real domain adaptation. <em>IROS</em>, 3844–3849. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Synthetic data generation is an appealing approach to generate novel traffic scenarios in autonomous driving. However, deep learning perception algorithms trained solely on synthetic data encounter serious performance drops when they are tested on real data. Such performance drops are commonly attributed to the domain gap between real and synthetic data. Domain adaptation methods that have been applied to mitigate the aforementioned domain gap achieve visually appealing results, but usually introduce semantic inconsistencies into the translated samples. In this work, we propose a novel, unsupervised, end-to-end domain adaptation network architecture that enables semantically consistent sim2real image transfer. Our method performs content disentanglement by employing shared content encoder and fixed style code.},
  archive   = {C_IROS},
  author    = {Mert Keser and Artem Savkin and Federico Tombari},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635948},
  pages     = {3844-3849},
  title     = {Content disentanglement for semantically consistent synthetic-to-real domain adaptation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Temporally-continuous probabilistic prediction using
polynomial trajectory parameterization. <em>IROS</em>, 3837–3843. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A commonly-used representation for motion prediction of actors is a sequence of waypoints (comprising positions and orientations) for each actor at discrete future time-points. While regressing waypoints is simple and flexible, it can exhibit unrealistic higher-order derivatives (such as acceleration) and approximation errors at intermediate time steps. To address this issue we propose a general representation for temporally-continuous probabilistic trajectory prediction that regresses polynomial parameterization coefficients. We evaluate the proposed representation on supervised trajectory prediction tasks using two large self-driving data sets. The results show realistic higher-order derivatives and better accuracy at interpolated time-points, as well as the benefits of the inferred noise distributions over the trajectories. Extensive experimental studies based on existing state-of-the-art models demonstrate the effectiveness of the proposed approach relative to other representations in predicting the future motions of vehicle, bicyclist, and pedestrian traffic actors.},
  archive   = {C_IROS},
  author    = {Zhaoen Su and Chao Wang and Henggang Cui and Nemanja Djuric and Carlos Vallespi-Gonzalez and David Bradley},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636751},
  pages     = {3837-3843},
  title     = {Temporally-continuous probabilistic prediction using polynomial trajectory parameterization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stereo waterdrop removal with row-wise dilated attention.
<em>IROS</em>, 3829–3836. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing vision systems for autonomous driving or robots are sensitive to waterdrops adhered to windows or camera lenses. Most recent waterdrop removal approaches take a single image as input and often fail to recover the missing content behind waterdrops faithfully. Thus, we propose a learning-based model for waterdrop removal with stereo images. To better detect and remove waterdrops from stereo images, we propose a novel row-wise dilated attention module to enlarge attention’s receptive field for effective information propagation between the two stereo images. In addition, we propose an attention consistency loss between the ground-truth disparity map and attention scores to enhance the left-right consistency in stereo images. Because of related datasets’ unavailability, we collect a real-world dataset that contains stereo images with and without waterdrops. Extensive experiments on our dataset suggest that our model outperforms state-of-the-art methods both quantitatively and qualitatively. Our source code and the stereo waterdrop dataset are available at https://github.com/VivianSZF/Stereo-Waterdrop-Removal.},
  archive   = {C_IROS},
  author    = {Zifan Shi and Na Fan and Dit-Yan Yeung and Qifeng Chen},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636216},
  pages     = {3829-3836},
  title     = {Stereo waterdrop removal with row-wise dilated attention},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Drive on pedestrian walk. TUK campus dataset. <em>IROS</em>,
3822–3828. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous driving in a pedestrian zone is a challenging task. Technische Universitaet Kaiserslautern (TUK) is currently researching autonomous driving on the university campus for elderly or disabled people. This paper presents a novel campus dataset from the TUK campus, recorded over the span of one year for an autonomous bus project. John Deere’s Gator X855D is used for the work which is equipped with an inertial GPS navigation system, stereo cameras, monocular camera, and lidar sensors. For pedestrian safety during autonomous driving, the sensors are attached to capture the view of all four directions. Each sensor is calibrated with respect to the rear axle center of the vehicle and the intrinsic/extrinsic calibration values are provided. Moreover, the loop closure is performed in every data sequence. Several pose estimation and deep learning techniques are implemented to validate the provided data. The dataset is publicly available 4 .},
  archive   = {C_IROS},
  author    = {Hannan Ejaz Keen and Qazi Hamza Jan and Karsten Berns},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636848},
  pages     = {3822-3828},
  title     = {Drive on pedestrian walk. TUK campus dataset},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Monocular 3D vehicle detection using uncalibrated traffic
cameras through homography. <em>IROS</em>, 3814–3821. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a method to extract the position and pose of vehicles in the 3D world from a single traffic camera. Most previous monocular 3D vehicle detection algorithms focused on cameras on vehicles from the perspective of a driver, and assumed known intrinsic and extrinsic calibration. On the contrary, this paper focuses on the same task using uncalibrated monocular traffic cameras. We observe that the homography between the road plane and the image plane is essential to 3D vehicle detection and the data synthesis for this task, and the homography can be estimated without the camera intrinsics and extrinsics. We conduct 3D vehicle detection by estimating the rotated bounding boxes (r-boxes) in the bird’s eye view (BEV) images generated from inverse perspective mapping. We propose a new regression target called tailed r-box and a dual-view network architecture which boosts the detection accuracy on warped BEV images. Experiments show that the proposed method can generalize to new camera and environment setups despite not seeing imaged from them during training.},
  archive   = {C_IROS},
  author    = {Minghan Zhu and Songan Zhang and Yuanxin Zhong and Pingping Lu and Huei Peng and John Lenneman},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636384},
  pages     = {3814-3821},
  title     = {Monocular 3D vehicle detection using uncalibrated traffic cameras through homography},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised vehicle re-identification via self-supervised
metric learning using feature dictionary. <em>IROS</em>, 3806–3813. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The key challenge of unsupervised vehicle re-identification (Re-ID) is learning discriminative features from unlabelled vehicle images. Numerous methods using domain adaptation have achieved outstanding performance, but those methods still need a labelled dataset as a source domain. This paper addresses an unsupervised vehicle Re-ID method, which no need any types of a labelled dataset, through a Self-supervised Metric Learning (SSML) based on a feature dictionary. Our method initially extracts features from vehicle images and stores them in a dictionary. Thereafter, based on the dictionary, the proposed method conducts dictionary-based positive label mining (DPLM) to search for positive labels. Pair-wise similarity, relative-rank consistency, and adjacent feature distribution similarity are jointly considered to find images that may belong to the same vehicle of a given probe image. The results of DPLM are applied to dictionary-based triplet loss (DTL) to improve the discriminativeness of learnt features and to refine the quality of the results of DPLM progressively. The iterative process with DPLM and DTL boosts the performance of unsupervised vehicle Re-ID. Experimental results demonstrate the effectiveness of the proposed method by producing promising vehicle Re-ID performance without a pre-labelled dataset. The source code for this paper is publicly available on https://github.com/andreYoo/VeRI_SSML_FD.git.},
  archive   = {C_IROS},
  author    = {Jongmin Yu and Hyeontaek Oh},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636545},
  pages     = {3806-3813},
  title     = {Unsupervised vehicle re-identification via self-supervised metric learning using feature dictionary},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RTVS: A lightweight differentiable MPC framework for
real-time visual servoing. <em>IROS</em>, 3798–3805. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent data-driven approaches to visual servoing have shown improved performances over classical methods due to precise feature matching and depth estimation. Some recent servoing approaches use a model predictive control (MPC) framework which generalise well to novel environments and are capable of incorporating dynamic constraints, but are computationally intractable in real-time, making it difficult to deploy in real-world scenarios. On the contrary, single-step methods optimise greedily and achieve high servoing rates, but lack the benefits of the MPC multi-step ahead formulation. In this paper, we make the best of both worlds and propose a lightweight visual servoing MPC framework which generates optimal control near real-time at a frequency of 10.52 Hz. This work utilises the differential cross-entropy sampling method for quick and effective control generation along with a lightweight neural network, significantly improving the servoing frequency. We also propose a flow depth normalisation layer which ameliorates the issue of inferior predictions of two view depth from the flow network. We conduct extensive experimentation on the Habitat simulator and show a notable decrease in servoing time in comparison with other approaches that optimise over a time horizon. We achieve the right balance between time and performance for visual servoing in six degrees of freedom (6DoF), while retaining the advantageous MPC formulation. Our code and dataset are publicly available † .},
  archive   = {C_IROS},
  author    = {M. Nomaan Qureshi and Pushkal Katara and Abhinav Gupta and Harit Pandya and Y V S Harish and AadilMehdi Sanchawala and Gourav Kumar and Brojeshwar Bhowmick and K. Madhava Krishna},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636290},
  pages     = {3798-3805},
  title     = {RTVS: A lightweight differentiable MPC framework for real-time visual servoing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-scale laplacian-based FMM for shape control.
<em>IROS</em>, 3792–3797. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Shape control has become a prominent research field as it enables the automation of tasks in many applications. Overall, deforming an object to a desired target shape by using few grippers is a major challenge. The limited information about the object dynamics, the need to combine small and large deformations in order to achieve certain target shapes and the non-linear nature of most deformable objects are factors that significantly hamper shape control performance. In this paper, we propose a shape control method for multi-robot manipulation of large-strain deformable objects. Our approach is based on multi-scale Laplacian descriptors that feed an FMM (Fast Marching Method) for elastic shape contour matching. The FMM&#39;s resulting path and the Laplacian operator are used to define a control strategy for the robot grippers. Simulation experiments carried out with an ARAP (As Rigid As Possible) deformation model provide satisfactory results.},
  archive   = {C_IROS},
  author    = {Ignacio Cuiral-Zueco and Gonzalo López-Nicolás},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636857},
  pages     = {3792-3797},
  title     = {Multi-scale laplacian-based FMM for shape control},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robot-assisted breast ultrasound scanning using geometrical
analysis of the seroma and image segmentation. <em>IROS</em>, 3784–3791.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a robotic ultrasound imaging method that scans the breast in two separate phases to acquire high-quality ultrasound images. Our proposed system controls five Degrees of Freedom (DoFs) of the robot that hold an ultrasound probe to perform precise scanning. This system finds the desired trajectory based on geometrical analysis of the target inside the breast in a pre-scan phase and uses this information to control the probe in a post-scan phase. The proposed method updates the desired values of rotational and translational movement of the probe in the post-scan by calculating the center of mass of segmented target in each acquired frame and the average of image confidence map. The proposed method has been tested experimentally on a plastisol phantom. Given a specific trajectory, the position and orientation of the probe have been controlled at each point of the trajectory. The experiments’ result shows us that our proposed visual servoing algorithm successfully controls the probe to look at target tissue and is fast enough for use in a robotic control loop.},
  archive   = {C_IROS},
  author    = {Mojtaba Akbari and Jay Carriere and Ron Sloboda and Tyler Meyer and Nawaid Usmani and Siraj Husain and Mahdi Tavakoli},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636401},
  pages     = {3784-3791},
  title     = {Robot-assisted breast ultrasound scanning using geometrical analysis of the seroma and image segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sampling-based MPC for constrained vision based control.
<em>IROS</em>, 3753–3758. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual servoing control schemes, such as Image-Based (IBVS), Pose Based (PBVS) or Hybrid-Based (HBVS) have been extensively developed over the last decades making possible their uses in a large number of applications. It is well-known that the main problems to be handled concern the presence of local minima or singularities, the visibility constraint, the joint limits, etc. Recently, Model Predictive Path Integral (MPPI) control algorithm has been developed for autonomous robot navigation tasks. In this paper, we propose a MPPI-VS framework applied for the control of a 6-DoF robot with 2D point, 3D point, and Pose Based Visual Servoing techniques. We performed intensive simulations under various operating conditions to show the potential advantages of the proposed control framework compared to the classical schemes. The effectiveness, the robustness and the capability in coping easily with the system constraints of the control framework are shown.},
  archive   = {C_IROS},
  author    = {Ihab S. Mohamed and Guillaume Allibert and Philippe Martinet},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635970},
  pages     = {3753-3758},
  title     = {Sampling-based MPC for constrained vision based control},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mind control of a service robot with visual servoing.
<em>IROS</em>, 3747–3752. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the growing elderly population globally, patients with severe movement disorders account for a large proportion. Moreover, the development of intelligent service equipment can better assist them in their daily. This paper proposes a new service robot control system. The brain-computer interface (BCI) based on Steady-State Visual Evoked Potentials (SSVEP) is used to acquire and process electroencephalogram(EEG) signals and output various control commands accordingly. Then, considering the visual fatigue of SSVEP-BCI, we added an object detection method based on Yolov3-tiny and saliency prediction to identify the patient’s selection intention intelligently. The results show that the subject can successfully complete the object delivery task with an average accuracy of 90.3\%. The proposed control system can help the patients control a service robot in a more intelligent and friendly way to realize some daily tasks.},
  archive   = {C_IROS},
  author    = {Lina Zhang and Zhe Sun and Feng Duan and Chi Zhu and Hiroshi Yokoi},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636079},
  pages     = {3747-3752},
  title     = {Mind control of a service robot with visual servoing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Design and evaluation of a hair combing system using a
general-purpose robotic arm. <em>IROS</em>, 3739–3746. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work introduces an approach for automatic hair combing by a lightweight robot. For people living with limited mobility, dexterity, or chronic fatigue, combing hair is often a difficult task that negatively impacts personal routines. We propose a modular system for enabling general robot manipulators to assist with a hair-combing task. The system consists of three main components. The first component is the segmentation module, which segments the location of hair in space. The second component is the path planning module that proposes automatically-generated paths through hair based on user input. The final component creates a trajectory for the robot to execute. We quantitatively evaluate the effectiveness of the paths planned by the system with 48 users and qualitatively evaluate the system with 30 users watching videos of the robot performing a hair-combing task in the physical world. The system is shown to effectively comb different hairstyles.},
  archive   = {C_IROS},
  author    = {Nathaniel Dennler and Eura Shin and Maja Matarić and Stefanos Nikolaidis},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636768},
  pages     = {3739-3746},
  title     = {Design and evaluation of a hair combing system using a general-purpose robotic arm},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Disentangling dense multi-cable knots. <em>IROS</em>,
3731–3738. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Disentangling two or more cables often requires many steps to remove crossings between and within cables. We formalize the problem of disentangling multiple cables and present an algorithm, Iterative Reduction Of Non-planar Multiple cAble kNots (IRON-MAN), that outputs robot actions to remove crossings from multi-cable knotted structures. IRON-MAN uses a learned perception system inspired by prior work in single-cable untying to imitate a graph-based supervisor, and operates on RGB image inputs of the workspace. Given a sequence of images as input, the system can disentangle two-cable twists, three-cable braids, and knots of two or three cables, such as overhand, square, carrick bend, sheet bend, crown, and fisherman’s knots. IRON-MAN keeps track of task-relevant keypoints corresponding to cable endpoints and crossings and iteratively disentangles the cables by identifying and undoing crossings that are critical to knot structure. Using a da Vinci surgical robot, we experimentally evaluate the effectiveness of IRON-MAN on disentangling multi-cable knots of types that appear in the training data, as well as generalizing to novel classes of multi-cable knots. Results suggest that IRON-MAN is effective in disentangling knots involving up to three cables with 80.5\% success and generalizing to knot types that are not present during training, with cables of identical or distinct colors. Supplementary material and videos can be found at https://tinyurl.com/multi-cable-disentangling.},
  archive   = {C_IROS},
  author    = {Vainavi Viswanath and Jennifer Grannen and Priya Sundaresan and Brijen Thananjeyan and Ashwin Balakrishna and Ellen Novoseller and Jeffrey Ichnowski and Michael Laskey and Joseph E. Gonzalez and Ken Goldberg},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636397},
  pages     = {3731-3738},
  title     = {Disentangling dense multi-cable knots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-target trajectory planning of a 6-DoF free-floating
space robot via reinforcement learning. <em>IROS</em>, 3724–3730. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Space robots have played an essential role in space junk removal. Compared with traditional model-based methods, model-free reinforcement learning methods are promising in tackling space capture missions, which is challenging due to the dynamic singular problem and measuring errors of dynamics parameters. Nevertheless, current research mostly focus on the single-target environment. In this paper, we propose a multi-target trajectory planning strategy of a 6-DoF free-floating space robot optimized by the Proximal Policy Optimization (PPO) algorithm. Furthermore, we adopt some augmentation techniques to improve the PPO algorithm on precision and stability of reaching multiple targets. In particular, we introduce an Action Ensembles Based on Poisson Distribution (AEP) method, which facilitates the policy to efficiently approximate the optimal policy. Our method can be easily extended to realize the task that the end-effector tracks a specific trajectory. We evaluate our approach on four tasks: circle trajectory tracking, external disturbances at joints, different masses of the base, and even single joint failure, without any further fine-tuning. The results suggest that the planning strategy has comparably high adaptability and anti-inference capacity. Qualitative results (videos) are available at [36].},
  archive   = {C_IROS},
  author    = {Shengjie Wang and Xiang Zheng and Yuxue Cao and Tao Zhang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636681},
  pages     = {3724-3730},
  title     = {A multi-target trajectory planning of a 6-DoF free-floating space robot via reinforcement learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Effect of assembly design on a walking multi-arm robotics
for in-space assembly. <em>IROS</em>, 3718–3723. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic In-space assembly (ISA) is the next step to building larger and more permanent structures in orbit. Robotic ISA offers a unique opportunity for engineers to design the robotic system and the structure at the same time. ISA structures can be optimized to minimize weight or the number of pieces but these decisions have large impacts on the complexity of the robotic system. This impact goes beyond just defining the length and number of joints for the robotic system; the assembly process itself will drive the robot design. Robot trajectories will result in the forces and torques being applied to individual truss pieces and to the whole assembly itself, which are driven by the assembly plan. This study focuses on some of the design concerns of a robotic ISA system; specifically focusing on a walking robotic as it assembles a linear truss.},
  archive   = {C_IROS},
  author    = {Katherine McBryan},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636179},
  pages     = {3718-3723},
  title     = {Effect of assembly design on a walking multi-arm robotics for in-space assembly},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simultaneous scene reconstruction and whole-body motion
planning for safe operation in dynamic environments. <em>IROS</em>,
3710–3717. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent work has demonstrated real-time mapping and reconstruction from dense perception, while motion planning based on distance fields has been shown to achieve fast, collision-free motion synthesis with good convergence properties. However, demonstration of a fully integrated system that can safely re-plan in unknown environments, in the presence of static and dynamic obstacles, has remained an open challenge. In this work, we first study the impact that signed and unsigned distance fields have on optimisation convergence, and the resultant error cost in trajectory optimisation problems in 2D path planning, arm manipulator motion planning, and whole-body loco-manipulation planning. We further analyse the performance of three state-of-the-art approaches to generating distance fields (Voxblox, Fiesta, and GPU-Voxels) for use in realtime environment reconstruction. Finally, we use our findings to construct a practical hybrid mapping and motion planning system which uses GPU-Voxels and GPMP2 to perform receding- horizon whole-body motion planning that can smoothly avoid moving obstacles in 3D space using live sensor data. Our results are validated in simulation and on a real-world Toyota Human Support Robot (HSR).},
  archive   = {C_IROS},
  author    = {Mark Nicholas Finean and Wolfgang Merkt and Ioannis Havoutis},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636860},
  pages     = {3710-3717},
  title     = {Simultaneous scene reconstruction and whole-body motion planning for safe operation in dynamic environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Risk-averse RRT* planning with nonlinear steering and
tracking controllers for nonlinear robotic systems under uncertainty.
<em>IROS</em>, 3681–3688. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a two-phase risk-averse architecture for controlling stochastic nonlinear robotic systems. We present Risk-Averse Nonlinear Steering RRT* (RANS-RRT*) as an RRT* variant that incorporates nonlinear dynamics by solving a nonlinear program (NLP) and accounts for risk by approximating the state distribution and performing a distributionally robust (DR) collision check to promote safe planning. The generated plan is used as a reference for a low-level tracking controller. We demonstrate three controllers: finite horizon linear quadratic regulator (LQR) with linearized dynamics around the reference trajectory, LQR with robustness-promoting multiplicative noise terms, and a nonlinear model predictive control law (NMPC). We demonstrate the effectiveness of our algorithm using unicycle dynamics under heavy-tailed Laplace process noise in a cluttered environment.},
  archive   = {C_IROS},
  author    = {Sleiman Safaoui and Benjamin J. Gravell and Venkatraman Renganathan and Tyler H. Summers},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636834},
  pages     = {3681-3688},
  title     = {Risk-averse RRT* planning with nonlinear steering and tracking controllers for nonlinear robotic systems under uncertainty},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accelerating kinodynamic RRT* through dimensionality
reduction. <em>IROS</em>, 3674–3680. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sampling-based motion planning algorithms such as RRT* are well-known for their ability to quickly find an initial solution and then converge to the optimal solution asymptotically as the number of samples tends to infinity. However, the convergence rate can be slow for high-dimensional planning problems, particularly for dynamical systems where the sampling space is not just the configuration space but the full state space. In this paper, we introduce the idea of using a partial-final-state-free (PFF) optimal controller in kinodynamic RRT* [1] to reduce the dimensionality of the sampling space. Instead of sampling the full state space, the proposed accelerated kinodynamic RRT*, called Kino-RRT*, only samples part of the state space, while the rest of the states are selected by the PFF optimal controller. We also propose a delayed and intermittent update of the optimal arrival time of all the edges in the RRT* tree to decrease the computation complexity. We tested the proposed algorithm using 4-D and 10-D state-space linear systems and showed that Kino-RRT* converges much faster than the kinodynamic RRT* algorithm.},
  archive   = {C_IROS},
  author    = {Dongliang Zheng and Panagiotis Tsiotras},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636754},
  pages     = {3674-3680},
  title     = {Accelerating kinodynamic RRT* through dimensionality reduction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PG-RRT: A gaussian mixture model driven, kinematically
constrained bi-directional RRT for robot path planning. <em>IROS</em>,
3666–3673. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Path planning and smooth trajectory generation are critical capabilities for efficient navigation of mobile robots operating in challenging and cluttered environments. For real time and autonomous operations of mobile robots, intelligent algorithms, efficient and light-weight compute, and smooth trajectory are key components. In this work, we propose an intelligent, probabilistic Gaussian mixture model driven Bi-RRT (PG-RRT) algorithm which generates nodes in the most probable regions for faster convergence. The proposed algorithm is tested in various simulated environments including highly cluttered obstacles. The experimental results of PG-RRT are compared with state-of-the-art path planning algorithms. The results show significant improvement in the number of iterations (up to 26X) and runtime (up to 17.5X) demonstrating the superiority of the proposed PG-RRT algorithm.},
  archive   = {C_IROS},
  author    = {Paras Sharma and Ankit Gupta and Dibyendu Ghosh and Vinayak Honkote and Ganeshram Nandakumar and Debasish Ghose},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636134},
  pages     = {3666-3673},
  title     = {PG-RRT: A gaussian mixture model driven, kinematically constrained bi-directional RRT for robot path planning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Geometric motion planning for a system on the cylindrical
surface. <em>IROS</em>, 3643–3649. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traditional geometric mechanics models used in locomotion analysis rely heavily on systems having symmetry in SE(2) (i.e., the dynamics and constraints are invariant with respect to a system’s position and orientation) to simplify motion planning. As a result, the symmetry assumption prevents locomotion analysis on non-flat surfaces because the system dynamics may vary as a function of position and orientation. In this paper, we develop geometric motion planning strategies for a mobile system moving on a position space whose manifold structure is a cylinder: constant non-zero curvature in one dimension and zero curvature in another. To handle this non-flat position space, we adapt conventional geometric mechanics tools - in particular the system connection and the constraint curvature function - to depend on the system orientation. In addition, we introduce a novel constraint projection method to a variational gait optimizer and demonstrate how to design gaits that allow the example system to move on the cylinder with optimal efficiency.},
  archive   = {C_IROS},
  author    = {Shuoqi Chen and Ruijie Fu and Ross Hatton and Howie Choset},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635861},
  pages     = {3643-3649},
  title     = {Geometric motion planning for a system on the cylindrical surface},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mobile recharger path planning and recharge scheduling in a
multi-robot environment. <em>IROS</em>, 3635–3642. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many multi-robot applications, mobile worker robots are often engaged in performing some tasks repetitively by following pre-computed trajectories. As these robots are battery-powered, they need to get recharged at regular intervals. We envision that, in the future, a few mobile recharger robots will be employed to supply charge to the energy-deficient worker robots recurrently to keep the overall efficiency of the system optimized. In this setup, we need to find the time instants and locations for the meeting of the worker robots and recharger robots optimally. We present a Satisfiability Modulo Theory (SMT)-based approach that captures the activities of the robots in the form of constraints in a sufficiently long finite-length time window (hypercycle) whose repetitions provide their perpetual behavior. Our SMT encoding ensures that for a chosen length of the hypercycle, the total waiting time of the worker robots due to charge constraints is minimized under certain condition, and close to optimal when the condition does not hold. Moreover, the recharger robots follow the most energy-efficient trajectories. We show the efficacy of our approach by comparing it with another variant of the SMT-based method which is not scalable but provides an optimal solution globally, and with a greedy algorithm.},
  archive   = {C_IROS},
  author    = {Tanmoy Kundu and Indranil Saha},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636078},
  pages     = {3635-3642},
  title     = {Mobile recharger path planning and recharge scheduling in a multi-robot environment},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DT*: Temporal logic path planning in a dynamic environment.
<em>IROS</em>, 3627–3634. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Path planning for a robot is one of the major problems in the area of robotics. When a robot is given a task in the form of a Linear Temporal Logic (LTL) specification such that the task needs to be carried out repetitively, we want the robot to follow the shortest cyclic path so that the number of times the robot completes the mission within a given duration gets maximized. In this paper, we address the LTL path planning problem in a dynamic environment where the newly arrived dynamic obstacles may invalidate some of the available paths at any arbitrary point in time. We present DT*, an SMT-based receding horizon planning strategy that solves an optimization problem repetitively based on the current status of the workspace to lead the robot to follow the best available path in the current situation. We implement our algorithm using the Z3 SMT solver and evaluate it extensively on an LTL specification capturing a pick-and-drop application in a warehouse environment and an office environment2. We compare our SMT-based algorithm with two carefully crafted greedy algorithms. Our experimental results show that the proposed algorithm can deal with the dynamism in the workspace in LTL path planning effectively.},
  archive   = {C_IROS},
  author    = {Priya Purohit and Indranil Saha},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636399},
  pages     = {3627-3634},
  title     = {DT*: Temporal logic path planning in a dynamic environment},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint space control via deep reinforcement learning.
<em>IROS</em>, 3619–3626. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The dominant way to control a robot manipulator uses hand-crafted differential equations leveraging some form of inverse kinematics / dynamics. We propose a simple, versatile joint-level controller that dispenses with differential equations entirely. A deep neural network, trained via model-free reinforcement learning, is used to map from task space to joint space. Experiments show the method capable of achieving similar error to traditional methods, while greatly simplifying the process by automatically handling redundancy, joint limits, and acceleration / deceleration profiles. The basic technique is extended to avoid obstacles by augmenting the input to the network with information about the nearest obstacles. Results are shown both in simulation and on a real robot via sim-to-real transfer of the learned policy. We show that it is possible to achieve sub-centimeter accuracy, both in simulation and the real world, with a moderate amount of training.},
  archive   = {C_IROS},
  author    = {Visak Kumar and David Hoeller and Balakumar Sundaralingam and Jonathan Tremblay and Stan Birchfield},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636477},
  pages     = {3619-3626},
  title     = {Joint space control via deep reinforcement learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast-learning grasping and pre-grasping via clutter
quantization and q-map masking. <em>IROS</em>, 3611–3618. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Grasping objects in cluttered scenarios is a challenging task in robotics. Performing pre-grasp actions such as pushing and shifting to scatter objects is a way to reduce clutter. Based on deep reinforcement learning, we propose a Fast-Learning Grasping (FLG) framework, that can integrate pre-grasping actions along with grasping to pick up objects from cluttered scenarios with reduced real-world training time. We associate rewards for performing moving actions with the change of environmental clutter and utilize a hybrid triggering method, leading to data-efficient learning and synergy. Then we use the output of an extended fully convolutional network as the value function of each pixel point of the workspace and establish an accurate estimation of the grasp probability for each action. We also introduce a mask function as prior knowledge to enable the agents to focus on the accurate pose adjustment to improve the effectiveness of collecting training data and, hence, to learn efficiently. We carry out pre-training of the FLG over simulated environment, and then the learnt model is transferred to the real world with minimal fine-tuning for further learning during actions. Experimental results demonstrate a 94\% grasp success rate and the ability to generalize to novel objects. Compared to state-of-the-art approaches in the literature, the proposed FLG framework can achieve similar or higher grasp success rate with lesser amount of training in the real world. Supplementary video is available at https://youtu.be/KTGj1fGU6ho.},
  archive   = {C_IROS},
  author    = {Dafa Ren and Xiaoqiang Ren and Xiaofan Wang and S. Tejaswi Digumarti and Guodong Shi},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636165},
  pages     = {3611-3618},
  title     = {Fast-learning grasping and pre-grasping via clutter quantization and Q-map masking},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Iterative coarse-to-fine 6D-pose estimation using
back-propagation. <em>IROS</em>, 3587–3594. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a 6D pose estimation method for an object from a single RGB image for a robotic grasping task. Many approaches estimate pose parameters from images taken from other viewpoints and use deep learning to achieve high accuracy. However, most of these methods are not robust to changes in object texture, and there is a possibility that the correct pose cannot be estimated by only one-time inference. Our aims are to reduce the number of failure cases and improve the accuracy by a novel architecture using the iterative backpropagation of a pose decoder network and pose estimation on intermediate representation. The error between random and target pose parameters are backpropagated to a neural network and the gradient for approaching the target pose is obtained. The pose parameter is updated using the obtained gradient, the error is calculated again, and backpropagation is re-performed. Repeating this process, we estimate a more accurate pose. Experiments using our own dataset show that estimation accuracy is improved and the number of failure cases is reduced. Furthermore, estimation by coarse-to-fine iterative processing is more accurate and faster. We also experiment with grasping using a UR5 robot and show that the robot can grasp objects without depth information when using the pose estimated by the proposed method.},
  archive   = {C_IROS},
  author    = {Ryosuke Araki and Kohsuke Mano and Tadanori Hirano and Tsubasa Hirakawa and Takayoshi Yamashita and Hironobu Fujiyoshi},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636098},
  pages     = {3587-3594},
  title     = {Iterative coarse-to-fine 6D-pose estimation using back-propagation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simultaneous semantic and collision learning for 6-DoF grasp
pose estimation. <em>IROS</em>, 3571–3578. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Grasping in cluttered scenes has always been a great challenge for robots, due to the requirement of the ability to well understand the scene and object information. Previous works usually assume that the geometry information of the objects is available, or utilize a step-wise, multi-stage strategy to predict the feasible 6-DoF grasp poses. In this work, we propose to formalize the 6-DoF grasp pose estimation as a simultaneous multi-task learning problem. In a unified framework, we jointly predict the feasible 6-DoF grasp poses, instance semantic segmentation, and collision information. The whole framework is jointly optimized and end-to-end differentiable. Our model is evaluated on large-scale benchmarks as well as the real robot system. On the public dataset, our method outperforms prior state-of-the-art methods by a large margin (+4.08 AP). We also demonstrate the implementation of our model on a real robotic platform and show that the robot can accurately grasp target objects in cluttered scenarios with a high success rate. Project link: https://openbyterobotics.github.io/sscl.},
  archive   = {C_IROS},
  author    = {Yiming Li and Tao Kong and Ruihang Chu and Yifeng Li and Peng Wang and Lei Li},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636012},
  pages     = {3571-3578},
  title     = {Simultaneous semantic and collision learning for 6-DoF grasp pose estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic domain adaptation for single-view 3D reconstruction.
<em>IROS</em>, 3563–3570. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning 3D object reconstruction from a single RGB image is a fundamental and extremely challenging problem for robots. As acquiring labeled 3D shape representations for real-world data is time-consuming and expensive, synthetic image-shape pairs are widely used for 3D reconstruction. However, the models trained on synthetic data set did not perform equally well on real-world images. The existing method used the domain adaptation to fill the domain gap between different data sets. Unlike the approach simply considered global distribution for domain adaptation, this paper presents a dynamic domain adaptation (DDA) network to extract domain-invariant image features for 3D reconstruction. The relative importance between global and local distributions are considered to reduce the discrepancy between synthetic and real-world data. In addition, graph convolution network (GCN) based mesh generation methods have achieved impressive results than voxel-based and point cloud-based methods. However, the global context in a graph is not effectively used due to the limited receptive field of GCN. In this paper, a multi-scale processing method for graph convolution network (GCN) is proposed to further improve the performance of GCN-based 3D reconstruction. The experiment results conducted on both synthetic and real-world data set have demonstrated the effectiveness of the proposed methods.},
  archive   = {C_IROS},
  author    = {Cong Yang and Housen Xie and Haihong Tian and Yuanlong Yu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636343},
  pages     = {3563-3570},
  title     = {Dynamic domain adaptation for single-view 3D reconstruction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semantic segmentation-assisted scene completion for LiDAR
point clouds. <em>IROS</em>, 3555–3562. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Outdoor scene completion is a challenging issue in 3D scene understanding, which plays an important role in intelligent robotics and autonomous driving. Due to the sparsity of LiDAR acquisition, it is far more complex for 3D scene completion and semantic segmentation. Since semantic features can provide constraints and semantic priors for completion tasks, the relationship between them is worth exploring. Therefore, we propose an end-to-end semantic segmentation-assisted scene completion network, including a 2D completion branch and a 3D semantic segmentation branch. Specifically, the network takes a raw point cloud as input, and merges the features from the segmentation branch into the completion branch hierarchically to provide semantic information. By adopting BEV representation and 3D sparse convolution, we can benefit from the lower operand while maintaining effective expression. Besides, the decoder of the segmentation branch is used as an auxiliary, which can be discarded in the inference stage to save computational consumption. Extensive experiments demonstrate that our method achieves competitive performance on SemanticKITTI dataset with low latency. Code and models will be released at https://github.com/jokester-zzz/SSA-SC.},
  archive   = {C_IROS},
  author    = {Xuemeng Yang and Hao Zou and Xin Kong and Tianxin Huang and Yong Liu and Wanlong Li and Feng Wen and Hongbo Zhang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636662},
  pages     = {3555-3562},
  title     = {Semantic segmentation-assisted scene completion for LiDAR point clouds},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving robot localisation by ignoring visual distraction.
<em>IROS</em>, 3549–3554. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Attention is an important component of modern deep learning. However, less emphasis has been put on its inverse: ignoring distraction. Our daily lives require us to explicitly avoid giving attention to salient visual features that confound the task we are trying to accomplish. This visual prioritisation allows us to concentrate on important tasks while ignoring visual distractors.In this work, we introduce Neural Blindness, which gives an agent the ability to completely ignore objects or classes that are deemed distractors. More explicitly, we aim to render a neural network completely incapable of representing specific chosen classes in its latent space. In a very real sense, this makes the network &quot;blind&quot; to certain classes, allowing and agent to focus on what is important for a given task, and demonstrates how this can be used to improve localisation.},
  archive   = {C_IROS},
  author    = {Oscar Mendez and Matthew Vowels and Richard Bowden},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636595},
  pages     = {3549-3554},
  title     = {Improving robot localisation by ignoring visual distraction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Correlate-and-excite: Real-time stereo matching via guided
cost volume excitation. <em>IROS</em>, 3542–3548. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Volumetric deep learning approach towards stereo matching aggregates a cost volume computed from input left and right images using 3D convolutions. Recent works showed that utilization of extracted image features and a spatially varying cost volume aggregation complements 3D convolutions. However, existing methods with spatially varying operations are complex, cost considerable computation time, and cause memory consumption to increase. In this work, we construct Guided Cost volume Excitation (GCE) and show that simple channel excitation of cost volume guided by image can improve performance considerably. Moreover, we propose a novel method of using top-k selection prior to soft-argmin disparity regression for computing the final disparity estimate. Combining our novel contributions, we present an end-to-end network that we call Correlate-and-Excite (CoEx). Extensive experiments of our model on the SceneFlow, KITTI 2012, and KITTI 2015 datasets demonstrate the effectiveness and efficiency of our model and show that our model outperforms other speed-based algorithms while also being competitive to other state-of-the-art algorithms. Codes will be made available at https://github.com/antabangun/coex.},
  archive   = {C_IROS},
  author    = {Antyanta Bangunharcana and Jae Won Cho and Seokju Lee and In So Kweon and Kyung-Soo Kim and Soohyun Kim},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635909},
  pages     = {3542-3548},
  title     = {Correlate-and-excite: Real-time stereo matching via guided cost volume excitation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised deep persistent monocular visual odometry and
depth estimation in extreme environments. <em>IROS</em>, 3534–3541. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, unsupervised deep learning approaches have received significant attention to estimating the depth and visual odometry (VO) from unlabelled monocular image sequences. However, their performance is limited in challenging environments due to perceptual degradation, occlusions, and rapid motions. Moreover, the existing unsupervised methods suffer from the lack of scale-consistency constraints across frames, which causes that the VO estimators fail to provide persistent trajectories over long sequences. In this study, we propose an unsupervised monocular deep VO framework that predicts a six-degrees-of-freedom pose camera motion and depth map of the scene from unlabelled RGB image sequences. We provide detailed quantitative and qualitative evaluations of the proposed framework on a) a challenging dataset collected during the DARPA Subterranean challenge 1 ; and b) the benchmark KITTI and Cityscapes datasets. The proposed approach significantly outperforms state-of-the-art unsupervised deep VO and depth prediction methods under perceptually degraded conditions providing better results for both pose estimation and depth recovery. Furthermore, it achieves state-of-the-art results in most of the VO and depth metrics on benchmark datasets. The presented approach is part of the solution used by the COSTAR team participating in the DARPA Subterranean Challenge.},
  archive   = {C_IROS},
  author    = {Yasin Almalioglu and Angel Santamaria-Navarro and Benjamin Morrell and Ali-Akbar Agha-Mohammadi},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636555},
  pages     = {3534-3541},
  title     = {Unsupervised deep persistent monocular visual odometry and depth estimation in extreme environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MDN-VO: Estimating visual odometry with confidence.
<em>IROS</em>, 3528–3533. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual Odometry (VO) is used in many applications including robotics and autonomous systems. However, traditional approaches based on feature matching are computationally expensive and do not directly address failure cases, instead relying on heuristic methods to detect failure. In this work, we propose a deep learning-based VO model to efficiently estimate 6-DoF poses, as well as a confidence model for these estimates. We utilise a CNN - RNN hybrid model to learn feature representations from image sequences. We then employ a Mixture Density Network (MDN) which estimates camera motion as a mixture of Gaussians, based on the extracted spatio-temporal representations. Our model uses pose labels as a source of supervision, but derives uncertainties in an unsupervised manner. We evaluate the proposed model on the KITTI and nuScenes datasets and report extensive quantitative and qualitative results to analyse the performance of both pose and uncertainty estimation. Our experiments show that the proposed model exceeds state-of-the-art performance in addition to detecting failure cases using the predicted pose uncertainty.},
  archive   = {C_IROS},
  author    = {Nimet Kaygusuz and Oscar Mendez and Richard Bowden},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636827},
  pages     = {3528-3533},
  title     = {MDN-VO: Estimating visual odometry with confidence},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ViNet: Pushing the limits of visual modality for
audio-visual saliency prediction. <em>IROS</em>, 3520–3527. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose the ViNet architecture for audio-visual saliency prediction. ViNet is a fully convolutional encoder-decoder architecture. The encoder uses visual features from a network trained for action recognition, and the decoder infers a saliency map via trilinear interpolation and 3D convolutions, combining features from multiple hierarchies. The overall architecture of ViNet is conceptually simple; it is causal and runs in real-time (60 fps). ViNet does not use audio as input and still outperforms the state-of-the-art audio-visual saliency prediction models on nine different datasets (three visual-only and six audio-visual datasets). ViNet also surpasses human performance on the CC, SIM and AUC metrics for the AVE dataset, and to our knowledge, it is the first model to do so. We also explore a variation of ViNet architecture by augmenting audio features into the decoder. To our surprise, upon sufficient training, the network becomes agnostic to the input audio and provides the same output irrespective of the input. Interestingly, we also observe similar behaviour in the previous state-of-the-art models [1] for audio-visual saliency prediction. Our findings contrast with previous works on deep learning-based audio-visual saliency prediction, suggesting a clear avenue for future explorations incorporating audio in a more effective manner. The code and pre-trained models are available at https://github.com/samyak0210/ViNet.},
  archive   = {C_IROS},
  author    = {Samyak Jain and Pradeep Yarlagadda and Shreyank Jyoti and Shyamgopal Karthik and Ramanathan Subramanian and Vineet Gandhi},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635989},
  pages     = {3520-3527},
  title     = {ViNet: Pushing the limits of visual modality for audio-visual saliency prediction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Safe continuous control with constrained model-based policy
optimization. <em>IROS</em>, 3512–3519. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The applicability of reinforcement learning (RL) algorithms in real-world domains often requires adherence to safety constraints, a need difficult to address given the asymptotic nature of the classic RL optimization objective. In contrast to the traditional RL objective, safe exploration considers the maximization of expected returns under safety constraints expressed in expected cost returns. We introduce a model-based safe exploration algorithm for constrained high-dimensional control to address the often prohibitively high sample complexity of model-free safe exploration algorithms. Further, we provide theoretical and empirical analyses regarding the implications of model-usage on constrained policy optimization problems and introduce a practical algorithm that accelerates policy search with model-generated data. The need for accurate estimates of a policy’s constraint satisfaction is in conflict with accumulating model-errors. We address this issue by quantifying model-uncertainty as the expected Kullback-Leibler divergence between predictions of an ensemble of probabilistic dynamics models and constrain this error-measure, resulting in an adaptive resampling scheme and dynamically limited rollout horizons. We evaluate this approach on several simulated constrained robot locomotion tasks with high-dimensional action- and state-spaces. Our empirical studies find that our algorithm reaches model-free performances with a 10-20 fold reduction of training samples while maintaining approximate constraint satisfaction levels of model-free methods.},
  archive   = {C_IROS},
  author    = {Moritz A. Zanger and Karam Daaboul and J. Marius Zöllner},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635984},
  pages     = {3512-3519},
  title     = {Safe continuous control with constrained model-based policy optimization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Motion planning for autonomous vehicles in the presence of
uncertainty using reinforcement learning. <em>IROS</em>, 3506–3511. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion planning under uncertainty is one of the main challenges in developing autonomous driving vehicles. In this work, we focus on the uncertainty in sensing and perception, resulted from a limited field of view, occlusions, and sensing range. This problem is often tackled by considering hypothetical hidden objects in occluded areas or beyond the sensing range to guarantee passive safety. However, this may result in conservative planning and expensive computation, particularly when numerous hypothetical objects need to be considered. We propose a reinforcement learning (RL) based solution to manage uncertainty by optimizing for the worst case outcome. This approach is in contrast to traditional RL, where the agents try to maximize the average expected reward. The proposed approach is built on top of the Distributional RL with its policy optimization maximizing the stochastic outcomes’ lower bound. This modification can be applied to a range of RL algorithms. As a proof-of-concept, the approach is applied to two different RL algorithms, Soft Actor-Critic and DQN. The approach is evaluated against two challenging scenarios of pedestrians crossing with occlusion and curved roads with a limited field of view. The algorithm is trained and evaluated using the SUMO traffic simulator. The proposed approach yields much better motion planning behavior compared to conventional RL algorithms and behaves comparably to humans driving style.},
  archive   = {C_IROS},
  author    = {Kasra Rezaee and Peyman Yadmellat and Simon Chamorro},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636480},
  pages     = {3506-3511},
  title     = {Motion planning for autonomous vehicles in the presence of uncertainty using reinforcement learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HARL-a: Hardware agnostic reinforcement learning through
adversarial selection. <em>IROS</em>, 3499–3505. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The use of reinforcement learning (RL) has led to huge advancements in the field of robotics. However data scarcity, brittle convergence and the gap between simulation &amp; real world environments, mean that most common RL approaches are subject to over fitting and fail to generalise to unseen environments. Hardware agnostic policies would mitigate this by allowing a single network to operate in a variety of test domains, where dynamics vary due to changes in robotic morphologies or internal parameters. We utilise the idea that learning to adapt a known and successful control policy is easier and more flexible than jointly learning numerous control policies for different morphologies.This paper presents the idea of Hardware Agnostic Reinforcement Learning using Adversarial selection (HARL-A). In this approach training examples are sampled using a novel adversarial loss function. This is designed to self regulate morphologies based on their learning potential. Simply applying our learning potential based loss function to current state-of-the-art already provides ~ 30\% improvement in performance. Meanwhile experiments using the full implementation of HARL-A report an average increase of 70\% to a standard RL baseline and 55\% compared with current state-of-the-art.},
  archive   = {C_IROS},
  author    = {Lucy Jackson and Steve Eckersley and Pete Senior and Simon Hadfield},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636167},
  pages     = {3499-3505},
  title     = {HARL-A: Hardware agnostic reinforcement learning through adversarial selection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Passing through narrow gaps with deep reinforcement
learning. <em>IROS</em>, 3492–3498. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The DARPA subterranean challenge requires teams of robots to traverse difficult and diverse underground environments. Traversing small gaps is one of the challenging scenarios that robots encounter. Imperfect sensor information makes it difficult for classical navigation methods, where behaviours require significant manual fine tuning. In this paper we present a deep reinforcement learning method for autonomously navigating through small gaps, where contact between the robot and the gap may be required. We first learn a gap behaviour policy to get through small gaps (only centimeters wider than the robot). We then learn a goal-conditioned behaviour selection policy that determines when to activate the gap behaviour policy. We train our policies in simulation and demonstrate their effectiveness with a large tracked robot in simulation and on the real platform. In simulation experiments, our approach achieves 93\% success rate when the gap behaviour is activated manually by an operator, and 63\% with autonomous activation using the behaviour selection policy. In real robot experiments, our approach achieves a success rate of 73\% with manual activation, and 40\% with autonomous behaviour selection. While we show the feasibility of our approach in simulation, the difference in performance between simulated and real world scenarios highlight the difficulty of direct sim-to-real transfer for deep reinforcement learning policies. In both the simulated and real world environments alternative methods were unable to traverse the gap.},
  archive   = {C_IROS},
  author    = {Brendan Tidd and Akansel Cosgun and Jürgen Leitner and Nicolas Hudson},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636773},
  pages     = {3492-3498},
  title     = {Passing through narrow gaps with deep reinforcement learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A joint imitation-reinforcement learning framework for
reduced baseline regret. <em>IROS</em>, 3485–3491. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In various control task domains, existing controllers provide a baseline level of performance that—though possibly suboptimal—should be maintained. Reinforcement learning (RL) algorithms that rely on extensive exploration of the state and action space can be used to optimize a control policy. However, fully exploratory RL algorithms may decrease performance below a baseline level during training. In this paper, we address the issue of online optimization of a control policy while minimizing regret with respect to a baseline policy performance. We present a joint imitation-reinforcement learning framework, denoted JIRL. The learning process in JIRL assumes the availability of a baseline policy and is designed with two objectives in mind (a) training while leveraging demonstrations from the baseline policy to minimize regret with respect to the baseline policy, and (b) eventually surpassing the baseline performance. JIRL addresses these objectives by initially learning to imitate the baseline policy and gradually shifting control from the baseline to an RL agent. Experimental results show that JIRL effectively accomplishes the aforementioned objectives in several, continuous action-space domains. The results demonstrate that JIRL is comparable to a state-of-the-art algorithm in its final performance while incurring significantly lower baseline regret during training. Moreover, the results show a reduction factor of up to 21 in baseline regret over a trust-region-based approach that guarantees monotonic policy improvement.},
  archive   = {C_IROS},
  author    = {Sheelabhadra Dey and Sumedh Pendurkar and Guni Sharon and Josiah P. Hanna},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636294},
  pages     = {3485-3491},
  title     = {A joint imitation-reinforcement learning framework for reduced baseline regret},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An efficient image-to-image translation HourGlass-based
architecture for object pushing policy learning. <em>IROS</em>,
3478–3484. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans effortlessly solve pushing tasks in everyday life but unlocking these capabilities remains a challenge in robotics because physics models of these tasks are often inaccurate or unattainable. State-of-the-art data-driven approaches learn to compensate for these inaccuracies or replace the approximated physics models altogether. Nevertheless, approaches like Deep Q-Networks (DQNs) suffer from local optima in large state-action spaces. Furthermore, they rely on well-chosen deep learning architectures and learning paradigms. In this paper, we propose to frame the learning of pushing policies (where to push and how) by DQNs as an image-to-image translation problem and exploit an Hourglass-based architecture. We present an architecture combining a predictor of which pushes lead to changes in the environment with a state-action value predictor dedicated to the pushing task. Moreover, we investigate positional information encoding to learn position-dependent policy behaviors. We demonstrate in simulation experiments with a UR5 robot arm that our overall architecture helps the DQN learn faster and achieve higher performance in a pushing task involving objects with unknown dynamics.},
  archive   = {C_IROS},
  author    = {Marco Ewerton and Angel Martínez-González and Jean-Marc Odobez},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636601},
  pages     = {3478-3484},
  title     = {An efficient image-to-image translation HourGlass-based architecture for object pushing policy learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sample-efficient reinforcement learning representation
learning with curiosity contrastive forward dynamics model.
<em>IROS</em>, 3471–3477. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Developing an agent in reinforcement learning (RL) that is capable of performing complex control tasks directly from high-dimensional observation such as raw pixels is a challenge as efforts still need to be made towards improving sample efficiency and generalization of RL algorithm. This paper considers a learning framework for a Curiosity Contrastive Forward Dynamics Model (CCFDM) to achieve a more sample-efficient RL based directly on raw pixels. CCFDM incorporates a forward dynamics model (FDM) and performs contrastive learning to train its deep convolutional neural network-based image encoder (IE) to extract conducive spatial and temporal information to achieve a more sample efficiency for RL. In addition, during training, CCFDM provides intrinsic rewards, produced based on FDM prediction error, and encourages the curiosity of the RL agent to improve exploration. The diverge and less-repetitive observations provided by both our exploration strategy and data augmentation available in contrastive learning improve not only the sample efficiency but also the generalization . Performance of existing model-free RL methods such as Soft Actor-Critic built on top of CCFDM outperforms prior state-of-the-art pixel-based RL methods on the DeepMind Control Suite benchmark.},
  archive   = {C_IROS},
  author    = {Thanh Nguyen and Tung M. Luu and Thang Vu and Chang D. Yoo},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636536},
  pages     = {3471-3477},
  title     = {Sample-efficient reinforcement learning representation learning with curiosity contrastive forward dynamics model},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Map-aided train navigation with IMU measurements.
<em>IROS</em>, 3465–3470. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous train navigation using only a low-cost MEMS IMU and a track map is considered in this paper. The approach is designed for urban rail or subway environments where GNSS measurements are unreliable or unavailable, and is intended as a baseline against which more complex sensor fusion approaches can be compared to ensure the consistency of the estimates. The estimator exploits the track motion constraint and information about position and velocity derived from centripetal acceleration and angular velocity measurements to improve the dead-reckoning solution and keep error and uncertainty bounded. In experimental validation over a 6 km run of a subway train during commuter service, the proposed approach had a maximum error of 6.0 m, validating the approach as an independent estimator.},
  archive   = {C_IROS},
  author    = {Marc-Antoine Lavoie and James Richard Forbes},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636461},
  pages     = {3465-3470},
  title     = {Map-aided train navigation with IMU measurements},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LiDAR degradation quantification for autonomous driving in
rain. <em>IROS</em>, 3458–3464. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous driving in rainy conditions remains a big challenge. One of the issues is sensor degradation. LiDAR is commonly used in autonomous driving systems to perceive and understand surrounding environments. However, LiDAR performance can be degraded by rain, thereby influencing other system performance (e.g., perception or localization). Therefore, knowing how much degradation exists in current LiDAR measurements is necessary. Most existing methods can only measure LiDAR degradation in controlled environments (e.g., a chamber with simulated rain); how to quantify LiDAR degradation in dynamic environments while the autonomous vehicle is moving is still a difficult problem. In this work, we propose a novel approach to address this problem using an anomaly detection method. Our method has been evaluated on simulated and real-world data. Experimental results demonstrate the effectiveness of our method to capture LiDAR degradation and yield reasonable degradation estimations. Our experimental data and codes are accessible from http://rain.smart.mit.edu/smartrain/.},
  archive   = {C_IROS},
  author    = {Chen Zhang and Zefan Huang and Marcelo H. Ang and Daniela Rus},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636694},
  pages     = {3458-3464},
  title     = {LiDAR degradation quantification for autonomous driving in rain},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vulnerability of connected autonomous vehicles networks to
periodic time-varying communication delays of certain frequency.
<em>IROS</em>, 3452–3457. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we consider periodic communication delays within the connected autonomous vehicles platoon. Periodic signals are fundamentally simple to create, and in this study we analyze whether certain amplitude or frequencies can cause instability. This is important as we discover in this study, the classical method of replacing time-varying delays with constant delays does not capture the complex stability boundary of periodic time delays which could be exploited by attackers to cause instability in the vehicle platoon. We use the semi-discretization method to obtain plant stability. Then, we take the average value of the time delay functions to characterize the maximal admissible delay region such that the time-delayed system remains stable and provide string stability analysis. Through numerical simulations, we verify the analytical results. We construct stability charts in different parameter spaces to explore the effects of the model parameters on the system stability. A specific range of frequencies was found that could destabilize the connected autonomous vehicle platoon.},
  archive   = {C_IROS},
  author    = {Isam Al-Darabsah and Kuei-Fang Hsueh and Mohammad Al Janaideh and Sue Ann Campbell and Deepa Kundur},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636739},
  pages     = {3452-3457},
  title     = {Vulnerability of connected autonomous vehicles networks to periodic time-varying communication delays of certain frequency},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pallet detection and docking strategy for autonomous pallet
truck AGV operation. <em>IROS</em>, 3444–3451. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automated guided vehicles operation in human populated factory environments is a challenging task, especially when there is a demand to operate without following fixed paths defined by guide wires, magnetic tape, magnets, or transponders embedded in the floor. The paper at hand introduces a vision-based method enabling safe and autonomous operation of pallet moving vehicles that accommodate pallet detection, pose estimation, docking control and pallet pick up in such industrial environments. A dedicated perception topology relying on monocular vision and laser-based measurements has been applied and installed on-board a novel robotic pallet truck. Pallet detection and pose estimation are performed in two steps. Firstly, a deep neural network is used for the fast isolation of pallets&#39; regions of interest and, secondly, model-based geometrical pattern matching on point cloud data is applied to extract the pallet pose. Robot alignment with candidate pallet is performed with a dedicated visual servoing controller. The developed method has been extensively evaluated both in simulated and real industrial environments with the pallet truck and proved to have real-time performance achieving increased accuracy in navigation, pallet detection and pick-up.},
  archive   = {C_IROS},
  author    = {Efthimios Tsiogas and Ioannis Kleitsiotis and Ioannis Kostavelis and Andreas Kargakos and Dimitris Giakoumis and Marc Bosch-Jorge and Raquel Julia Ros and Rafa López Tarazón and Spyridon Likothanassis and Dimitrios Tzovaras},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636270},
  pages     = {3444-3451},
  title     = {Pallet detection and docking strategy for autonomous pallet truck AGV operation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploration-RRT: A multi-objective path planning and
exploration framework for unknown and unstructured environments.
<em>IROS</em>, 3429–3435. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This article establishes the Exploration-RRT algorithm: A novel general-purpose combined exploration and path planning algorithm, based on a multi-goal Rapidly-Exploring Random Trees (RRT) framework. Exploration-RRT (ERRT) has been specifically designed for utilization in 3D exploration missions, with partially or completely unknown and unstructured environments. The novel proposed ERRT is based on a multi-objective optimization framework and it is able to take under consideration the potential information gain, the distance travelled, and the actuation costs, along trajectories to pseudorandom goals, generated from considering the on-board sensor model and the non-linear model of the utilized platform. In this article, the algorithmic pipeline of the ERRT will be established and the overall applicability and efficiency of the proposed scheme will be presented on an application with an Unmanned Aerial Vehicle (UAV) model, equipped with a 3D lidar, in a simulated operating environment, with the goal of exploring a completely unknown area as efficiently and quickly as possible.},
  archive   = {C_IROS},
  author    = {Björn Lindqvist and Ali-Akbar Agha-Mohammadi and George Nikolakopoulos},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636243},
  pages     = {3429-3435},
  title     = {Exploration-RRT: A multi-objective path planning and exploration framework for unknown and unstructured environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep semantic segmentation at the edge for autonomous
navigation in vineyard rows. <em>IROS</em>, 3421–3428. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Precision agriculture is a fast-growing field that aims at introducing affordable and effective automation into agricultural processes. Nowadays, algorithmic solutions for navigation in vineyards require expensive sensors and high computational workloads that preclude large-scale applicability of autonomous robotic platforms in real business case scenarios. From this perspective, our novel proposed control leverages the latest advancement in machine perception and edge AI techniques to achieve highly affordable and reliable navigation inside vineyard rows with low computational and power consumption. Indeed, using a custom-trained segmentation network and a low-range RGB-D camera, we are able to take advantage of the semantic information of the environment to produce smooth trajectories and stable control in different vineyards scenarios. Moreover, the segmentation maps generated by the control algorithm itself could be directly exploited as filters for a vegetative assessment of the crop status. Extensive experimentations and evaluations against real-world data and simulated environments demonstrated the effectiveness and intrinsic robustness of our methodology.},
  archive   = {C_IROS},
  author    = {Diego Aghi and Simone Cerrato and Vittorio Mazzia and Marcello Chiaberge},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635969},
  pages     = {3421-3428},
  title     = {Deep semantic segmentation at the edge for autonomous navigation in vineyard rows},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SpikeMS: Deep spiking neural network for motion
segmentation. <em>IROS</em>, 3414–3420. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Spiking Neural Networks (SNN) are the so-called third generation of neural networks which attempt to more closely match the functioning of the biological brain. They inherently encode temporal data, allowing for training with less energy usage and can be extremely energy efficient when coded on neuromorphic hardware. In addition, they are well suited for tasks involving event-based sensors, which match the event-based nature of the SNN. However, SNNs have not been as effectively applied to real-world, large-scale tasks as standard Artificial Neural Networks (ANNs) due to the algorithmic and training complexity. To exacerbate the situation further, the input representation is unconventional and requires careful analysis and deep understanding. In this paper, we propose SpikeMS, the first deep encoder-decoder SNN architecture for the real-world large-scale problem of motion segmentation using the event-based DVS camera as input. To accomplish this, we introduce a novel spatio-temporal loss formulation that includes both spike counts and classification labels in conjunction with the use of new techniques for SNN backpropagation. In addition, we show that SpikeMS is capable of incremental predictions, or predictions from smaller amounts of test data than it is trained on. This is invaluable for providing outputs even with partial input data for low-latency applications and those requiring fast predictions. We evaluated SpikeMS on challenging synthetic and real-world sequences from EV-IMO, EED and MOD datasets and achieving results on a par with a comparable ANN method, but using potentially 50 times less power.},
  archive   = {C_IROS},
  author    = {Chethan M. Parameshwara and Simin Li and Cornelia Fermüller and Nitin J. Sanket and Matthew S. Evanusa and Yiannis Aloimonos},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636506},
  pages     = {3414-3420},
  title     = {SpikeMS: Deep spiking neural network for motion segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MapFusion: A general framework for 3D object detection with
HDMaps. <em>IROS</em>, 3406–3413. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D object detection is a key perception component in autonomous driving. Most recent approaches are based on LiDAR sensors only or fused with cameras. Maps (e.g., High Definition Maps), a basic infrastructure for intelligent vehicles, however, have not been well exploited for boosting object detection tasks. In this paper, we propose a simple but effective framework - MapFusion to integrate the map information into modern 3D object detector pipelines. In particular, we design a FeatureAgg module for HD Map feature extraction and fusion, and a MapSeg module as an auxiliary segmentation head for the detection backbone. Our proposed MapFusion is detector independent and can be easily integrated into different detectors. The experimental results of three different baselines on large public autonomous driving dataset demonstrate the superiority of the proposed framework. By fusing the map information, we can achieve 1.27 to 2.79 points improvements for mean Average Precision (mAP) on three strong 3D object detection baselines.},
  archive   = {C_IROS},
  author    = {Jin Fang and Dingfu Zhou and Xibin Song and Liangjun Zhang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636724},
  pages     = {3406-3413},
  title     = {MapFusion: A general framework for 3D object detection with HDMaps},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Part-aware data augmentation for 3D object detection in
point cloud. <em>IROS</em>, 3391–3397. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Data augmentation has greatly contributed to improving the performance in image recognition tasks, and a lot of related studies have been conducted. However, data augmentation on 3D point cloud data has not been much explored. 3D label has more sophisticated and rich structural information than the 2D label, so it enables more diverse and effective data augmentation. In this paper, we propose part-aware data augmentation (PA-AUG) that can better utilize rich information of 3D label to enhance the performance of 3D object detectors. PA-AUG divides objects into partitions and stochastically applies five augmentation methods to each local region. It is compatible with existing point cloud data augmentation methods and can be used universally regardless of the detector’s architecture. PA-AUG has improved the performance of state-of-the-art 3D object detector for all classes of the KITTI dataset and has the equivalent effect of increasing the train data by about 2.5×. We also show that PA-AUG not only increases performance for a given dataset but also is robust to corrupted data. The code is available at https://github.com/sky77764/pa-aug.pytorch},
  archive   = {C_IROS},
  author    = {Jaeseok Choi and Yeji Song and Nojun Kwak},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635887},
  pages     = {3391-3397},
  title     = {Part-aware data augmentation for 3D object detection in point cloud},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PLUMENet: Efficient 3D object detection from stereo images.
<em>IROS</em>, 3383–3390. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D object detection is a key component of many robotic applications such as self-driving vehicles. While many approaches rely on expensive 3D sensors such as LiDAR to produce accurate 3D estimates, methods that exploit stereo cameras have recently shown promising results at a lower cost. Existing approaches tackle this problem in two steps: first depth estimation from stereo images is performed to produce a pseudo LiDAR point cloud, which is then used as input to a 3D object detector. However, this approach is suboptimal due to the representation mismatch, as the two tasks are optimized in two different metric spaces. In this paper we propose a model that unifies these two tasks and performs them in the same metric space. Specifically, we directly construct a pseudo LiDAR feature volume (PLUME) in 3D space, which is then used to solve both depth estimation and object detection tasks. Our approach achieves state-of-the-art performance with much faster inference times when compared to existing methods on the challenging KITTI benchmark [1].},
  archive   = {C_IROS},
  author    = {Yan Wang and Bin Yang and Rui Hu and Ming Liang and Raquel Urtasun},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635875},
  pages     = {3383-3390},
  title     = {PLUMENet: Efficient 3D object detection from stereo images},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). APEX: Unsupervised, object-centric scene segmentation and
tracking for robot manipulation. <em>IROS</em>, 3375–3382. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in unsupervised learning for object detection, segmentation, and tracking hold significant promise for applications in robotics. A common approach is to frame these tasks as inference in probabilistic latent-variable models. In this paper, however, we show that the current state-of-the-art struggles with visually complex scenes such as typically encountered in robot manipulation tasks. We propose APEX, a new latent-variable model which is able to segment and track objects in more realistic scenes featuring objects that vary widely in size and texture, including the robot arm itself. This is achieved by a principled mask normalisation algorithm and a high-resolution scene encoder. To evaluate our approach, we present results on the real-world Sketchy dataset. This dataset, however, does not contain ground truth masks and object IDs for a quantitative evaluation. We thus introduce the Panda Pushing Dataset (P2D) which shows a Panda arm interacting with objects on a table in simulation and which includes ground-truth segmentation masks and object IDs for tracking. In both cases, APEX comprehensively outperforms the current state-of-the-art in unsupervised object segmentation and tracking. We demonstrate the efficacy of our segmentations for robot skill execution on an object arrangement task, where we also achieve the best or comparable performance among all the baselines.},
  archive   = {C_IROS},
  author    = {Yizhe Wu and Oiwi Parker Jones and Martin Engelcke and Ingmar Posner},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636711},
  pages     = {3375-3382},
  title     = {APEX: Unsupervised, object-centric scene segmentation and tracking for robot manipulation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Camera parameters aware motion segmentation network with
compensated optical flow. <em>IROS</em>, 3368–3374. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning to distinguish independent moving objects from the observed optical flow with a moving camera remains challenging. In this work, we first present a novel camera pose compensation (CPC) scheme. With the help of ingenious geometric analysis, it breaks the observed optical flow into patterns that are easier to interpret for the motion segmentation network. Secondly, we further refine such compensation with a camera parameter aware (CPA) module to account for poses’ errors in the CPC processing and enhance the entire network’s tolerance to noises. Additionally, an MMPNet is developed to intensify the identification ability of overall motion patterns. It reaches a larger receptive field with a bottom-up information transmission structure and integrates motion information at different granularities. We demonstrate the benefits of our framework on FlyingThings3D and Monkaa datasets. Without the complement of semantic information, our approach outperforms the top methods for moving objects segmentation.},
  archive   = {C_IROS},
  author    = {Xianshun Wang and Dongchen Zhu and Shaojie Xu and Wenjun Shi and Yanqing Liu and Jiamao Li and Xiaolin Zhang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636806},
  pages     = {3368-3374},
  title     = {Camera parameters aware motion segmentation network with compensated optical flow},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust initialization of multi-camera SLAM with limited view
overlaps and inaccurate extrinsic calibration. <em>IROS</em>, 3361–3367.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a robust initialization method for a multi-camera visual SLAM system where cameras have only a limited common field of views and inaccurate extrinsic calibration. The limited common field of views leads to only a few common features that can be matched between cameras. Inaccurate extrinsic poses, caused by vibrations or misplacement of cameras after offline calibration, make it even harder for triangulating the seed 3D points to initialize the SLAM system successfully. Instead of taking the extrinsic parameters as constants for feature matching and 3D point triangulation as most multi-camera systems did, we propose to take the inaccurate extrinsic poses as soft constraints to accommodate the calibration errors. Our initialization method consists of two stages by matching across different cameras and between two key frames. Both stages involve optimizing the cost functions that contain the extrinsic pose priors from inaccurate calibration parameters. By incorporating those soft pose constraints, we may avoid false feature matching and triangulation caused by inaccurate extrinsic parameters, while keeping solution space limited when only a few feature correspondences exist. The results in real-world tests show that such a simple solution can improve the success rate of SLAM initialization notably, even when the pose priors from the offline calibration differ significantly from the real ones.},
  archive   = {C_IROS},
  author    = {Ang Li and Danping Zou and Wenxian Yu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636045},
  pages     = {3361-3367},
  title     = {Robust initialization of multi-camera SLAM with limited view overlaps and inaccurate extrinsic calibration},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient multimodal belief propagation for robust SLAM
using clustering based reparameterization. <em>IROS</em>, 3354–3360. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Due to the presence of ambiguities caused by sensor noise and structural similarity, simultaneous localization and mapping (SLAM) observation models are typically multimodal. The multimodal inference process can be directly dealt with by belief propagation (BP) using weighted Gaussian mixture messages, but for efficiency, a combinatorial explosion of the complexity must be suitably relaxed. In this study, we present an effective multimodal BP SLAM for robust inference with ambiguities. Using Gaussian bandwidth mean shift and cluster-based reparameterization, we reduce the number of Gaussian components in each message due to the BP nature. The proposed algorithm reduces the number of components of the product by summarizing indistinguishable modes in weighted Gaussian mixtures and keeping only the significant modes, making BP computationally efficient.},
  archive   = {C_IROS},
  author    = {Seungwon Choi and Tae-Wan Kim},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636040},
  pages     = {3354-3360},
  title     = {Efficient multimodal belief propagation for robust SLAM using clustering based reparameterization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Consistent SLAM using local optimization with virtual prior
topologies. <em>IROS</em>, 3321–3328. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the present work we address the problem of achieving a consistent estimator for SLAM. We propose a novel method capable of computing approximately consistent global uncertainties without scaling in complexity with the total size of the explored area. The method allows arbitrary selection of local areas for optimization, introducing a methodology for building a virtual prior in bounded time. The constructed prior topologies serve as a way of contextualizing the local optimizations, resulting in consistent uncertainty estimations. This overcomes several shortcomings of previous approaches that rely on conditioning (fixing variables) and/or sliding window marginalization. Evaluations are presented in different simulated scenarios comparing results against a global batch optimization and other canonical approaches.},
  archive   = {C_IROS},
  author    = {Gastón Castro and Facundo Pessacg and Pablo De Cristóforis},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636787},
  pages     = {3321-3328},
  title     = {Consistent SLAM using local optimization with virtual prior topologies},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Geometry-based graph pruning for lifelong SLAM.
<em>IROS</em>, 3313–3320. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Lifelong SLAM considers long-term operation of a robot where already mapped locations are revisited many times in changing environments. As a result, traditional graph-based SLAM approaches eventually become extremely slow due to the continuous growth of the graph and the loss of sparsity. Both problems can be addressed by a graph pruning algorithm. It carefully removes vertices and edges to keep the graph size reasonable while preserving the information needed to provide good SLAM results. We propose a novel method that considers geometric criteria for choosing the vertices to be pruned. It is efficient, easy to implement, and leads to a graph with evenly spread vertices that remain part of the robot trajectory. Furthermore, we present a novel approach of marginalization that is more robust to wrong loop closures than existing methods. The proposed algorithm is evaluated on two publicly available real-world long-term datasets and compared to the unpruned case as well as ground truth. We show that even on a long dataset (25h), our approach manages to keep the graph sparse and the speed high while still providing good accuracy (40 times speed up, 6cm map error compared to unpruned case).},
  archive   = {C_IROS},
  author    = {Gerhard Kurz and Matthias Holoch and Peter Biber},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636530},
  pages     = {3313-3320},
  title     = {Geometry-based graph pruning for lifelong SLAM},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A general framework for lifelong localization and mapping in
changing environment. <em>IROS</em>, 3305–3312. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The environment of most real-world scenarios such as malls and supermarkets changes at all times. A pre-built map that does not account for these changes becomes out-of-date easily. Therefore, it is necessary to have an up-to-date model of the environment to facilitate long-term operation of a robot. To this end, this paper presents a general lifelong simultaneous localization and mapping (SLAM) framework. Our framework uses a multiple session map representation, and exploits an efficient map updating strategy that includes map building, pose graph refinement and sparsification. To mitigate the unbounded increase of memory usage, we propose a map-trimming method based on the Chow-Liu maximum-mutual-information spanning tree. The proposed SLAM framework has been comprehensively validated by over a month of robot deployment in real supermarket environment. Furthermore, we release the dataset collected from the indoor and outdoor changing environment with the hope to accelerate lifelong SLAM research in the community. Our dataset is available at https://github.com/sanduan168/lifelong-SLAM-dataset.},
  archive   = {C_IROS},
  author    = {Min Zhao and Xin Guo and Le Song and Baoxing Qin and Xuesong Shi and Gim Hee Lee and Guanghui Sun},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635985},
  pages     = {3305-3312},
  title     = {A general framework for lifelong localization and mapping in changing environment},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Depth ranging performance evaluation and improvement for
RGB-d cameras on field-based high-throughput phenotyping robots.
<em>IROS</em>, 3299–3304. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {RGB-D cameras have been successfully used for indoor High-ThroughPut Phenotyping (HTPP). However, their capability and feasibility for in-field HTPP applications still need to be evaluated. To solve the problem, we evaluate the depth-ranging performances of a consumer-level RGB-D camera (RealSense D435i) under in-field scenarios. First, we focus on determining their optimal ranging areas for different crop organs. Second, based on the evaluation results, we analyze the influences of light intensity on depth measurements and propose a brightness-and-distance based Support Vector Regression Strategy, to compensate the ranging error. Finally, we give an intuitive accuracy ranking diagram for RealSense D435i under natural lighting intensities. Experimental results show that: 1) RealSense D435i has good ranging performances on in-field HTPP. 2) Our error compensation model can effectively reduce the influences of lighting intensity and target distance.},
  archive   = {C_IROS},
  author    = {Zhengqiang Fan and Na Sun and Quan Qiu and Tao Li and Chunjiang Zhao},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636211},
  pages     = {3299-3304},
  title     = {Depth ranging performance evaluation and improvement for RGB-D cameras on field-based high-throughput phenotyping robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A robust illumination-invariant camera system for
agricultural applications. <em>IROS</em>, 3292–3298. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Object detection and semantic segmentation are two of the most widely adopted deep learning algorithms in agricultural applications. One of the major sources of variability in image quality acquired outdoors for such tasks is changing lighting conditions that can alter the appearance of the objects or the contents of the entire image. While transfer learning and data augmentation reduce the need for large amount of data to train deep neural networks to some extent, the large variety of cultivars and the lack of shared datasets in agriculture makes wide-scale field deployments difficult. In this paper, we present an active lighting-based camera system that generates robust and uniform images in any lighting conditions. We provide extensive validation experiments to evaluate the consistency in the quality of the images. Metrics for assessing image uniformity like Structural Similarity (SSIM) index and the Peak Signal to Noise Ratio (PSNR) ranged from 83.78 to 93.79 and from 25.30 to 31.78 respectively, showing stability over a day with changing sunlight. The validation stage also showed that the generated images effectively reduce the amount of training samples for object detection using deep neural networks. The camera system was then deployed in real field experiments for counting buds in dormant vines and shoots in early season grape vines as well as for counting apples in orchards. The mean absolute errors obtained when compared to ground truth were 5\%, 2.55\% and 8.57\%, respectively.},
  archive   = {C_IROS},
  author    = {Abhisesh Silwal and Tanvir Parhar and Francisco Yandun and Harjatin Baweja and George Kantor},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636542},
  pages     = {3292-3298},
  title     = {A robust illumination-invariant camera system for agricultural applications},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards intelligent fruit picking with in-hand sensing.
<em>IROS</em>, 3285–3291. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Studies have shown that picking techniques play an important role in determining fruit quality at harvest (e.g. bruising, stem retention, etc). When picking fruit such as apples and pears, professional pickers use active perception, incorporating both visual and tactile input about fruit orientation, stem location, and the fruit’s immediate surroundings. This combination of tactile, visual, and force feedback is what enables human workers to execute dynamic movements that quickly and efficiently remove fruit from the tree without damage. However, much of the prior work on robotic fruit picking has formulated the harvesting problem as a position-control problem, using visual feedback for closed-loop end-effector placement while disregarding feedback on physical contact. As a first step towards more intelligent fruit picking — combining proprioception, localized sensing, and observed forces — we have developed a custom end-effector with multiple in-hand sensors, including tactile sensors on the fingertips. This paper presents the mechatronic design of the device as well as results from multiple outdoor picking trials with a Honeycrisp apple tree. Preliminary results show that, with multi-modal sensing, fruit slip, fruit separation from the tree, and fruit release from the hand can be detected.},
  archive   = {C_IROS},
  author    = {Lisa M. Dischinger and Miranda Cravetz and Jacob Dawes and Callen Votzke and Chelse VanAtter and Matthew L. Johnston and Cindy M. Grimm and Joseph R. Davidson},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636341},
  pages     = {3285-3291},
  title     = {Towards intelligent fruit picking with in-hand sensing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robotic lime picking by considering leaves as permeable
obstacles. <em>IROS</em>, 3278–3284. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The problem of robotic lime picking is challenging; lime plants have dense foliage which makes it difficult for a robotic arm to grasp a lime without coming in contact with leaves. Existing approaches either do not consider leaves, or treat them as obstacles and completely avoid them, often resulting in undesirable or infeasible plans. We focus on reaching a lime in the presence of dense foliage by considering the leaves of a plant as permeable obstacles with a collision cost. We then adapt the rapidly exploring random tree star (RRT*) algorithm for the problem of fruit harvesting by incorporating the cost of collision with leaves into the path cost. To reduce the time required for finding low-cost paths to goal, we bias the growth of the tree using an artificial potential field (APF). We compare our proposed method with prior work in a 2-D environment and a 6-DOF robot simulation. Our experiments and a real-world demonstration on a robotic lime picking task demonstrate the applicability of our approach.},
  archive   = {C_IROS},
  author    = {Heramb Nemlekar and Ziang Liu and Suraj Kothawade and Sherdil Niyaz and Barath Raghavan and Stefanos Nikolaidis},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636396},
  pages     = {3278-3284},
  title     = {Robotic lime picking by considering leaves as permeable obstacles},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Viewpoint planning for fruit size and position estimation.
<em>IROS</em>, 3271–3277. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern agricultural applications require knowledge about the position and size of fruits on plants. However, occlusions from leaves typically make obtaining this information difficult. We present a novel viewpoint planning approach that builds up an octree of plants with labeled regions of interest (ROIs), i.e., fruits. Our method uses this octree to sample viewpoint candidates that increase the information around the fruit regions and evaluates them using a heuristic utility function that takes into account the expected information gain. Our system automatically switches between ROI targeted sampling and exploration sampling, which considers general frontier voxels, depending on the estimated utility. When the plants have been sufficiently covered with the RGB-D sensor, our system clusters the ROI voxels and estimates the position and size of the detected fruits. We evaluated our approach in simulated scenarios and compared the resulting fruit estimations with the ground truth. The results demonstrate that our combined approach outperforms a sampling method that does not explicitly consider the ROIs to generate viewpoints in terms of the number of discovered ROI cells. Furthermore, we show the real-world applicability by testing our framework on a robotic arm equipped with an RGB-D camera installed on an automated pipe-rail trolley in a capsicum glasshouse.},
  archive   = {C_IROS},
  author    = {Tobias Zaenker and Claus Smitt and Chris McCool and Maren Bennewitz},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636701},
  pages     = {3271-3277},
  title     = {Viewpoint planning for fruit size and position estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A low-cost robot with autonomous recharge and navigation for
weed control in fields with narrow row spacing. <em>IROS</em>,
3263–3270. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern herbicide application in agricultural set-tings typically relies on either large scale sprayers that dispense herbicide over crops and weeds alike or portable sprayers that require labor intensive manual operation. The former method results in overuse of herbicide and reduction in crop yield while the latter is often untenable in large scale operations. This paper presents the first fully autonomous robot for weed management for row crops capable of computer vision based navigation, weed detection, complete field coverage, and automatic recharge for under $400. The target application is autonomous inter-row weed control in crop fields, e.g. flax and canola, where the spacing between croplines is as small as one foot. The proposed robot is small enough to pass between croplines at all stages of plant growth while detecting weeds and spraying herbicide. A recharging system incorporates newly designed robotic hardware, a ramp, a robotic charging arm, and a mobile charging station. An integrated vision algorithm is employed to assist with charger alignment effectively. Combined, they enable the robot to work continuously in the field without access to electricity. In addition, a color-based contour algorithm combined with preprocessing techniques is applied for robust navigation relying on the input from the onboard monocular camera. Incorporating such compact robots into farms could help automate weed control, even during late stages of growth, and reduce herbicide use by targeting weeds with precision. The robotic platform is field-tested in the flaxseed fields of North Dakota.},
  archive   = {C_IROS},
  author    = {Yayun Du and Bhrugu Mallajosyula and Deming Sun and Jingyi Chen and Zihang Zhao and Mukhlesur Rahman and Mohiuddin Quadir and Mohammad Khalid Jawed},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636267},
  pages     = {3263-3270},
  title     = {A low-cost robot with autonomous recharge and navigation for weed control in fields with narrow row spacing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Few-leaf learning: Weed segmentation in grasslands.
<em>IROS</em>, 3248–3254. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous robotic weeding in grasslands requires robust weed segmentation. Deep learning models can provide solutions to this problem, but they need to be trained on large amounts of images, which in the case of grasslands are notoriously difficult to obtain and manually annotate. In this work we introduce Few-leaf Learning, a concept that facilitates the training of accurate weed segmentation models and can lead to easier generation of weed segmentation datasets with minimal human annotation effort. Our approach builds upon the fact that each plant species within the same field has relatively uniform visual characteristics due to similar environmental influences. Thus, we can train a field-and-day-specific weed segmentation model on synthetic training data stemming from just a handful of annotated weed leaves. We demonstrate the efficacy of our approach for different fields and for two common grassland weeds: Rumex obtusifolius (broad-leaved dock) and Cirsium vulgare (spear thistle). Our code is publicly available at https://github.com/RGring/WeedAnnotator.},
  archive   = {C_IROS},
  author    = {Ronja Güldenring and Evangelos Boukas and Ole Ravn and Lazaros Nalpantidis},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636770},
  pages     = {3248-3254},
  title     = {Few-leaf learning: Weed segmentation in grasslands},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fuzzy PID controller based on yaw angle prediction of a
spherical robot. <em>IROS</em>, 3242–3247. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, a fuzzy PID controller based on yaw angle prediction is applied to design an attitude controller for a spherical rolling robot. The robot consists of a 2-DOF pendulum located inside a spherical shell with freedom to rotate about the transversal and longitudinal axis. The proposed controller allows the robot to autonomously change its parameters to adapt to different environments based on current state. The past researches on the motion of spherical robots mostly focused on simulation or ideal experimental environment. But in this paper, a physical system is built and experiments are carried out to demonstrate the effectiveness, robustness and adaptability of the controller.},
  archive   = {C_IROS},
  author    = {Yixu Wang and Xiaoqing Guan and Tao Hu and Ziang Zhang and You Wang and Zhan Wang and Yifan Liu and Guang Li},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636425},
  pages     = {3242-3247},
  title     = {Fuzzy PID controller based on yaw angle prediction of a spherical robot},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Supervised autonomy for remote teleoperation of hybrid
wheel-legged mobile manipulator robots. <em>IROS</em>, 3234–3241. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes an improved supervised autonomy framework for remote teleoperation of a quadrupedal bimanual mobile manipulator in an unknown environment, with the usage of advanced perception technology and allowing the operator to easily assist the robot with decision making for executing tasks on the fly. First, the perception system uses lightweight deep neural network-based Single Shot Detector (SSD) MobileNet on RGB images to detect objects and highlight them to the human operator via an intuitive interactive visualization interface. After object and action selections are made by the operator, segmentation of object point cloud and 3D surfaces based on random sample consensus is performed, followed by object pose localization by using keypoint extraction. Based on the localized object, mobile manipulation motion to perform the operator-selected action is planned and executed with the help of a state estimator for the hybrid wheel-legged robot. Thanks to the autonomy of the robot in perception and manipulation, the complexity of teleoperating the robot is reduced to specifying the essential task objectives. Experimental results on the real robot, with full system integration, for 2 task scenarios, namely passage clearing and object retrieval, demonstrate a high average success rate of 92.2\% over a total of 90 trials.},
  archive   = {C_IROS},
  author    = {Samuel Cheong and Tai Pang Chen and Cihan Acar and Yangwei You and Yuda Chen and Wan Leong Sim and Keng Peng Tee},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635997},
  pages     = {3234-3241},
  title     = {Supervised autonomy for remote teleoperation of hybrid wheel-legged mobile manipulator robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Design optimization of musculoskeletal humanoids with
maximization of redundancy to compensate for muscle rupture.
<em>IROS</em>, 3227–3233. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Musculoskeletal humanoids have various biomimetic advantages, and the redundant muscle arrangement allowing for variable stiffness control is one of the most important. In this study, we focus on one feature of the redundancy, which enables the humanoid to keep moving even if one of its muscles breaks, an advantage that has not been dealt with in many studies. In order to make the most of this advantage, the design of muscle arrangement is optimized by considering the maximization of minimum available torque that can be exerted when one muscle breaks. This method is applied to the elbow of a musculoskeletal humanoid Musashi with simulations, the design policy is extracted from the optimization results, and its effectiveness is confirmed with the actual robot.},
  archive   = {C_IROS},
  author    = {Kento Kawaharazuka and Yasunori Toshimitsu and Manabu Nishiura and Yuya Koga and Yusuke Omura and Yuki Asano and Kei Okada and Koji Kawasaki and Masayuki Inaba},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636845},
  pages     = {3227-3233},
  title     = {Design optimization of musculoskeletal humanoids with maximization of redundancy to compensate for muscle rupture},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Momentum based whole-body optimal planning for a
single-spherical-wheeled balancing mobile manipulator. <em>IROS</em>,
3221–3226. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a planning and control framework for dynamic, whole-body motions for dynamically stable shape-accelerating mobile manipulators. This class of robots are inherently unstable and require careful coordination between the upper and lower body to maintain balance while performing arm motion tasks. Solutions to this problem either use a complex, full-body nonlinear dynamic model of the robot or a highly simplified model of the robot. Here we explore the use of centroidal dynamics which has recently become a popular approach for designing balancing controllers for humanoid robots. We describe a framework where we first solve a trajectory optimization problem offline. We define balancing for a ballbot in terms of the centroidal momentum instead of other approaches like ZMP or angular velocity that are more commonly used. The generated motion is tracked using a PD-PID cascading balancing controller for the body and torque controller for the arms. We demonstrate that this framework is capable of generating dynamic motion plans and control inputs with examples on the CMU ballbot, a single-spherical-wheeled balancing mobile manipulator.},
  archive   = {C_IROS},
  author    = {Roberto Shu and Ralph Hollis},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636752},
  pages     = {3221-3226},
  title     = {Momentum based whole-body optimal planning for a single-spherical-wheeled balancing mobile manipulator},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RRT-based path planning for follow-the-leader motion of
hyper-redundant manipulators. <em>IROS</em>, 3198–3204. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hyper-redundant manipulators with slender body and high dexterity are widely applied for operations in confined spaces. Among the motion planning methods for these operations, the follow-the-leader motion controller is generally developed to avoid the obstacles, while the path trajectories are usually given. In this paper, we present an autonomous motion planner with a specialized rapidly exploring random tree (Sp-RRT) approach for follow-the-leader motion of hyper-redundant manipulators. Starting from the target pose in the workspace, the exploring tree can expand to multiple entrances while guaranteeing the final pose of the manipulator’s end-effector. Meanwhile, the dexterity of hyper-redundant manipulators (even with different segments) can be utilized sufficiently with customized expanding parameters. Simulation results compared with existing methods are conducted to demonstrate the aforementioned characteristics and effectiveness. For further validation, we experimentally verify the development with our custom-built hyper-redundant manipulator to realize the generated path with follow-the-leader motion.},
  archive   = {C_IROS},
  author    = {Hanghang Wei and Yang Zheng and Guoying Gu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635876},
  pages     = {3198-3204},
  title     = {RRT-based path planning for follow-the-leader motion of hyper-redundant manipulators},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning symbolic operators for task and motion planning.
<em>IROS</em>, 3182–3189. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic planning problems in hybrid state and action spaces can be solved by integrated task and motion planners (TAMP) that handle the complex interaction between motion-level decisions and task-level plan feasibility. TAMP approaches rely on domain-specific symbolic operators to guide the task-level search, making planning efficient. In this work, we formalize and study the problem of operator learning for TAMP. Central to this study is the view that operators define a lossy abstraction of the transition model of a domain. We then propose a bottom-up relational learning method for operator learning and show how the learned operators can be used for planning in a TAMP system. Experimentally, we provide results in three domains, including long-horizon robotic planning tasks. We find our approach to substantially outperform several baselines, including three graph neural network-based model-free approaches from the recent literature. Video: https://youtu.be/iVfpX9BpBRo. Code: https://git.io/JCT0g},
  archive   = {C_IROS},
  author    = {Tom Silver and Rohan Chitnis and Joshua Tenenbaum and Leslie Pack Kaelbling and Tomás Lozano-Pérez},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635941},
  pages     = {3182-3189},
  title     = {Learning symbolic operators for task and motion planning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatial action maps augmented with visit frequency maps for
exploration tasks. <em>IROS</em>, 3175–3181. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning has been widely applied in exploration, navigation, manipulation, and other fields. Most of the relevant techniques generate kinematic commands (e.g., move, stop, turn) for agents based on the current state information. However, recent dense action representations based research, such as spatial action maps, pointing way-points to the agent in the same domain as its observation of the state shows great promise in mobile manipulation tasks. Inspired by that, we make the first step towards using a spatial action maps based method to effectively explore novel environmental spaces. To reduce the chance of redundant exploration, the visit frequency map (VFM) and its corresponding reward function are introduced to direct the agent to actively search previously unexplored areas. In the experimental section, our work was compared to the same method without VFM and the method based on traditional steering commands with the same input data in various environments. The results show conclusively that our method is more efficient than other methods. The project page is: https://github.com/zxwang96/sam-exploration},
  archive   = {C_IROS},
  author    = {Zixing Wang and Nikolaos Papanikolopoulos},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636813},
  pages     = {3175-3181},
  title     = {Spatial action maps augmented with visit frequency maps for exploration tasks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A general task and motion planning framework for multiple
manipulators. <em>IROS</em>, 3168–3174. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many manipulation tasks combine high-level discrete planning over actions with low-level motion planning over continuous robot motions. Task and motion planning (TMP) provides a powerful general framework to combine discrete and geometric reasoning, and solvers have been previously proposed for single-robot problems. Multi-robot TMP expands the range of TMP problems that can be solved but poses significant challenges when considering scalability and solution quality. We present a general TMP framework designed for multiple robotic manipulators. This is based on two contributions. First, we propose an optimal task planner designed to support simultaneous discrete actions. Second, we introduce an intermediate scheduler layer between task planner and motion planner to evaluate alternate robot assignments to these actions. This aggressively explores the search space and typically reduces the number of expensive task planning calls. Several benchmarks with a rich set of actions for two manipulators are evaluated. We show promising results in scalability and solution quality of our TMP framework with the scheduler for up to six objects. A demonstration indicates scalability to up to five robots.},
  archive   = {C_IROS},
  author    = {Tianyang Pan and Andrew M. Wells and Rahul Shome and Lydia E. Kavraki},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636119},
  pages     = {3168-3174},
  title     = {A general task and motion planning framework for multiple manipulators},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Intelligent execution through plan analysis. <em>IROS</em>,
3162–3167. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Intelligent robots need to generate and execute plans. In order to deal with the complexity of real environments, planning makes some assumptions about the world. When executing plans, the assumptions are usually not met. Most works have focused on the negative impact of this fact and the use of replanning after execution failures. Instead, we focus on the positive impact, or opportunities to find better plans. When planning, the proposed technique finds and stores those opportunities. Later, during execution, the monitoring system can use them to focus perception and repair the plan, instead of replanning from scratch. Experiments in several paradigmatic robotic tasks show how the approach outperforms standard replanning strategies.},
  archive   = {C_IROS},
  author    = {Daniel Borrajo and Manuela Veloso},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635833},
  pages     = {3162-3167},
  title     = {Intelligent execution through plan analysis},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Probabilistic inference in planning for partially observable
long horizon problems. <em>IROS</em>, 3154–3161. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For autonomous service robots to successfully perform long horizon tasks in the real world, they must act intelligently in partially observable environments. Most Task and Motion Planning approaches assume full observability of their state space, making them ineffective in stochastic and partially observable domains that reflect the uncertainties in the real world. We propose an online planning and execution approach for performing long horizon tasks in partially observable domains. Given the robot’s belief and a plan skeleton composed of symbolic actions, our approach grounds each symbolic action by inferring continuous action parameters needed to execute the plan successfully. To achieve this, we formulate the problem of joint inference of action parameters as a Hybrid Constraint Satisfaction Problem (H-CSP) and solve the H-CSP using Belief Propagation. The robot executes the resulting parameterized actions, updates its belief of the world and replans when necessary. Our approach is able to efficiently solve partially observable tasks in a realistic kitchen simulation environment. Our approach outperformed an adaptation of the state-of-the-art method across our experiments.},
  archive   = {C_IROS},
  author    = {Alphonsus Adu-Bredu and Nikhil Devraj and Pin-Han Lin and Zhen Zeng and Odest Chadwicke Jenkins},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636685},
  pages     = {3154-3161},
  title     = {Probabilistic inference in planning for partially observable long horizon problems},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Assembly planning by recognizing a graphical instruction
manual. <em>IROS</em>, 3138–3145. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a robot assembly planning method by automatically reading the graphical instruction manuals designed for humans. Essentially, the method generates an Assembly Task Sequence Graph (ATSG) by recognizing a graphical instruction manual. An ATSG is a graph describing the assembly task procedure by detecting types of parts included in the instruction images, completing the missing information automatically, and correcting the detection errors automatically. To build an ATSG, the proposed method first extracts the information of the parts contained in each image of the graphical instruction manual. Then, by using the extracted part information, it estimates the proper work motions and tools for the assembly task. After that, the method builds an ATSG by considering the relationship between the previous and following images, which makes it possible to estimate the undetected parts caused by occlusion using the information of the entire image series. Finally, by collating the total number of each part with the generated ATSG, the excess or deficiency of parts are investigated, and task procedures are removed or added according to those parts. In the experiment section, we build an ATSG using the proposed method to a graphical instruction manual for a chair and demonstrate the action sequences found in the ATSG can be performed by a dual-arm robot execution. The results show the proposed method is effective and simplifies robot teaching in automatic assembly.},
  archive   = {C_IROS},
  author    = {Issei Sera and Natsuki Yamanobe and Ixchel G. Ramirez-Alpizar and Zhenting Wang and Weiwei Wan and Kensuke Harada},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636041},
  pages     = {3138-3145},
  title     = {Assembly planning by recognizing a graphical instruction manual},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Prioritized indoor exploration with a dynamic deadline.
<em>IROS</em>, 3131–3137. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Indoor exploration using mobile robots has typically focused on exploring the entire environment without considering deadlines. This paper introduces a priority-based exploration algorithm for situations with an initially unknown and dynamically assigned deadline. The goal of our exploration strategy is to determine the geometric structure of an unknown environment as rapidly as possible and return to the home location. This is necessary for dangerous environments where an initial rapid robot exploration provides critical information about the layout for subsequent operations. For example, firefighters, for whom time is of the essence, can utilize the map generated by this robotic exploration to navigate a building on fire. We present a three-part strategy to solve this problem. First, we represent the physical environment as an exploration graph, whose vertices represent the local environment with its geometric and semantic information. Second, we assign priority values to these vertices based on their environment regions. Third, we present a graph exploration algorithm that employs the vertex priorities. Simulation experiments on a set of graph environments and Gazebo environments demonstrate that, in contrast to prior approaches that ignore the semantic information, our priority-based exploration algorithm enables the robot to efficiently explore more of the environment while satisfying its deadline constraints.},
  archive   = {C_IROS},
  author    = {Sayantan Datta and Srinivas Akella},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636199},
  pages     = {3131-3137},
  title     = {Prioritized indoor exploration with a dynamic deadline},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semantically informed next best view planning for autonomous
aerial 3D reconstruction. <em>IROS</em>, 3125–3130. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To capture the geometry of an object by an autonomous system, next best view (NBV) planning can be used to determine the path a robot will take. However, current NBV planning algorithms do not distinguish between objects that need to be mapped and everything else in the environment; leading to inefficient search strategies. In this paper we present a novel approach for NBV planning that accounts for the importance of objects in the environment to inform navigation. Using weighted entropy to encode object utilities computed via semantic segmentation, we evaluate our approach over a set of virtual Gazebo environments comparable to construction scales. Our results show that using semantic information reduces the time required to capture a target object by at least 40 percent.},
  archive   = {C_IROS},
  author    = {Sebastian A. Kay and Simon Julier and Vijay M. Pawar},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636352},
  pages     = {3125-3130},
  title     = {Semantically informed next best view planning for autonomous aerial 3D reconstruction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Topology-guided path planning for reliable visual navigation
of MAVs. <em>IROS</em>, 3117–3124. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual navigation has been widely used for state estimation of micro aerial vehicles (MAVs). For stable visual navigation, MAVs should generate perception-aware paths which guarantee enough visible landmarks. Many previous works on perception-aware path planning focused on sampling-based planners. However, they may suffer from sample inefficiency, which leads to computational burden for finding a global optimal path. To address this issue, we suggest a perception-aware path planner which utilizes topological information of environments. Since the topological class of a path and visible landmarks during traveling the path are closely related, the proposed algorithm checks distinctive topological classes to choose the class with abundant visual information. Topological graph is extracted from the generalized Voronoi diagram of the environment and initial paths with different topological classes are found. To evaluate the perception quality of the classes, we divide the initial path into discrete segments where the points in each segment share similar visual information. The optimal class with high perception quality is selected, and a graph-based planner is utilized to generate path within the class. With simulations and real-world experiments, we confirmed that the proposed method could guarantee accurate visual navigation compared with the perception-agnostic method while showing improved computational efficiency than the sampling-based perception-aware planner.},
  archive   = {C_IROS},
  author    = {Dabin Kim and Gyeong Chan Kim and Youngseok Jang and H. Jin Kim},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636469},
  pages     = {3117-3124},
  title     = {Topology-guided path planning for reliable visual navigation of MAVs},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semantic-aware active perception for UAVs using deep
reinforcement learning. <em>IROS</em>, 3101–3108. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents a semantic-aware path-planning pipeline for Unmanned Aerial Vehicles (UAVs) using deep reinforcement learning for vision-based navigation in challenging environments. Driven by the maturity of works in semantic segmentation, the proposed path-planning architecture uses reinforcement learning to distinguish the parts of the scene that are perceptually more informative using semantic cues, in effect guiding more robust, repeatable, and accurate navigation of the UAV to the predefined goal destination. Assuming that the UAV performs vision-based state estimation, such as keyframe-based visual odometry, and semantic segmentation onboard, the proposed deep policy network continuously evaluates the optimal relative perceptual informativeness of each semantic class in view. A perception-aware path planner uses these informativeness values to perform trajectory optimization in order to generate the next best action with respect to the current state and the perception quality of the surroundings, essentially guiding the UAV to avoid flying over perceptually degraded regions. Thanks to the use of semantic cues, the policy can be trained in a large number of non-photorealistic randomly-generated scenes, and results to an architecture that is generalizable to environments with the same semantic classes, independently of their visual appearance. Extensive evaluations on challenging, photorealistic simulations reveal a remarkable improvement in robustness and success rate with the proposed approach over the state of the art in active perception. Video – https://youtu.be/RaO3whUBVnc},
  archive   = {C_IROS},
  author    = {Luca Bartolomei and Lucas Teixeira and Margarita Chli},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635893},
  pages     = {3101-3108},
  title     = {Semantic-aware active perception for UAVs using deep reinforcement learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An optical spatial localization system for tracking unmanned
aerial vehicles using a single dynamic vision sensor. <em>IROS</em>,
3093–3100. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper reports a novel optical localization method, including both the hardware design and algorithm design, to track mobile Unmanned Aerial Vehicles (UAVs). The method relies on a circle-shaped blinking LED marker installed on the UAV and uses a single Dynamic Vision Sensing (DVS) camera to sense the temporal difference of the video streams. A temporal-filtering algorithm processes the video stream and detects the target marker by filtering out the background image. The triangulation-based spatial localization algorithm captures the trace of the target with the help of the prior knowledge of the physical size of the marker. The proposed system was evaluated in flight tests and compared with ground truth data provided by a motion capture system. The proposed system provides a simple and accurate localization solution for UAV tracking with a low computing overhead.},
  archive   = {C_IROS},
  author    = {Hunter Stuckey and Amer Al-Radaideh and Leonardo Escamilla and Liang Sun and Luis Garcia Carrillo and Wei Tang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636665},
  pages     = {3093-3100},
  title     = {An optical spatial localization system for tracking unmanned aerial vehicles using a single dynamic vision sensor},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SiamAPN++: Siamese attentional aggregation network for
real-time UAV tracking. <em>IROS</em>, 3086–3092. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, the Siamese-based method has stood out from multitudinous tracking methods owing to its state-of-the-art (SOTA) performance. Nevertheless, due to various special challenges in UAV tracking, e.g., severe occlusion and fast motion, most existing Siamese-based trackers hardly combine superior performance with high efficiency. To this concern, in this paper, a novel attentional Siamese tracker (SiamAPN++) is proposed for real-time UAV tracking. By virtue of the attention mechanism, we conduct a special attentional aggregation network (AAN) consisting of self-AAN and cross-AAN for raising the representation ability of features eventually. The former AAN aggregates and models the self-semantic interdependencies of the single feature map via spatial and channel dimensions. The latter aims to aggregate the cross-interdependencies of two different semantic features including the location information of anchors. In addition, the anchor proposal network based on dual features is proposed to raise its robustness of tracking objects with various scales. Experiments on two well-known authoritative benchmarks are conducted, where SiamAPN++ outperforms its baseline SiamAPN and other SOTA trackers. Besides, real-world tests onboard a typical embedded platform demonstrate that SiamAPN++ achieves promising tracking results with real-time speed.},
  archive   = {C_IROS},
  author    = {Ziang Cao and Changhong Fu and Junjie Ye and Bowen Li and Yiming Li},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636309},
  pages     = {3086-3092},
  title     = {SiamAPN++: Siamese attentional aggregation network for real-time UAV tracking},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DarkLighter: Light up the darkness for UAV tracking.
<em>IROS</em>, 3079–3085. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent years have witnessed the fast evolution and promising performance of the convolutional neural network (CNN)-based trackers, which aim at imitating biological visual systems. However, current CNN-based trackers can hardly generalize well to low-light scenes that are commonly lacked in the existing training set. In indistinguishable night scenarios frequently encountered in unmanned aerial vehicle (UAV) tracking-based applications, the robustness of the state-of-the-art (SOTA) trackers drops significantly. To facilitate aerial tracking in the dark through a general fashion, this work proposes a low-light image enhancer namely DarkLighter, which dedicates to alleviate the impact of poor illumination and noise iteratively. A lightweight map estimation network, i.e., ME-Net, is trained to efficiently estimate illumination maps and noise maps jointly. Experiments are conducted with several SOTA trackers on numerous UAV dark tracking scenes. Exhaustive evaluations demonstrate the reliability and universality of DarkLighter, with high efficiency. Moreover, DarkLighter has further been implemented on a typical UAV system. Real-world tests at night scenes have verified its practicability and dependability.},
  archive   = {C_IROS},
  author    = {Junjie Ye and Changhong Fu and Guangze Zheng and Ziang Cao and Bowen Li},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636680},
  pages     = {3079-3085},
  title     = {DarkLighter: Light up the darkness for UAV tracking},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FAST-dynamic-vision: Detection and tracking dynamic objects
with event and depth sensing. <em>IROS</em>, 3071–3078. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The development of aerial autonomy has enabled aerial robots to fly agilely in complex environments. However, dodging fast-moving objects in flight remains a challenge, limiting the further application of unmanned aerial vehicles (UAVs). The bottleneck of solving this problem is the accurate perception of rapid dynamic objects. Recently, event cameras have shown great potential in solving this problem. This paper presents a complete perception system including ego-motion compensation, object detection, and trajectory prediction for fast-moving dynamic objects with low latency and high precision. Firstly, we propose an accurate ego-motion compensation algorithm by considering both rotational and translational motion for more robust object detection. Then, for dynamic object detection, an event camera-based efficient regression algorithm is designed. Finally, we propose an optimization-based approach that asynchronously fuses event and depth cameras for trajectory prediction. Extensive real-world experiments and benchmarks are performed to validate our framework. Moreover, our code will be released to benefit related researches.},
  archive   = {C_IROS},
  author    = {Botao He and Haojia Li and Siyuan Wu and Dong Wang and Zhiwei Zhang and Qianli Dong and Chao Xu and Fei Gao},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636448},
  pages     = {3071-3078},
  title     = {FAST-dynamic-vision: Detection and tracking dynamic objects with event and depth sensing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploring consequential robot sound: Should we make robots
quiet and kawaii-et? <em>IROS</em>, 3056–3062. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {All robots create consequential sound—sound produced as a result of the robot’s mechanisms—yet little work has explored how sound impacts human-robot interaction. Recent work shows that the sound of different robot mechanisms affects perceived competence, trust, human-likeness, and discomfort. However, the physical sound characteristics responsible for these perceptions have not been clearly identified. In this paper, we aim to explore key characteristics of robot sound that might influence perceptions. A pilot study from our past work showed that quieter and higher-pitched robots may be perceived as more competent and less discomforting. To better understand how variance in these attributes affects perception, we performed audio manipulations on two sets of industrial robot arm videos within a series of four new studies presented in this paper. Results confirmed that quieter robots were perceived as less discomforting. In addition, higher-pitched robots were perceived as more energetic, happy, warm, and competent. Despite the robot’s industrial purpose and appearance, participants seemed to prefer more &quot;cute&quot; (or &quot;kawaii&quot;) sound profiles, which could have implications for the design of more acceptable and fulfilling sound profiles for human-robot interactions with practical collaborative robots.},
  archive   = {C_IROS},
  author    = {Brian J. Zhang and Knut Peterson and Christopher A. Sanchez and Naomi T. Fitter},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636365},
  pages     = {3056-3062},
  title     = {Exploring consequential robot sound: Should we make robots quiet and kawaii-et?},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Effects of conversational contexts and forms of non-lexical
backchannel on user perception of robots. <em>IROS</em>, 3042–3047. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A non-lexical backchannel is known to be dependent on the conversational context, and its form can be distinguished by the social relation between the speaker and the listener in the Korean language. Thus, to investigate the effect of a non-lexical backchannel, we conducted a 2 (context: information-centric versus emotion-centric) × 3 (forms of backchannel: &quot;ne&quot; versus &quot;eo&quot; versus &quot;eum&quot;) mixed-participant experiment (N = 96). After watching video stimuli, the participants evaluated a robot using one of the three forms of non-lexical backchannels under both contexts. No significant main effect of the forms of the non-lexical backchannel was found. By contrast, we found that task attraction and appropriateness of the robot were rated more positively under the information-centric context than under the emotion-centric context. The functions of the non-lexical backchannel of attentive listening and understanding were manifested more under the information-centric context than under the emotion-centric context. Furthermore, these functions mediated the effect of conversational context on the user perception of the robot in terms of task attraction and appropriateness.},
  archive   = {C_IROS},
  author    = {Sangmin Kim and Sukyung Seok and Jongsuk Choi and Yoonseob Lim and Sonya S. Kwak},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636589},
  pages     = {3042-3047},
  title     = {Effects of conversational contexts and forms of non-lexical backchannel on user perception of robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semantic-based explainable AI: Leveraging semantic scene
graphs and pairwise ranking to explain robot failures. <em>IROS</em>,
3034–3041. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When interacting in unstructured human environments, occasional robot failures are inevitable. When such failures occur, everyday people, rather than trained technicians, will be the first to respond. Existing natural language explanations hand-annotate contextual information from an environment to help everyday people understand robot failures. However, this methodology lacks generalizability and scalability. In our work, we introduce a more generalizable semantic explanation framework. Our framework autonomously captures the semantic information in a scene to produce semantically descriptive explanations for everyday users. To generate failure-focused explanations that are semantically grounded, we lever-ages both semantic scene graphs to extract spatial relations and object attributes from an environment, as well as pairwise ranking. Our results show that these semantically descriptive explanations significantly improve everyday users’ ability to both identify failures and provide assistance for recovery than the existing state-of-the-art context-based explanations.},
  archive   = {C_IROS},
  author    = {Devleena Das and Sonia Chernova},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635890},
  pages     = {3034-3041},
  title     = {Semantic-based explainable AI: Leveraging semantic scene graphs and pairwise ranking to explain robot failures},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mobile robot yielding cues for human-robot spatial
interaction. <em>IROS</em>, 3028–3033. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile robots are increasingly being deployed in public spaces such as shopping malls, airports, and urban sidewalks. Most of these robots are designed with human-aware motion planning capabilities but are not designed to communicate with pedestrians. Pedestrians encounter these robots without prior understanding of the robots’ behaviour, which can cause discomfort, confusion, and delayed social acceptance. In this research, we explore the common human-robot interaction at a doorway or bottleneck in a structured environment. We designed and evaluated communication cues used by a robot when yielding to a pedestrian in this scenario. We conducted an online user study with 102 participants using videos of a set of robot-to-human yielding cues. Results show that a Robot Retreating cue was the most socially acceptable cue. Repeated measures and Friedman’s ANOVAs on components of social acceptability were statistically significant (p = .01) and had small and medium effect sizes (η p 2 = .04, η p 2 = .08). The results of this work help guide the development of mobile robots for public spaces.},
  archive   = {C_IROS},
  author    = {Nicholas J. Hetherington and Ryan Lee and Marlene Haase and Elizabeth A. Croft and H. F. Machiel Van der Loos},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636367},
  pages     = {3028-3033},
  title     = {Mobile robot yielding cues for human-robot spatial interaction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ROS for human-robot interaction. <em>IROS</em>, 3020–3027.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Integrating real-time, complex social signal processing into robotic systems – especially in real-world, multi-party interaction situations – is a challenge faced by many in the Human-Robot Interaction (HRI) community. The difficulty is compounded by the lack of any standard model for human representation that would facilitate the development and interoperability of social perception components and pipelines. We introduce in this paper a set of conventions and standard interfaces for HRI scenarios, designed to be used with the Robot Operating System (ROS). It directly aims at promoting interoperability and re-usability of core functionality between the many HRI-related software tools, from skeleton tracking, to face recognition, to natural language processing. Importantly, these interfaces are designed to be relevant to a broad range of HRI applications, from high-level crowd simulation, to group-level social interaction modelling, to detailed modelling of human kinematics. We demonstrate these interfaces by providing a reference pipeline implementation, packaged to be easily downloaded and evaluated by the community.},
  archive   = {C_IROS},
  author    = {Youssef Mohamed and Séverin Lemaignan},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636816},
  pages     = {3020-3027},
  title     = {ROS for human-robot interaction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A transformable human-carrying wheel–leg mobility for daily
use. <em>IROS</em>, 3005–3011. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There is increasing demand for robots that provide a mode of transportation in environments in which people coexist. However, conventional mobile robots, especially those carrying people, are limited in terms of their environments and tasks. For example, wheeled robots are limited to moving on flat ground. Walking robots are limited to entertainment and so on. The originality of the present paper is the development of a novel movement mechanism for a mobility apparatus that can handle various daily use scenes. We first clarify functional requirements for daily use. We then propose a transformable human-carrying wheel–leg mobility. The leg is based on a serial link and compactly uses a parallel link mechanism such that the motor is placed on top, which improves responsiveness when having a high payload and compact shape. By conducting simulations and experiments with the prototype, it is confirmed that expected operations can be realized. In particular, it is confirmed that the weight of the leg tips is reduced, such that the shaking of the waist in the direction of travel during the ascent of a step is reduced by 68\%. The above results reveal that the prototype can be used in daily life.},
  archive   = {C_IROS},
  author    = {Noriaki Imaoka and Kohei Kimura and Shintaro Noda and Yohei Kakiuchi and Masayuki Inaba and Takeshi Ando},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636058},
  pages     = {3005-3011},
  title     = {A transformable human-carrying wheel–leg mobility for daily use},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Telemanipulation via virtual reality interfaces with
enhanced environment models. <em>IROS</em>, 2999–3004. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Extreme environments, such as search and rescue missions, defusing bombs, or exploring extraterrestrial planets, are unsafe environments for humans to be in. Robots enable humans to explore and interact in these environments through remote presence and teleoperation and virtual reality provides a medium to create immersive and easy-to-use teleoperation interfaces. However, current virtual reality interfaces are still very limited in their capabilities. In this work, we aim to advance robot teleoperation virtual reality interfaces by developing an environment reconstruction methodology capable of recognizing objects in a robot’s environment and rendering high fidelity models inside a virtual reality headset. We compare our proposed environment reconstruction method against traditional point cloud streaming by having operators plan waypoint trajectories to accomplish a pick-and-place task. Overall, our results show that users find our environment reconstruction method more usable and less cognitive work compared to raw point cloud streaming.},
  archive   = {C_IROS},
  author    = {Murphy Wonsick and Tarık Keleștemur and Stephen Alt and Tașkın Padır},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636005},
  pages     = {2999-3004},
  title     = {Telemanipulation via virtual reality interfaces with enhanced environment models},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generating active explicable plans in human-robot teaming.
<em>IROS</em>, 2993–2998. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Intelligent robots are redefining a multitude of critical domains but are still far from being fully capable of assisting human peers in day-to-day tasks. An important requirement of collaboration is for each teammate to maintain and respect an understanding of the others’ expectations of itself. Lack of which may lead to serious issues such as loose coordination between teammates, reduced situation awareness, and ultimately teaming failures. Hence, it is important for robots to behave explicably by meeting the human’s expectations. One of the challenges here is that the expectations of the human are often hidden and can change dynamically as the human interacts with the robot. However, existing approaches to generating explicable plans often assume that the human’s expectations are known and static. In this paper, we propose the idea of active explicable planning to relax this assumption. We apply a Bayesian approach to model and predict dynamic human belief and expectations to make explicable planning more anticipatory. We hypothesize that active explicable plans can be more efficient and explicable at the same time, when compared to explicable plans generated by the existing methods. In our experimental evaluation, we verify that our approach generates more efficient explicable plans while successfully capturing the dynamic belief change of the human teammate.},
  archive   = {C_IROS},
  author    = {Akkamahadevi Hanni and Yu Zhang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636643},
  pages     = {2993-2998},
  title     = {Generating active explicable plans in human-robot teaming},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-scenario contacts handling for collaborative robots
applications. <em>IROS</em>, 2985–2992. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The goal of this work is to propose a way of dealing with physical interactions for collaborative robots that will ensure the safety of a human operator and improve the performance of a common task by implementing multiple robot behavior scenarios. In this scope, all collisions of a robotic arm are detected and analyzed to chooses an appropriate reaction strategy. The points of contact on the robot’s surface for each collision are estimated, the external forces are identified and collisions are classified by the set of predefined categories. Based on these categories and the current robot state, the algorithm chose an appropriate behavior scenario.All presented algorithms are based only on proprioceptive sensors information and were tested in a simulated environment and on the real collaborative robots KUKA iiwa and Universal Robots UR10e. The result for contact localization showed 4 cm mean accuracy, the classification algorithm was able to identify collisions with hard and soft objects with 98\% accuracy for KUKA iiwa 14.},
  archive   = {C_IROS},
  author    = {Dmitry Popov and Stanislav Mikhel and Rauf Yagfarov and Alexandr Klimchik and Anatol Pashkevich},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636113},
  pages     = {2985-2992},
  title     = {Multi-scenario contacts handling for collaborative robots applications},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PiPo-net: A semi-automatic and polygon-based annotation
method for pathological images. <em>IROS</em>, 2978–2984. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Metastatic involvement of lymph nodes is one of the most important prognostic variables for many cancers. Several deep learning based algorithms have been developed to segment metastatic regions in pathological images to help predict prognosis. However, the training of these methods requires a large amount of annotated data, and the labeling task is an extremely time-consuming process for human annotators. In order to reduce the annotation burden, we for the first time propose a semi-automatic annotation method (PiPo-Net) for the labeling of pathological images. The method is comprised of two subnetworks, a pixel-wise segmentation network (Pi-Net) and a polygon-based annotation network (Po-Net). The Pi-Net adopts an improved encoder-decoder architecture and can effectively aggregate multi-scale image features. The Po-Net is built on the Pi-Net and leverages a two-layer recurrent neural network to generate tight-bounded polygons for the metastatic regions. Corresponding to the proposed network architecture, a loss function called PiPo-loss is introduced to help optimize the whole network. The main advantage of our method is that it integrates human annotators into the prediction loop, allowing to iteratively refine the predictions according to the suggestions from human annotators. We evaluate our method on Camelyon16 database and achieve a Dice score of 91\% in the initial annotation attempt. We also demonstrate the effectiveness of the human-network collaborative annotation, which achieves promising labeling results, verifying the advantages of our proposed method.},
  archive   = {C_IROS},
  author    = {Yuqi Fang and Delong Zhu and Niyun Zhou and Li Liu and Jianhua Yao},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636146},
  pages     = {2978-2984},
  title     = {PiPo-net: A semi-automatic and polygon-based annotation method for pathological images},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Non-local graph convolutional network for joint activity
recognition and motion prediction. <em>IROS</em>, 2970–2977. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D skeleton-based motion prediction and activity recognition are two interwoven tasks in human behaviour analysis. In this work, we propose a motion context modeling methodology that provides a new way to combine the advantages of both graph convolutional neural networks and recurrent neural networks for joint human motion prediction and activity recognition. Our approach is based on using an LSTM encoder-decoder and a non-local feature extraction attention mechanism to model the spatial correlation of human skeleton data and temporal correlation among motion frames. The proposed network can easily include two output branches, one for Activity Recognition and one for Future Motion Prediction, which can be jointly trained for enhanced performance. Experimental results on Human 3.6M, CMU Mocap and NTU RGB-D datasets show that our proposed approach provides the best prediction capability among baseline LSTM-based methods, while achieving comparable performance to other state-of-the-art methods.},
  archive   = {C_IROS},
  author    = {Dianhao Zhang and Ngo Anh Vien and Mien Van and Seán McLoone},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636107},
  pages     = {2970-2977},
  title     = {Non-local graph convolutional network for joint activity recognition and motion prediction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Human-robot collaboration for heavy object manipulation:
Kinesthetic teaching of the role of wheeled mobile manipulator.
<em>IROS</em>, 2962–2969. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human-robot collaboration (HRC) significantly extends robotic systems’ applications when working in spaces like houses, hospitals, or laboratories. However, new challenges appear during a close collaboration between humans and robots and imitating the movement of humans by robots. Learning from demonstration (LfD), or kinesthetic teaching, is a popular approach to help teach a robot human behavior by demonstrations without the need to explicitly reprogram the robot for different procedures. In this paper, we propose a method for object manipulation, including lifting, carrying, and lowering the object through a collaboration of a human with a wheeled mobile manipulator (WMM). The WMM is first trained with the help of a human demonstrator to collaborate with the user to execute the task. Then, the WMM will independently cooperate with the user by reproducing the learned skills to perform the same task. The redundancy of the WMM will also be employed to enhance its force exertion capability in the vertical direction to offset the object’s weight. The advantages and effectiveness of the proposed method are investigated through experiments.},
  archive   = {C_IROS},
  author    = {Hongjun Xing and Ali Torabi and Liang Ding and Haibo Gao and Weihua Li and Vivian K. Mushahwar and Mahdi Tavakoli},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635910},
  pages     = {2962-2969},
  title     = {Human-robot collaboration for heavy object manipulation: Kinesthetic teaching of the role of wheeled mobile manipulator},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A static model for a stiffness-adjustable snake-like robot.
<em>IROS</em>, 2956–2961. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In minimally invasive surgery, miniaturisation and in situ adjustable stiffness of robotic manipulators are desired features. Previous research proposed a simple and effective tendon-driven curve-joint manipulator design using a variable neutral-line mechanism, which highly satisfies both criteria. A kinematic model was developed for such a manipulator based on the geometry of the structure. However, such a model assumes that joint angles are all equal between disks without a rigorous derivation, and fails if not all the shapes of the disks are identical. Moreover, the model does not involve an analysis of the tension of each tendon. This paper suggested a static model for predicting the articulation of such a manipulator given the applied tensions on driving tendons. It validates the assumption of equally distributed joint angles and works for manipulators with more general configurations of disks and tendons. It also sets a foundation for further development of tension based control and external force estimation. Simulations on Adams were conducted to prove the correctness of the proposed model. A video demonstrating the simulation results can be found via https://youtu.be/MXhL1LGwLtw},
  archive   = {C_IROS},
  author    = {Di Shun Huang and Jian Hu and Liuchunzi Guo and Yi Sun and Liao Wu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636734},
  pages     = {2956-2961},
  title     = {A static model for a stiffness-adjustable snake-like robot},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards a manipulator system for disposal of waste from
patients undergoing chemotherapy. <em>IROS</em>, 2949–2955. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There has been an increasing demand to automate the non-patient care matters so that the clinical staff can focus on delivering patient care. For example, out-patients undergoing chemotherapy increases their toilet usage frequency due to the treatment. As they are undergoing chemotherapy, their output waste contains a level of chemical. This task is compulsory yet troublesome and time-consuming so it is often desired to be removed from the nursing staff for them to focus on patient care. Hence, in this paper, we propose a manipulator system to automatically dispose the bedpan used by patients undergoing chemotherapy. The main technical challenge lies in the removal of the bedpan from the commode as the interaction of the grasping is highly dynamic, along with the different conditions of the bedpans. To address this manipulation issue, a Residual Reinforcement Learning (RRL) method that leverages vision-based commode pose estimation and the reinforcement learning (RL)-based uncertainty compensation for improvement of the grasping accuracy is proposed to increase the robustness of the disposal. The experiments conducted show that the manipulator can dispose the bedpan without human intervention and the proposed method achieves a 100\% success rate while the traditional method without RL is only 50\%.},
  archive   = {C_IROS},
  author    = {Hsieh-Yu Li and Lay Siong Ho and Achala Athukorala and Wan Yun Lu and Audelia Dharmawan and Jane Li Feng Guo and Mabel May Leng Tan and Kok Cheong Wong and Nuri Syahida Ng and Maxim Mei Xin Tan and Hong Choon Oh and Daniel Tiang and Wei Wei Hong and Franklin Tan and Gek Kheng Png and Ivan Khoo and Chau Yuen and Pon Poh Hsu and Chen Ee Lee and U-Xuan Tan},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636865},
  pages     = {2949-2955},
  title     = {Towards a manipulator system for disposal of waste from patients undergoing chemotherapy},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust event detection based on spatio-temporal latent
action unit using skeletal information. <em>IROS</em>, 2941–2948. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a novel dictionary learning approach to detect event anomalities using skeletal information extracted from RGBD video. The event action is represented as several latent action atoms and composed of latent spatial and temporal attributes. We aim to construct a network able to learn from few examples and also rules defined by the user. The skeleton frames are clustered by an initial K-means method. Each skeleton frame is assigned with a varying weight parameter and fed into our Gradual Online Dictionary Learning (GODL) algorithm. During the training process, outlier frames will be gradually filtered by reducing the weight that is inversely proportional to a cost. To strictly distinguish the event action from similar actions and robustly acquire its action units, we build a latent unit temporal structure for each sub-action.We validate the method at the example of fall event detection on NTU RGB+D dataset, because it provides a benchmark available for comparison. We present the experimental validation of the achieved accuracy, recall, and precision. Our approach achieves the best performance in precision and accuracy of human fall event detection, compared with other existing dictionary learning methods. Our method remains the highest accuracy and the lowest variance, with increasing noise ratio.},
  archive   = {C_IROS},
  author    = {Hao Xing and Yuxuan Xue and Mingchuan Zhou and Darius Burschka},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636553},
  pages     = {2941-2948},
  title     = {Robust event detection based on spatio-temporal latent action unit using skeletal information},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A dual doctor-patient twin paradigm for transparent remote
examination, diagnosis, and rehabilitation. <em>IROS</em>, 2933–2940.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The need for comprehensive telemedicine solutions is becoming increasingly relevant due to challenges associated with the ageing population, the increasing shortage of health-care providers, and, more recently, the global pandemic. Existing solutions primarily focus on, e.g., electronic medical records, audiovisual connections, and, in some cases, robotic systems with very basic capabilities. Here we present a fundamentally new, holistic approach to a remote doctor visit, which enables transparent remote examination, anomaly detection, diagnosis, and rehabilitation. Our dual doctor-patient twin paradigm involves two robotic systems: one representing the doctor to the patient (&quot;GARMI&quot;) and one representing the patient to the doctor (&quot;MUCKI&quot;). Through bidirectional telepresence control, this system enables transparent, natural, remote haptic interaction between doctor and patient. The control, interaction, and knowledge transfer to the doctor is enhanced by AI-based visual motion and facial expression analysis as well as a digital twin of the patient. Thus, each stage of a doctor visit can be replicated in the context of telemedicine and shared autonomy: from first assessment to observation-based and remote physical examination, to a better-informed doctor diagnosis and robot-assisted telerehabilitation.},
  archive   = {C_IROS},
  author    = {Mario Tröbinger and Andrei Costinescu and Hao Xing and Jean Elsner and Tingli Hu and Abdeldjallil Naceri and Luis Figueredo and Elisabeth Jensen and Darius Burschka and Sami Haddadin},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636626},
  pages     = {2933-2940},
  title     = {A dual doctor-patient twin paradigm for transparent remote examination, diagnosis, and rehabilitation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic modeling of hand-object interactions via tactile
sensing. <em>IROS</em>, 2874–2881. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tactile sensing is critical for humans to perform everyday tasks. While significant progress has been made in analyzing object grasping from vision, it remains unclear how we can utilize tactile sensing to reason about and model the dynamics of hand-object interactions. In this work, we employ a high-resolution tactile glove to perform four different interactive activities on a diversified set of objects. We propose a framework aiming at predicting the 3d locations of both the hand and the object purely from the touch data by combining a predictive model and a contrastive learning module. This framework can reason about the interaction patterns from the tactile data, hallucinate the changes in the environment, esti-mate the uncertainty of the prediction, and generalize to unseen objects. We also provide detailed ablation studies regarding different system designs as well as visualizations of the predicted trajectories. This work takes a step on dynamics modeling in hand-object interactions from dense tactile sensing, which opens the door for future applications in activity learning, human-computer interactions, and imitation learning for robotics.},
  archive   = {C_IROS},
  author    = {Qiang Zhang and Yunzhu Li and Yiyue Luo and Wan Shou and Michael Foshey and Junchi Yan and Joshua B. Tenenbaum and Wojciech Matusik and Antonio Torralba},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636361},
  pages     = {2874-2881},
  title     = {Dynamic modeling of hand-object interactions via tactile sensing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A force recognition system for distinguishing click
responses of various objects. <em>IROS</em>, 2860–2865. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study proposes a technique for determining completion of robotic tasks considering a characteristics that some features on force responses are common among various objects. In particular, this paper focuses on the click response because its pattern is common to many objects, although the magnitude of the response differs depending on the object. A discriminator for detecting the click response based on the force sensor values, which combines mel-frequency cepstral coefficient and time-delay neural network is introduced. Based on this discriminator, how to improve the generalization performance to untrained objects is investigated to detect click responses in general for various objects. The experimental results show that the performance can be improved by including several kinds of objects with close time constants and different amplitudes of click responses in the training data.},
  archive   = {C_IROS},
  author    = {Koyo Sato and Sho Sakaino and Toshiaki Tsuji},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636174},
  pages     = {2860-2865},
  title     = {A force recognition system for distinguishing click responses of various objects},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Active visuo-tactile point cloud registration for accurate
pose estimation of objects in an unknown workspace. <em>IROS</em>,
2838–2844. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a novel active visuo-tactile based methodology wherein the accurate estimation of the time-invariant SE(3) pose of objects is considered for autonomous robotic manipulators. The robot equipped with tactile sensors on the gripper is guided by a vision estimate to actively explore and localize the objects in the unknown workspace. The robot is capable of reasoning over multiple potential actions, and execute the action to maximize information gain to update the current belief of the object. We formulate the pose estimation process as a linear translation invariant quaternion filter (TIQF) by decoupling the estimation of translation and rotation and formulating the update and measurement model in linear form. We perform pose estimation sequentially on acquired measurements using very sparse point cloud (≤ 15 points) as acquiring each measurement using tactile sensing is time consuming. Furthermore, our proposed method is computationally efficient to perform an exhaustive uncertainty-based active touch selection strategy in real-time without the need for trading information gain with execution time. We evaluated the performance of our approach extensively in simulation and by a robotic system.},
  archive   = {C_IROS},
  author    = {Prajval Kumar Murali and Michael Gentner and Mohsen Kaboli},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636877},
  pages     = {2838-2844},
  title     = {Active visuo-tactile point cloud registration for accurate pose estimation of objects in an unknown workspace},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A control and drive system for pneumatic soft robots:
PneuSoRD. <em>IROS</em>, 2822–2829. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This article describes an open-source hardware platform for controlling pneumatic soft robotic systems and presents the comparison of control schemes with on-off and proportional valves. The Pneumatic Soft Robotics Driver (PneuSoRD) can be used with up to one pump and pressure accumulator, 26 on-off valves, and 5 proportional valves, any of which can be operated in open or closed-loop control using up to 12 sensor inputs, which allows for the simultaneous control of a large number of soft actuators. The electronic driver connects to a National Instruments myRIO controller or an Arduino Due with the use of an adapter shield. A library of pressure control algorithms in both LabVIEW and Simulink is provided that includes bang-bang control, hysteresis control and PID control using on-off or proportional valves. LabVIEW and Simulink provide user-friendly interfaces for rapid prototyping of control algorithms and real-time evaluation of pressure dynamics. The characteristics and performance of these control methods and pneumatic setups are evaluated to simplify the choice of valves and control algorithm for a given application.},
  archive   = {C_IROS},
  author    = {Taylor R. Young and Matheus S. Xavier and Yuen K. Yong and Andrew J. Fleming},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635874},
  pages     = {2822-2829},
  title     = {A control and drive system for pneumatic soft robots: PneuSoRD},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamics computation of a hybrid multi-link humanoid robot
integrating rigid and soft bodies. <em>IROS</em>, 2816–2821. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study presents dynamics computation and control of a hybrid multi-link system that integrates rigid- and soft-bodies. It is a challenging problem to install a softness in a robot system, which is an important factor in human body. Softness achieved by human muscles and ligaments contributes to dynamic motion. Flexibility of a sports prosthetic leg allows a handicapped person to run. However, traditional algorithms of dynamics computation for a robot system or human skeletal model only consider a rigid-body multi-link system. Recent progress in soft robotics such as piecewise constant strain (PCS) model provides the way to compute dynamics of soft deformation with a low computational cost. We construct a hybrid multi-link system integrating rigid-body and the PCS model. For controlling a humanoid robot with soft links, we implement a dynamics computation with a floating-base and derive the center-of-gravity Jacobian matrix of the hybrid link system. Moreover, we demonstrate a forward dynamics simulation of a humanoid robot with prosthetic legs.},
  archive   = {C_IROS},
  author    = {Taiki Ishigaki and Ko Yamamoto},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636733},
  pages     = {2816-2821},
  title     = {Dynamics computation of a hybrid multi-link humanoid robot integrating rigid and soft bodies},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hybrid dual jacobian approach for autonomous control of
concentric tube robots in unknown constrained environments.
<em>IROS</em>, 2809–2815. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Concentric Tube Robots (CTR) have been gaining ground in minimally-invasive robotic surgeries due to their small footprint, compliance, and high dexterity. CTRs can assure safe interaction with soft tissue, provided that precise and effective motion control is achieved. Controlling the motion of CTRs is still challenging. Commonly used model-based control approaches often employ simplified geometric/dynamic assumptions, which could be very inaccurate in the presence of unmodelled disturbances and external interaction forces. Additionally, application of emerging data-driven algorithms in real-time control of CTRs is limited due to the fact that these controllers require considerable amount of time to let the algorithm develop enough to reach a desired accuracy and relevancy. In this paper, we present a hybrid approach to overcome the aforementioned difficulties. This hybrid solution uses the solution of a kinematic model of the robot to estimate initial values for a model-free data-driven method. The proposed algorithm combines both model-based and data-driven algorithms to provide real-time motion control of CTRs interacting with an unknown external environment. Three different simulations studies were performed to thoroughly evaluate the efficacy of the proposed hybrid control approach as compared to two common model-based and data-driven control techniques. The results demonstrate superior performance of the proposed method. The root-mean-square error of the proposed hybrid approach is less than 1.1 mm, which is 9 times less than a common model-based controller.},
  archive   = {C_IROS},
  author    = {Balint Thamo and Farshid Alambeigi and Kev Dhaliwal and Mohsen Khadem},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636085},
  pages     = {2809-2815},
  title     = {A hybrid dual jacobian approach for autonomous control of concentric tube robots in unknown constrained environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Manipulating a whip in 3D via dynamic primitives.
<em>IROS</em>, 2803–2808. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A prominent challenge in the field of robotics is manipulation of flexible objects. One major factor that makes this task difficult is the complex dynamics emerging from its high-dimensional structure. This argues against the use of popular optimization-based approaches, which scale poorly with system dimension (the &quot;curse of dimensionality&quot;). Nevertheless, almost indifferent to this complexity, humans handle it on a daily basis, without any apparent difficulty.Inspired by human motor control, we propose that encoding movements based on dynamic primitives can simplify the task of manipulating flexible objects and provides a way around the curse of dimensionality. Using an extreme example — manipulating a whip — we tested in simulation whether targets at various locations could be reached with a whip by using a controller based on dynamic primitives. Regardless of the target location, this approach successfully managed the complexity of a 54 degree-of-freedom system (yielding a 108-dimensional state-space representation) and identified an upper-limb movement that achieved the task. This approach did not require a detailed model of the whip, which thereby significantly simplified the computational complexity of the control task. We believe that this approach may facilitate robotic manipulation of flexible materials, and in general afford a simplified way to control dynamically complex objects.},
  archive   = {C_IROS},
  author    = {Moses C. Nah and Aleksei Krotov and Marta Russo and Dagmar Sternad and Neville Hogan},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636257},
  pages     = {2803-2808},
  title     = {Manipulating a whip in 3D via dynamic primitives},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Amplification of clamping mechanism using
internally-balanced magnetic unit. <em>IROS</em>, 2765–2771. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Machines tend to use powerful actuators and large gearboxes to bear large loads, which are inconvenient in terms of responsiveness as they affect the duration of operations. Thus, to compensate the force to grasp an object, we propose a clamping mechanism implementing the internally-balanced magnetic unit (IB Magnet) as a force amplifier, which is a mechanism able to switch attached and detached states of a permanent magnet with an external force considerably smaller than its original attractive force. To realize the bi-parting constitution of fingers, a new compensation method using conical coil springs was designed to provide both precision and miniaturization. Relative to the constitution with a single motored screw, the prototype gripper for proof of concept successfully amplified the grasping force at most to 292.2\% assisted by the magnetic attraction, while keeping the increase in power consumption of a DC motor only by 11.8\%, making the force-energy efficiency 2.6 times larger. Thus, it was verified that the proposed gripper enables the use of actuators and current supplies that require less power.},
  archive   = {C_IROS},
  author    = {Tori Shimizu and Kenjiro Tadakuma and Masahiro Watanabe and Eri Takane and Masashi Konyo and Satoshi Tadokoro},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636470},
  pages     = {2765-2771},
  title     = {Amplification of clamping mechanism using internally-balanced magnetic unit},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Coloured petri nets for monitoring human actions in flexible
human-robot teams. <em>IROS</em>, 2749–2756. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human-robot collaboration in shared workspaces enables companies to improve efficiency and the quality of work for human workers. A novel research direction in this field is that human and robot dynamically negotiate which actions to perform. This requires the robot to permanently monitor the current task state and actions the human has performed. We envision a system that tracks the task progress based on visual input. We assume that a task specification is provided in terms of a coloured Petri net. The contribution of this paper is twofold. We introduce the notion of emissions for coloured Petri nets that incorporate partial observability and uncertainty. We then show how one can efficiently determine the evolution of the net when a sequence of partial observations is provided. To this end, we determine a small set of transition as candidates in a firing sequence first. Then, transition sequences are sampled to obtain markings compatible with the latest observation. We evaluate our algorithm in a simulated environment on a pick and place task and compare it to an aging-based approach from literature. Results show that our algorithm achieves higher precision compared to the aging-based approach. Running times indicate that the update procedure can run at 15 Hz on average.},
  archive   = {C_IROS},
  author    = {Nico Höllerich and Dominik Henrich},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636428},
  pages     = {2749-2756},
  title     = {Coloured petri nets for monitoring human actions in flexible human-robot teams},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling and control of PANTHERA self-reconfigurable
pavement sweeping robot under actuator constraints. <em>IROS</em>,
2742–2748. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The focus of this paper is (i) to derive a suitable dynamic model for a self-reconfigurable pavement sweeping robot PANTHERA and (ii) to design a robust controller for the same to tackle uncertainties stemming from the reconfiguration process, external disturbances and from actuator saturation. To meet the first objective, an Euler-Lagrangian dynamic model is proposed to incorporate the effects of configuration changes on the system dynamics. Based on this model, the second objective is met via designing a singular perturbation based robust controller which can tackle the aforementioned uncertainties without violating the actuation limits. To circumvent the vulnerability toward actuator saturation, the proposed controller is built on contraction theory, which, compared to a conventional Lyapunov theory based design, allows to improve closed-loop tracking performance without reducing the singular perturbation parameter. Experimental results on the PANTHERA reconfigurable robot validate the effectiveness of the proposed controller over the state of the art.},
  archive   = {C_IROS},
  author    = {Madan Mohan Rayguru and M. R. Elara and A. A. Hayat and B. Ramalingam and Spandan Roy},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635841},
  pages     = {2742-2748},
  title     = {Modeling and control of PANTHERA self-reconfigurable pavement sweeping robot under actuator constraints},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Active exploration and mapping via iterative covariance
regulation over continuous SE(3) trajectories. <em>IROS</em>, 2735–2741.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper develops iterative Covariance Regulation (iCR), a novel method for active exploration and mapping for a mobile robot equipped with on-board sensors. The problem is posed as optimal control over the SE(3) pose kinematics of the robot to minimize the differential entropy of the map conditioned the potential sensor observations. We introduce a differentiable field of view formulation, and derive iCR via the gradient descent method to iteratively update an open-loop control sequence in continuous space so that the covariance of the map estimate is minimized. We demonstrate autonomous exploration and uncertainty reduction in simulated occupancy grid environments.},
  archive   = {C_IROS},
  author    = {Shumon Koga and Arash Asgharivaskasi and Nikolay Atanasov},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636486},
  pages     = {2735-2741},
  title     = {Active exploration and mapping via iterative covariance regulation over continuous SE(3) trajectories},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Model-based trajectory prediction and hitting velocity
control for a new table tennis robot. <em>IROS</em>, 2728–2734. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Currently, most table tennis robots concentrate on the canonical position control problem while ignoring the actual velocity control requirements. In this paper, we consider these requirements and propose a new table tennis robot framework. First, a tailor-made mechanical structure is designed such that the robot can reach large workspaces. Thereafter, in the table tennis trajectory prediction process, a clustering algorithm is introduced to screen the heterogeneous predicted hitting points and filter the invalid ones, thereby significantly improving the prediction accuracy. By using quintic polynomial trajectory planning, smooth and stable high-speed control of the robot hitting motion can be obtained. Finally, a position-based strategy and a velocity-based strategy are devised for returning the table tennis. Extensive experiments demonstrate that the accuracy of the ball&#39;s trajectory prediction algorithm is more than 92\%. The success rate of returning the ball exceeds 95\% at the ball velocity of 3-7 m/s, and the velocity-based strategy performs better compared with the position-based approach at the ball velocity of 7-9 m/s.},
  archive   = {C_IROS},
  author    = {Yunfeng Ji and Xiaoyi Hu and Yutao Chen and Yue Mao and Gang Wang and Qingdu Li and Jianwei Zhang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636000},
  pages     = {2728-2734},
  title     = {Model-based trajectory prediction and hitting velocity control for a new table tennis robot},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sensor fusion-based anthropomorphic control of
under-actuated bionic hand in dynamic environment. <em>IROS</em>,
2722–2727. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Under-actuated bionic hands have achieved tremendous popularity in many fields because of their advantages of lightweight, budget-friendly, satisfactory flexibility, and adaptability. Except for the bionic mechanical design, various anthropomorphic control strategies have been proposed and investigated in the last decades. However, due to its under-actuated characteristic, there are still many challenges for anthropomorphic control of all the degrees of freedom (DOFs) using less input. It is challenging to map the human hand kinematic synergies on robotic hands, particularly for a dynamic environment. Therefore, it is worth studying how to control the under-actuated bionic hand effectively in a dynamic environment. In this paper, an anthropomorphic control method is proposed using sensor fusion of hand kinematic inputs to control the under-actuated bionic hand. In order to map the kinematics of human fingers to the bionic hand, a novel finger bending angle is defined to represent the posture of human fingers. Multiple Leap Motion Controllers (LMC) are fused to estimate the stable and accurate finger bending angles to avoid the occlusion problem. Finally, experiments with real-time control of the under-actuated bionic hand are implemented to demonstrate the proposed approach’s effectiveness.},
  archive   = {C_IROS},
  author    = {Hang Su and Junhao Zhang and Junling Fu and Salih Ertug Ovur and Wen Qi and Guoxin Li and Yingbai Hu and Zhijun Li},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636436},
  pages     = {2722-2727},
  title     = {Sensor fusion-based anthropomorphic control of under-actuated bionic hand in dynamic environment},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NudgeSeg: Zero-shot object segmentation by repeated physical
interaction. <em>IROS</em>, 2714–2712. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in object segmentation have demonstrated that deep neural networks excel at object segmentation for specific classes in color and depth images. However, their performance is dictated by the number of classes and objects used for training, thereby hindering generalization to never seen objects or zero-shot samples. To exacerbate the problem further, object segmentation using image frames rely on recognition and pattern matching cues. Instead, we utilize the ‘active’ nature of a robot and their ability to ‘interact’ with the environment to induce additional geometric constraints for segmenting zero-shot samples.In this paper, we present the first framework to segment unknown objects in a cluttered scene by repeatedly ‘nudging’ at the objects and moving them to obtain additional motion cues at every step using only a monochrome monocular camera. We call our framework NudgeSeg. These motion cues are used to refine the segmentation masks. We successfully test our approach to segment novel objects in various cluttered scenes and provide an extensive study with image and motion segmentation methods. We show an impressive average detection rate of over 86\% on zero-shot objects.},
  archive   = {C_IROS},
  author    = {Chahat Deep Singh and Nitin J. Sanket and Chethan M. Parameshwara and Cornelia Fermüller and Yiannis Aloimonos},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635832},
  pages     = {2714-2712},
  title     = {NudgeSeg: Zero-shot object segmentation by repeated physical interaction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tactile slip detection in the wild leveraging distributed
sensing of both normal and shear forces. <em>IROS</em>, 2708–2713. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to detect that a grasped object is slipping from the robot gripper is a crucial skill for autonomous robotic manipulation. However, current solutions for automatic slip detection do not perform well in real-world unstructured settings, in which a wide variety of gripper-object interactions could occur. Tactile and force sensing are the most suitable sensory modalities to detect such events, and the recent technological advances in the field are generating novel interesting opportunities. In this work, we propose a data-driven method for automatic slip detection that leverages a novel sensor, which combines the advantages of tactile and force sensing, i.e. distributed measurements of normal and shear contact forces. Interestingly, our model is trained (and tested) uniquely with data obtained during routine robot operations (i.e. in the wild) rather than during a controlled data collection procedure. We compare different sets of tactile/force features to highlight the advantages provided by the different sensory modalities, and we report results that show good detection performances on our in-the-wild dataset, which we make publicly available.},
  archive   = {C_IROS},
  author    = {Rodrigo Zenha and Brice Denoun and Claudio Coppola and Lorenzo Jamone},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636602},
  pages     = {2708-2713},
  title     = {Tactile slip detection in the wild leveraging distributed sensing of both normal and shear forces},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Large-area conformable sensor for proximity, light touch,
and pressure-based gesture recognition. <em>IROS</em>, 2700–2707. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a capacitance-based sensor array for physical human-robot interaction (pHRI) applications that can measure the proximity, near-zero-force (NZF) contacts, and pressure between a robot and human body. The top segment including the electrodes is made of soft, stretchable materials, while the bottom segment consists of electrodes patterned from a thin copper film. The resulting device is soft and conformable to smooth curved surfaces of robot links while ensuring high signal integrity. It can be fabricated in different sizes from fingertips to torso because the fabrication process employs conventional, scalable methods. Using this sensor, we investigate the problem of recognizing gentle contact gestures often seen in affectionate physical interactions. The output of this multi-modal sensor is a 2D array compatible with machine learning algorithms used for pressure and image-based recognition problems. We utilize the spatio-temporal information of the 2D capacitance data by applying two existing deep neural network architectures. The highest accuracy achieved is over 99\% in 7-class recognition of contact gestures involving proximity, NZF contacts, and medium pressure.},
  archive   = {C_IROS},
  author    = {Mirza S. Sarwar and Katsu Yamane},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635943},
  pages     = {2700-2707},
  title     = {Large-area conformable sensor for proximity, light touch, and pressure-based gesture recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multimodal VAE active inference controller. <em>IROS</em>,
2693–2699. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Active inference, a theoretical construct inspired by brain processing, is a promising alternative to control artificial agents. However, current methods do not yet scale to high-dimensional inputs in continuous control. Here we present a novel active inference torque controller for industrial arms that maintains the adaptive characteristics of previous proprioceptive approaches but also enables large-scale multimodal integration (e.g., raw images). We extended our previous mathematical formulation by including multimodal state representation learning using a linearly coupled multimodal variational autoencoder. We evaluated our model on a simulated 7DOF Franka Emika Panda robot arm and compared its behavior with a previous active inference baseline and the Panda built-in optimized controller. Results showed improved tracking and control in goal-directed reaching due to the increased representation power, high robustness to noise and adaptability in changes on the environmental conditions and robot parameters without the need to relearn the generative models nor parameters retuning.},
  archive   = {C_IROS},
  author    = {Cristian Meo and Pablo Lanillos},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636394},
  pages     = {2693-2699},
  title     = {Multimodal VAE active inference controller},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robotic occlusion reasoning for efficient object existence
prediction. <em>IROS</em>, 2686–2692. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reasoning about potential occlusions is essential for robots to efficiently predict whether an object exists in an environment. Though existing work shows that a robot with active perception can achieve various tasks, it is still unclear if occlusion reasoning can be achieved. To answer this question, we introduce the task of robotic object existence prediction: when being asked about an object, a robot needs to move as few steps as possible around a table with randomly placed objects to predict whether the queried object exists. To address this problem, we propose a novel recurrent neural network model that can be jointly trained with supervised and reinforcement learning methods using a curriculum training strategy. Experimental results show that 1) both active perception and occlusion reasoning are necessary to successfully achieve the task; 2) the proposed model demonstrates a good occlusion reasoning ability by achieving a similar prediction accuracy to an exhaustive exploration baseline while requiring only about 10\% of the baseline’s number of movement steps on average; and 3) the model generalizes to novel object combinations with a moderate loss of accuracy.},
  archive   = {C_IROS},
  author    = {Mengdi Li and Cornelius Weber and Matthias Kerzel and Jae Hee Lee and Zheni Zeng and Zhiyuan Liu and Stefan Wermter},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635947},
  pages     = {2686-2692},
  title     = {Robotic occlusion reasoning for efficient object existence prediction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An adversarial objective for scalable exploration.
<em>IROS</em>, 2670–2677. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collecting new experience is costly in many robotic tasks, so determining how to efficiently explore in a new environment to learn as much as possible in as few trials as possible is an important problem for robotics. In this paper, we propose a method for exploring for the purpose of learning a dynamics model. Our key idea is to minimize a score given by a discriminator network as an objective for a planner which chooses actions. This discriminator is optimized jointly with a prediction model and enables our active learning approach to sample sequences of observations and actions which result in predictions considered the least realistic by the discriminator. Comparable existing exploration methods cannot operate in many prediction-planning pipelines used in robotic learning without hardware modifications to standard robotics platforms in order to accommodate their large compute requirements, so the primary contribution of our adversarial exploration method is scalability. We demonstrate progressively increased performance of our adversarial exploration approach compared to leading model-based exploration strategies as compute is restricted in simulated environments. We further demonstrate the ability of our adversarial method to scale to a robotic manipulation prediction-planning pipeline where we improve sample efficiency and prediction performance for a domain transfer problem.},
  archive   = {C_IROS},
  author    = {Bernadette Bucher and Karl Schmeckpeper and Nikolai Matni and Kostas Daniilidis},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636298},
  pages     = {2670-2677},
  title     = {An adversarial objective for scalable exploration},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Explaining the decisions of deep policy networks for robotic
manipulations. <em>IROS</em>, 2663–2669. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep policy networks enable robots to learn behaviors to solve various real-world complex tasks in an end-to-end fashion. However, they lack transparency to provide the reasons of actions. Thus, such a black-box model often results in low reliability and disruptive actions during the deployment of the robot in practice. To enhance its transparency, it is important to explain robot behaviors by considering the extent to which each input feature contributes to determining a given action. In this paper, we present an explicit analysis of deep policy models through input attribution methods to explain how and to what extent each input feature affects the decisions of the robot policy models. To this end, we present two methods for applying input attribution methods to robot policy networks: (1) we measure the importance factor of each joint torque to re ect the influence of the motor torque on the end-effector movement, and (2) we modify a relevance propagation method to handle negative inputs and outputs in deep policy networks properly. To the best of our knowledge, this is the first report to identify the dynamic changes of input attributions of multi-modal sensor inputs in deep policy networks online for robotic manipulation.},
  archive   = {C_IROS},
  author    = {Seongun Kim and Jaesik Choi},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636594},
  pages     = {2663-2669},
  title     = {Explaining the decisions of deep policy networks for robotic manipulations},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep leg tracking by detection and gait analysis in 2D range
data for intelligent robotic assistants. <em>IROS</em>, 2657–2662. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Online human leg tracking and gait analysis are crucial functionalities for mobility assistant robots, like intelligent walkers. Usually, such walkers are equipped with various sensors for the extraction of human-related features for adaptive human-robot interaction and assistance. We treat the gait detection problem jointly, presenting a novel method for detecting and recognizing gait features from 2D range data produced by a laser sensor mounted on a robotic walker. We propose an effective Convolutional Neural Network (CNN) as a powerful feature extractor for detecting the user’s leg centers in range data represented as occupancy grid maps. We couple the CNN with a Long Short Term Memory (LSTM) network for learning the legs’ motion temporal dynamics while walking, improving the prior detection, and providing better leg occlusion handling. Moreover, we perform gait analysis by recognizing gait phases over both legs by feeding the leg tracking output to a subsequent LSTM. Our proposed lightweight framework has been trained and tested on real patients-data. The presented experimental results show our method’s efficiency in providing accurate detections compared to state-of-the-art and application to an online system due to its high frequency, making it a competitive method for gait detection on robotic mobility assistants.},
  archive   = {C_IROS},
  author    = {Danai Efstathiou and Georgia Chalvatzaki and Athanasios Dometios and Dionisios Spiliopoulos and Costas S. Tzafestas},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636588},
  pages     = {2657-2662},
  title     = {Deep leg tracking by detection and gait analysis in 2D range data for intelligent robotic assistants},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Laser-based side-by-side following for human-following
robots. <em>IROS</em>, 2651–2656. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A mobile robot that follows behind humans in structured environments has to face the challenge of full occlusion caused by the walls when the target person makes a turn at the corridor intersections. This may result in short-term, even a permanent loss of the target from the field of view of the Human-Following Robots (HFRs). Concerning this issue, a novel side-by-side following method for HFRs is addressed. In this paper, HFRs detect the legs of target person and different types of corridor intersections using the onboard laser scanner at first. Then, we provide a corridor detector method to cluster the geometric structure constraint between the target and corridor intersections. At last, a Side-by-side Following Leg Tracker (SFLT) is designed by integrating the laser information, in order to increase the visible time of the target person, while the target is turning at the corridor intersections. The corridor detector method and SFLT method have been simulated in MATLAB. Moreover, the approach of side-by-side following has been implemented in the Robot Operating System (ROS) of real-life robots in the corridor environment. The results from simulation and practical experiment show that, by using our method, HFRs were able to successfully follow the human92.0\% while a mobile robot meeting potential occlusions at corridor intersections.},
  archive   = {C_IROS},
  author    = {Hanchen Yao and Houde Dai and Enhao Zhao and Penghua Liu and Ran Zhao},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636458},
  pages     = {2651-2656},
  title     = {Laser-based side-by-side following for human-following robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self attention guided depth completion using RGB and sparse
LiDAR point clouds. <em>IROS</em>, 2643–2650. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the problem of completing per pixel dense depth map using a single RGB image and the sparse point cloud of the scene. Depth prediction from RGB image is a hard problem and while dense point clouds obtained from LiDAR sensors can be used in addition to RGB image, the cost of such sensors is a significant barrier. Having LiDAR sensors which capture sparse point clouds is a reasonable middle ground. We propose a novel architecture which incorporates geometric primitives and self attention mechanisms, to improve the prediction. The motivation of self attention is to capture the correlations between scene and object elements, e.g. between the right and left window of car, early on in the network. While that for using geometric primitives is to have a high level clustering cue to enable the network to exploit similar correlations. In addition, we enforce complimentarity in the predictions made with RGB and sparse LiDAR respectively, this forces the two corresponding branches to focus on hard areas which are not already well predicted by the other branch. With exhaustive experiments on KITTI depth completion benchmark, NYU v2 and Matterport3D we show that the proposed method provides state-of-the-art results.},
  archive   = {C_IROS},
  author    = {Siddharth Srivastava and Gaurav Sharma},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636310},
  pages     = {2643-2650},
  title     = {Self attention guided depth completion using RGB and sparse LiDAR point clouds},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust and long-term monocular teach and repeat navigation
using a single-experience map. <em>IROS</em>, 2635–2642. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a robust monocular visual teach-and-repeat (VT&amp;R) navigation system for long-term operation in outdoor environments. The approach leverages deep-learned descriptors to deal with the high illumination variance of the real world. In particular, a tailored self-supervised descriptor, DarkPoint, is proposed for autonomous navigation in outdoor environments. We seamlessly integrate the localisation with control, in which proportional–integral control is used to eliminate the visual error with the pitfall of the unknown depth. Consequently, our approach achieves day-to-night navigation using a single-experience map and is able to repeat complex and fast manoeuvres. To verify our approach, we performed a vast array of navigation experiments in various outdoor environments, where both navigation accuracy and robustness of the proposed system are investigated. The experimental results show that our approach is superior to the baseline method with regards to accuracy and robustness.},
  archive   = {C_IROS},
  author    = {Li Sun and Marwan Taher and Christopher Wild and Cheng Zhao and Yu Zhang and Filip Majer and Zhi Yan and Tomáš Krajník and Tony Prescott and Tom Duckett},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635886},
  pages     = {2635-2642},
  title     = {Robust and long-term monocular teach and repeat navigation using a single-experience map},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Assembly action understanding from fine-grained hand
motions, a multi-camera and deep learning approach. <em>IROS</em>,
2628–2634. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This article presents a novel software architecture enabling the analysis of assembly actions from fine-grained hand motions. Unlike previous works that compel humans to wear ad-hoc devices or visual markers in the human body, our approach enables users to move without additional burdens. Modules developed are able to: (i) reconstruct the 3D motions of body and hands keypoints using multi-camera systems; (ii) recognize objects manipulated by humans, and (iii) analyze the relationship between the human motions and the manipulated objects. We implement different solutions based on OpenPose and Mediapipe for body and hand keypoint detection. Additionally, we discuss the suitability of these solutions for enabling real-time data processing. We also propose a novel method using Long Short-Term Memory (LSTM) deep neural networks to analyze the relationship between the detected human motions and manipulated objects. Experimental validations show the superiority of the proposed approach against previous works based on Hidden Markov Models (HMMs).},
  archive   = {C_IROS},
  author    = {Enrique Coronado and Kosuke Fukuda and Ixchel G. Ramirez-Alpizar and Natsuki Yamanobe and Gentiane Venture and Kensuke Harada},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636715},
  pages     = {2628-2634},
  title     = {Assembly action understanding from fine-grained hand motions, a multi-camera and deep learning approach},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-supervised scale recovery for monocular depth and
egomotion estimation. <em>IROS</em>, 2620–2627. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The self-supervised loss formulation for jointly training depth and egomotion neural networks with monocular images is well studied and has demonstrated state-of-the-art accuracy. One of the main limitations of this approach, however, is that the depth and egomotion estimates are only determined up to an unknown scale. In this paper, we present a novel scale recovery loss that enforces consistency between a known camera height and the estimated camera height, generating metric (scaled) depth and egomotion predictions. We show that our proposed method is competitive with other scale recovery techniques that require more information. Further, we demonstrate that our method facilitates network retraining within new environments, whereas other scale-resolving approaches are incapable of doing so. Notably, our egomotion network is able to produce more accurate estimates than a similar method which recovers scale at test time only.},
  archive   = {C_IROS},
  author    = {Brandon Wagstaff and Jonathan Kelly},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635938},
  pages     = {2620-2627},
  title     = {Self-supervised scale recovery for monocular depth and egomotion estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Monocular teach-and-repeat navigation using a deep steering
network with scale estimation. <em>IROS</em>, 2613–2619. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a novel monocular teach-and-repeat navigation system with the capability of scale awareness, i.e. the absolute distance between observation and goal images. It decomposes the navigation task into a sequence of visual servoing sub-tasks to approach consecutive goal/node images in a topological map. To be specific, a novel hybrid model, named deep steering network is proposed to infer the navigation primitives according to the learned local feature and scale for each visual servoing sub-task. A novel architecture, Scale-Transformer, is developed to estimate the absolute scale between the observation and goal image pair from a set of matched deep representations to assist repeating navigation. The experiments demonstrate that our scale-aware teach-and-repeat method achieves satisfying navigation accuracy, and converges faster than the monocular methods without scale correction given an inaccurate initial pose. The proposed network is integrated into an onboard system deployed on a real robot to achieve real-time navigation in a real environment. A demonstration video can be found online: https://youtu.be/ctlwDaMKnHw},
  archive   = {C_IROS},
  author    = {Cheng Zhao and Li Sun and Tomáš Krajník and Tom Duckett and Zhi Yan},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635912},
  pages     = {2613-2619},
  title     = {Monocular teach-and-repeat navigation using a deep steering network with scale estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Market-based multi-robot coordination with HTN planning.
<em>IROS</em>, 2606–2612. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a decentralized approach that simultaneously allocates and decomposes high level tasks among various robots. The approach exploits HTN structures and algorithms, that are used within an auction-based allocation scheme, and aims at dealing with complex tasks with causal or temporal relations. The paper formalizes the approach, and depicts how HTN planning processes are used to estimate bids and distribute tasks. Results on a statistical series of coverage problems are presented and their performance is assessed through a comparison with a state of the art algorithm.},
  archive   = {C_IROS},
  author    = {Antoine Milot and Estelle Chauveau and Simon Lacroix and Charles Lesire},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636286},
  pages     = {2606-2612},
  title     = {Market-based multi-robot coordination with HTN planning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Source seeking by dynamic source location estimation.
<em>IROS</em>, 2598–2605. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper focuses on the problem of multi-robot source-seeking, where a group of mobile sensors localizes and moves close to a single source using only local measurements. Drawing inspiration from the optimal sensor placement research, we develop an algorithm that estimates the source location while approaches the source following gradient descent steps on a loss function defined on the Fisher information. We show that exploiting Fisher information gives a higher chance of obtaining an accurate source location estimate and naturally leads the sensors to the source. Our numerical experiments demonstrate the advantages of our algorithm, including faster convergence to the source than other algorithms, flexibility in the choice of the loss function, and robustness to measurement modeling errors. Moreover, the performance improves as the number of sensors increases, showing the advantage of using multi-robots in our source-seeking algorithm. We also implement physical experiments to test the algorithm on small ground vehicles with light sensors, demonstrating success in seeking a moving light source.},
  archive   = {C_IROS},
  author    = {Tianpeng Zhang and Victor Qin and Yujie Tang and Na Li},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636841},
  pages     = {2598-2605},
  title     = {Source seeking by dynamic source location estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Desperate times call for desperate measures: Towards
risk-adaptive task allocation. <em>IROS</em>, 2592–2597. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-robot task allocation (MRTA) problems involve optimizing the allocation of robots to tasks. MRTA problems are known to be challenging when tasks require multiple robots and the team is composed of heterogeneous robots. These challenges are further exacerbated when we need to account for uncertainties encountered in the real-world. In this work, we address coalition formation in heterogeneous multi-robot teams with uncertain capabilities. We specifically focus on tasks that require coalitions to collectively satisfy certain minimum requirements. Existing approaches to uncertainty-aware task allocation either maximize expected pay-off (risk-neutral approaches) or improve worst-case or near-worst-case outcomes (risk-averse approaches). Within the context of our problem, we demonstrate the inherent limitations of unilaterally ignoring or avoiding risk and show that these approaches can in fact reduce the probability of satisfying task requirements. Inspired by models that explain foraging behaviors in animals, we develop a risk-adaptive approach to task allocation. Our approach adaptively switches between risk-averse and risk-seeking behavior in order to maximize the probability of satisfying task requirements. Comprehensive numerical experiments conclusively demonstrate that our risk-adaptive approach outperforms risk-neutral and risk-averse approaches. We also demonstrate the effectiveness of our approach using a simulated multi-robot emergency response scenario.},
  archive   = {C_IROS},
  author    = {Max Rudolph and Sonia Chernova and Harish Ravichandar},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635955},
  pages     = {2592-2597},
  title     = {Desperate times call for desperate measures: Towards risk-adaptive task allocation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Planning for aerial robot teams for wide-area biometric and
phenotypic data collection. <em>IROS</em>, 2586–2591. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents an efficient and implementable solution to the problem of joint task allocation and path planning in a multi-UAV platform. The sensing requirement associated with the task gives rise to an uncanny variant of the traditional vehicle routing problem with coverage/sensing constraints. As is the case in several multi-robot path-planning problems, our problem reduces to an mTSP problem. In order to tame the computational challenges associated with the problem, we propose a hierarchical solution that decouples the vehicle routing problem from the target allocation problem. As a tangible solution to the allocation problem, we use a clustering-based technique that incorporates temporal uncertainty in the cardinality and position of the robots. Finally, we implement the proposed techniques on our multi-quadcopter platforms.},
  archive   = {C_IROS},
  author    = {Shashwata Mandal and Tianshuang Gao and Sourabh Bhattacharya},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636549},
  pages     = {2586-2591},
  title     = {Planning for aerial robot teams for wide-area biometric and phenotypic data collection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-agent reinforcement learning for visibility-based
persistent monitoring. <em>IROS</em>, 2563–2570. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Visibility-based Persistent Monitoring (VPM) problem seeks to find a set of trajectories (or controllers) for robots to persistently monitor a changing environment. Each robot has a sensor, such as a camera, with a limited field-of-view that is obstructed by obstacles in the environment. The robots may need to coordinate with each other to ensure no point in the environment is left unmonitored for long periods of time. We model the problem such that there is a penalty that accrues every time step if a point is left unmonitored. However, the dynamics of the penalty are unknown to us. We present a Multi-Agent Reinforcement Learning (MARL) algorithm for the VPM problem. Specifically, we present a Multi-Agent Graph Attention Proximal Policy Optimization (MA-G-PPO) algorithm that takes as input the local observations of all agents combined with a low resolution global map to learn a policy for each agent. The graph attention allows agents to share their information with others leading to an effective joint policy. Our main focus is to understand how effective MARL is for the VPM problem. We investigate five research questions with this broader goal. We find that MA-G-PPO is able to learn a better policy than the non-RL baseline in most cases, the effectiveness depends on agents sharing information with each other, and the policy learnt shows emergent behavior for the agents.},
  archive   = {C_IROS},
  author    = {Jingxi Chen and Amrish Baskaran and Zhongshun Zhang and Pratap Tokekar},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635898},
  pages     = {2563-2570},
  title     = {Multi-agent reinforcement learning for visibility-based persistent monitoring},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Downing a rogue drone with a team of aerial radio signal
jammers. <em>IROS</em>, 2555–2562. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work proposes a novel distributed control framework in which a team of pursuer agents equipped with a radio jamming device cooperate in order to track and radio-jam a rogue target in 3D space, with the ultimate purpose of disrupting its communication and navigation circuitry. The target evolves in 3D space according to a stochastic dynamical model and it can appear and disappear from the surveillance area at random times. The pursuer agents cooperate in order to estimate the probability of target existence and its spatial density from a set of noisy measurements in the presence of clutter. Additionally, the proposed control framework allows a team of pursuer agents to optimally choose their radio transmission levels and their mobility control actions in order to ensure uninterrupted radio jamming to the target, as well as to avoid the jamming interference among the team of pursuer agents. Extensive simulation analysis of the system’s performance validates the applicability of the proposed approach.},
  archive   = {C_IROS},
  author    = {Savvas Papaioannou and Panayiotis Kolios and Georgios Ellinas},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636065},
  pages     = {2555-2562},
  title     = {Downing a rogue drone with a team of aerial radio signal jammers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Safe and fast path planner for minimally invasive surgery.
<em>IROS</em>, 2549–2554. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Path planning of a tool in Minimally Invasive Surgery (MIS) can provide assistance to the surgeons by giving solutions for faster and safe tool movements during the surgery. However, the main challenge in this problem is to address non-uniform tool shape for planning that can change due to the tool’s dexterity. A typical robotic path planning approach by describing the robot’s feasible movements using C-space is applied in this work. Unlike the robotic path planning problem, the C-space description capturing the movement does not give any closed-form solution due to high degree of freedom associated with the tool moved by human hands. Hence, an interval-based approach is used for describing the C-space. The proposed interval-based approach is capable of dividing the space into feasible and non-feasible intervals of different sizes which helps to reduce the search area and cover the obstacles in a refined manner. This paper presents collision-free and fast path computation using interval arithmetic between any two points in a 2D- surgical environment cluttered with obstacles for a surgical tool robot.},
  archive   = {C_IROS},
  author    = {Shubhangi Nema and Leena Vachhani},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636268},
  pages     = {2549-2554},
  title     = {Safe and fast path planner for minimally invasive surgery},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PlannerFlows: Learning motion samplers with normalising
flows. <em>IROS</em>, 2542–2548. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sampling-based motion planning is the predominant paradigm in many real-world robotic applications, but its performance is immensely dependent on the quality of the samples. The majority of traditional planners are inefficient as they use uninformative sampling distributions instead of exploiting structures and patterns in the problem to guide better sampling strategies. Moreover, most current learning-based planners are susceptible to posterior collapse or mode collapse due to the sparsity and highly varying nature of C-Space and motion plan configurations. This work introduces a conditional normalising flow-based distribution learned through previous experiences, which improves existing methods’ sampling scheme. Our distribution can be conditioned on the current problem instance to provide informative prior to sample configurations within promising regions. When we train our sampler with an expert planner, the resulting distribution is often near-optimal, and the planner can find a solution faster, with less invalid samples and less initial cost. The normalising flow-based distribution uses simple invertible transformations that are very computationally efficient, and our optimisation formulation explicitly avoids mode collapse in contrast to other existing learning-based sampler. Finally, we provide a formulation and theoretical foundation to sample from the distribution efficiently. Experimentally we demonstrate utilising the flow-based distribution in a sampling-based motion planner allows a solution to be found faster, with fewer samples and better overall runtime performance.},
  archive   = {C_IROS},
  author    = {Tin Lai and Fabio Ramos},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636190},
  pages     = {2542-2548},
  title     = {PlannerFlows: Learning motion samplers with normalising flows},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generation of human-like arm motions using sampling-based
motion planning. <em>IROS</em>, 2534–2541. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Natural and human-like arm motions are promising features to facilitate social understanding of humanoid robots. To this end, we integrate biophysical characteristics of human arm-motions into sampling-based motion planning. We show the generality of our method by evaluating it with multiple manipulators. Our first contribution is to introduce a set of cost functions to optimize for human-like arm postures during collision-free motion planning. In a subsequent step, an optimization phase is used to improve the human-likeness of the initial path. Additionally, we present an interpolation approach for generating obstacle-aware and multi-modal velocity profiles. We thus generate collision-free and human-like motions in narrow passages while allowing for natural acceleration in free space.},
  archive   = {C_IROS},
  author    = {Carl Gäbert and Sascha Kaden and Ulrike Thomas},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636068},
  pages     = {2534-2541},
  title     = {Generation of human-like arm motions using sampling-based motion planning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HyperPlan: A framework for motion planning algorithm
selection and parameter optimization. <em>IROS</em>, 2511–2518. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Over the years, many motion planning algorithms have been proposed. It is often unclear which algorithm might be best suited for a particular class of problems. The problem is compounded by the fact that algorithm performance can be highly dependent on parameter settings. This paper shows that hyperparameter optimization is an effective tool in both algorithm selection and parameter tuning over a given set of motion planning problems. We present different loss functions for optimization that capture different notions of optimality. The approach is evaluated on a broad range of scenes using two different manipulators, a Fetch and a Baxter. We show that optimized planning algorithm performance significantly improves upon baseline performance and generalizes broadly in the sense that performance improvements carry over to problems that are very different from the ones considered during optimization.},
  archive   = {C_IROS},
  author    = {Mark Moll and Constantinos Chamzas and Zachary Kingston and Lydia E. Kavraki},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636651},
  pages     = {2511-2518},
  title     = {HyperPlan: A framework for motion planning algorithm selection and parameter optimization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interpretable run-time prediction and planning in co-robotic
environments. <em>IROS</em>, 2504–2510. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile robots are traditionally developed to be reactive and avoid collisions with surrounding humans, often moving in unnatural ways without following social protocols, forcing people to behave very differently from human-human interaction rules. Humans, on the other hand, are seamlessly able to understand why they may interfere with surrounding humans and change their behavior based on their reasoning, resulting in smooth, intuitive avoiding behaviors. In this paper, we propose an approach for a mobile robot to avoid interfering with the desired paths of surrounding humans. We leverage a library of previously observed trajectories to design a decision-tree based interpretable monitor that: i) predicts whether the robot is interfering with surrounding humans, ii) explains what behaviors are causing either prediction, and iii) plans corrective behaviors if interference is predicted. We also propose a validation scheme to improve the predictive model at run-time. The proposed approach is validated with simulations and experiments involving an unmanned ground vehicle (UGV) performing go-to-goal operations in the presence of humans, demonstrating non-interfering behaviors and run-time learning.},
  archive   = {C_IROS},
  author    = {Rahul Peddi and Nicola Bezzo},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636282},
  pages     = {2504-2510},
  title     = {Interpretable run-time prediction and planning in co-robotic environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mechanical design and evaluation of a selectively-actuated
MRI-compatible continuum neurosurgical robot. <em>IROS</em>, 2498–2503.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The combination of a dexterous continuum robot and magnetic resonance imaging can potentially improve surgical precision and minimize brain manipulation in a minimally invasive neurosurgical procedure. In this work, a seven degree-of-freedom (DoF) continuum neurosurgical robot was developed. The main innovation lies in the design of a safe and robust switching mechanism and gear-based quick-connect mechanism that, respectively, allow selective actuation of the 6-DoF end effector using only three motors and highly efficient end effector exchange. Its performance has been validated in experiments involving multi-segment dexterous motion. We also evaluated the robotic system on a human cadaver head in a clinical 3-Tesla MRI. The entire workflow of robotic system set-up was implemented, confirming its clinical feasibility. The signal-to-noise ratio (SNR) drop was consistently less than 6\% throughout various stages of end effector motion.},
  archive   = {C_IROS},
  author    = {Shing Shin Cheng and Xuefeng Wang and Seokhwan Jeong and Matt Kole and Steven Roys and Rao P. Gullapalli},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636128},
  pages     = {2498-2503},
  title     = {Mechanical design and evaluation of a selectively-actuated MRI-compatible continuum neurosurgical robot},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HanGrawler 2: Super-high-speed and large-payload ceiling
mobile robot using crawler. <em>IROS</em>, 2491–2497. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ceiling mobile robots are anticipated for trans-porting component parts in production sites. In our previous study, we had developed a crawler-type ceiling mobile robot named &quot;HanGrawler.&quot; In this study, we aim to realize 1 m/s and 90°/s speed movement on par with existing ground carriers, and reveal the factors contributing to the improvement in speed and stabilization. Accordingly, we develop HanGrawler 2, which inherits the basic approach of HanGrawler. In HanGrawler 2, the mechanism for hanging from the ceiling moves faster, and the driving power of the linear and rotational movements is improved drastically. Newly installed sensors detect the hanging mechanism and robot body position relative to the ceiling plate. By calculating the insertion timing based on the position of the hanging mechanism, the certainty of insertion is improved. Support rollers pressed against the ceiling stabilize the traveling posture of HanGrawler 2. Performance evaluation experiments confirm that HanGrawler 2 can travel linearly at a speed of 1 m/s with a 40 kg load and turn at a rotational speed of 90°/s. Based on the experimental results, it is confirmed that the design solutions for HanGrawler 2 are effective in realizing reliable ceiling mobility and that the high-responsiveness inherited from the original HanGrawler mechanism contributes to the realization of high-speed movement.},
  archive   = {C_IROS},
  author    = {Takehito Yoshida and Yudai Yamada and Shin’Ichi Warisawa and Rui Fukui},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636228},
  pages     = {2491-2497},
  title     = {HanGrawler 2: Super-high-speed and large-payload ceiling mobile robot using crawler},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A highly maneuverable hybrid energy-efficient rolling/flying
system. <em>IROS</em>, 2485–2490. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Spherical robots are typically comprised of an actuation unit enclosed by a spherical shell. Among nonholonomic systems, spherical robots offer the best maneuverability and lowest energy consumption (due to their omnidirectional movement and single contact point with the ground). This allows them to traverse rough and uneven terrains. Further, using their ability to roll on the ground, they can provide a significantly higher operating time compared to aerial-only robots. Unfortunately, these robots are under-emphasized by researchers compared to other robots (i.e., legged or wheeled robots). Additionally, despite their potential to be used in a multitude of real-world applications, spherical robots have not been successfully adopted by the industry. This is due to the lack of controllability and traversability of the developed designs. In this paper, we introduce a hybrid rolling/flying robot. This design benefits from a flywheel to reduce the effects of the terrain (shocks and vibrations) on the camera and sensors. Our design allows the application of existing control algorithms of drones (such as PX4) on a rolling system. In addition, we propose a dynamics model that can use the point cloud representation of the terrain to simulate the motion of the system with applications in real-time modeling and control.},
  archive   = {C_IROS},
  author    = {Sahand Sabet and Mohit Singh and Mohammad Poursina and Parviz E. Nikravesh},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636811},
  pages     = {2485-2490},
  title     = {A highly maneuverable hybrid energy-efficient Rolling/Flying system},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling and analysis of tensegrity robot for passive
dynamic walking. <em>IROS</em>, 2479–2484. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a planar tensegrity robot that walks passively and cyclically on a gentle downhill, where its gait versatility can be strengthened by applying actuation forces on the connection cables. The novelty of this work is that we design the structure of this passive robot inspired by the rimless wheel, which naturally generates cyclic locomotion. Consequently, its mathematical model is analytically derived based on passive dynamic walking. Besides, the limb support conditions and dynamics effects induced by the collisions can be precisely determined accordingly. Moreover, numerical simulation is performed to show the typical gait pattern, and resonance phenomenon is observed. Finally, a preliminary experimental study is conducted to prove the validity of the mathematical model. The robot we developed and the mathematical model we derived enable further extensions on the gait analysis and model-based control by conveniently adopting efficient passivemimic walking techniques.},
  archive   = {C_IROS},
  author    = {Yanqiu Zheng and Longchuan Li and Fumihiko Asano and Cong Yan and Xindi Zhao and Haosong Chen},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636633},
  pages     = {2479-2484},
  title     = {Modeling and analysis of tensegrity robot for passive dynamic walking},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modular pipe climber III with three-output open
differential. <em>IROS</em>, 2473–2478. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The paper introduces the novel Modular Pipe Climber III with a Three-Output Open Differential (3-OOD) mechanism to eliminate slipping of the tracks due to the changing cross-sections of the pipe. This will be achieved in any orientation of the robot. Previous pipe climbers use three-wheel/track modules, each with an individual driving mechanism to achieve stable traversing. Slipping of tracks is prevalent in such robots when it encounters the pipe turns. Thus, active control of each module’s speed is employed to mitigate the slip, thereby requiring substantial control effort. The proposed pipe climber implements the 3-OOD to address this issue by allowing the robot to mechanically modulate the track speeds as it encounters a turn. The proposed 3-OOD is the first three-output differential to realize the functional abilities of a traditional two-output differential.},
  archive   = {C_IROS},
  author    = {Rama Vadapalli and Saharsh Agarwal and Vishnu Kumar and Kartik Suryavanshi and Nagamanikandan G and K Madhava Krishna},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635853},
  pages     = {2473-2478},
  title     = {Modular pipe climber III with three-output open differential},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Design and analysis of a robotic out-pipe grinding system
with friction actuating. <em>IROS</em>, 2467–2472. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To cope with the requirements on efficiency and labour-saving of the out-pipe surface grinding tasks in the wild, several proposals are revealed and discussed. The one benefiting from the characteristics of planetary gear transmission and friction actuating mechanism are expatiated. To realize full coverage of out-pipe surface, the self-rotation and revolution motions of every polishing tool (cutter) are actuated by the same motor, and the friction force produced in grinding process acts as suitable tractive force for the forward travel of the grinding system. The friction statics analysis is established to illustrate the force transmission. Compression spring system are utilized to realize force equilibrium and support passive diameter adaptability. The proposed robotic grinding system is characterized by less actuator, online grinding capability and high working efficiency. It has clear advantages regarding manufacturing costs and control complexity. As the result of prototype experiments, performance of smooth grinding the out-pipe surface is confirmed.},
  archive   = {C_IROS},
  author    = {Mingyuan Wang and Sheng Bao and Jianjun Yuan and Shugen Ma and Shijie Guo and Weiwei Wan},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636189},
  pages     = {2467-2472},
  title     = {Design and analysis of a robotic out-pipe grinding system with friction actuating},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BogieBot: A climbing robot in cluttered confined space of
bogies with ferrous metal surfaces. <em>IROS</em>, 2459–2466. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Proactive inspection is essential for prediction and prevention of rolling stock component failures. The conventional process for inspecting bogies under trains presents significant challenges for inspectors who need to visually check the tight and cluttered environment. We propose a miniature multi-link climbing robot, called BogieBot, that can be deployed inside the undercarriage areas of trains and other large vehicles for inspection and maintenance purposes. BogieBot can carry a visual sensor or manipulator on its main body. The novel compact design utilises six identical couple joints and two mechanically switchable magnetic grippers that together, empower multi-modal climbing and manipulation. The proposed mechanism is kinematically redundant, allowing the robot to perform self-motions in a tight space and manoeuvre around obstacles. The mechanism design and various analyses on the forward and inverse kinematic, work-space, and self-motions of BogieBot are presented. The robot is demonstrated to perform challenging navigation tasks in different scenarios involving simulated complex environments.},
  archive   = {C_IROS},
  author    = {Mohammad Adinehvand and Ehsan Asadi and Chow Y. Lai and Hamid Khayyam and Kevin Tan and Reza Hoseinnezhad},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636412},
  pages     = {2459-2466},
  title     = {BogieBot: A climbing robot in cluttered confined space of bogies with ferrous metal surfaces},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised monocular depth learning with integrated
intrinsics and spatio-temporal constraints. <em>IROS</em>, 2451–2458.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Monocular depth inference has gained tremendous attention from researchers in recent years and remains as a promising replacement for expensive time-of-flight sensors, but issues with scale acquisition and implementation overhead still plague these systems. To this end, this work presents an unsupervised learning framework that is able to predict at-scale depth maps and egomotion, in addition to camera intrinsics, from a sequence of monocular images via a single network. Our method incorporates both spatial and temporal geometric constraints to resolve depth and pose scale factors, which are enforced within the supervisory reconstruction loss functions at training time. Only unlabeled stereo sequences are required for training the weights of our single-network architecture, which reduces overall implementation overhead as compared to previous methods. Our results demonstrate strong performance when compared to the current state-of-the-art on multiple sequences of the KITTI driving dataset and can provide faster training times with its reduced network complexity.},
  archive   = {C_IROS},
  author    = {Kenny Chen and Alexandra Pogue and Brett T. Lopez and Ali-Akbar Agha-Mohammadi and Ankur Mehta},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636030},
  pages     = {2451-2458},
  title     = {Unsupervised monocular depth learning with integrated intrinsics and spatio-temporal constraints},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual identification of articulated object parts.
<em>IROS</em>, 2443–2450. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As autonomous robots interact and navigate around real-world environments such as homes, it is useful to reliably identify and manipulate articulated objects, such as doors and cabinets. Many prior works in object articulation identification require manipulation of the object, either by the robot or a human. While recent works have addressed predicting articulation types from visual observations alone, they often assume prior knowledge of category-level kinematic motion models or sequence of observations where the articulated parts are moving according to their kinematic constraints. In this work, we propose FormNet, a neural network that identifies the articulation mechanisms between pairs of object parts from a single frame of an RGB-D image and segmentation masks. The network is trained on 100k synthetic images of 149 articulated objects from 6 categories. Synthetic images are rendered via a photorealistic simulator with domain randomization. Our proposed model predicts motion residual flows of object parts, and these flows are used to determine the articulation type and parameters. The network achieves an articulation type classification accuracy of 82.5\% on novel object instances in trained categories. Experiments also show how this method enables generalization to novel categories and can be applied to real-world images without fine-tuning.},
  archive   = {C_IROS},
  author    = {Vicky Zeng and Tabitha Edith Lee and Jacky Liang and Oliver Kroemer},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636054},
  pages     = {2443-2450},
  title     = {Visual identification of articulated object parts},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast image-anomaly mitigation for autonomous mobile robots.
<em>IROS</em>, 2436–2442. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Camera anomalies like rain or dust can severely degrade image quality and its related tasks, such as localization and segmentation. In this work we address this important issue by implementing a pre-processing step that can effectively mitigate such artifacts in a real-time fashion, thus supporting the deployment of autonomous systems with limited compute capabilities. We propose a shallow generator with aggregation, trained in an adversarial setting to solve the ill-posed problem of reconstructing the occluded regions. We add an enhancer to further preserve high-frequency details and image colorization. We also produce one of the largest publicly available datasets 1 to train our architecture and use realistic synthetic raindrops to obtain an improved initialization of the model. We benchmark our framework on existing datasets and on our own images obtaining state-of-the-art results while enabling real-time performance, with up to 40x faster inference time than existing approaches.},
  archive   = {C_IROS},
  author    = {Gianmario Fumagalli and Yannick Huber and Marcin Dymczyk and Roland Siegwart and Renaud Dubé},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635855},
  pages     = {2436-2442},
  title     = {Fast image-anomaly mitigation for autonomous mobile robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Automatic learning system for object function points from
random shape generation and physical validation. <em>IROS</em>,
2428–2435. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we aim to recognize function points of category-agnostic objects and perform object manipulation. To recognize function points of various shapes, it is necessary to train with a large amount of training data. Also, it is necessary to take into account not only visual information but also physics and interaction between objects. To solve these problems, we are working on the automatic generation of training data by detecting function points from a physical simulation. In the proposed system, we add simulation with target task operation and goal state, which allows a robot to acquire the target function point recognizer. We also use GAN to generate various random shapes and render them with random domains, and train Deep Neural Networks on these data. These enable the robot to recognize function points of unseen objects in the real world and realize manipulation.},
  archive   = {C_IROS},
  author    = {Kosuke Takeuchi and Iori Yanokura and Yohei Kakiuchi and Kei Okada and Masayuki Inaba},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635895},
  pages     = {2428-2435},
  title     = {Automatic learning system for object function points from random shape generation and physical validation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bootstrapped self-supervised training with monocular video
for semantic segmentation and depth estimation. <em>IROS</em>,
2420–2427. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For a robot deployed in the world, it is desirable to have the ability of autonomous learning to improve its initial pre-set knowledge. We formalize this as a bootstrapped self-supervised learning problem where a system is initially bootstrapped with supervised training on a labeled dataset and we look for a self-supervised training method that can subsequently improve the system over the supervised training baseline using only unlabeled data. In this work, we leverage temporal consistency between frames in monocular video to per-form this bootstrapped self-supervised training. We show that a well-trained state-of-the-art semantic segmentation network can be further improved through our method. In addition, we show that the bootstrapped self-supervised training framework can help a network learn depth estimation better than pure supervised training or self-supervised training.},
  archive   = {C_IROS},
  author    = {Yihao Zhang and John J. Leonard},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636330},
  pages     = {2420-2427},
  title     = {Bootstrapped self-supervised training with monocular video for semantic segmentation and depth estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scene descriptor expressing ambiguity in information
recovery based on incomplete partial observation. <em>IROS</em>,
2414–2419. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent studies, the widespread of deep learning has made many kinds of large-scale image datasets available and it has enabled to improve the performance of image-based 3-D scene reconstruction. Several studies estimate whole 3-D scenes including occluded or unseen parts consistent with the obtained partial observations by integrating prior knowledge from training datasets with them, under no camera parameters nor image landmark correspondence are known. Although they generate “discrete” scene instances, they cannot represent and treat their “ambiguity” at all.This paper proposes a novel deep-learning-based framework that can directly represent and treat the ambiguity of scene reconstructions. We introduce a neural network which encodes a target scene as a descriptor. The network takes partial observations as input and outputs a parametric set of the scene descriptors containing all scenes consistent with given observations. The input observations may be “incomplete” in the sense that they do not have enough pieces of information to uniquely determine the whole scene due to neither geometry in-formation nor landmark correspondences available (ill-defined cases). The network is trained based on the dataset of the complete 3-D scenes and possible partial observations so that it can predict the unseen parts from incomplete observations. The paper introduces the method to induce such a descriptor space into the encoder/decoder architecture by employing novel definitions of loss functions measuring “validity”, “consistency” and “reproducibility”. When the series of partial and incom-plete observations for the same 3-D scene is obtained, the reconstruction ambiguity is explicitly treated by parametrically integrating the descriptor set for each observation into one descriptor set.},
  archive   = {C_IROS},
  author    = {Takaaki Fukui and Tadashi Matsuo and Nobutaka Shimada},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636576},
  pages     = {2414-2419},
  title     = {Scene descriptor expressing ambiguity in information recovery based on incomplete partial observation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Overcoming obstructions via bandwidth-limited multi-agent
spatial handshaking. <em>IROS</em>, 2406–2413. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we address bandwidth-limited and obstruction-prone collaborative perception, specifically in the context of multi-agent semantic segmentation. This setting presents several key challenges, including processing and ex-changing unregistered robotic swarm imagery. To be successful, solutions must effectively leverage multiple non-static and intermittently-overlapping RGB perspectives, while heeding bandwidth constraints and overcoming unwanted foreground obstructions. As such, we propose an end-to-end learn-able Multi-Agent Spatial Handshaking network (MASH) to process, compress, and propagate visual information across a robotic swarm. Our distributed communication module operates directly (and exclusively) on raw image data, without additional input requirements such as pose, depth, or warping data. We demonstrate superior performance of our model compared against several baselines in a photo-realistic multi-robot AirSim environment, especially in the presence of image occlusions. Our method achieves an absolute 11\% IoU improvement over strong baselines.},
  archive   = {C_IROS},
  author    = {Nathaniel Glaser and Yen-Cheng Liu and Junjiao Tian and Zsolt Kira},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636761},
  pages     = {2406-2413},
  title     = {Overcoming obstructions via bandwidth-limited multi-agent spatial handshaking},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to design and construct bridge without blueprint.
<em>IROS</em>, 2398–2405. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous assembly has been a desired functionality of many intelligent robot systems. We study a new challenging assembly task, designing and constructing a bridge without a blueprint. In this task, the robot needs to first design a feasible bridge architecture for arbitrarily wide cliffs and then manipulate the blocks reliably to construct a stable bridge according to the proposed design. In this paper, we propose a bi-level approach to tackle this task. At the high level, the system learns a bridge blueprint policy in a physical simulator using deep reinforcement learning and curriculum learning. A policy is represented as an attention-based neural network with object-centric input, which enables generalization to different number of blocks and cliff widths. For low-level control, we implement a motion-planning-based policy for real-robot motion control, which can be directly combined with a trained blueprint policy for real-world bridge construction without tuning. In our field study, our bi-level robot system demonstrates the capability of manipulating blocks to construct a diverse set of bridges with different architectures.},
  archive   = {C_IROS},
  author    = {Yunfei Li and Tao Kong and Lei Li and Yifeng Li and Yi Wu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636280},
  pages     = {2398-2405},
  title     = {Learning to design and construct bridge without blueprint},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DEALIO: Data-efficient adversarial learning for imitation
from observation. <em>IROS</em>, 2391–2397. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In imitation learning from observation (IfO), a learning agent seeks to imitate a demonstrating agent using only observations of the demonstrated behavior without access to the control signals generated by the demonstrator. Recent methods based on adversarial imitation learning have led to state-of-the-art performance on IfO problems, but they typically suffer from high sample complexity due to a reliance on data-inefficient, model-free reinforcement learning algorithms. This issue makes them impractical to deploy in real-world settings, where gathering samples can incur high costs in terms of time, energy, and risk. In this work, we hypothesize that we can incorporate ideas from model-based reinforcement learning with adversarial methods for IfO in order to increase the data efficiency of these methods without sacrificing performance. Specifically, we consider time-varying linear Gaussian policies, and propose a method that integrates the linear-quadratic regulator with path integral policy improvement into an existing adversarial IfO framework. The result is a more data-efficient IfO algorithm with better performance, which we show empirically in four simulation domains: using far fewer interactions with the environment, the proposed method exhibits similar or better performance than the existing technique.},
  archive   = {C_IROS},
  author    = {Faraz Torabi and Garrett Warnell and Peter Stone},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636169},
  pages     = {2391-2397},
  title     = {DEALIO: Data-efficient adversarial learning for imitation from observation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Residual feedback learning for contact-rich manipulation
tasks with uncertainty. <em>IROS</em>, 2383–2390. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While classic control theory offers state of the art solutions in many problem scenarios, it is often desired to improve beyond the structure of such solutions and surpass their limitations. To this end, residual policy learning (RPL) offers a formulation to improve existing controllers with reinforcement learning (RL) by learning an additive &quot;residual&quot; to the output of a given controller. However, the applicability of such an approach highly depends on the structure of the controller. Often, internal feedback signals of the controller limit an RL algorithm to adequately change the policy and, hence, learn the task. We propose a new formulation that addresses these limitations by also modifying the feedback signals to the controller with an RL policy and show superior performance of our approach on a contact-rich peg-insertion task under position and orientation uncertainty. In addition, we use a recent Cartesian impedance control architecture as the control framework which can be available to us as a black-box while assuming no knowledge about its input/output structure, and show the difficulties of standard RPL. Furthermore, we introduce an adaptive curriculum for the given task to gradually increase the task difficulty in terms of position and orientation uncertainty. A video showing the results can be found at https://youtu.be/SAZm_Krze7U.},
  archive   = {C_IROS},
  author    = {Alireza Ranjbar and Ngo Anh Vien and Hanna Ziesche and Joschka Boedecker and Gerhard Neumann},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636176},
  pages     = {2383-2390},
  title     = {Residual feedback learning for contact-rich manipulation tasks with uncertainty},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CLAMGen: Closed-loop arm motion generation via multi-view
vision-based RL. <em>IROS</em>, 2376–2382. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a vision-based reinforcement learning (RL) approach for closed-loop trajectory generation in an arm reaching problem. Arm trajectory generation is a fundamental robotics problem which entails finding collision-free paths to move the robot’s body (e.g. arm) in order to satisfy a goal (e.g. place end-effector at a point). While classical methods typically require the model of the environment to solve a planning, search or optimization problem, learning-based approaches hold the promise of directly mapping from observations to robot actions. However, learning a collision-avoidance policy using RL remains a challenge for various reasons, including, but not limited to, partial observability, poor exploration, low sample efficiency, and learning instabilities. To address these challenges, we present a residual-RL method that leverages a greedy goal-reaching RL policy as the base to improve exploration, and the base policy is augmented with residual state-action values and residual actions learned from images to avoid obstacles. Further more, we introduce novel learning objectives and techniques to improve 3D understanding from multiple image views and sample efficiency of our algorithm. Compared to RL baselines, our method achieves superior performance in terms of success rate.},
  archive   = {C_IROS},
  author    = {Iretiayo Akinola and Zizhao Wang and Peter Allen},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636369},
  pages     = {2376-2382},
  title     = {CLAMGen: Closed-loop arm motion generation via multi-view vision-based RL},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-supervised online reward shaping in sparse-reward
environments. <em>IROS</em>, 2369–2375. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce Self-supervised Online Reward Shaping (SORS), which aims to improve the sample efficiency of any RL algorithm in sparse-reward environments by automatically densifying rewards. The proposed framework alternates between classification-based reward inference and policy update steps—the original sparse reward provides a self-supervisory signal for reward inference by ranking trajectories that the agent observes, while the policy update is performed with the newly inferred, typically dense reward function. We introduce theory that shows that, under certain conditions, this alteration of the reward function will not change the optimal policy of the original MDP, while potentially increasing learning speed significantly. Experimental results on several sparse-reward environments demonstrate that, across multiple domains, the proposed algorithm is not only significantly more sample efficient than a standard RL baseline using sparse rewards, but, at times, also achieves similar sample efficiency compared to when hand-designed dense reward functions are used.},
  archive   = {C_IROS},
  author    = {Farzan Memarian and Wonjoon Goo and Rudolf Lioutikov and Scott Niekum and Ufuk Topcu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636020},
  pages     = {2369-2375},
  title     = {Self-supervised online reward shaping in sparse-reward environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inclined quadrotor landing using deep reinforcement
learning. <em>IROS</em>, 2361–2368. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Landing a quadrotor on an inclined surface is a challenging maneuver. The final state of any inclined landing trajectory is not an equilibrium, which precludes the use of most conventional control methods. We propose a deep reinforcement learning approach to design an autonomous landing controller for inclined surfaces. Using the proximal policy optimization (PPO) algorithm with sparse rewards and a tailored curriculum learning approach, an inclined landing policy can be trained in simulation in less than 90 minutes on a standard laptop. The policy then directly runs on a real Crazyflie 2.1 quadrotor and successfully performs real inclined landings in a flying arena. A single policy evaluation takes approximately 2.5 ms, which makes it suitable for a future embedded implementation on the quadrotor.},
  archive   = {C_IROS},
  author    = {Jacob E. Kooi and Robert Babuška},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636096},
  pages     = {2361-2368},
  title     = {Inclined quadrotor landing using deep reinforcement learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A mixed reality supervision and telepresence interface for
outdoor field robotics. <em>IROS</em>, 2345–2352. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collaborative human-robot field operations rely on timely decision-making and coordination, which can be challenging for heterogeneous teams operating in large-scale deployments. In this work, we present the design of an immersive, mixed reality (MR) interface to support sense-making and situational awareness based on the data collection capabilities of both human and robotic team members. Our solution integrates state-of-the-art methods in environment mapping and MR so that users may gain rapid insights regarding the working environment, the current and previous locations of human and robot team members, and the environment data such team members have collected. We describe the implementation of our system, share lessons learned in collaborating with emergency responders throughout our design process, and offer a vision for the use of immersive displays for human-robot field team deployments in large-scale outdoor environments.},
  archive   = {C_IROS},
  author    = {Michael Walker and Zhaozhong Chen and Matthew Whitlock and David Blair and Danielle Albers Szafir and Christoffer Heckman and Daniel Szafir},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636090},
  pages     = {2345-2352},
  title     = {A mixed reality supervision and telepresence interface for outdoor field robotics},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An industrial robot for firewater piping inspection and
mapping. <em>IROS</em>, 2337–2344. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The design of an industrial pipe inspection robot for 4&quot; firewater piping inspection and mapping with some new and unreported features is presented. The robot consists of a compact 3D MEMs Lidar with non-linear scanning pattern in the form of Lissajous trajectories that produces a dense scan of inner pipe surfaces. Evaluating the surface integrals of these 3D point clouds acquired at pipe turns allow for turn direction discrimination on the horizontal plane which is not realizable using IMUs. A non-SLAM method is used for robust localization: the actual distance traveled by the robot inside a pipe network is measured explicitly by measuring tether displacement; this measurand is then used as the arc length parameter of a parametric curve. This parameterization represents the robot trajectory such that the 3D robot positions along this trajectory are defined in terms of distances traveled (arc length of parametric curve). Pose transitions measured using an on-board IMU combined with the arc-length parametric trajectory let us determine the full 6D robot pose inside featureless pipes. Instantaneous Lidar scans and the 6D poses are used to create a high density 3D map of the pipe network to help identify pipe joint deformation at cm level.},
  archive   = {C_IROS},
  author    = {Aneesh N. Chand and Naji Zuhdi and Asmadi Mansor and Asif Iqbal and Faiz Rustam and Walter Baur},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636247},
  pages     = {2337-2344},
  title     = {An industrial robot for firewater piping inspection and mapping},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Trajectory selection for power-over-tether atmospheric
sensing UAS. <em>IROS</em>, 2321–2328. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Power-over-tether aircraft is an effective tool for persistent spatiotemporal monitoring of environmental phenomena. This paper presents the design and evaluation of flight trajectories for the tethered aircraft unmanned system (TAUS) sensing a dynamic temperature field. TAUS is a novel power-over-tether-based unmanned aerial system (UAS) configured for long-term, high throughput atmospheric monitoring. It is unique in that it provides position control while measuring atmospheric properties on-board the aircraft and with sensors along the tether. We validated the robotic system by conducting outdoor experiments to characterize the sensor performance against a meteorological tower. We found minimal sensing error at the corresponding altitude relative to the ground truth installation. We then used the experimental data to simulate four trajectories (Lawn-mower, Spiral, Star, and Flower) on power-tethered and untethered system models to evaluate performance factors related to trajectory selection. The analysis of the simulated data indicated that the power-tethered Star trajectory performed well concerning key performance factors when measuring changing atmospheric fields.},
  archive   = {C_IROS},
  author    = {Daniel A. Rico and Francisco Muñoz-Arriola and Carrick Detweiler},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636364},
  pages     = {2321-2328},
  title     = {Trajectory selection for power-over-tether atmospheric sensing UAS},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Perceptive autonomous stair climbing for quadrupedal robots.
<em>IROS</em>, 2313–2320. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies autonomous stair climbing for quadrupedal robots with perception. Enabling quadrupeds to reliably climb staircases greatly expands their applicability in practical scenarios. For this structured task, we develop a simple yet effective perception and control framework for autonomous quadrupedal stair climbing. By exploiting the structural knowledge about the staircases, the proposed framework first extracts the geometric information about the staircase from measurements of the perception system. Then, the climbing velocity and associated foothold references during stair climbing are generated via simple optimization algorithms based on the geometric information about the staircase. Given these references, we use model predictive control based approach to generate input joint torques for controlling the quadruped to complete the whole stair climbing task. Simulation validations using the full dynamic model of the Unitree’s Aliengo quadruped with the MuJoCo simulator are performed, which demonstrate successful autonomous climbing of various staircases with different geometries. Effectiveness of the proposed strategy is further validated through hardware experiments on the real Aliengo robot with different real-world staircases.},
  archive   = {C_IROS},
  author    = {Shuhao Qi and Wenchun Lin and Zejun Hong and Hua Chen and Wei Zhang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636302},
  pages     = {2313-2320},
  title     = {Perceptive autonomous stair climbing for quadrupedal robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiclass terrain classification using sound and vibration
from mobile robot terrain interaction. <em>IROS</em>, 2305–2312. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Offroad mobile robot perception systems must be able to learn robust terrain classification models. Models built from computer vision often fail in their ability to generalize to new environments where appearance characteristics change. Sound and vibration signals from robot-terrain interaction can be used to classify the terrain from characteristics that vary less between environments. Previous work using sound and vibration for terrain classification has only classified ground terrain types. We extend here to building a 7-class multiclass classifier that can classify both ground and above-ground terrain types in challenging outdoor off-road settings, thereby increasing the semantic richness of the terrain classification. Our contributions include: 1) We instrument a robotic vehicle with a variety of sound and vibration sensors mounted at different vehicle locations and directions, as well as color cameras. 2) We collect interactive and visual field data from many outdoor off-road sites with different environments. 3) We build multiclass classifiers for different combinations of sound and vibration signals, and we autonomously learn the optimal signal combination. We compare this against a single microphone from our previous work [1]. 4) We benchmark both of these results against a state-of-the art vision system. All of these multiclass classifiers are tested at different locations from where they are trained. By using one microphone instead of the vision system, we increase balanced accuracy from 70\% to 82\%. By using the optimal sound and vibration combination, we increase balanced accuracy from 82\% to 87\%. All four of these contributions are field robotics in nature: we build a sensor system and then we use that system to collect new field data that allows for a comparative evaluation of different modules of the system. Such datasets do not exist that include these varying sensors on varying field terrain. We are also contributing to machine learning research by a) showing how the acoustic classification from our previous work can be extended to new sensors, and then b) implementing an additional learning process for choosing the optimal combination.},
  archive   = {C_IROS},
  author    = {Jacqueline Libby and Anthony Stentz},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636237},
  pages     = {2305-2312},
  title     = {Multiclass terrain classification using sound and vibration from mobile robot terrain interaction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive terrain traversability prediction based on
multi-source transfer gaussian processes. <em>IROS</em>, 2297–2304. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study addresses the challenge of predicting the terrain traversability of off-road vehicles. When an off-road vehicle is operated on rough terrains or slopes of unconsolidated materials, it is crucial to accurately predict terrain traversability for efficient operations and to avoid critical mobility risks. However, the prediction of traversability is challenging, especially for the prediction of possibly risky terrains because for such terrains, the traverse data available is either limited or non-existent. To address this limitation, this study proposes an adaptive terrain traversability prediction method based on the multi-source transfer Gaussian process regression (MS-TGPR). The proposed method utilizes limited data available on low risk terrains of the target environment to enhance the prediction accuracy by leveraging past traverse experiences on multiple types of terrain surfaces. The effectiveness of the proposed method is demonstrated using a slip dataset of various terrain surfaces and geometries.},
  archive   = {C_IROS},
  author    = {Hiroaki Inotsume and Takashi Kubota},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636528},
  pages     = {2297-2304},
  title     = {Adaptive terrain traversability prediction based on multi-source transfer gaussian processes},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3D reactive control and frontier-based exploration for
unstructured environments. <em>IROS</em>, 2289–2296. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The paper proposes a reliable and robust planning solution to the long range robotic navigation problem in extremely cluttered environments. A two-layer planning architecture is proposed that leverages both the environment map and the direct depth sensor information to ensure maximal information gain out of the onboard sensors. A frontier-based pose sampling technique is used with a fast marching cost-to-go calculation to select a goal pose and plan a path to maximize robot exploration rate. An artificial potential function approach, relying on direct depth measurements, enables the robot to follow the path while simultaneously avoiding small scene obstacles that are not captured in the map due to mapping and localization uncertainties. We demonstrate the feasibility and robustness of the proposed approach through field deployments in a structurally complex warehouse using a micro-aerial vehicle (MAV) with all the sensing and computations performed onboard.},
  archive   = {C_IROS},
  author    = {Shakeeb Ahmad and Andrew B. Mills and Eugene R. Rush and Eric W. Frew and J. Sean Humbert},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636575},
  pages     = {2289-2296},
  title     = {3D reactive control and frontier-based exploration for unstructured environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cross-layer configuration optimization for localization on
resource-constrained devices. <em>IROS</em>, 2282–2288. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile devices are increasingly expected to sup-port high-performance cyber-physical applications in small form factors, e.g., drones and rovers. However, the gap between hardware limitations of these devices and application requirements is still prohibitive – conflicting goals such as robust, accurate, and efficient execution must be managed carefully to achieve acceptable operation. In this paper, we explore the tradeoff between performance and efficiency in such cyber-physical systems, specifically with respect to localization (a core task for any mobile autonomous device). We perform a design space exploration (DSE) given a number of configurable parameters for both localization algorithm and platform layers. Given the configuration space, we formulate a cross-layer multi-objective optimization problem to explore the tradeoff between localization accuracy and power consumption. We then propose a predictive model for robust execution that can be used to determine desirable configurations at runtime in the face of environmental changes.},
  archive   = {C_IROS},
  author    = {Sandra Hernández and José Araujo and Patric Jensfelt and Ioannis Karagiannis and Ananya Muddukrishna and Bryan Donyanavard},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635978},
  pages     = {2282-2288},
  title     = {Cross-layer configuration optimization for localization on resource-constrained devices},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time geo-localization using satellite imagery and
topography for unmanned aerial vehicles. <em>IROS</em>, 2275–2281. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The capabilities of autonomous flight with unmanned aerial vehicles (UAVs) have significantly increased in recent times. However, basic problems such as fast and robust geo-localization in GPS-denied environments still remain unsolved. Existing research has primarily concentrated on improving the accuracy of localization at the cost of long and varying computation time in various situations, which often necessitates the use of powerful ground station machines. In order to make image-based geo-localization online and pragmatic for lightweight embedded systems on UAVs, we propose a framework that is reliable in changing scenes, flexible about computing resource allocation and adaptable to common camera placements. The framework is comprised of two stages: offline database preparation and online inference. At the first stage, color images and depth maps are rendered as seen from potential vehicle poses quantized over the satellite and topography maps of anticipated flying areas. A database is then populated with the global and local descriptors of the rendered images. At the second stage, for each captured real-world query image, top global matches are retrieved from the database and the vehicle pose is further refined via local descriptor matching. We present field experiments of image-based localization on two different UAV platforms to validate our results.},
  archive   = {C_IROS},
  author    = {Shuxiao Chen and Xiangyu Wu and Mark W. Mueller and Koushil Sreenath},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636705},
  pages     = {2275-2281},
  title     = {Real-time geo-localization using satellite imagery and topography for unmanned aerial vehicles},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Monte-carlo localization in underground parking lots using
parking slot numbers. <em>IROS</em>, 2267–2274. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous Valet Parking (AVP) in an under- ground garage is an emerging smart vehicle solution that the community believes to be solvable with close-to-market sensors. Absence of GPS signals and a high degree of self-similarity however render global visual localization in such environments a highly challenging problem. We present a novel underground parking localization method that relies on text recognition in the wild as well as optical character recognition (OCR) to automatically detect parking slot numbers. The detected numbers are then correlated with both geometric as well as semantic information extracted from an offline map of the environment. The resulting measurement model is embedded into a probabilistic Monte-Carlo localization framework. The success of our method is demonstrated on multiple real-world sequences in one of the largest underground parking garages in Shanghai.},
  archive   = {C_IROS},
  author    = {Li Cui and Chunyan Rong and Jingyi Huang and Andre Rosendo and Laurent Kneip},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636465},
  pages     = {2267-2274},
  title     = {Monte-carlo localization in underground parking lots using parking slot numbers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Localization with directional coordinates. <em>IROS</em>,
2253–2258. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A coordinate system is proposed that replaces the usual three-dimensional Cartesian x, y, z position coordinates, for use in robotic localization applications. Range, azimuth, and elevation measurement models become greatly simplified, and, unlike spherical coordinates, the proposed coordinates do not suffer from the same kinematic singularities and angle wraparound. When compared to Cartesian coordinates, the proposed coordinate system results in a significantly enhanced ability to represent the true distribution of robot positions, ultimately leading to large improvements in state estimation consistency.},
  archive   = {C_IROS},
  author    = {Charles Champagne Cossette and Mohammed Shalaby and David Saussié and James Richard Forbes},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636122},
  pages     = {2253-2258},
  title     = {Localization with directional coordinates},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EventVLAD: Visual place recognition with reconstructed edges
from event cameras. <em>IROS</em>, 2247–2252. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Event cameras are neuromorphic vision sensors that are able to capture high dynamic range with low latency in microseconds, without motion blur. Their strength lies in the unique representation of data as asynchronous events, enabling detection of scene structures less invariantly from dynamic luminance changes. However, a single event does not represent spatial information, and events must be integrated to translate into meaningful information. Therefore, state-of-the-art deep learning algorithms have focused on reconstructing the original scene from events. However, as environmental variances are also captured throughout events and restored in reconstructed images, simple reconstruction does not help achieving robust visual place recognition. In this paper, we suggest to use reconstructed event edges denoised for place recognition. While brightness wavers with dynamic environmental variances, edge contours only change with gradient magnitude scale. We utilize the high dynamic range of event cameras to detect these scaled edges from different environments and show that using reconstructed edges shows robust performance in overcoming day-to-night illumination variance without a large training set.},
  archive   = {C_IROS},
  author    = {Alex Junho Lee and Ayoung Kim},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635907},
  pages     = {2247-2252},
  title     = {EventVLAD: Visual place recognition with reconstructed edges from event cameras},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep unsupervised learning based visual odometry with
multi-scale matching and latent feature constraint. <em>IROS</em>,
2239–2246. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A novel siamese autoencoder visual odometry system named SAEVO is proposed in this paper. SAEVO can jointly estimate the 6-DoF pose and the depth using deep neural networks trained with monocular clips only. The main idea of the proposed method is an unsupervised deep learning scheme that combines siamese networks with auto-encoder for multi-scale matching to estimate ego-motion. Also, two unsupervised losses are designed to align extracted features from the siamese autoencoder networks. A system overview is shown in Fig. 1. The experiments on KITTI and CityScapes datasets demonstrate the SAEVO achieves good performance in terms of pose and depth accuracy, and competitive performance to state-of-the-art methods.},
  archive   = {C_IROS},
  author    = {Zhenzhen Liang and Qixin Wang and Yuanlong Yu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636131},
  pages     = {2239-2246},
  title     = {Deep unsupervised learning based visual odometry with multi-scale matching and latent feature constraint},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Finding robust 2D-to-3D correspondence with LSTM score
estimation for camera localization. <em>IROS</em>, 2232–2238. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {2D-to-3D correspondence estimation is the key step of 3D model-based image localization, and most of the existing research in this field focuses on improving the feature matching performance. Even with the best feature matching method, there are still some outliers, and thus, almost all the methods simply apply the RANSAC algorithm to select the inliers and estimate the camera pose afterwards. However, the reliability of RANSAC depends considerably on the inlier ratio. Once the inlier ratio decreases, for example a challenging scenario occurs, it will be unable to select the inliers well and lead to a worse camera pose. In this study, we attempted to build a neural network to learn the geometric relationship between 2D images and the 3D model to select the correct correspondence from the initial 2D-to-3D matching results to improve the performance of camera localization. Because the number of inputs, i.e., the number of 2D-to-3D correspondences, is unknown and different for each image, we propose a PointNet-based Geometric Consistency Network (GCC-Net) for the correct correspondence estimation and an LSTM-based Hypothesis Rating Network (HR-Net) to enhance GCC-Net with the camera localization loss. Experimental results showed that the proposed method outperforms RANSAC considerably on the camera pose estimation, particularly when the inlier ratio of the initial correspondence was low.},
  archive   = {C_IROS},
  author    = {Tsu-Kuan Huang and Po-Heng Chen and Li-Yang Wang and Kuan-Wen Chen},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636516},
  pages     = {2232-2238},
  title     = {Finding robust 2D-to-3D correspondence with LSTM score estimation for camera localization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RoboSLAM: Dense RGB-d SLAM for humanoid robots.
<em>IROS</em>, 2224–2231. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the current paper we investigate the challenges of localizing walking humanoid robots using Visual SLAM (VSLAM). We propose a novel dense RGB-D SLAM framework that seamlessly integrates with the dynamic state of a humanoid, to provide real-time localization and dense mapping of its surroundings. Following the path of recent research in humanoid localization, in the current work we explore the integration between a VSLAM system and the humanoid state, by considering the gait cycle and the feet contacts. We analyze how these effects undermine the quality of data acquisition and association for VSLAM, by capturing the unilateral ground forces at the robot’s feet, and design a system that mitigates their impact.We evaluate our framework on both open and closed-loop bipedal gaits, using a low-cost humanoid platform, and demonstrate that it outperforms kinematic odometry and state-of-the-art dense RGB-D VSLAM methods, by continuously localizing the robot, even in the face of highly irregular and unstable motions.},
  archive   = {C_IROS},
  author    = {Emmanouil Hourdakis and Stylianos Piperakis and Panos Trahanias},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636044},
  pages     = {2224-2231},
  title     = {RoboSLAM: Dense RGB-D SLAM for humanoid robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust pose estimation based on normalized information
distance. <em>IROS</em>, 2217–2223. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dense image alignment works by minimizing the photometric error of two images since it is assumed that the illumination changes between images close in time remain the same—this is what is called the brightness constancy assumption. However, this assumption does not hold with long-term maps since illumination changes continually from day to day (morning, afternoon, evening) and is dependent on certain external conditions like weather or even seasons. In this work, we present an image registration algorithm based on the Normalized Information Distance (NID) that is shown to be robust to extreme illumination changes comparing to the traditional direct methods. The pose is estimated by minimizing the NID function with the help of the nonlinear least square optimization library G2O. We share our source code 1 (CPU and GPU version) for the benefit of the community, which can be a strong basis for future tracking and mapping system based on NID.},
  archive   = {C_IROS},
  author    = {Zhaozhong Chen and Christoffer Heckman},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636532},
  pages     = {2217-2223},
  title     = {Robust pose estimation based on normalized information distance},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Plane segmentation using depth-dependent flood fill.
<em>IROS</em>, 2210–2216. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The detection of planar surfaces in a point cloud is a popular technique for the extraction of drivable or walkable surfaces and for tabletop segmentation. Unfortunately, RGB-D sensors are quite noisy and provide incomplete data, which makes the extraction of surfaces more challenging. Also, it is desirable to process the point cloud data in real time, which at a rate of approximately 30 Hz, leaves only a small amount of computation time per frame. We have already developed a real time-capable plane segmentation method [1] that exploits the organized structure of RGB-D point clouds in order to implement a computationally efficient region growing algorithm. It uses the point-plane distance to assign points to their segments rather than inherently unreliable surface normals. Now we are presenting an improvement where we adapt thresholds and other parameters of our algorithm to the measured depth in order to account for an increasing scatter of the points at larger distances from the camera. We estimate a minimum detectable plane size in pixels dependent on the measured depth. This enables us to stride in pixel coordinates with larger steps that are adaptive to the measured depth and to implement more robust sanity checks of depth-dependent size. Apart from a speed-up of the runtime of our algorithm, the segmentation quality also increased. We show a comparison between our improvement, our previous version, and other state-of-the-art methods evaluated on multiple commonly available datasets.},
  archive   = {C_IROS},
  author    = {Arindam Roychoudhury and Marceli Missura and Maren Bennewitz},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635930},
  pages     = {2210-2216},
  title     = {Plane segmentation using depth-dependent flood fill},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Object-augmented RGB-d SLAM for wide-disparity
relocalisation. <em>IROS</em>, 2203–2209. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel object-augmented RGB-D SLAM system that is capable of constructing a consistent object map and performing relocalisation based on centroids of objects in the map. The approach aims to overcome the view dependence of appearance-based relocalisation methods using point features or images. During the map construction, we use a pre-trained neural network to detect objects and estimate 6D poses from RGB-D data. An incremental probabilistic model is used to aggregate estimates over time to create the object map. Then in relocalisation, we use the same network to extract objects-of-interest in the ‘lost’ frames. Pairwise geometric matching finds correspondences between map and frame objects, and probabilistic absolute orientation followed by application of iterative closest point to dense depth maps and object centroids gives relocalisation. Results of experiments in desktop environments demonstrate very high success rates even for frames with widely different viewpoints from those used to construct the map, significantly outperforming two appearance- based methods.},
  archive   = {C_IROS},
  author    = {Yuhang Ming and Xingrui Yang and Andrew Calway},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636034},
  pages     = {2203-2209},
  title     = {Object-augmented RGB-D SLAM for wide-disparity relocalisation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adversarial attacks on camera-LiDAR models for 3D car
detection. <em>IROS</em>, 2189–2194. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most autonomous vehicles (AVs) rely on LiDAR and RGB camera sensors for perception. Using these point cloud and image data, perception models based on deep neural nets (DNNs) have achieved state-of-the-art performance in 3D detection. The vulnerability of DNNs to adversarial attacks have been heavily investigated in the RGB image domain and more recently in the point cloud domain, but rarely in both domains simultaneously. Multi-modal perception systems used in AVs can be divided into two broad types: cascaded models which use each modality independently, and fusion models which learn from different modalities simultaneously. We propose a universal and physically realizable adversarial attack for each type, and study and contrast their respective vulnerabilities to attacks. We place a single adversarial object with specific shape and texture on top of a car with the objective of making this car evade detection. Evaluating on the popular KITTI benchmark, our adversarial object made the host vehicle escape detection by each model type more than 50\% of the time. The dense RGB input contributed more to the success of the adversarial attacks on both cascaded and fusion models.},
  archive   = {C_IROS},
  author    = {Mazen Abdelfattah and Kaiwen Yuan and Z. Jane Wang and Rabab Ward},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636638},
  pages     = {2189-2194},
  title     = {Adversarial attacks on camera-LiDAR models for 3D car detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Decoder modulation for indoor depth completion.
<em>IROS</em>, 2181–2188. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Depth completion recovers a dense depth map from sensor measurements. Current methods are mostly tailored for very sparse depth measurements from LiDARs in outdoor settings, while for indoor scenes Time-of-Flight (ToF) or structured light sensors are mostly used. These sensors provide semi-dense maps, with dense measurements in some regions and almost empty in others. We propose a new model that takes into account the statistical difference between such regions. Our main contribution is a new decoder modulation branch added to the encoder-decoder architecture. The encoder extracts features from the concatenated RGB image and raw depth. Given the mask of missing values as input, the proposed modulation branch controls the decoding of a dense depth map from these features differently for different regions. This is implemented by modifying the spatial distribution of output signals inside the decoder via Spatially-Adaptive Denormalization (SPADE) blocks. Our second contribution is a novel on-the- y sensor simulation strategy that allows us to train on a semi-dense sensor data when the ground truth depth map is not available. Our model achieves the state of the art results on indoor Matterport3D dataset [1]. Being designed for semi-dense input depth, our model is still competitive with LiDAR-oriented approaches on the KITTI dataset [2]. Our sensor simulation strategy significantly improves prediction quality with no dense ground truth available, as validated on the NYUv2 dataset [3].},
  archive   = {C_IROS},
  author    = {Dmitry Senushkin and Mikhail Romanov and Ilia Belikov and Nikolay Patakin and Anton Konushin},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636870},
  pages     = {2181-2188},
  title     = {Decoder modulation for indoor depth completion},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatial imagination with semantic cognition for mobile
robots. <em>IROS</em>, 2174–2180. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The imagination of the surrounding environment based on the experience and semantic cognition has great potential to extend the limited observations to leverage the ability for mapping, collision avoidance and path planning. This paper provides a training-based algorithm for mobile robots to perform spatial imagination based on semantic cognition and evaluates the proposed method for the mapping task. We utilize a photo-realistic simulation environment, Habitat, for training and evaluation. The trained model is composed of Resent-18 as encoder and U-net as the backbone. We demonstrate that the algorithm can perform imagination for unseen parts of the object universally, by recalling the images and experience and compare our approach with traditional semantic mapping methods. It is found that our approach will improve the efficiency and accuracy of semantic mapping.},
  archive   = {C_IROS},
  author    = {Zhengcheng Shen and Linh Kästner and Jens Lambrecht},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636550},
  pages     = {2174-2180},
  title     = {Spatial imagination with semantic cognition for mobile robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image-based joint state estimation pipeline for sensorless
manipulators. <em>IROS</em>, 2158–2165. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion planning is a largely solved problem for robot arms with joint state feedback, but remains an area of research for sensorless manipulators such as toy robot arms and heavy equipment such as excavators and cranes. A promising approach to this problem is deep learning, which employs a pre-trained convolutional neural network to identify manipulator links and estimate joint states from a monocular camera video feed. Whereas manual labeling of training image sets is tedious and non-transferable, a simulation environment can automatically generate labeled training image sets of any size. The issue is the gap between simulated and real-world images. This paper solves this problem by implementing a Generative Adversarial Network. The complete joint state estimation pipeline is implemented and tested in hardware experiments to validate our proposed approach.},
  archive   = {C_IROS},
  author    = {Mingjie Han and Bowen Xie and Martin Barczyk and Alireza Bayat},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636439},
  pages     = {2158-2165},
  title     = {Image-based joint state estimation pipeline for sensorless manipulators},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A learning approach to robot-agnostic force-guided high
precision assembly. <em>IROS</em>, 2151–2157. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work we propose a learning approach to high-precision robotic assembly problems. We focus on the contact-rich phase, where the assembly pieces are in close contact with each other. Unlike many learning-based approaches that heavily rely on vision or spatial tracking, our approach takes force/torque in task space as the only observation. Our training environment is robotless, as the end-effector is not attached to any specific robot. Trained policies can then be applied to different robotic arms without re-training. This approach can greatly reduce complexity to perform contact-rich robotic assembly in the real world, especially in unstructured settings such as in architectural construction. To achieve it, we have developed a new distributed RL agent, named Recurrent Distributed DDPG (RD2), which extends Ape-X DDPG[1] with recurrency and makes two structural improvements on prioritized experience replay[2]. Our results show that RD2 is able to solve two fundamental high-precision assembly tasks, lap-joint and peg-in-hole, and outperforms two state-of-the-art algorithms, Ape-X DDPG and PPO with LSTM. We have successfully evaluated our robot-agnostic policies on three robotic arms, Kuka KR60, Franka Panda, and UR10, in simulation. The video presenting our experiments is available at https://sites.google.com/view/rd2-rl},
  archive   = {C_IROS},
  author    = {Jieliang Luo and Hui Li},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636328},
  pages     = {2151-2157},
  title     = {A learning approach to robot-agnostic force-guided high precision assembly},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Task-consistent path planning for mobile 3D printing.
<em>IROS</em>, 2143–2150. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we explore the problem of task-consistent path planning for printing-in-motion via Mobile Manipulators (MM). MM offer a potentially unlimited planar workspace and flexibility for print operations. However, most existing methods have only mobility to relocate an arm which then prints while stationary. In this paper we present a new fully autonomous path planning approach for mobile material deposition. We use a modified version of Rapidly-exploring Random Tree Star (RRT*) algorithm, which is informed by a constrained Inverse Reachability Map (IRM) to ensure task consistency. Collision avoidance and end-effector reachability are respected in our approach. Our method also detects when a print path cannot be completed in a single execution. In this case it will decompose the path into several segments and reposition the base accordingly.},
  archive   = {C_IROS},
  author    = {Julius Sustarevas and Dimitrios Kanoulas and Simon Julier},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635916},
  pages     = {2143-2150},
  title     = {Task-consistent path planning for mobile 3D printing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time motion planning of a hydraulic excavator using
trajectory optimization and model predictive control. <em>IROS</em>,
2135–2142. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automation of excavation tasks requires real-time trajectory planning satisfying various constraints. To guarantee both constraint feasibility and real-time trajectory re-plannability, we present an integrated framework for real-time optimization-based trajectory planning of a hydraulic excavator. The proposed framework is composed of two main modules: a global planner and a real-time local planner. The global planner computes the entire global trajectory considering excavation volume and energy minimization while the local counterpart tracks the global trajectory in a receding horizon manner, satisfying dynamic feasibility, physical constraints, and disturbance-awareness. We validate the proposed planning algorithm in a simulation environment where two types of operations are conducted in the presence of emulated disturbance from hydraulic friction and soil-bucket interaction: shallow and deep excavation. The optimized global trajectories are obtained in an order of a second, which is tracked by the local planner at faster than 30 Hz. To the best of our knowledge, this work presents the first real-time motion planning framework that satisfies constraints of a hydraulic excavator, such as force/torque, power, cylinder displacement, and flow rate limits.},
  archive   = {C_IROS},
  author    = {Dongjae Lee and Inkyu Jang and Jeonghyun Byun and Hoseong Seo and H. Jin Kim},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635965},
  pages     = {2135-2142},
  title     = {Real-time motion planning of a hydraulic excavator using trajectory optimization and model predictive control},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hybrid data-driven modelling for inverse control of
hydraulic excavators. <em>IROS</em>, 2127–2134. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a comprehensive comparison of hybrid Data-Driven Control (DDC) applied on a hydraulic excavator. DDC offers a state-of-the-art, high performance control based on data and expert knowledge. On the one hand, expert knowledge is complex to adapt to each unique excavator requiring substantial engineering efforts. On the other hand, purely data based control overcomes this drawback by adapting a Neural Network (NN) inverse control directly on measured input/output data. Yet, coverage of the entire phase space and extrapolation to unknown situations is challenging for solely data-driven approaches. On a real demonstrator, we analyze expert white box methods, solely data-driven black box approaches and a hybrid grey box approach which combines a data-driven and simplified expert model. We examine trajectory tracking performance, engineering effort and safe exploration as goal criteria. Besides various experiments for testing safety, we apply a Support-Vector-Machine (SVM) to analyze the extrapolation fitness of the data-driven components to unknown data.},
  archive   = {C_IROS},
  author    = {Jonas Weigand and Julian Raible and Nico Zantopp and Ozan Demir and Adrian Trachte and Achim Wagner and Martin Ruskowski},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636269},
  pages     = {2127-2134},
  title     = {Hybrid data-driven modelling for inverse control of hydraulic excavators},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reinforcement learning control of a forestry crane
manipulator. <em>IROS</em>, 2121–2126. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Forestry machines are heavy vehicles performing complex manipulation tasks in unstructured production forest environments. Together with the complex dynamics of the onboard hydraulically actuated cranes, the rough forest terrains have posed a particular challenge in forestry automation. In this study, the feasibility of applying reinforcement learning control to forestry crane manipulators is investigated in a simulated environment. Our results show that it is possible to learn successful actuator-space control policies for energy efficient log grasping by invoking a simple curriculum in a deep reinforcement learning setup. Given the pose of the selected logs, our best control policy reaches a grasping success rate of 97\%. Including an energy-optimization goal in the reward function, the energy consumption is significantly reduced compared to control policies learned without incentive for energy optimization, while the increase in cycle time is marginal. The energy-optimization effects can be observed in the overall smoother motion and acceleration profiles during crane manipulation.},
  archive   = {C_IROS},
  author    = {Jennifer Andersson and Kenneth Bodin and Daniel Lindmark and Martin Servin and Erik Wallin},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636219},
  pages     = {2121-2126},
  title     = {Reinforcement learning control of a forestry crane manipulator},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Alternating drive-and-glide flight navigation of a kiteplane
for sound source position estimation. <em>IROS</em>, 2114–2120. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Drone audition, namely the hearing capability of a drone, is expected to compensate for the drawbacks of visual sensors in search-and-rescue missions. Current multi-rotor drones have limitations of flight duration and sound processing due to ego-noise generated by rotors and air-flow. Drone audition for a kiteplane, i.e., a fixed-wing drone that can fly slowly and stably, has not been investigated. This paper proposes &quot;Alternating Drive-and-Glide Flight Navigation&quot;(AltDGFNavi) of a kiteplane for sound source position estimation. AltDGFNavi consists of two functions: periodical switching rotor for driving and gliding to reduce ego-noise, and dynamic flight path generation to fly close to the target. AltDGFNavi was evaluated through numerical simulations, and the results of sound source position estimation demonstrated the effectiveness of AltDGFNavi.},
  archive   = {C_IROS},
  author    = {Makoto Kumon and Hiroshi G. Okuno and Shuichi Tajima},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636136},
  pages     = {2114-2120},
  title     = {Alternating drive-and-glide flight navigation of a kiteplane for sound source position estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fully-online always-adaptation of transfer functions and its
application to sound source localization and separation. <em>IROS</em>,
2100–2105. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses fully-online always-adaptation of a transfer function for robot audition systems based on microphone array processing. The transfer function represents signal propagation characteristics between a microphone and a sound source, which provides essential information for real-world scene analysis, such as sound source localization and separation for robots. Although it is commonly defined as a stationary function, it should be considered together with room acoustics and their environmental changes for practical use, that is, it should be defined as a dynamically-changing function. To fulfill this requirement, we propose a fully-online always-adaptation method for a transfer function, by continuously estimating the transfer function from the observed signals in a passive manner, while performing sound source localization and separation. The proposed method was implemented on open source robot audition software HARK as modules which works online. These modules are applied to sound source localization and separation which are primary functions in robot audition. Experimental results showed that the proposed method successfully adapted to an office environment and improved the performance of sound source localization and separation at a close level to the transfer function recorded in the room.},
  archive   = {C_IROS},
  author    = {Kazuhiro Nakadai and Masayuki Takigahira and Yusuke Kawai and Hirofumi Nakajima},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636631},
  pages     = {2100-2105},
  title     = {Fully-online always-adaptation of transfer functions and its application to sound source localization and separation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SSC: Semantic scan context for large-scale place
recognition. <em>IROS</em>, 2092–2099. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Place recognition gives a SLAM system the ability to correct cumulative errors. Unlike images that contain rich texture features, point clouds are almost pure geometric information which makes place recognition based on point clouds challenging. Existing works usually encode low-level features such as coordinate, normal, reflection intensity, etc., as local or global descriptors to represent scenes. Besides, they often ignore the translation between point clouds when matching descriptors. Different from most existing methods, we explore the use of high-level features, namely semantics, to improve the descriptor’s representation ability. Also, when matching descriptors, we try to correct the translation between point clouds to improve accuracy. Concretely, we propose a novel global descriptor, Semantic Scan Context, which explores semantic information to represent scenes more effectively. We also present a two-step global semantic ICP to obtain the 3D pose (x, y, yaw) used to align the point cloud to improve matching performance. Our experiments on the KITTI dataset show that our approach outperforms the state-of-the- art methods with a large margin. Our code is available at: https://github.com/lilin-hitcrt/SSC.},
  archive   = {C_IROS},
  author    = {Lin Li and Xin Kong and Xiangrui Zhao and Tianxin Huang and Wanlong Li and Feng Wen and Hongbo Zhang and Yong Liu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635904},
  pages     = {2092-2099},
  title     = {SSC: Semantic scan context for large-scale place recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CORAL: Colored structural representation for bi-modal place
recognition. <em>IROS</em>, 2084–2091. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Place recognition is indispensable for a drift-free localization system. Due to the variations of the environment, place recognition using single-modality has limitations. In this paper, we propose a bi-modal place recognition method, which can extract a compound global descriptor from the two modalities, vision and LiDAR. Specifically, we first build the elevation image generated from 3D points as a structural representation. Then, we derive the correspondences between 3D points and image pixels that are further used in merging the pixel-wise visual features into the elevation map grids. In this way, we fuse the structural features and visual features in the consistent bird-eye view frame, yielding a semantic representation, namely CORAL. And the whole network is called CORAL-VLAD. Comparisons on the Oxford RobotCar show that CORAL-VLAD has superior performance against other state-of-the-art methods. We also demonstrate that our network can be generalized to other scenes and sensor configurations on cross-city datasets.},
  archive   = {C_IROS},
  author    = {Yiyuan Pan and Xuecheng Xu and Weijie Li and Yunxiang Cui and Yue Wang and Rong Xiong},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635839},
  pages     = {2084-2091},
  title     = {CORAL: Colored structural representation for bi-modal place recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MV-FractalDB: Formula-driven supervised learning for
multi-view image recognition. <em>IROS</em>, 2076–2083. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The paper proposes a method for automatic multi-view dataset construction based on formula-driven supervised learning (FDSL). Although data collection and human annotation of 3D objects are labor-intensive, we automatically generate their training data and labels in the proposed multi-view dataset. To create a large-scale multi-view dataset, we employ fractal geometry, which is considered the background information of many objects in the real world. We project in a circle from the rendered 3D fractal models to construct the Multi-view Fractal DataBase (MV-FractalDB), which is then used to make a pre-trained CNN model. According to the experimental results, the MV-FractalDB pre-trained model surpasses the accuracies with self-supervised methods (e.g., SimCLR and MoCo) and is close to supervised methods (e.g., ImageNet) in terms of performance rates on multi-view image datasets. We demonstrate the potential of FDSL for multi-view image recognition.},
  archive   = {C_IROS},
  author    = {Ryosuke Yamada and Ryo Takahashi and Ryota Suzuki and Akio Nakamura and Yusuke Yoshiyasu and Ryusuke Sagawa and Hirokatsu Kataoka},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635946},
  pages     = {2076-2083},
  title     = {MV-FractalDB: Formula-driven supervised learning for multi-view image recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Object-to-scene: Learning to transfer object knowledge to
indoor scene recognition. <em>IROS</em>, 2069–2075. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate perception of the surrounding scene is helpful for robots to make reasonable judgments and behaviours. Therefore, developing effective scene representation and recognition methods are of significant importance in robotics. Currently, a large body of research focuses on developing novel auxiliary features and networks to improve indoor scene recognition ability. However, few of them focus on directly constructing object features and relations for indoor scene recognition. In this paper, we analyze the weaknesses of current methods and propose an Object-to-Scene (OTS) method, which extracts object features and learns object relations to recognize indoor scenes. The proposed OTS first extracts object features based on the segmentation network and the proposed object feature aggregation module (OFAM). Afterwards, the object relations are calculated and the scene representation is constructed based on the proposed object attention module (OAM) and global relation aggregation module (GRAM). The final results in this work show that OTS successfully extracts object features and learns object relations from the segmentation network. Moreover, OTS outperforms the state-of-the-art methods by more than 2\% on indoor scene recognition without using any additional streams. Code is publicly available at: https://github.com/FreeformRobotics/OTS.},
  archive   = {C_IROS},
  author    = {Bo Miao and Liguang Zhou and Ajmal Saeed Mian and Tin Lun Lam and Yangsheng Xu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636700},
  pages     = {2069-2075},
  title     = {Object-to-scene: Learning to transfer object knowledge to indoor scene recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reactive control for bipedal running over random discrete
terrain under uncertainty. <em>IROS</em>, 2061–2068. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a reactive control framework for robotic bipedal running over random discrete terrain using a shift-invariant funnel library that can be composed into motion plans. The main contribution of the paper is the formalization of the funnel library and the introduction of a receding-horizon reactive planner that utilizes this funnel library. The proposed controller generates a robust motion plan on the fly, without the need for predefined footholds or center of mass trajectory, via sequential-composition of pre-computed funnels. The proposed method utilizes three layers of robustness: high-level reactive planning, low-level robust control, and funnel-based, pre-computed, performance guarantees. For clarity, we provide a detailed example, using the 5 degrees-of-freedom sagittal biped model. We demonstrate in simulation the ability of the framework to overcome uncertainty in sensing and model parameters.},
  archive   = {C_IROS},
  author    = {Omer Nir and Amir Degani},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636809},
  pages     = {2061-2068},
  title     = {Reactive control for bipedal running over random discrete terrain under uncertainty},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). XAI-n: Sensor-based robot navigation using expert policies
and decision trees. <em>IROS</em>, 2053–2060. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel sensor-based learning navigation algorithm to compute a collision-free trajectory for a robot in dense and dynamic environments with moving obstacles or targets. Our approach uses deep reinforcement learning-based expert policy that is trained using a sim2real paradigm. In order to increase the reliability and handle the failure cases of the expert policy, we combine with a policy extraction technique to transform the resulting policy into a decision tree format. We use properties of decision trees to analyze and modify the policy and improve performance of navigation algorithm including smoothness, frequency of oscillation, frequency of immobilization, and obstruction of target. Overall, we are able to modify the policy to design an improved learning algorithm without retraining. We highlight the benefits of our approach in simulated environments and navigating a Clearpath Jackal robot among moving pedestrians. (Videos at this url: https://gamma.umd.edu/researchdirections/xrl/navviper)},
  archive   = {C_IROS},
  author    = {Aaron M. Roth and Jing Liang and Dinesh Manocha},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636759},
  pages     = {2053-2060},
  title     = {XAI-N: Sensor-based robot navigation using expert policies and decision trees},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mechanical search on shelves using lateral access x-RAY.
<em>IROS</em>, 2045–2052. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Finding an occluded object in a lateral access environment such as a shelf or cabinet is a problem that arises in many contexts such as warehouses, retail, healthcare, shipping, and homes. While this problem, known as mechanical search, is well-studied in overhead access environments, lateral access environments introduce constraints on the poses of objects and on available grasp actions, and pushing actions are preferred to preserve the environment structure. We propose LAX-RAY (Lateral Access maXimal Reduction in support Area of occupancY distribution): a system that combines target object occupancy distribution prediction with a mechanical search policy that sequentially pushes occluding objects to reveal a given target object. For scenarios with extruded polygonal objects, we introduce two lateral-access search policies that encode a history of predicted target distributions and can plan up to three actions into the future. We introduce a First-Order Shelf Simulator (FOSS) and use it to evaluate these policies in 800 simulated random shelf environments per policy. We also evaluate in 5 physical shelf environments using a Fetch robot with an embedded PrimeSense RGBD Camera and an attached pushing blade. The policies outperform baselines by up to 25\% in simulation and up to 60\% in physical experiments. Additionally, the two-step prediction policy is the highest performing in simulation for 8 objects with a 69\% success rate, suggesting a tradeoff between future information and prediction errors. Code, videos, and supplementary material can be found at https://sites.google.com/berkeley.edu/lax-ray.},
  archive   = {C_IROS},
  author    = {Huang Huang and Marcus Dominguez-Kuhne and Vishal Satish and Michael Danielczuk and Kate Sanders and Jeffrey Ichnowski and Andrew Lee and Anelia Angelova and Vincent Vanhoucke and Ken Goldberg},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636629},
  pages     = {2045-2052},
  title     = {Mechanical search on shelves using lateral access X-RAY},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards autonomous parking using vision-only sensors.
<em>IROS</em>, 2038–2044. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing autonomous parking solutions usually require special signs, pre-built maps or accurate ranging sensors to achieve reliable perception of the parking environment, but these methods are difficult to popularize because they either require preconditions or are expensive for production cars. In this paper, we propose a vision-only autonomous parking solution based on only six cameras. Through the appropriate depth estimation algorithms, our method obtains the pixel level depth of the image, and constructs a dense point cloud, so as to realize the fine perception of the parking environment. An improved Radon transform based parking space detection method are applied for better parking space detection method. Our proposed method achieves processing speed of above 5 Hz on a intermediate level computing platform. Furthermore, we demonstrate the practicability of the proposed system in real-world parking lots.},
  archive   = {C_IROS},
  author    = {Yi Yang and Miaoxin Pan and Sitan Jiang and Jianhang Wang and Wei Wang and Junbo Wang and Meiling Wang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636106},
  pages     = {2038-2044},
  title     = {Towards autonomous parking using vision-only sensors},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deformation recovery control and post-impact trajectory
replanning for collision-resilient mobile robots. <em>IROS</em>,
2030–2037. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The paper focuses on collision-inclusive motion planning for impact-resilient mobile robots. We propose a new deformation recovery and replanning strategy to handle collisions that may occur at run-time. Contrary to collision avoidance methods that generate trajectories only in conservative local space or require collision checking that has high computational cost, our method directly generates (local) trajectories with imposing only waypoint constraints. If a collision occurs, our method then estimates the post-impact state and computes from there an intermediate waypoint to recover from the collision. To achieve so, we develop two novel components: 1) a deformation recovery controller that optimizes the robot’s states during post-impact recovery phase, and 2) a post-impact trajectory replanner that adjusts the next waypoint with the information from the collision for the robot to pass through and generates a polynomial-based minimum effort trajectory. The proposed strategy is evaluated experimentally with an omnidirectional impact-resilient wheeled robot. The robot is designed in house, and it can perceive collisions with the aid of Hall effect sensors embodied between the robot’s main chassis and a surrounding deflection ring-like structure.},
  archive   = {C_IROS},
  author    = {Zhouyu Lu and Zhichao Liu and Konstantinos Karydis},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636276},
  pages     = {2030-2037},
  title     = {Deformation recovery control and post-impact trajectory replanning for collision-resilient mobile robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-resolution POMDP planning for multi-object search in
3D. <em>IROS</em>, 2022–2029. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots operating in households must find objects on shelves, under tables, and in cupboards. In such environments, it is crucial to search efficiently at 3D scale while coping with limited field of view and the complexity of searching for multiple objects. Principled approaches to object search frequently use Partially Observable Markov Decision Process (POMDP) as the underlying framework for computing search strategies, but constrain the search space in 2D. In this paper, we present a POMDP formulation for multi-object search in a 3D region with a frustum-shaped field-of-view. To efficiently solve this POMDP, we propose a multi-resolution planning algorithm based on online Monte-Carlo tree search. In this approach, we design a novel octree-based belief representation to capture uncertainty of the target objects at different resolution levels, then derive abstract POMDPs at lower resolutions with dramatically smaller state and observation spaces. Evaluation in a simulated 3D domain shows that our approach finds objects more efficiently and successfully compared to a set of baselines without resolution hierarchy in larger instances under the same computational requirement. We demonstrate our approach on a mobile robot to find objects placed at different heights in two 10m 2 ×2m regions by moving its base and actuating its torso.},
  archive   = {C_IROS},
  author    = {Kaiyu Zheng and Yoonchang Sung and George Konidaris and Stefanie Tellex},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636737},
  pages     = {2022-2029},
  title     = {Multi-resolution POMDP planning for multi-object search in 3D},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-resolution elevation mapping and safe landing site
detection with applications to planetary rotorcraft. <em>IROS</em>,
1990–1997. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a resource-efficient approach to provide an autonomous UAV with an on-board perception method to detect safe, hazard-free landing sites during flights over complex 3D terrain. We aggregate 3D measurements acquired from a sequence of monocular images by a Structure-from-Motion approach into a local, robot-centric, multi-resolution elevation map of the overflown terrain, which fuses depth measurements according to their lateral surface resolution (pixel-footprint) in a probabilistic framework based on the concept of dynamic Level of Detail. Map aggregation only requires depth maps and the associated poses, which are obtained from an on-board Visual Odometry algorithm. An efficient landing site detection method then exploits the features of the underlying multi-resolution map to detect safe landing sites based on slope, roughness, and quality of the reconstructed terrain surface. The evaluation of the performance of the mapping and landing site detection modules are analyzed independently and jointly in simulated and real-world experiments in order to establish the efficacy of the proposed approach.},
  archive   = {C_IROS},
  author    = {Pascal Schoppmann and Pedro F. Proença and Jeff Delaune and Michael Pantic and Timo Hinzmann and Larry Matthies and Roland Siegwart and Roland Brockers},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636507},
  pages     = {1990-1997},
  title     = {Multi-resolution elevation mapping and safe landing site detection with applications to planetary rotorcraft},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Target-visible polynomial trajectory generation within an
MAV team. <em>IROS</em>, 1982–1989. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous aerial videography is a challenging task, which involves collision avoidance against obstacles and visibility guaranteed target tracking in unstructured environments. In this paper, we organize a two micro aerial vehicle (MAV) team, which consists of a target agent responsible for a specific mission and a camera agent for filming the target agent. Especially, this paper focuses on trajectory planning of the camera agent to chase without occlusion of target agent. Our trajectory planner module includes two phases of guaranteeing target visibility. In the first phase, we generate homotopic safe flight corridor (SFC) to attain target-visible regions. In the subsequent phase, we generate a safe and smooth trajectory with the continuous visibility constraint based on the SFC, using quadratic programming (QP). Regardless of complexity of map, our planner converts an overall problem to a single QP and generates a steady flight trajectory without undesirable fluctuating motion, while guaranteeing all-time visibility. We validate our approach in Gazebo simulations and a real-world experiment.},
  archive   = {C_IROS},
  author    = {Yunwoo Lee and Jungwon Park and Boseong Jeon and H. Jin Kim},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636446},
  pages     = {1982-1989},
  title     = {Target-visible polynomial trajectory generation within an MAV team},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Autonomous flights in dynamic environments with onboard
vision. <em>IROS</em>, 1966–1973. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce a complete system for autonomous flight of quadrotors in dynamic environments with onboard sensing. Extended from existing work, we develop an occlusion-aware dynamic perception method based on depth images, which classifies obstacles as dynamic and static. For representing generic dynamic environment, we model dynamic objects with moving ellipsoids and fuse static ones into an occupancy grid map. To achieve dynamic avoidance, we design a planning method composed of modified kinodynamic path searching and gradient-based optimization. The method leverages manually constructed gradients without maintaining a signed distance field (SDF), making the planning procedure finished in milliseconds. We integrate the above methods into a customized quadrotor system and thoroughly test it in real-world experiments, verifying its effective collision avoidance in dynamic environments.},
  archive   = {C_IROS},
  author    = {Yingjian Wang and Jialin Ji and Qianhao Wang and Chao Xu and Fei Gao},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636117},
  pages     = {1966-1973},
  title     = {Autonomous flights in dynamic environments with onboard vision},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Why fly blind? Event-based visual guidance for ornithopter
robot flight. <em>IROS</em>, 1958–1965. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The development of perception and control methods that allow bird-scale flapping-wing robots (a.k.a. ornithopters) to perform autonomously is an under-researched area. This paper presents a fully onboard event-based method for ornithopter robot visual guidance. The method uses event cameras to exploit their fast response and robustness against motion blur in order to feed the ornithopter control loop at high rates (100 Hz). The proposed scheme visually guides the robot using line features extracted in the event image plane and controls the flight by actuating over the horizontal and vertical tail deflections. It has been validated on board a real ornithopter robot with real-time computation in low-cost hardware. The experimental evaluation includes sets of experiments with different maneuvers indoors and outdoors.},
  archive   = {C_IROS},
  author    = {A. Gómez Eguíluz and J.P. Rodríguez-Gómez and R. Tapia and F.J. Maldonado and J.Á. Acosta and J.R. Martínez-de Dios and A. Ollero},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636315},
  pages     = {1958-1965},
  title     = {Why fly blind? event-based visual guidance for ornithopter robot flight},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Persuasion strategies for social robot to keep humans
accepting daily different recommendations. <em>IROS</em>, 1950–1957. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Social robots are used in daily life. One of the applications of social robots is as recommendation systems. Previous research has mainly investigated how persuasive recommendations can be improved by focusing on the non-verbal/verbal behavior of robots. However, to use robots as recommendation systems every day, it is extremely important to examine the persistence of repeated persuasion over a long term, rather than the effect of one-time persuasion. Therefore, the objective of this study was to investigate the persistence of repeated persuasive of robots. For this purpose, robots with three types of behavior (Expert Behavior, Local Behavior, and Growth Behavior) recommended nutrition bars in a situation of daily consumption behavior for two weeks. We could confirm significant differences in the persistent persuasiveness in each behavior. The results suggested that the combination of value co-creation using local information and meta-trust expression had a significant impact on the persistence of the repeated persuasiveness of the robots in the longitudinal period. However, the acceptance of the recommendation robot system decreased due to the increase in the amount of information during recommendation; therefore, a new recommend system to solve this problem is desired.},
  archive   = {C_IROS},
  author    = {Yuki Okafuji and Jun Baba and Junya Nakanishi and Joichiro Amada and Yuichiro Yoshikawa and Hiroshi Ishiguro},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636772},
  pages     = {1950-1957},
  title     = {Persuasion strategies for social robot to keep humans accepting daily different recommendations},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Human-robot greeting: Tracking human greeting mental states
and acting accordingly. <em>IROS</em>, 1935–1941. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile social robots should be able to engage in interaction with people effectively. However, greeting someone is a complex task since it implies an exchange of social signals. Adam Kendon modeled human greetings as a set of six phases: initiation of approach, distance salutation, head dip, approach, final approach, and close salutation. Based on Kendon’s model, we propose a system for mobile social robots that manages the greeting process through the exchange of social signals. A Hidden Markov Model keeps track of the greeting stage through the observation of the human gestures, while a behavior tree generates appropriate robot actions. We used publicly available datasets to train the Hidden Markov Model. Evaluation on test sets showed an average greeting phase estimation accuracy of 80.9\%. We tested the full system (Hidden Markov Model + Behavior Tree) in simulation and in a real world pilot experiment using the Vizzy robot, and it recognized and replicated the correct phase with an accuracy of 91.8\% and 53.8\%, respectively.},
  archive   = {C_IROS},
  author    = {Manuel Carvalho and João Avelino and Alexandre Bernardino and Rodrigo Ventura and Plinio Moreno},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635894},
  pages     = {1935-1941},
  title     = {Human-robot greeting: Tracking human greeting mental states and acting accordingly},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Design of taking a walk with a robot that receives care from
a person and indirectly mediates communication with strangers.
<em>IROS</em>, 1927–1934. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We report the results of our study on whether taking a walk with a child-like robot that a person takes care of can generate interaction with the surrounding people whom the person has never met before. As the number of single-person households increases, it is expected that more people will live with not only robots that can be useful for people but also robots that people can take care of. Our study is important because we explored the possibilities of such a robot to create interaction between people in the community, which has been lost in recent years. In this paper, we designed the behavior of a robot that follows a person and learns about the scenery while walking together and implemented it using Pepper. Then, the first author walked around the university building with the robot and observed the initial reactions of the surrounding people. As we expected, the surrounding people interacted with the first author by talking to the robot as if it were a child and helping the robot. This paper contributes to taking a step forward a new research theme ‘taking a walk with a robot and a person’ by focusing on the important role of robots in the future relationship with a person: care-receiving from a person and communication mediation between a person and others.},
  archive   = {C_IROS},
  author    = {Kanae Kochigami and Kei Okada and Masayuki Inaba},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636533},
  pages     = {1927-1934},
  title     = {Design of taking a walk with a robot that receives care from a person and indirectly mediates communication with strangers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Translating natural language instructions to computer
programs for robot manipulation. <em>IROS</em>, 1919–1926. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It is highly desirable for robots that work alongside humans to be able to understand instructions in natural language. Existing language conditioned imitation learning models directly predict the actuator commands from the image observation and the instruction text. Rather than directly predicting actuator commands, we propose translating the natural language instruction to a Python function which queries the scene by accessing the output of the object detector and controls the robot to perform the specified task. This enables the use of non-differentiable modules such as a constraint solver when computing commands to the robot. Moreover, the labels in this setup are significantly more informative computer programs that capture the intent of the expert rather than teleoperated demonstrations. We show that the proposed method performs better than training a neural network to directly predict the robot actions.},
  archive   = {C_IROS},
  author    = {Sagar Gubbi Venkatesh and Raviteja Upadrashta and Bharadwaj Amrutur},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636342},
  pages     = {1919-1926},
  title     = {Translating natural language instructions to computer programs for robot manipulation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An integrated approach to context-sensitive moral cognition
in robot cognitive architectures. <em>IROS</em>, 1911–1918. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Acceptance of social robots in human-robot collaborative environments depends on the robots’ sensitivity to human moral and social norms. Robot behavior that violates norms may decrease trust and lead human interactants to blame the robot and view it negatively. Hence, for long-term acceptance, social robots need to detect possible norm violations in their action plans and refuse to perform such plans. This paper integrates the Distributed, Integrated, Affect, Reflection, Cognition (DIARC) robot architecture (implemented in the Agent Development Environment (ADE)) with a novel place recognition module and a norm-aware task planner to achieve context-sensitive moral reasoning. This will allow the robot to reject inappropriate commands and comply with context-sensitive norms. In a validation scenario, our results show that the robot would not comply with a human command to violate a privacy norm in a private context.},
  archive   = {C_IROS},
  author    = {Ryan Blake Jackson and Sihui Li and Santosh Balajee Banisetty and Sriram Siva and Hao Zhang and Neil Dantam and Tom Williams},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636434},
  pages     = {1911-1918},
  title     = {An integrated approach to context-sensitive moral cognition in robot cognitive architectures},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Collaborative storytelling with social robots.
<em>IROS</em>, 1903–1910. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Storytelling plays a central role in human socializing and entertainment, and research on conducting storytelling with robots is gaining interest. However, much of this research assumes that story content is curated. In this paper, we expand the recently-proposed task of collaborative storytelling, where an intelligent agent and a person collaborate to create a unique story by taking turns adding to it, for application to social robot and consider the design implications that arise. Since latency can be detrimental to human-robot interaction, we examine the performance-latency trade-offs of an existing generate-and-rank-based approach to collaborative storytelling by finding the optimal ranker’s sample size that strikes the best balance between quality and computational cost. We improve on existing evaluation that was previously based on system-generated stories by having human participants play the collaborative storytelling game with our system and comparing the stories they create with our system to a naive baseline. Finally, we conduct a pilot elicitation survey that sheds light on issues to consider when adapting our collaborative storytelling system to a social robot. Our evaluation shows that participants have a positive view of collaborative storytelling with a social robot and consider rich, emoting capabilities to be key to enjoyment.},
  archive   = {C_IROS},
  author    = {Eric Nichols and Leo Gao and Yurii Vasylkiv and Randy Gomez},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636409},
  pages     = {1903-1910},
  title     = {Collaborative storytelling with social robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A safety-aware architecture for task scheduling and
execution for human-robot collaboration. <em>IROS</em>, 1895–1902. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In collaborative robotic applications, human and robot have to work together to accomplish a common job, composed by a set of tasks. In order to achieve an efficient human-robot collaboration (HRC), it is important to have an integration between a proper task scheduling strategy and a task execution strategy. The first must deal with the variability of the two agents, while the second must deal with the safety standards. In this paper, we propose an integrated architecture for task scheduling and execution in a collaborative cell. The tasks are dynamically scheduled handling the uncertainity in both the human and the robot behaviors. Subsequently, at the execution level, the task is accomplished computing trajectories comply with the safety regulations. The planning information are mutually integrated in real-time with the scheduling procedure in order improve the HRC.},
  archive   = {C_IROS},
  author    = {Andrea Pupa and Cristian Secchi},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636855},
  pages     = {1895-1902},
  title     = {A safety-aware architecture for task scheduling and execution for human-robot collaboration},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning and interactive design of shared control templates.
<em>IROS</em>, 1887–1894. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Controlling a robotic arm to achieve manipulation tasks is challenging for humans. Especially if only low-dimensional input signals can be provided, as is often the case for users with motor impairments. Using shared control to provide task-specific guidance and constraints facilitates control – for instance with the Shared Control Templates (SCT) framework – and enables even complex activities of daily living to be performed successfully. However, designing SCTs is a laborious task requiring robotic expertise. To make such design easier and faster, we propose a method for semi-automatically designing SCTs on the basis of demonstrations. Furthermore, we propose two similarity metrics, and demonstrate how these can be used to transfer knowledge from one SCT to another. We demonstrate that the SCTs so acquired can be successfully used in shared control for everyday tasks such as opening a drawer or a cupboard on our assistive robot EDAN.},
  archive   = {C_IROS},
  author    = {Gabriel Quere and Samuel Bustamante and Annette Hagengruber and Jörn Vogel and Franz Steinmetz and Freek Stulp},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636047},
  pages     = {1887-1894},
  title     = {Learning and interactive design of shared control templates},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extending referring expression generation through shared
knowledge about past human-robot collaborative activity. <em>IROS</em>,
1879–1886. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Being able to refer to an object, a person, or a place in a non-ambiguous manner is a need when one has to achieve collaborative activities with a partner. This is the so-called Referring Expression Generation (REG) problem. While widely used for Human-Robot Interaction, state of the art approaches restrict its use to the current environment. We propose a novel extension to the REG which takes full advantage of the Human-Robot shared knowledge about past actions as additional information to generate Referring Expressions. We show that our approach is usable with a domain-independent ontology as a knowledge base and that it can also use a semantic representation of past activity to generate RE. We illustrate our method through simulated situations and discuss its efficiency and pertinence.},
  archive   = {C_IROS},
  author    = {Guillaume Sarthou and Guilhem Buisan and Aurélie Clodic and Rachid Alami},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636796},
  pages     = {1879-1886},
  title     = {Extending referring expression generation through shared knowledge about past human-robot collaborative activity},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An analysis of human-robot information streams to inform
dynamic autonomy allocation. <em>IROS</em>, 1872–1878. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A dynamic autonomy allocation framework automatically shifts how much control lies with the human versus the robotics autonomy, for example based on factors such as environmental safety or user preference. To investigate the question of which factors should drive dynamic autonomy allocation, we perform a human subject study to collect ground truth data that shifts between levels of autonomy during shared-control robot operation. Information streams from the human, the interaction between the human and the robot, and the environment are analyzed. Machine learning methods—both classical and deep learning—are trained on this data. An analysis of information streams from the human-robot team suggests features which capture the interaction between the human and the robotics autonomy are the most informative in predicting when to shift autonomy levels. Even the addition of data from the environment does little to improve upon this predictive power. The features learned by deep networks, in comparison to the hand-engineered features, prove variable in their ability to represent shift-relevant information. This work demonstrates the classification power of human-only and human-robot interaction information streams for use in the design of shared-control frameworks, and provides insights into the comparative utility of various data streams and methods to extract shift-relevant information from those data.},
  archive   = {C_IROS},
  author    = {Christopher X. Miller and Temesgen Gebrekristos and Michael Young and Enid Montague and Brenna Argall},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636637},
  pages     = {1872-1878},
  title     = {An analysis of human-robot information streams to inform dynamic autonomy allocation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving competence via iterative state space refinement.
<em>IROS</em>, 1865–1871. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite considerable efforts by human designers, accounting for every unique situation that an autonomous robotic system deployed in the real world could face is often an infeasible task. As a result, many such deployed systems still rely on human assistance in various capacities to complete certain tasks while staying safe. Competence-aware systems (CAS) is a recently proposed model for reducing such reliance on human assistance while in turn optimizing the system’s global autonomous operation by learning its own competence. However, such systems are limited by a fixed model of their environment and may perform poorly if their a priori planning model does not include certain features that emerge as important over the course of the system’s deployment. In this paper, we propose a method for improving the competence of a CAS over time by identifying important state features missing from the system’s model and incorporating them into its state representation, thereby refining its state space. Our approach exploits information that exists in the standard CAS model and adds no extra work to the human. The result is an agent that better predicts human involvement, improving its competence, reliability, and overall performance.},
  archive   = {C_IROS},
  author    = {Connor Basich and Justin Svegliato and Allyson Beach and Kyle H. Wray and Stefan Witwicki and Shlomo Zilberstein},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636239},
  pages     = {1865-1871},
  title     = {Improving competence via iterative state space refinement},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cooperative assistance in robotic surgery through
multi-agent reinforcement learning. <em>IROS</em>, 1859–1864. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cognitive cooperative assistance in robot-assisted surgery holds the potential to increase quality of care in minimally invasive interventions. Automation of surgical tasks promises to reduce the mental exertion and fatigue of surgeons. In this work, multi-agent reinforcement learning is demonstrated to be robust to the distribution shift introduced by pairing a learned policy with a human team member. Multi-agent policies are trained directly from images in simulation to control multiple instruments in a sub task of the minimally invasive removal of the gallbladder. These agents are evaluated individually and in cooperation with humans to demonstrate their suitability as autonomous assistants. Compared to human teams, the hybrid teams with artificial agents perform better considering completion time (44.4\% to 71.2\% shorter) as well as number of collisions (44.7\% to 98.0\% fewer). Path lengths, however, increase under control of an artificial agent (11.4\% to 33.5\% longer). A multi-agent formulation of the learning problem was favored over a single-agent formulation on this surgical sub task, due to the sequential learning of the two instruments. This approach may be extended to other tasks that are difficult to formulate within the standard reinforcement learning framework. Multi-agent reinforcement learning may shift the paradigm of cognitive robotic surgery towards seamless cooperation between surgeons and assistive technologies.},
  archive   = {C_IROS},
  author    = {Paul Maria Scheikl and Balázs Gyenes and Tornike Davitashvili and Rayan Younis and André Schulze and Beat P. Müller-Stich and Gerhard Neumann and Martin Wagner and Franziska Mathis-Ullrich},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636193},
  pages     = {1859-1864},
  title     = {Cooperative assistance in robotic surgery through multi-agent reinforcement learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to share autonomy across repeated interaction.
<em>IROS</em>, 1851–1858. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Wheelchair-mounted robotic arms (and other assistive robots) should help their users perform everyday tasks. One way robots can provide this assistance is shared autonomy. Within shared autonomy, both the human and robot maintain control over the robot’s motion: as the robot becomes confident it understands what the human wants, it increasingly intervenes to automate the task. But how does the robot know what tasks the human may want to perform in the first place? Today’s shared autonomy approaches often rely on prior knowledge: for example, the robot must know the set of possible human goals a priori. In the long-term, however, this prior knowledge will inevitably break down — sooner or later the human will reach for a goal that the robot did not expect. In this paper we propose a learning approach to shared autonomy that takes advantage of repeated interactions. Learning to assist humans would be impossible if they performed completely different tasks at every interaction: but our insight is that users living with physical disabilities repeat important tasks on a daily basis (e.g., opening the fridge, making coffee, and having dinner). We introduce an algorithm that exploits these repeated interactions to recognize the human’s task, replicate similar demonstrations, and return control when unsure. As the human repeatedly works with this robot, our approach continually learns to assist tasks that were never specified beforehand: these tasks include both discrete goals (e.g., reaching a cup) and continuous skills (e.g., opening a drawer). Across simulations and an in-person user study, we demonstrate that robots leveraging our approach match existing shared autonomy methods for known goals, and outperform imitation learning baselines on new tasks. See videos here: https://youtu.be/NazeLVbQ2og},
  archive   = {C_IROS},
  author    = {Ananth Jonnavittula and Dylan P. Losey},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636748},
  pages     = {1851-1858},
  title     = {Learning to share autonomy across repeated interaction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards safe in situ needle manipulation for robot assisted
lumbar injection in interventional MRI. <em>IROS</em>, 1835–1842. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Lumbar injection is an image-guided procedure performed manually for diagnosis and treatment of lower back pain and leg pain. Previously, we have developed and verified an MR-Conditional robotic solution to assisting the needle insertion process. Drawing on our clinical experiences, a virtual remote center of motion (RCM) constraint is implemented to enable our robot to mimic a clinician’s hand motion to adjust the needle tip position in situ. Force and image data are collected to study the needle behavior in gel phantoms during this motion, and a mechanics-based needle-tissue interaction model is proposed and evaluated to further examine the underlying physics. This work extends the commonly-adopted notion of an RCM for flexible needles, and introduces new motion parameters to describe the needle behavior. The model parameters can be tuned to match the experimental result to sub-millimeter accuracy, and this proposed needle manipulation method presents a safer alternative to laterally translating the needle during in situ needle adjustments.},
  archive   = {C_IROS},
  author    = {Yanzhou Wang and Gang Li and Ka-Wai Kwok and Kevin Cleary and Russell H. Taylor and Iulian Iordachita},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636220},
  pages     = {1835-1842},
  title     = {Towards safe in situ needle manipulation for robot assisted lumbar injection in interventional MRI},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analytical tip force estimation on tendon-driven catheters
through inverse solution of cosserat rod model. <em>IROS</em>,
1829–1834. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tip force estimation on continuum arms is of crucial clinical importance for catheter-based procedures, i.e., catheter-based ablation therapies. In this study, an analytical solution for force estimation based on inverse Cosserat rod modeling was proposed and validated. Initially, a previously validated Bezier-based shape interpolation was used to parameterize the deformation and the kinematics and balance equations of the catheter were derived thereof. The tip force estimation problem was formulated as an inverse problem with a functional minimization technique and was solved analytically. In the end, the proposed method was experimentally tested for accuracy and computation efficiency through a series of simulations and experiments. The results showed that the estimated forces were in agreement with reference measurement with a mean-absolute error of 0.024 ± 0.020 N and a computation time of 7 ± 5 ms per frame. The exhibited performance was comparable to other studies and was in compliance with the requirements of catheter-based procedures.},
  archive   = {C_IROS},
  author    = {Amir Hooshiar and Amir Sayadi and Mohammad Jolaei and Javad Dargahi},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636560},
  pages     = {1829-1834},
  title     = {Analytical tip force estimation on tendon-driven catheters through inverse solution of cosserat rod model},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SurRoL: An open-source reinforcement learning centered and
dVRK compatible platform for surgical robot learning. <em>IROS</em>,
1821–1828. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous surgical execution relieves tedious routines and surgeon’s fatigue. Recent learning-based methods, especially reinforcement learning (RL) based methods, achieve promising performance for dexterous manipulation, which usually requires the simulation to collect data efficiently and reduce the hardware cost. The existing learning-based simulation platforms for medical robots suffer from limited scenarios and simplified physical interactions, which degrades the real-world performance of learned policies. In this work, we designed SurRoL, an RL-centered simulation platform for surgical robot learning compatible with the da Vinci Research Kit (dVRK). The designed SurRoL integrates a user-friendly RL library for algorithm development and a real-time physics engine, which is able to support more PSM/ECM scenarios and more realistic physical interactions. Ten learning-based surgical tasks are built in the platform, which are common in the real autonomous surgical execution. We evaluate SurRoL using RL algorithms in simulation, provide in-depth analysis, deploy the trained policies on the real dVRK, and show that our SurRoL achieves better transferability in the real world.},
  archive   = {C_IROS},
  author    = {Jiaqi Xu and Bin Li and Bo Lu and Yun-Hui Liu and Qi Dou and Pheng-Ann Heng},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635867},
  pages     = {1821-1828},
  title     = {SurRoL: An open-source reinforcement learning centered and dVRK compatible platform for surgical robot learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Force feedback on hand rest function in master manipulator
for robotic surgery. <em>IROS</em>, 1815–1820. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In robotic surgeries employing the master-slave operation scheme, various haptic devices have been adopted as master manipulators. The main challenge of the haptic device is to contribute to a sensitive reaction to external forces for operators. Since force perception on fingertips is impaired by unstable entire hand condition, stable hand condition helps to improve force perception. This study proposed a master manipulator that enables a force feedback at the gripping fingertips on a stable hand condition with a hand rest function. In this framework, the hand rest function was realized by switching the impedance value in admittance control for the translation section of the master manipulator and the displaying force was generated based on the pivot point, which depends on only the movement of the translation section. With master-slave operation, an 8-drawing experiment was performed under multiple force feedback and hand conditions after evaluating the effectiveness of a hand rest on sensitivity for external force in a preliminary experiment. The results showed that the hand rest function contributed to effective force feedback.},
  archive   = {C_IROS},
  author    = {Solmon Jeong and Kotaro Tadano},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636632},
  pages     = {1815-1820},
  title     = {Force feedback on hand rest function in master manipulator for robotic surgery},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Feasibility of remote landmark identification for
cricothyrotomy using robotic palpation. <em>IROS</em>, 1808–1814. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cricothyrotomy is a life-saving emergency intervention that secures an alternate airway route after a neck injury or obstruction. The procedure starts with identifying the correct location (the cricothyroid membrane) for creating an incision to insert an endotracheal tube. This location is determined using a combination of visual and palpation cues. Enabling robot-assisted remote cricothyrotomy may extend this life-saving procedure to injured soldiers or patients who may not be readily accessible for on-site intervention during search-and-rescue scenarios. As a first step towards achieving this goal, this paper explores the feasibility of palpation-assisted landmark identification for cricothyrotomy. Using a cricothyrotomy training simulator, we explore several alternatives for in-situ remote localization of the cricothyroid membrane. These alternatives include a) unaided telemanipulation, b) telemanipulation with direct force feedback, c) telemanipulation with superimposed motion excitation for on-line stiffness estimation and display, and d) fully autonomous palpation scan initialized based on the user’s understanding of key anatomical landmarks. Using the manually digitized cricothyroid membrane location as ground truth, we compare these four methods for accuracy and repeatability of identifying the landmark for cricothyrotomy, time of completion, and ease of use. These preliminary results suggest that the accuracy of remote cricothyrotomy landmark identification is improved when the user is aided with visual and force cues. They also show that, with proper user initialization, landmark identification using remote palpation is feasible - therefore satisfying a key prerequisite for future robotic solutions for remote cricothyrotomy.},
  archive   = {C_IROS},
  author    = {Neel Shihora and Rashid Yasin and Ryan Walsh and Nabil Simaan},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636481},
  pages     = {1808-1814},
  title     = {Feasibility of remote landmark identification for cricothyrotomy using robotic palpation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-axis FBG-based tactile sensor for gripping in space.
<em>IROS</em>, 1794–1799. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tactile sensing can improve end-effector control and grasp quality, especially for free-flying robots where target approach and alignment present particular challenges. However, many current tactile sensing technologies are not suitable for the harsh environment of space. We present a tactile sensor that measures normal and biaxial shear strains in the pads of a gripper using a single optical fiber with Bragg grating (FBG) sensors. Compared to conventional wired solutions, the encapsulated optical fibers are immune to electromagnetic interference — critical in the harsh environment of space. Sampling is possible at over 1 kHz to detect dynamic events. We mount sensor pads on a custom two-fingered gripper with independent control of the distal and proximal phalanges, allowing for grip readjustment based on sensing data. Calibrated sensor data for forces match those from a commercial multiaxial load cell with an average 96.2\% RMS for all taxels. We demonstrate the gripper on tasks motivated by the Astrobee free-flying robots in the International Space Station (ISS): gripping corners, detecting misaligned grasps, and improving load sharing over the contact areas in pinch grasps.},
  archive   = {C_IROS},
  author    = {Samuel Frishman and Julia Di and Zulekha Karachiwalla and Richard J. Black and Kian Moslehi and Trey Smith and Brian Coltin and Bijan Moslehi and Mark R. Cutkosky},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635998},
  pages     = {1794-1799},
  title     = {A multi-axis FBG-based tactile sensor for gripping in space},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-chamber smart suction cup for adaptive gripping and
haptic exploration. <em>IROS</em>, 1786–1793. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel robot end-effector for gripping and haptic exploration. Tactile sensing through suction flow monitoring is achieved with a new suction cup design that contains multiple chambers for air flow. Each chamber connects with its own remote pressure transducer, which enables both absolute and differential pressure measures between chambers. By changing the overall vacuum applied to this smart suction cup, it can perform different functions such as gentle haptic exploration (low pressure) and monitoring breaks in the seal during strong astrictive gripping (high pressure). Haptic exploration of surfaces through sliding and palpation can guide the selection of suction grasp locations and help to identify the local surface geometry. During suction gripping, a trained LSTM network can localize breaks in the suction seal between four quadrants with up to 97\% accuracy and detects breaks in the suction seal early enough to avoid total grasp failure.},
  archive   = {C_IROS},
  author    = {Tae Myung Huh and Kate Sanders and Michael Danielczuk and Monica Li and Yunliang Chen and Ken Goldberg and Hannah S. Stuart},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635852},
  pages     = {1786-1793},
  title     = {A multi-chamber smart suction cup for adaptive gripping and haptic exploration},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-contained kinematic calibration of a novel whole-body
artificial skin for human-robot collaboration. <em>IROS</em>, 1778–1785.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present an accelerometer-based kinematic calibration algorithm to accurately estimate the pose of multiple sensor units distributed along a robot body. Our approach is self-contained, can be used on any robot provided with a Denavit-Hartenberg kinematic model, and on any skin equipped with Inertial Measurement Units (IMUs). To validate the proposed method, we first conduct extensive experimentation in simulation and demonstrate a sub-cm positional error from ground truth data—an improvement of six times with respect to prior work; subsequently, we then perform a real-world evaluation on a seven degrees-of-freedom collaborative platform. For this purpose, we additionally introduce a novel design for a stand-alone artificial skin equipped with an IMU for use with the proposed algorithm and a proximity sensor for sensing distance to nearby objects. In conclusion, in this work, we demonstrate seamless integration between a novel hardware design, an accurate calibration method, and preliminary work on applications: the high positional accuracy effectively enables to locate distributed proximity data and allows for a distributed avoidance controller to safely avoid obstacles and people without the need of additional sensing.},
  archive   = {C_IROS},
  author    = {Kandai Watanabe and Matthew Strong and Mary West and Caleb Escobedo and Ander Aramburu and Krishna Chaitanya Kodur and Alessandro Roncone},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636493},
  pages     = {1778-1785},
  title     = {Self-contained kinematic calibration of a novel whole-body artificial skin for human-robot collaboration},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A low-cost modular system of customizable, versatile, and
flexible tactile sensor arrays. <em>IROS</em>, 1771–1777. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The key role of tactile sensing for human grasping and manipulation is widely acknowledged, but most industrial robot grippers and even multi-fingered hands are still designed and used without any tactile sensors. While the basic design principles for resistive or capacitive sensors are well known, several factors keep tactile sensing from large-scale deployment — high sensor costs, short lifespan, poor reliability, difficult production processes, a lack of suitable software and tools for system integration, and the unique requirement for tactile sensors to conform to application-specific shapes.In this work, we describe a very simple but efficient approach to design low-cost resistive matrix sensors, where sensor layout and geometry, taxel-size, and measurement sensitivity can be customized over a wide range. Sensor assembly needs nothing more than a hobby cutting plotter for precise cutting of aluminum tape and Velostat foils, as well as adhesive plastic tape. Our electronics combines transimpedance amplifiers with common Arduino microcontrollers, supporting standard communication protocols, and using either cabled or wireless data transfer to the host. We present three different application examples and sketch our ROS software for sensor calibration and visualization. All parts of our project, including detailed building instructions, bill-of-materials, electronics, and firmware are available open-source.},
  archive   = {C_IROS},
  author    = {Niklas Fiedler and Philipp Ruppel and Yannick Jonetzko and Norman Hendrich and Jianwei Zhang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635851},
  pages     = {1771-1777},
  title     = {A low-cost modular system of customizable, versatile, and flexible tactile sensor arrays},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AuraSense: Robot collision avoidance by full surface
proximity detection. <em>IROS</em>, 1763–1770. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Perceiving obstacles and avoiding collisions is fundamental to the safe operation of a robot system, particularly when the robot must operate in highly dynamic human environments. Proximity detection using on-robot sensors can be used to avoid or mitigate impending collisions. However, existing proximity sensing methods are orientation and placement dependent, resulting in blind spots even with large numbers of sensors. In this paper, we introduce the phenomenon of the Leaky Surface Wave (LSW), a novel sensing modality, and present AuraSense, a proximity detection system using the LSW. AuraSense is the first system to realize no-dead-spot proximity sensing for robot arms. It requires only a single pair of piezoelectric transducers, and can easily be applied to off-the-shelf robots with minimal modifications. We further introduce a set of signal processing techniques and a lightweight neural network to address the unique challenges in using the LSW for proximity sensing. Finally, we demonstrate a prototype system consisting of a single piezoelectric element pair on a robot manipulator, which validates our design. We conducted several micro benchmark experiments and performed more than 2000 on-robot proximity detection trials with various potential robot arm materials, colliding objects, approach patterns, and robot movement patterns. AuraSense achieves 100\% and 95.3\% true positive proximity detection rates when the arm approaches static and mobile obstacles respectively, with a true negative rate over 99\%, showing the real-world viability of this system.},
  archive   = {C_IROS},
  author    = {Xiaoran Fan and Riley Simmons-Edler and Daewon Lee and Larry Jackel and Richard Howard and Daniel Lee},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635919},
  pages     = {1763-1770},
  title     = {AuraSense: Robot collision avoidance by full surface proximity detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extended tactile perception: Vibration sensing through tools
and grasped objects. <em>IROS</em>, 1755–1762. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans display the remarkable ability to sense the world through tools and other held objects. For example, we are able to pinpoint impact locations on a held rod and tell apart different textures using a rigid probe. In this work, we consider how we can enable robots to have a similar capacity, i.e., to embody tools and extend perception using standard grasped objects. We propose that vibro-tactile sensing using dynamic tactile sensors on the robot fingers, along with machine learning models, enables robots to decipher contact information that is transmitted as vibrations along rigid objects. This paper reports on extensive experiments using the BioTac micro-vibration sensor and a new event dynamic sensor, the NUSkin, capable of multi-taxel sensing at 4 kHz. We demonstrate that fine localization on a held rod is possible using our approach (with errors less than 1 cm on a 20 cm rod). Next, we show that vibro-tactile perception can lead to reasonable grasp stability prediction during object handover, and accurate food identification using a standard fork. We find that multi-taxel vibro-tactile sensing at a sufficiently high sampling rate (above 2 kHz) led to the best performance across the various tasks and objects. Taken together, our results provide both evidence and guidelines for using vibro-tactile perception to extend tactile perception, which we believe will lead to enhanced competency with tools and better physical human-robot interaction.},
  archive   = {C_IROS},
  author    = {Tasbolat Taunyazov and Luar Shui Song and Eugene Lim and Hian Hian See and David Lee and Benjamin C.K. Tee and Harold Soh},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636677},
  pages     = {1755-1762},
  title     = {Extended tactile perception: Vibration sensing through tools and grasped objects},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A soft somesthetic robotic finger based on conductive
working liquid and an origami structure. <em>IROS</em>, 1748–1754. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The tactile and proprioceptive sensation increases human manipulability, and soft tissue compliance stabilizes the grasping function. However, it is challenging to transpose this system to the small confined space of soft robotic fingers due to the material properties and complex wiring entailed. Furthermore, soft robotic fingers also incorporate actuating components, making such a system more difficult to bring to fruition. Therefore, optimizing soft robotic finger structure for greater functionality and manufacturability would be a desirable innovation. In this study, we developed a soft somesthetic robotic finger based on the conductive working liquid and an origami structure. The proposed design comprises an origami structure, porous scaffolds, and a silicone-coated fabric outer layer. The robotic finger was filled with conductive liquid used for both somesthetic sensing and bending actuation simultaneously. The origami structure was fabricated by connecting printed circuit boards (PCBs) and a polyimide film, with electrodes embedded on each PCB to enable somesthetic sensing. The electrodes were used to inject currents and measure voltage, with the measured data then used to reconstruct the deformation map and hinge angle by means of electrical resistance tomography (ERT). The experimental results confirm that the robotic finger could acquire tactile and proprioceptive information in real-time.},
  archive   = {C_IROS},
  author    = {Junhwi Cho and Kyungseo Park and Hwayeong Jeong and Jung Kim},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636431},
  pages     = {1748-1754},
  title     = {A soft somesthetic robotic finger based on conductive working liquid and an origami structure},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Constant fluidic mass control for soft actuators using
artificial neural network algorithm. <em>IROS</em>, 1732–1739. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft fluidic actuators are increasingly being used for wearable haptic devices due to their high energy density and low encumbrance. These actuators are typically controlled using constant fluidic pressure control (CFPC), where the actuator pressure is switched between a high pressure source and atmospheric pressure using a fluidic valve. However, this type of control has several limitations for soft actuators including limited dynamic range, slow actuator response, low pressure control resolution and unnatural haptic interaction. In this paper, we present a novel control strategy for soft fluidic actuators, called constant fluidic mass control (CFMC), where the mass of fluid introduced into the actuator is kept constant during actuation, rather than the pressure as in CFPC. Our experimental results show that compared to CFPC, CFMC results in a larger dynamic range of actuator output forces, faster actuator response time to reach a desired target pressure, and higher resolution of pressure control, which makes it particularly useful for wearable haptics. In addition, CFMC enables analog pressure control and we present a neural-network-based supervised learning algorithm for accurate pressure control of soft actuators. Results show that our algorithm can predict actuator pressure with an accuracy of 99\% and can be generalized to different soft TPU-fabric fluidic actuators.},
  archive   = {C_IROS},
  author    = {Heng Xu and Priyanshu Agarwal and Benjamin Stephens-Fripp},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636630},
  pages     = {1732-1739},
  title     = {Constant fluidic mass control for soft actuators using artificial neural network algorithm},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Task driven skill learning in a soft-robotic arm.
<em>IROS</em>, 1716–1723. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we introduce a novel technique that aims to dynamically control a two-module bio-inspired soft-robotic arm in order to qualitatively reproduce a path defined by sparse way-points. The main idea behind this work is based on the assumption that a complex trajectory may be derived as a combination of a discrete set of parameterizable simple movements, as suggested by Movement Primitive (MP) theory. Capitalising on recent advances in this field, the proposed controller uses a Probabilistic MP (ProMP) model which initially creates an abstract mapping in the primitive-level between the task and the actuation space, and subsequently guides the movement’s composition by exploiting its unique properties - conditioning and blending. At the same time, a learning-based adaptive controller updates the composition parameters by estimating the inverse kinematics of the robot, while an auxiliary process through replanning ensures that the trajectory complies with the new estimation. The learning architecture is evaluated on both a simulation model, and a real soft-robotic arm. The research findings show that the proposed methodology constitutes a novel approach that successfully manages to simplify the trajectory control task for robots of complex dynamics when high-precision is not required.},
  archive   = {C_IROS},
  author    = {Paris Oikonomou and Athanasios Dometios and Mehdi Khamassi and Costas S. Tzafestas},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636812},
  pages     = {1716-1723},
  title     = {Task driven skill learning in a soft-robotic arm},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Position control and variable-height trajectory tracking of
a soft pneumatic legged robot. <em>IROS</em>, 1708–1709. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft pneumatic legged robots show promise in their ability to traverse a range of different types of terrain, including natural unstructured terrain met in applications like precision agriculture. They can adapt their body morphology to the intricacies of the terrain at hand, thus enabling robust and resilient locomotion. In this paper we capitalize upon recent developments on soft pneumatic legged robots to introduce a closed-loop trajectory tracking control scheme for operation over flat ground. Closed-loop pneumatic actuation feedback is achieved via a compact and portable pneumatic regulation board. Experimental results reveal that our soft legged robot can precisely control its body height and orientation while in quasi-static operation based on a geometric model. The robot can track both straight line and curved trajectories as well as variable-height trajectories. This work lays the basis to enable autonomous navigation for soft legged robots.},
  archive   = {C_IROS},
  author    = {Zhichao Liu and Konstantinos Karydis},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635966},
  pages     = {1708-1709},
  title     = {Position control and variable-height trajectory tracking of a soft pneumatic legged robot},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Soft manipulator fault detection and identification using
ANC-based LSTM. <em>IROS</em>, 1702–1707. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Timely fault detection and identification (FDI) of soft manipulators are critical in the design of surgical systems to improve reliability. However, due to the intrinsic compliance of soft manipulators, their end effectors vibrate during the dynamic control process, which introduces noise into the measured signals and makes FDI of soft manipulators challenging. This paper proposes a novel method to accomplish these tasks based on Long Short Term Memory (LSTM) recurrent neural network. Based on LSTM network, a new Attention-based Noise Compensation (ANC) module is proposed to enable the network to filter the noise merged with signals input in a self-supervision manner. Moreover, weighted cross entropy loss is introduced to balance the normal and faulty samples in the training set. Of the 9930 samples presented to the model, 9489 are correctly diagnosed in less than 1.0 second, which implies that the method can learn the spatial and temporal dependence of the signals and distinguish the healthy modes from the faulty ones. Finally, we compare the ANC-based method with the vanilla LSTM method and the state-of-art Bruin et al. method. From the comparison, we conclude that the ANC-based method proposed in this paper not only shortens the time cost of the FDI process but also suppresses the sensitivity of diagnosis results to noise. Source code, pre-trained models and dataset are available on https://github.com/IRMVLab/ANC-LSTM-fault-detection.},
  archive   = {C_IROS},
  author    = {Haoyuan Gu and Hanjiang Hu and Hesheng Wang and Weidong Chen},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636868},
  pages     = {1702-1707},
  title     = {Soft manipulator fault detection and identification using ANC-based LSTM},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sim2Sim evaluation of a novel data-efficient differentiable
physics engine for tensegrity robots. <em>IROS</em>, 1694–1701. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning policies in simulation is promising for reducing human effort when training robot controllers. This is especially true for soft robots that are more adaptive and safe but also more difficult to accurately model and control. The sim2real gap is the main barrier to successfully transfer policies from simulation to a real robot. System identification can be applied to reduce this gap but traditional identification methods require a lot of manual tuning. Data-driven alternatives can tune dynamical models directly from data but are often data hungry, which also incorporates human effort in collecting data. This work proposes a data-driven, end-to-end differentiable simulator focused on the exciting but challenging domain of tensegrity robots. To the best of the authors’ knowledge, this is the first differentiable physics engine for tensegrity robots that supports cable, contact, and actuation modeling. The aim is to develop a reasonably simplified, data-driven simulation, which can learn approximate dynamics with limited ground truth data. The dynamics must be accurate enough to generate policies that can be transferred back to the ground-truth system. As a first step in this direction, the current work demonstrates sim2sim transfer, where the unknown physical model of MuJoCo acts as a ground truth system. Two different tensegrity robots are used for evaluation and learning of locomotion policies, a 6-bar and a 3-bar tensegrity. The results indicate that only 0.25\% of ground truth data are needed to train a policy that works on the ground truth system when the differentiable engine is used for training against training the policy directly on the ground truth system.},
  archive   = {C_IROS},
  author    = {Kun Wang and Mridul Aanjaneya and Kostas Bekris},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636783},
  pages     = {1694-1701},
  title     = {Sim2Sim evaluation of a novel data-efficient differentiable physics engine for tensegrity robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pairwise preferences-based optimization of a path-based
velocity planner in robotic sealing tasks. <em>IROS</em>, 1674–1681. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Production plants are being re-designed to implement human-centered solutions. Especially considering high added-value operations, robots are required to optimize their behavior to achieve a task quality at least comparable to the one obtained by the skilled operators. A manual programming and tuning of the manipulator is not an efficient solution, requiring to adopt towards automated strategies. Adding external sensors (e.g., cameras) increases the robotic cell complexity and it doesn’t solve the issue since it is usually difficult to build explicit reward functions measuring the robot performance, while it is easier for the user to define a qualitative comparison between two experiments. According to these needs, in this paper, the recently-developed preferences-based optimization approach GLISp is employed and adapted to tune the novel developed path-based velocity planner. The implemented solution defines an intuitive human-centered procedure, capable of transferring (through pairwise preferences between experiments) the task knowledge from the operator to the manipulator. A Franka EMIKA panda robot has been employed as a test platform to perform a robotic sealing task (i.e., material deposition task), validating the proposed methodology. The proposed approach has been compared with a programming by demonstration approach, and with the manual tuning of the path-based velocity planner. Achieved results demonstrate the improved deposition quality obtained with the proposed optimized path-based velocity planner methodology in a limited number of experimental trials (20).},
  archive   = {C_IROS},
  author    = {Loris Roveda and Beatrice Maggioni and Elia Marescotti and Asad Ali Shahid and Andrea Maria Zanchettin and Alberto Bemporad and Dario Piga},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636184},
  pages     = {1674-1681},
  title     = {Pairwise preferences-based optimization of a path-based velocity planner in robotic sealing tasks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time hamilton-jacobi reachability analysis of
autonomous system with an FPGA. <em>IROS</em>, 1666–1673. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hamilton-Jacobi (HJ) reachability analysis is a powerful technique used to verify the safety of autonomous systems. HJ reachability is ideal for analysing nonlinear systems with disturbances and flexible set representations. A drawback to this approach is that it suffers from the curse of dimensionality, which prevents real-time deployment on safety-critical systems. In this paper, we show that a customized hardware design on an Field Programmable Gate Array (FPGA) could accelerate 4D grid-based HJ reachability analysis up to 14 times compared to an optimized implementation and 103 times compared to state-of-the-art MATLAB toolboxes on a 16-thread CPU. Because of this, we are able to achieve guaranteed real- time collision avoidance in dynamic environments that abruptly change with a 4D car model by re-solving the HJ partial differential equation (PDE) at a frequency of 4Hz on an FPGA. Our design can overcome the complex data access pattern while taking advantage of the parallel nature of the computations for solving the HJ PDE. The low latency of our computation is consistent, which is crucial for safety-critical systems. The methodology presented here is without loss of generality: it can potentially be applied to different systems dynamics, and more- over, leveraged for higher dimensional systems. We validate our approach in real world collision avoidance experiments with a robot car in a changing environment. We also provide the code of our hardware design and an AWS AFI image.},
  archive   = {C_IROS},
  author    = {Minh Bui and Michael Lu and Reza Hojabr and Mo Chen and Arrvindh Shriraman},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636410},
  pages     = {1666-1673},
  title     = {Real-time hamilton-jacobi reachability analysis of autonomous system with an FPGA},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rapid convex optimization of centroidal dynamics using block
coordinate descent. <em>IROS</em>, 1658–1665. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we explore the use of block coordinate descent (BCD) to optimize the centroidal momentum dynamics for dynamically consistent multi-contact behaviors. The centroidal dynamics have recently received a large amount of attention in order to create physically realizable motions for robots with hands and feet while being computationally more tractable than full rigid body dynamics models. Our contribution lies in exploiting the structure of the dynamics in order to simplify the original non-convex problem into two convex subproblems. We iterate between these two subproblems for a set number of iterations or until a consensus is reached. We explore the properties of the proposed optimization method for the centroidal dynamics and verify in simulation that motions generated by our approach can be tracked by the quadruped Solo12. In addition, we compare our method to a recently proposed convexification using a sequence of convex relaxations as well as a more standard interior point method used in the off-the-shelf solver IPOPT to show that our approach finds similar, if not better, trajectories (in terms of cost), and is more than four times faster than both approaches. Finally, compared to previous approaches, we note its practicality due to the convex nature of each subproblem which allows our method to be used with any off-the-shelf quadratic programming solver.},
  archive   = {C_IROS},
  author    = {Paarth Shah and Avadesh Meduri and Wolfgang Merkt and Majid Khadiv and Ioannis Havoutis and Ludovic Righetti},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635856},
  pages     = {1658-1665},
  title     = {Rapid convex optimization of centroidal dynamics using block coordinate descent},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning environment constraints in collaborative robotics:
A decentralized leader-follower approach. <em>IROS</em>, 1636–1641. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a leader-follower hierarchical strategy for two robots collaboratively transporting an object in a partially known environment with obstacles. Both robots sense the local surrounding environment and react to obstacles in their proximity. We consider no explicit communication, so the local environment information and the control actions are not shared between the robots. At any given time step, the leader solves a model predictive control (MPC) problem with its known set of obstacles and plans a feasible trajectory to complete the task. The follower estimates the inputs of the leader and uses a policy to assist the leader while reacting to obstacles in its proximity. The leader infers obstacles in the follower’s vicinity by using the difference between the predicted and the real-time estimated follower control action. A method to switch the leader-follower roles is used to improve the control performance in tight environments. The efficacy of our approach is demonstrated with detailed comparisons to two alternative strategies, where it achieves the highest success rate, while completing the task fastest.},
  archive   = {C_IROS},
  author    = {Monimoy Bujarbaruah and Yvonne R. Stürz and Conrad Holda and Karl H. Johansson and Francesco Borrelli},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636444},
  pages     = {1636-1641},
  title     = {Learning environment constraints in collaborative robotics: A decentralized leader-follower approach},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Phase-SLAM: Mobile structured light illumination for full
body 3D scanning. <em>IROS</em>, 1617–1624. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Full body scanning plays an important role in automated industrial manufacture and inspection. It requires the fusion of multi-view point cloud data and consumes large computational resources when the geometry of corresponding point clouds are unknown. Structured Light Illumination (SLI) is one of the most promising indoor 3D imaging techniques, but also has the same weakness for fusing the multi-view point clouds. This work proposes a mobile SLI system to alleviate this problem for full body 3D scanning. We derive the geometric relation between the phase and the motion of the mobile system, and develop an optimization approach to estimate the pose by comparing the phase image pair before and after the motion. Further more, a graph-based Simultaneous Localization And Mapping (SLAM) framework is built to improve the global accuracy of the pose estimation. By using the 2D phase comparison, the proposed method is more accurate than the normal 2D image feature point matching, and has lower computational complexity than the 3D point registration. The proposed system is experimented in both 3D Simulator and real environment. The yielded full body scan results demonstrated its higher accuracy and more efficiency than the current methods.},
  archive   = {C_IROS},
  author    = {Xi Zheng and Rui Ma and Rui Gao and Qi Hao},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636457},
  pages     = {1617-1624},
  title     = {Phase-SLAM: Mobile structured light illumination for full body 3D scanning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RoRD: Rotation-robust descriptors and orthographic views for
local feature matching. <em>IROS</em>, 1593–1600. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The use of local detectors and descriptors in typical computer vision pipelines works well until variations in viewpoint and appearance change become extreme. Past research in this area has typically focused on one of two approaches to this challenge: the use of projections into spaces more suitable for feature matching under extreme viewpoint changes, and attempting to learn features that are inherently more robust to viewpoint change. In this paper, we present a novel framework that combines the learning of invariant descriptors through data augmentation and orthographic viewpoint projection. We propose rotation-robust local descriptors, learnt through training data augmentation based on rotation homographies, and a correspondence ensemble technique that combines vanilla feature correspondences with those obtained through rotation-robust features. Using a range of benchmark datasets as well as contributing a new bespoke dataset for this research domain, we evaluate the effectiveness of the proposed approach on key tasks including pose estimation and visual place recognition. Our system outperforms a range of baseline and state-of-the-art techniques, including enabling higher levels of place recognition precision across opposing place viewpoints, and achieves practically useful performance levels even under extreme viewpoint changes. We reduce pose estimation error by 86.72\% relative to state of the art.},
  archive   = {C_IROS},
  author    = {Udit Singh Parihar and Aniket Gujarathi and Kinal Mehta and Satyajit Tourani and Sourav Garg and Michael Milford and K. Madhava Krishna},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636619},
  pages     = {1593-1600},
  title     = {RoRD: Rotation-robust descriptors and orthographic views for local feature matching},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Similarity-aware fusion network for 3D semantic
segmentation. <em>IROS</em>, 1585–1592. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a similarity-aware fusion network (SAFNet) to adaptively fuse 2D images and 3D point clouds for 3D semantic segmentation. Existing fusion-based methods achieve superior performances by integrating information from multiple modalities. However, they heavily rely on the projection-based correspondence between 2D pixels and 3D points and can only perform the information fusion in a fixed manner, so that their performances cannot be easily migrated to a more realistic scenario where the collected data often lack strict pair-wise features for prediction. To address this, we employ a late fusion strategy where we first learn the geometric and contextual similarities between the input and back-projected (from 2D pixels) point clouds and utilize them to guide the fusion of two modalities to further exploit complementary information. Specifically, we employ a geometric similarity module (GSM) to directly compare the spatial coordinate distributions of pair-wise 3D neighborhoods, and a contextual similarity module (CSM) to aggregate and compare spatial contextual information of corresponding central points. The two proposed modules can effectively measure how much image features can help predictions, enabling the network to adaptively adjust the contributions of two modalities to the final prediction of each point. Experimental results on ScanNetV2 [1] benchmark demonstrate that SAFNet outperforms existing state-of-the-art fusion-based approaches across various data integrity.},
  archive   = {C_IROS},
  author    = {Linqing Zhao and Jiwen Lu and Jie Zhou},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636494},
  pages     = {1585-1592},
  title     = {Similarity-aware fusion network for 3D semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Through the looking glass: Diminishing occlusions in robot
vision systems with mirror reflections. <em>IROS</em>, 1578–1584. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The quality of robot vision greatly affects the performance of automation systems, where occlusions stand as one of the biggest challenges. If the target is occluded from the sensor, detecting and grasping such objects become very challenging. For example, when multiple robot arms cooperate in a single workplace, occlusions will be created under the robot arm itself and hide objects underneath. While occlusions can be greatly reduced by installing multiple sensors, the increase in sensor costs cannot be ignored. Moreover, the sensor placements must be rearranged every time the robot operation routine and layout change.To diminish occlusions, we propose the first robot vision system with tilt-type mirror reflection sensing. By instantly tilting the sensor itself, we obtain two sensing results with different views: conventional direct line-of-sight sensing and non-line-of-sight sensing via mirror reflections. Our proposed system removes occlusions adaptively by detecting the occlusions in the scene and dynamically configuring the sensor tilt angle to sense the detected occluded area. Thus, sensor rearrangements are not required even after changes in robot operation or layout. Since the required hardware is the tilt-unit and a commercially available mirror, the cost increase is marginal. Through experiments, we show that our system can achieve a similar detection accuracy as systems with multiple sensors, regardless of the single-sensor implementation.},
  archive   = {C_IROS},
  author    = {Kentaro Yoshioka and Hidenori Okuni and Tuan Thanh Ta and Akihide Sai},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636366},
  pages     = {1578-1584},
  title     = {Through the looking glass: Diminishing occlusions in robot vision systems with mirror reflections},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A bio-inspired multi-sensor system for robust orientation
and position estimation. <em>IROS</em>, 1570–1577. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The nature animals have evolved highly efficient and robust organs that support their complex daily navigation tasks. To mimic animal’s navigation capability, we present a novel bio-inspired navigation system that draws inspirations from nature animals in this paper. The system consists of a three-axis magnetometer, a monocular camera, a micro inertial measurement unit (MIMU) and a polarization camera. While dead reckoning, orientation, and landmark recognition are considered as three most important capability for various species, we also designed corresponding algorithms based on the bio-inspired sensing system to perform autonomous navigation. In detail, the dead reckoning component is accomplished by integrating the monocular camera and the MIMU into a visual inertial odometry (VIO) and the orientation capability is achieved by combining the absolute orientation from the magnetometer with the relative orientation from the VIO. A loop closure detection is then used as the landmark recognition component to reduce the navigation drifts. All the three components are fused with a graph optimization method to generate the robust navigation result. To valid the proposed navigation sensing system and the algorithms, we have conducted series of experiments on ground and aerial unmanned vehicles, and have added orientation noise to testify the accuracy and robustness of the system.},
  archive   = {C_IROS},
  author    = {Jia Xie and Xiaofeng He and Jun Mao and Lilian Zhang and Guoliang Han and Wenzhou Zhou and Xiaoping Hu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635932},
  pages     = {1570-1577},
  title     = {A bio-inspired multi-sensor system for robust orientation and position estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Success weighted by completion time: A dynamics-aware
evaluation criteria for embodied navigation. <em>IROS</em>, 1562–1569.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present Success weighted by Completion Time (SCT), a new metric for evaluating navigation performance for mobile robots. Several related works on navigation have used Success weighted by Path Length (SPL) as the primary method of evaluating the path an agent makes to a goal location, but SPL is limited in its ability to properly evaluate agents with complex dynamics. In contrast, SCT explicitly takes the agent’s dynamics model into consideration, and aims to accurately capture how well the agent has approximated the fastest navigation behavior afforded by its dynamics. While several embodied navigation works use point-turn dynamics, we focus on unicycle-cart dynamics for our agent, which better exempli-fies the dynamics model of popular mobile robotics platforms (e.g., LoCoBot, TurtleBot, Fetch, etc.). We also present RRT*-Unicycle, an algorithm for unicycle dynamics that estimates the fastest collision-free path and completion time from a starting pose to a goal location in an environment containing obstacles. We experiment with deep reinforcement learning and reward shaping to train and compare the navigation performance of agents with different dynamics models. In evaluating these agents, we show that in contrast to SPL, SCT is able to capture the advantages in navigation speed a unicycle model has over a simpler point-turn model of dynamics. Lastly, we show that we can successfully deploy our trained models and algorithms outside of simulation in the real world. We embody our agents in a real robot to navigate an apartment, and show that they can generalize in a zero-shot manner. A video summary is available here: https://youtu.be/QOQ56XVIYVE},
  archive   = {C_IROS},
  author    = {Naoki Yokoyama and Sehoon Ha and Dhruv Batra},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636743},
  pages     = {1562-1569},
  title     = {Success weighted by completion time: A dynamics-aware evaluation criteria for embodied navigation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mapless humanoid navigation using learned latent dynamics.
<em>IROS</em>, 1555–1561. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel Deep Reinforcement Learning approach to address the mapless navigation problem, in which the locomotion actions of a humanoid robot are taken online based on the knowledge encoded in learned models. Planning happens by generating open-loop trajectories in a learned latent space that captures the dynamics of the environment. Our planner considers visual (RGB images) and non-visual observations (e.g., attitude estimations). This confers the agent upon awareness not only of the scenario, but also of its own state. In addition, we incorporate a termination likelihood predictor model as an auxiliary loss function of the control policy, which enables the agent to anticipate terminal states of success and failure. In this manner, the sample efficiency of the approach for episodic tasks is increased. Our model is evaluated on the NimbRo-OP2X humanoid robot that navigates in scenes avoiding collisions efficiently in simulation and with the real hardware.},
  archive   = {C_IROS},
  author    = {André Brandenburger and Diego Rodriguez and Sven Behnke},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636593},
  pages     = {1555-1561},
  title     = {Mapless humanoid navigation using learned latent dynamics},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Probabilistic visual navigation with bidirectional image
prediction. <em>IROS</em>, 1539–1546. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans can robustly follow a visual trajectory defined by a sequence of images (i.e. a video) regardless of substantial changes in the environment or the presence of obstacles. We aim at endowing similar visual navigation capabilities to mobile robots solely equipped with a RGB fisheye camera. We propose a novel probabilistic visual navigation system that learns to follow a sequence of images with bidirectional visual predictions conditioned on possible navigation velocities. By predicting bidirectionally (from start towards goal and vice versa) our method extends its predictive horizon enabling the robot to go around unseen large obstacles that are not visible in the video trajectory. Learning how to react to obstacles and potential risks in the visual field is achieved by imitating human teleoperators. Since the human teleoperation commands are diverse, we propose a probabilistic representation of trajectories that we can sample to find the safest path. We evaluate our navigation system quantitatively and qualitatively in multiple simulated and real environments and compare to state-of-the-art baselines. Our approach outperforms the most recent visual navigation methods with a large margin with regard to goal arrival rate, subgoal coverage rate, and success weighted by path length (SPL). Our method also generalizes to new robot embodiments never used during training.},
  archive   = {C_IROS},
  author    = {Noriaki Hirose and Shun Taguchi and Fei Xia and Roberto Martín-Martín and Kosuke Tahara and Masanori Ishigaki and Silvio Savarese},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636340},
  pages     = {1539-1546},
  title     = {Probabilistic visual navigation with bidirectional image prediction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DT-loc: Monocular visual localization on HD vector map using
distance transforms of 2D semantic detections. <em>IROS</em>, 1531–1538.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Localizing a vehicle on a prebuilt HD vector map is a prerequisite for many autonomous driving applications. Existing visual localization approaches usually require a separate local feature layer to function. The separate localization layer suffers from the robustness issue inherited from the local features. Also, it could be difficult to create a feature layer that aligns perfectly with an existing vector map. In this paper, we propose a monocular visual localization method that exploits the vector map directly as the localization layer. The method detects semantic traffic elements from the images and matches them with the vectors in the map. To deal with the harmful problem of false matches, we propose to align the vector map to the distance transforms of the semantic detections, which enables a non-explicit and differentiable data association process. The system is able to achieve centimeter and sub-meter accuracies in lateral and longitudinal directions, respectively.},
  archive   = {C_IROS},
  author    = {Chi Zhang and Hao Liu and Hao Li and Kun Guo and Kuiyuan Yang and Rui Cai and Zhiwei Li},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636419},
  pages     = {1531-1538},
  title     = {DT-loc: Monocular visual localization on HD vector map using distance transforms of 2D semantic detections},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). POMP++: Pomcp-based active visual search in unknown indoor
environments. <em>IROS</em>, 1523–1530. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we focus on the problem of learning online an optimal policy for Active Visual Search (AVS) of objects in unknown indoor environments. We propose POMP++, a planning strategy that introduces a novel formulation on top of the classic Partially Observable Monte Carlo Planning (POMCP) framework, to allow training-free online policy learning in unknown environments. We present a new belief reinvigoration strategy that enables the use of POMCP with a dynamically growing state space to address the online generation of the floor map. We evaluate our method on two public benchmark datasets, AVD that is acquired by real robotic platforms and Habitat ObjectNav that is rendered from real 3D scene scans, achieving the best success rate with an improvement of &gt;10\% over the state-of-the-art methods.},
  archive   = {C_IROS},
  author    = {Francesco Giuliari and Alberto Castellini and Riccardo Berra and Alessio Del Bue and Alessandro Farinelli and Marco Cristani and Francesco Setti and Yiming Wang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635866},
  pages     = {1523-1530},
  title     = {POMP++: Pomcp-based active visual search in unknown indoor environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-robot task assignment for aerial tracking with
viewpoint constraints. <em>IROS</em>, 1515–1522. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the problem of assigning a team of drones to autonomously capture a set desired shots of a dynamic target in the presence of obstacles. We present a two-stage planning pipeline that generates offline an assignment of drone to shots and locally optimizes online the viewpoint. Given desired shot parameters, the high-level planner uses a visibility heuristic to predict good times for capturing each shot and uses an Integer Linear Program to compute drone assignments. An online Model Predictive Control algorithm uses the assignments as reference to capture the shots. The algorithm is validated in hardware with a pair of drones and a remote controlled car.},
  archive   = {C_IROS},
  author    = {Aaron Ray and Alyssa Pierson and Hai Zhu and Javier Alonso-Mora and Daniela Rus},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636719},
  pages     = {1515-1522},
  title     = {Multi-robot task assignment for aerial tracking with viewpoint constraints},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An interleaved approach to trait-based task allocation and
scheduling. <em>IROS</em>, 1507–1514. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To realize effective heterogeneous multi-robot teams, researchers must leverage individual robots’ relative strengths and coordinate their individual behaviors. Specifically, heterogeneous multi-robot systems must answer three important questions: who (task allocation), when (scheduling), and how (motion planning). While specific variants of each of these problems are known to be NP-Hard, their interdependence only exacerbates the challenges involved in solving them together. In this paper, we present a novel framework that interleaves task allocation, scheduling, and motion planning. We introduce a search-based approach for trait-based time-extended task allocation named Incremental Task Allocation Graph Search (ITAGS). In contrast to approaches that solve the three problems in sequence, ITAGS’s interleaved approach enables efficient search for allocations while simultaneously satisfying scheduling constraints and accounting for the time taken to execute motion plans. To enable effective interleaving, we develop a convex combination of two search heuristics that optimizes the satisfaction of task requirements as well as the makespan of the associated schedule. We demonstrate the efficacy of ITAGS using detailed ablation studies and comparisons against two state-of-the-art algorithms in a simulated emergency response domain.},
  archive   = {C_IROS},
  author    = {Glen Neville and Andrew Messing and Harish Ravichandar and Seth Hutchinson and Sonia Chernova},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636569},
  pages     = {1507-1514},
  title     = {An interleaved approach to trait-based task allocation and scheduling},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Path optimization for cooperative mapping using multiple
robots with limited sensing capabilities. <em>IROS</em>, 1499–1506. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study addresses the problem of path optimization for conducting a mapping mission using a multi-robot system with limited sensing capability, which aims to ensure efficient mapping with emphasis on the cooperative aspect of the mission. To achieve the cooperative mapping, a new path planning algorithm is proposed which can take advantage of the multi-robot system while dealing with the lack of observability due to the nature of bearing-only or range-only sensing. The Fisher information matrix is used to estimate the mapping uncertainty affected by the robots’ geometric configuration. Also, a simple method to predict the convergence rate of the uncertainty over a short time horizon is presented for efficient path planning. The performance of the proposed algorithm is shown through simulations and compared with other path planning algorithms.},
  archive   = {C_IROS},
  author    = {Kyungseo Kim and Jinwhan Kim},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635934},
  pages     = {1499-1506},
  title     = {Path optimization for cooperative mapping using multiple robots with limited sensing capabilities},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Encirclement guaranteed cooperative pursuit with robust
model predictive control. <em>IROS</em>, 1473–1479. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies a novel encirclement guaranteed cooperative pursuit problem involving N pursuers and a single evader in an unbounded two-dimensional game domain. Throughout the game, the pursuers are required to maintain encirclement of the evader, i.e., the evader should always stay inside the convex hull generated by all the pursuers, in addition to achieving the classical capture condition. To tackle this challenging cooperative pursuit problem, a robust model predictive control (RMPC) based formulation framework is first introduced, which simultaneously accounts for the encirclement and capture requirements under the assumption that the evader’s action is unavailable to all pursuers. Despite the reformulation, the resulting RMPC problem involves a bilinear constraint due to the encirclement requirement. To further handle such a bilinear constraint, a novel encirclement guaranteed partitioning scheme is devised that simplifies the original bilinear RMPC problem to a number of linear tube MPC (TMPC) problems solvable in a decentralized manner. Simulation experiments demonstrate the effectiveness of the proposed solution framework. Furthermore, comparisons with existing approaches show that the explicit consideration of the encirclement condition significantly improves the chance of successful capture of the evader in various scenarios.},
  archive   = {C_IROS},
  author    = {Chen Wang and Hua Chen and Jia Pan and Wei Zhang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636127},
  pages     = {1473-1479},
  title     = {Encirclement guaranteed cooperative pursuit with robust model predictive control},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Combined routing and scheduling of heterogeneous transport
and service agents. <em>IROS</em>, 1466–1472. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper investigates servicing waypoints in a wide area using collaborative deployments of vehicles with heterogeneous range and mobility constraints. We formulate a joint planning problem for a single transport truck and multiple service drones in which the truck is constrained to a road and must deploy a team of range-constrained drones to visit waypoints. The need to deploy, collect, and redeploy drones over multiple flights introduces both route finding and scheduling aspects to this problem. We solve large problem instances by decoupling our approach into a service drone route finding phase and a transport truck scheduling phase. Numerical simulations explore the qualitative character of the driving schedule and the quantitative marginal value of adding additional drones to the team as a function of agent number and relative speed. The combination of road network constraints and range constraints make this problem especially relevant to wide area forestry, last-mile delivery, and ecological monitoring applications.},
  archive   = {C_IROS},
  author    = {Saaketh Narayan and James Paulos and Steven W. Chen and Sandeep Manjanna and Vijay Kumar},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636684},
  pages     = {1466-1472},
  title     = {Combined routing and scheduling of heterogeneous transport and service agents},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed event- and self-triggered coverage control with
speed constrained unicycle robots. <em>IROS</em>, 1458–1465. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Voronoi coverage control is a particular problem of importance in the area of multi-robot systems, which considers a network of multiple autonomous robots, tasked with optimally covering a large area. This is a common task for fleets of fixed-wing Unmanned Aerial Vehicles (UAVs), which are described in this work by a unicycle model with constant forward-speed constraints. We develop event-based control/communication algorithms to relax the resource requirements on wireless communication and control actuators, an important feature for battery-driven or otherwise energy-constrained systems. To overcome the drawback that the event-triggered algorithm requires continuous measurement of system states, we propose a self-triggered algorithm to estimate the next triggering time. Hardware experiments illustrate the theoretical results.},
  archive   = {C_IROS},
  author    = {Yuni Zhou and Lingxuan Kong and Stefan Sosnowski and Qingchen Liu and Sandra Hirche},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636524},
  pages     = {1458-1465},
  title     = {Distributed event- and self-triggered coverage control with speed constrained unicycle robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploring imitation learning for autonomous driving with
feedback synthesizer and differentiable rasterization. <em>IROS</em>,
1450–1457. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a learning-based planner that aims to robustly drive a vehicle by mimicking human drivers’ driving behavior. We leverage a mid-to-mid approach that allows us to manipulate the input to our imitation learning network freely. With that in mind, we propose a novel feedback synthesizer for data augmentation. It allows our agent to gain more driving experience in various previously unseen environments that are likely to encounter, thus improving overall performance. This is in contrast to prior works that rely purely on random synthesizers. Furthermore, rather than completely commit to imitating, we introduce task losses that penalize undesirable behaviors, such as collision, off-road, and so on. Unlike prior works, this is done by introducing a differentiable vehicle rasterizer that directly converts the waypoints output by the network into images. This effectively avoids the usage of heavyweight ConvLSTM networks, therefore, yields a faster model inference time. About the network architecture, we exploit an attention mechanism that allows the network to reason critical objects in the scene and produce better interpretable attention heatmaps. To further enhance the safety and robustness of the network, we add an optional optimization-based post-processing planner improving the driving comfort. We comprehensively validate our method’s effectiveness in different scenarios that are specifically created for evaluating self-driving vehicles. Results demonstrate that our learning-based planner achieves high intelligence and can handle complex situations. Detailed ablation and visualization analysis are included to further demonstrate each of our proposed modules’ effectiveness in our method.},
  archive   = {C_IROS},
  author    = {Jinyun Zhou and Rui Wang and Xu Liu and Yifei Jiang and Shu Jiang and Jiaming Tao and Jinghao Miao and Shiyu Song},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636795},
  pages     = {1450-1457},
  title     = {Exploring imitation learning for autonomous driving with feedback synthesizer and differentiable rasterization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PILOT: Efficient planning by imitation learning and
optimisation for safe autonomous driving. <em>IROS</em>, 1442–1449. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Achieving a proper balance between planning quality, safety and efficiency is a major challenge for autonomous driving. Optimisation-based motion planners are capable of producing safe, smooth and comfortable plans, but often at the cost of runtime efficiency. On the other hand, naïvely deploying trajectories produced by efficient-to-run deep imitation learning approaches might risk compromising safety. In this paper, we present PILOT a planning framework that comprises an imitation neural network followed by an efficient optimiser that actively rectifies the network’s plan, guaranteeing fulfilment of safety and comfort requirements. The objective of the efficient optimiser is the same as the objective of an expensive-to-run optimisation-based planning system that the neural network is trained offline to imitate. This efficient optimiser provides a key layer of online protection from learning failures or deficiency in out-of-distribution situations that might compromise safety or comfort. Using a state-of-the-art, runtime-intensive optimisation-based method as the expert, we demonstrate in simulated autonomous driving experiments in CARLA that PILOT achieves a seven-fold reduction in runtime when compared to the expert it imitates without sacrificing planning quality.},
  archive   = {C_IROS},
  author    = {Henry Pulver and Francisco Eiras and Ludovico Carozza and Majd Hawasly and Stefano V. Albrecht and Subramanian Ramamoorthy},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636862},
  pages     = {1442-1449},
  title     = {PILOT: Efficient planning by imitation learning and optimisation for safe autonomous driving},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust and recursively feasible real-time trajectory
planning in unknown environments. <em>IROS</em>, 1434–1441. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion planners for mobile robots in unknown environments face the challenge of simultaneously maintaining both robustness against unmodeled uncertainties and persistent feasibility of the trajectory-finding problem. That is, while dealing with uncertainties, a motion planner must update its trajectory, adapting to the newly revealed environment in real-time; failing to do so may involve unsafe circumstances. Many existing planning algorithms guarantee these by maintaining the clearance needed to perform an emergency brake, which is itself a robust and persistently feasible maneuver. However, such maneuvers are not applicable for systems in which braking is impossible or risky, such as fixed-wing aircraft. To that end, we propose a real-time robust planner that recursively guarantees persistent feasibility without any need of braking. The planner ensures robustness against bounded uncertainties and persistent feasibility by constructing a loop of sequentially composed funnels, starting from the receding horizon local trajectory’s forward reachable set. We implement the proposed algorithm for a robotic car tracking a speed-fixed reference trajectory. The experiment results show that the proposed algorithm can be run at faster than 16 Hz, while successfully keeping the system away from entering any dead end, to maintain safety and feasibility.},
  archive   = {C_IROS},
  author    = {Inkyu Jang and Dongjae Lee and Seungjae Lee and H. Jin Kim},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636048},
  pages     = {1434-1441},
  title     = {Robust and recursively feasible real-time trajectory planning in unknown environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CR-LSTM: Collision-prior guided social refinement for
pedestrian trajectory prediction. <em>IROS</em>, 1427–1433. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pedestrian trajectory prediction is a challenge because of the complex social interactions in context and the elusive intention of each pedestrian. Collision avoidance is one of the most common social interactions in real world, while existing data-driven works have not handled it well yet. In order to address this issue, we propose a framework that considers the theory about the minimum distance between each pedestrian-pedestrian pair and the corresponding time as the collision related prior knowledge. With the prior, our social refinement module, called Collision-prior Guided Refinement, can be guided to understand the collision situations of a crowd through a message passing mechanism. To focus on more useful information from context, we also introduce pedestrian-wise attention and collision gate to jointly judge collision potential for all pedestrian-pedestrian pairs. Experimental results demonstrate that our framework can achieve competitive results on ETH and UCY datasets by comparing with existing works. In addition, it indicates the superiority of our framework in the aspect of collision avoidance.},
  archive   = {C_IROS},
  author    = {Zhaoxin Su and Sanyuan Zhang and Wei Hua},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636722},
  pages     = {1427-1433},
  title     = {CR-LSTM: Collision-prior guided social refinement for pedestrian trajectory prediction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Path-constrained optimal trajectory planning for robot
manipulators with obstacle avoidance. <em>IROS</em>, 1421–1426. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we develop a novel path-constrained and collision-free optimal trajectory planning algorithm for robot manipulators in the presence of obstacles for the following problem: Given a desired sequence of discrete waypoints of robot configurations, a set of robot kinematic and dynamic constraints, and a set of obstacles, determine a time and jerk optimal and collision-free trajectory for the robot passing through the given waypoints with constant speed. Our approach in developing the robot path through the waypoints relies on the orthogonal collocation method where the states are represented with Legendre polynomials in the Barycentric form; the transcription process efficiently converts the continuous-time formulation of the optimal control problem (both time and jerk optimal) into a discrete non-linear program. In addition, we provide an efficient method for avoiding robot self-collisions (of joints and links) and collisions with workspace obstacles by modeling them as the union of spheres and cylinders in the workspace. The resulting collision free optimal trajectory provides smooth and constrained motion for the robot passing through all the waypoints in the given prescribed sequence. The proposed method is validated using numerical simulations and experiments on a six degree-of-freedom robot.},
  archive   = {C_IROS},
  author    = {Yalun Wen and Prabhakar R. Pagilla},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636674},
  pages     = {1421-1426},
  title     = {Path-constrained optimal trajectory planning for robot manipulators with obstacle avoidance},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised path regression networks. <em>IROS</em>,
1413–1420. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We demonstrate that challenging shortest path problems can be solved via direct spline regression from a neural network, trained in an unsupervised manner (i.e. without requiring ground truth optimal paths for training). To achieve this, we derive a geometry-dependent optimal cost function whose minima guarantees collision-free solutions. Our method beats state-of-the-art supervised learning baselines for shortest path planning, with a much more scalable training pipeline, and a significant speedup in inference time.},
  archive   = {C_IROS},
  author    = {Michal Pándy and Daniel Lenton and Ronald Clark},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636818},
  pages     = {1413-1420},
  title     = {Unsupervised path regression networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Trajectory optimization for rendezvous planning using
quadratic bézier curves. <em>IROS</em>, 1405–1412. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we consider a trajectory planning problem where an autonomous vehicle aims to rendezvous with another cooperating vehicle in minimum time. The first vehicle has kinematic constraints, consequently feasible trajectories must have a maximum curvature less than a specified limit. Rendezvous is said to occur at the instant that the two vehicles are collocated with the same heading. We propose a technique to construct a trajectory, composed of piecewise quadratic Bézier curves, that satisfies the vehicle motion constraints and achieves rendezvous in minimum time. The methodology begins by finding safe flight corridors, which are constructed from sequences of triangles using constrained Delaunay triangulation of the feasible space; the triangles define the bounds of Bézier curves. We formulate the necessary constraints for continuity and feasibility as functions of the control points that define the Bézier curves, and the resulting optimization problem is solved using a nonlinear programming solver. The techniques developed were tested using simulated scenarios, and we present the results which highlight the efficacy of the proposed solution approach. Furthermore, the algorithm was implemented and tested in a field test and those results are presented.},
  archive   = {C_IROS},
  author    = {Satyanarayana G. Manyam and David W. Casbeer and Isaac E. Weintraub and Colin Taylor},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636535},
  pages     = {1405-1412},
  title     = {Trajectory optimization for rendezvous planning using quadratic bézier curves},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Designing rotary linkages for polar motions. <em>IROS</em>,
1384–1391. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Polar linkages have two degrees-of-freedom (DOF) where one input joint angle controls the length of a radial segment while another controls its angle. Considering a theoretical planar robot model, this mapping between joint angles to output motions can be shown to be energetically advantageous over the ubiquitous two-revolute linkage. Since a polar linkage’s typical construction involves a moving prismatic joint, it is cumbersome to implement alongside rotary electromagnetic actuators offsetting any advantage. In this paper, we present a procedure for designing polar linkages using only revolute joints. The procedure starts with a pre-existing single DOF straight line linkage and then finds the dimensions of a three-link attachment to produce the second DOF. In the end, the straight line linkage actuates the polar length and the attachment actuates the polar angle. The design process is framed under optimization with an objective that is both polynomial and invariant to the number of discretization points. This enables the techniques of numerical continuation to efficiently find complete sets of minima. We demonstrate our procedure with an example in which multiple minima are found including the global minimum. This computed design solution is then fabricated in order to validate the designed kinematics.},
  archive   = {C_IROS},
  author    = {Aravind Baskar and Chang Liu and Mark Plecnik and Jonathan D. Hauenstein},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636587},
  pages     = {1384-1391},
  title     = {Designing rotary linkages for polar motions},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel design of mobile robotic system for opening and
transitioning through a watertight ship door. <em>IROS</em>, 1378–1383.
(<a href="https://doi.org/10.1109/IROS51168.2021.9635942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent offshore drilling activities have dramatically bloomed oil and gas production. Due to extreme weather, such as hurricanes and tsunamis, offshore oil platforms may need to be constantly monitored in case of unexpected dangers. Using robots to monitor and prevent these dangerous situations is a cost-effective and safer solution compared to any human involvement. However, one major drawback for robotic intervention is the ability of the robot to operate water tight ship doors designed for marina facilities. Current studies have been focused on robots manipulating normal doors in people&#39;s daily life. However their presented methodology may not be suitable for opening heavy watertight ship doors since they have a special locking structure which needs much larger torque to open. This study explores the possibility of opening heavy watertight ship doors with an ordinary mobile robot under remote control. Given the watertight ship door’s unique structure, a door opening tool with a pawl mechanism is designed for a mobile robot, which consists of a robotic arm and a wheeled platform, to complete the task. Experimental results have shown that the mobile robot can operate the tool to open the door and transition through the door.},
  archive   = {C_IROS},
  author    = {Wenyu Zuo and Rahul Venkatraman and Gangbing Song and Zheng Chen},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635942},
  pages     = {1378-1383},
  title     = {A novel design of mobile robotic system for opening and transitioning through a watertight ship door},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Embedding a nonlinear strict oscillatory mode into a
segmented leg. <em>IROS</em>, 1370–1377. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic legs often lag behind the performance of their biological counterparts. The inherent passive dynamics of natural legs largely influences the locomotion and can be abstracted through the spring-loaded inverted pendulum (SLIP) model. This model is often approximated in physical robotic legs using a leg with minimal mass. Our work aims to embed the SLIP dynamics by using a nonlinear strict oscillatory mode into a segmented robotic leg with significant mass, to minimize the control required for achieving periodic motions. For the first time, we provide a realization of a nonlinear oscillatory mode in a robotic leg prototype. This is achieved by decoupling the polar task dynamics and fulfilling the resulting conditions with the physical leg design. Extensive experiments validate that the robotic leg effectively embodies the strict mode. The decoupled leg-length dynamic is exhibited in leg configurations corresponding to the stance and flight phases of the locomotion task, both for the passive system and when actuating the motors. We additionally show that the leg retains this behavior while performing jumping in place experiments.},
  archive   = {C_IROS},
  author    = {Anna Sesselmann and Florian Loeffl and Cosimo Della Santina and Maximo A. Roa and Alin Albu-Schäffer},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636605},
  pages     = {1370-1377},
  title     = {Embedding a nonlinear strict oscillatory mode into a segmented leg},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Control-aware design optimization for bio-inspired quadruped
robots. <em>IROS</em>, 1354–1361. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a control-aware design optimization method for quadrupedal robots. In particular, we show that it is possible to analytically differentiate typical, inverse dynamics-based whole body controllers with respect to design parameters, and that gradient-based methods can be used to efficiently improve an initial morphological design according to well-established metrics. We apply our design optimization method to various types of quadrupedal robots, including designs that feature closed kinematic chains. The methodology we present enables a principled comparison of different types of optimized legged robot designs. Our experiments, for example, suggest that mechanically-coupled three-link leg designs present notable advantages in terms of performance and efficiency over the common two-link leg designs used in most quadrupedal robots today.},
  archive   = {C_IROS},
  author    = {Flavio De Vincenti and Dongho Kang and Stelian Coros},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636415},
  pages     = {1354-1361},
  title     = {Control-aware design optimization for bio-inspired quadruped robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attention augmented ConvLSTM for environment prediction.
<em>IROS</em>, 1346–1353. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safe and proactive planning in robotic systems generally requires accurate predictions of the environment. Prior work on environment prediction applied video frame prediction techniques to bird’s-eye view environment representations, such as occupancy grids. ConvLSTM-based frameworks used previously often result in significant blurring of the predictions, loss of static environment structure, and vanishing of moving objects, thus hindering their applicability for use in safety-critical applications. In this work, we propose two extensions to the ConvLSTM architecture to address these issues. We present the Temporal Attention Augmented ConvLSTM (TAAConvLSTM) and Self-Attention Augmented ConvLSTM (SAAConvLSTM) frameworks for spatiotemporal occupancy grid prediction, and demonstrate improved performance over baseline architectures on the real-world KITTI and Waymo datasets. We provide our implementation at https: //github.com/sisl/AttentionAugmentedConvLSTM.},
  archive   = {C_IROS},
  author    = {Bernard Lange and Masha Itkina and Mykel J. Kochenderfer},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636386},
  pages     = {1346-1353},
  title     = {Attention augmented ConvLSTM for environment prediction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Differentiable factor graph optimization for learning
smoothers. <em>IROS</em>, 1339–1345. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A recent line of work has shown that end-to-end optimization of Bayesian filters can be used to learn state estimators for systems whose underlying models are difficult to hand-design or tune, while retaining the core advantages of probabilistic state estimation. As an alternative approach for state estimation in these settings, we present an end-to-end approach for learning state estimators modeled as factor graph-based smoothers. By unrolling the optimizer we use for maximum a posteriori inference in these probabilistic graphical models, our method is able to learn probabilistic system models in the full context of an overall state estimator, while also taking advantage of the distinct accuracy and runtime advantages that smoothers offer over recursive filters. We study our approach using two fundamental state estimation problems, object tracking and visual odometry, where we demonstrate a significant improvement over existing baselines. Our work comes with an extensive code release, which includes training and evaluation scripts, as well as Python libraries for Lie theory and factor graph optimization: https://sites.google.com/view/diffsmoothing/.},
  archive   = {C_IROS},
  author    = {Brent Yi and Michelle A. Lee and Alina Kloss and Roberto Martín-Martín and Jeannette Bohg},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636300},
  pages     = {1339-1345},
  title     = {Differentiable factor graph optimization for learning smoothers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RaP-net: A region-wise and point-wise weighting network to
extract robust features for indoor localization. <em>IROS</em>,
1331–1338. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Feature extraction plays an important role in visual localization. Unreliable features on dynamic objects or repetitive regions will interfere with feature matching and challenge indoor localization greatly. To address the problem, we propose a novel network, RaP-Net, to simultaneously predict region-wise invariability and point-wise reliability, and then extract features by considering both of them. We also introduce a new dataset, named OpenLORIS-Location, to train the proposed network. The dataset contains 1553 images from 93 indoor locations. Various appearance changes between images of the same location are included and can help the model to learn the invariability in typical indoor scenes. Experimental results show that the proposed RaP-Net trained with OpenLORIS-Location dataset achieves excellent performance in the feature matching task and significantly outperforms state-of-the-arts feature algorithms in indoor localization. The RaPNet code and dataset are available at https://github.com/ivipsourcecode/RaP-Net.},
  archive   = {C_IROS},
  author    = {Dongjiang Li and Jinyu Miao and Xuesong Shi and Yuxin Tian and Qiwei Long and Tianyu Cai and Ping Guo and Hongfei Yu and Wei Yang and Haosong Yue and Qi Wei and Fei Qiao},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636248},
  pages     = {1331-1338},
  title     = {RaP-net: A region-wise and point-wise weighting network to extract robust features for indoor localization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). INeRF: Inverting neural radiance fields for pose estimation.
<em>IROS</em>, 1323–1330. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present iNeRF, a framework that performs mesh-free pose estimation by &quot;inverting&quot; a Neural Radiance Field (NeRF). NeRFs have been shown to be remarkably effective for the task of view synthesis — synthesizing photorealistic novel views of real-world scenes or objects. In this work, we investigate whether we can apply analysis-by-synthesis via NeRF for mesh-free, RGB-only 6DoF pose estimation – given an image, find the translation and rotation of a camera relative to a 3D object or scene. Our method assumes that no object mesh models are available during either training or test time. Starting from an initial pose estimate, we use gradient descent to minimize the residual between pixels rendered from a NeRF and pixels in an observed image. In our experiments, we first study 1) how to sample rays during pose refinement for iNeRF to collect informative gradients and 2) how different batch sizes of rays affect iNeRF on a synthetic dataset. We then show that for complex real-world scenes from the LLFF dataset [21], iNeRF can improve NeRF by estimating the camera poses of novel images and using these images as additional training data for NeRF. Finally, we show iNeRF can perform categorylevel object pose estimation, including object instances not seen during training, with RGB images by inverting a NeRF model inferred from a single view.},
  archive   = {C_IROS},
  author    = {Lin Yen-Chen and Pete Florence and Jonathan T. Barron and Alberto Rodriguez and Phillip Isola and Tsung-Yi Lin},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636708},
  pages     = {1323-1330},
  title     = {INeRF: Inverting neural radiance fields for pose estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A registration-aided domain adaptation network for 3D point
cloud based place recognition. <em>IROS</em>, 1317–1322. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the field of large-scale SLAM for autonomous driving and mobile robotics, 3D point cloud based place recognition has aroused significant research interest due to its robustness to changing environments with drastic daytime and weather variance. However, it is time-consuming and effort-costly to obtain high-quality point cloud data for place recognition model training and ground truth for registration in the real world. To this end, a novel registration-aided 3D domain adaptation network for point cloud based place recognition is proposed. A structure-aware registration network is introduced to help to learn features with geometric information and a 6-DoFs pose between two point clouds with partial overlap can be estimated. The model is trained through a synthetic virtual LiDAR dataset through GTA-V with diverse weather and daytime conditions and domain adaptation is implemented to the real-world domain by aligning the global features. Our results outperform state-of-the-art 3D place recognition baselines or achieve comparable on the real-world Oxford RobotCar dataset with the visualization of registration on the virtual dataset.},
  archive   = {C_IROS},
  author    = {Zhijian Qiao and Hanjiang Hu and Weiang Shi and Siyuan Chen and Zhe Liu and Hesheng Wang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635878},
  pages     = {1317-1322},
  title     = {A registration-aided domain adaptation network for 3D point cloud based place recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PTT: Point-track-transformer module for 3D single object
tracking in point clouds. <em>IROS</em>, 1310–1316. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D single object tracking is a key issue for robotics. In this paper, we propose a transformer module called Point-Track-Transformer (PTT) for point cloud-based 3D single object tracking. PTT module contains three blocks for feature embedding, position encoding, and self-attention feature computation. Feature embedding aims to place features closer in the embedding space if they have similar semantic information. Position encoding is used to encode coordinates of point clouds into high dimension distinguishable features. Self-attention generates refined attention features by computing attention weights. Besides, we embed the PTT module into the open-source state-of-the-art method P2B to construct PTT-Net. Experiments on the KITTI dataset reveal that our PTT-Net surpasses the state-of-the-art by a noticeable margin $\left( {\sim 10\% } \right)$. Additionally, PTT-Net could achieve real-time performance (~40FPS) on NVIDIA 1080Ti GPU. Our code is open-sourced for the robotics community at https://github.com/shanjiayao/PTT.},
  archive   = {C_IROS},
  author    = {Jiayao Shan and Sifan Zhou and Zheng Fang and Yubo Cui},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636821},
  pages     = {1310-1316},
  title     = {PTT: Point-track-transformer module for 3D single object tracking in point clouds},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EVReflex: Dense time-to-impact prediction for event-based
obstacle avoidance. <em>IROS</em>, 1304–1309. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The broad scope of obstacle avoidance has led to many kinds of computer vision-based approaches. Despite its popularity, it is not a solved problem. Traditional computer vision techniques using cameras and depth sensors often focus on static scenes, or rely on priors about the obstacles. Recent developments in bio-inspired sensors present event cameras as a compelling choice for dynamic scenes. Although these sensors have many advantages over their frame-based counterparts, such as high dynamic range and temporal resolution, event-based perception has largely remained in 2D. This often leads to solutions reliant on heuristics and specific to a particular task.We show that the fusion of events and depth overcomes the failure cases of each individual modality when performing obstacle avoidance. Our proposed approach unifies event camera and lidar streams to estimate metric Time-To-Impact (TTI) without prior knowledge of the scene geometry or obstacles. In addition, we release an extensive event-based dataset with six visual streams spanning over 700 scanned scenes.},
  archive   = {C_IROS},
  author    = {Celyn Walters and Simon Hadfield},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636327},
  pages     = {1304-1309},
  title     = {EVReflex: Dense time-to-impact prediction for event-based obstacle avoidance},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On explainability and sensor-adaptability of a robot tactile
texture representation using a two-stage recurrent networks.
<em>IROS</em>, 1296–1303. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to simultaneously distinguish objects, materials, and their associated physical properties is one fundamental function of the sense of touch. Recent advances in the development of tactile sensors and machine learning techniques allow more accurate and complex modelling of robotic tactile sensations. However, many state-of-the-art (SotA) approaches focus solely on constructing black-box models to achieve ever higher classification accuracy and fail to adapt across sensors with unique spatial-temporal data formats. In this work, we propose an Explainable and Sensor-Adaptable Recurrent Networks (ExSARN) model for tactile texture representation. The ExSARN model consists of a two-stage recurrent networks fed by a sensor-specific header network. The first stage recurrent network emulates our human touch receptors and decouples sensor-specific tactile sensations into different frequency response bands, while the second stage codes the overall temporal signature as a variational recurrent autoen-coder. We infuse the latent representation with ternary labels to qualitatively represent texture properties (e.g. roughness and stiffness), which facilitates representation learning and provide explainability to the latent space. The ExSARN model is tested on texture datasets collected with two different tactile sensors. Our results show that the proposed model not only achieves higher accuracy, but also provides adaptability across sensors with different sampling frequencies and data formats. The addition of the crudely obtained qualitative property labels offers a practical approach to enhance the interpretability of the latent space, facilitate property inference on unseen materials, and improve the overall performance of the model.},
  archive   = {C_IROS},
  author    = {Ruihan Gao and Tian Tian and Zhiping Lin and Yan Wu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636380},
  pages     = {1296-1303},
  title     = {On explainability and sensor-adaptability of a robot tactile texture representation using a two-stage recurrent networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A conformal mapping-based framework for robot-to-robot and
sim-to-real transfer learning. <em>IROS</em>, 1289–1295. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel method for transferring motion planning and control policies between a teacher and a learner robot. With this work, we propose to reduce the sim-to-real gap, transfer knowledge designed for a specific system into a different robot, and compensate for system aging and failures. To solve this problem we introduce a Schwarz–Christoffel mapping-based method to geometrically stretch and fit the control inputs from the teacher into the learner command space. We also propose a method based on primitive motion generation to create motion plans and control inputs compatible with the learner’s capabilities. Our approach is validated with simulations and experiments with different robotic systems navigating occluding environments.},
  archive   = {C_IROS},
  author    = {Shijie Gao and Nicola Bezzo},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636682},
  pages     = {1289-1295},
  title     = {A conformal mapping-based framework for robot-to-robot and sim-to-real transfer learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Shaping progressive net of reinforcement learning for policy
transfer with human evaluative feedback. <em>IROS</em>, 1281–1288. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep reinforcement learning has achieved significant success in many fields, but will confront sampling efficiency and safety problems when applying to robot control in the real world. Sim-to-real transfer learning was proposed to make use of samples in the simulation and overcome the gap between simulation and real world. In this paper, we focus on improving Progressive Neural Network — an effective sim-to-real learning method, by proposing Interactive Progressive Network Learning (IPNL). IPNL integrates progressive network and interactive reinforcement learning (interactive RL) which learns from evaluative feedback provided by an observing human trainer. We test our method using five RL tasks with discrete or continuous actions in OpenAI Gym and a sinusoids curve following task with AUV simulator on the Gazebo platform. Our results suggest that while Progressive Network has good performance when transferring from tasks with low-dimensional state space to those with high-dimensional one but has little effect for transferring from high-dimensional tasks to low-dimensional ones, IPNL allows an agent to learn a more stable policy with better performance faster for both cases. More importantly, our further analysis indicate that there is a synergy between Progressive Network and interactive RL for improving the agent’s learning. Our results in the path following of AUV shed light on the potential of applying our method in the real world tasks.},
  archive   = {C_IROS},
  author    = {Rongshun Juan and Jie Huang and Randy Gomez and Keisuke Nakamura and Qixin Sha and Bo He and Guangliang Li},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636061},
  pages     = {1281-1288},
  title     = {Shaping progressive net of reinforcement learning for policy transfer with human evaluative feedback},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian meta-learning for few-shot policy adaptation across
robotic platforms. <em>IROS</em>, 1274–1280. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning methods can achieve significant performance but require a large amount of training data collected on the same robotic platform. A policy trained with expensive data is rendered useless after making even a minor change to the robot hardware. In this paper, we address the challenging problem of adapting a policy, trained to perform a task, to a novel robotic hardware platform given only few demonstrations of robot motion trajectories on the target robot. We formulate it as a few-shot meta-learning problem where the goal is to find a meta-model that captures the common structure shared across different robotic platforms such that data-efficient adaptation can be performed. We achieve such adaptation by introducing a learning framework consisting of a probabilistic gradient-based meta-learning algorithm that models the uncertainty arising from the few-shot setting with a low-dimensional latent variable. We experimentally evaluate our framework on a simulated reaching and a real-robot picking task using 400 simulated robots generated by varying the physical parameters of an existing set of robotic platforms. Our results show that the proposed method can successfully adapt a trained policy to different robotic platforms with novel physical parameters and the superiority of our meta-learning algorithm compared to state-of-the-art methods for the introduced few-shot policy adaptation problem.},
  archive   = {C_IROS},
  author    = {Ali Ghadirzadeh and Xi Chen and Petra Poklukar and Chelsea Finn and Mårten Björkman and Danica Kragic},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636628},
  pages     = {1274-1280},
  title     = {Bayesian meta-learning for few-shot policy adaptation across robotic platforms},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Knowledge transfer across imaging modalities via
simultaneous learning of adaptive autoencoders for high-fidelity mobile
robot vision. <em>IROS</em>, 1267–1273. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Enabling mobile robots for solving challenging and diverse shape, texture, and motion related tasks with high fidelity vision requires the integration of novel multimodal imaging sensors and advanced fusion techniques. However, it is associated with high cost, power, hardware modification, and computing requirements which limit its scalability. In this paper, we propose a novel Simultaneously Learned Auto Encoder Domain Adaptation (SAEDA)-based transfer learning technique to empower noisy sensing with advanced sensor suite capabilities. In this regard, SAEDA trains both source and target auto-encoders together on a single graph to obtain the domain invariant feature space between the source and target domains on simultaneously collected data. Then, it uses the domain invariant feature space to transfer knowledge between different signal modalities. The evaluation has been done on two collected datasets (LiDAR and Radar) and one existing dataset (LiDAR, Radar and Video) which provides a significant improvement in quadruped robot-based classification (home floor and human activity recognition) and regression (surface roughness estimation) problems. We also integrate our sensor suite and SAEDA framework on two real-time systems (vacuum cleaning and Mini-Cheetah quadruped robots) for studying the feasibility and usability.},
  archive   = {C_IROS},
  author    = {Md Mahmudur Rahman and Tauhidur Rahman and Donghyun Kim and Mohammad Arif Ul Alam},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636360},
  pages     = {1267-1273},
  title     = {Knowledge transfer across imaging modalities via simultaneous learning of adaptive autoencoders for high-fidelity mobile robot vision},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Domain curiosity: Learning efficient data collection
strategies for domain adaptation. <em>IROS</em>, 1259–1266. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Domain adaptation is a common problem in robotics, with applications such as transferring policies from simulation to real world and lifelong learning. Performing such adaptation, however, requires informative data about the environment to be available during the adaptation. In this paper, we present domain curiosity—a method of training exploratory policies that are explicitly optimized to provide data that allows a model to learn about the unknown aspects of the environment. In contrast to most curiosity methods, our approach explicitly rewards learning, which makes it robust to environment noise without sacrificing its ability to learn. We evaluate the proposed method by comparing how much a model can learn about environment dynamics given data collected by the proposed approach, compared to standard curious and random policies. The evaluation is performed using a toy environment, two simulated robot setups, and on a real-world haptic exploration task. The results show that the proposed method allows data-efficient and accurate estimation of dynamics.},
  archive   = {C_IROS},
  author    = {Karol Arndt and Oliver Struckmeier and Ville Kyrki},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635864},
  pages     = {1259-1266},
  title     = {Domain curiosity: Learning efficient data collection strategies for domain adaptation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Delay aware universal notice network: Real world multi-robot
transfer learning. <em>IROS</em>, 1251–1258. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {General purpose simulators provide cheap training data to learn complex robotic skills. However, the transition from simulation to reality is often very challenging for the agent. One major issue is the delay on the physical robot that may deteriorate the performance of the deployed agent. Furthermore, once a successfully trained learning-based control policy is available, re-purposing the knowledge acquired by the agent to enable a structurally distinct agent to perform the same task is hazardous if done naively. In this work, we address the above issues with a single method, the DA-UNN (Delay Aware Universal Notice Network), which decomposes the knowledge into robot-specific and task-specific modules for fast transfer. Our framework deals with delays immanent to physical systems in order to improve sim2real transfer. We evaluate the efficiency of our approach using simulated and actual robots on a dynamic manipulation task where delay management is crucial.},
  archive   = {C_IROS},
  author    = {Samuel Beaussant and Sébastien Lengagne and Benoit Thuilot and Olivier Stasse},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635917},
  pages     = {1251-1258},
  title     = {Delay aware universal notice network: Real world multi-robot transfer learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Autonomous vehicle navigation in semi-structured
environments based on sparse waypoints and LiDAR road-tracking.
<em>IROS</em>, 1244–1250. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {During the last decades, the research endeavours on autonomous driving found great resonance in Advanced Driver-Assistance Solutions that equipped the contemporary civilian vehicles and significantly boosted their driver-less mobility. The existing applications are mostly focused on urban scenarios where signs, road lanes and markers are well defined and ordered favouring the motion of the vehicles whilst, less attention has been paid to the semi-structured and rural environments where traffic infrastructure is scarce. The paper at hand introduces a holistic framework for autonomous vehicles navigation in semi-structured environments. Semantic cues fused with geometrical information of LiDAR data are used for road detection and tracking. OpenStreetMaps are employed as a rough route planner, the waypoints of which are rectified via a probability distribution function over the visible area of vehicle’s vicinity. Thus, vehicle’s localization is obtained by Normal Distribution Transform (NDT) SLAM, where the covariance of egomotion estimation is obtained by processing short-term 3D maps, fused with GPS measurements by means of an extended Kalman filter. Local planning and execution of vehicle’s motion is applied on local cost maps formulated by the union of 2D laser readings and the detected road boundaries fitted through Bézier curves. The complete framework has been evaluated with the aid of a real Autonomous Guided Vehicle in a constrained semi-structured urban area, exhibiting robust navigation performance.},
  archive   = {C_IROS},
  author    = {Kosmas Tsiakas and Ioannis Kostavelis and Antonios Gasteratos and Dimitrios Tzovaras},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636082},
  pages     = {1244-1250},
  title     = {Autonomous vehicle navigation in semi-structured environments based on sparse waypoints and LiDAR road-tracking},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Minimizing safety interference for safe and comfortable
automated driving with distributional reinforcement learning.
<em>IROS</em>, 1236–1243. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite recent advances in reinforcement learning (RL), its application in safety critical domains like autonomous vehicles is still challenging. Although penalizing RL agents for risky situations can help to learn safe policies, it may also lead to highly conservative behavior. In this paper, we propose a distributional RL framework in order to learn adaptive policies which allow to tune their level of conservativity at run-time based on the desired comfort and utility. Using a proactive safety verification approach, the proposed framework can guarantee that actions generated from RL are failsafe according to the worst-case assumptions. Concurrently, the policy is encouraged to minimize safety interference and generate more comfortable behavior. We trained and evaluated the proposed approach and baseline policies using a high level simulator with a variety of randomized scenarios including several corner cases which rarely happen in reality but are very crucial. In light of our experiments, the behavior of policies learned using distributional RL is adaptive at run-time and robust to the environment uncertainty. Quantitatively, the learned distributional RL agent reduces the average driving time more than 50\% compared to the normal DQN policy. It also requires 83\% less safety interference compared to the rule-based policy while only slightly increasing the average driving time. We also study sensitivity of the learned policy in environments with higher perception noise and show that our algorithm learns policies that can still drive reliable when the perception noise is two times higher than in the training configuration in automated merging and crossing at occluded intersections.},
  archive   = {C_IROS},
  author    = {Danial Kamran and Tizian Engelgeh and Marvin Busch and Johannes Fischer and Christoph Stiller},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636847},
  pages     = {1236-1243},
  title     = {Minimizing safety interference for safe and comfortable automated driving with distributional reinforcement learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised traffic scene generation with synthetic 3D
scene graphs. <em>IROS</em>, 1229–1235. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Image synthesis driven by computer graphics achieved recently a remarkable realism, yet synthetic image data generated this way reveals a significant domain gap with respect to real-world data. This is especially true in autonomous driving scenarios, which represent a critical aspect for over-coming utilizing synthetic data for training neural networks. We propose a method based on domain-invariant scene representation to directly synthesize traffic scene imagery without rendering. Specifically, we rely on synthetic scene graphs as our internal representation and introduce an unsupervised neural network architecture for realistic traffic scene synthesis. We enhance synthetic scene graphs with spatial information about the scene and demonstrate the effectiveness of our approach through scene manipulation.},
  archive   = {C_IROS},
  author    = {Artem Savkin and Rachid Ellouze and Nassir Navab and Federico Tombari},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636318},
  pages     = {1229-1235},
  title     = {Unsupervised traffic scene generation with synthetic 3D scene graphs},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Connecting deep-reinforcement-learning-based obstacle
avoidance with conventional global planners using waypoint generators.
<em>IROS</em>, 1213–1220. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep Reinforcement Learning has emerged as an efficient dynamic obstacle avoidance method in highly dynamic environments. It has the potential to replace overly conservative or inefficient navigation approaches. However, integrating Deep Reinforcement Learning into existing navigation systems is still an open frontier due to the myopic nature of Deep-Reinforcement-Learning-based navigation, which hinders its widespread integration into current navigation systems. In this paper, we propose the concept of an intermediate planner to interconnect novel Deep-Reinforcement-Learning-based obstacle avoidance with conventional global planning methods using waypoint generation. Therefore, we integrate different waypoint generators into existing navigation systems and compare the joint system against traditional ones. We found an increased performance in terms of safety, efficiency and path smoothness, especially in highly dynamic environments.},
  archive   = {C_IROS},
  author    = {Linh Kästner and Xinlin Zhao and Teham Buiyan and Junhui Li and Zhengcheng Shen and Jens Lambrecht and Cornelius Marx},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636039},
  pages     = {1213-1220},
  title     = {Connecting deep-reinforcement-learning-based obstacle avoidance with conventional global planners using waypoint generators},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Autonomous drone racing with deep reinforcement learning.
<em>IROS</em>, 1205–1212. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many robotic tasks, such as autonomous drone racing, the goal is to travel through a set of waypoints as fast as possible. A key challenge for this task is planning the timeoptimal trajectory, which is typically solved by assuming perfect knowledge of the waypoints to pass in advance. The resulting solution is either highly specialized for a single-track layout, or suboptimal due to simplifying assumptions about the platform dynamics. In this work, a new approach to near-time-optimal trajectory generation for quadrotors is presented. Leveraging deep reinforcement learning and relative gate observations, our approach can compute near-time-optimal trajectories and adapt the trajectory to environment changes. Our method exhibits computational advantages over approaches based on trajectory optimization for non-trivial track configurations. The proposed approach is evaluated on a set of race tracks in simulation and the real world, achieving speeds of up to 60kmh −1 with a physical quadrotor.},
  archive   = {C_IROS},
  author    = {Yunlong Song and Mats Steinweg and Elia Kaufmann and Davide Scaramuzza},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636053},
  pages     = {1205-1212},
  title     = {Autonomous drone racing with deep reinforcement learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On fault classification in connected autonomous vehicles
using supervised machine learning. <em>IROS</em>, 1198–1204. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Different health-monitoring techniques were considered in the literature to enhance the safety and stability of Connected Autonomous Vehicle (CAV) platoons. The health-monitoring processes include fault detection, localization, and mitigation. It is evident that mitigating these faults is faster and more reliable if the fault structure is known. To this end, we consider classifying the fault class using supervised machine learning. We first model a heterogeneous CAV platoon with three different common faults separately. These faults are bounded actuator disturbances (namely, engine bearing knock), False Data Injection (FDI) attack, and communication time delay. We consider two supervised machine learning classifiers, the first classifier determines whether the fault is bounded disturbances or communication delay, and the second classifier determines whether the disturbances are in the physical or cyber layer. We have compared four machine learning techniques for each classifier, Support Vector Machine (SVM), Naive Bayes (NB), Quadratic Discriminant (QD), and K-Nearest Neighbors (KNN). The classifiers are trained firstly on the simulation model, then are tested on a different set of observations and tested experimentally on a platoon of three autonomous robots. The highest accuracy was achieved by considering SVM for the first classifier and QD for the second classifier. The overall classification accuracy achieved is 96.8\% for the simulation test and 92.1\% for the experiment.},
  archive   = {C_IROS},
  author    = {Abdelrahman Khalil and Mohammad Al Janaideh},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636741},
  pages     = {1198-1204},
  title     = {On fault classification in connected autonomous vehicles using supervised machine learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recalling direct 2D-3D matches for large-scale visual
localization. <em>IROS</em>, 1191–1197. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Estimating the 6-DoF camera pose of an image with respect to a 3D scene model, known as visual localization, is a fundamental problem in many computer vision and robotics tasks. Among various visual localization methods, the direct 2D-3D matching method has become the preferred method for many practical applications due to its computational efficiency. When using direct 2D-3D matching methods in large-scale scenes, a vocabulary tree can be used to accelerate the matching process, which will also induce the quantization artifacts leading to reduce the inlier ratio and decrease the localization accuracy. To this end, in this paper two simple and effective mechanisms, called visibility-based recalling and space-based recalling, are proposed to recover lost matches caused by the quantization artifacts, thus can largely improve the localization accuracy and success rate without increasing too much computational time. Experimental results on long-term visual localization benchmarks demonstrate the effectiveness of our method compared with state-of-the-arts.},
  archive   = {C_IROS},
  author    = {Zhuo Song and Chuting Wang and Yuqian Liu and Shuhan Shen},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635897},
  pages     = {1191-1197},
  title     = {Recalling direct 2D-3D matches for large-scale visual localization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SemSegMap – 3D segment-based semantic localization.
<em>IROS</em>, 1183–1190. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Localization is an essential task for mobile autonomous robotic systems that want to use pre-existing maps or create new ones in the context of SLAM. Today, many robotic platforms are equipped with high-accuracy 3D LiDAR sensors, which allow a geometric mapping, and cameras able to provide semantic cues of the environment. Segment-based mapping and localization have been applied with great success to 3D point-cloud data, while semantic understanding has been shown to improve localization performance in vision based systems. In this paper we combine both modalities in SemSegMap, extending SegMap into a segment based mapping framework able to also leverage color and semantic data from the environment to improve localization accuracy and robustness. In particular, we present new segmentation and descriptor extraction processes. The segmentation process benefits from additional distance information from color and semantic class consistency resulting in more repeatable segments and more overlap after re-visiting a place. For the descriptor, a tight fusion approach in a deep-learned descriptor extraction network is performed leading to a higher descriptiveness for landmark matching. We demonstrate the advantages of this fusion on multiple simulated and real-world datasets and compare its performance to various baselines. We show that we are able to find 50.9\% more high-accuracy prior-less global localizations compared to SegMap on challenging datasets using very compact maps while also providing accurate full 6 DoF pose estimates in real-time.},
  archive   = {C_IROS},
  author    = {Andrei Cramariuc and Florian Tschopp and Nikhilesh Alatur and Stefan Benz and Tillmann Falck and Marius Brühlmeier and Benjamin Hahn and Juan Nieto and Roland Siegwart},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636156},
  pages     = {1183-1190},
  title     = {SemSegMap – 3D segment-based semantic localization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online spatio-temporal calibration of tightly-coupled
ultrawideband-aided inertial localization. <em>IROS</em>, 1161–1168. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The combination of ultrawideband (UWB) radios and inertial measurement units (IMU) can provide accurate positioning in environments where the Global Positioning System (GPS) service is either unavailable or has unsatisfactory performance. The two sensors, IMU and UWB radio, are often not co-located on a moving system. The UWB radio is typically located at the extremities of the system to ensure reliable communication, whereas the IMUs are located closer to its center of gravity. Furthermore, without hardware or software synchronization, data from heterogeneous sensors can arrive at different time instants resulting in temporal offsets. If uncalibrated, these spatial and temporal offsets can degrade the positioning performance. In this paper, using observability and identifiability criteria, we derive the conditions required for successfully calibrating the spatial and the temporal offset parameters of a tightly-coupled UWB-IMU system. We also present an online method for jointly calibrating these offsets. The results show that our calibration approach results in improved positioning accuracy while simultaneously estimating (i) the spatial offset parameters to millimeter precision and (ii) the temporal offset parameter to millisecond precision.},
  archive   = {C_IROS},
  author    = {Abhishek Goudar and Angela P. Schoellig},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636625},
  pages     = {1161-1168},
  title     = {Online spatio-temporal calibration of tightly-coupled ultrawideband-aided inertial localization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High accuracy three-dimensional self-localization using
visual markers and inertia measurement unit. <em>IROS</em>, 1154–1160.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Technologies for estimating self-position and orientation are important for both humans and robots. These technologies allow robots to perform tasks such as carrying objects and allow people to reach their destinations. Although self-position estimation technologies using GPS and laser rangefinders have been developed, few methods can be used by both humans and robots. Therefore, we developed a method that can estimate three-dimensional position and orientation using visual markers and an inertia measurement unit (IMU). Self-position can be measured with high accuracy by using a visual marker and monocular camera, but such measurement data is discrete and sparse. In contrast, an IMU can continuously measure acceleration data, but data obtained from an acceleration sensor are double-integrated, which increases position error. By combining visual marker and IMU information, position error calculations based on the acceleration sensor can be corrected, and the movement path of the object can be estimated. In demonstration experiments, the proposed method accurately estimates the three-dimensional movement distance when a person walks about 13 m, with an average error of about 40.3mm.},
  archive   = {C_IROS},
  author    = {Kunihiro Ogata and Hideyuki Tanaka and Yoshio Matsumoto},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636749},
  pages     = {1154-1160},
  title     = {High accuracy three-dimensional self-localization using visual markers and inertia measurement unit},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Coarse-to-fine semantic localization with HD map for
autonomous driving in structural scenes. <em>IROS</em>, 1146–1153. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robust and accurate localization is an essential component for robotic navigation and autonomous driving. The use of cameras for localization with high definition map (HD Map) provides an affordable localization sensor set. Existing methods suffer from pose estimation failure due to error prone data association or initialization with accurate initial pose requirement. In this paper, we propose a cost-effective vehicle localization system with HD map for autonomous driving that uses cameras as primary sensors. To this end, we formulate vision-based localization as a data association problem that maps visual semantics to landmarks in HD map. Specifically, system initialization is finished in a coarse to fine manner by combining coarse GPS (Global Positioning System) measurement and fine pose searching. In tracking stage, vehicle pose is refined by implicitly aligning the semantic segmentation result between image and landmarks in HD maps with photometric consistency. Finally, vehicle pose is computed by pose graph optimization in a sliding window fashion. We evaluate our method on two datasets and demonstrate that the proposed approach yields promising localization results in different driving scenarios. Additionally, our approach is suitable for both monocular camera and multi-cameras that provides flexibility and improves robustness for the localization system.},
  archive   = {C_IROS},
  author    = {Chengcheng Guo and Minjie Lin and Heyang Guo and Pengpeng Liang and Erkang Cheng},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635923},
  pages     = {1146-1153},
  title     = {Coarse-to-fine semantic localization with HD map for autonomous driving in structural scenes},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SNE-RoadSeg+: Rethinking depth-normal translation and deep
supervision for freespace detection. <em>IROS</em>, 1140–1145. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Freespace detection is a fundamental component of autonomous driving perception. Recently, deep convolutional neural networks (DCNNs) have achieved impressive performance for this task. In particular, SNE-RoadSeg, our previously proposed method based on a surface normal estimator (SNE) and a data-fusion DCNN (RoadSeg), has achieved impressive performance in freespace detection. However, SNE-RoadSeg is computationally intensive, and it is difficult to execute in real time. To address this problem, we introduce SNE-RoadSeg+, an upgraded version of SNE-RoadSeg. SNE-RoadSeg+ consists of 1) SNE+, a module for more accurate surface normal estimation, and 2) RoadSeg+, a data-fusion DCNN that can greatly minimize the trade-off between accuracy and efficiency with the use of deep supervision. Extensive experimental results have demonstrated the effectiveness of our SNE+ for surface normal estimation and the superior performance of our SNE-RoadSeg+ over all other freespace detection approaches. Specifically, our SNE-RoadSeg+ runs in real time, and meanwhile, achieves the state-of-the-art performance on the KITTI road benchmark. Our project page is at https://www.sne-roadseg.site/sne-roadseg-plus.},
  archive   = {C_IROS},
  author    = {Hengli Wang and Rui Fan and Peide Cai and Ming Liu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636723},
  pages     = {1140-1145},
  title     = {SNE-RoadSeg+: Rethinking depth-normal translation and deep supervision for freespace detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ISSAFE: Improving semantic segmentation in accidents by
fusing event-based data. <em>IROS</em>, 1132–1139. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ensuring the safety of all traffic participants is a prerequisite for bringing intelligent vehicles closer to practical applications. The assistance system should not only achieve high accuracy under normal conditions, but obtain robust perception against extreme situations. However, traffic accidents that involve object collisions, deformations, overturns, etc., yet unseen in most training sets, will largely harm the performance of existing semantic segmentation models. To tackle this issue, we present a rarely addressed task regarding semantic segmentation in accidental scenarios, along with an accident dataset DADA-seg. It contains 313 various accident sequences with 40 frames each, of which the time windows are located before and during a traffic accident. Every 11th frame is manually annotated for benchmarking the segmentation performance. Furthermore, we propose a novel event-based multi-modal segmentation architecture ISSAFE. Our experiments indicate that event-based data can provide complementary information to stabilize semantic segmentation under adverse conditions by preserving fine-grain motion of fast-moving foreground (crash objects) in accidents. Our approach achieves +8.2\% mIoU performance gain on the proposed evaluation set, exceeding more than 10 state-of-the-art segmentation methods. The proposed ISSAFE architecture is demonstrated to be consistently effective for models learned on multiple source databases including Cityscapes, KITTI-360, BDD and ApolloScape.},
  archive   = {C_IROS},
  author    = {Jiaming Zhang and Kailun Yang and Rainer Stiefelhagen},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636109},
  pages     = {1132-1139},
  title     = {ISSAFE: Improving semantic segmentation in accidents by fusing event-based data},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semantic image alignment for vehicle localization.
<em>IROS</em>, 1124–1131. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate and reliable localization is a fundamental requirement for autonomous vehicles to use map information in higher-level tasks such as navigation or planning. In this paper, we present a novel approach to vehicle localization in dense semantic maps, including vectorized high-definition maps or 3D meshes, using semantic segmentation from a monocular camera. We formulate the localization task as a direct image alignment problem on semantic images, which allows our approach to robustly track the vehicle pose in semantically labeled maps by aligning virtual camera views rendered from the map to sequences of semantically segmented camera images. In contrast to existing visual localization approaches, the system does not require additional keypoint features, handcrafted localization landmark extractors or expensive LiDAR sensors. We demonstrate the wide applicability of our method on a diverse set of semantic mesh maps generated from stereo or LiDAR as well as manually annotated HD maps and show that it achieves reliable and accurate localization in real-time.},
  archive   = {C_IROS},
  author    = {Markus Herb and Matthias Lemberger and Marcel M. Schmitt and Alexander Kurz and Tobias Weiherer and Nassir Navab and Federico Tombari},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636517},
  pages     = {1124-1131},
  title     = {Semantic image alignment for vehicle localization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CP-loss: Connectivity-preserving loss for road curb
detection in autonomous driving with aerial images. <em>IROS</em>,
1117–1123. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Road curb detection is important for autonomous driving. It can be used to determine road boundaries to constrain vehicles on roads, so that potential accidents could be avoided. Most of the current methods detect road curbs online using vehicle-mounted sensors, such as cameras or 3-D Lidars. However, these methods usually suffer from severe occlusion issues. Especially in highly-dynamic traffic environments, most of the field of view is occupied by dynamic objects. To alleviate this issue, we detect road curbs offline using high-resolution aerial images in this paper. Moreover, the detected road curbs can be used to create high-definition (HD) maps for autonomous vehicles. Specifically, we first predict the pixel-wise segmentation map of road curbs, and then conduct a series of post-processing steps to extract the graph structure of road curbs. To tackle the disconnectivity issue in the segmentation maps, we propose an innovative connectivity-preserving loss (CP-loss) to improve the segmentation performance. The experimental results on a public dataset demonstrate the effectiveness of our proposed loss function. This paper is accompanied with a demonstration video and a supplementary document, which are available at https://sites.google.com/view/cp-loss.},
  archive   = {C_IROS},
  author    = {Zhenhua Xu and Yuxiang Sun and Lujia Wang and Ming Liu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636060},
  pages     = {1117-1123},
  title     = {CP-loss: Connectivity-preserving loss for road curb detection in autonomous driving with aerial images},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LiDAR-based drivable region detection for autonomous
driving. <em>IROS</em>, 1110–1116. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For autonomous driving, drivable region detection is one of the most basic and essential tasks. In this paper, a novel LiDAR-based drivable region detection algorithm which could output a complete, accurate and stable result is proposed. To promote the completeness of the detection result, the Bayesian generalized kernel inference and bilateral filtering are utilized to estimate the attribute of those unobserved cells. To ensure the traversability, a region growing operator is performed on the normal vector map which reflects the slope of the terrain, thus closely related to the traversability of the vehicle. To improve the result’s stability, information from multiple frames are fused together in the Kalman Filter framework. Experiments are performed both on public dataset and our own dataset. Experimental results show that the proposed algorithm could run in real-time and outperforms state-of-the-art approaches.},
  archive   = {C_IROS},
  author    = {Hanzhang Xue and Hao Fu and Ruike Ren and Jintao Zhang and Bokai Liu and Yiming Fan and Bin Dai},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636289},
  pages     = {1110-1116},
  title     = {LiDAR-based drivable region detection for autonomous driving},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Local memory attention for fast video semantic segmentation.
<em>IROS</em>, 1102–1109. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel neural network module that transforms an existing single-frame semantic segmentation model into a video semantic segmentation pipeline. In contrast to prior works, we strive towards a simple, fast, and general module that can be integrated into virtually any single-frame architecture. Our approach aggregates a rich representation of the semantic information in past frames into a memory module. Information stored in the memory is then accessed through an attention mechanism. In contrast to previous memory-based approaches, we propose a fast local attention layer, providing temporal appearance cues in the local region of prior frames. We further fuse these cues with an encoding of the current frame through a second attention-based module. The segmentation decoder processes the fused representation to predict the final semantic segmentation. We integrate our approach into two popular semantic segmentation networks: ERFNet and PSPNet. We observe an improvement in segmentation performance on Cityscapes by 1.7\% and 2.1\% in mIoU respectively, while increasing inference time of ERFNet by only 1.5ms. Source code is available at https://github.com/mattpfr/lmanet.},
  archive   = {C_IROS},
  author    = {Matthieu Paul and Martin Danelljan and Luc Van Gool and Radu Timofte},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636192},
  pages     = {1102-1109},
  title     = {Local memory attention for fast video semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TUPPer-map: Temporal and unified panoptic perception for 3D
metric-semantic mapping. <em>IROS</em>, 1094–1101. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose TUPPer-Map, a metric-semantic mapping framework based on the unified panoptic segmentation and temporal data association. In contrast to the previous mapping method, our framework integrates the data association stage into the holistic pixel-level segmentation stage in an end-to-end fashion, taking advantage of both intra-frame and inter-frame spatial and temporal knowledge. Firstly, we unify two-branch instance segmentation network and semantic segmentation network into a single network by sharing the backbone net, maximizing the 2D panoptic segmentation performance. Next, we leverage geometric segmentation to refine the segments predicted via deep learning. Then, we design a novel deep learning based data association module to track the object instances across different frames. Optical flow of consecutive frames and alignment of ROI (Region of Interest) candidates are learned to predict the frame-consistent instance label. At last, 2D semantics are integrated into 3D volume by TSDF raycasting to build the final map. We evaluated the performance of our framework extensively over the SceneNN, ScanNet v2 and Cityscapes-VPS datasets. Our experimental results demonstrate the superiority of TUPPer-Map over existing semantic mapping methods. Overall, our work illustrates that using learning based data association strategy can enable a more unified perception network for 3D mapping.},
  archive   = {C_IROS},
  author    = {Zhiliu Yang and Chen Liu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636599},
  pages     = {1094-1101},
  title     = {TUPPer-map: Temporal and unified panoptic perception for 3D metric-semantic mapping},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learn to differ: Sim2Real small defection segmentation
network. <em>IROS</em>, 1070–1077. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent studies on deep-learning-based small defection segmentation approaches are trained in specific settings and tend to be limited by fixed context. Throughout the training, the network inevitably learns the representation of the background of the training data before figuring out the defection. They underperform in the inference stage once the context changed and can only be solved by training in every new settings. This eventually leads to the limitation in practical robotic applications where contexts keep varying. To cope with this, instead of training a network context by context and hoping it to generalize, why not stop misleading it with any limited context and start training it with pure simulation? In this paper, we propose the network SSDS that learns a way of distinguishing small defections between two images regardless of the context, so that the network can be trained once for all. A small defection detection layer utilizing the pose sensitivity of phase correlation between images is introduced and is followed by an outlier masking layer. The network is trained on randomly generated simulated data with simple shapes and is generalized across the real world. Finally, SSDS is validated on real-world collected data and demonstrates the ability that even when trained in cheap simulation, SSDS can still find small defections in the real world showing the effectiveness and its potential for practical applications. Code is available here},
  archive   = {C_IROS},
  author    = {Zexi Chen and Zheyuan Huang and Hongxiang Yu and Zhongxiang Zhou and Yunkai Wang and Xuecheng Xu and Qimeng Tan and Yue Wang and Rong Xiong},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636491},
  pages     = {1070-1077},
  title     = {Learn to differ: Sim2Real small defection segmentation network},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Combining learning from demonstration with learning by
exploration to facilitate contact-rich tasks. <em>IROS</em>, 1062–1069.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collaborative robots are expected to work alongside humans and directly replace human workers in some cases, thus effectively responding to rapid changes in assembly lines. Current methods for programming contact-rich tasks, particularly in heavily constrained spaces, tend to be fairly inefficient. Therefore, faster and more intuitive approaches are urgently required for robot teaching. This study focuses on combining visual servoing-based learning from demonstration (LfD) and force-based learning by exploration (LbE) to enable the fast and intuitive programming of contact-rich tasks with minimal user efforts. Two learning approaches were developed and integrated into a framework, one relying on human-to-robot motion mapping (visual servoing approach) and the other relying on force-based reinforcement learning. The developed framework implements the noncontact demonstration teaching method based on the visual servoing approach and optimizes the demonstrated robot target positions according to the detected contact state. The developed framework is compared with two most commonly used baseline techniques, i.e., teach pendant-based teaching and hand-guiding teaching. Furthermore, the efficiency and reliability of the framework are validated via comparison experiments involving the teaching and execution of contact-rich tasks. The proposed framework shows the best performance in terms of the teaching time, execution success rate, risk of damage, and ease of use.},
  archive   = {C_IROS},
  author    = {Yunlei Shi and Zhaopeng Chen and Yansong Wu and Dimitri Henkel and Sebastian Riedel and Hongxu Liu and Qian Feng and Jianwei Zhang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636417},
  pages     = {1062-1069},
  title     = {Combining learning from demonstration with learning by exploration to facilitate contact-rich tasks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Assembly sequence generation for new objects via experience
learned from similar object. <em>IROS</em>, 1054–1061. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Assembly orders of components have direct influence on feasibility and efficiency of assembly process in manufacturing and are usually defined by experienced operators. To automate the assembly sequence generation process, we present a method using the idea of case-based reasoning, which can take advantage of experience of a reference assembly to generate the assembly sequence of a new assembly. First, a novel assembly representation method named assembly graph is present in which nodes indicating components’ 3D shape information and edges indicating the geometry constraints. Second, a similar components retrieve process is conducted based on assembly graph representation. Then, the assembly sequence is generated by applying the assembly order of retrieved components to the new ones. Next, the generated sequence is revised to satisfy the inherent constraints in the new assembly which is formulated as a contact graph. Finally, the revised sequence is stored into a case library with corresponding assembly model. We apply the proposed method to generate assembly sequences for chair assemblies and experimental results show its effectiveness and flexibility.},
  archive   = {C_IROS},
  author    = {Zhongxiang Zhou and Rong Xiong and Zexi Chen and Yue Wang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636531},
  pages     = {1054-1061},
  title     = {Assembly sequence generation for new objects via experience learned from similar object},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Benchmarking off-the-shelf solutions to robotic assembly
tasks. <em>IROS</em>, 1046–1053. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, many learning based approaches have been studied to realize robotic manipulation and assembly tasks, often including vision and force/tactile feedback. How-ever, it is unclear what the baseline state-of-the-art performance is and what the bottleneck problems are. In this work, we evaluate off-the-shelf (OTS) industrial solutions on a recently introduced benchmark, the National Institute of Standards and Technology (NIST) Assembly Task Board. A set of assembly tasks is introduced and baseline methods are provided to understand their intrinsic difficulty. Multiple sensor-based robotic solutions are then evaluated, including hybrid force/motion control and 2D/3D pattern matching. An end-to-end integrated solution that accomplishes the tasks is also provided.The results and findings throughout the study reveal a few noticeable factors that impede the adoptions of the OTS solutions: dependency on expertise, limited applicability, lack of interoperability, no scene awareness or error recovery mechanisms, and high cost. This paper also provides a first attempt of an objective benchmark performance on the NIST Assembly Task Boards as a reference comparison for future works on this problem.},
  archive   = {C_IROS},
  author    = {Wenzhao Lian and Tim Kelch and Dirk Holz and Adam Norton and Stefan Schaal},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636586},
  pages     = {1046-1053},
  title     = {Benchmarking off-the-shelf solutions to robotic assembly tasks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). State estimation and model-predictive control for
multi-robot handling and tracking of AGV motions using iGPS.
<em>IROS</em>, 1038–1045. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a solution for simultaneous handling of large components with industrial robots performing synchronized motions with an AGV in flexible flow assembly. For this purpose, we implement an Extended Kalman Filter with a global localization system to track an AGV and multiple manipulators. We propose a model-predictive controller for force compliance and trajectory tracking in multi-robot cooperative, decentralized, and fast manipulation tasks. In order to show the effectiveness of our system, we assemble a truck windshield using two industrial robots and an AGV in motion. In our experiments, we reliably achieve assembly tolerances of 1.5mm at AGV velocities up to $400\frac{{{\text{mm}}}}{{\text{s}}}$. The presented system makes flexible assembly systems with AGVs and freely reconfigurable manipulators possible. It enables the automation of high variant, low volume, large size assembly tasks such as aircraft, truck or steel beam assembly, which are mostly manual processes at present.},
  archive   = {C_IROS},
  author    = {Christoph Storm and Henrik Hose and Robert H. Schmitt},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636116},
  pages     = {1038-1045},
  title     = {State estimation and model-predictive control for multi-robot handling and tracking of AGV motions using iGPS},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CovarianceNet: Conditional generative model for correct
covariance prediction in human motion prediction. <em>IROS</em>,
1031–1037. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The correct characterization of uncertainty when predicting human motion is equally important as the accuracy of this prediction. We present a new method to correctly predict the uncertainty associated with the predicted distribution of future trajectories. Our approach, CovariaceNet, is based on a Conditional Generative Model with Gaussian latent variables in order to predict the parameters of a bi-variate Gaussian distribution. The combination of CovarianceNet with a motion prediction model results in a hybrid approach that outputs a uni-modal distribution. We will show how some state of the art methods in motion prediction become overconfident when predicting uncertainty, according to our proposed metric and validated in the ETH data-set [1]. CovarianceNet correctly predicts uncertainty, which makes our method suitable for applications that use predicted distributions, e.g., planning or decision making.},
  archive   = {C_IROS},
  author    = {Aleksey Postnikov and Aleksander Gamayunov and Gonzalo Ferrer},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635968},
  pages     = {1031-1037},
  title     = {CovarianceNet: Conditional generative model for correct covariance prediction in human motion prediction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GRIT: Fast, interpretable, and verifiable goal recognition
with learned decision trees for autonomous driving. <em>IROS</em>,
1023–1030. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It is important for autonomous vehicles to have the ability to infer the goals of other vehicles (goal recognition), in order to safely interact with other vehicles and predict their future trajectories. This is a difficult problem, especially in urban environments with interactions between many vehicles. Goal recognition methods must be fast to run in real time and make accurate inferences. As autonomous driving is safety- critical, it is important to have methods which are human interpretable and for which safety can be formally verified. Existing goal recognition methods for autonomous vehicles fail to satisfy all four objectives of being fast, accurate, interpretable and verifiable. We propose Goal Recognition with Interpre table Trees (GRIT), a goal recognition system which achieves these objectives. GRIT makes use of decision trees trained on vehicle trajectory data. We evaluate GRIT on two datasets, showing that GRIT achieved fast inference speed and comparable accuracy to two deep learning baselines, a planning-based goal recognition method, and an ablation of GRIT. We show that the learned trees are human interpretable and demonstrate how properties of GRIT can be formally verified using a satisfiability modulo theories (SMT) solver.},
  archive   = {C_IROS},
  author    = {Cillian Brewitt and Balint Gyevnar and Samuel Garcin and Stefano V. Albrecht},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636279},
  pages     = {1023-1030},
  title     = {GRIT: Fast, interpretable, and verifiable goal recognition with learned decision trees for autonomous driving},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Safety-oriented pedestrian occupancy forecasting.
<em>IROS</em>, 1015–1022. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we address an important problem in self-driving, forecasting multi-pedestrian motion and their shared scene occupancy map, which is critical for safe navigation. Our contributions are two-fold. First, we advocate for predicting both the individual motions as well as the scene occupancy map in order to effectively deal with missing detections caused by postprocessing, e.g. confidence thresholding and non-maximum suppression. Second, we propose a Scene-Actor Graph Neural Network (SA-GNN) which captures the interactions among pedestrians within the same scene, including those that have not been detected, by preserving the relative spatial information of pedestrians via 2D convolution and via message passing. We show that our scene-occupancy predictions are more accurate than those from state-of-the-art motion forecasting methods, while also matching their performance in pedestrian motion forecasting metrics on two large-scale real-world datasets, nuScenes and ATG4D.},
  archive   = {C_IROS},
  author    = {Katie Luo and Sergio Casas and Renjie Liao and Xinchen Yan and Yuwen Xiong and Wenyuan Zeng and Raquel Urtasun},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636691},
  pages     = {1015-1022},
  title     = {Safety-oriented pedestrian occupancy forecasting},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simultaneous prediction of pedestrian trajectory and actions
based on context information iterative reasoning. <em>IROS</em>,
1007–1014. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pedestrian trajectories and actions prediction in complex environment is challenging due to the complexity of human behavior and a variety of internal and external stimuli. Much works has gone towards predicting trajectories and actions separately without mining the coupling relationships between them, which is an important information for our humans to reason and predict. Inspired by this, we propose an end-to-end joint context information iterative reasoning network (CIR-Net). Specifically, a novel heterogeneous spatiotemporal graph module (HST-Graph) is proposed to encode and aggregate multiple types of context information of the motion pattern and the scene. And an action-trajectory hybrid guidance module is proposed to enhance the ability of long-time prediction by utilizing the internal coupling between actions and trajectory. Moreover, an iterative reasoning structure is designed to iteratively correcting the trajectory and actions prediction error. Experimental results on the ETH&amp;UCY and VIRAT datasets demonstrate the favorable performance of the framework.},
  archive   = {C_IROS},
  author    = {Bo Chen and Decai Li and Yuqing He},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636252},
  pages     = {1007-1014},
  title     = {Simultaneous prediction of pedestrian trajectory and actions based on context information iterative reasoning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-modal scene-compliant user intention estimation in
navigation. <em>IROS</em>, 1001–1006. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A multi-modal framework to generate user intention distributions when operating a mobile vehicle is proposed in this work. The model learns from past observed trajectories and leverages traversability information derived from the visual surroundings to produce a set of future trajectories, suitable to be directly embedded into a perception-action shared control strategy on a mobile agent, or as a safety layer to supervise the prudent operation of the vehicle. We base our solution on a conditional Generative Adversarial Network with Long-Short Term Memory cells to capture trajectory distributions conditioned on past trajectories, further fused with traversability probabilities derived from visual segmentation with a Convolutional Neural Network. The proposed data-driven framework results in a significant reduction in error of the predicted trajectories (versus the ground truth) from comparable strategies in the literature (e.g. Social-GAN) that fail to account for information other than the agent’s past history. Experiments were conducted on a dataset collected with a custom wheelchair model built onto the open-source urban driving simulator CARLA, proving also that the proposed framework can be used with a small, unannotated dataset.},
  archive   = {C_IROS},
  author    = {Kavindie Katuwandeniya and Stefan H. Kiss and Lei Shi and Jaime Valls Miro},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636142},
  pages     = {1001-1006},
  title     = {Multi-modal scene-compliant user intention estimation in navigation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multimodal and hybrid framework for human navigational
intent inference. <em>IROS</em>, 993–1000. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Understanding human navigational intent is essential for robots to be able to interact with and navigate around humans safely and naturally. Current methods typically perform inference through only one mode of perception such as human motion trajectory, and a single theoretical framework such as a learning-based or classical approach. In contrast, this paper studies prediction of human navigational intent using multimodal perception within a hybrid framework. Our framework consists of two modules: a) a learning-based prediction module to predict a human’s future goal position, and b) a classical control theory-inspired reconstruction module to reconstruct a possible future trajectory or a set of possible future positions using the predicted future goal position. For the prediction module, we propose an end-to-end LSTM-CNN hybrid neural network for predicting a human’s future position in the real world, given human motion, human body pose and head orientation. This visual information from an egocentric perspective is used to make predictions of a human’s future position in world space, essential for robotic navigation algorithms and planning. In the reconstruction module, we present two control theoretic methods to reconstruct possible future trajectories of human: trajectory generation for differentially flat system and reachability analysis. We evaluate the performance of our framework on a newly collected dataset called SFU-Store-Nav. Experimental results reveal that our method outperforms various baselines especially when a relatively small amount of data is available.},
  archive   = {C_IROS},
  author    = {Zhitian Zhang and Jimin Rhim and Angelica Lim and Mo Chen},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635900},
  pages     = {993-1000},
  title     = {A multimodal and hybrid framework for human navigational intent inference},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An efficient understandability objective for dynamic optimal
control. <em>IROS</em>, 986–992. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion optimization for legible robot intent has largely ignored the robot’s dynamics, citing burdensome complexity that prevents online deployment. Even where the original task (to be communicated) could be solved on the dynamical system, the legibility problem (to communicate that task’s intent) could not. This work simplifies the legibility objective to have equivalent computational complexity as the original objective to be communicated. This enables any optimal control algorithm that can solve the original task to also solve the legible version of that task.Along the way, we expand the definition of &quot;intent&quot; to include any parameter of the optimal control problem, thereby opening the door to extend communications beyond merely desired end-points to running preferences or even, in the future, hard capabilities or safety constraints. We demonstrate how this method can replicate the properties introduced in previous communicative motion state-of-the-art (like legibility, exaggeration, and anticipation) as well as apply to non-holonomic dynamical systems.},
  archive   = {C_IROS},
  author    = {D. Livingston McPherson and S. Shankar Sastry},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636007},
  pages     = {986-992},
  title     = {An efficient understandability objective for dynamic optimal control},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Consolidating kinematic models to promote coordinated mobile
manipulations. <em>IROS</em>, 979–985. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We construct a Virtual Kinematic Chain (VKC) that readily consolidates the kinematics of the mobile base, the arm, and the object to be manipulated in mobile manipulations. Accordingly, a mobile manipulation task is represented by altering the state of the constructed VKC, which can be converted to a motion planning problem, formulated and solved by trajectory optimization. This new VKC perspective of mobile manipulation allows a service robot to (i) produce well-coordinated motions, suitable for complex household environments, and (ii) perform intricate multi-step tasks while interacting with multiple objects without an explicit definition of intermediate goals. In simulated experiments, we validate these advantages by comparing the VKC-based approach with baselines that solely optimize individual components. The results manifest that VKC-based joint modeling and planning promote task success rates and produce more efficient trajectories.},
  archive   = {C_IROS},
  author    = {Ziyuan Jiao and Zeyu Zhang and Xin Jiang and David Han and Song-Chun Zhu and Yixin Zhu and Hangxin Liu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636351},
  pages     = {979-985},
  title     = {Consolidating kinematic models to promote coordinated mobile manipulations},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cross-modal analysis of human detection for robotics: An
industrial case study. <em>IROS</em>, 971–978. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Advances in sensing and learning algorithms have led to increasingly mature solutions for human detection by robots, particularly in selected use-cases such as pedestrian detection for self-driving cars or close-range person detection in consumer settings. Despite this progress, the simple question which sensor-algorithm combination is best suited for a person detection task at handƒ remains hard to answer. In this paper, we tackle this issue by conducting a systematic cross-modal analysis of sensor-algorithm combinations typically used in robotics. We compare the performance of state-of-the-art person detectors for 2D range data, 3D lidar, and RGB-D data as well as selected combinations thereof in a challenging industrial use-case.We further address the related problems of data scarcity in the industrial target domain, and that recent research on human detection in 3D point clouds has mostly focused on autonomous driving scenarios. To leverage these methodological advances for robotics applications, we utilize a simple, yet effective multi-sensor transfer learning strategy by extending a strong image-based RGB-D detector to provide cross-modal supervision for lidar detectors in the form of weak 3D bounding box labels.Our results show a large variance among the different approaches in terms of detection performance, generalization, frame rates and computational requirements. As our use-case contains difficulties representative for a wide range of service robot applications, we believe that these results point to relevant open challenges for further research and provide valuable support to practitioners for the design of their robot system.},
  archive   = {C_IROS},
  author    = {Timm Linder and Narunas Vaskevicius and Robert Schirmer and Kai O. Arras},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636158},
  pages     = {971-978},
  title     = {Cross-modal analysis of human detection for robotics: An industrial case study},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards a user adaptive assistive robot: Learning from
demonstration using navigation functions. <em>IROS</em>, 965–970. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Elderly and mobility impaired people need special attention during bathing activities, since these tasks are demanding in body flexibility. Our aim is to build an assistive robotic bathing system, in order to increase the independence and safety of this procedure. Towards this end, the expertise of professional carers for bathing sequences and appropriate motions have to be adopted, in order to achieve natural, physical human - robot interaction. In this paper a Navigation Function (NF) approach is proposed in order to reproduce the way an expert clinical carer executes the bathing activities by means of construction repulsive potential fields (&quot;virtual obstacles&quot;) for an assistive bath robot. The produced vector field, constructed based on the demonstration procedure, is used for real-time motion behavior planning tasks, which exploits the visual information from Depth sensors and the advantages of the NF approach, to estimate the reference pose for the end- effector of the assistive robotic system. The proposed method guarantees globally asymptotic convergence to the learned from demonstration washing motion, within the deformable and moving body-part limits, while in addition, restricted areas on the body surface are avoided. The proposed method is evaluated using real experimental data, obtained from human subjects during pouring water task demonstration.},
  archive   = {C_IROS},
  author    = {Xanthi S. Papageorgiou and Athanasios C. Dometios and Costas S. Tzafestas},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636200},
  pages     = {965-970},
  title     = {Towards a user adaptive assistive robot: Learning from demonstration using navigation functions},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning robotic contact juggling. <em>IROS</em>, 958–964.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic contact juggling is a challenging task in which robots must control the movement of a ball rapidly and indirectly without holding it while keeping the ball in and sometimes out of contact with the robot’s body. In this work, we address the problem of learning such robotic contact juggling from trial and error via model-based reinforcement learning (MBRL). The key insight is that complex robot-ball interactions of the contact juggling actually consist of a small set of simple dynamics that each corresponds to a distinct interaction &quot;primitive&quot; such as touching and releasing the ball. Accordingly, we develop a tailored MBRL method that incrementally fits a set of simple dynamics models to the movements of a robot and a ball while also learning a switching model that can select a proper dynamics model depending on the current state and action. The learned model can then be used in an MBRL framework to seek optimal juggling control. We demonstrated the effectiveness of our approach on a simulator of contact juggling performed by a robotic arm.},
  archive   = {C_IROS},
  author    = {Kazutoshi Tanaka and Masashi Hamaya and Devwrat Joshi and Felix von Drigalski and Ryo Yonetani and Takamitsu Matsubara and Yoshihisa Ijiri},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636790},
  pages     = {958-964},
  title     = {Learning robotic contact juggling},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Motion strategy using opponent player’s serial learning for
air-hockey robots. <em>IROS</em>, 952–957. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, there have been many studies on sports robots that can play against humans, including studies on the strategies that sports robots use by taking into account the physical conditions of their opponents. However, there have been few studies on strategies that take into account psychological conditions of the opponents, such as carelessness and habituation. This paper proposes a motion strategy for an air-hockey robot that intentionally coaxes the opponent to learn the robot’s attack sequence and then uses a different attack sequence to catch the opponent off guard. We explicitly model the change in reaction time during serial learning and represent the opponent’s reaction as an evaluation function. By applying this evaluation function to a game tree and selecting the optimal motion, the robot could catch the opponent by surprise. Actual experiments with several subjects confirmed the effectiveness of the proposed method.},
  archive   = {C_IROS},
  author    = {Shotaro Fukuda and Koichiro Tadokoro and Akio Namiki},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635854},
  pages     = {952-957},
  title     = {Motion strategy using opponent player’s serial learning for air-hockey robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AutoPhoto: Aesthetic photo capture using reinforcement
learning. <em>IROS</em>, 944–951. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The process of capturing a well-composed photo is difficult and it takes years of experience to master. We propose a novel pipeline for an autonomous agent to automatically capture an aesthetic photograph by navigating within a local region in a scene. Instead of classical optimization over heuristics such as the rule-of-thirds, we adopt a data-driven aesthetics estimator to assess photo quality. A reinforcement learning framework is used to optimize the model with respect to the learned aesthetics metric. We train our model in simulation with indoor scenes, and we demonstrate that our system can capture aesthetic photos in both simulation and real world environments on a ground robot. To our knowledge, this is the first system that can automatically explore an environment to capture an aesthetic photo with respect to a learned aesthetic estimator. Source code is at https://github.com/HadiZayer/AutoPhoto},
  archive   = {C_IROS},
  author    = {Hadi AlZayer and Hubert Lin and Kavita Bala},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636788},
  pages     = {944-951},
  title     = {AutoPhoto: Aesthetic photo capture using reinforcement learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Imagination-enabled robot perception. <em>IROS</em>,
936–943. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many of today’s robot perception systems aim at accomplishing perception tasks that are too simplistic and too hard. They are too simplistic because they do not require the perception systems to provide all the information needed to accomplish manipulation tasks. Typically the perception results do not include information about the part structure of objects, articulation mechanisms and other attributes needed for adapting manipulation behavior. On the other hand, the perception problems stated are also too hard because — unlike humans— the perception systems cannot leverage the expectations about what they will see to their full potential. Therefore, we investigate a variation of robot perception tasks suitable for robots accomplishing everyday manipulation tasks, such as household robots or a robot in a retail store. In such settings it is reasonable to assume that robots know most objects and have detailed models of them. We propose a perception system that maintains its beliefs about its environment as a scene graph with physics simulation and visual rendering. When detecting objects, the perception system retrieves the model of the object and places it at the corresponding place in a VR-based environment model. The physics simulation ensures that object detections that are physically not possible are rejected and scenes can be rendered to generate expectations at the image level. The result is a perception system that can provide useful information for manipulation tasks.},
  archive   = {C_IROS},
  author    = {Patrick Mania and Franklin Kenghagho Kenfack and Michael Neumann and Michael Beetz},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636359},
  pages     = {936-943},
  title     = {Imagination-enabled robot perception},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards autonomous area inspection with a bio-inspired
underwater legged robot. <em>IROS</em>, 930–935. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, a new category of bio-inspired legged robots moving directly on the seabed have been proposed to complement the abilities of traditional underwater vehicles and to enhance manipulation and sampling tasks. So far, only tele-operated use of underwater legged robots has been reported and in this paper we attempt to fill such gap by presenting the first step towards autonomous area inspection. First, we present a 3 dimensional single-legged model for underwater hopping locomotion and derive a path following control strategy. Later, we adapt such control strategy to an underwater hexapod robot SILVER2 on the robotic simulator Webots. Finally, we simulate a full autonomous mission consisting in the inspection of an area over a pre-defined path, target recognition, transition to a safer gait and target approach. Our results show the feasibility of the approach and encourage the implementation of the presented control strategy on the robot SILVER2.},
  archive   = {C_IROS},
  author    = {Giacomo Picardi and Rossana Lovecchio and Marcello Calisti},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636316},
  pages     = {930-935},
  title     = {Towards autonomous area inspection with a bio-inspired underwater legged robot},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Coupling-dependent convergence behavior of phase oscillators
with tegotae-control. <em>IROS</em>, 923–929. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A bio-inspired way to model locomotion is using a network of coupled phase oscillators to create a Central Pattern Generator (CPG). The recently developed feedback control method tegotae includes exteroceptive force feedback into the governing phase update equations, leading to gait limit cycles. However, the oscillator coupling weights are often determined empirically. Here, we first investigate how the coupling coefficients influence the limit cycle convergence behavior on a 2- and 3-limbed structure in simulation. We find that the convergence with tegotae can be improved by introducing appropriate cross-couplings. This results in a smoother convergence and steady-state behavior where each individual oscillator drives the full network to a common convergence state in comparison to competing convergence states with ill-chosen cross-couplings. We then validate the findings in hardware and hypothesize how the appropriate couplings could be derived directly from the morphology, potentially eliminating the empiric determination.},
  archive   = {C_IROS},
  author    = {Simon Hauser and Matthieu Dujany and Jonathan Arreguit and Auke Ijspeert and Fumiya Iida},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636646},
  pages     = {923-929},
  title     = {Coupling-dependent convergence behavior of phase oscillators with tegotae-control},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quadrupedal template model for parametric stability analysis
of trotting gaits. <em>IROS</em>, 916–922. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simple template models have proven useful for understanding the underlying dynamics of legged locomotion. The most common one, the SLIP model, considers the legs as linear springs with constant stiffness, and it explains well the radial dynamics of the legs. However, in order to study the influence of the leg swing dynamics and leg segmentation on gait stability, more complex models are required. This paper introduces a novel template model for quadrupedal gait, which considers these additional aspects. The dynamic behavior of the model is analyzed via numerical simulation, using a continuation approach. By conducting a parametric analysis on the trotting gait and analyzing its stability, we identify the influence of the main model parameters, leading to marginally unstable limit cycles. These numerical results are applicable to the design of more efficient elastic quadrupedal robots.},
  archive   = {C_IROS},
  author    = {Lorenzo Boffa and Anna Sesselmann and Máximo A. Roa},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635974},
  pages     = {916-922},
  title     = {Quadrupedal template model for parametric stability analysis of trotting gaits},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The usage of kinematic singularities to produce periodic
high-powered locomotion. <em>IROS</em>, 909–915. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Legged robots primarily energize their center of mass through external contact during stance phase. This links their range of possible motions to actuator power limits applied during usually short periods of time. Enabling limb actuators to pump energy into the system during non-contact phases can greatly extend the energetic profile of possible motions. However, funneling this extra energy into useful dynamics is a problem of its own. In this paper, we propose designing limb mechanisms that purposefully integrate kinematic singularities into their configuration space to enable airborne energization and enhance gait periodicity before sensors and feedback control are incorporated. The result produces a mechanical reflex that unloads spring energy stored during flight phase into a useful push-off motion as triggered by the onset of stance phase. The implementation generates an &quot;S&quot; shaped curve into a specific slice of the configuration space, motivating the name S-curve. We compare our proposed approach to more conventional strategies and survey the design parameters that can be used to shape an S-curve. Ranges of useful S-curves are determined through a simulation study and a linkage mechanism capable of producing an S-curve is displayed.},
  archive   = {C_IROS},
  author    = {Chang Liu and Mark Plecnik},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636062},
  pages     = {909-915},
  title     = {The usage of kinematic singularities to produce periodic high-powered locomotion},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling and trajectory optimization for standing long
jumping of a quadruped with a preloaded elastic prismatic spine.
<em>IROS</em>, 902–908. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel methodology to model and optimize trajectories of a quadrupedal robot with spinal compliance to improve standing jump performance compared to quadrupeds with a rigid spine. We introduce an elastic model for a prismatic robotic spine that is actively preloaded and mechanically lock-enabled at initial and maximum length, and develop a constrained trajectory optimization method to cooptimize the elastic parameters and motion trajectories toward enhanced jumping distance. Results reveal that a less stiff spring is likely to facilitate jumping performance not as a direct propelling source but as a means to unleash more motor power for propelling by trading-off overall energy efficiency. We also visualize the impact of spring coefficients on the overall optimization routine from energetic perspectives to identify the suitable parameter region.},
  archive   = {C_IROS},
  author    = {Keran Ye and Konstantinos Karydis},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636606},
  pages     = {902-908},
  title     = {Modeling and trajectory optimization for standing long jumping of a quadruped with a preloaded elastic prismatic spine},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tachyon: Design and control of high payload, robust, and
dynamic quadruped robot with series-parallel elastic actuators.
<em>IROS</em>, 894–901. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a quadruped robot, Tachyon, which aims to achieve high payload, robust, and dynamic locomotion on the various terrain with high energy efficiency. Thanks to a novel compact series-parallel elastic actuator (SPEA) on the upper link and a four-bar linkage design in the knee joint for constant vertical foot force, the 41-kg robot can carry more than 20 kg of payloads with dynamic walking. The combination of a robust horizontal CoM stabilizer and SPEA joint torque controller provides low impedance force controllability of the whole-body even when only the knee joint can accurately detect its joint torque. The major performance of Tachyon is demonstrated by carrying a 20-kg rice bag—half of its body weight—and climbing stairs with dynamic locomotion. When the robot climbs the stairs, the SPEA parallel spring improves energy efficiency by 16\% and is also effective for various other gaits. The robustness of the robot is also shown by its high flexibility and fall avoidance capability when an unknown disturbance of 400 N or more is applied.},
  archive   = {C_IROS},
  author    = {Yasuhisa Kamikawa and Masaya Kinoshita and Noriaki Takasugi and Katsufumi Sugimoto and Toshimitsu Kai and Takashi Kito and Atsushi Sakamoto and Kenichiro Nagasaka and Yasunori Kawanami},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636196},
  pages     = {894-901},
  title     = {Tachyon: Design and control of high payload, robust, and dynamic quadruped robot with series-parallel elastic actuators},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pain expression-based visual feedback method for care
training assistant robot with musculoskeletal symptoms. <em>IROS</em>,
862–869. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A human patient simulator (HPS) can achieve effective visual-, auditory-, text-, and alarm-based feedback methods in care or nursing education. Among these, the method of visual feedback is important to design an HPS that can express emotions or feelings of pain like an actual human does because this method allows an immediate reaction between robots and humans. This study aims to develop an avatar-based visual feedback method for a care training assistant robot that can express pain states in joint care education. First, this study introduces its own pain facial expression database from Ritsumeikan University (RU-PITENS) for an avatar with pain expression. The RU-PITENS database contains pain images of 41 Japanese people in their 20s, 30s, 40s, and 60s, and an experiment of pain stimulus is conducted based on transcutaneous electrical nerve stimulation, which is low-cost and easy to use in daily life. Based on the pain images in the RU-PITENS database, we generated an avatar with pain expression to achieve the goal of our study. Since the RUPITENS database does not contain the quantitative pain level, the Siamese network was used to calculate the pain intensity. In addition, the care training assistant robot (CaTARo) developed in our previous study reproduces symptoms of musculoskeletal diseases, and the pain of CaTARo was measured using fuzzy logic theory. As a result, a visual feedback system was constructed to express five types of pain (no pain at all, very faint, weak, moderate, and strong pain) with avatars according to the intensity of the pain output of CaTARo in care training environments.},
  archive   = {C_IROS},
  author    = {Miran Lee and Dinh Tuan Tran and Joo-Ho Lee},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636671},
  pages     = {862-869},
  title     = {Pain expression-based visual feedback method for care training assistant robot with musculoskeletal symptoms},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). “Pretending to be okay in a sad voice”: Social robot’s usage
of verbal and nonverbal cue combination and its effect on human empathy
and behavior inducement. <em>IROS</em>, 854–861. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inducing a user’s behavior through social interaction is a goal that a social robot aims to achieve. It has been argued that empathy has a strong effect on behavior inducement. In human-human interaction, it has been verified that the influence of a nonverbal cue on empathy outweighs that of a verbal cue when those are used in a combined way. The objectives of this study are to explore if such outweighing effect of nonverbal cues is maintained in human-robot interaction (HRI) and to investigate the mechanism of communication cues’ effects by analyzing the mediation structure with the following mediator variables: perceived emotion, perceived intentionality, perceived malfunction, and empathy inducement. To this end, we conducted 2 (verbal type: positive verbal cue vs. negative verbal cue) × 2 (nonverbal type: positive nonverbal cue vs. negative nonverbal cue) within-participant experiment (N = 48). The experiment created a situation in which the social robot was harshly criticized during a conversation. The analysis of experiment results showed the outweighing effect of a nonverbal cue was maintained. When a nonverbal cue conveyed a negative, situation-accordant emotion, it had a decisive effect on perceived emotion, empathy, and behavior inducement. In contrast, a verbal cue induced participants’ empathy and behavior when it conveyed a positive, situation-discordant emotion. This inconsistency between verbal cue and nonverbal cue made the combination of positive verbal cue and negative nonverbal cue have the strongest effect. It implies that participants had different expectations for each of the two communication cues, just as they did in social interactions with human beings. It implies that participants might have applied normative expectations for social interaction with human beings to the social robot.},
  archive   = {C_IROS},
  author    = {Byeong June Moon and JongSuk Choi and Sonya S. Kwak},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636709},
  pages     = {854-861},
  title     = {&quot;Pretending to be okay in a sad voice&quot;: Social robot’s usage of verbal and nonverbal cue combination and its effect on human empathy and behavior inducement},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Personalization of human-robot gestural communication
through voice interaction grounding. <em>IROS</em>, 846–853. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we develop a gestural communication perception system for a social robot companion that is able to autonomously learn novel gestures on-the-fly. The system constantly tracks human gestural activities with a camera and evaluates the performed gestures under an open-set assumption. This allows for the identification of unknown gestures. Once detected, the system stores motion sequences of the novel gesture class and employs a dialogue interaction with the human to automatically label the unknown gesture. Subsequently, the gestural model is updated, grounding the unknown gesture through dialog interaction. In our experiment, we evaluate a neural network with varying threshold values for the open gesture recognition with unknown detection. Results show that the general classifier reaches an accuracy of more than 83\%, and an f1-score of 0.79 in an open-ended scenario. The method is furthermore tested in a first in-lab interaction setting, which shows the system usability and its potential for future personalized human-robot gestural communication.},
  archive   = {C_IROS},
  author    = {Heike Brock and Randy Gomez},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636105},
  pages     = {846-853},
  title     = {Personalization of human-robot gestural communication through voice interaction grounding},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Text-based robot emotion and human-like emotional
transition. <em>IROS</em>, 838–845. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Studies on the production of emotions have been conducted to create robotic facial expressions. The reported methodologies for generating emotions for a robot have focused on recognizing a user’s emotions using devices, such as cameras and microphones, and then generating the reactive emotions of a robot according to the user’s emotions. However, these methodologies may have some limitations in delivering emotions in the robot that match the robot’s utterances to users. In this paper, we propose a methodology for producing robotic emotions suitable for a robot’s utterances based on texts so that it can be applied to various fields such as robotic dialogue and reading services. To produce human-like emotions in a robot, our methodology applied patterns of human emotional changes as well as the resilience theory that humans have an ability to recover their emotional states considering their own personality over time. We measured the performance of the model for analyzing texts and observed that there is a linear correlation between the predicted emotions and the annotated ones from humans. Furthermore, when we carried out the experiments based on scenarios, our methodology could produce human-like patterns of emotional changes and the robot could recover its emotional state on its own.},
  archive   = {C_IROS},
  author    = {Yu-Jung Chae and Tae-Hee Jeon and ChangHwan Kim and Sung-Kee Park},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636793},
  pages     = {838-845},
  title     = {Text-based robot emotion and human-like emotional transition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). “Safe skin” - a low-cost capacitive proximity-force-fusion
sensor for safety in robots. <em>IROS</em>, 807–813. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents the design and evaluation of the low-cost capacitive proximity-force-fusion sensor &quot;safe skin&quot;, which can measure simultaneously the proximity of humans as well as the contact force. It was designed such that the force and proximity sensing functions can work concurrently without interfering with each other. Moreover, active shielding, on-chip digitization and ground isolation are implemented for the sensor to minimize the influence from stray capacitance and electromagnetic interference (EMI) from the environment, which ensures that the sensor has a high system robustness for industrial applications. The prototype version has the capability of detecting a grounded human hand sized object from a distance of 400 mm. Moreover, forces in the range of 5 to 40 N can be measured, with 43.7\% hysteresis and 6.7\% nonlinearity. Due to its sensing characteristics, when used on a robot, the sensor could be used to ensure the safety of nearby humans in the future. The sensor could also potentially be used as an interface for human-robot interaction (HRI).},
  archive   = {C_IROS},
  author    = {Zhen Wang and Heyang Gao and Alexander Schmitz and Sophon Somlor and Tito Pradhono Tomo and Shigeki Sugano},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636308},
  pages     = {807-813},
  title     = {&quot;Safe skin&quot; - a low-cost capacitive proximity-force-fusion sensor for safety in robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Radar based target tracking and classification for efficient
robot speed control in fenceless environments. <em>IROS</em>, 799–806.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Awareness of its surroundings is a crucial capability for a robot meant to be working alongside other robots or human operators. When considering safety norms and modalities, in particular the Speed and Separation Monitoring (SSM), proper proximity information can make the difference in the overall efficiency of a use case, for example avoiding unnecessary penalizations in the cycle-time. This paper presents a method to exploit the proximity perception capabilities of radar sensors to construct a continuous speed control algorithm for a UR10 robot. With respect to standard implementations of the SSM in industrial and collaborative environments, the proposed speed control is enhanced by the addition of direct human’s velocity measurement, full direction of travel and target classification. The results are evalauted according to the SSM metrics for safety and productivity, showing an overall increase in efficiency while still maintaining safety level requirements.},
  archive   = {C_IROS},
  author    = {Barnaba Ubezio and Christian Schöffmann and Lucas Wohlhart and Stephan Mülbacher-Karrer and Hubert Zangl and Michael Hofbaur},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636170},
  pages     = {799-806},
  title     = {Radar based target tracking and classification for efficient robot speed control in fenceless environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sampling-based inverse reinforcement learning algorithms
with safety constraints. <em>IROS</em>, 791–798. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Planning for robotic systems is frequently formulated as an optimization problem. Instead of manually tweaking the parameters of the cost function, they can be learned from human demonstrations by Inverse Reinforcement Learning (IRL). Common IRL approaches employ a maximum entropy trajectory distribution that can be learned with soft reinforcement learning, where the reward maximization is regularized with an entropy objective. The consideration of safety constraints is of paramount importance for human-robot collaboration. For this reason, our work addresses maximum entropy IRL in constrained environments. Our contribution to this research area is threefold: (1) We propose Constrained Soft Reinforcement Learning (CSRL), an extension of soft reinforcement learning to Constrained Markov Decision Processes (CMDPs). (2) We transfer maximum entropy IRL to CMDPs based on CSRL. (3) We show that using importance sampling in maximum entropy IRL in constrained environments introduces a bias and fails to achieve feature matching. In our evaluation we consider the tactical lane change decision of an autonomous vehicle in a highway scenario modeled in the SUMO traffic simulation.},
  archive   = {C_IROS},
  author    = {Johannes Fischer and Christoph Eyberg and Moritz Werling and Martin Lauer},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636672},
  pages     = {791-798},
  title     = {Sampling-based inverse reinforcement learning algorithms with safety constraints},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dual-filtering for on-line simultaneously estimate weights
and phase parameter of probabilistic movement primitives for human-robot
collaboration. <em>IROS</em>, 784–790. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Probabilistic Movement Primitives (ProMPs) is an essential issue and framework for robotics Learning from Demonstration (LfD). It has been successfully applied to the robotics field in tasks such as skill acquisition and Human-Robot Collaboration (HRC). In this paper, we focus on its adaptability in the HRC scenario, in which the adaptability of the ProMPs allows the robot to predict the future movement of its human partner and plan its movement accordingly, given the observed human movement. Most of the existing works about the application of the ProMPs in HRC either only focus on the estimation of the weights on-line and lack the estimation of the phase parameter or merely depend on the prior distribution of the phase parameter. As a result, these methods can lead to a misinterpretation of the basis matrix when the divergence between the prior distribution and the posterior distribution of the phase parameter becomes large, resulting in a divergence of the estimation of the weights. In this paper, we propose a Dual-Filtering method for the ProMPs, which is able to simultaneously on-line estimate the weights and phase parameter for the ProMPs. The preliminary experimental result demonstrates the proposed method is able to provide better prediction performance and more accurate estimation of the phase parameter in comparison with the previous works.},
  archive   = {C_IROS},
  author    = {Ren.C Luo and Licong Mai},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636654},
  pages     = {784-790},
  title     = {Dual-filtering for on-line simultaneously estimate weights and phase parameter of probabilistic movement primitives for human-robot collaboration},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online verification of impact-force-limiting control for
physical human-robot interaction. <em>IROS</em>, 777–783. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans must remain unharmed during their interaction with robots. We present a new method guaranteeing impact force limits when humans and robots share a workspace. Formal guarantees are realized using an online verification method, which plans and verifies fail-safe maneuvers through predicting reachable impact forces by considering all future possible scenarios. We model collisions as a coupled human-robot dynamical system with uncertainties and identify reachset-conforming models based on real-world collision experiments. The effectiveness of our approach for human-robot co-existence is demonstrated for the human hand interacting with the end effector of a six-axis robot manipulator with force sensing. By integrating a human pose detection system, the efficiency of robot movements increases.},
  archive   = {C_IROS},
  author    = {Stefan B. Liu and Matthias Althoff},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636610},
  pages     = {777-783},
  title     = {Online verification of impact-force-limiting control for physical human-robot interaction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Capturing skill state in curriculum learning for human skill
acquisition. <em>IROS</em>, 771–776. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans learn complex motor skills with practice and training. Though the learning process is not fully understood, several theories from motor learning, neuroscience, education, and game design suggest that curriculum-based training may be the key to efficient skill acquisition. However, designing such a curriculum and understanding its effects on learning are challenging problems. In this paper, we define the Human-skill Curriculum Markov Decision Process (H-CMDP) to systematize the design of training protocols. We also identify a vocabulary of performance features to enable the approximation for a human’s skill level across a variety of cognitive and motor tasks. A novel task domain is introduced as a testbed to evaluate the effectiveness of our approach. Human subject experiments show that (1) participants can learn to improve their performance in tasks within this domain, (2) the learning is quantifiable via our performance features, and (3) the domain is flexible enough to create distinct levels of difficulty. The long-term goal of this work is to systematize the process of curriculum-based training toward the design of protocols for robot-mediated rehabilitation.},
  archive   = {C_IROS},
  author    = {Keya Ghonasgi and Reuth Mirsky and Sanmit Narvekar and Bharath Masetty and Adrian M. Haith and Peter Stone and Ashish D. Deshpande},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636850},
  pages     = {771-776},
  title     = {Capturing skill state in curriculum learning for human skill acquisition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A wearable robotic device for assistive navigation and
object manipulation. <em>IROS</em>, 765–770. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a hand-worn assistive device to assist a visually impaired person with object manipulation. The device uses a Google Pixel 3 as the computational platform, a Structure Core (SC) sensor for perception, a speech interface, and a haptic interface for human-device interaction. W-ROMA is intended to assist a visually impaired person to locate a target object (nearby or afar) and guide the user to move towards and eventually take a hold of the object. To achieve this objective, three functions, including object detection, wayfinding, and motion guidance, are developed. Object detection locates the target object’s position if it falls within the camera’s field of view. Wayfinding enables the user to approach the object. The haptic/speech interface guides the user to move close to the object and then guides the hand to reach the object. A new visual-inertial odometery (VIO), called RGBD-VIO, is devised to accurately estimate the device’s pose (position and orientation), which is then used to generate the motion command to guide the user and his/her hand to reach the object. Experimental results demonstrate that RGBD-VIO outperforms the state-of-the-art VIO methods in 6-DOF device pose estimation and the device is effective in assistive object manipulation.},
  archive   = {C_IROS},
  author    = {Lingqiu Jin and He Zhang and Cang Ye},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636126},
  pages     = {765-770},
  title     = {A wearable robotic device for assistive navigation and object manipulation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novel wax based piezo actuator for autonomous deep
anterior lamellar keratoplasty (piezo-DALK). <em>IROS</em>, 757–764. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper reports the design and evaluation of a novel piezo based actuator for needle drive in autonomous Deep Anterior Lamellar Keratoplasty (piezo-DALK). The actuator weighs less than 8g and is 20mm x 20mm x 10.5mm in size, making it ideal for eye-mounted applications. Mean open loop positional deviation was 1.17 ± 3.15um, and system repeatability and accuracy were 17.16um and 18.33um, respectively. Stall force was found to vary linearly with the cooling cycle and the actuator achieved a maximum drive force of 3.98N. When simulating the DALK procedure in synthetic corneal tissue, the piezo-DALK achieved a penetration depth of 643.56um which was equivalent to 92.1\% of the total corneal thickness. This correlated closely with our desired depth of 90\% ± 5\% and took 2.5 hours to achieve. This work represents the first eye mountable actuator capable of &quot;Big Bubble&quot; needle drive for autonomous DALK procedures.},
  archive   = {C_IROS},
  author    = {J. D. Opfermann and M. Barbic and M. Khrenov and S. Guo and N. R. Sarfaraz and J. U. Kang and A. Krieger},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636153},
  pages     = {757-764},
  title     = {A novel wax based piezo actuator for autonomous deep anterior lamellar keratoplasty (Piezo-DALK)},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic-based RCM torque controller for robotic-assisted
minimally invasive surgery. <em>IROS</em>, 733–740. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we propose a novel flexible and optimization-free controller for standard torque-controlled manipulator for Robotic-Assisted Minimally Invasive Surgery. A novel method has been developed to model the constraint introduced by the laparoscopic tool, i.e. the remote center of motion, exploiting closed chain manipulators theory, and the final controller was synthesized considering the effects the constraint produces at a dynamic level. A set of simulations has been performed in a trajectory tracking task to validate the performances of the proposed controller. Performances have been also tested in a real experimental scenario with a KUKA LWR 4+ with 7 degrees of freedom endowed with a laparoscopic-like tool. Results show the effectiveness of the proposed controller and its capability of modifying the trajectory in order to preserve the RCM constraint.},
  archive   = {C_IROS},
  author    = {Marco Minelli and Cristian Secchi},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636823},
  pages     = {733-740},
  title     = {Dynamic-based RCM torque controller for robotic-assisted minimally invasive surgery},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning compliant grasping and manipulation by
teleoperation with adaptive force control. <em>IROS</em>, 717–724. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we focus on improving the robot’s dexterous capability by exploiting visual sensing and adaptive force control. TeachNet, a vision-based teleoperation learning framework, is exploited to map human hand postures to a multi-fingered robot hand. We augment TeachNet, which is originally based on an imprecise kinematic mapping and position-only servoing, with a biomimetic learning-based compliance control algorithm for dexterous manipulation tasks. This compliance controller takes the mapped robotic joint angles from TeachNet as the desired goal, computes the desired joint torques. It is derived from a computational model of the biomimetic control strategy in human motor learning, which allows adapting the control variables (impedance and feedforward force) online during the execution of the reference joint angle trajectories. The simultaneous adaptation of the impedance and feedforward profiles enables the robot to interact with the environment in a compliant manner. Our approach has been verified in multiple tasks in physics simulation, i.e., grasping, opening-a-door, turning-a-cap, and touching-a-mouse, and has shown more reliable performances than the existing position control and the fixed-gain-based force control approaches.},
  archive   = {C_IROS},
  author    = {Chao Zeng and Shuang Li and Yiming Jiang and Qiang Li and Zhaopeng Chen and Chenguang Yang and Jianwei Zhang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636832},
  pages     = {717-724},
  title     = {Learning compliant grasping and manipulation by teleoperation with adaptive force control},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robot hand based on a spherical parallel mechanism for
within-hand rotations about a fixed point. <em>IROS</em>, 709–716. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Rotating a grasped object about all three spatial axes is challenging, because kinematically redundant robot hands require complex control schemes for within-hand rotations, and simple parallel grippers require inefficient whole arm motions. We present a novel 3-finger robot hand design inspired by a spherical parallel mechanism that achieves these rotations with just 3 actuators. The hand is designed such that at every hand-object configuration, the object pose moves along a sphere with a fixed center, which is determined by the intersection of the fingers’ revolute axes and is independent of the object shape, pose, and the initial grasp configuration. We optimize the hand based on 3-RRS spherical manipulator to maximize both its rotational workspace size and manipulation motion quality. From these parameters, we implement and experimentally evaluate the hand design through grasping tests, manipulation characterization, and real-world task scenarios, which show that the hand is able to grasp a variety of object geometries and accomplish precise single and multi-DOF rotations about a fixed point. We believe this design can remarkably improve robustness and simplify control for dexterous within-hand rotations, which finds utility in augmenting the capabilities of low-DOF robot arms without an active wrist.},
  archive   = {C_IROS},
  author    = {Vatsal V. Patel and Aaron M. Dollar},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636704},
  pages     = {709-716},
  title     = {Robot hand based on a spherical parallel mechanism for within-hand rotations about a fixed point},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time safety and control of robotic manipulators with
torque saturation in operational space. <em>IROS</em>, 702–708. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a real-time safety and control for robot manipulators using control barrier functions and control Lyapunov functions in operational space. We first define the operational space in terms of system dynamics, jacobian, and torques and then ensure safety by designing Control Barrier Functions (CBF) around the body links of the robotic manipulator. The control barrier function provides provable collision-free behavior for the robotic manipulator by modifying the nominal control in a minimally invasive manner to formally satisfy the safety constraints. CBFs are formulated as a quadratic programming problem, which can be solved in real-time. We also design a controller based on Rapidly Exponentially Stabilizing Control Lyapunov Function (RESCLF) and quadratic programming to meet multiple objectives while ensuring exponential convergence. We then extend our formulation to solve RESCLF and CBF in a unified formulation to design the controller while ensuring the safety of manipulators and guaranteeing the torque saturation. The efficacy of the proposed approach is shown on 7 Degree of Freedom (DoF) KUKA LBR iiwa robot using Dynamic Animation and Robotics Toolkit (DART) physics engine.},
  archive   = {C_IROS},
  author    = {Muhammad Ali Murtaza and Sergio Aguilera and Vahid Azimi and Seth Hutchinson},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636794},
  pages     = {702-708},
  title     = {Real-time safety and control of robotic manipulators with torque saturation in operational space},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deformation control of a deformable object based on visual
and tactile feedback. <em>IROS</em>, 675–681. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we presented a new method for deformation control of deformable objects, which utilizes both visual and tactile feedback. At present, manipulation of deformable objects is basically formulated by assuming positional constraints. But in fact, in many situations manipulation has to be performed under actively applied force constraints. This scenario is considered in this research. In the proposed scheme a tactile feedback is integrated to ensure a stable contact between the robot end-effector and the soft object to be manipulated. The controlled contact force is also utilized to regulate the deformation of the soft object with its shape measured by a vision sensor. The effectiveness of the proposed method is demonstrated by a book page turning and shaping experiment.},
  archive   = {C_IROS},
  author    = {Yuhao Guo and Xin Jiang and Yunhui Liu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636157},
  pages     = {675-681},
  title     = {Deformation control of a deformable object based on visual and tactile feedback},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Casting manipulation of unknown string by robot arm.
<em>IROS</em>, 668–674. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Casting manipulation has been studied to expand the robot’s movable range. In this manipulation, the robot throws and reaches the end effector to a distant target. Usually, a special casting manipulator, which consists of rigid arm links and specific flexible linear objects, is constructed for an effective casting manipulation. However, the special manipulator cannot perform normal manipulations, such as picking and placing, grasping, and operating objects. We propose that the normal robot arm, which can perform normal tasks, picks up an unknown string in the surrounding environment and realizes casting manipulation with it. As the properties of the string are not provided in advance, it is crucial how to reflect it in casting manipulation. This is realized by the motion generation of the robot arm with the simulation of string movement, actual string manipulation by the robot arm, and string parameter estimation from the actual string movement. After repeating these three steps, the simulated string movement approximates the actual to realize casting manipulation with the unknown string. We confirmed the effectiveness of the proposed method through experiments. The try of this study will lead to enhancement of the performance of home service robot, exploration robot, rescue robot and entertainment robot.},
  archive   = {C_IROS},
  author    = {Kenta Tabata and Hiroaki Seki and Tokuo Tsuji and Tatsuhiro Hiramitsu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635837},
  pages     = {668-674},
  title     = {Casting manipulation of unknown string by robot arm},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic modelling and visco-elastic parameter identification
of a fibre-reinforced soft fluidic elastomer manipulator. <em>IROS</em>,
661–667. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A dynamic model of a soft fibre-reinforced fluidic elastomer is presented and experimentally verified, which can be used for model-based controller design. Due to the inherent visco-(hyper)elastic characteristics and nonlinear time-dependent behaviour of soft fluidic elastomer robots, analytic dynamic modelling is challenging. The fibre reinforced noninflatable soft fluidic elastomer robot used in this paper can produce both planar and spatial movements. Dynamic equations are developed for both cases. Parameters, related to the viscoelastic behaviour of the robot during elongation and bending motion, are identified experimentally and incorporated into our model. The modified dynamic model is then validated in experiments comparing the time responses of the physical robot with the corresponding outputs of the simulation model. The results validate the accuracy of the proposed dynamic model.},
  archive   = {C_IROS},
  author    = {Azadeh Shariati and Jialei Shi and Sarah Spurgeon and Helge A Wurdemann},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636785},
  pages     = {661-667},
  title     = {Dynamic modelling and visco-elastic parameter identification of a fibre-reinforced soft fluidic elastomer manipulator},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SoPrA: Fabrication &amp; dynamical modeling of a scalable
soft continuum robotic arm with integrated proprioceptive sensing.
<em>IROS</em>, 653–660. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Due to their inherent compliance, soft robots are more versatile than rigid linked robots when they interact with their environment, such as object manipulation or biomimetic motion, and are considered to be the key element in introducing robots to everyday environments. Although various soft robotic actuators exist, past research has focused primarily on designing and analyzing single components. Limited effort has been made to combine each component to create an overall capable, integrated soft robot. Ideally, the behavior of such a robot can be accurately modeled, and its motion within an environment uses its proprioception, without requiring external sensors. This work presents a design and modeling process for a Soft continuum Proprioceptive Arm (SoPrA) actuated by pneumatics. The integrated design is suitable for an analytical model due to its internal capacitive flex sensor for proprioceptive measurements and its fiber-reinforced fluidic elastomer actuators. The proposed analytical dynamical model accounts for the inertial effects of the actuator’s mass and the material properties, and predicts in real-time the soft robot’s behavior. Our estimation method integrates the analytical model with proprioceptive sensors to calculate external forces, all without relying on an external motion capture system. SoPrA is validated in a series of experiments demonstrating the model’s and sensor’s accuracy in estimation. SoPrA will enable soft arm manipulation including force sensing while operating in obstructed environments that disallows exteroceptive measurements.},
  archive   = {C_IROS},
  author    = {Yasunori Toshimitsu and Ki Wan Wong and Thomas Buchner and Robert Katzschmann},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636539},
  pages     = {653-660},
  title     = {SoPrA: Fabrication &amp; dynamical modeling of a scalable soft continuum robotic arm with integrated proprioceptive sensing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Shape-centric modeling for soft robot inchworm locomotion.
<em>IROS</em>, 645–652. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft robot modeling tends to prioritize soft robot dynamics in order to recover how they might behave. Soft robot design tends to focus on how to use compliant elements with actuation to effect certain canonical movement profiles. For soft robot locomotors, these profiles should lead to locomotion. Naturally, there is a gap between the emphasis of computational modeling and the needs of locomotion design. This paper proposes to consider modeling and computation efforts directed more toward understanding soft robot-world interactions with locomotion in mind. With a SMA-actuated inchworm as the soft robot to model and control, the framework is a combination of shape identification and geometric modeling that culminates in control equations of motion. When applied to the task of gait-based locomotion, the equations operate in a low dimensional shape-based gait space. Simulated and experimentally applied gaits for an inchworm model showed qualitatively similar outcomes, while the measured net displacement per gait cycle coincided within 9\%. This result advances the idea that a shape-centric approach to soft robot modeling for control and locomotion may provide predictive locomotive models.},
  archive   = {C_IROS},
  author    = {Alexander H. Chang and Caitlin Freeman and Arun Niddish Mahendran and Vishesh Vikas and Patricio A. Vela},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636695},
  pages     = {645-652},
  title     = {Shape-centric modeling for soft robot inchworm locomotion},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Soft-CCD algorithm for inverse kinematics of soft continuum
manipulators. <em>IROS</em>, 639–644. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To date, soft robots have been increasingly designed and analyzed, especially, Soft Continuum Manipulators (SCMs). Due to dexterous deformability, their Inverse Kinematics (IK) is still difficult to solve. Cyclic Coordinate Descent (CCD) algorithm is one of the classical optimization algorithms to solve IK of rigid manipulators with prismatic or rotational joints. However, it cannot be directly extrapolated to SCMs with configuration space parameters such as center arc, bending angle, and torsion angle. Here, we modified the CCD algorithm from a new view and proposed several tricks to set constraints. Numerical and experimental results show that the soft-CCD algorithm can quickly and accurately generate solutions for IK of SCMs. This study provides the necessary kinematic foundation for fault tolerance, obstacle avoidance, trajectory planning, and other further explorations of SCMs.},
  archive   = {C_IROS},
  author    = {Zhiyuan Zhang and Songtao Wang and Deshan Meng and Xueqian Wang and Bin Liang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635921},
  pages     = {639-644},
  title     = {Soft-CCD algorithm for inverse kinematics of soft continuum manipulators},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Analytical modeling of a soft pneu-net actuator based on
finite strain beam theory. <em>IROS</em>, 632–638. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a simple analytical model for pneu-net soft actuator. The model is based on Euler– Bernoulli finite strain hyperelastic thin cantilever beam theory. The deformation of the air chambers is modelled using infinitesimal strain membrane theory. The proposed theoretical model estimates the deformation and force characteristics of the actuator. The developed model accounts the axial stretch and forces applied to the actuator. The theoretical deformation and force characteristics of different actuators are compared with finite element (FE) model and experimental characteristics. The theoretically estimated deformation and force of the actuator are similar to the FE model, but the theoretical model computation time is less than 1\% of the FE model.},
  archive   = {C_IROS},
  author    = {Sachin Sachin and Zhongkui Wang and Shinichi Hirai},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635996},
  pages     = {632-638},
  title     = {Analytical modeling of a soft pneu-net actuator based on finite strain beam theory},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A parameter identification method for static cosserat rod
models: Application to soft material actuators with exteroceptive
sensors. <em>IROS</em>, 624–631. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft material robotics is a rather young research field in the robotics and material science communities. A popular design is the soft pneumatic actuator (SPA) which, if connected serially, becomes a highly compliant manipulator. This high compliance makes it possible to adapt to the environment and in the future might be very useful for manipulation tasks in narrow and wound environments. A central topic is the modelling of the manipulators. While comparatively rigid continuum robots are build of metal or other materials, that conduct a linear behaviour, the material used in soft material robotics often exhibits a nonlinear stress-strain relationship. In this paper we contribute an identification method for material parameters and data-based approach within the constitutive equations of a Cosserat rod model. We target bending and extension stiffness, consider shear and neglect torsional strains. The proposed method is applicable to any continuum robot which can be modelled by the classic theory of special Cosserat rods, including constraint models, and shows great improvement in experimental results with mean position errors of 0.59\% reference length.},
  archive   = {C_IROS},
  author    = {Max Bartholdt and Mats Wiese and Moritz Schappler and Svenja Spindeldreier and Annika Raatz},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636447},
  pages     = {624-631},
  title     = {A parameter identification method for static cosserat rod models: Application to soft material actuators with exteroceptive sensors},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Soft robot configuration estimation and control using
simultaneous localization and mapping. <em>IROS</em>, 616–623. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we present a novel approach to accomplishing soft robot configuration estimation and control using RGB-D cameras and SLAM-based methods. By placing cameras on the unactuated sections of our large-scale (approximately 2 meters long) pneumatic soft robot, we can map an environment and then estimate the orientation of the robot links using landmark-based localization. Using the orientations of each camera we can solve for the joint configurations between them. We first show that this method works for a traditional rigid robot (Baxter) where we can compare against the ground truth encoder values. For Baxter, the median joint angle error was on the order of 1-2 ◦ . We then show that the SLAM-based method provides estimates for soft robot configuration that are within 1 ◦ when compared to our past methods of using a HTC Vive Tracker. While HTC Vive Trackers and commonly used motion capture systems require externally mounted sensors placed in the robot’s environment, the SLAM-based estimation method presented here works in any visually feature-rich environment. Finally we show that this method of estimation is effective for closed-loop control of soft robots by controlling our large-scale soft robot through a series of joint configurations.},
  archive   = {C_IROS},
  author    = {Christian Sorensen and Phillip Hyatt and Matthew Ricks and Seth Nielsen and Marc D. Killpack},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635896},
  pages     = {616-623},
  title     = {Soft robot configuration estimation and control using simultaneous localization and mapping},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Human motion imitation using optimal control with
time-varying weights. <em>IROS</em>, 608–615. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Research in biomechanics hypothesizes that human motion is optimal with respect to an unknown cost function that varies depending on the action and/or task. This unknown cost function is often approximated as the weighted sum of a set of features or basis cost functions. As a person performs a sequence of actions, the weights associated to each of these basis functions are likely to vary over time. Given a human demonstration and the corresponding cost weight trajectory recovered via inverse optimal control (IOC), this paper proposes an optimal control (OC) method that can generate robot motion based on human movement using time-varying cost function weights. By using time-varying weights, the proposed optimal control method can handle changing optimization criteria without segmentation. The method is evaluated both in simulation and with recorded human data. Using human demonstration data, we demonstrate the reproduction of pick-and-place motions with an average end-effector error at the pick place location within 0.82 cm, which is significantly lower than the average trajectory error, indicating that the approach correctly prioritizes reaching the pick and place locations without manual segmentation.},
  archive   = {C_IROS},
  author    = {Shouyo Ishida and Tatsuki Harada and Pamela Carreno-Medrano and Dana Kulić and Gentiane Venture},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636520},
  pages     = {608-615},
  title     = {Human motion imitation using optimal control with time-varying weights},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Car racing line optimization with genetic algorithm using
approximate homeomorphism. <em>IROS</em>, 601–607. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In every timed car race, the goal is to drive through the racing track as fast as possible. The total time depends on selection of the racing line. Following a better racing line often decides who wins. In this paper, we solve the optimal racing line problem using a genetic algorithm. We propose a novel racing line encoding based on a homeomorphic transformation called Matryoshka mapping. We evaluate the fitness of racing lines by lap time estimation using a vehicle model suitable for F1/10 autonomous racing competition. By comparing to the former state-of-the-art, we show that our method is able to find racing lines with lower lap times. Specifically, on one of the testing tracks, we achieve 2.5\% improvement.},
  archive   = {C_IROS},
  author    = {Jaroslav Klapálek and Antonín Novák and Michal Sojka and Zdeněk Hanzálek},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636503},
  pages     = {601-607},
  title     = {Car racing line optimization with genetic algorithm using approximate homeomorphism},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Closed-loop robotic cooking of scrambled eggs with a
salinity-based “taste” sensor. <em>IROS</em>, 594–600. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The sense of taste is fundamental to a human chef’s ability to cook tasty food. To develop robots that can demonstrate human-like cooking, robots need to be equipped with a sense of taste and enabled to use this perception capability to improve or understand the food which they are cooking. We propose a first study of using a salinity sensor to provide a robot with a sense of saltiness. We then demonstrate how this artificial taste receptor can be used to create an autonomous closed-loop cooking system that uses a measurement of saltiness to improve the cooking process of preparing scrambled eggs. Specifically, we show that the sensor measurements can be mapped to different taste metrics such as the overall saltiness and state of mixing the eggs, and how the cooking process can be adapted to match a human-cooked dish, hence individual preferences.},
  archive   = {C_IROS},
  author    = {Grzegorz Sochacki and Josie Hughes and Simon Hauser and Fumiya Iida},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636750},
  pages     = {594-600},
  title     = {Closed-loop robotic cooking of scrambled eggs with a salinity-based ‘Taste’ sensor},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient and reactive planning for high speed robot air
hockey. <em>IROS</em>, 586–593. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Highly dynamic robotic tasks require high-speed and reactive robots. These tasks are particularly challenging due to the physical constraints, hardware limitations, and the high uncertainty of dynamics and sensor measures. To face these issues, it’s crucial to design robotics agents that generate precise and fast trajectories and react immediately to environmental changes. Air hockey is an example of this kind of task. Due to the environment’s characteristics, it is possible to formalize the problem and derive clean mathematical solutions. For these reasons, this environment is perfect for pushing to the limit the performance of currently available general-purpose robotic manipulators. Using two Kuka Iiwa 14, we show how to design a policy for general-purpose robotic manipulators for the air hockey game. We demonstrate that a real robot arm can perform fast-hitting movements and that the two robots can play against each other on a medium-size air hockey table in simulation.},
  archive   = {C_IROS},
  author    = {Puze Liu and Davide Tateo and Haitham Bou-Ammar and Jan Peters},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636263},
  pages     = {586-593},
  title     = {Efficient and reactive planning for high speed robot air hockey},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Probabilistic iterative LQR for short time horizon MPC.
<em>IROS</em>, 579–585. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Optimal control is often used in robotics for planning a trajectory to achieve some desired behavior, as expressed by the cost function. Most works in optimal control focus on finding a single optimal trajectory, which is then typically tracked by another controller. In this work, we instead consider trajectory distribution as the solution of an optimal control problem, resulting in better tracking performance and a more stable controller. A Gaussian distribution is first obtained from an iterative Linear Quadratic Regulator (iLQR) solver. A short horizon Model Predictive Control (MPC) is then used to track this distribution. We show that tracking the distribution is more cost-efficient and robust as compared to tracking the mean or using iLQR feedback control. The proposed method is validated with kinematic control of 7-DoF Panda manipulator and dynamic control of 6-DoF quadcopter in simulation.},
  archive   = {C_IROS},
  author    = {Teguh Santoso Lembono and Sylvain Calinon},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636295},
  pages     = {579-585},
  title     = {Probabilistic iterative LQR for short time horizon MPC},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Embedded hardware appropriate fast 3D trajectory
optimization for fixed wing aerial vehicles by leveraging hidden convex
structures. <em>IROS</em>, 571–578. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most commercially available fixed-wing aerial vehicles (FWV) can carry only small, lightweight computing hardware such as Jetson TX2 onboard. Solving non-linear trajectory optimization on these computing resources is computationally challenging even while considering only the kinematic motion model. Most importantly, the computation time increases sharply as the environment becomes more cluttered. In this paper, we take a step towards overcoming this bottleneck and propose a trajectory optimizer that achieves online performance on both conventional laptops/desktops and Jetson TX2 in a typical urban environment setting. Our optimizer builds on the novel insight that the seemingly non-linear trajectory optimization problem for FWV has an implicit multi-convex structure. Our optimizer exploits these computational structures by bringing together diverse concepts from Alternating Minimization, Bregman iteration, and Alternating Direction Method of Multipliers. We show that our optimizer outperforms the state-of-the-art implementation of sequential quadratic programming approach in optimal control solver ACADO in computation time and solution quality measured in terms of control and goal reaching cost.},
  archive   = {C_IROS},
  author    = {Vivek Kantilal Adajania and Houman Masnavi and Fatemeh Rastgar and Karl Kruusamae and Arun Kumar Singh},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636337},
  pages     = {571-578},
  title     = {Embedded hardware appropriate fast 3D trajectory optimization for fixed wing aerial vehicles by leveraging hidden convex structures},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Offset-free model predictive control: A ball catching
application with a spherical soft robotic arm. <em>IROS</em>, 563–570.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an offset-free model predictive controller for fast and accurate control of a spherical soft robotic arm. In this control scheme, a linear model is combined with an online disturbance estimation technique to systematically compensate model deviations. Dynamic effects such as material relaxation resulting from the use of soft materials can be addressed to achieve offset-free tracking. The tracking error can be reduced by 35\% when compared to a standard model predictive controller without a disturbance compensation scheme. The improved tracking performance enables the realization of a ball catching application, where the spherical soft robotic arm can catch a ball thrown by a human.},
  archive   = {C_IROS},
  author    = {Yaohui Huang and Matthias Hofer and Raffaello D’Andrea},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636608},
  pages     = {563-570},
  title     = {Offset-free model predictive control: A ball catching application with a spherical soft robotic arm},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Direct bundle adjustment for 3D image fusion with
application to transesophageal echocardiography. <em>IROS</em>, 548–554.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel algorithm for fusing a sequence of 3D images, named as Direct Bundle Adjustment (DBA). This algorithm simultaneously optimizes the global pose parameters of image frames and the intensity values of the fused global image using the 3D image data directly (without extracting features from the images). This one-step 3D image fusion approach is achieved by formulating the problem as an optimization problem to minimize the intensity differences between the global image and the corresponding points in the different local images. The proposed DBA method is particularly useful in the scenarios where distinct features are not available, such as Transesophageal Echocardiography (TEE) images. We validate the proposed method via simulated and in-vivo 3D TEE images. It is shown that the proposed method is robust to intensity noises and much more accurate than the conventional sequential fusion method.},
  archive   = {C_IROS},
  author    = {Zhehua Mao and Liang Zhao and Shoudong Huang and Yiting Fan and Alex Pui-Wai Lee},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636721},
  pages     = {548-554},
  title     = {Direct bundle adjustment for 3D image fusion with application to transesophageal echocardiography},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). StereoCNC: A stereovision-guided robotic laser system.
<em>IROS</em>, 540–547. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a stereovision-guided robotic laser system that can conduct laser ablation on targets selected by human operators in the color image, referred as StereoCNC. Two digital cameras are integrated into a previously developed robotic laser system to add a color sensing modality and formulate the stereovision. A calibration method is implemented to register the coordinate frames between stereo cameras and the laser system, modelled as a 3D-to-3D Least-squares problem. This problem is solved by a RANSAC-based 3D rigid transformation method and the calibration reprojection errors are used to characterize a 3D error field by Gaussian process regression. This regression error model is used to predict an error value for each data point of a stereo-reconstructed point cloud and an optimization problem is formulated to adjust the surgical site to a new position with minimum reprojection errors. Based on the calibrated system and the error model, a stereovision-guided laser-tissue removal pipeline is proposed to precisely locate, target, and ablate a surface region. The pipeline is validated by the experiments on phantoms with color texture and various geometric shapes. The overall targeting accuracy of the system achieves an average RMSE of 0.13±0.02 mm and maximum error of 0.34±0.06 mm, as measured by pre- and post-laser ablation images. The results show potential applications of using the developed stereovision-guided robotic system for superficial laser surgery, including dermatologic applications or removal of exposed tumorous tissue in neurosurgery.},
  archive   = {C_IROS},
  author    = {Guangshen Ma and Weston Ross and Patrick J. Codd},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636050},
  pages     = {540-547},
  title     = {StereoCNC: A stereovision-guided robotic laser system},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LaneRCNN: Distributed representations for graph-centric
motion forecasting. <em>IROS</em>, 532–539. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Forecasting the future behaviors of dynamic actors is an important task in many robotics applications such as self-driving. It is extremely challenging as actors have latent intentions and their trajectories are governed by complex interactions between the other actors, themselves, and the map. In this paper, we propose LaneRCNN, a graph-centric motion forecasting model that captures the actor-to-actor and the actor-to-map relations in a distributed and structured manner. Relying on a specially designed graph encoder, we learn a local graph representation per actor (LaneRoI) to encode its past motions and the local map topology. We further develop an interaction module which permits efficient message passing among local graph representations within a shared global lane graph. Moreover, we parameterize the output trajectories based on lane graphs, a more amenable prediction parameterization. We demonstrate the effectiveness of our approach on the challenging Argoverse [1] motion forecasting benchmark and achieve state-of-the-art performance.},
  archive   = {C_IROS},
  author    = {Wenyuan Zeng and Ming Liang and Renjie Liao and Raquel Urtasun},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636035},
  pages     = {532-539},
  title     = {LaneRCNN: Distributed representations for graph-centric motion forecasting},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Localization and control of magnetic suture needles in
cluttered surgical site with blood and tissue. <em>IROS</em>, 524–531.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-time visual localization of needles is necessary for various surgical applications, including surgical automation and visual feedback. In this study we investigate localization and autonomous robotic control of needles in the context of our magneto-suturing system. Our system holds the potential for surgical manipulation with the benefit of minimal invasiveness and reduced patient side effects. However, the nonlinear magnetic fields produce unintuitive forces and demand delicate position-based control that exceeds the capabilities of direct human manipulation. This makes automatic needle localization a necessity. Our localization method combines neural network-based segmentation and classical techniques, and we are able to consistently locate our needle with 0.73mm RMS error in clean environments and 2.72mm RMS error in challenging environments with blood and occlusion. The average localization RMS error is 2.16 mm for all environments we used in the experiments. We combine this localization method with our closed-loop feedback control system to demonstrate the further applicability of localization to autonomous control. Our needle is able to follow a running suture path in (1) no blood, no tissue; (2) heavy blood, no tissue; (3) no blood, with tissue; and (4) heavy blood, with tissue environments. The tip position tracking error ranges from 2.6mm to 3.7mm RMS, opening the door towards autonomous suturing tasks.},
  archive   = {C_IROS},
  author    = {Will Pryor and Yotam Barnoy and Suraj Raval and Xiaolong Liu and Lamar Mair and Daniel Lerner and Onder Erin and Gregory D. Hager and Yancy Diaz-Mercado and Axel Krieger},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636441},
  pages     = {524-531},
  title     = {Localization and control of magnetic suture needles in cluttered surgical site with blood and tissue},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust and accurate point set registration with generalized
bayesian coherent point drift. <em>IROS</em>, 516–523. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Point set registration (PSR) is an essential problem in surgical navigation and image-guided surgery (IGS). It can help align the pre-operative volumetric images with the intra-operative surgical space. The performances of PSR are susceptible to noise and outliers, which are the cases in real-world surgical scenarios. In this paper, we provide a novel point set registration method that utilizes the features extracted from the PSs and can guarantee the convergence of the algorithm simultaneously. More specifically, we formulate the PSR with normal vectors by generalizing the bayesian coherent point drift (BCPD) into the six-dimension scenario. Our contributions can be summarized as follows. (1) The PSR problem with normal vectors is formulated by generalizing the Bayesian coherent point drift (BCPD) approach; (2) The updated parameters during the algorithm&#39;s iterations are given in closed-forms; (3) Extensive experiments have been done to verify the proposed approach and its significant improvements over the BCPD has been validated. We have validated our proposed registration approach on both the human femur model. Results demonstrate that our proposed method outperforms the state-of-the-art registration methods and the convergence is guaranteed at the same time.},
  archive   = {C_IROS},
  author    = {Ang Zhang and Zhe Min and Jin Pan and Max Q.-H. Meng},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635908},
  pages     = {516-523},
  title     = {Robust and accurate point set registration with generalized bayesian coherent point drift},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Using depth vision for terrain detection during active
locomotion. <em>IROS</em>, 508–515. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vision-based systems for terrain detection are ubiquitous in mobile robotics, while such systems recently emerged for locomotion assistance of disabled people. For instance, wearable devices embedding vision sensors can assist people in navigation; or guide lower-limb prosthesis or exoskeleton controller to retrieve gait patterns being adapted to the executed task (overground walking, stairs, slopes, etc.). In this research, we present a vision-based algorithm achieving the detection of flat ground, steps, and ramps, using a depth camera. The raw point cloud data obtained from the depth camera passes through multiple processing steps to extract environmental features, and then achieves terrain detection by feeding these features into a classification tree. The camera was mounted on a custom-made wearable tool that can be placed on the chest of human users. This contribution reports a pilot validation study with 6 healthy subjects moving in an indoor environment containing a rich set of different types of terrains. Our method can predict the locomotion modes up to three steps in front of the user. Moreover, it is able to perform terrain detection even if the path is partially occluded by another walker. The method accuracy with a cleared path was found to be above 90\% for all locomotion modes.},
  archive   = {C_IROS},
  author    = {Ali H. A. Al-dabbagh and Renaud Ronsse},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636077},
  pages     = {508-515},
  title     = {Using depth vision for terrain detection during active locomotion},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast and robust bio-inspired teach and repeat navigation.
<em>IROS</em>, 500–507. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fully autonomous mobile robots have a multitude of potential applications, but guaranteeing robust navigation performance remains an open research problem. For many tasks such as repeated infrastructure inspection, item delivery, or inventory transport, a route repeating capability can be sufficient and offers potential practical advantages over a full navigation stack. Previous teach and repeat research has achieved high performance in difficult conditions predominantly by using sophisticated, expensive sensors, and has often had high computational requirements. Biological systems, such as small animals and insects like seeing ants, offer a proof of concept that robust and generalisable navigation can be achieved with extremely limited visual systems and computing power. In this work we create a novel asynchronous formulation for teach and repeat navigation that fully utilises odometry information, paired with a correction signal driven by much more computationally lightweight visual processing than is typically required. This correction signal is also decoupled from the robot’s motor control, allowing its rate to be modulated by the available computing capacity. We evaluate this approach with extensive experimentation on two different robotic platforms, the Consequential Robotics Miro and the Clearpath Jackal robots, across navigation trials totalling more than 6000 metres in a range of challenging indoor and outdoor environments. Our approach continues to succeed when multiple state-of-the-art systems fail due to low resolution images, unreliable odometry, or lighting change, while requiring significantly less compute. We also – for the first time – demonstrate versatile cross-platform teach and repeat without changing parameters, in which we learn to navigate a route with one robot and repeat that route using a completely different robot.},
  archive   = {C_IROS},
  author    = {Dominic Dall’Osto and Tobias Fischer and Michael Milford},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636334},
  pages     = {500-507},
  title     = {Fast and robust bio-inspired teach and repeat navigation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NavTuner: Learning a scene-sensitive family of navigation
policies. <em>IROS</em>, 492–499. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The advent of deep learning has inspired research into end-to-end learning for a variety of problem domains in robotics. For navigation, the resulting methods may not have the generalization properties desired let alone match the performance of traditional methods. Instead of learning a navigation policy, we explore learning an adaptive policy in the parameter space of an existing navigation module. Having adaptive parameters provides the navigation module with a family of policies that can be dynamically reconfigured based on the local scene structure and addresses the common assertion in machine learning that engineered solutions are inflexible. Of the methods tested, reinforcement learning (RL) is shown to provide a significant performance boost to a modern navigation method through reduced sensitivity of its success rate to environmental clutter. The outcomes indicate that RL as a meta-policy learner, or dynamic parameter tuner, effectively robustifies algorithms sensitive to external, measurable nuisance factors.},
  archive   = {C_IROS},
  author    = {Haoxin Ma and Justin S. Smith and Patricio A. Vela},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636185},
  pages     = {492-499},
  title     = {NavTuner: Learning a scene-sensitive family of navigation policies},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning navigation skills for legged robots with learned
robot embeddings. <em>IROS</em>, 484–491. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent work has shown results on learning navigation policies for idealized cylinder agents in simulation and transferring them to real wheeled robots. Deploying such navigation policies on legged robots can be challenging due to their complex dynamics, and the large dynamical difference between cylinder agents and legged systems. In this work, we learn hierarchical navigation policies that account for the low-level dynamics of legged robots, such as maximum speed, slipping, contacts, and learn to successfully navigate cluttered indoor environments. To enable transfer of policies learned in simulation to new legged robots and hardware, we learn dynamics-aware navigation policies across multiple robots with robot-specific embeddings. The learned embedding is optimized on new robots, while the rest of the policy is kept fixed, allowing for quick adaptation. We train our policies across three legged robots in simulation - 2 quadrupeds (A1, AlienGo) and a hexapod (Daisy). At test time, we study the performance of our learned policy on two new legged robots in simulation (Laikago, 4-legged Daisy), and one real-world quadrupedal robot (A1). Our experiments show that our learned policy can sample-efficiently generalize to previously unseen robots, and enable sim-to-real transfer of navigation policies for legged robots.},
  archive   = {C_IROS},
  author    = {Joanne Truong and Denis Yarats and Tianyu Li and Franziska Meier and Sonia Chernova and Dhruv Batra and Akshara Rai},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635911},
  pages     = {484-491},
  title     = {Learning navigation skills for legged robots with learned robot embeddings},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Trajectory-constrained deep latent visual attention for
improved local planning in presence of heterogeneous terrain.
<em>IROS</em>, 460–467. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a reward-predictive, model-based learning method featuring trajectory-constrained visual attention for use in mapless, local visual navigation tasks. Our method learns to place visual attention at locations in latent image space which follow trajectories caused by vehicle control actions to later enhance predictive accuracy during planning. Our attention model is jointly optimized by the task-specific loss and additional trajectory-constraint loss, allowing adaptability yet encouraging a regularized structure for improved generalization and reliability. Importantly, visual attention is applied in latent feature map space instead of raw image space to promote efficient planning. We validated our model in visual navigation tasks of planning low turbulence, collision-free trajectories in off-road settings and hill climbing with locking differentials in the presence of slippery terrain. Experiments involved randomized procedural generated simulation and real-world environments. We found our method improved generalization and learning efficiency when compared to no-attention and self-attention alternatives.},
  archive   = {C_IROS},
  author    = {Stefan Wapnick and Travis Manderson and David Meger and Gregory Dudek},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636422},
  pages     = {460-467},
  title     = {Trajectory-constrained deep latent visual attention for improved local planning in presence of heterogeneous terrain},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Re-attention is all you need: Memory-efficient scene text
detection via re-attention on uncertain regions. <em>IROS</em>, 452–459.
(<a href="https://doi.org/10.1109/IROS51168.2021.9636510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Scene text detection plays an important role on vision-based robot navigation to many potential landmarks such as nameplates, information signs, floor button in the elevators. Recently, scene text detection with segmentation-based methods has been receiving more and more attention. The segmentation results can be used to efficiently predict scene text of various shapes, such as irregular text in most scene text images. However, two kinds of texts remain unsolved: 1) tiny and 2) blurry instances. Moreover, the annotations for tiny/blurry texts are usually ignored during training, while tiny/blurry texts can still offer visual auxiliaries for robots to understand the world. Therefore, in this paper, we propose a new approach to effectively detect both clear and blurry texts. Specifically, we propose a re-attention module without increasing the learnable parameters, which first predicts the region of texts as the candidate region and leverages the same network to detect the candidate region again for reducing the required memory. Moreover, to avoid the errors from the first detection propagating to the re-attended area, we propose a new fusion module that learns to integrate the results of the re-attended regions and the first prediction. Experimental results manifest that the proposed method outperforms state-of-the-art methods on four challenging datasets.},
  archive   = {C_IROS},
  author    = {Hsiang-Chun Chang and Hung-Jen Chen and Yu-Chia Shen and Hong-Han Shuai and Wen-Huang Cheng},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636510},
  pages     = {452-459},
  title     = {Re-attention is all you need: Memory-efficient scene text detection via re-attention on uncertain regions},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Motion field consensus with locality preservation: A
geometric confirmation strategy for loop closure detection.
<em>IROS</em>, 445–451. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Loop closure detection (LCD), which aims to deal with the drift emerging when robots travel around the route, plays a key role in a simultaneous localization and mapping system. Unlike most current methods which focus on seeking an appropriate representation of images, we propose a novel two-stage pipeline dominated by the estimation of spatial geometric relationship. When a query image occurs, we select semantically similar images based on the SuperPoint network and the aggregated selective match kernel in the first stage, and then conduct robust geometric confirmation to verify true loop-closing pairs in the second stage. Based on the potential property of motion field in the LCD scene, a robust feature matching algorithm, termed as motion field consensus with locality preservation (MFC-LP), is proposed. In particular, we exploit the smoothness prior to guide the learning of the motion field for an image pair in a reproducing kernel Hilbert space (RKHS). Meanwhile, to enhance the local relevance of motion vectors, we design a locality preservation mechanism thus making the learned motion field more accurate. Extensive experiments on several publicly available datasets reveal that MFC-LP has a good performance in the general feature matching task and the proposed pipeline outperforms the current state-of-the-art approaches in the LCD task.},
  archive   = {C_IROS},
  author    = {Kaining Zhang and Xingyu Jiang and Xiaoguang Mei and Huabing Zhou and Jiayi Ma},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636769},
  pages     = {445-451},
  title     = {Motion field consensus with locality preservation: A geometric confirmation strategy for loop closure detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scalable distributed planning for multi-robot, multi-target
tracking. <em>IROS</em>, 437–444. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In multi-robot multi-target tracking, robots coordinate to monitor groups of targets moving about an environment. We approach planning for such scenarios by formulating a receding-horizon, multi-robot sensing problem with a mutual information objective. Such problems are NP-Hard in general. Yet, our objective is submodular which enables certain greedy planners to guarantee constant-factor suboptimality. However, these greedy planners require robots to plan their actions in sequence, one robot at a time, so planning time is at least proportional to the number of robots. Solving these problems becomes intractable for large teams, even for distributed implementations. Our prior work proposed a distributed planner (RSP) which reduces this number of sequential steps to a constant, even for large numbers of robots, by allowing robots to plan in parallel while ignoring some of each others’ decisions. Although that analysis is not applicable to target tracking, we prove a similar guarantee, that RSP planning approaches performance guarantees for fully sequential planners, by employing a novel bound which takes advantage of the independence of target motions to quantify effective redundancy between robots’ observations and actions. Further, we present analysis that explicitly accounts for features of practical implementations including approximations to the objective and anytime planning. Simulation results—available via open source release—for target tracking with ranging sensors demonstrate that our planners consistently approach the performance of sequential planning (in terms of position uncertainty) given only 2–8 planning steps and for as many as 96 robots with a 24x reduction in the number of sequential steps in planning. Thus, this work makes planning for multi-robot target tracking tractable at much larger scales than before, for practical planners and general tracking problems.},
  archive   = {C_IROS},
  author    = {Micah Corah and Nathan Michael},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636729},
  pages     = {437-444},
  title     = {Scalable distributed planning for multi-robot, multi-target tracking},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deadlock prediction and recovery for distributed collision
avoidance with buffered voronoi cells. <em>IROS</em>, 429–436. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a distributed multi-robot collision avoidance algorithm based on the concept of Buffered Voronoi Cells (BVC). We propose a novel algorithm for avoiding deadlocks consisting of three stages: deadlock prediction, deadlock recovery, and deadlock recovery success prediction. Simple heuristics (such as the right-hand rule) are often used to avoid deadlocks. Such heuristics might reduce deadlock in simple configurations and sparsely populated environments, but they begin to fail in complex configurations and more densely populated environments. We evaluate the performance of our algorithm using an open-source web-based multi-robot simulation. The results show that while the proposed algorithm does not eliminate the occurrence of deadlocks, it drastically reduces their occurrence, and leads to a considerable improvement in performance, especially in high-density environments. We also validate the real-world performance of the proposed algorithm in live experiments.},
  archive   = {C_IROS},
  author    = {Mohammed Abdullhak and Andrew Vardy},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636609},
  pages     = {429-436},
  title     = {Deadlock prediction and recovery for distributed collision avoidance with buffered voronoi cells},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning connectivity for data distribution in robot teams.
<em>IROS</em>, 413–420. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many algorithms for control of multi-robot teams operate under the assumption that low-latency, global state information necessary to coordinate agent actions can readily be disseminated among the team. However, in harsh environments with no existing communication infrastructure, robots must form ad-hoc networks, forcing the team to operate in a distributed fashion. To overcome this challenge, we propose a task-agnostic, decentralized, low-latency method for data distribution in ad-hoc networks using Graph Neural Networks (GNN). Our approach enables multi-agent algorithms based on global state information to function by ensuring it is available at each robot. To do this, agents glean information about the topology of the network from packet transmissions and feed it to a GNN running locally which instructs the agent when and where to transmit the latest state information. We train the distributed GNN communication policies via reinforcement learning using the average Age of Information as the reward function and show that it improves training stability compared to task-specific reward functions. Our approach performs favorably compared to industry-standard methods for data distribution such as random flooding and round robin. We also show that the trained policies generalize to larger teams of both static and mobile agents.},
  archive   = {C_IROS},
  author    = {Ekaterina Tolstaya and Landon Butler and Daniel Mox and James Paulos and Vijay Kumar and Alejandro Ribeiro},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636873},
  pages     = {413-420},
  title     = {Learning connectivity for data distribution in robot teams},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robofleet: Open source communication and management for
fleets of autonomous robots. <em>IROS</em>, 406–412. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Long-term deployment of a fleet of mobile robots requires reliable and secure two-way communication channels between individual robots and remote human operators for supervision and tasking. Existing open-source solutions to this problem degrade in performance in challenging real-world situations such as intermittent and low-bandwidth connectivity, do not provide security control options, and can be computationally expensive on hardware-constrained mobile robot platforms. In this paper, we present Robofleet, a lightweight open-source system which provides inter-robot communication, remote monitoring, and remote tasking for a heterogenous fleet of ROS-enabled service-mobile robots that is designed with the practical goals of resilience to network variance and security control in mind.Robofleet supports multi-user, multi-robot communication via a central server. This architecture deduplicates network traffic between robots, significantly reducing overall network load when compared with native ROS communication. This server also functions as a single entrypoint into the system, enabling security control and user authentication. Individual robots run the lightweight Robofleet client, which is responsible for exchanging messages with the Robofleet server. It automatically adapts to adverse network conditions through backpressure monitoring as well as topic-level priority control, ensuring that safety-critical messages are successfully transmitted. Finally, the system includes a web-based visualization tool that can be run on any internet-connected, browser-enabled device to monitor and control the fleet.We compare Robofleet to existing methods of robotic communication, and demonstrate that it provides superior resilience to network variance while maintaining performance that exceeds that of widely-used systems.},
  archive   = {C_IROS},
  author    = {Kavan Singh Sikand and Logan Zartman and Sadegh Rabiee and Joydeep Biswas},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635830},
  pages     = {406-412},
  title     = {Robofleet: Open source communication and management for fleets of autonomous robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). OneVision: Centralized to distributed controller synthesis
with delay compensation. <em>IROS</em>, 398–405. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a new algorithm to simplify the controller development for distributed robotic systems subject to external observations, disturbances, and communication delays. Unlike prior approaches that propose specialized solutions to handling communication latency for specific robotic applications, our algorithm uses an arbitrary centralized controller as the specification and automatically generates distributed controllers with communication management and delay compensation. We formulate our goal as nonlinear optimal control— using a regret minimizing objective that measures how much the distributed agents behave differently from the delay-free centralized response—and solve for optimal actions w.r.t. local estimations of this objective using gradient-based optimization. We analyze our proposed algorithm’s behavior under a linear time-invariant special case and prove that the closed-loop dynamics satisfy a form of input-to-state stability w.r.t. unexpected disturbances and observations. Our experimental results on both simulated and real-world robotic tasks demonstrate the practical usefulness of our approach and show significant improvement over several baseline approaches.},
  archive   = {C_IROS},
  author    = {Jiayi Wei and Tongrui Li and Swarat Chaudhuri and Isil Dillig and Joydeep Biswas},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636164},
  pages     = {398-405},
  title     = {OneVision: Centralized to distributed controller synthesis with delay compensation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal scheduling and non-cooperative distributed model
predictive control for multiple robotic manipulators. <em>IROS</em>,
390–397. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Application of multiple robotic manipulators in a shared workspace is still restricted to repetitive tasks limiting their flexible deployment for production systems. Still, existing motion control algorithms cannot be performed online for arbitrary environments in case of multiple manipulators cooperating with each other. In this work we propose a scalable and real-time capable motion control algorithm based on non-cooperative distributed model predictive control. Furthermore, we propose an optimal scheduling algorithm, which provides optimal setpoints to each robot’s motion controller that prevents possible deadlocks beforehand. We validate our approach on a simulative setup of four robotic manipulators for multiple pick and place scenarios.},
  archive   = {C_IROS},
  author    = {Nigora Gafur and Vassilios Yfantis and Martin Ruskowski},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636118},
  pages     = {390-397},
  title     = {Optimal scheduling and non-cooperative distributed model predictive control for multiple robotic manipulators},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatial constraint generation for motion planning in dynamic
environments. <em>IROS</em>, 384–389. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel method to generate spatial constraints for motion planning in dynamic environments. Motion planning methods for autonomous driving and mobile robots typically need to rely on the spatial constraints imposed by a map-based global planner to generate a collision-free trajectory. These methods may fail without an offline map or where the map is invalid due to dynamic changes in the environment such as road obstruction, construction, and traffic congestion. To address this problem, triangulation-based methods can be used to obtain a spatial constraint. However, the existing methods fall short when dealing with dynamic environments and may lead the motion planner to an unrecoverable state. In this paper, we propose a new method to generate a sequence of channels across different triangulation mesh topologies to serve as the spatial constraints. This can be applied to motion planning of autonomous vehicles or robots in cluttered, unstructured environments. The proposed method is evaluated and compared with other triangulation-based methods in synthetic and complex scenarios collected from a real-world autonomous driving dataset. We have shown that the proposed method results in a more stable, long-term plan with a higher task completion rate, faster arrival time, a higher rate of successful plans, and fewer collisions compared to existing methods.},
  archive   = {C_IROS},
  author    = {Han Hu and Peyman Yadmellat},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636067},
  pages     = {384-389},
  title     = {Spatial constraint generation for motion planning in dynamic environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Disruption-limited planning for robot navigation in dynamic
environments. <em>IROS</em>, 377–383. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Path planning in the presence of dynamic obstacles is a fundamental problem in robotics with widespread applications. A typical approach to such problems is that a robot predicts the trajectories of dynamic obstacles, and plans its path while avoiding them. Such a formulation becomes limiting though for scenarios where an agent cannot complete its task efficiently, without disrupting the movement of dynamic obstacles. For example, when merging in heavy traffic or navigating through crowded corridors. In this paper, we propose a paradigm for planning in dynamic environments, called Disruption-Limited Planning (DLP), that allows a robot to disrupt the motions of dynamic obstacles in order to accomplish its task. DLP builds on the premise that while a robot may have to disrupt others’ trajectories to achieve its goals, it should try to limit the disruption. DLP assumes that it can estimate others’ response to its own actions/plans, and plans its own path while ensuring that no other agents’ disrupted trajectory cost gets worse than w-times their initial trajectory costs. While our formulation is motivated by the Stackelberg competitions, we show that DLP can be both more expressive and computationally more efficient compared to a Stackelberg planner. We present DLP paradigm, develop its efficient implementation based on A*, analyze its theoretical properties, and apply it to multiple planning in dynamic environment problems, including x,y,time planning, planning for self-driving, and planning for arm manipulation. We compare DLP with purely altruistic, purely egocentric, and optimal Stackelberg planners, demonstrating the efficacy of DLP over these alternatives.},
  archive   = {C_IROS},
  author    = {Sandip Aine and Yash Oza and Maxim Likhachev},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636274},
  pages     = {377-383},
  title     = {Disruption-limited planning for robot navigation in dynamic environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploring learning for intercepting projectiles with a
robot-held stick. <em>IROS</em>, 369–376. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For many tasks, including table tennis, catching, and sword fighting, a critical step is intercepting the incoming object with a robot arm or held tool. Solutions to robot arm interception via learning, specifically reinforcement learning (RL), have become prevalent, as they provide robust solutions to the robot arm interception problem, even for high degree of freedom robotic systems. Despite numerous solutions, there has been little exploration into the factors of learning that impact solution quality. Thus, there is little insight into what problem features lead to better learning success. In this paper, we explore the parameters that impact solution quality. We find that link position observations outperform joint angle observations in terms of learning speed, performance, ability to utilize more than one frame of observation, and generalization to situations not trained for. These results are immediately applicable to RL for robot arm interception tasks.},
  archive   = {C_IROS},
  author    = {John E. G. Baxter and Torin Adamson and Satomi Sugaya and Lydia Tapia},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635993},
  pages     = {369-376},
  title     = {Exploring learning for intercepting projectiles with a robot-held stick},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Closed-loop fast marching tree (CL-FMT*) with application to
helicopter landing trajectory planning. <em>IROS</em>, 346–351. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion planning for complex dynamic systems such as helicopters is a challenging problem due to non-holonomic and nonlinear differential constraints. Approaches for optimal kinodynamics motion planning have only been demonstrated for simple dynamic systems such as Dubins car or linear systems. In this paper we present Closed-loop FMT* (CL-FMT*) which is an extension of FMT* [7] that uses closed-loop prediction. With closed-loop prediction the computationally intense two-point-boundary-value steering procedure can be avoided. We describe the effectiveness of CL-FMT* compared to CL-RRT*. We then demonstrate the use of CL-FMT* for helicopter landing trajectory planning with realistic dynamics constrains.},
  archive   = {C_IROS},
  author    = {Navid Dadkhah Tehrani and Igor Cherepinsky and Sean Carlson},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636509},
  pages     = {346-351},
  title     = {Closed-loop fast marching tree (CL-FMT*) with application to helicopter landing trajectory planning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). B-spline path planner for safe navigation of mobile robots.
<em>IROS</em>, 339–345. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a 2D path planning algorithm in a non-convex workspace defined as a sequence of connected convex polytopes. The reference path is parameterized as a B-spline curve, which is guaranteed to entirely remain within the workspace by exploiting the local convexity property and by formulating linear constraints on the control points of the B-spline. The novelties of the paper lie in the use of the equivalent Bézier representation of the B-spline curve, which significantly reduces the conservatism in the local convexity bound and in the integration of these constraints into a convex quadratic optimization problem, which minimizes the curve length. The algorithm is successfully validated in both simulations and experiments, by providing obstacle-free reference paths on real occupancy grid maps obtained from the laser scan data of a mobile robot platform.},
  archive   = {C_IROS},
  author    = {Ngoc Thinh Nguyen and Lars Schilling and Michael Sebastian Angern and Heiko Hamann and Floris Ernst and Georg Schildbach},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636612},
  pages     = {339-345},
  title     = {B-spline path planner for safe navigation of mobile robots},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Radar visual inertial odometry and radar thermal inertial
odometry: Robust navigation even in challenging visual conditions.
<em>IROS</em>, 331–338. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose to fuse radar measurements with Visual Inertial Odometry (RVIO) or Thermal Inertial Odometry (RTIO). FMCW radar sensor data enables to estimate the 3D ego velocity independent of the visual conditions. Fusion with VIO or TIO heavily improves the robustness in challenging conditions such as darkness, direct sunlight or fog.Specifically, we propose RRxIO: An extension to the state of the art filter based VIO framework Robust Visual Inertial Odometry (ROVIO) to fuse radar sensor data. Due to the drift free 3D radar ego velocity estimates, scale errors are reduced and only a small number of features needs to be tracked to achieve good results. This yields faster runtimes while outperforming state of the art VIO frameworks regarding accuracy. RVIO is able to bridge phases of degraded or even no visual features resulting in a low cost system even for poor visual conditions. RTIO is robust even in environments with small temperature gradients and bridges phases of no thermal images caused by Non-Uniformity Corrections (NUCs).We evaluated our system with various experiments in different environments and visual conditions. Comparison to other state of the art dual domain approaches including radar-inertial, visual-inertial and thermal-inertial proves superior performance of our approach regarding accuracy and processing time.},
  archive   = {C_IROS},
  author    = {Christopher Doer and Gert F. Trommer},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636799},
  pages     = {331-338},
  title     = {Radar visual inertial odometry and radar thermal inertial odometry: Robust navigation even in challenging visual conditions},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Continuous-time radar-inertial odometry for automotive
radars. <em>IROS</em>, 323–330. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an approach for radar-inertial odometry which uses a continuous-time framework to fuse measurements from multiple automotive radars and an inertial measurement unit (IMU). Adverse weather conditions do not have a significant impact on the operating performance of radar sensors unlike that of camera and LiDAR sensors. Radar’s robustness in such conditions and the increasing prevalence of radars on passenger vehicles motivate us to look at the use of radar for ego-motion estimation. A continuous-time trajectory representation is applied not only as a framework to enable heterogeneous and asynchronous multi-sensor fusion, but also, to facilitate efficient optimization by being able to compute poses and their derivatives in closed-form and at any given time along the trajectory. We compare our continuous-time estimates to those from a discrete-time radar-inertial odometry approach and show that our continuous-time method outperforms the discrete-time method. To the best of our knowledge, this is the first time a continuous-time framework has been applied to radar-inertial odometry.},
  archive   = {C_IROS},
  author    = {Yin Zhi Ng and Benjamin Choi and Robby Tan and Lionel Heng},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636014},
  pages     = {323-330},
  title     = {Continuous-time radar-inertial odometry for automotive radars},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-sensor fusion incorporating adaptive transformation
for reconfigurable pavement sweeping robot. <em>IROS</em>, 300–306. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An efficient sensors fusion framework in an autonomous robot is necessary for various functions like object detection and perception enhancement. Multi-sensor calibration techniques are used to fuse multiple static sensors into a single frame of reference. However, for reconfigurable robots, sensors can change pose during reconfiguration need a robust adaptive sensor fusion to account for the relative change in sensor position and orientation. We propose an adaptive sensor fusion framework that can be implemented on any reconfiguration robot to adjust calibration parameters. Our paper formulated an adaptive sensor fusion method, implemented it in real-time on an autonomous reconfigurable pavement sweeping robot called Panthera, and demonstrated qualitatively the accuracy of the proposed sensor fusion framework for environment perception during robot reconfiguring on the pavement.},
  archive   = {C_IROS},
  author    = {A.P. Povendhan and Lim Yi and A. A. Hayat and Anh Vu Le and K. L. J. Kai and B. Ramalingam and M. R. Elara},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636464},
  pages     = {300-306},
  title     = {Multi-sensor fusion incorporating adaptive transformation for reconfigurable pavement sweeping robot},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reactive visual odometry scheduling based on noise analysis
using an adaptive extended kalman filter. <em>IROS</em>, 294–299. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A new strategy is proposed for scheduling Visual Odometry (VO) measurements for wheeled ground vehicles. Rather than having a fixed interval or distance between image acquisitions, we propose to trigger VO based on covariances from an Adaptive Extended Kalman Filter. The adopted model uses process noise to drive wheel slip estimation, which, when correctly identified, can be used with Wheel Odometry to provide frequent position estimates. When more dynamic terrain is detected, more VO measurements are scheduled to maintain localization accuracy. On the other hand, when the terrain is stable, VO usage is limited. The system is validated in a simple one-dimensional case using data captured during field trials using a representative rover. The results are promising as trajectories that were subjected to large errors are corrected.},
  archive   = {C_IROS},
  author    = {Mateusz Tomasz Malinowski and Arthur Richards and Mark Woods},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636214},
  pages     = {294-299},
  title     = {Reactive visual odometry scheduling based on noise analysis using an adaptive extended kalman filter},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Lvio-fusion: A self-adaptive multi-sensor fusion SLAM
framework using actor-critic method. <em>IROS</em>, 286–293. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {State estimation with sensors is essential for mobile robots. Due to different performance of sensors in different environments, how to fuse measurements of various sensors is a problem. In this paper, we propose a tightly coupled multi-sensor fusion framework, Lvio-Fusion, which fuses stereo camera, Lidar, IMU, and GPS based on the graph optimization. Especially for urban traffic scenes, we introduce a segmented global pose graph optimization with GPS and loop-closure, which can eliminate accumulated drifts. Additionally, we creatively use a actor-critic method in reinforcement learning to adaptively adjust sensors’ weight. After training, actor-critic agent can provide the system better and dynamic sensors’ weight. We evaluate the performance of our system on public datasets and compare it with other state-of-the-art methods, which shows that the proposed method achieves high estimation accuracy and robustness to various environments. And our implementations are open source and highly scalable.},
  archive   = {C_IROS},
  author    = {Yupeng Jia and Haiyong Luo and Fang Zhao and Guanlin Jiang and Yuhang Li and Jiaquan Yan and Zhuqing Jiang and Zitian Wang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635905},
  pages     = {286-293},
  title     = {Lvio-fusion: A self-adaptive multi-sensor fusion SLAM framework using actor-critic method},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised learning of depth estimation and visual
odometry for sparse light field cameras. <em>IROS</em>, 278–285. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While an exciting diversity of new imaging devices is emerging that could dramatically improve robotic perception, the challenges of calibrating and interpreting these cameras have limited their uptake in the robotics community. In this work we generalise techniques from unsupervised learning to allow a robot to autonomously interpret new kinds of cameras. We consider emerging sparse light field (LF) cameras, which capture a subset of the 4D LF function describing the set of light rays passing through a plane. We introduce a generalised encoding of sparse LFs that allows unsupervised learning of odometry and depth. We demonstrate the proposed approach outperforming monocular, stereo and conventional techniques for dealing with 4D imagery, yielding more accurate odometry and depth maps and delivering these with metric scale. We anticipate our technique to generalise to a broad class of LF and sparse LF cameras, and to enable unsupervised recalibration for coping with shifts in camera behaviour over the lifetime of a robot. This work represents a first step toward streamlining the integration of new kinds of imaging devices in robotics applications.},
  archive   = {C_IROS},
  author    = {S. Tejaswi Digumarti and Joseph Daniel and Ahalya Ravendran and Ryan Griffiths and Donald G. Dansereau},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636570},
  pages     = {278-285},
  title     = {Unsupervised learning of depth estimation and visual odometry for sparse light field cameras},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to drop points for LiDAR scan synthesis.
<em>IROS</em>, 222–229. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D laser scanning by LiDAR sensors plays an important role for mobile robots to understand their surroundings. Nevertheless, not all systems have high resolution and accuracy due to hardware limitations, weather conditions, and so on. Generative modeling of LiDAR data as scene priors is one of the promising solutions to compensate for unreliable or incomplete observations. In this paper, we propose a novel generative model for learning LiDAR data based on generative adversarial networks. As in the related studies, we process LiDAR data as a compact yet lossless representation, a cylindrical depth map. However, despite the smoothness of real-world objects, many points on the depth map are dropped out through the laser measurement, which causes learning difficulty on generative models. To circumvent this issue, we introduce measurement uncertainty into the generation process, which allows the model to learn a disentangled representation of the underlying shape and the dropout noises from a collection of real LiDAR data. To simulate the lossy measurement, we adopt a differentiable sampling framework to drop points based on the learned uncertainty. We demonstrate the effectiveness of our method on synthesis and reconstruction tasks using two datasets. We further showcase potential applications by restoring LiDAR data with various types of corruption.},
  archive   = {C_IROS},
  author    = {Kazuto Nakashima and Ryo Kurazume},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636747},
  pages     = {222-229},
  title     = {Learning to drop points for LiDAR scan synthesis},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-supervised disentangled representation learning for
third-person imitation learning. <em>IROS</em>, 214–221. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans learn to imitate by observing others. However, robot imitation learning generally requires expert demonstrations in the first-person view (FPV). Collecting such FPV videos for every robot could be very expensive.Third-person imitation learning (TPIL) is the concept of learning action policies by observing other agents in a third-person view (TPV), similar to what humans do. This ultimately allows utilizing human and robot demonstration videos in TPV from many different data sources, for the policy learning. In this paper, we present a TPIL approach for robot tasks with egomotion. Although many robot tasks with ground/aerial mobility often involve actions with camera egomotion, study on TPIL for such tasks has been limited. Here, FPV and TPV observations are visually very different; FPV shows egomotion while the agent appearance is only observable in TPV. To enable better state learning for TPIL, we propose our disentangled representation learning method. We use a dual auto-encoder structure plus representation permutation loss and time-contrastive loss to ensure the state and viewpoint representations are well disentangled. Our experiments show the effectiveness of our approach.},
  archive   = {C_IROS},
  author    = {Jinghuan Shang and Michael S. Ryoo},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636363},
  pages     = {214-221},
  title     = {Self-supervised disentangled representation learning for third-person imitation learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SSTN: Self-supervised domain adaptation thermal object
detection for autonomous driving. <em>IROS</em>, 206–213. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The perception of the environment plays a decisive role in the safe and secure operation of autonomous vehicles. The perception of the surrounding is way similar to human vision. The human’s brain perceives the environment by utilizing different sensory channels and develop a view-invariant representation model. In this context, different exteroceptive sensors like cameras, Lidar, are deployed on the autonomous vehicle to perceive the environment. These sensors have illustrated their benefit in the visible spectrum domain yet in the adverse weather conditions; for instance, they have limited operational capability at night, leading to fatal accidents. This work explores thermal object detection to model a view-invariant model representation by employing the self-supervised contrastive learning approach. We have proposed a deep neural network Self Supervised Thermal Network (SSTN) for learning the feature embedding to maximize the information between visible and infrared spectrum domain by contrastive learning. Later, these learned feature representations are employed for thermal object detection using a multi-scale encoder-decoder transformer network. The proposed method is extensively evaluated on the two publicly available datasets: the FLIR-ADAS dataset and the KAIST Multi-Spectral dataset. The experimental results illustrate the efficacy of the proposed method.},
  archive   = {C_IROS},
  author    = {Farzeen Munir and Shoaib Azam and Moongu Jeon},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636353},
  pages     = {206-213},
  title     = {SSTN: Self-supervised domain adaptation thermal object detection for autonomous driving},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Acceleration of actor-critic deep reinforcement learning for
visual grasping by state representation learning based on a preprocessed
input image. <em>IROS</em>, 198–205. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For robotic grasping tasks with diverse target objects, some deep learning-based methods have achieved state-of-the-art results using direct visual input. In contrast, actor-critic deep reinforcement learning (RL) methods typically perform very poorly when applied to grasp diverse objects, especially when learning from raw images and sparse rewards. To render these RL techniques feasible for vision-based grasping tasks, we used state representation learning (SRL), in which we encode essential information for subsequent use in RL. However, typical representation learning procedures are unsuitable for extracting pertinent information for learning grasping skills owing to the high complexity of visual inputs for representation learning, in which a robot attempts to grasp a target object. We found that the proposed preprocessed input image is the key to capturing effectively a compact representation. This enables deep RL to learn robotic grasping skills from highly varied and diverse visual inputs. Further, we demonstrate the effectiveness of the proposed approach with varying levels of preprocessing in a realistic simulated environment. We also describe how the resulting model can be transferred to a real-world robot and also demonstrate a 68\% success rate on real-world grasp attempts.},
  archive   = {C_IROS},
  author    = {Taewon Kim and Yeseong Park and Youngbin Park and Sang Hyoung Lee and Il Hong Suh},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635931},
  pages     = {198-205},
  title     = {Acceleration of actor-critic deep reinforcement learning for visual grasping by state representation learning based on a preprocessed input image},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Low dimensional state representation learning with robotics
priors in continuous action spaces. <em>IROS</em>, 190–197. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning algorithms have proven to be capable of solving complicated robotics tasks in an end-to-end fashion without any need for hand-crafted features or policies. Especially in the context of robotics, in which the cost of real-world data is usually extremely high, Reinforcement Learning solutions achieving high sample efficiency are needed. In this paper, we propose a framework combining the learning of a low-dimensional state representation, from high-dimensional observations coming from the robot’s raw sensory readings, with the learning of the optimal policy, given the learned state representation. We evaluate our framework in the context of mobile robot navigation in the case of continuous state and action spaces. Moreover, we study the problem of transferring what learned in the simulated virtual environment to the real robot without further retraining using real-world data in the presence of visual and depth distractors, such as lighting changes and moving obstacles. A video of our experiments can be found at: https://youtu.be/rUdGPKr2Wuo.},
  archive   = {C_IROS},
  author    = {Nicolò Botteghi and Khaled Alaa and Mannes Poel and Beril Sirmacek and Christoph Brune and Abeje Mersha and Stefano Stramigioli},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635936},
  pages     = {190-197},
  title     = {Low dimensional state representation learning with robotics priors in continuous action spaces},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DeepKoCo: Efficient latent planning with a task-relevant
koopman representation. <em>IROS</em>, 183–189. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents DeepKoCo, a novel modelbased agent that learns a latent Koopman representation from images. This representation allows DeepKoCo to plan efficiently using linear control methods, such as linear model predictive control. Compared to traditional agents, DeepKoCo learns taskrelevant dynamics, thanks to the use of a tailored lossy autoencoder network that allows DeepKoCo to learn latent dynamics that reconstruct and predict only observed costs, rather than all observed dynamics. As our results show, DeepKoCo achieves a similar final performance as traditional model-free methods on complex control tasks, while being considerably more robust to distractor dynamics, making the proposed agent more amenable for real-life applications.},
  archive   = {C_IROS},
  author    = {Bas van der Heijden and Laura Ferranti and Jens Kober and Robert Babuška},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636408},
  pages     = {183-189},
  title     = {DeepKoCo: Efficient latent planning with a task-relevant koopman representation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). COCOI: Contact-aware online context inference for
generalizable non-planar pushing. <em>IROS</em>, 176–182. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {General contact-rich manipulation problems are long-standing challenges in robotics due to the difficulty of understanding complicated contact physics. Deep reinforcement learning (RL) has shown great potential in solving robot manipulation tasks. However, existing RL policies have limited adaptability to environments with diverse dynamics properties, which is pivotal in solving many contact-rich manipulation tasks. In this work, we propose Contact-aware Online COntext Inference (COCOI), a deep RL method that encodes a context embedding of dynamics properties online using contact-rich interactions. We sample sensor data using a novel contact-aware strategy and formulate an interpretable dynamics transition module. We study this method based on a novel and challenging non-planar pushing task, where the robot uses a monocular camera image and wrist force torque sensor reading to push an object to a goal location while keeping it upright. We run extensive experiments to demonstrate the capability of COCOI in a wide range of settings and dynamics properties in simulation, and also in a sim-to-real transfer scenario on a real robot (Webpage: https://context-inference.github.io/).},
  archive   = {C_IROS},
  author    = {Zhuo Xu and Wenhao Yu and Alexander Herzog and Wenlong Lu and Chuyuan Fu and Masayoshi Tomizuka and Yunfei Bai and C. Karen Liu and Daniel Ho},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636836},
  pages     = {176-182},
  title     = {COCOI: Contact-aware online context inference for generalizable non-planar pushing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Monitoring and diagnosability of perception systems.
<em>IROS</em>, 168–175. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Perception is a critical component of high-integrity applications of robotics and autonomous systems, such as self-driving vehicles. In these applications, failure of perception systems may put human life at risk, and a broad adoption of these technologies requires the development of methodologies to guarantee and monitor safe operation. Despite the paramount importance of perception systems, currently there is no formal approach for system-level monitoring. In this work, we propose a mathematical model for runtime monitoring and fault detection and identification in perception systems. Towards this goal, we draw connections with the literature on diagnosability in multiprocessor systems, and generalize it to account for modules with heterogeneous outputs that interact over time. The resulting temporal diagnostic graphs (i) provide a framework to reason over the consistency of perception outputs –across modules and over time– thus enabling fault detection, (ii) allow us to establish formal guarantees on the maximum number of faults that can be uniquely identified in a given perception system, and (iii) enable the design of efficient algorithms for fault identification. We demonstrate our monitoring system, dubbed PerSyS, in realistic simulations using the LGSVL self-driving simulator and the Apollo Auto autonomy software stack, and show that PerSyS is able to detect failures in challenging scenarios (including scenarios that have caused self-driving car accidents in recent years), and is able to correctly identify faults while entailing a minimal computation overhead (&lt; 5 ms on a single-core CPU).},
  archive   = {C_IROS},
  author    = {Pasquale Antonante and David I. Spivak and Luca Carlone},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636497},
  pages     = {168-175},
  title     = {Monitoring and diagnosability of perception systems},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Road graphical neural networks for autonomous roundabout
driving. <em>IROS</em>, 162–167. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel autonomous driving frame-work that leverages graph-based features of roads, such as road positions and connections. The proposed method is divided into two parts: a low-level controller which follows the trajectory calculated by a graph-based path planner, and a high-level controller which determines the speed of the vehicle to follow the traffic flow. The high-level controller uses a road graphical neural network (Road-GNN), which encodes a road graph into latent features to perceive the surrounding environment. We use a 3D driving simulator to test the performance of Road-GNN, which is implemented based on the satellite image data of 30 roundabout intersections. To show that the proposed method can be generalized to various road environments, the proposed method is tested using roundabouts which are different from the training set. In the experiment, the proposed method successfully trains the agent and drives an ego-vehicle through various roundabout environments. The results show that the graph-based method is effective for autonomous driving.},
  archive   = {C_IROS},
  author    = {Timothy Ha and Gunmin Lee and Dohyeong Kim and Songhwai Oh},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636411},
  pages     = {162-167},
  title     = {Road graphical neural networks for autonomous roundabout driving},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust policy search for an agile ground vehicle under
perception uncertainty. <em>IROS</em>, 154–161. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning robust policies for robotic systems operating in presence of uncertainty is a challenging task. For safe navigation, in addition to the natural stochasticity of the environment and vehicle dynamics, the perception uncertainty associated with dynamic entities, e.g. pedestrians, must be accounted for during motion planning. To this end, we construct an algorithm with built-in robustness to uncertainty by directly minimizing an upper confidence bound on the expected cost of trajectories instead of employing a standard approach based on minimizing the expected cost itself. Perception uncertainty is incorporated into the policy search framework by predicting each pedestrian’s intent belief and propagating their state distribution in time using closed-loop goal-directed dynamics. We train the policy in simulation and show that it could be transferred to an agile ground vehicle for successful autonomous robot navigation in presence of pedestrians with perception uncertainty. We further show the superior performance of this policy over a policy that does not consider pedestrian intent and perception uncertainty.},
  archive   = {C_IROS},
  author    = {Shahriar Sefati and Subhransu Mishra and Matthew Sheckells and Kapil D. Katyal and Jin Bai and Gregory D. Hager and Marin Kobilarov},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636552},
  pages     = {154-161},
  title     = {Robust policy search for an agile ground vehicle under perception uncertainty},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). From agile ground to aerial navigation: Learning from
learned hallucination. <em>IROS</em>, 148–153. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a self-supervised Learning from Learned Hallucination (LfLH) method to learn fast and reactive motion planners for ground and aerial robots to navigate through highly constrained environments. The recent Learning from Hallucination (LfH) paradigm for autonomous navigation executes motion plans by random exploration in completely safe obstacle-free spaces, uses hand-crafted hallucination techniques to add imaginary obstacles to the robot’s perception, and then learns motion planners to navigate in realistic, highly-constrained, dangerous spaces. However, current hand-crafted hallucination techniques need to be tailored for specific robot types (e.g., a differential drive ground vehicle), and use approximations heavily dependent on certain assumptions (e.g., a short planning horizon). In this work, instead of manually designing hallucination functions, LfLH learns to hallucinate obstacle configurations, where the motion plans from random exploration in open space are optimal, in a self-supervised manner. LfLH is robust to different robot types and does not make assumptions about the planning horizon. Evaluated in both simulated and physical environments with a ground and an aerial robot, LfLH outperforms or performs comparably to previous hallucination approaches, along with sampling- and optimization-based classical methods.},
  archive   = {C_IROS},
  author    = {Zizhao Wang and Xuesu Xiao and Alexander J Nettekoven and Kadhiravan Umasankar and Anika Singh and Sriram Bommakanti and Ufuk Topcu and Peter Stone},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636402},
  pages     = {148-153},
  title     = {From agile ground to aerial navigation: Learning from learned hallucination},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Latent attention augmentation for robust autonomous driving
policies. <em>IROS</em>, 130–136. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Model-free reinforcement learning has become a viable approach for vision-based robot control. However, sample complexity and adaptability to domain shifts remain persistent challenges when operating in high-dimensional observation spaces (images, LiDAR), such as those that are involved in autonomous driving. In this paper, we propose a flexible framework by which a policy’s observations are augmented with robust attention representations in the latent space to guide the agent’s attention during training. Our method encodes local and global descriptors of the augmented state representations into a compact latent vector, and scene dynamics are approximated by a recurrent network that processes the latent vectors in sequence. We outline two approaches for constructing attention maps; a supervised pipeline leveraging semantic segmentation networks, and an unsupervised pipeline relying only on classical image processing techniques. We conduct our experiments in simulation and test the learned policy against varying seasonal effects and weather conditions. Our design decisions are supported in a series of ablation studies. The results demonstrate that our state augmentation method both improves learning efficiency and encourages robust domain adaptation when compared to common end-to-end frameworks and methods that learn directly from intermediate representations.},
  archive   = {C_IROS},
  author    = {Ran Cheng and Christopher Agia and Florian Shkurti and David Meger and Gregory Dudek},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636449},
  pages     = {130-136},
  title     = {Latent attention augmentation for robust autonomous driving policies},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gaussian process-based interpretable runtime adaptation for
safe autonomous systems operations in unstructured environments.
<em>IROS</em>, 123–129. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous vehicles may not behave as expected when subject to environmental disturbances. For instance, control commands suitable for driving on dry, paved roads may lead to unsafe conditions and undesired deviations when on slippery dirt or icy roads. Furthermore, it becomes increasingly important to offer human-understandable explanations of autonomous robots’ actions – especially when they operate in a space shared with humans such as public roads. In this work, we present an interpretable, risk-aware online adaptation approach that leverages a combination of Gaussian process regression and decision tree theory. Our approach predicts the system state in unknown environments using pre-trained models, which are continuously refined during runtime using a fast lookup table-based procedure. Every time the originally intended behavior is predicted to cause an unsafe state, our method computes a set of safe behaviors under the current model uncertainty and risk. The most suitable behavior is subsequently selected depending on mission requirements and a human-understandable explanation about the system’s decision-making is delivered. The proposed framework is applied to unmanned ground vehicles and validated in simulations, in which we demonstrate a successful safety-critical cargo delivery through a priori unknown, rough and slippery terrain.},
  archive   = {C_IROS},
  author    = {Christian Gall and Nicola Bezzo},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636490},
  pages     = {123-129},
  title     = {Gaussian process-based interpretable runtime adaptation for safe autonomous systems operations in unstructured environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Obstacle avoidance onboard MAVs using a FMCW radar.
<em>IROS</em>, 117–122. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Micro Air Vehicles (MAVs) are increasingly being used for complex or hazardous tasks in enclosed and cluttered environments such as surveillance or search and rescue. With this comes the necessity for sensors that can operate in poor visibility conditions to facilitate with navigation and avoidance of objects or people. Radar sensors in particular can provide more robust sensing of the environment when traditional sensors such as cameras fail in the presence of dust, fog or smoke. While extensively used in autonomous driving, miniature FMCW radars on MAVs have been relatively unexplored. This study aims to investigate to what extent this sensor is of use in these environments by employing traditional signal processing such as multi-target tracking and velocity obstacles. The viability of the solution is evaluated with an implementation on board a MAV by running trial tests in an indoor environment containing obstacles and by comparison with a human pilot, demonstrating the potential for the sensor to provide a more robust sense and avoid function in fully autonomous MAVs.},
  archive   = {C_IROS},
  author    = {Nikhil Wessendorp and Raoul Dinaux and Julien Dupeyroux and Guido C. H. E. de Croon},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635901},
  pages     = {117-122},
  title     = {Obstacle avoidance onboard MAVs using a FMCW radar},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-calibrated dense 3D sensor using multiple cross
line-lasers based on light sectioning method and visual odometry.
<em>IROS</em>, 94–100. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Among various 3D capturing systems, since the system with line lasers based on the light sectioning method is simple and accurate, it has widely attracted many developers and used for many purposes. In addition, there is no need to synchronize the camera and the laser and also the configuration of the camera and the lasers is flexible, and thus, the system can be used for extreme conditions, such as underwater. There are two open problems for the system. The first problem is a low density of the 3D shape obtained from a single image, i.e., just several curves. The second problem is the accuracy of line detection in the wild. In this paper, we propose a self-calibration method using visual odometry (VO) to bundle a large number of frames to increase the density to solve the first problem. We also propose a robust line detection algorithm using CNN to solve the second problem. Comparative experiments prove the effectiveness of our proposed method. In addition, the system was tested in the extreme condition for demonstration.},
  archive   = {C_IROS},
  author    = {Genki Nagamatsu and Jun Takamatsu and Takafumi Iwaguchi and Diego Thomas and Hiroshi Kawasaki},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636505},
  pages     = {94-100},
  title     = {Self-calibrated dense 3D sensor using multiple cross line-lasers based on light sectioning method and visual odometry},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning state-dependent sensor measurement models with
limited sensor measurements. <em>IROS</em>, 86–93. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a two-stage transfer learning method for training state-dependent sensor measurement models (SDSMMs) with limited sensor data. This method can alleviate collecting sizeable sensor and ground truth data to learn accurate sensor models, especially when we must learn many sensor models (for example, a fleet of autonomous cars, drones, or warehouse robots). In the first stage, we use prior knowledge of the sensor (such as a physical model) to generate a sizeable artificial dataset. Then the artificial dataset is used to pre-train an SDSMM. The second stage fine-tunes the pre-trained SDSMM using a &quot;small&quot; number of data collected by our target real sensor. To our knowledge, we are the first to learn measurement distributions using data generated from a physical model and data from a real sensor. We evaluated our proposed method using the Extended Kalman Particle Filter and a real-world localization dataset collected by several robots. Compared to the prior method, the proposed method achieved comparable performance with as little as ~19\% of the real training data.},
  archive   = {C_IROS},
  author    = {Troi Williams and Yu Sun},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636407},
  pages     = {86-93},
  title     = {Learning state-dependent sensor measurement models with limited sensor measurements},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the descriptive power of LiDAR intensity images for
segment-based loop closing in 3-d SLAM. <em>IROS</em>, 79–85. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose an extension to the segment-based global localization method for LiDAR SLAM using descriptors learned considering the visual context of the segments. A new architecture of the deep neural network is presented that learns the visual context acquired from synthetic LiDAR intensity images. This approach allows a single multi-beam LiDAR to produce rich and highly descriptive location signatures. The method is tested on two public datasets, demonstrating an improved descriptiveness of the new descriptors, and more reliable loop closure detection in SLAM. Attention analysis of the network is used to show the importance of focusing on the broader context rather than only on the 3-D segment.},
  archive   = {C_IROS},
  author    = {Jan Wietrzykowski and Piotr Skrzypczyński},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636698},
  pages     = {79-85},
  title     = {On the descriptive power of LiDAR intensity images for segment-based loop closing in 3-D SLAM},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint depth and normal estimation from real-world
time-of-flight raw data. <em>IROS</em>, 71–78. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel approach to joint depth and normal estimation for time-of-flight (ToF) sensors. Our model learns to predict the high-quality depth and normal maps jointly from ToF raw sensor data. To achieve this, we meticulously constructed the first large-scale dataset (named ToF-100) with paired raw ToF data and ground-truth high-resolution depth maps provided by an industrial depth camera. In addition, we also design a simple but effective framework for joint depth and normal estimation, applying a robust Chamfer loss via jittering to improve the performance of our model. Our experiments demonstrate that our proposed method can efficiently reconstruct high-resolution depth and normal maps and significantly outperforms state-of-the-art approaches.},
  archive   = {C_IROS},
  author    = {Rongrong Gao and Na Fan and Changlin Li and Wentao Liu and Qifeng Chen},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636508},
  pages     = {71-78},
  title     = {Joint depth and normal estimation from real-world time-of-flight raw data},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time monocular human depth estimation and segmentation
on embedded systems. <em>IROS</em>, 55–62. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Estimating a scene’s depth to achieve collision avoidance against moving pedestrians is a crucial and fundamental problem in the robotic field. This paper proposes a novel, low complexity network architecture for fast and accurate human depth estimation and segmentation in indoor environments, aiming to applications for resource-constrained platforms (including battery-powered aerial, micro-aerial, and ground vehicles) with a monocular camera being the primary perception module. Following the encoder-decoder structure, the proposed framework consists of two branches, one for depth prediction and another for semantic segmentation. Moreover, network structure optimization is employed to improve its forward inference speed. Exhaustive experiments on three self-generated datasets prove our pipeline’s capability to execute in real-time, achieving higher frame rates than contemporary state-of-the-art frameworks (114.6 frames per second on an NVIDIA Jetson Nano GPU with TensorRT) while maintaining comparable accuracy.},
  archive   = {C_IROS},
  author    = {Shan An and Fangru Zhou and Mei Yang and Haogang Zhu and Changhong Fu and Konstantinos A. Tsintotas},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636518},
  pages     = {55-62},
  title     = {Real-time monocular human depth estimation and segmentation on embedded systems},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CORSAIR: Convolutional object retrieval and symmetry-AIded
registration. <em>IROS</em>, 47–54. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper considers online object-level mapping using partial point-cloud observations obtained online in an unknown environment. We develop an approach for fully Convolutional Object Retrieval and Symmetry-AIded Registration (CORSAIR). Our model extends the Fully Convolutional Geo-metric Features model to learn a global object-shape embedding in addition to local point-wise features from the point-cloud observations. The global feature is used to retrieve a similar object from a category database, and the local features are used for robust pose registration between the observed and the retrieved object. Our formulation also leverages symmetries, present in the object shapes, to obtain promising local-feature pairs from different symmetry classes for matching. We present results from synthetic and real-world datasets with different object categories to verify the robustness of our method.},
  archive   = {C_IROS},
  author    = {Tianyu Zhao and Qiaojun Feng and Sai Jadhav and Nikolay Atanasov},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636347},
  pages     = {47-54},
  title     = {CORSAIR: Convolutional object retrieval and symmetry-AIded registration},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BORM: Bayesian object relation model for indoor scene
recognition. <em>IROS</em>, 39–46. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Scene recognition is a fundamental task in robotic perception. For human beings, scene recognition is reasonable because they have abundant object knowledge of the real world. The idea of transferring prior object knowledge from humans to scene recognition is significant but still less exploited. In this paper, we propose to utilize meaningful object representations for indoor scene representation. First, we utilize an improved object model (IOM) as a baseline that enriches the object knowledge by introducing a scene parsing algorithm pretrained on the ADE20K dataset with rich object categories related to the indoor scene. To analyze the object co-occurrences and pairwise object relations, we formulate the IOM from a Bayesian perspective as the Bayesian object relation model (BORM). Meanwhile, we incorporate the proposed BORM with the PlacesCNN model as the combined Bayesian object relation model (CBORM) for scene recognition and significantly outperforms the state-of-the-art methods on the reduced Places365 dataset, and SUN RGB-D dataset without retraining, showing the excellent generalization ability of the proposed method. Code can be found at https://github.com/FreeformRobotics/BORM.},
  archive   = {C_IROS},
  author    = {Liguang Zhou and Jun Cen and Xingchao Wang and Zhenglong Sun and Tin Lun Lam and Yangsheng Xu},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636024},
  pages     = {39-46},
  title     = {BORM: Bayesian object relation model for indoor scene recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A deep learning-based indoor scene classification approach
enhanced with inter-object distance semantic features. <em>IROS</em>,
32–38. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Convolutional Neural Networks (CNNs) have been increasingly applied in visual classification tasks by replacing hand-crafted features with deep features. However, problems such as inter-class similarity and intra-class variation led to the need of obtaining more descriptive features. To accomplish this, a new semantic inter-object relationship approach is proposed, which is based on the distance relationships between recognized objects. This new source of information represents how close or apart objects belonging to two object classes are, which, together with the number of object occurrences, allows to develop a more descriptive semantic feature representation of the scene. To exploit such semantic features, a two-branch CNN architecture based on 1D and 2D convolutional layers, is proposed. Also, an enhancement version, GSF 2 AppV2, of the Global and Semantic Feature Fusion described in [1] is proposed by integrating the new semantic inter-object relationship approach, as well as the aforementioned two-branch CNN architecture. The GSF 2 AppV2 is also composed of a CNN-based global feature branch, where five different CNN-based feature extraction approaches were assessed as global feature extraction modules. Moreover, to combine global and semantic features, two feature fusion approaches are proposed and evaluated: correlation and triple concatenation. GSF 2 AppV2 was evaluated in two benchmark datasets: the SUN RGB-D and NYU Depth V2. State-of-the-art results were achieved on both datasets, showing the effectiveness of the proposed semantic feature approach on the pipeline.},
  archive   = {C_IROS},
  author    = {Ricardo Pereira and Luís Garrote and Tiago Barros and Ana Lopes and Urbano J. Nunes},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636242},
  pages     = {32-38},
  title     = {A deep learning-based indoor scene classification approach enhanced with inter-object distance semantic features},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Memory-based semantic segmentation for off-road unstructured
natural environments. <em>IROS</em>, 24–31. (<a
href="https://doi.org/10.1109/IROS51168.2021.9636620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the availability of many datasets tailored for autonomous driving in real-world urban scenes, semantic segmentation for urban driving scenes achieves significant progress. However, semantic segmentation for off-road, unstructured environments is not widely studied. Directly applying existing segmentation networks often results in performance degradation as they cannot overcome intrinsic problems in such environments, such as illumination changes. In this paper, a built-in memory module for semantic segmentation is proposed to overcome these problems. The memory module stores significant representations of training images as memory items. In addition to the encoder embedding like items together, the proposed memory module is specifically designed to cluster together instances of the same class even when there are significant variances in embedded features. Therefore, it makes segmentation networks better deal with unexpected illumination changes. A triplet loss is used in training to minimize redundancy in storing discriminative representations of the memory module. The proposed memory module is general so that it can be adopted in a variety of networks. We conduct experiments on the Robot Unstructured Ground Driving (RUGD) dataset and RELLIS dataset, which are collected from off-road, unstructured natural environments. Experimental results show that the proposed memory module improves the performance of existing segmentation networks and contributes to capturing unclear objects over various off-road, unstructured natural scenes with equivalent computational cost and network parameters. As the proposed method can be integrated into compact networks, it presents a viable approach for resource-limited small autonomous platforms.},
  archive   = {C_IROS},
  author    = {Youngsaeng Jin and David Han and Hanseok Ko},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9636620},
  pages     = {24-31},
  title     = {Memory-based semantic segmentation for off-road unstructured natural environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Up-to-down network: Fusing multi-scale context for 3D
semantic scene completion. <em>IROS</em>, 16–23. (<a
href="https://doi.org/10.1109/IROS51168.2021.9635888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An efficient 3D scene perception algorithm is a vital component for autonomous driving and robotics systems. In this paper, we focus on semantic scene completion, which is a task of jointly estimating the volumetric occupancy and semantic labels of objects. Since the real-world data is sparse and occluded, this is an extremely challenging task. We propose a novel framework, named Up-to-Down network (UDNet), to achieve the large-scale semantic scene completion with an encoder-decoder architecture for voxel grids. The novel up-to-down block can effectively aggregate multi-scale context information to improve labeling coherence, and the atrous spatial pyramid pooling module is leveraged to expand the receptive field while preserving detailed geometric information. Besides, the proposed multi-scale fusion mechanism efficiently aggregates global background information and improves the semantic completion accuracy. Moreover, to further satisfy the needs of different tasks, our UDNet can accomplish the multi-resolution semantic completion, achieving faster but coarser completion. Detailed experiments in the semantic scene completion benchmark of SemanticKITTI illustrate that our proposed framework surpasses the state-of-the-art methods with remarkable margins and a real-time inference speed by using only voxel grids as input.},
  archive   = {C_IROS},
  author    = {Hao Zou and Xuemeng Yang and Tianxin Huang and Chujuan Zhang and Yong Liu and Wanlong Li and Feng Wen and Hongbo Zhang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS51168.2021.9635888},
  pages     = {16-23},
  title     = {Up-to-down network: Fusing multi-scale context for 3D semantic scene completion},
  year      = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
