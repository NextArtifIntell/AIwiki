<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ICCV_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="iccv---1611">ICCV - 1611</h2>
<ul>
<li><details>
<summary>
(2021c). PointBA: Towards backdoor attacks in 3D point cloud.
<em>ICCV</em>, 16472–16481. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D deep learning has been increasingly more popular for a variety of tasks including many safety-critical applications. However, recently several works raise the security issues of 3D deep models. Although most of them consider adversarial attacks, we identify that backdoor attack is indeed a more serious threat to 3D deep learning systems but remains unexplored. We present the backdoor attacks in 3D point cloud with a unified framework that exploits the unique properties of 3D data and networks. In particular, we design two attack approaches on point cloud: the poison-label backdoor attack (PointPBA) and the clean- label backdoor attack (PointCBA). The first one is straight-forward and effective in practice, while the latter is more sophisticated assuming there are certain data inspections. The attack algorithms are mainly motivated and developed by 1) the recent discovery of 3D adversarial samples suggesting the vulnerability of deep models under spatial transformation; 2) the proposed feature disentanglement technique that manipulates the feature of the data through optimization methods and its potential to embed a new task. Extensive experiments show the efficacy of the PointPBA with over 95\% success rate across various 3D datasets and models, and the more stealthy PointCBA with around 50\% success rate. Our proposed backdoor attack in 3D point cloud is expected to perform as a baseline for improving the robustness of 3D deep models.},
  archive   = {C_ICCV},
  author    = {Xinke Li and Zhirui Chen and Yue Zhao and Zekun Tong and Yabang Zhao and Andrew Lim and Joey Tianyi Zhou},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01618},
  pages     = {16472-16481},
  title     = {PointBA: Towards backdoor attacks in 3D point cloud},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Black-box detection of backdoor attacks with limited
information and data. <em>ICCV</em>, 16462–16471. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although deep neural networks (DNNs) have made rapid progress in recent years, they are vulnerable in adversarial environments. A malicious backdoor could be embedded in a model by poisoning the training dataset, whose intention is to make the infected model give wrong predictions during inference when the specific trigger appears. To mitigate the potential threats of backdoor attacks, various backdoor detection and defense methods have been proposed. However, the existing techniques usually require the poisoned training data or access to the white-box model, which is commonly unavailable in practice. In this paper, we propose a black-box backdoor detection (B3D) method to identify backdoor attacks with only query access to the model. We introduce a gradient-free optimization algorithm to reverse-engineer the potential trigger for each class, which helps to reveal the existence of backdoor attacks. In addition to backdoor detection, we also propose a simple strategy for reliable predictions using the identified backdoored models. Extensive experiments on hundreds of DNN models trained on several datasets corroborate the effectiveness of our method under the black-box setting against various backdoor attacks.},
  archive   = {C_ICCV},
  author    = {Yinpeng Dong and Xiao Yang and Zhijie Deng and Tianyu Pang and Zihao Xiao and Hang Su and Jun Zhu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01617},
  pages     = {16462-16471},
  title     = {Black-box detection of backdoor attacks with limited information and data},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rethinking the backdoor attacks’ triggers: A frequency
perspective. <em>ICCV</em>, 16453–16461. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Backdoor attacks have been considered a severe security threat to deep learning. Such attacks can make models perform abnormally on inputs with predefined triggers and still retain state-of-the-art performance on clean data. While backdoor attacks have been thoroughly investigated in the image domain from both attackers’ and defenders’ sides, an analysis in the frequency domain has been missing thus far.This paper first revisits existing backdoor triggers from a frequency perspective and performs a comprehensive analysis. Our results show that many current backdoor attacks exhibit severe high-frequency artifacts, which persist across different datasets and resolutions. We further demonstrate these high-frequency artifacts enable a simple way to detect existing backdoor triggers at a detection rate of 98.50\% without prior knowledge of the attack details and the target model. Acknowledging previous attacks’ weaknesses, we propose a practical way to create smooth backdoor triggers without high-frequency artifacts and study their detectability. We show that existing defense works can benefit by incorporating these smooth triggers into their design consideration. Moreover, we show that the detector tuned over stronger smooth triggers can generalize well to unseen weak smooth triggers. In short, our work emphasizes the importance of considering frequency analysis when designing both backdoor attacks and defenses in deep learning.},
  archive   = {C_ICCV},
  author    = {Yi Zeng and Won Park and Z. Morley Mao and Ruoxi Jia},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01616},
  pages     = {16453-16461},
  title     = {Rethinking the backdoor attacks’ triggers: A frequency perspective},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Invisible backdoor attack with sample-specific triggers.
<em>ICCV</em>, 16443–16452. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, backdoor attacks pose a new security threat to the training process of deep neural networks (DNNs). Attackers intend to inject hidden backdoors into DNNs, such that the attacked model performs well on benign samples, whereas its prediction will be maliciously changed if hidden backdoors are activated by the attacker-defined trigger. Existing backdoor attacks usually adopt the setting that triggers are sample-agnostic, i.e., different poisoned samples contain the same trigger, resulting in that the attacks could be easily mitigated by current backdoor defenses. In this work, we explore a novel attack paradigm, where backdoor triggers are sample-specific. In our attack, we only need to modify certain training samples with invisible perturbation, while not need to manipulate other training components (e.g., training loss, and model structure) as required in many existing attacks. Specifically, inspired by the recent advance in DNN-based image steganography, we generate sample-specific invisible additive noises as backdoor triggers by encoding an attacker-specified string into benign images through an encoder-decoder network. The mapping from the string to the target label will be generated when DNNs are trained on the poisoned dataset. Extensive experiments on benchmark datasets verify the effectiveness of our method in attacking models with or without defenses. The code will be available at https://github.com/yuezunli/ISSBA.},
  archive   = {C_ICCV},
  author    = {Yuezun Li and Yiming Li and Baoyuan Wu and Longkang Li and Ran He and Siwei Lyu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01615},
  pages     = {16443-16452},
  title     = {Invisible backdoor attack with sample-specific triggers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CLEAR: Clean-up sample-targeted backdoor in neural networks.
<em>ICCV</em>, 16433–16442. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The data poisoning attack has raised serious security concerns on the safety of deep neural networks, since it can lead to neural backdoor that misclassifies certain inputs crafted by an attacker. In particular, the sample-targeted backdoor attack is a new challenge. It targets at one or a few specific samples, called target samples, to misclassify them to a target class. Without a trigger planted in the backdoor model, the existing backdoor detection schemes fail to detect the sample-targeted backdoor as they depend on reverse-engineering the trigger or strong features of the trigger. In this paper, we propose a novel scheme to detect and mitigate sample-targeted backdoor attacks. We discover and demonstrate a unique property of the sample-targeted backdoor, which forces a boundary change such that small &quot;pockets&quot; are formed around the target sample. Based on this observation, we propose a novel defense mechanism to pinpoint a malicious pocket by &quot;wrapping&quot; them into a tight convex hull in the feature space. We design an effective algorithm to search for such a convex hull and remove the backdoor by fine-tuning the model using the identified malicious samples with the corrected label according to the convex hull. The experiments show that the proposed approach is highly efficient for detecting and mitigating a wide range of sample-targeted backdoor attacks.},
  archive   = {C_ICCV},
  author    = {Liuwan Zhu and Rui Ning and Chunsheng Xin and Chonggang Wang and Hongyi Wu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01614},
  pages     = {16433-16442},
  title     = {CLEAR: Clean-up sample-targeted backdoor in neural networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Revisiting adversarial robustness distillation: Robust soft
labels make student better. <em>ICCV</em>, 16423–16432. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Adversarial training is one effective approach for training robust deep neural networks against adversarial attacks. While being able to bring reliable robustness, adversarial training (AT) methods in general favor high capacity models, i.e., the larger the model the better the robustness. This tends to limit their effectiveness on small models, which are more preferable in scenarios where storage or computing resources are very limited (e.g., mobile devices). In this paper, we leverage the concept of knowledge distillation to improve the robustness of small models by distilling from adversarially trained large models. We first revisit several state-of-the-art AT methods from a distillation perspective and identify one common technique that can lead to improved robustness: the use of robust soft labels – predictions of a robust model. Following this observation, we propose a novel adversarial robustness distillation method called Robust Soft Label Adversarial Distillation (RSLAD) to train robust small student models. RSLAD fully exploits the robust soft labels produced by a robust (adversarially-trained) large teacher model to guide the student’s learning on both natural and adversarial examples in all loss terms. We empirically demonstrate the effectiveness of our RSLAD approach over existing adversarial training and distillation methods in improving the robustness of small models against state-of-the-art attacks including the AutoAttack. We also provide a set of understandings on our RSLAD and the importance of robust soft labels for adversarial robustness distillation. Code: https://github.com/zibojia/RSLAD.},
  archive   = {C_ICCV},
  author    = {Bojia Zi and Shihao Zhao and Xingjun Ma and Yu-Gang Jiang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01613},
  pages     = {16423-16432},
  title     = {Revisiting adversarial robustness distillation: Robust soft labels make student better},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Defending against universal adversarial patches by clipping
feature norms. <em>ICCV</em>, 16414–16422. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Physical-world adversarial attacks based on universal adversarial patches have been proved to be able to mislead deep convolutional neural networks (CNNs), exposing the vulnerability of real-world visual classification systems based on CNNs. In this paper, we empirically reveal and mathematically explain that the universal adversarial patches usually lead to deep feature vectors with very large norms in popular CNNs. Inspired by this, we propose a simple yet effective defending approach using a new feature norm clipping (FNC) layer which is a differentiable module that can be flexibly inserted in different CNNs to adaptively suppress the generation of large norm deep feature vectors. FNC introduces no trainable parameter and only very low computational overhead. However, experiments on multiple datasets validate that it can effectively improve the robustness of different CNNs towards white-box universal patch attacks while maintaining a satisfactory recognition accuracy for clean samples.},
  archive   = {C_ICCV},
  author    = {Cheng Yu and Jiansheng Chen and Youze Xue and Yuyang Liu and Weitao Wan and Jiayu Bao and Huimin Ma},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01612},
  pages     = {16414-16422},
  title     = {Defending against universal adversarial patches by clipping feature norms},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Low curvature activations reduce overfitting in adversarial
training. <em>ICCV</em>, 16403–16413. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Adversarial training is one of the most effective defenses against adversarial attacks. Previous works suggest that overfitting is a dominant phenomenon in adversarial training leading to a large generalization gap between test and train accuracy in neural networks. In this work, we show that the observed generalization gap is closely related to the choice of the activation function. In particular, we show that using activation functions with low (exact or approximate) curvature values has a regularization effect that significantly reduces both the standard and robust generalization gaps in adversarial training. We observe this effect for both differentiable/smooth activations such as SiLU as well as non-differentiable/non-smooth activations such as LeakyReLU. In the latter case, the &quot;approximate&quot; curvature of the activation is low. Finally, we show that for activation functions with low curvature, the double descent phenomenon for adversarially trained models does not occur.},
  archive   = {C_ICCV},
  author    = {Vasu Singla and Sahil Singla and Soheil Feizi and David Jacobs},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01611},
  pages     = {16403-16413},
  title     = {Low curvature activations reduce overfitting in adversarial training},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Practical relative order attack in deep ranking.
<em>ICCV</em>, 16393–16402. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent studies unveil the vulnerabilities of deep ranking models, where an imperceptible perturbation can trigger dramatic changes in the ranking result. While previous attempts focus on manipulating absolute ranks of certain candidates, the possibility of adjusting their relative order remains under-explored. In this paper, we formulate a new adversarial attack against deep ranking systems, i.e., the Order Attack, which covertly alters the relative order among a selected set of candidates according to an attacker-specified permutation, with limited interference to other unrelated candidates. Specifically, it is formulated as a triplet-style loss imposing an inequality chain reflecting the specified permutation. However, direct optimization of such white-box objective is infeasible in a real-world attack scenario due to various black-box limitations. To cope with them, we propose a Short-range Ranking Correlation metric as a surrogate objective for black-box Order Attack to approximate the white-box method. The Order Attack is evaluated on the Fashion-MNIST and Stanford-Online-Products datasets under both white-box and black-box threat models. The black-box attack is also successfully implemented on a major e-commerce platform. Comprehensive experimental evaluations demonstrate the effectiveness of the proposed methods, revealing a new type of ranking model vulnerability.},
  archive   = {C_ICCV},
  author    = {Mo Zhou and Le Wang and Zhenxing Niu and Qilin Zhang and Yinghui Xu and Nanning Zheng and Gang Hua},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01610},
  pages     = {16393-16402},
  title     = {Practical relative order attack in deep ranking},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cross-modality person re-identification via modality
confusion and center aggregation. <em>ICCV</em>, 16383–16392. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cross-modality person re-identification is a challenging task due to large cross-modality discrepancy and intramodality variations. Currently, most existing methods focus on learning modality-specific or modality-shareable features by using the identity supervision or modality label. Different from existing methods, this paper presents a novel Modality Confusion Learning Network (MCLNet). Its basic idea is to confuse two modalities, ensuring that the optimization is explicitly concentrated on the modality-irrelevant perspective. Specifically, MCLNet is designed to learn modality-invariant features by simultaneously minimizing inter-modality discrepancy while maximizing cross-modality similarity among instances in a single framework. Furthermore, an identity-aware marginal center aggregation strategy is introduced to extract the centralization features, while keeping diversity with a marginal constraint. Finally, we design a camera-aware learning scheme to enrich the discriminability. Extensive experiments on SYSU-MM01 and RegDB datasets show that MCLNet outperforms the state-of-the-art by a large margin. On the large-scale SYSU-MM01 dataset, our model can achieve 65.40\% and 61.98\% in terms of Rank-1 accuracy and mAP value.},
  archive   = {C_ICCV},
  author    = {Xin Hao and Sanyuan Zhao and Mang Ye and Jianbing Shen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01609},
  pages     = {16383-16392},
  title     = {Cross-modality person re-identification via modality confusion and center aggregation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A simple baseline for weakly-supervised scene graph
generation. <em>ICCV</em>, 16373–16382. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate the weakly-supervised scene graph generation, which is a challenging task since no correspondence of label and object is provided. The previous work regards such correspondence as a latent variable which is iteratively updated via nested optimization of the scene graph generation objective. However, we further reduce the complexity by decoupling it into an efficient first-order graph matching module optimized via contrastive learning to obtain such correspondence, which is used to train a standard scene graph generation model. The extensive experiments show that such a simple pipeline can significantly surpass the previous state-of-the-art by more than 30\% on the Visual Genome dataset, both in terms of graph matching accuracy and scene graph quality. We believe this work serves as a strong baseline for future research. Code is available at https://github.com/jshi31/WS-SGG.},
  archive   = {C_ICCV},
  author    = {Jing Shi and Yiwu Zhong and Ning Xu and Yin Li and Chenliang Xu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01608},
  pages     = {16373-16382},
  title     = {A simple baseline for weakly-supervised scene graph generation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). From general to specific: Informative scene graph generation
via balance adjustment. <em>ICCV</em>, 16363–16372. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The scene graph generation (SGG) task aims to detect visual relationship triplets, i.e., subject, predicate, object, in an image, providing a structural vision layout for scene understanding. However, current models are stuck in common predicates, e.g., &quot;on&quot; and &quot;at&quot;, rather than informative ones, e.g., &quot;standing on&quot; and &quot;looking at&quot;, resulting in the loss of precise information and overall performance. If a model only uses &quot;stone on road&quot; rather than &quot;blocking&quot; to describe an image, it is easy to misunderstand the scene. We argue that this phenomenon is caused by two key imbalances between informative predicates and common ones, i.e., semantic space level imbalance and training sample level imbalance. To tackle this problem, we propose BA-SGG, a simple yet effective SGG framework based on balance adjustment but not the conventional distribution fitting. It integrates two components: Semantic Adjustment (SA) and Balanced Predicate Learning (BPL), respectively for adjusting these imbalances. Benefited from the model-agnostic process, our method is easily applied to the state-of-the-art SGG models and significantly improves the SGG performance. Our method achieves 14.3\%, 8.0\%, and 6.1\% higher Mean Recall (mR) than that of the Transformer model at three scene graph generation sub-tasks on Visual Genome, respectively. Codes are publicly available 1 .},
  archive   = {C_ICCV},
  author    = {Yuyu Guo and Lianli Gao and Xuanhan Wang and Yuxuan Hu and Xing Xu and Xu Lu and Heng Tao Shen and Jingkuan Song},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01607},
  pages     = {16363-16372},
  title     = {From general to specific: Informative scene graph generation via balance adjustment},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatial-temporal transformer for dynamic scene graph
generation. <em>ICCV</em>, 16352–16362. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dynamic scene graph generation aims at generating a scene graph of the given video. Compared to the task of scene graph generation from images, it is more challenging because of the dynamic relationships between objects and the temporal dependencies between frames allowing for a richer semantic interpretation. In this paper, we propose Spatial-temporal Transformer (STTran), a neural network that consists of two core modules: (1) a spatial encoder that takes an input frame to extract spatial context and reason about the visual relationships within a frame, and (2) a temporal decoder which takes the output of the spatial encoder as input in order to capture the temporal dependencies between frames and infer the dynamic relationships. Furthermore, STTran is flexible to take varying lengths of videos as input without clipping, which is especially important for long videos. Our method is validated on the benchmark dataset Action Genome (AG). The experimental results demonstrate the superior performance of our method in terms of dynamic scene graphs. Moreover, a set of ablative studies is conducted and the effect of each proposed module is justified. Code available at: https://github.com/yrcong/STTran.},
  archive   = {C_ICCV},
  author    = {Yuren Cong and Wentong Liao and Hanno Ackermann and Bodo Rosenhahn and Michael Ying Yang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01606},
  pages     = {16352-16362},
  title     = {Spatial-temporal transformer for dynamic scene graph generation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unconditional scene graph generation. <em>ICCV</em>,
16342–16351. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite recent advancements in single-domain or single-object image generation, it is still challenging to generate complex scenes containing diverse, multiple objects and their interactions. Scene graphs, composed of nodes as objects and directed-edges as relationships among objects, offer an alternative representation of a scene that is more semantically grounded than images. We hypothesize that a generative model for scene graphs might be able to learn the underlying semantic structure of real-world scenes more effectively than images, and hence, generate realistic novel scenes in the form of scene graphs. In this work, we explore a new task for the unconditional generation of semantic scene graphs. We develop a deep auto-regressive model called SceneGraphGen which can directly learn the probability distribution over labelled and directed graphs using a hierarchical recurrent architecture. The model takes a seed object as input and generates a scene graph in a sequence of steps, each step generating an object node, followed by a sequence of relationship edges connecting to the previous nodes. We show that the scene graphs generated by SceneGraphGen are diverse and follow the semantic patterns of real-world scenes. Additionally, we demonstrate the application of the generated graphs in image synthesis, anomaly detection and scene graph completion.},
  archive   = {C_ICCV},
  author    = {Sarthak Garg and Helisa Dhamo and Azade Farshad and Sabrina Musatian and Nassir Navab and Federico Tombari},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01605},
  pages     = {16342-16351},
  title     = {Unconditional scene graph generation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph-to-3D: End-to-end generation and manipulation of 3D
scenes using scene graphs. <em>ICCV</em>, 16332–16341. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Controllable scene synthesis consists of generating 3D information that satisfy underlying specifications. Thereby, these specifications should be abstract, i.e. allowing easy user interaction, whilst providing enough interface for detailed control. Scene graphs are representations of a scene, composed of objects (nodes) and inter-object relationships (edges), proven to be particularly suited for this task, as they allow for semantic control on the generated content. Previous works tackling this task often rely on synthetic data, and retrieve object meshes, which naturally limits the generation capabilities. To circumvent this issue, we instead propose the first work that directly generates shapes from a scene graph in an end-to-end manner. In addition, we show that the same model supports scene modification, using the respective scene graph as interface. Leveraging Graph Convolutional Networks (GCN) we train a variational Auto-Encoder on top of the object and edge categories, as well as 3D shapes and scene layouts, allowing latter sampling of new scenes and shapes.},
  archive   = {C_ICCV},
  author    = {Helisa Dhamo and Fabian Manhardt and Nassir Navab and Federico Tombari},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01604},
  pages     = {16332-16341},
  title     = {Graph-to-3D: End-to-end generation and manipulation of 3D scenes using scene graphs},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Few-shot visual relationship co-localization. <em>ICCV</em>,
16322–16331. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, given a small bag of images, each containing a common but latent predicate, we are interested in localizing visual subject-object pairs connected via the common predicate in each of the images. We refer to this novel problem as visual relationship co-localization or VRC as an abbreviation. VRC is a challenging task, even more so than the well-studied object co-localization task. This becomes further challenging when using just a few images, the model has to learn to co-localize visual subject-object pairs connected via unseen predicates. To solve VRC, we propose an optimization framework to select a common visual relationship in each image of the bag. The goal of the optimization framework is to find the optimal solution by learning visual relationship similarity across images in a few-shot setting. To obtain robust visual relationship representation, we utilize a simple yet effective technique that learns relationship embedding as a translation vector from visual subject to visual object in a shared space. Further, to learn visual relationship similarity, we utilize a proven meta-learning technique commonly used for few-shot classification tasks. Finally, to tackle the combinatorial complexity challenge arising from an exponential number of feasible solutions, we use a greedy approximation inference algorithm that selects approximately the best solution.We extensively evaluate our proposed framework on variations of bag sizes obtained from two challenging public datasets, namely VrR-VG and VG-150, and achieve impressive visual co-localization performance.},
  archive   = {C_ICCV},
  author    = {Revant Teotia and Vaibhav Mishra and Mayank Maheshwari and Anand Mishra},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01603},
  pages     = {16322-16331},
  title     = {Few-shot visual relationship co-localization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Salient object ranking with position-preserved attention.
<em>ICCV</em>, 16311–16321. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Instance segmentation can detect where the objects are in an image, but hard to understand the relationship between them. We pay attention to a typical relationship, relative saliency. A closely related task, salient object detection, predicts a binary map highlighting a visually salient region while hard to distinguish multiple objects. Directly combining two tasks by post-processing also leads to poor performance. There is a lack of research on relative saliency at present, limiting the practical applications such as content-aware image cropping, video summary, and image labeling.In this paper, we study the Salient Object Ranking (SOR) task, which manages to assign a ranking order of each detected object according to its visual saliency. We propose the first end-to-end framework of the SOR task and solve it in a multi-task learning fashion. The framework handles instance segmentation and salient object ranking simultaneously. In this framework, the SOR branch is independent and flexible to cooperate with different detection methods, so that easy to use as a plugin. We also intro-duce a Position-Preserved Attention (PPA) module tailored for the SOR branch. It consists of the position embedding stage and feature interaction stage. Considering the importance of position in saliency comparison, we preserve absolute coordinates of objects in ROI pooling operation and then fuse positional information with semantic features in the first stage. In the feature interaction stage, we apply the attention mechanism to obtain proposals’ contextualized representations to predict their relative ranking orders. Extensive experiments have been conducted on the ASR dataset. Without bells and whistles, our proposed method outperforms the former state-of-the-art method significantly. The code will be released publicly available on https://github.com/EricFH/SOR.},
  archive   = {C_ICCV},
  author    = {Hao Fang and Daoxin Zhang and Yi Zhang and Minghao Chen and Jiawei Li and Yao Hu and Deng Cai and Xiaofei He},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01602},
  pages     = {16311-16321},
  title     = {Salient object ranking with position-preserved attention},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vision-language transformer and query generation for
referring segmentation. <em>ICCV</em>, 16301–16310. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we address the challenging task of referring segmentation. The query expression in referring segmentation typically indicates the target object by describing its relationship with others. Therefore, to find the target one among all instances in the image, the model must have a holistic understanding of the whole image. To achieve this, we reformulate referring segmentation as a direct attention problem: finding the region in the image where the query language expression is most attended to. We introduce transformer and multi-head attention to build a network with an encoder-decoder attention mechanism architecture that &quot;queries&quot; the given image with the language expression. Furthermore, we propose a Query Generation Module, which produces multiple sets of queries with different attention weights that represent the diversified comprehensions of the language expression from different aspects. At the same time, to find the best way from these diversified comprehensions based on visual clues, we further propose a Query Balance Module to adaptively select the output features of these queries for a better mask generation. Without bells and whistles, our approach is light-weight and achieves new state-of-the-art performance consistently on three referring segmentation datasets, RefCOCO, RefCOCO+, and G-Ref. Our code is available at https://github.com/henghuiding/Vision-Language-Transformer.},
  archive   = {C_ICCV},
  author    = {Henghui Ding and Chang Liu and Suchen Wang and Xudong Jiang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01601},
  pages     = {16301-16310},
  title     = {Vision-language transformer and query generation for referring segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Condensing a sequence to one informative frame for video
recognition. <em>ICCV</em>, 16291–16300. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Video is complex due to large variations in motion and rich content in fine-grained visual details. Abstracting useful information from such information-intensive media requires exhaustive computing resources. This paper studies a two-step alternative that first condenses the video sequence to an informative &quot;frame&quot; and then exploits off-the-shelf image recognition system on the synthetic frame. A valid question is how to define &quot;useful information&quot; and then distill it from a video sequence down to one synthetic frame. This paper presents a novel Informative Frame Synthesis (IFS) architecture that incorporates three objective tasks, i.e., appearance reconstruction, video categorization, motion estimation, and two regularizers, i.e., adversarial learning, color consistency. Each task equips the synthetic frame with one ability, while each regularizer enhances its visual quality. With these, by jointly learning the frame synthesis in an end-to-end manner, the generated frame is expected to encapsulate the required spatio-temporal information useful for video analysis. Extensive experiments are conducted on the large-scale Kinetics dataset. When comparing to baseline methods that map video sequence to a single image, IFS shows superior performance. More remarkably, IFS consistently demonstrates evident improvements on image-based 2D networks and clip-based 3D networks, and achieves comparable performance with the state-of-the-art methods with less computational cost.},
  archive   = {C_ICCV},
  author    = {Zhaofan Qiu and Ting Yao and Yan Shu and Chong-Wah Ngo and Tao Mei},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01600},
  pages     = {16291-16300},
  title     = {Condensing a sequence to one informative frame for video recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Refining action segmentation with hierarchical video
representations. <em>ICCV</em>, 16282–16290. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose Hierarchical Action Segmentation Refiner (HASR), which can refine temporal action segmentation results from various models by understanding the overall context of a given video in a hierarchical way. When a backbone model for action segmentation estimates how the given video can be segmented, our model extracts segment-level representations based on frame-level features, and extracts a video-level representation based on the segment-level representations. Based on these hierarchical representations, our model can refer to the overall context of the entire video, and predict how the segment labels that are out of context should be corrected. Our HASR can be plugged into various action segmentation models (MS-TCN, SSTDA, ASRF), and improve the performance of state-of-the-art models based on three challenging datasets (GTEA, 50Salads, and Breakfast). For example, in 50Salads dataset, the segmental edit score improves from 67.9\% to 77.4\% (MS-TCN), from 75.8\% to 77.3\% (SSTDA), from 79.3\% to 81.0\% (ASRF). In addition, our model can refine the segmentation result from the unseen backbone model, which was not referred to when training HASR. This generalization performance would make HASR be an effective tool for boosting up the existing approaches for temporal action segmentation. Our code is available at https://github.com/cotton-ahn/HASR_iccv2021.},
  archive   = {C_ICCV},
  author    = {Hyemin Ahn and Dongheui Lee},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01599},
  pages     = {16282-16290},
  title     = {Refining action segmentation with hierarchical video representations},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Region-aware contrastive learning for semantic segmentation.
<em>ICCV</em>, 16271–16281. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent works have made great success in semantic segmentation by exploiting contextual information in a local or global manner within individual image and supervising the model with pixel-wise cross entropy loss. However, from the holistic view of the whole dataset, semantic relations not only exist inside one single image, but also prevail in the whole training data, which makes solely considering intra-image correlations insufficient. Inspired by recent progress in unsupervised contrastive learning, we propose the region-aware contrastive learning (RegionContrast) for semantic segmentation in the supervised manner. In order to enhance the similarity of semantically similar pixels while keeping the discrimination from others, we employ contrastive learning to realize this objective. With the help of memory bank, we explore to store all the representative features into the memory. Without loss of generality, to efficiently incorporate all training data into the memory bank while avoiding taking too much computation resource, we propose to construct region centers to represent features from different categories for every image. Hence, the proposed region-aware contrastive learning is performed in a region level for all the training data, which saves much more memory than methods exploring the pixel-level relations. The proposed RegionContrast brings little computation cost during training and requires no extra overhead for testing. Extensive experiments demonstrate that our method achieves state-of-the-art performance on three benchmark datasets including Cityscapes, ADE20K and COCO Stuff.},
  archive   = {C_ICCV},
  author    = {Hanzhe Hu and Jinshi Cui and Liwei Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01598},
  pages     = {16271-16281},
  title     = {Region-aware contrastive learning for semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Perception-aware multi-sensor fusion for 3D LiDAR semantic
segmentation. <em>ICCV</em>, 16260–16270. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D LiDAR (light detection and ranging) semantic segmentation is important in scene understanding for many applications, such as auto-driving and robotics. For example, for autonomous cars equipped with RGB cameras and LiDAR, it is crucial to fuse complementary information from different sensors for robust and accurate segmentation. Existing fusion-based methods, however, may not achieve promising performance due to the vast difference between the two modalities. In this work, we investigate a collaborative fusion scheme called perception-aware multi-sensor fusion (PMF) to exploit perceptual information from two modalities, namely, appearance information from RGB images and spatio-depth information from point clouds. To this end, we first project point clouds to the camera coordinates to provide spatio-depth information for RGB images. Then, we propose a two-stream network to extract features from the two modalities, separately, and fuse the features by effective residual-based fusion modules. Moreover, we propose additional perception-aware losses to measure the perceptual difference between the two modalities. Extensive experiments on two benchmark data sets show the superiority of our method. For example, on nuScenes, our PMF outperforms the state-of-the-art method by 0.8\% in mIoU.},
  archive   = {C_ICCV},
  author    = {Zhuangwei Zhuang and Rong Li and Kui Jia and Qicheng Wang and Yuanqing Li and Mingkui Tan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01597},
  pages     = {16260-16270},
  title     = {Perception-aware multi-sensor fusion for 3D LiDAR semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Transformer-based attention networks for continuous
pixel-wise prediction. <em>ICCV</em>, 16249–16259. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While convolutional neural networks have shown a tremendous impact on various computer vision tasks, they generally demonstrate limitations in explicitly modeling long-range dependencies due to the intrinsic locality of the convolution operation. Initially designed for natural language processing tasks, Transformers have emerged as alternative architectures with innate global self-attention mechanisms to capture long-range dependencies. In this paper, we propose TransDepth, an architecture that benefits from both convolutional neural networks and transformers. To avoid the network losing its ability to capture locallevel details due to the adoption of transformers, we propose a novel decoder that employs attention mechanisms based on gates. Notably, this is the first paper that applies transformers to pixel-wise prediction problems involving continuous labels (i.e., monocular depth prediction and surface normal estimation). Extensive experiments demonstrate that the proposed TransDepth achieves state-of-theart performance on three challenging datasets. Our code is available at: https://github.com/ygjwd12345/TransDepth.},
  archive   = {C_ICCV},
  author    = {Guanglei Yang and Hao Tang and Mingli Ding and Nicu Sebe and Elisa Ricci},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01596},
  pages     = {16249-16259},
  title     = {Transformer-based attention networks for continuous pixel-wise prediction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Point transformer. <em>ICCV</em>, 16239–16248. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-attention networks have revolutionized natural language processing and are making impressive strides in image analysis tasks such as image classification and object detection. Inspired by this success, we investigate the application of self-attention networks to 3D point cloud processing. We design self-attention layers for point clouds and use these to construct self-attention networks for tasks such as semantic scene segmentation, object part segmentation, and object classification. Our Point Transformer design improves upon prior work across domains and tasks. For example, on the challenging S3DIS dataset for large-scale semantic scene segmentation, the Point Transformer attains an mIoU of 70.4\% on Area 5, outperforming the strongest prior model by 3.3 absolute percentage points and crossing the 70\% mIoU threshold for the first time.},
  archive   = {C_ICCV},
  author    = {Hengshuang Zhao and Li Jiang and Jiaya Jia and Philip Torr and Vladlen Koltun},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01595},
  pages     = {16239-16248},
  title     = {Point transformer},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Adaptive focus for efficient video recognition.
<em>ICCV</em>, 16229–16238. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we explore the spatial redundancy in video recognition with the aim to improve the computational efficiency. It is observed that the most informative region in each frame of a video is usually a small image patch, which shifts smoothly across frames. Therefore, we model the patch localization problem as a sequential decision task, and propose a reinforcement learning based approach for efficient spatially adaptive video recognition (AdaFocus). In specific, a light-weighted ConvNet is first adopted to quickly process the full video sequence, whose features are used by a recurrent policy network to localize the most task-relevant regions. Then the selected patches are inferred by a high-capacity network for the final prediction. During offline inference, once the informative patch sequence has been generated, the bulk of computation can be done in parallel, and is efficient on modern GPU devices. In addition, we demonstrate that the proposed method can be easily extended by further considering the temporal redundancy, e.g., dynamically skipping less valuable frames. Extensive experiments on five benchmark datasets, i.e., ActivityNet, FCVID, MiniKinetics, Something-Something V1&amp;V2, demonstrate that our method is significantly more efficient than the competitive baselines. Code is available at https://github.com/blackfeather-wang/AdaFocus.},
  archive   = {C_ICCV},
  author    = {Yulin Wang and Zhaoxi Chen and Haojun Jiang and Shiji Song and Yizeng Han and Gao Huang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01594},
  pages     = {16229-16238},
  title     = {Adaptive focus for efficient video recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SurfGen: Adversarial 3D shape synthesis with explicit
surface discriminators. <em>ICCV</em>, 16218–16228. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in deep generative models have led to immense progress in 3D shape synthesis. While existing models are able to synthesize shapes represented as voxels, point-clouds, or implicit functions, these methods only indirectly enforce the plausibility of the final 3D shape surface. Here we present a 3D shape synthesis framework (SurfGen) that directly applies adversarial training to the object surface. Our approach uses a differentiable spherical projection layer to capture and represent the explicit zero isosurface of an implicit 3D generator as functions defined on the unit sphere. By processing the spherical representation of 3D object surfaces with a spherical CNN in an adversarial setting, our generator can better learn the statistics of natural shape surfaces. We evaluate our model on large-scale shape datasets, and demonstrate that the end-to-end trained model is capable of generating high fidelity 3D shapes with diverse topology.},
  archive   = {C_ICCV},
  author    = {Andrew Luo and Tianqin Li and Wen-Hao Zhang and Tai Sing Lee},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01593},
  pages     = {16218-16228},
  title     = {SurfGen: Adversarial 3D shape synthesis with explicit surface discriminators},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CTRL-c: Camera calibration TRansformer with
line-classification. <em>ICCV</em>, 16208–16217. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Single image camera calibration is the task of estimating the camera parameters from a single input image, such as the vanishing points, focal length, and horizon line. In this work, we propose Camera calibration TRansformer with Line-Classification (CTRL-C), an end-to-end neural network-based approach to single image camera calibration, which directly estimates the camera parameters from an image and a set of line segments. Our network adopts the transformer architecture to capture the global structure of an image with multi-modal inputs in an end-to-end manner. We also propose an auxiliary task of line classification to train the network to extract the global geometric information from lines effectively. Our experiments demonstrate that CTRL-C outperforms the previous state-of-the-art methods on the Google Street View and SUN360 benchmark datasets. Code is available at https://github.com/jwlee-vcl/CTRL-C.},
  archive   = {C_ICCV},
  author    = {Jinwoo Lee and Hyunsung Go and Hyunjoon Lee and Sunghyun Cho and Minhyuk Sung and Junho Kim},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01592},
  pages     = {16208-16217},
  title     = {CTRL-C: Camera calibration TRansformer with line-classification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). A closer look at rotation-invariant deep point cloud
analysis. <em>ICCV</em>, 16198–16207. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the deep point cloud analysis tasks where the inputs of the networks are randomly rotated. Recent progress in rotation-invariant point cloud analysis is mainly driven by converting point clouds into their respective canonical poses, and principal component analysis (PCA) is a practical tool to achieve this. Due to the imperfect alignment of PCA, most of the current works are devoted to developing powerful network structures and features to overcome this deficiency, without thoroughly analyzing the PCA-based canonical poses themselves. In this work, we present a detailed study w.r.t. the PCA-based canonical poses of point clouds. Our investigation reveals that the ambiguity problem associated with the PCA-based canonical poses is handled insufficiently in some recent works. To this end, we develop a simple pose selector module for disambiguation, which presents noticeable enhancement (i.e., 5.3\% classification accuracy) over state-of-the-art approaches on the challenging real-world dataset. 1},
  archive   = {C_ICCV},
  author    = {Feiran Li and Kent Fujiwara and Fumio Okura and Yasuyuki Matsushita},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01591},
  pages     = {16198-16207},
  title     = {A closer look at rotation-invariant deep point cloud analysis},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PU-EVA: An edge-vector based approximation solution for
flexible-scale point cloud upsampling. <em>ICCV</em>, 16188–16197. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {High-quality point clouds have practical significance for point-based rendering, semantic understanding, and surface reconstruction. Upsampling sparse, noisy and non-uniform point clouds for a denser and more regular approximation of target objects is a desirable but challenging task. Most existing methods duplicate point features for upsampling, constraining the upsampling scales at a fixed rate. In this work, the arbitrary point clouds upsampling rates are achieved via edge-vector based affine combinations, and a novel design of Edge-Vector based Approximation for Flexible-scale Point clouds Upsampling (PU-EVA) is proposed. The edge-vector based approximation encodes neighboring connectivity via affine combinations based on edge vectors, and restricts the approximation error within a second-order term of Taylor’s Expansion. Moreover, the EVA upsampling decouples the upsampling scales with network architecture, achieving the arbitrary upsampling rates in one-time training. Qualitative and quantitative evaluations demonstrate that the proposed PU-EVA outperforms the state-of-the-arts in terms of proximity-to-surface, distribution uniformity, and geometric details preservation.},
  archive   = {C_ICCV},
  author    = {Luqing Luo and Lulu Tang and Wanyi Zhou and Shizheng Wang and Zhi-Xin Yang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01590},
  pages     = {16188-16197},
  title     = {PU-EVA: An edge-vector based approximation solution for flexible-scale point cloud upsampling},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Full-velocity radar returns by radar-camera fusion.
<em>ICCV</em>, 16178–16187. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A distinctive feature of Doppler radar is the measurement of velocity in the radial direction for radar points. However, the missing tangential velocity component hampers object velocity estimation as well as temporal integration of radar sweeps in dynamic scenes. Recognizing that fusing camera with radar provides complementary information to radar, in this paper we present a closed-form solution for the point-wise, full-velocity estimate of Doppler returns using the corresponding optical flow from camera images. Additionally, we address the association problem between radar returns and camera images with a neural network that is trained to estimate radar-camera correspondences. Experimental results on the nuScenes dataset verify the validity of the method and show significant improvements over the state-of-the-art in velocity estimation and accumulation of radar points.},
  archive   = {C_ICCV},
  author    = {Yunfei Long and Daniel Morris and Xiaoming Liu and Marcos Castro and Punarjay Chakravarty and Praveen Narayanan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01589},
  pages     = {16178-16187},
  title     = {Full-velocity radar returns by radar-camera fusion},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attack as the best defense: Nullifying image-to-image
translation GANs via limit-aware adversarial attack. <em>ICCV</em>,
16168–16177. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Due to the great success of image-to-image (Img2Img) translation GANs, many applications with ethics issues arise, e.g., DeepFake and DeepNude, presenting a challenging problem to prevent the misuse of these techniques. In this work, we tackle the problem by a new adversarial attack scheme, namely the Nullifying Attack, which cancels the image translation process and proposes a corresponding framework, the Limit-Aware Self-Guiding Gradient Sliding Attack (LaS-GSA) under a black-box setting. In other words, by processing the image with the proposed LaS-GSA before publishing, any image translation functions can be nullified, which prevents the images from malicious manipulations. First, we introduce the limit-aware RGF and the gradient sliding mechanism to estimate the gradient that adheres to the adversarial limit, i.e., the pixel value limitations of the adversarial example. We theoretically prove that our model is able to avoid the error caused by the projection in both the direction and the length. Then, an effective self-guiding prior is extracted solely from the threat model and the target image to efficiently leverage the prior information and guide the gradient estimation process. Extensive experiments demonstrate that LaS-GSA requires fewer queries to nullify the image translation process with higher success rates than 4 state-of-the-art methods.},
  archive   = {C_ICCV},
  author    = {Chin-Yuan Yeh and Hsi-Wen Chen and Hong-Han Shuai and De-Nian Yang and Ming-Syan Chen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01588},
  pages     = {16168-16177},
  title     = {Attack as the best defense: Nullifying image-to-image translation GANs via limit-aware adversarial attack},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Knowledge-enriched distributional model inversion attacks.
<em>ICCV</em>, 16158–16167. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Model inversion (MI) attacks are aimed at reconstructing training data from model parameters. Such attacks have triggered increasing concerns about privacy, especially given a growing number of online model repositories. However, existing MI attacks against deep neural networks (DNNs) have large room for performance improvement. We present a novel inversion-specific GAN that can better distill knowledge useful for performing attacks on private models from public data. In particular, we train the discriminator to differentiate not only the real and fake samples but the soft-labels provided by the target model. Moreover, unlike previous work that directly searches for a single data point to represent a target class, we propose to model a private data distribution for each target class. Our experiments show that the combination of these techniques can significantly boost the success rate of the state-of-the-art MI attacks by 150\%, and generalize better to a variety of datasets and models. Our code is available at https://github.com/SCccc21/Knowledge-Enriched-DMI.},
  archive   = {C_ICCV},
  author    = {Si Chen and Mostafa Kahla and Ruoxi Jia and Guo-Jun Qi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01587},
  pages     = {16158-16167},
  title     = {Knowledge-enriched distributional model inversion attacks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Aha! Adaptive history-driven attack for decision-based
black-box models. <em>ICCV</em>, 16148–16157. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The decision-based black-box attack means to craft adversarial examples with only the top-1 label of the victim model available. A common practice is to start from a large perturbation and then iteratively reduce it with a deterministic direction and a random one while keeping it adversarial. The limited information obtained from each query and inefficient direction sampling impede attack efficiency, making it hard to obtain a small enough perturbation within a limited number of queries. To tackle this problem, we propose a novel attack method termed Adaptive History-driven Attack (AHA) which gathers information from all historical queries as the prior for current sampling. Moreover, to balance between the deterministic direction and the random one, we dynamically adjust the coefficient according to the ratio of the actual magnitude reduction to the expected one. Such a strategy improves the success rate of queries during optimization, letting adversarial examples move swiftly along the decision boundary. Our method can also integrate with subspace optimization like dimension reduction to further improve efficiency. Extensive experiments on both ImageNet and CelebA datasets demonstrate that our method achieves at least 24.3\% lower magnitude of perturbation on average with the same number of queries. Finally, we prove the practical potential of our method by evaluating it on popular defense methods and a real-world system provided by MEGVII Face++.},
  archive   = {C_ICCV},
  author    = {Jie Li and Rongrong Ji and Peixian Chen and Baochang Zhang and Xiaopeng Hong and Ruixin Zhang and Shaoxin Li and Jilin Li and Feiyue Huang and Yongjian Wu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01586},
  pages     = {16148-16157},
  title     = {Aha! adaptive history-driven attack for decision-based black-box models},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Admix: Enhancing the transferability of adversarial attacks.
<em>ICCV</em>, 16138–16147. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep neural networks are known to be extremely vulnerable to adversarial examples under white-box setting. Moreover, the malicious adversaries crafted on the surrogate (source) model often exhibit black-box transferability on other models with the same learning task but having different architectures. Recently, various methods are proposed to boost the adversarial transferability, among which the input transformation is one of the most effective approaches. We investigate in this direction and observe that existing transformations are all applied on a single image, which might limit the adversarial transferability. To this end, we propose a new input transformation based attack method called Admix that considers the input image and a set of images randomly sampled from other categories. Instead of directly calculating the gradient on the original input, Admix calculates the gradient on the input image admixed with a small portion of each add-in image while using the original label of the input to craft more transferable adversaries. Empirical evaluations on standard ImageNet dataset demonstrate that Admix could achieve significantly better transferability than existing input transformation methods under both single model setting and ensemble-model setting. By incorporating with existing input transformations, our method could further improve the transferability and outperforms the state-of-the-art combination of input transformations by a clear margin when attacking nine advanced defense models under ensemble-model setting. Code is available at https://github.com/JHL-HUST/Admix.},
  archive   = {C_ICCV},
  author    = {Xiaosen Wang and Xuanran He and Jingdong Wang and Kun He},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01585},
  pages     = {16138-16147},
  title     = {Admix: Enhancing the transferability of adversarial attacks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian deep basis fitting for depth completion with
uncertainty. <em>ICCV</em>, 16127–16137. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work we investigate the problem of uncertainty estimation for image-guided depth completion. We extend Deep Basis Fitting (DBF) [54] for depth completion within a Bayesian evidence framework to provide calibrated perpixel variance. The DBF approach frames the depth completion problem in terms of a network that produces a set of low-dimensional depth bases and a differentiable least squares fitting module that computes the basis weights using the sparse depths. By adopting a Bayesian treatment, our Bayesian Deep Basis Fitting (BDBF) approach is able to 1) predict high-quality uncertainty estimates and 2) enable depth completion with few or no sparse measurements. We conduct controlled experiments to compare BDBF against commonly used techniques for uncertainty estimation under various scenarios. Results show that our method produces better uncertainty estimates with accurate depth prediction.},
  archive   = {C_ICCV},
  author    = {Chao Qu and Wenxin Liu and Camillo J. Taylor},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01584},
  pages     = {16127-16137},
  title     = {Bayesian deep basis fitting for depth completion with uncertainty},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Glimpse-attend-and-explore: Self-attention for active visual
exploration. <em>ICCV</em>, 16117–16126. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Active visual exploration aims to assist an agent with a limited field of view to understand its environment based on partial observations made by choosing the best viewing directions in the scene. Recent methods have tried to ad-dress this problem either by using reinforcement learning, which is difficult to train, or by uncertainty maps, which are task-specific and can only be implemented for dense prediction tasks. In this paper, we propose the Glimpse-Attend-and-Explore model which: (a) employs self-attention to guide the visual exploration instead of task-specific uncertainty maps; (b) can be used for both dense and sparse prediction tasks; and (c) uses a contrastive stream to further improve the representations learned. Unlike previous works, we show the application of our model on multiple tasks like reconstruction, segmentation and classification. Our model provides encouraging results while being less dependent on dataset bias in driving the exploration. We further perform an ablation study to investigate the features and attention learned by our model. Finally, we show that our self-attention module learns to attend different regions of the scene by minimizing the loss on the downstream task. Code: https://github.com/soroushseifi/glimpse-attend-explore.},
  archive   = {C_ICCV},
  author    = {Soroush Seifi and Abhishek Jha and Tinne Tuytelaars},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01583},
  pages     = {16117-16126},
  title     = {Glimpse-attend-and-explore: Self-attention for active visual exploration},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The surprising effectiveness of visual odometry techniques
for embodied PointGoal navigation. <em>ICCV</em>, 16107–16116. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It is fundamental for personal robots to reliably navigate to a specified goal. To study this task, PointGoal navigation has been introduced in simulated Embodied AI environments. Recent advances solve this PointGoal navigation task with near-perfect accuracy (99.6\% success) in photo-realistically simulated environments, assuming noiseless egocentric vision, noiseless actuation and most importantly, perfect localization. However, under realistic noise models for visual sensors and actuation, and without access to a &quot;GPS and Compass sensor,&quot; the 99.6\%-success agents for PointGoal navigation only succeed with 0.3\%. 1 In this work, we demonstrate the surprising effectiveness of visual odometry for the task of PointGoal navigation in this realistic setting, i.e., with realistic noise models for perception and actuation and without access to GPS and Compass sensors. We show that integrating visual odometry techniques into navigation policies improves the state-of-the-art on the popular Habitat PointNav benchmark by a large margin, improving success from 64.5\% to 71.7\% while executing 6.4 times faster.},
  archive   = {C_ICCV},
  author    = {Xiaoming Zhao and Harsh Agrawal and Dhruv Batra and Alexander Schwing},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01582},
  pages     = {16107-16116},
  title     = {The surprising effectiveness of visual odometry techniques for embodied PointGoal navigation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Auxiliary tasks and exploration enable ObjectGoal
navigation. <em>ICCV</em>, 16097–16106. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {ObjectGoal Navigation (ObjectNav) is an embodied task wherein agents are to navigate to an object instance in an unseen environment. Prior works have shown that end-to-end ObjectNav agents that use vanilla visual and recurrent modules, e.g. a CNN+RNN, perform poorly due to overfitting and sample inefficiency. This has motivated current state-of-the-art methods to mix analytic and learned components and operate on explicit spatial maps of the environment. We instead re-enable a generic learned agent by adding auxiliary learning tasks and an exploration reward. Our agents achieve 24.5\% success and 8.1\% SPL, a 37\% and 8\% relative improvement over prior state-of-the-art, respectively, on the Habitat ObjectNav Challenge [35]. From our analysis, we propose that agents will act to simplify their visual inputs so as to smooth their RNN dynamics, and that auxiliary tasks reduce overfitting by minimizing effective RNN dimensionality; i.e. a performant ObjectNav agent that must maintain coherent plans over long horizons does so by learning smooth, low-dimensional recurrent dynamics. Site: joel99.github.io/objectnav/},
  archive   = {C_ICCV},
  author    = {Joel Ye and Dhruv Batra and Abhishek Das and Erik Wijmans},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01581},
  pages     = {16097-16106},
  title     = {Auxiliary tasks and exploration enable ObjectGoal navigation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LookOut: Diverse multi-future prediction and planning for
self-driving. <em>ICCV</em>, 16087–16096. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present LOOKOUT, a novel autonomy system that perceives the environment, predicts a diverse set of futures of how the scene might unroll and estimates the trajectory of the SDV by optimizing a set of contingency plans over these future realizations. In particular, we learn a diverse joint distribution over multi-agent future trajectories in a traffic scene that covers a wide range of future modes with high sample efficiency while leveraging the expressive power of generative models. Unlike previous work in diverse motion forecasting, our diversity objective explicitly rewards sampling future scenarios that require distinct reactions from the self-driving vehicle for improved safety. Our contingency planner then finds comfortable and non-conservative trajectories that ensure safe reactions to a wide range of future scenarios. Through extensive evaluations, we show that our model demonstrates significantly more diverse and sample-efficient motion forecasting in a large-scale self-driving dataset as well as safer and less-conservative motion plans in long-term closed-loop simulations when compared to current state-of-the-art models.},
  archive   = {C_ICCV},
  author    = {Alexander Cui and Sergio Casas and Abbas Sadat and Renjie Liao and Raquel Urtasun},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01580},
  pages     = {16087-16096},
  title     = {LookOut: Diverse multi-future prediction and planning for self-driving},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). RAIN: Reinforced hybrid attention inference network for
motion forecasting. <em>ICCV</em>, 16076–16086. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion forecasting plays a significant role in various domains (e.g., autonomous driving, human-robot interaction), which aims to predict future motion sequences given a set of historical observations. However, the observed elements may be of different levels of importance. Some information may be irrelevant or even distracting to the forecasting in certain situations. To address this issue, we propose a generic motion forecasting framework (named RAIN) with dynamic key information selection and ranking based on a hybrid attention mechanism. The general framework is instantiated to handle multi-agent trajectory prediction and human motion forecasting tasks, respectively. In the former task, the model learns to recognize the relations between agents with a graph representation and to determine their relative significance. In the latter task, the model learns to capture the temporal proximity and dependency in long-term human motions. We also propose an effective double-stage training pipeline with an alternating training strategy to optimize the parameters in different modules of the framework. We validate the framework on both synthetic simulations and motion forecasting benchmarks in different domains, demonstrating that our method not only achieves state-of-the-art forecasting performance, but also provides interpretable and reasonable hybrid attention weights.},
  archive   = {C_ICCV},
  author    = {Jiachen Li and Fan Yang and Hengbo Ma and Srikanth Malla and Masayoshi Tomizuka and Chiho Choi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01579},
  pages     = {16076-16086},
  title     = {RAIN: Reinforced hybrid attention inference network for motion forecasting},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VolumeFusion: Deep depth fusion for 3D scene reconstruction.
<em>ICCV</em>, 16066–16075. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To reconstruct a 3D scene from a set of calibrated views, traditional multi-view stereo techniques rely on two distinct stages: local depth maps computation and global depth maps fusion. Recent studies concentrate on deep neural architectures for depth estimation by using conventional depth fusion method or direct 3D reconstruction network by regressing Truncated Signed Distance Function (TSDF). In this paper, we advocate that replicating the traditional two stages framework with deep neural networks improves both the interpretability and the accuracy of the results. As mentioned, our network operates in two steps: 1) the local computation of the local depth maps with a deep MVS technique, and, 2) the depth maps and images’ features fusion to build a single TSDF volume. In order to improve the matching performance between images acquired from very different viewpoints (e.g., large-baseline and rotations), we introduce a rotation-invariant 3D convolution kernel called PosedConv. The effectiveness of the proposed architecture is underlined via a large series of experiments conducted on the ScanNet dataset where our approach compares favorably against both traditional and deep learning techniques.},
  archive   = {C_ICCV},
  author    = {Jaesung Choe and Sunghoon Im and Francois Rameau and Minjun Kang and In So Kweon},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01578},
  pages     = {16066-16075},
  title     = {VolumeFusion: Deep depth fusion for 3D scene reconstruction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GP-S3Net: Graph-based panoptic sparse semantic segmentation
network. <em>ICCV</em>, 16056–16065. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Panoptic segmentation as an integrated task of both static environmental understanding and dynamic object identification, has recently begun to receive broad research interest. In this paper, we propose a new computationally efficient LiDAR based panoptic segmentation framework, called GP-S3Net. GP-S3Net is a proposal-free approach in which no object proposals are needed to identify the objects in contrast to conventional two-stage panoptic systems, where a detection network is incorporated for capturing instance information. Our new design consists of a novel instance-level network to process the semantic results by constructing a graph convolutional network to identify objects (foreground), which later on are fused with the back-ground classes. Through the fine-grained clusters of the foreground objects from the semantic segmentation back-bone, over-segmentation priors are generated and subsequently processed by 3D sparse convolution to embed each cluster. Each cluster is treated as a node in the graph and its corresponding embedding is used as its node feature. Then a GCNN predicts whether edges exist between each cluster pair. We utilize the instance label to generate ground truth edge labels for each constructed graph in order to supervise the learning. Extensive experiments demonstrate that GP-S3Net outperforms the current state-of-the-art approaches, by a significant margin across available datasets such as, nuScenes and SemanticPOSS, ranking 1 st on the competitive public SemanticKITTI leaderboard upon publication.},
  archive   = {C_ICCV},
  author    = {Ryan Razani and Ran Cheng and Enxu Li and Ehsan Taghavi and Yuan Ren and Liu Bingbing},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01577},
  pages     = {16056-16065},
  title     = {GP-S3Net: Graph-based panoptic sparse semantic segmentation network},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Variational attention: Propagating domain-specific knowledge
for multi-domain learning in crowd counting. <em>ICCV</em>, 16045–16055.
(<a href="https://doi.org/10.1109/ICCV48922.2021.01576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In crowd counting, due to the problem of laborious labelling, it is perceived intractability of collecting a new large-scale dataset which has plentiful images with large diversity in density, scene, etc. Thus, for learning a general model, training with data from multiple different datasets might be a remedy and be of great value. In this paper, we resort to the multi-domain joint learning and propose a simple but effective Domain-specific Knowledge Propagating Network (DKPNet) for unbiasedly learning the knowledge from multiple diverse data domains at the same time. It is mainly achieved by proposing the novel Variational Attention(VA) technique for explicitly modeling the attention distributions for different domains. And as an extension to VA, Intrinsic Variational Attention(InVA) is proposed to handle the problems of over-lapped domains and sub-domains. Extensive experiments have been conducted to validate the superiority of our DKPNet over several popular datasets, including ShanghaiTech A/B, UCF-QNRF and NWPU.},
  archive   = {C_ICCV},
  author    = {Binghui Chen and Zhaoyi Yan and Ke Li and Pengyu Li and Biao Wang and Wangmeng Zuo and Lei Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01576},
  pages     = {16045-16055},
  title     = {Variational attention: Propagating domain-specific knowledge for multi-domain learning in crowd counting},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Regularizing nighttime weirdness: Efficient self-supervised
monocular depth estimation in the dark. <em>ICCV</em>, 16035–16044. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Monocular depth estimation aims at predicting depth from a single image or video. Recently, self-supervised methods draw much attention since they are free of depth annotations and achieve impressive performance on several daytime benchmarks. However, they produce weird outputs in more challenging nighttime scenarios because of low visibility and varying illuminations, which bring weak textures and break brightness-consistency assumption, respectively. To address these problems, in this paper we propose a novel framework with several improvements: (1) we introduce Priors-Based Regularization to learn distribution knowledge from unpaired depth maps and prevent model from being incorrectly trained; (2) we leverage Mapping-Consistent Image Enhancement module to enhance image visibility and contrast while maintaining brightness consistency; and (3) we present Statistics-Based Mask strategy to tune the number of removed pixels within textureless regions, using dynamic statistics. Experimental results demonstrate the effectiveness of each component. Mean-while, our framework achieves remarkable improvements and state-of-the-art results on two nighttime datasets. Code is available at https://github.com/w2kun/RNW.},
  archive   = {C_ICCV},
  author    = {Kun Wang and Zhenyu Zhang and Zhiqiang Yan and Xiang Li and Baobei Xu and Jun Li and Jian Yang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01575},
  pages     = {16035-16044},
  title     = {Regularizing nighttime weirdness: Efficient self-supervised monocular depth estimation in the dark},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-supervised real-to-sim scene generation. <em>ICCV</em>,
16024–16034. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Synthetic data is emerging as a promising solution to the scalability issue of supervised deep learning, especially when real data are difficult to acquire or hard to annotate. Synthetic data generation, however, can itself be prohibitively expensive when domain experts have to manually and painstakingly oversee the process. More-over, neural networks trained on synthetic data often do not perform well on real data because of the domain gap. To solve these challenges, we propose Sim2SG, a self-supervised automatic scene generation technique for matching the distribution of real data. Importantly, Sim2SG does not require supervision from the real-world dataset, thus making it applicable in situations for which such annotations are difficult to obtain. Sim2SG is designed to bridge both the content and appearance gaps, by matching the content of real data, and by matching the features in the source and target domains. We select scene graph (SG) generation as the downstream task, due to the limited availability of labeled datasets. Experiments demonstrate significant improvements over leading baselines in reducing the domain gap both qualitatively and quantitatively, on several synthetic datasets as well as the real-world KITTI dataset.},
  archive   = {C_ICCV},
  author    = {Aayush Prakash and Shoubhik Debnath and Jean-Francois Lafleche and Eric Cameracci and Gavriel State and Stan Birchfield and Marc T. Law},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01574},
  pages     = {16024-16034},
  title     = {Self-supervised real-to-sim scene generation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MonteFloor: Extending MCTS for reconstructing accurate
large-scale floor plans. <em>ICCV</em>, 16014–16023. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel method for reconstructing floor plans from noisy 3D point clouds. Our main contribution is a principled approach that relies on the Monte Carlo Tree Search (MCTS) algorithm to maximize a suitable objective function efficiently despite the complexity of the problem. Like previous work, we first project the input point cloud to a top view to create a density map and extract room proposals from it. Our method selects and optimizes the polygonal shapes of these room proposals jointly to fit the density map and outputs an accurate vectorized floor map even for large complex scenes. To do this, we adapt MCTS, an algorithm originally designed to learn to play games, to select the room proposals by maximizing an objective function combining the fitness with the density map as predicted by a deep network and regularizing terms on the room shapes. We also introduce a refinement step to MCTS that adjusts the shape of the room proposals. For this step, we propose a novel differentiable method for rendering the polygonal shapes of these proposals. We evaluate our method on the recent and challenging Structured3D and Floor-SP datasets and show a significant improvement over the state-of-the-art, without imposing any hard constraints nor assumptions on the floor plan configurations.},
  archive   = {C_ICCV},
  author    = {Sinisa Stekovic and Mahdi Rad and Friedrich Fraundorfer and Vincent Lepetit},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01573},
  pages     = {16014-16023},
  title     = {MonteFloor: Extending MCTS for reconstructing accurate large-scale floor plans},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). RPVNet: A deep and efficient range-point-voxel fusion
network for LiDAR point cloud segmentation. <em>ICCV</em>, 16004–16013.
(<a href="https://doi.org/10.1109/ICCV48922.2021.01572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Point clouds can be represented in many forms (views), typically, point-based sets, voxel-based cells or range-based images(i.e., panoramic view). The point-based view is geometrically accurate, but it is disordered, which makes it difficult to find local neighbors efficiently. The voxel-based view is regular, but sparse, and computation grows cubicly when voxel resolution increases. The range-based view is regular and generally dense, however spherical projection makes physical dimensions distorted. Both voxel-and range-based views suffer from quantization loss, especially for voxels when facing large-scale scenes. In order to utilize different view’s advantages and alleviate their own shortcomings in fine-grained segmentation task, we propose a novel range-point-voxel fusion network, namely RPVNet. In this network, we devise a deep fusion framework with multiple and mutual information interactions among these three views, and propose a gated fusion module (termed as GFM), which can adaptively merge the three features based on concurrent inputs. Moreover, the proposed RPV interaction mechanism is highly efficient, and we summarize it to a more general formulation. By leveraging this efficient interaction and relatively lower voxel resolution, our method is also proved to be more efficient. Finally, we evaluated the proposed model on two large-scale datasets, i.e., SemanticKITTI and nuScenes, and it shows state-of-the-art performance on both of them. Note that, our method currently ranks 1st on SemanticKITTI leaderboard without any extra tricks.},
  archive   = {C_ICCV},
  author    = {Jianyun Xu and Ruixiang Zhang and Jian Dou and Yushi Zhu and Jie Sun and Shiliang Pu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01572},
  pages     = {16004-16013},
  title     = {RPVNet: A deep and efficient range-point-voxel fusion network for LiDAR point cloud segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HRegNet: A hierarchical network for large-scale outdoor
LiDAR point cloud registration. <em>ICCV</em>, 15994–16003. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Point cloud registration is a fundamental problem in 3D computer vision. Outdoor LiDAR point clouds are typically large-scale and complexly distributed, which makes the registration challenging. In this paper, we propose an efficient hierarchical network named HRegNet for large-scale out-door LiDAR point cloud registration. Instead of using all points in the point clouds, HRegNet performs registration on hierarchically extracted keypoints and descriptors. The overall framework combines the reliable features in deeper layer and the precise position information in shallower layers to achieve robust and precise registration. We present a correspondence network to generate correct and accurate keypoints correspondences. Moreover, bilateral consensus and neighborhood consensus are introduced for keypoints matching and novel similarity features are designed to in-corporate them into the correspondence network, which significantly improves the registration performance. Besides, the whole network is also highly efficient since only a small number of keypoints are used for registration. Extensive experiments are conducted on two large-scale outdoor LiDAR point cloud datasets to demonstrate the high accuracy and efficiency of the proposed HRegNet. The project website is https://ispc-group.github.io/hregnet.},
  archive   = {C_ICCV},
  author    = {Fan Lu and Guang Chen and Yinlong Liu and Lijun Zhang and Sanqing Qu and Shu Liu and Rongqi Gu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01571},
  pages     = {15994-16003},
  title     = {HRegNet: A hierarchical network for large-scale outdoor LiDAR point cloud registration},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). P2-net: Joint description and detection of local features
for pixel and point matching. <em>ICCV</em>, 15984–15993. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurately describing and detecting 2D and 3D key-points is crucial to establishing correspondences across images and point clouds. Despite a plethora of learning-based 2D or 3D local feature descriptors and detectors having been proposed, the derivation of a shared descriptor and joint keypoint detector that directly matches pixels and points remains under-explored by the community. This work takes the initiative to establish fine-grained correspondences between 2D images and 3D point clouds. In order to directly match pixels and points, a dual fully-convolutional framework is presented that maps 2D and 3D inputs into a shared latent representation space to simultaneously describe and detect keypoints. Furthermore, an ultra-wide reception mechanism and a novel loss function are designed to mitigate the intrinsic information variations between pixel and point local regions. Extensive experimental results demonstrate that our framework shows competitive performance in fine-grained matching between images and point clouds and achieves state-of-the-art results for the task of indoor visual localization. Our source code is available at https://github.com/BingCS/P2-Net.},
  archive   = {C_ICCV},
  author    = {Bing Wang and Changhao Chen and Zhaopeng Cui and Jie Qin and Chris Xiaoxuan Lu and Zhengdi Yu and Peijun Zhao and Zhen Dong and Fan Zhu and Niki Trigoni and Andrew Markham},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01570},
  pages     = {15984-15993},
  title     = {P2-net: Joint description and detection of local features for pixel and point matching},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Deep hough voting for robust global registration.
<em>ICCV</em>, 15974–15983. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Point cloud registration is the task of estimating the rigid transformation that aligns a pair of point cloud fragments. We present an efficient and robust framework for pairwise registration of real-world 3D scans, leveraging Hough voting in the 6D transformation parameter space. First, deep geometric features are extracted from a point cloud pair to compute putative correspondences. We then construct a set of triplets of correspondences to cast votes on the 6D Hough space, representing the transformation parameters in sparse tensors. Next, a fully convolutional refinement module is applied to refine the noisy votes. Finally, we identify the consensus among the correspondences from the Hough space, which we use to predict our final transformation parameters. Our method outperforms state-of-the-art methods on 3DMatch and 3DLoMatch benchmarks while achieving comparable performance on KITTI odometry dataset. We further demonstrate the generalizability of our approach by setting a new state-of-the-art on ICL-NUIM dataset, where we integrate our module into a multi-way registration pipeline.},
  archive   = {C_ICCV},
  author    = {Junha Lee and Seungwook Kim and Minsu Cho and Jaesik Park},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01569},
  pages     = {15974-15983},
  title     = {Deep hough voting for robust global registration},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploiting scene graphs for human-object interaction
detection. <em>ICCV</em>, 15964–15973. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human-Object Interaction (HOI) detection is a fundamental visual task aiming at localizing and recognizing interactions between humans and objects. Existing works focus on the visual and linguistic features of the humans and objects. However, they do not capitalise on the high-level and semantic relationships present in the image, which provides crucial contextual and detailed relational knowledge for HOI inference. We propose a novel method to exploit this information, through the scene graph, for the Human-Object Interaction (SG2HOI) detection task. Our method, SG2HOI, incorporates the SG information in two ways: (1) we embed a scene graph into a global context clue, serving as the scene-specific environmental context; and (2) we build a relation-aware message-passing module to gather relationships from objects&#39; neighborhood and transfer them into interactions. Empirical evaluation shows that our SG2HOI method outperforms the state-of-the-art methods on two benchmark HOI datasets: V-COCO and HICO-DET. Code will be available at https://github.com/ht014/SG2HOI.},
  archive   = {C_ICCV},
  author    = {Tao He and Lianli Gao and Jingkuan Song and Yuan-Fang Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01568},
  pages     = {15964-15973},
  title     = {Exploiting scene graphs for human-object interaction detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pose correction for highly accurate visual localization in
large-scale indoor spaces. <em>ICCV</em>, 15954–15963. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Indoor visual localization is significant for various applications such as autonomous robots, augmented reality, and mixed reality. Recent advances in visual localization have demonstrated their feasibility in large-scale indoor spaces through coarse-to-fine methods that typically employ three steps: image retrieval, pose estimation, and pose selection. However, further research is needed to improve the accuracy of large-scale indoor visual localization. We demonstrate that the limitations in the previous methods can be attributed to the sparsity of image positions in the database, which causes view-differences between a query and a retrieved image from the database. In this paper, to address this problem, we propose a novel module, named pose correction, that enables re-estimation of the pose with local feature matching in a similar view by reorganizing the local features. This module enhances the accuracy of the initially estimated pose and assigns more reliable ranks. Furthermore, the proposed method achieves a new state-of-the-art performance with an accuracy of more than 90\% within 1.0 m in the challenging indoor benchmark dataset InLoc for the first time. 1},
  archive   = {C_ICCV},
  author    = {Janghun Hyeon and Joohyung Kim and Nakju Doh},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01567},
  pages     = {15954-15963},
  title     = {Pose correction for highly accurate visual localization in large-scale indoor spaces},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graspness discovery in clutters for fast and accurate grasp
detection. <em>ICCV</em>, 15944–15953. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Efficient and robust grasp pose detection is vital for robotic manipulation. For general 6 DoF grasping, conventional methods treat all points in a scene equally and usually adopt uniform sampling to select grasp candidates. However, we discover that ignoring where to grasp greatly harms the speed and accuracy of current grasp pose detection methods. In this paper, we propose &quot;graspness&quot;, a quality based on geometry cues that distinguishes graspable area in cluttered scenes. A look-ahead searching method is proposed for measuring the graspness and statistical results justify the rationality of our method. To quickly detect graspness in practice, we develop a neural network named graspness model to approximate the searching process. Extensive experiments verify the stability, generality and effectiveness of our graspness model, allowing it to be used as a plug-and-play module for different methods. A large improvement in accuracy is witnessed for various previous methods after equipping our graspness model. Moreover, we develop GSNet, an end-to-end network that incorporate our graspness model for early filtering of low quality predictions. Experiments on a large scale benchmark, GraspNet-1Billion, show that our method outperforms previous arts by a large margin (30 + AP) and achieves a high inference speed.},
  archive   = {C_ICCV},
  author    = {Chenxi Wang and Hao-Shu Fang and Minghao Gou and Hongjie Fang and Jin Gao and Cewu Lu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01566},
  pages     = {15944-15953},
  title     = {Graspness discovery in clutters for fast and accurate grasp detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interpretation of emergent communication in heterogeneous
collaborative embodied agents. <em>ICCV</em>, 15933–15943. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Communication between embodied AI agents has received increasing attention in recent years. Despite its use, it is still unclear whether the learned communication is interpretable and grounded in perception. To study the grounding of emergent forms of communication, we first introduce the collaborative multi-object navigation task ‘CoMON.&#39; In this task, an ‘oracle agent&#39; has detailed environment information in the form of a map. It communicates with a ‘navigator agent&#39; that perceives the environment visually and is tasked to find a sequence of goals. To succeed at the task, effective communication is essential. CoMON hence serves as a basis to study different communication mechanisms between heterogeneous agents, that is, agents with different capabilities and roles. We study two common communication mechanisms and analyze their communication patterns through an egocentric and spatial lens. We show that the emergent communication can be grounded to the agent observations and the spatial structure of the 3D environment.},
  archive   = {C_ICCV},
  author    = {Shivansh Patel and Saim Wani and Unnat Jain and Alexander Schwing and Svetlana Lazebnik and Manolis Savva and Angel X. Chang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01565},
  pages     = {15933-15943},
  title     = {Interpretation of emergent communication in heterogeneous collaborative embodied agents},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Episodic transformer for vision-and-language navigation.
<em>ICCV</em>, 15922–15932. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Interaction and navigation defined by natural language instructions in dynamic environments pose significant challenges for neural agents. This paper focuses on addressing two challenges: handling long sequence of subtasks, and understanding complex human instructions. We propose Episodic Transformer (E.T.), a multimodal transformer that encodes language inputs and the full episode history of visual observations and actions. To improve training, we leverage synthetic instructions as an intermediate representation that decouples understanding the visual appearance of an environment from the variations of natural language instructions. We demonstrate that encoding the history with a transformer is critical to solve compositional tasks, and that pretraining and joint training with synthetic instructions further improve the performance. Our approach sets a new state of the art on the challenging ALFRED benchmark, achieving 38.4\% and 8.5\% task success rates on seen and unseen test splits.},
  archive   = {C_ICCV},
  author    = {Alexander Pashevich and Cordelia Schmid and Chen Sun},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01564},
  pages     = {15922-15932},
  title     = {Episodic transformer for vision-and-language navigation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Context-aware scene graph generation with Seq2Seq
transformers. <em>ICCV</em>, 15911–15921. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Scene graph generation is an important task in computer vision aimed at improving the semantic understanding of the visual world. In this task, the model needs to detect objects and predict visual relationships between them. Most of the existing models predict relationships in parallel assuming their independence. While there are different ways to capture these dependencies, we explore a conditional approach motivated by the sequence-to-sequence (Seq2Seq) formalism. Different from the previous research, our proposed model predicts visual relationships one at a time in an autoregressive manner by explicitly conditioning on the already predicted relationships. Drawing from translation models in NLP, we propose an encoder-decoder model built using Transformers where the encoder captures global context and long range interactions. The decoder then makes sequential predictions by conditioning on the scene graph constructed so far. In addition, we introduce a novel reinforcement learning-based training strategy tailored to Seq2Seq scene graph generation. By using a self-critical policy gradient training approach with Monte Carlo search we directly optimize for the (mean) recall metrics and bridge the gap between training and evaluation. Experimental results on two public benchmark datasets demonstrate that our Seq2Seq learning approach achieves strong empirical performance, outperforming previous state-of-the-art, while remaining efficient in terms of training and inference time. Full code for this work is available here: https://github.com/layer6ai-labs/SGG-Seq2Seq.},
  archive   = {C_ICCV},
  author    = {Yichao Lu and Himanshu Rai and Jason Chang and Boris Knyazev and Guangwei Yu and Shashank Shekhar and Graham W. Taylor and Maksims Volkovs},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01563},
  pages     = {15911-15921},
  title     = {Context-aware scene graph generation with Seq2Seq transformers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploring long tail visual relationship recognition with
large vocabulary. <em>ICCV</em>, 15901–15910. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Several approaches have been proposed in recent literature to alleviate the long-tail problem, mainly in object classification tasks. In this paper, we make the first largescale study concerning the task of Long-Tail Visual Relationship Recognition (LTVRR). LTVRR aims at improving the learning of structured visual relationships that come from the long-tail (e.g., &quot;rabbit grazing on grass&quot;). In this setup, the subject, relation, and object classes each follow a long-tail distribution. To begin our study and make a future benchmark for the community, we introduce two LTVRR-related benchmarks, dubbed VG8K-LT and GQA-LT, built upon the widely used Visual Genome and GQA datasets. We use these benchmarks to study the performance of several state-of-the-art long-tail models on the LTVRR setup. Lastly, we propose a visiolinguistic hubless (VilHub) loss and a Mixup augmentation technique adapted to LTVRR setup, dubbed as RelMix. Both VilHub and RelMix can be easily integrated on top of existing models and despite being simple, our results show that they can remarkably improve the performance, especially on tail classes. Benchmarks, code, and models have been made available at: https://github.com/Vision-CAIR/LTVRR.},
  archive   = {C_ICCV},
  author    = {Sherif Abdelkarim and Aniket Agarwal and Panos Achlioptas and Jun Chen and Jiaji Huang and Boyang Li and Kenneth Church and Mohamed Elhoseiny},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01562},
  pages     = {15901-15910},
  title     = {Exploring long tail visual relationship recognition with large vocabulary},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Grounding consistency: Distilling spatial common sense for
precise visual relationship detection. <em>ICCV</em>, 15891–15900. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Scene Graph Generators (SGGs) are models that, given an image, build a directed graph where each edge represents a predicted subject predicate object triplet. Most SGGs silently exploit datasets&#39; bias on relationships&#39; context, i.e. its subject and object, to improve recall and neglect spatial and visual evidence, e.g. having seen a glut of data for person wearing shirt, they are overconfident that every person is wearing every shirt. Such imprecise predictions are mainly ascribed to the lack of negative examples for most relationships, which obstructs models from meaningfully learning predicates, even those that have ample positive examples. We first present an indepth investigation of the context bias issue to showcase that all examined state-of-the-art SGGs share the above vulnerabilities. In response, we propose a semi-supervised scheme that forces predicted triplets to be grounded consistently back to the image, in a closed-loop manner. The developed spatial common sense can be then distilled to a student SGG and substantially enhance its spatial reasoning ability. This Grounding Consistency Distillation (GCD) approach is model-agnostic and benefits from the superfluous unlabeled samples to retain the valuable context information and avert memorization of annotations. Furthermore, we demonstrate that current metrics disregard unlabeled samples, rendering themselves incapable of reflecting context bias, then we mine and incorporate during evaluation hard-negatives to reformulate precision as a reliable metric. Extensive experimental comparisons exhibit large quantitative - up to 70\% relative precision boost on VG200 dataset - and qualitative improvements to prove the significance of our GCD method and our metrics towards refocusing graph generation as a core aspect of scene understanding. Code available at https://github.com/deeplab-ai/grounding-consistent-vrd.},
  archive   = {C_ICCV},
  author    = {Markos Diomataris and Nikolaos Gkanatsios and Vassilis Pitsikalis and Petros Maragos},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01561},
  pages     = {15891-15900},
  title     = {Grounding consistency: Distilling spatial common sense for precise visual relationship detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Topic scene graph generation by attention distillation from
caption. <em>ICCV</em>, 15880–15890. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {If an image tells a story, the image caption is the briefest narrator. Generally, a scene graph prefers to be an omniscient &quot;generalist&quot;, while the image caption is more willing to be a &quot;specialist&quot;, which outlines the gist. Lots of previous studies have found that a scene graph is not as practical as expected unless it can reduce the trivial contents and noises. In this respect, the image caption is a good tutor. To this end, we let the scene graph borrow the ability from the image caption so that it can be a specialist on the basis of remaining all-around, resulting in the socalled Topic Scene Graph. What an image caption pays attention to is distilled and passed to the scene graph for estimating the importance of partial objects, relationships, and events. Specifically, during the caption generation, the attention about individual objects in each time step is collected, pooled, and assembled to obtain the attention about relationships, which serves as weak supervision for regularizing the estimated importance scores of relationships. In addition, as this attention distillation process provides an opportunity for combining the generation of image caption and scene graph together, we further transform the scene graph into linguistic form with rich and free-form expressions by sharing a single generation model with image caption. Experiments show that attention distillation brings significant improvements in mining important relationships without strong supervision, and the topic scene graph shows great potential in subsequent applications.},
  archive   = {C_ICCV},
  author    = {Wenbin Wang and Ruiping Wang and Xilin Chen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01560},
  pages     = {15880-15890},
  title     = {Topic scene graph generation by attention distillation from caption},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual graph memory with unsupervised representation for
visual navigation. <em>ICCV</em>, 15870–15879. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel graph-structured memory for visual navigation, called visual graph memory (VGM), which consists of unsupervised image representations obtained from navigation history. The proposed VGM is constructed incrementally based on the similarities among the unsupervised representations of observed images, and these representations are learned from an unlabeled image dataset. We also propose a navigation framework that can utilize the proposed VGM to tackle visual navigation problems. By incorporating a graph convolutional network and the attention mechanism, the proposed agent refers to the VGM to navigate the environment while simultaneously building the VGM. Using the VGM, the agent can embed its navigation history and other useful task-related information. We validate our approach on the visual navigation tasks using the Habitat simulator with the Gibson dataset, which provides a photo-realistic simulation environment. The extensive experimental results show that the proposed navigation agent with VGM surpasses the state-of-the-art approaches on image-goal navigation tasks. Project Page: https://sites.google.com/view/iccv2021vgm},
  archive   = {C_ICCV},
  author    = {Obin Kwon and Nuri Kim and Yunho Choi and Hwiyeon Yoo and Jeongho Park and Songhwai Oh},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01559},
  pages     = {15870-15879},
  title     = {Visual graph memory with unsupervised representation for visual navigation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Segmentation-grounded scene graph generation. <em>ICCV</em>,
15859–15869. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Scene graph generation has emerged as an important problem in computer vision. While scene graphs provide a grounded representation of objects, their locations and relations in an image, they do so only at the granularity of proposal bounding boxes. In this work, we propose the first, to our knowledge, framework for pixel-level segmentation-grounded scene graph generation. Our framework is agnostic to the underlying scene graph generation method and address the lack of segmentation annotations in target scene graph datasets (e.g., Visual Genome [24]) through transfer and multi-task learning from, and with, an auxiliary dataset (e.g., MS COCO [29]). Specifically, each target object being detected is endowed with a segmentation mask, which is expressed as a lingual-similarity weighted linear combination over categories that have annotations present in an auxiliary dataset. These inferred masks, along with a Gaussian masking mechanism which grounds the relations at a pixel-level within the image, allow for improved relation prediction. The entire framework is end-to-end trainable and is learned in a multi-task manner. Code is available at github.com/ubc-vision/segmentation-sg.},
  archive   = {C_ICCV},
  author    = {Siddhesh Khandelwal and Mohammed Suhail and Leonid Sigal},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01558},
  pages     = {15859-15869},
  title     = {Segmentation-grounded scene graph generation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploring relational context for multi-task dense
prediction. <em>ICCV</em>, 15849–15858. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The timeline of computer vision research is marked with advances in learning and utilizing efficient contextual representations. Most of them, however, are targeted at improving model performance on a single downstream task. We consider a multi-task environment for dense prediction tasks, represented by a common backbone and independent task-specific heads. Our goal is to find the most efficient way to refine each task prediction by capturing cross-task contexts dependent on tasks’ relations. We explore various attention-based contexts, such as global and local, in the multi-task setting and analyze their behavior when applied to refine each task independently. Empirical findings confirm that different source-target task pairs benefit from different context types. To automate the selection process, we propose an Adaptive Task-Relational Context (ATRC) module, which samples the pool of all available contexts for each task pair using neural architecture search and outputs the optimal configuration for deployment. Our method achieves state-of-the-art performance on two important multi-task benchmarks, namely NYUD-v2 and PASCAL-Context. The proposed ATRC has a low computational toll and can be used as a drop-in refinement module for any supervised multi-task architecture.},
  archive   = {C_ICCV},
  author    = {David Brüggemann and Menelaos Kanakis and Anton Obukhov and Stamatios Georgoulis and Luc Van Gool},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01557},
  pages     = {15849-15858},
  title     = {Exploring relational context for multi-task dense prediction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enhanced boundary learning for glass-like object
segmentation. <em>ICCV</em>, 15839–15848. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Glass-like objects such as windows, bottles, and mirrors exist widely in the real world. Sensing these objects has many applications, including robot navigation and grasping. However, this task is very challenging due to the arbitrary scenes behind glass-like objects. This paper aims to solve the glass-like object segmentation problem via enhanced boundary learning. In particular, we first propose a novel refined differential module that outputs finer boundary cues. We then introduce an edge-aware point-based graph convolution network module to model the global shape along the boundary. We use these two modules to design a decoder that generates accurate and clean segmentation results, especially on the object contours. Both modules are lightweight and effective: they can be embedded into various segmentation models. In extensive experiments on three recent glass-like object segmentation datasets, including Trans10k, MSD, and GDD, our approach establishes new state-of-the-art results. We also illustrate the strong generalization properties of our method on three generic segmentation datasets, including Cityscapes, BDD, and COCO Stuff. Code and models will be available for further research.},
  archive   = {C_ICCV},
  author    = {Hao He and Xiangtai Li and Guangliang Cheng and Jianping Shi and Yunhai Tong and Gaofeng Meng and Véronique Prinet and LuBin Weng},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01556},
  pages     = {15839-15848},
  title     = {Enhanced boundary learning for glass-like object segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interaction via bi-directional graph of semantic region
affinity for scene parsing. <em>ICCV</em>, 15828–15838. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we devote to address the challenging problem of scene parsing. It is well known that pixels in an image are highly correlated with each other, especially those from the same semantic region, while treating pixels independently fails to take advantage of such correlations. In this work, we treat each respective region in an image as a whole, and capture the structure topology as well as the affinity among different regions. To this end, we first divide the entire feature maps to different regions and extract respective global features from them. Next, we construct a directed graph whose nodes are regional features, and the bi-directional edges connecting every two nodes are the affinities between the regional features they represent. After that, we transfer the affinity-aware nodes in the directed graph back to corresponding regions of the image, which helps to model the region dependencies and mitigate unrealistic results. In addition, to further boost the correlation among pixels, we propose a region-level loss that evaluates all pixels in a region as a whole and motivates the network to learn the exclusive regional feature per class. With the proposed approach, we achieves new state-of-the-art segmentation results on PASCAL-Context, ADE20K, and COCO-Stuff consistently.},
  archive   = {C_ICCV},
  author    = {Henghui Ding and Hui Zhang and Jun Liu and Jiaxin Li and Zijian Feng and Xudong Jiang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01555},
  pages     = {15828-15838},
  title     = {Interaction via bi-directional graph of semantic region affinity for scene parsing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). In-place scene labelling and understanding with implicit
scene representation. <em>ICCV</em>, 15818–15827. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Semantic labelling is highly correlated with geometry and radiance reconstruction, as scene entities with similar shape and appearance are more likely to come from similar classes. Recent implicit neural reconstruction techniques are appealing as they do not require prior training data, but the same fully self-supervised approach is not possible for semantics because labels are human-defined properties.We extend neural radiance fields (NeRF) to jointly encode semantics with appearance and geometry, so that complete and accurate 2D semantic labels can be achieved using a small amount of in-place annotations specific to the scene. The intrinsic multi-view consistency and smoothness of NeRF benefit semantics by enabling sparse labels to efficiently propagate. We show the benefit of this approach when labels are either sparse or very noisy in room-scale scenes. We demonstrate its advantageous properties in various interesting applications such as an efficient scene labelling tool, novel semantic view synthesis, label denoising, super-resolution, label interpolation and multi-view semantic label fusion in visual semantic mapping systems.},
  archive   = {C_ICCV},
  author    = {Shuaifeng Zhi and Tristan Laidlow and Stefan Leutenegger and Andrew J. Davison},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01554},
  pages     = {15818-15827},
  title     = {In-place scene labelling and understanding with implicit scene representation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generative compositional augmentations for scene graph
prediction. <em>ICCV</em>, 15807–15817. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inferring objects and their relationships from an image in the form of a scene graph is useful in many applications at the intersection of vision and language. We consider a challenging problem of compositional generalization that emerges in this task due to a long tail data distribution. Current scene graph generation models are trained on a tiny fraction of the distribution corresponding to the most frequent compositions, e.g. . However, test images might contain zero- and few-shot compositions of objects and relationships, e.g. . Despite each of the object categories and the predicate (e.g. ‘on’) being frequent in the training data, the models often fail to properly understand such unseen or rare compositions. To improve generalization, it is natural to attempt increasing the diversity of the training distribution. However, in the graph domain this is non-trivial. To that end, we propose a method to synthesize rare yet plausible scene graphs by perturbing real ones. We then propose and empirically study a model based on conditional generative adversarial networks (GANs) that allows us to generate visual features of perturbed scene graphs and learn from them in a joint fashion. When evaluated on the Visual Genome dataset, our approach yields marginal, but consistent improvements in zero- and few-shot metrics. We analyze the limitations of our approach indicating promising directions for future research.},
  archive   = {C_ICCV},
  author    = {Boris Knyazev and Harm de Vries and Cătălina Cangea and Graham W. Taylor and Aaron Courville and Eugene Belilovsky},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01553},
  pages     = {15807-15817},
  title     = {Generative compositional augmentations for scene graph prediction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual distant supervision for scene graph generation.
<em>ICCV</em>, 15796–15806. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Scene graph generation aims to identify objects and their relations in images, providing structured image representations that can facilitate numerous applications in computer vision. However, scene graph models usually require supervised learning on large quantities of labeled data with intensive human annotation. In this work, we propose visual distant supervision, a novel paradigm of visual relation learning, which can train scene graph models without any human-labeled data. The intuition is that by aligning commonsense knowledge bases and images, we can automatically create large-scale labeled data to provide distant supervision for visual relation learning. To alleviate the noise in distantly labeled data, we further propose a framework that iteratively estimates the probabilistic relation labels and eliminates the noisy ones. Comprehensive experimental results show that our distantly supervised model outperforms strong weakly supervised and semi-supervised baselines. By further incorporating human-labeled data in a semi-supervised fashion, our model outperforms state-of-the-art fully supervised models by a large margin (e.g., 8.3 micro- and 7.8 macro-recall@50 improvements for predicate classification in Visual Genome evaluation). We make the data and code for this paper publicly available at https://github.com/thunlp/VisualDS.},
  archive   = {C_ICCV},
  author    = {Yuan Yao and Ao Zhang and Xu Han and Mengdi Li and Cornelius Weber and Zhiyuan Liu and Stefan Wermter and Maosong Sun},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01552},
  pages     = {15796-15806},
  title     = {Visual distant supervision for scene graph generation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MGNet: Monocular geometric scene understanding for
autonomous driving. <em>ICCV</em>, 15784–15795. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce MGNet, a multi-task framework for monocular geometric scene understanding. We define monocular geometric scene understanding as the combination of two known tasks: Panoptic segmentation and self-supervised monocular depth estimation. Panoptic segmentation captures the full scene not only semantically, but also on an instance basis. Self-supervised monocular depth estimation uses geometric constraints derived from the camera measurement model in order to measure depth from monocular video sequences only. To the best of our knowledge, we are the first to propose the combination of these two tasks in one single model. Our model is designed with focus on low latency to provide fast inference in real-time on a single consumer-grade GPU. During deployment, our model produces dense 3D point clouds with instance aware semantic labels from single high-resolution camera images. We evaluate our model on two popular autonomous driving benchmarks, i.e., Cityscapes and KITTI, and show competitive performance among other real-time capable methods. Source code is available at https://github.com/markusschoen/MGNet.},
  archive   = {C_ICCV},
  author    = {Markus Schön and Michael Buchholz and Klaus Dietmayer},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01551},
  pages     = {15784-15795},
  title     = {MGNet: Monocular geometric scene understanding for autonomous driving},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NEAT: Neural attention fields for end-to-end autonomous
driving. <em>ICCV</em>, 15773–15783. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Efficient reasoning about the semantic, spatial, and temporal structure of a scene is a crucial prerequisite for autonomous driving. We present NEural ATtention fields (NEAT), a novel representation that enables such reasoning for end-to-end imitation learning models. NEAT is a continuous function which maps locations in Bird’s Eye View (BEV) scene coordinates to waypoints and semantics, using intermediate attention maps to iteratively compress high-dimensional 2D image features into a compact representation. This allows our model to selectively attend to relevant regions in the input while ignoring information irrelevant to the driving task, effectively associating the images with the BEV representation. In a new evaluation setting involving adverse environmental conditions and challenging scenarios, NEAT outperforms several strong baselines and achieves driving scores on par with the privileged CARLA expert used to generate its training data. Furthermore, visualizing the attention maps for models with NEAT intermediate representations provides improved interpretability.},
  archive   = {C_ICCV},
  author    = {Kashyap Chitta and Aditya Prakash and Andreas Geiger},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01550},
  pages     = {15773-15783},
  title     = {NEAT: Neural attention fields for end-to-end autonomous driving},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Continual neural mapping: Learning an implicit scene
representation from sequential observations. <em>ICCV</em>, 15762–15772.
(<a href="https://doi.org/10.1109/ICCV48922.2021.01549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances have enabled a single neural network to serve as an implicit scene representation, establishing the mapping function between spatial coordinates and scene properties. In this paper, we make a further step towards continual learning of the implicit scene representation directly from sequential observations, namely Continual Neural Mapping. The proposed problem setting bridges the gap between batch-trained implicit neural representations and commonly used streaming data in robotics and vision communities. We introduce an experience replay approach to tackle an exemplary task of continual neural mapping: approximating a continuous signed distance function (SDF) from sequential depth images as a scene geometry representation. We show for the first time that a single network can represent scene geometry over time continually without catastrophic forgetting, while achieving promising trade-offs between accuracy and efficiency.},
  archive   = {C_ICCV},
  author    = {Zike Yan and Yuxin Tian and Xuesong Shi and Ping Guo and Peng Wang and Hongbin Zha},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01549},
  pages     = {15762-15772},
  title     = {Continual neural mapping: Learning an implicit scene representation from sequential observations},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The functional correspondence problem. <em>ICCV</em>,
15752–15761. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to find correspondences in visual data is the essence of most computer vision tasks. But what are the right correspondences? The task of visual correspondence is well defined for two different images of same object instance. In case of two images of objects belonging to same category, visual correspondence is reasonably well-defined in most cases. But what about correspondence between two objects of completely different category – e.g., a shoe and a bottle? Does there exist any correspondence? Inspired by humans’ ability to: (a) generalize beyond semantic categories and; (b) infer functional affordances, we introduce the problem of functional correspondences in this paper. Given images of two objects, we ask a simple question: what is the set of correspondences between these two images for a given task? For example, what are the correspondences between a bottle and shoe for the task of pounding or the task of pouring. We introduce a new dataset: FunKPoint that has ground truth correspondences for 10 tasks and 20 object categories. We also introduce a modular task-driven representation for attacking this problem and demonstrate that our learned representation is effective for this task. But most importantly, because our supervision signal is not bound by semantics, we show that our learned representation can generalize better on few-shot classification problem. We hope this paper will inspire our community to think beyond semantics and focus more on cross-category generalization and learning representations for robotics tasks.},
  archive   = {C_ICCV},
  author    = {Zihang Lai and Senthil Purushwalkam and Abhinav Gupta},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01548},
  pages     = {15752-15761},
  title     = {The functional correspondence problem},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). H2O: A benchmark for visual human-human object handover
analysis. <em>ICCV</em>, 15742–15751. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Object handover is a common human collaboration behavior that attracts attention from researchers in Robotics and Cognitive Science. Though visual perception plays an important role in the object handover task, the whole handover process has not been specifically explored. In this work, we propose a novel rich-annotated dataset, H2O, for visual analysis of human-human object handovers. The H2O, which contains 18K video clips involving 15 people who hand over 30 objects to each other, is a multi-purpose benchmark. It can support several vision-based tasks, from which, we specifically provide a baseline method, RGPNet, for a less-explored task named Receiver Grasp Prediction. Extensive experiments show that the RGPNet can produce plausible grasps based on the giver’s hand-object states in the pre-handover phase. Besides, we also report the hand and object pose errors with existing baselines and show that the dataset can serve as the video demonstrations for robot imitation learning on the handover task.},
  archive   = {C_ICCV},
  author    = {Ruolin Ye and Wenqiang Xu and Zhendong Xue and Tutian Tang and Yanfeng Wang and Cewu Lu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01547},
  pages     = {15742-15751},
  title     = {H2O: A benchmark for visual human-human object handover analysis},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Act the part: Learning interaction strategies for
articulated object part discovery. <em>ICCV</em>, 15732–15741. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {People often use physical intuition when manipulating articulated objects, irrespective of object semantics. Motivated by this observation, we identify an important embodied task where an agent must play with objects to recover their parts. To this end, we introduce Act the Part (AtP) to learn how to interact with articulated objects to discover and segment their pieces. By coupling action selection and motion segmentation, AtP is able to isolate structures to make perceptual part recovery possible without semantic labels. Our experiments show AtP learns efficient strategies for part discovery, can generalize to unseen categories, and is capable of conditional reasoning for the task. Although trained in simulation, we show convincing transfer to real world data with no fine-tuning. A summery video, interactive demo, and code will be available at https://atp.cs.columbia.edu.},
  archive   = {C_ICCV},
  author    = {Samir Yitzhak Gadre and Kiana Ehsani and Shuran Song},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01546},
  pages     = {15732-15741},
  title     = {Act the part: Learning interaction strategies for articulated object part discovery},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toward human-like grasp: Dexterous grasping via semantic
representation of object-hand. <em>ICCV</em>, 15721–15731. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, many dexterous robotic hands have been designed to assist or replace human hands in executing various tasks. But how to teach them to perform dexterous operations like human hands is still a challenging task. In this paper, we propose a grasp synthesis framework to make robots grasp and manipulate objects like human beings. We first build a dataset by accurately segmenting the functional areas of the object and annotating semantic touch code for each functional area to guide the dexterous hand to complete the functional grasp and post-grasp manipulation. This dataset contains 18 categories of 129 objects selected from four datasets, and 15 people participated in data annotation. Then we carefully design four loss functions to constrain the model, which successfully generates the functional grasp of dexterous hand under the guidance of semantic touch code. The thorough experiments in synthetic data show our model can robustly generate functional grasp, even for objects that the model has not see before.},
  archive   = {C_ICCV},
  author    = {Tianqiang Zhu and Rina Wu and Xiangbo Lin and Yi Sun},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01545},
  pages     = {15721-15731},
  title     = {Toward human-like grasp: Dexterous grasping via semantic representation of object-hand},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Safety-aware motion prediction with unseen vehicles for
autonomous driving. <em>ICCV</em>, 15711–15720. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion prediction of vehicles is critical but challenging due to the uncertainties in complex environments and the limited visibility caused by occlusions and limited sensor ranges. In this paper, we study a new task, safety-aware motion prediction with unseen vehicles for autonomous driving. Unlike the existing trajectory prediction task for seen vehicles, we aim at predicting an occupancy map that indicates the earliest time when each location can be occupied by either seen and unseen vehicles. The ability to predict unseen vehicles is critical for safety in autonomous driving. To tackle this challenging task, we propose a safety-aware deep learning model with three new loss functions to predict the earliest occupancy map. Experiments on the large-scale autonomous driving nuScenes dataset show that our proposed model significantly outperforms the state-of-the-art baselines on the safety-aware motion prediction task. To the best of our knowledge, our approach is the first one that can predict the existence of unseen vehicles in most cases. Project page at https://github.com/xrenaa/Safety-Aware-Motion-Prediction.},
  archive   = {C_ICCV},
  author    = {Xuanchi Ren and Tao Yang and Li Erran Li and Alexandre Alahi and Qifeng Chen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01544},
  pages     = {15711-15720},
  title     = {Safety-aware motion prediction with unseen vehicles for autonomous driving},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learnable boundary guided adversarial training.
<em>ICCV</em>, 15701–15710. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Previous adversarial training raises model robustness under the compromise of accuracy on natural data. In this paper, we reduce natural accuracy degradation. We use the model logits from one clean model to guide learning of another one robust model, taking into consideration that logits from the well trained clean model embed the most discriminative features of natural data, e.g., generalizable classifier boundary. Our solution is to constrain logits from the robust model that takes adversarial examples as input and makes it similar to those from the clean model fed with corresponding natural data. It lets the robust model inherit the classifier boundary of the clean model. Moreover, we observe such boundary guidance can not only preserve high natural accuracy but also benefit model robustness, which gives new insights and facilitates progress for the adversarial community. Finally, extensive experiments on CIFAR-10, CIFAR-100, and Tiny ImageNet testify to the effectiveness of our method. We achieve new state-of-the-art robustness on CIFAR-100 without additional real or synthetic data with auto-attack benchmark 1 . Our code is available at https://github.com/dvlab-research/LBGAT.},
  archive   = {C_ICCV},
  author    = {Jiequan Cui and Shu Liu and Liwei Wang and Jiaya Jia},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01543},
  pages     = {15701-15710},
  title     = {Learnable boundary guided adversarial training},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robustness and generalization via generative adversarial
training. <em>ICCV</em>, 15691–15700. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While deep neural networks have achieved remarkable success in various computer vision tasks, they often fail to generalize to new domains and subtle variations of input images. Several defenses have been proposed to improve the robustness against these variations. However, current defenses can only withstand the specific attack used in training, and the models often remain vulnerable to other input variations. Moreover, these methods often degrade performance of the model on clean images and do not generalize to out-of-domain samples. In this paper we present Generative Adversarial Training, an approach to simultaneously improve the model’s generalization to the test set and out-of-domain samples as well as its robustness to unseen adversarial attacks. Instead of altering a low-level pre-defined aspect of images, we generate a spectrum of low-level, mid-level and high-level changes using generative models with a disentangled latent space. Adversarial training with these examples enable the model to withstand a wide range of attacks by observing a variety of input alterations during training. We show that our approach not only improves performance of the model on clean images and out-of-domain samples but also makes it robust against unforeseen attacks and outperforms prior work. We validate effectiveness of our method by demonstrating results on various tasks such as classification, segmentation and object detection.},
  archive   = {C_ICCV},
  author    = {Omid Poursaeed and Tianxing Jiang and Harry Yang and Serge Belongie and Ser-Nam Lim},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01542},
  pages     = {15691-15700},
  title     = {Robustness and generalization via generative adversarial training},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Triggering failures: Out-of-distribution detection by
learning from local adversarial attacks in semantic segmentation.
<em>ICCV</em>, 15681–15690. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we tackle the detection of out-of-distribution (OOD) objects in semantic segmentation. By analyzing the literature, we found that current methods are either accurate or fast but not both which limits their usability in real world applications. To get the best of both aspects, we propose to mitigate the common shortcomings by following four design principles: decoupling the OOD detection from the segmentation task, observing the entire segmentation network instead of just its output, generating training data for the OOD detector by leveraging blind spots in the segmentation network and focusing the generated data on localized regions in the image to simulate OOD objects. Our main contribution is a new OOD detection architecture called ObsNet associated with a dedicated training scheme based on Local Adversarial Attacks (LAA). We validate the soundness of our approach across numerous ablation studies. We also show it obtains top performances both in speed and accuracy when compared to ten recent methods of the literature on three different datasets.},
  archive   = {C_ICCV},
  author    = {Victor Besnier and Andrei Bursuc and David Picard and Alexandre Briot},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01541},
  pages     = {15681-15690},
  title     = {Triggering failures: Out-of-distribution detection by learning from local adversarial attacks in semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RobustNav: Towards benchmarking robustness in embodied
navigation. <em>ICCV</em>, 15671–15680. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As an attempt towards assessing the robustness of embodied navigation agents, we propose RobustNav, a framework to quantify the performance of embodied navigation agents when exposed to a wide variety of visual – affecting RGB inputs – and dynamics – affecting transition dynamics – corruptions. Most recent efforts in visual navigation have typically focused on generalizing to novel target environments with similar appearance and dynamics characteristics. With RobustNav, we find that some standard embodied navigation agents significantly underperform (or fail) in the presence of visual or dynamics corruptions. We systematically analyze the kind of idiosyncrasies that emerge in the behavior of such agents when operating under corruptions. Finally, for visual corruptions in RobustNav, we show that while standard techniques to improve robustness such as data-augmentation and self-supervised adaptation offer some zero-shot resistance and improvements in navigation performance, there is still a long way to go in terms of recovering lost performance relative to clean &quot;non-corrupt&quot; settings, warranting more research in this direction. Our code is available at https://github.com/allenai/robustnav.},
  archive   = {C_ICCV},
  author    = {Prithvijit Chattopadhyay and Judy Hoffman and Roozbeh Mottaghi and Aniruddha Kembhavi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01540},
  pages     = {15671-15680},
  title     = {RobustNav: Towards benchmarking robustness in embodied navigation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VIL-100: A new dataset and a baseline model for video
instance lane detection. <em>ICCV</em>, 15661–15670. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Lane detection plays a key role in autonomous driving. While car cameras always take streaming videos on the way, current lane detection works mainly focus on individual images (frames) by ignoring dynamics along the video. In this work, we collect a new video instance lane detection (VIL-100) dataset, which contains 100 videos with in total 10,000 frames, acquired from different real traffic scenarios. All the frames in each video are manually annotated to a high-quality instance-level lane annotation, and a set of frame-level and video-level metrics are included for quantitative performance evaluation. Moreover, we propose a new baseline model, named multi-level memory aggregation network (MMA-Net), for video instance lane detection. In our approach, the representation of current frame is enhanced by attentively aggregating both local and global memory features from other frames. Experiments on the new collected dataset show that the proposed MMA-Net outperforms state-of-the-art lane detection methods and video object segmentation methods. We release our dataset and code at https://github.com/yujun0-0/MMA-Net.},
  archive   = {C_ICCV},
  author    = {Yujun Zhang and Lei Zhu and Wei Feng and Huazhu Fu and Mingqian Wang and Qingxia Li and Cheng Li and Song Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01539},
  pages     = {15661-15670},
  title     = {VIL-100: A new dataset and a baseline model for video instance lane detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-view radar semantic segmentation. <em>ICCV</em>,
15651–15660. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Understanding the scene around the ego-vehicle is key to assisted and autonomous driving. Nowadays, this is mostly conducted using cameras and laser scanners, despite their reduced performance in adverse weather conditions. Automotive radars are low-cost active sensors that measure properties of surrounding objects, including their relative speed, and have the key advantage of not being impacted by rain, snow or fog. However, they are seldom used for scene understanding due to the size and complexity of radar raw data and the lack of annotated datasets. Fortunately, recent open-sourced datasets have opened up research on classification, object detection and semantic segmentation with raw radar signals using end-to-end trainable models. In this work, we propose several novel architectures, and their associated losses, which analyse multiple &quot;views&quot; of the range-angle-Doppler radar tensor to segment it semantically. Experiments conducted on the recent CARRADA dataset demonstrate that our best model outperforms alternative models, derived either from the semantic segmentation of natural images or from radar scene understanding, while requiring significantly fewer parameters. Both our code and trained models are available at https://github.com/valeoai/MVRSS.},
  archive   = {C_ICCV},
  author    = {Arthur Ouaknine and Alasdair Newson and Patrick Pérez and Florence Tupin and Julien Rebut},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01538},
  pages     = {15651-15660},
  title     = {Multi-view radar semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Structured bird’s-eye-view traffic scene understanding from
onboard images. <em>ICCV</em>, 15641–15650. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous navigation requires structured representation of the road network and instance-wise identification of the other traffic agents. Since the traffic scene is defined on the ground plane, this corresponds to scene understanding in the bird’s-eye-view (BEV). However, the onboard cameras of autonomous cars are customarily mounted horizontally for a better view of the surrounding, making this task very challenging. In this work, we study the problem of extracting a directed graph representing the local road network in BEV coordinates, from a single onboard camera image. Moreover, we show that the method can be extended to detect dynamic objects on the BEV plane. The semantics, locations, and orientations of the detected objects together with the road graph facilitates a comprehensive understanding of the scene. Such understanding becomes fundamental for the downstream tasks, such as path planning and navigation. We validate our approach against powerful baselines and show that our network achieves superior performance. We also demonstrate the effects of various design choices through ablation studies. Code: https://github.com/ybarancan/STSU},
  archive   = {C_ICCV},
  author    = {Yigit Baran Can and Alexander Liniger and Danda Pani Paudel and Luc Van Gool},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01537},
  pages     = {15641-15650},
  title     = {Structured bird’s-eye-view traffic scene understanding from onboard images},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Road anomaly detection by partial image reconstruction with
segmentation coupling. <em>ICCV</em>, 15631–15640. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel approach to the detection of unknown objects in the context of autonomous driving. The problem is formulated as anomaly detection, since we assume that the unknown stuff or object appearance cannot be learned. To that end, we propose a reconstruction module that can be used with many existing semantic segmentation networks, and that is trained to recognize and reconstruct road (drivable) surface from a small bottleneck. We postulate that poor reconstruction of the road surface is due to areas that are outside of the training distribution, which is a strong indicator of an anomaly. The road structural similarity error is coupled with the semantic segmentation to incorporate information from known classes and produce final per-pixel anomaly scores. The proposed JSR-Net was evaluated on four datasets, Lost-and-found, Road Anomaly, Road Obstacles, and FishyScapes, achieving state-of-art performance on all, reducing the false positives significantly, while typically having the highest average precision for wide range of operation points.},
  archive   = {C_ICCV},
  author    = {Tomas Vojir and Tomáš Šipka and Rahaf Aljundi and Nikolay Chumerin and Daniel Olmeda Reino and Jiri Matas},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01536},
  pages     = {15631-15640},
  title     = {Road anomaly detection by partial image reconstruction with segmentation coupling},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). AutoShape: Real-time shape-aware monocular 3D object
detection. <em>ICCV</em>, 15621–15630. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing deep learning-based approaches for monocular 3D object detection in autonomous driving often model the object as a rotated 3D cuboid while the object’s geometric shape has been ignored. In this work, we propose an approach for incorporating the shape-aware 2D/3D constraints into the 3D detection framework. Specifically, we employ the deep neural network to learn distinguished 2D keypoints in the 2D image domain and regress their corresponding 3D coordinates in the local 3D object coordinate first. Then the 2D/3D geometric constraints are built by these correspondences for each object to boost the detection performance. For generating the ground truth of 2D/3D keypoints, an automatic model-fitting approach has been proposed by fitting the deformed 3D object model and the object mask in the 2D image. The proposed framework has been verified on the public KITTI dataset and the experimental results demonstrate that by using additional geometrical constraints the detection performance has been significantly improved as compared to the baseline method. More importantly, the proposed framework achieves state-of-the-art performance with real time. Data and code will be available at https://github.com/zongdai/AutoShape},
  archive   = {C_ICCV},
  author    = {Zongdai Liu and Dingfu Zhou and Feixiang Lu and Jin Fang and Liangjun Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01535},
  pages     = {15621-15630},
  title     = {AutoShape: Real-time shape-aware monocular 3D object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust 2D/3D vehicle parsing in arbitrary camera views for
CVIS. <em>ICCV</em>, 15611–15620. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel approach to robustly detect and perceive vehicles in different camera views as part of a cooperative vehicle-infrastructure system (CVIS). Our formulation is designed for arbitrary camera views and makes no assumptions about intrinsic or extrinsic parameters. First, to deal with multi-view data scarcity, we propose a part-assisted novel view synthesis algorithm for data augmentation. We train a part-based texture inpainting network in a self-supervised manner. Then we render the textured model into the background image with the target 6-DoF pose. Second, to handle various camera parameters, we present a new method that produces dense mappings between image pixels and 3D points to perform robust 2D/3D vehicle parsing. Third, we build the first CVIS dataset for bench-marking, which annotates more than 1540 images (14017 instances) from real-world traffic scenarios. We combine these novel algorithms and datasets to develop a robust approach for 2D/3D vehicle parsing for CVIS. In practice, our approach outperforms SOTA methods on 2D detection, in-stance segmentation, and 6-DoF pose estimation by 3.8\%, 4.3\%, and 2.9\%, respectively.},
  archive   = {C_ICCV},
  author    = {Hui Miao and Feixiang Lu and Zongdai Liu and Liangjun Zhang and Dinesh Manocha and Bin Zhou},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01534},
  pages     = {15611-15620},
  title     = {Robust 2D/3D vehicle parsing in arbitrary camera views for CVIS},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Prediction by anticipation: An action-conditional prediction
method based on interaction learning. <em>ICCV</em>, 15601–15610. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In autonomous driving (AD), accurately predicting changes in the environment can effectively improve safety and comfort. Due to complex interactions among traffic participants, however, it is very hard to achieve accurate prediction for a long horizon. To address this challenge, we propose prediction by anticipation, which views interaction in terms of a latent probabilistic generative process wherein some vehicles move partly in response to the anticipated motion of other vehicles. Under this view, consecutive data frames can be factorized into sequential samples from an action-conditional distribution that effectively generalizes to a wider range of actions and driving situations. Our proposed prediction model, variational Bayesian in nature, is trained to maximize the evidence lower bound (ELBO) of the log-likelihood of this conditional distribution. Evaluations of our approach with prominent AD datasets NGSIM I-80 and Argoverse show significant improvement over current state-of-the-art in both accuracy and generalization.},
  archive   = {C_ICCV},
  author    = {Ershad Banijamali and Mohsen Rohani and Elmira Amirloo and Jun Luo and Pascal Poupart},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01533},
  pages     = {15601-15610},
  title     = {Prediction by anticipation: An action-conditional prediction method based on interaction learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Procedure planning in instructional videos via contextual
modeling and model-based policy learning. <em>ICCV</em>, 15591–15600.
(<a href="https://doi.org/10.1109/ICCV48922.2021.01532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning new skills by observing humans’ behaviors is an essential capability of AI. In this work, we leverage instructional videos to study humans’ decision-making processes, focusing on learning a model to plan goal-directed actions in real-life videos. In contrast to conventional action recognition, goal-directed actions are based on expectations of their outcomes requiring causal knowledge of potential consequences of actions. Thus, integrating the environment structure with goals is critical for solving this task. Previous works learn a single world model will fail to distinguish various tasks, resulting in an ambiguous la-tent space; planning through it will gradually neglect the desired outcomes since the global information of the future goal degrades quickly as the procedure evolves. We address these limitations with a new formulation of procedure planning and propose novel algorithms to model human behaviors through Bayesian Inference and model-based Imitation Learning. Experiments conducted on real-world instructional videos show that our method can achieve state-of-the-art performance in reaching the indicated goals. Furthermore, the learned contextual information presents interesting features for planning in a latent space.},
  archive   = {C_ICCV},
  author    = {Jing Bi and Jiebo Luo and Chenliang Xu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01532},
  pages     = {15591-15600},
  title     = {Procedure planning in instructional videos via contextual modeling and model-based policy learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bifold and semantic reasoning for pedestrian behavior
prediction. <em>ICCV</em>, 15580–15590. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pedestrian behavior prediction is one of the major challenges for intelligent driving systems. Pedestrians often exhibit complex behaviors influenced by various contextual elements. To address this problem, we propose BiPed, a multitask learning framework that simultaneously predicts trajectories and actions of pedestrians by relying on multi-modal data. Our method benefits from 1) a bifold encoding approach where different data modalities are processed in-dependently allowing them to develop their own representations, and jointly to produce a representation for all modalities using shared parameters; 2) a novel interaction modeling technique that relies on categorical semantic parsing of the scenes to capture interactions between target pedestrians and their surroundings; and 3) a bifold prediction mechanism that uses both independent and shared decoding of multimodal representations. Using public pedestrian behavior benchmark datasets for driving, PIE and JAAD, we highlight the benefits of the proposed method for behavior prediction and show that our model achieves state-of-the-art performance and improves trajectory and action prediction by up to 22\% and 9\% respectively. We further investigate the contributions of the proposed reasoning techniques via extensive ablation studies.},
  archive   = {C_ICCV},
  author    = {Amir Rasouli and Mohsen Rohani and Jun Luo},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01531},
  pages     = {15580-15590},
  title     = {Bifold and semantic reasoning for pedestrian behavior prediction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to drive from a world on rails. <em>ICCV</em>,
15570–15579. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We learn an interactive vision-based driving policy from pre-recorded driving logs via a model-based approach. A forward model of the world supervises a driving policy that predicts the outcome of any potential driving trajectory. To support learning from pre-recorded logs, we assume that the world is on rails, meaning neither the agent nor its actions influence the environment. This assumption greatly simplifies the learning problem, factorizing the dynamics into a non-reactive world model and a low-dimensional and compact forward model of the ego-vehicle. Our approach computes action-values for each training trajectory using a tabular dynamic-programming evaluation of the Bellman equations; these action-values in turn supervise the final vision-based driving policy. Despite the world-on-rails assumption, the final driving policy acts well in a dynamic and reactive world. It outperforms imitation learning as well as model-based and model-free reinforcement learning on the challenging CARLA NoCrash benchmark. It is also an order of magnitude more sample-efficient than state-of-the-art model-free reinforcement learning techniques on navigational tasks in the ProcGen benchmark.},
  archive   = {C_ICCV},
  author    = {Dian Chen and Vladlen Koltun and Philipp Krähenbühl},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01530},
  pages     = {15570-15579},
  title     = {Learning to drive from a world on rails},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Personalized trajectory prediction via distribution
discrimination. <em>ICCV</em>, 15560–15569. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trajectory prediction is confronted with the dilemma to capture the multi-modal nature of future dynamics with both diversity and accuracy. In this paper, we present a distribution discrimination (DisDis) method to predict personalized motion patterns by distinguishing the potential distributions. Motivated by that the motion pattern of each person is personalized due to his/her habit, our DisDis learns the latent distribution to represent different motion patterns and optimize it by the contrastive discrimination. This distribution discrimination encourages latent distributions to be more discriminative. Our method can be integrated with existing multi-modal stochastic predictive models as a plug-and-play module to learn the more discriminative latent distribution. To evaluate the latent distribution, we further propose a new metric, probability cumulative minimum distance (PCMD) curve, which cumulatively calculates the minimum distance on the sorted probabilities. Experimental results on the ETH and UCY datasets show the effectiveness of our method. 1},
  archive   = {C_ICCV},
  author    = {Guangyi Chen and Junlong Li and Nuoxing Zhou and Liangliang Ren and Jiwen Lu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01529},
  pages     = {15560-15569},
  title     = {Personalized trajectory prediction via distribution discrimination},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Crowd counting with partial annotations in an image.
<em>ICCV</em>, 15550–15559. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To fully leverage the data captured from different scenes with different view angles while reducing the annotation cost, this paper studies a novel crowd counting setting, i.e. only using partial annotations in each image as training data. Inspired by the repetitive patterns in the annotated and unannotated regions as well as the ones between them, we design a network with three components to tackle those unannotated regions: i) in an Unannotated Regions Characterization (URC) module, we employ a memory bank to only store the annotated features, which could help the visual features extracted from these annotated regions flow to these unannotated regions; ii) For each image, Feature Distribution Consistency (FDC) regularizes the feature distributions of annotated head and unannotated head regions to be consistent; iii) a Cross-regressor Consistency Regularization (CCR) module is designed to learn the visual features of unannotated regions in a self-supervised style. The experimental results validate the effectiveness of our proposed model under the partial annotation setting for several datasets, such as ShanghaiTech, UCF-CC-50, UCF-QNRF, NWPU-Crowd and JHU-CROWD++. With only 10\% annotated regions in each image, our proposed model achieves better performance than the recent methods and baselines under semi-supervised or active learning settings on all datasets. The code is https://github.com/svip-lab/CrwodCountingPAL.},
  archive   = {C_ICCV},
  author    = {Yanyu Xu and Ziming Zhong and Dongze Lian and Jing Li and Zhengxin Li and Xinxing Xu and Shenghua Gao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01528},
  pages     = {15550-15559},
  title     = {Crowd counting with partial annotations in an image},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Excavating the potential capacity of self-supervised
monocular depth estimation. <em>ICCV</em>, 15540–15549. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-supervised methods play an increasingly important role in monocular depth estimation due to their great potential and low annotation cost. To close the gap with supervised methods, recent works take advantage of extra constraints, e.g., semantic segmentation. However, these methods will inevitably increase the burden on the model. In this paper, we show theoretical and empirical evidence that the potential capacity of self-supervised monocular depth estimation can be excavated without increasing this cost. In particular, we propose (1) a novel data augmentation approach called data grafting, which forces the model to explore more cues to infer depth besides the vertical image position, (2) an exploratory self-distillation loss, which is supervised by the self-distillation label generated by our new post-processing method - selective post-processing, and (3) the full-scale network, designed to endow the encoder with the specialization of depth estimation task and enhance the representational power of the model. Extensive experiments show that our contributions can bring significant performance improvement to the baseline with even less computational overhead, and our model, named EPCDepth, surpasses the previous state-of-the-art methods even those supervised by additional constraints. Code is available at https://github.com/prstrive/EPCDepth.},
  archive   = {C_ICCV},
  author    = {Rui Peng and Ronggang Wang and Yawen Lai and Luyang Tang and Yangang Cai},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01527},
  pages     = {15540-15549},
  title     = {Excavating the potential capacity of self-supervised monocular depth estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatial uncertainty-aware semi-supervised crowd counting.
<em>ICCV</em>, 15529–15539. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Semi-supervised approaches for crowd counting attract attention, as the fully supervised paradigm is expensive and laborious due to its request for a large number of images of dense crowd scenarios and their annotations. This paper proposes a spatial uncertainty-aware semi-supervised approach via regularized surrogate task (binary segmentation) for crowd counting problems. Different from existing semi-supervised learning-based crowd counting methods, to exploit the unlabeled data, our proposed spatial uncertainty-aware teacher-student framework focuses on high confident regions’ information while addressing the noisy supervision from the unlabeled data in an end-to-end manner. Specifically, we estimate the spatial uncertainty maps from the teacher model’s surrogate task to guide the feature learning of the main task (density regression) and the surrogate task of the student model at the same time. Besides, we introduce a simple yet effective differential transformation layer to enforce the inherent spatial consistency regularization between the main task and the surrogate task in the student model, which helps the surrogate task to yield more reliable predictions and generates high-quality uncertainty maps. Thus, our model can also address the task-level perturbation problems that occur spatial inconsistency between the primary and surrogate tasks in the student model. Experimental results on four challenging crowd counting datasets demonstrate that our method achieves superior performance to the state-of-the-art semi-supervised methods. Code is available at : https://github.com/smallmax00/SUA_crowd_counting},
  archive   = {C_ICCV},
  author    = {Yanda Meng and Hongrun Zhang and Yitian Zhao and Xiaoyun Yang and Xuesheng Qian and Xiaowei Huang and Yalin Zheng},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01526},
  pages     = {15529-15539},
  title     = {Spatial uncertainty-aware semi-supervised crowd counting},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FOVEA: Foveated image magnification for autonomous
navigation. <em>ICCV</em>, 15519–15528. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Efficient processing of high-resolution video streams is safety-critical for many robotics applications such as autonomous driving. Image downsampling is a commonly adopted technique to ensure the latency constraint is met. However, this naive approach greatly restricts an object detector’s capability to identify small objects. In this paper, we propose an attentional approach that elastically magnifies certain regions while maintaining a small input canvas. The magnified regions are those that are believed to have a high probability of containing an object, whose signal can come from a dataset-wide prior or frame-level prior computed from recent object predictions. The magnification is implemented by a KDE-based mapping to transform the bounding boxes into warping parameters, which are then fed into an image sampler with anti-cropping regularization. The detector is then fed with the warped image and we apply a differentiable backward mapping to get bounding box outputs in the original space. Our regional magnification allows algorithms to make better use of high-resolution input without incurring the cost of high-resolution processing. On the autonomous driving datasets Argoverse-HD and BDD100K, we show our proposed method boosts the detection AP over standard Faster R-CNN, with and without finetuning. Additionally, building on top of the previous state-of-the-art in streaming detection, our method sets a new record for streaming AP on Argoverse-HD (from 17.8 to 23.0 on a GTX 1080 Ti GPU), suggesting that it has achieved a superior accuracy-latency tradeoff.},
  archive   = {C_ICCV},
  author    = {Chittesh Thavamani and Mengtian Li and Nicolas Cebron and Deva Ramanan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01525},
  pages     = {15519-15528},
  title     = {FOVEA: Foveated image magnification for autonomous navigation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Revealing the reciprocal relations between self-supervised
stereo and monocular depth estimation. <em>ICCV</em>, 15509–15518. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current self-supervised depth estimation algorithms mainly focus on either stereo or monocular only, neglecting the reciprocal relations between them. In this paper, we propose a simple yet effective framework to improve both stereo and monocular depth estimation by leveraging the underlying complementary knowledge of the two tasks. Our approach consists of three stages. In the first stage, the proposed stereo matching network termed StereoNet is trained on image pairs in a self-supervised manner. Second, we introduce an occlusion-aware distillation (OA Distillation) module, which leverages the predicted depths from StereoNet in non-occluded regions to train our monocular depth estimation network named SingleNet. At last, we design an occlusion-aware fusion module (OA Fusion), which generates more reliable depths by fusing estimated depths from StereoNet and SingleNet given the occlusion map. Furthermore, we also take the fused depths as pseudo labels to supervise StereoNet in turn, which brings StereoNet’s performance to a new height. Extensive experiments on KITTI dataset demonstrate the effectiveness of our proposed framework. We achieve new SOTA performance on both stereo and monocular depth estimation tasks.},
  archive   = {C_ICCV},
  author    = {Zhi Chen and Xiaoqing Ye and Wei Yang and Zhenbo Xu and Xiao Tan and Zhikang Zou and Errui Ding and Xinming Zhang and Liusheng Huang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01524},
  pages     = {15509-15518},
  title     = {Revealing the reciprocal relations between self-supervised stereo and monocular depth estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Perturbed self-distillation: Weakly supervised large-scale
point cloud semantic segmentation. <em>ICCV</em>, 15500–15508. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large-scale point cloud semantic segmentation has wide applications. Current popular researches mainly focus on fully supervised learning which demands expensive and tedious manual point-wise annotation. Weakly supervised learning is an alternative way to avoid this exhausting an-notation. However, for large-scale point clouds with few labeled points, the network is difficult to extract discriminative features for unlabeled points, as well as the regularization of topology between labeled and unlabeled points is usually ignored, resulting in incorrect segmentation results.To address this problem, we propose a perturbed self-distillation (PSD) framework. Specifically, inspired by self-supervised learning, we construct the perturbed branch and enforce the predictive consistency among the perturbed branch and original branch. In this way, the graph topology of the whole point cloud can be effectively established by the introduced auxiliary supervision, such that the in-formation propagation between the labeled and unlabeled points will be realized. Besides point-level supervision, we present a well-integrated context-aware module to explicitly regularize the affinity correlation of labeled points. Therefore, the graph topology of the point cloud can be further refined. The experimental results evaluated on three large-scale datasets show the large gain (3.0\% on average) against recent weakly supervised methods and comparable results to some fully supervised methods.},
  archive   = {C_ICCV},
  author    = {Yachao Zhang and Yanyun Qu and Yuan Xie and Zonghao Li and Shanshan Zheng and Cuihua Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01523},
  pages     = {15500-15508},
  title     = {Perturbed self-distillation: Weakly supervised large-scale point cloud semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ReDAL: Region-based and diversity-aware active learning for
point cloud semantic segmentation. <em>ICCV</em>, 15490–15499. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite the success of deep learning on supervised point cloud semantic segmentation, obtaining large-scale point-by-point manual annotations is still a significant challenge. To reduce the huge annotation burden, we propose a Region-based and Diversity-aware Active Learning (ReDAL), a general framework for many deep learning approaches, aiming to automatically select only informative and diverse sub-scene regions for label acquisition. Observing that only a small portion of annotated regions are sufficient for 3D scene understanding with deep learning, we use softmax entropy, color discontinuity, and structural complexity to measure the information of sub-scene regions. A diversity-aware selection algorithm is also developed to avoid redundant annotations resulting from selecting informative but similar regions in a querying batch. Extensive experiments show that our method highly outperforms previous active learning strategies, and we achieve the performance of 90\% fully supervised learning, while less than 15\% and 5\% annotations are required on S3DIS and SemanticKITTI datasets, respectively.},
  archive   = {C_ICCV},
  author    = {Tsung-Han Wu and Yueh-Cheng Liu and Yu-Kai Huang and Hsin-Ying Lee and Hung-Ting Su and Ping-Chia Huang and Winston H. Hsu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01522},
  pages     = {15490-15499},
  title     = {ReDAL: Region-based and diversity-aware active learning for point cloud semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Warp-refine propagation: Semi-supervised auto-labeling via
cycle-consistency. <em>ICCV</em>, 15479–15489. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep learning models for semantic segmentation rely on expensive, large-scale, manually annotated datasets. Labelling is a tedious process that can take hours per image. Automatically annotating video sequences by propagating sparsely labeled frames through time is a more scalable alternative. In this work, we propose a novel label propagation method, termed Warp-Refine Propagation, that combines semantic cues with geometric cues to efficiently auto-label videos. Our method learns to refine geometrically-warped labels and infuse them with learned semantic priors in a semi-supervised setting by leveraging cycle-consistency across time. We quantitatively show that our method improves label-propagation by a noteworthy margin of 13.1 mIoU on the ApolloScape dataset. Furthermore, by training with the auto-labelled frames, we achieve competitive results on three semantic-segmentation benchmarks, improving the state-of-the-art by a large margin of 1.8 and 3.61 mIoU on NYU-V2 and KITTI, while matching the current best results on Cityscapes.},
  archive   = {C_ICCV},
  author    = {Aditya Ganeshan and Alexis Vallet and Yasunori Kudo and Shin-Ichi Maeda and Tommi Kerola and Rareş Ambruş and Dennis Park and Adrien Gaidon},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01521},
  pages     = {15479-15489},
  title     = {Warp-refine propagation: Semi-supervised auto-labeling via cycle-consistency},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VMNet: Voxel-mesh network for geodesic-aware 3D semantic
segmentation. <em>ICCV</em>, 15468–15478. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, sparse voxel-based methods have be-come the state-of-the-arts for 3D semantic segmentation of indoor scenes, thanks to the powerful 3D CNNs. Nevertheless, being oblivious to the underlying geometry, voxel-based methods suffer from ambiguous features on spatially close objects and struggle with handling complex and irregular geometries due to the lack of geodesic information. In view of this, we present Voxel-Mesh Network (VMNet), a novel 3D deep architecture that operates on the voxel and mesh representations leveraging both the Euclidean and geodesic information. Intuitively, the Euclidean information extracted from voxels can offer contextual cues representing interactions between nearby objects, while the geodesic information extracted from meshes can help separate objects that are spatially close but have disconnected surfaces. To incorporate such information from the two domains, we design an intra-domain attentive module for effective feature aggregation and an inter-domain attentive module for adaptive feature fusion. Experimental results validate the effectiveness of VMNet: specifically, on the challenging ScanNet dataset for large-scale segmentation of indoor scenes, it outperforms the state-of-the-art SparseConvNet and MinkowskiNet (74.6\% vs 72.5\% and 73.6\% in mIoU) with a simpler network structure (17M vs 30M and 38M parameters). Code release: https://github.com/hzykent/VMNet},
  archive   = {C_ICCV},
  author    = {Zeyu Hu and Xuyang Bai and Jiaxiang Shang and Runze Zhang and Jiayu Dong and Xin Wang and Guangyuan Sun and Hongbo Fu and Chiew-Lan Tai},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01520},
  pages     = {15468-15478},
  title     = {VMNet: Voxel-mesh network for geodesic-aware 3D semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning inner-group relations on point clouds.
<em>ICCV</em>, 15457–15467. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The prevalence of relation networks in computer vision is in stark contrast to underexplored point-based methods. In this paper, we explore the possibilities of local relation operators and survey their feasibility. We propose a scalable and efficient module, called group relation aggregator. The module computes a feature of a group based on the aggregation of the features of the inner-group points weighted by geometric relations and semantic relations. We adopt this module to design our RPNet. We further verify the expandability of RPNet, in terms of both depth and width, on the tasks of classification and segmentation. Surprisingly, empirical results show that wider RPNet fits for classification, while deeper RPNet works better on segmentation. RPNet achieves state-of-the-art for classification and segmentation on challenging benchmarks. We also compare our local aggregator with PointNet++, with around 30\% parameters and 50\% computation saving. Finally, we conduct experiments to reveal the robustness of RPNet with regard to rigid transformation and noises.},
  archive   = {C_ICCV},
  author    = {Haoxi Ran and Wei Zhuo and Jun Liu and Li Lu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01519},
  pages     = {15457-15467},
  title     = {Learning inner-group relations on point clouds},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical aggregation for 3D instance segmentation.
<em>ICCV</em>, 15447–15456. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Instance segmentation on point clouds is a fundamental task in 3D scene perception. In this work, we propose a concise clustering-based framework named HAIS, which makes full use of spatial relation of points and point sets. Considering clustering-based methods may result in over-segmentation or under-segmentation, we introduce the hierarchical aggregation to progressively generate instance proposals, i.e., point aggregation for preliminarily clustering points to sets and set aggregation for generating complete instances from sets. Once the complete 3D instances are obtained, a sub-network of intra-instance prediction is adopted for noisy points filtering and mask quality scoring. HAIS is fast (only 410ms per frame on Titan X)) and does not require non-maximum suppression. It ranks 1st on the ScanNet v2 benchmark 1 , achieving the highest 69.9\% AP 50 and surpassing previous state-of-the-art (SOTA) methods by a large margin. Besides, the SOTA results on the S3DIS dataset validate the good generalization ability. Code is available at https://github.com/hustvl/HAIS.},
  archive   = {C_ICCV},
  author    = {Shaoyu Chen and Jiemin Fang and Qian Zhang and Wenyu Liu and Xinggang Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01518},
  pages     = {15447-15456},
  title     = {Hierarchical aggregation for 3D instance segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HiFT: Hierarchical feature transformer for aerial tracking.
<em>ICCV</em>, 15437–15446. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most existing Siamese-based tracking methods execute the classification and regression of the target object based on the similarity maps. However, they either employ a single map from the last convolutional layer which degrades the localization accuracy in complex scenarios or separately use multiple maps for decision making, introducing intractable computations for aerial mobile platforms. Thus, in this work, we propose an efficient and effective hierarchical feature transformer (HiFT) for aerial tracking. Hierarchical similarity maps generated by multi-level convolutional layers are fed into the feature transformer to achieve the interactive fusion of spatial (shallow layers) and semantics cues (deep layers). Consequently, not only the global contextual information can be raised, facilitating the target search, but also our end-to-end architecture with the transformer can efficiently learn the interdependencies among multi-level features, thereby discovering a tracking-tailored feature space with strong discriminability. Comprehensive evaluations on four aerial benchmarks have proven the effectiveness of HiFT. Real-world tests on the aerial platform have strongly validated its practicability with a real-time speed. Our code is available at https://github.com/vision4robotics/HiFT.},
  archive   = {C_ICCV},
  author    = {Ziang Cao and Changhong Fu and Junjie Ye and Bowen Li and Yiming Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01517},
  pages     = {15437-15446},
  title     = {HiFT: Hierarchical feature transformer for aerial tracking},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SPG: Unsupervised domain adaptation for 3D object detection
via semantic point generation. <em>ICCV</em>, 15426–15436. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In autonomous driving, a LiDAR-based object detector should perform reliably at different geographic locations and under various weather conditions. While recent 3D detection research focuses on improving performance within a single domain, our study reveals that the performance of modern detectors can drop drastically cross-domain. In this paper, we investigate unsupervised domain adaptation (UDA) for LiDAR-based 3D object detection. On the Waymo Domain Adaptation [49] dataset, we identify the deteriorating point cloud quality as the root cause of the performance drop. To address this issue, we present Semantic Point Generation (SPG), a general approach to enhance the reliability of LiDAR detectors against domain shifts. Specifically, SPG generates semantic points at the predicted fore-ground regions and faithfully recovers missing parts of the foreground objects, which are caused by phenomena such as occlusions, low reflectance, or weather interference. By merging the semantic points with the original points, we obtain an augmented point cloud, which can be directly consumed by modern LiDAR-based detectors. To validate the wide applicability of SPG, we experiment with two representative detectors, PointPillars [22] and PV-RCNN [45]. On the UDA task, SPG significantly improves both detectors across all object categories of interest and at all difficulty levels. SPG can also benefit object detection in the original domain. On the Waymo Open Dataset [49] and KITTI [17], SPG improves 3D detection results of these two methods across all categories. Combined with PV-RCNN [45], SPG achieves state-of-the-art 3D detection results on KITTI.},
  archive   = {C_ICCV},
  author    = {Qiangeng Xu and Yin Zhou and Weiyue Wang and Charles R. Qi and Dragomir Anguelov},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01516},
  pages     = {15426-15436},
  title     = {SPG: Unsupervised domain adaptation for 3D object detection via semantic point generation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 4D-net for learned multi-modal alignment. <em>ICCV</em>,
15415–15425. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present 4D-Net, a 3D object detection approach, which utilizes 3D Point Cloud and RGB sensing information, both in time. We are able to incorporate the 4D information by performing a novel dynamic connection learning across various feature representations and levels of abstraction, as well as by observing geometric constraints. Our approach outperforms the state-of-the-art and strong base-lines on the Waymo Open Dataset. 4D-Net is better able to use motion cues and dense image information to detect distant objects more successfully. We will open source the code.},
  archive   = {C_ICCV},
  author    = {AJ Piergiovanni and Vincent Casser and Michael S. Ryoo and Anelia Angelova},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01515},
  pages     = {15415-15425},
  title     = {4D-net for learned multi-modal alignment},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Standardized max logits: A simple yet effective approach for
identifying unexpected road obstacles in urban-scene segmentation.
<em>ICCV</em>, 15405–15414. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Identifying unexpected objects on roads in semantic segmentation (e.g., identifying dogs on roads) is crucial in safetycritical applications. Existing approaches use images of unexpected objects from external datasets or require additional training (e.g., retraining segmentation networks or training an extra network), which necessitate a non-trivial amount of labor intensity or lengthy inference time. One possible alternative is to use prediction scores of a pretrained network such as the max logits (i.e., maximum values among classes before the final softmax layer) for detecting such objects. However, the distribution of max logits of each predicted class is significantly different from each other, which degrades the performance of identifying unexpected objects in urban-scene segmentation. To address this issue, we propose a simple yet effective approach that standardizes the max logits in order to align the different distributions and reflect the relative meanings of max logits within each predicted class. Moreover, we consider the local regions from two different perspectives based on the intuition that neighboring pixels share similar semantic information. In contrast to previous approaches, our method does not utilize any external datasets or require additional training, which makes our method widely applicable to existing pretrained segmentation models. Such a straightforward approach achieves a new state-of-the-art performance on the publicly available Fishyscapes Lost &amp; Found leader-board with a large margin. Our code is publicly available at this link 1 .},
  archive   = {C_ICCV},
  author    = {Sanghun Jung and Jungsoo Lee and Daehoon Gwak and Sungha Choi and Jaegul Choo},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01514},
  pages     = {15405-15414},
  title     = {Standardized max logits: A simple yet effective approach for identifying unexpected road obstacles in urban-scene segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rethinking 360° image visual attention modelling with
unsupervised learning. <em>ICCV</em>, 15394–15404. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite the success of self-supervised representation learning on planar data, to date it has not been studied on 360° images. In this paper, we extend recent advances in contrastive learning to learn latent representations that are sufficiently invariant to be highly effective for spherical saliency prediction as a downstream task. We argue that omni-directional images are particularly suited to such an approach due to the geometry of the data domain. To verify this hypothesis, we design an unsupervised framework that effectively maximizes the mutual information between the different views from both the equator and the poles. We show that the decoder is able to learn good quality saliency distributions from the encoder embeddings. Our model compares favorably with fully-supervised learning methods on the Salient360!, VR-EyeTracking and Sitzman datasets. This performance is achieved using an encoder that is trained in a completely unsupervised way and a relatively lightweight supervised decoder (3.8 × fewer parameters in the case of the ResNet50 encoder). We believe that this combination of supervised and unsupervised learning is an important step toward flexible formulations of human visual attention. The results can be reproduced on GitHub},
  archive   = {C_ICCV},
  author    = {Yasser Abdelaziz Dahou Djilali and Tarun Krishna and Kevin McGuinness and Noel E. O’Connor},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01513},
  pages     = {15394-15404},
  title     = {Rethinking 360° image visual attention modelling with unsupervised learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning of visual relations: The devil is in the tails.
<em>ICCV</em>, 15384–15393. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Significant effort has been recently devoted to modeling visual relations. This has mostly addressed the design of architectures, typically by adding parameters and increasing model complexity. However, visual relation learning is a long-tailed problem, due to the combinatorial nature of joint reasoning about groups of objects. Increasing model complexity is, in general, illsuited for long-tailed problems due to their tendency to overfit. In this paper, we explore an alternative hypothesis, denoted the Devil is in the Tails. Under this hypothesis, better performance is achieved by keeping the model simple but improving its ability to cope with long-tailed distributions. To test this hypothesis, we devise a new approach for training visual relationships models, which is inspired by state-of-the-art long-tailed recognition literature. This is based on an iterative decoupled training scheme, denoted Decoupled Training for Devil in the Tails (DT2). DT2 employs a novel sampling approach, Alternating Class-Balanced Sampling (ACBS), to capture the interplay between the long-tailed entity and predicate distributions of visual relations. Results show that, with an extremely simple architecture, DT2-ACBS significantly out-performs much more complex state-of-the-art methods on scene graph generation tasks. This suggests that the development of sophisticated models must be considered in tandem with the long-tailed nature of the problem.},
  archive   = {C_ICCV},
  author    = {Alakh Desai and Tz-Ying Wu and Subarna Tripathi and Nuno Vasconcelos},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01512},
  pages     = {15384-15393},
  title     = {Learning of visual relations: The devil is in the tails},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FLAR: A unified prototype framework for few-sample lifelong
active recognition. <em>ICCV</em>, 15374–15383. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Intelligent agents with visual sensors are allowed to actively explore their observations for better recognition performance. This task is referred to as Active Recognition (AR). Currently, most methods toward AR are implemented under a fixed-category setting, which constrains their applicability in realistic scenarios that need to incrementally learn new classes without retraining from scratch. Further, collecting massive data for novel categories is expensive. To address this demand, in this paper, we propose a unified framework towards Few-sample Lifelong Active Recognition (FLAR), which aims at performing active recognition on progressively arising novel categories that only have few training samples. Three difficulties emerge with FLAR: the lifelong recognition policy learning, the knowledge preservation of old categories, and the lack of training samples. To this end, our approach integrates prototypes, a robust representation for limited training samples, into a reinforcement learning solution, which motivates the agent to move towards views resulting in more discriminative features. Catastrophic forgetting during lifelong learning is then alleviated with knowledge distillation. Extensive experiments across two datasets, respectively for object and scene recognition, demonstrate that even without large training samples, the proposed approach could learn to actively recognize novel categories in a class-incremental behavior.},
  archive   = {C_ICCV},
  author    = {Lei Fan and Peixi Xiong and Wei Wei and Ying Wu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01511},
  pages     = {15374-15383},
  title     = {FLAR: A unified prototype framework for few-sample lifelong active recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pose invariant topological memory for visual navigation.
<em>ICCV</em>, 15364–15373. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Planning for visual navigation using topological memory, a memory graph consisting of nodes and edges, has been recently well-studied. The nodes correspond to past observations of a robot, and the edges represent the reachability predicted by a neural network (NN). Most prior methods, however, often fail to predict the reachability when the robot takes different poses, i.e. the direction the robot faces, at close positions. This is because the methods observe first-person view images, which significantly changes when the robot changes its pose, and thus it is fundamentally difficult to correctly predict the reachability from them. In this paper, we propose pose invariant topological memory (POINT) to address the problem. POINT observes omnidirectional images and predicts the reachability by using a spherical convolutional NN, which has a rotation invariance property and enables planning regardless of the robot’s pose. Additionally, we train the NN by contrastive learning with data augmentation to enable POINT to plan with robustness to changes in environmental conditions, such as light conditions and the presence of unseen objects. Our experimental results show that POINT outperforms conventional methods under both the same and different environmental conditions. In addition, the results with the KITTI-360 dataset show that POINT is more applicable to real-world environments than conventional methods.},
  archive   = {C_ICCV},
  author    = {Asuto Taniguchi and Fumihiro Sasaki and Ryota Yamashina},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01510},
  pages     = {15364-15373},
  title     = {Pose invariant topological memory for visual navigation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). THDA: Treasure hunt data augmentation for semantic
navigation. <em>ICCV</em>, 15354–15363. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Can general-purpose neural models learn to navigate? For PointGoal navigation (‘go to Δx, Δy’), the answer is a clear ‘yes’ – mapless neural models composed of task-agnostic components (CNNs and RNNs) trained with large-scale model-free reinforcement learning achieve near-perfect performance [27]. However, for ObjectGoal navigation (‘find a TV’), this is an open question; one we tackle in this paper. The current best-known result on ObjectNav with general-purpose models is 6\% success rate [25].First, we show that the key problem is overfitting. Large-scale training results in 94\% success rate on training environments and only 8\% in validation. We observe that this stems from agents memorizing environment layouts during training – sidestepping the need for exploration and directly learning shortest paths to nearby goal objects. We show that this is a natural consequence of optimizing for the task metric (which in fact penalizes exploration), is enabled by powerful observation encoders, and is possible due to the finite set of training environment configurations.Informed by our findings, we introduce Treasure Hunt Data Augmentation (THDA) to address overfitting in ObjectNav. THDA inserts 3D scans of household objects at arbitrary scene locations and uses them as ObjectNav goals – augmenting and greatly expanding the set of training layouts. Taken together with our other proposed changes, we improve the state of art on the Habitat ObjectGoal Navigation benchmark by 90\% (from 14\% success rate to 27\%) and path efficiency by 48\% (from 7.5 SPL to 11.1 SPL).},
  archive   = {C_ICCV},
  author    = {Oleksandr Maksymets and Vincent Cartillier and Aaron Gokaslan and Erik Wijmans and Wojciech Galuba and Stefan Lee and Dhruv Batra},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01509},
  pages     = {15354-15363},
  title     = {THDA: Treasure hunt data augmentation for semantic navigation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scaling up instance annotation via label propagation.
<em>ICCV</em>, 15344–15353. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Manually annotating object segmentation masks is very time-consuming. While interactive segmentation methods offer a more efficient alternative, they become unaffordable at a large scale because the cost grows linearly with the number of annotated masks. In this paper, we propose a highly efficient annotation scheme for building large datasets with object segmentation masks. At a large scale, images contain many object instances with similar appearance. We exploit these similarities by using hierarchical clustering on mask predictions made by a segmentation model. We propose a scheme that efficiently searches through the hierarchy of clusters and selects which clusters to annotate. Humans manually verify only a few masks per cluster, and the labels are propagated to the whole cluster. Through a large-scale experiment to populate 1M unlabeled images with object segmentation masks for 80 object classes, we show that (1) we obtain 1M object segmentation masks with an total annotation time of only 290 hours; (2) we reduce annotation time by 76× compared to manual annotation; (3) the segmentation quality of our masks is on par with those from manually annotated datasets. Code, data, and models are available online 1 .},
  archive   = {C_ICCV},
  author    = {Dim P. Papadopoulos and Ethan Weber and Antonio Torralba},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01508},
  pages     = {15344-15353},
  title     = {Scaling up instance annotation via label propagation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Scribble-supervised semantic segmentation inference.
<em>ICCV</em>, 15334–15343. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a progressive segmentation inference (PSI) framework to tackle with scribble-supervised semantic segmentation. In virtue of latent contextual dependency, we encapsulate two crucial cues, contextual pattern propagation and semantic label diffusion, to enhance and refine pixel-level segmentation results from partially known seeds. In contextual pattern propagation, different-granular contextual patterns are correlated and leveraged to properly diffuse pattern information based on graphical model, so as to increase the inference confidence of pixel label prediction. Further, depending on high-confidence scores of estimated pixels, the initial annotated seeds are progressively spread over the image through dynamically learning an adaptive decision strategy. The two cues are finally modularized to form a close-looping update process during pixel-wise label inference. Extensive experiments demonstrate that our proposed progressive segmentation inference can benefit from the combination of spatial and semantic context cues, and meantime achieve the state-of-the-art performance on two public scribble segmentation datasets.},
  archive   = {C_ICCV},
  author    = {Jingshan Xu and Chuanwei Zhou and Zhen Cui and Chunyan Xu and Yuge Huang and Pengcheng Shen and Shaoxin Li and Jian Yang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01507},
  pages     = {15334-15343},
  title     = {Scribble-supervised semantic segmentation inference},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PrimitiveNet: Primitive instance segmentation with local
primitive embedding under adversarial metric. <em>ICCV</em>,
15323–15333. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present PrimitiveNet, a novel approach for high-resolution primitive instance segmentation from point clouds on a large scale. Our key idea is to transform the global segmentation problem into easier local tasks. We train a high-resolution primitive embedding network to predict explicit geometry features and implicit latent features for each point. The embedding is jointly trained with an adversarial network as a primitive discriminator to decide whether points are from the same primitive instance in local neighborhoods. Such local supervision encourages the learned embedding and discriminator to describe local surface properties and robustly distinguish different instances. At inference time, network predictions are followed by a region growing method to finalize the segmentation. Experiments show that our method outperforms existing state-of-the-arts based on mean average precision by a significant margin (46.3\%) on ABC dataset [31]. We can process extremely large real scenes covering more than 0.1km 2 . Ablation studies highlight the contribution of our core designs. Finally, our method can improve geometry processing algorithms to abstract scans as lightweight models. Code and data will be available based on Pytorch 1 and Mindspore 2 .},
  archive   = {C_ICCV},
  author    = {Jingwei Huang and Yanfeng Zhang and Mingwei Sun},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01506},
  pages     = {15323-15333},
  title     = {PrimitiveNet: Primitive instance segmentation with local primitive embedding under adversarial metric},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep metric learning for open world semantic segmentation.
<em>ICCV</em>, 15313–15322. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Classical close-set semantic segmentation networks have limited ability to detect out-of-distribution (OOD) objects, which is important for safety-critical applications such as autonomous driving. Incrementally learning these OOD objects with few annotations is an ideal way to enlarge the knowledge base of the deep learning models. In this paper, we propose an open world semantic segmentation system that includes two modules: (1) an open-set semantic segmentation module to detect both in-distribution and OOD objects. (2) an incremental few-shot learning module to gradually incorporate those OOD objects into its existing knowledge base. This open world semantic segmentation system behaves like a human being, which is able to identify OOD objects and gradually learn them with corresponding supervision. We adopt the Deep Metric Learning Network (DMLNet) with contrastive clustering to implement open-set semantic segmentation. Compared to other open-set semantic segmentation methods, our DMLNet achieves state-of-the-art performance on three challenging open-set semantic segmentation datasets without using additional data or generative models. On this basis, two incremental few-shot learning methods are further proposed to progressively improve the DMLNet with the annotations of OOD objects.},
  archive   = {C_ICCV},
  author    = {Jun Cen and Peng Yun and Junhao Cai and Michael Yu Wang and Ming Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01505},
  pages     = {15313-15322},
  title     = {Deep metric learning for open world semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Continuous copy-paste for one-stage multi-object tracking
and segmentation. <em>ICCV</em>, 15303–15312. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current one-step multi-object tracking and segmentation (MOTS) methods lag behind recent two-step methods. By separating the instance segmentation stage from the tracking stage, two-step methods can exploit non-video datasets as extra data for training instance segmentation. Moreover, instances belonging to different IDs on different frames, rather than limited numbers of instances in raw consecutive frames, can be gathered to allow more effective hard example mining in the training of trackers. In this paper, we bridge this gap by presenting a novel data augmentation strategy named continuous copy-paste (CCP). Our intuition behind CCP is to fully exploit the pixel-wise annotations provided by MOTS to actively increase the number of instances as well as unique instance IDs in training. Without any modifications to frameworks, current MOTS methods achieve significant performance gains when trained with CCP. Based on CCP, we propose the first effective one-stage online MOTS method named CCPNet, which generates instance masks as well as the tracking results in one shot. Our CCPNet surpasses all state-of-the-art methods by large margins (3.8\% higher sMOTSA and 4.1\% higher MOTSA for pedestrians on the KITTI MOTS Validation) and ranks 1st on the KITTI MOTS leaderboard. Evaluations across three datasets also demonstrate the effectiveness of both CCP and CCPNet. Our codes are publicly available at: https://github.com/detectRecog/CCP.},
  archive   = {C_ICCV},
  author    = {Zhenbo Xu and Ajin Meng and Zhenbo Shi and Wei Yang and Zhi Chen and Liusheng Huang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01504},
  pages     = {15303-15312},
  title     = {Continuous copy-paste for one-stage multi-object tracking and segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical disentangled representation learning for
outdoor illumination estimation and editing. <em>ICCV</em>, 15293–15302.
(<a href="https://doi.org/10.1109/ICCV48922.2021.01503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Data-driven sky models have gained much attention in outdoor illumination prediction recently, showing superior performance against analytical models. However, naively compressing an outdoor panorama into a low-dimensional latent vector, as existing models have done, causes two major problems. One is the mutual interference between the HDR intensity of the sun and the complex textures of the surrounding sky, and the other is the lack of fine-grained control over independent lighting factors due to the entangled representation. To address these issues, we propose a hierarchical disentangled sky model (HDSky) for outdoor illumination prediction. With this model, any outdoor panorama can be hierarchically disentangled into several factors based on three well-designed autoencoders. The first autoencoder compresses each sunny panorama into a sky vector and a sun vector with some constraints. The second autoencoder and the third autoencoder further disentangle the sun intensity and the sky intensity from the sun vector and the sky vector with several customized loss functions respectively. Moreover, a unified framework is designed to predict all-weather sky information from a single outdoor image. Through extensive experiments, we demonstrate that the proposed model significantly improves the accuracy of outdoor illumination prediction. It also allows users to intuitively edit the predicted panorama (e.g., changing the position of the sun while preserving others), without sacrificing physical plausibility.},
  archive   = {C_ICCV},
  author    = {Piaopiao Yu and Jie Guo and Fan Huang and Cheng Zhou and Hongwei Che and Xiao Ling and Yanwen Guo},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01503},
  pages     = {15293-15302},
  title     = {Hierarchical disentangled representation learning for outdoor illumination estimation and editing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DenseTNT: End-to-end trajectory prediction from dense goal
sets. <em>ICCV</em>, 15283–15292. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Due to the stochasticity of human behaviors, predicting the future trajectories of road agents is challenging for autonomous driving. Recently, goal-based multi-trajectory prediction methods are proved to be effective, where they first score over-sampled goal candidates and then select a final set from them. However, these methods usually involve goal predictions based on sparse pre-defined anchors and heuristic goal selection algorithms. In this work, we propose an anchor-free and end-to-end trajectory prediction model, named DenseTNT, that directly outputs a set of trajectories from dense goal candidates. In addition, we introduce an offline optimization-based technique to provide multi-future pseudo-labels for our final online model. Experiments show that DenseTNT achieves state-of-the-art performance, ranking 1 st on the Argoverse motion forecasting benchmark and being the 1 st place winner of the 2021 Waymo Open Dataset Motion Prediction Challenge.},
  archive   = {C_ICCV},
  author    = {Junru Gu and Chen Sun and Hang Zhao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01502},
  pages     = {15283-15292},
  title     = {DenseTNT: End-to-end trajectory prediction from dense goal sets},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LSG-CPD: Coherent point drift with local surface geometry
for point cloud registration. <em>ICCV</em>, 15273–15282. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Probabilistic point cloud registration methods are becoming more popular because of their robustness. However, unlike point-to-plane variants of iterative closest point (ICP) which incorporate local surface geometric information such as surface normals, most probabilistic methods (e.g., coherent point drift (CPD)) ignore such information and build Gaussian mixture models (GMMs) with isotropic Gaussian covariances. This results in sphere-like GMM components which only penalize the point-to-point distance between the two point clouds. In this paper, we propose a novel method called CPD with Local Surface Geometry (LSG-CPD) for rigid point cloud registration. Our method adaptively adds different levels of point-to-plane penalization on top of the point-to-point penalization based on the flatness of the local surface. This results in GMM components with anisotropic covariances. We formulate point cloud registration as a maximum likelihood estimation (MLE) problem and solve it with the Expectation-Maximization (EM) algorithm. In the E step, we demonstrate that the computation can be recast into simple matrix manipulations and efficiently computed on a GPU. In the M step, we perform an unconstrained optimization on a matrix Lie group to efficiently update the rigid transformation of the registration. The proposed method outperforms state-of-the-art algorithms in terms of accuracy and robustness on various datasets captured with range scanners, RGBD cameras, and LiDARs. Also, it is significantly faster than modern implementations of CPD. The source code is available at https://github.com/ChirikjianLab/LSG-CPD.git.},
  archive   = {C_ICCV},
  author    = {Weixiao Liu and Hongtao Wu and Gregory S. Chirikjian},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01501},
  pages     = {15273-15282},
  title     = {LSG-CPD: Coherent point drift with local surface geometry for point cloud registration},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fog simulation on real LiDAR point clouds for 3D object
detection in adverse weather. <em>ICCV</em>, 15263–15272. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work addresses the challenging task of LiDAR-based 3D object detection in foggy weather. Collecting and annotating data in such a scenario is very time, labor and cost intensive. In this paper, we tackle this problem by simulating physically accurate fog into clear-weather scenes, so that the abundant existing real datasets captured in clear weather can be repurposed for our task. Our contributions are twofold: 1) We develop a physically valid fog simulation method that is applicable to any LiDAR dataset. This unleashes the acquisition of large-scale foggy training data at no extra cost. These partially synthetic data can be used to improve the robustness of several perception methods, such as 3D object detection and tracking or simultaneous localization and mapping, on real foggy data. 2) Through extensive experiments with several state-of-the-art detection approaches, we show that our fog simulation can be leveraged to significantly improve the performance for 3D object detection in the presence of fog. Thus, we are the first to provide strong 3D object detection baselines on the Seeing Through Fog dataset. Our code is available at www.trace.ethz.ch/lidar fog simulation.},
  archive   = {C_ICCV},
  author    = {Martin Hahner and Christos Sakaridis and Dengxin Dai and Luc Van Gool},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01500},
  pages     = {15263-15272},
  title     = {Fog simulation on real LiDAR point clouds for 3D object detection in adverse weather},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FIERY: Future instance prediction in bird’s-eye view from
surround monocular cameras. <em>ICCV</em>, 15253–15262. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Driving requires interacting with road agents and predicting their future behaviour in order to navigate safely. We present FIERY: a probabilistic future prediction model in bird’s-eye view from monocular cameras. Our model predicts future instance segmentation and motion of dynamic agents that can be transformed into non-parametric future trajectories. Our approach combines the perception, sensor fusion and prediction components of a traditional autonomous driving stack by estimating bird’s-eye-view prediction directly from surround RGB monocular camera inputs. FIERY learns to model the inherent stochastic nature of the future solely from camera driving data in an end-to-end manner, without relying on HD maps, and predicts multimodal future trajectories. We show that our model outperforms previous prediction baselines on the NuScenes and Lyft datasets. The code and trained models are available at https://github.com/wayveai/fiery.},
  archive   = {C_ICCV},
  author    = {Anthony Hu and Zak Murez and Nikhil Mohan and Sofía Dudas and Jeffrey Hawke and Vijay Badrinarayanan and Roberto Cipolla and Alex Kendall},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01499},
  pages     = {15253-15262},
  title     = {FIERY: Future instance prediction in bird’s-eye view from surround monocular cameras},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust small object detection on the water surface through
fusion of camera and millimeter wave radar. <em>ICCV</em>, 15243–15252.
(<a href="https://doi.org/10.1109/ICCV48922.2021.01498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, unmanned surface vehicles (USVs) have been experiencing growth in various applications. With the expansion of USVs’ application scenes from the typical marine areas to inland waters, new challenges arise for the object detection task, which is an essential part of the perception system of USVs. In our work, we focus on a relatively unexplored task for USVs in inland waters: small object detection on water surfaces, which is of vital importance for safe autonomous navigation and USVs’ certain missions such as floating waste cleaning. Considering the limitations of vision-based object detection, we propose a novel radar-vision fusion based method for robust small object detection on water surfaces. By using a novel representation format of millimeter wave radar point clouds and applying a deep-level multi-scale fusion of RGB images and radar data, the proposed method can efficiently utilize the characteristics of radar data and improve the accuracy and robustness for small object detection on water surfaces. We test the method on the real-world floating bottle dataset that we collected and released. The result shows that, our method improves the average detection accuracy significantly compared to the vision-based methods and achieves state-of-the-art performance. Besides, the proposed method performs robustly when single sensor degrades.},
  archive   = {C_ICCV},
  author    = {Yuwei Cheng and Hu Xu and Yimin Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01498},
  pages     = {15243-15252},
  title     = {Robust small object detection on the water surface through fusion of camera and millimeter wave radar},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BabelCalib: A universal approach to calibrating central
cameras. <em>ICCV</em>, 15233–15242. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing calibration methods occasionally fail for large field-of-view cameras due to the non-linearity of the underlying problem and the lack of good initial values for all parameters of the used camera model. This might occur because a simpler projection model is assumed in an initial step, or a poor initial guess for the internal parameters is pre-defined. A lot of the difficulties of general camera calibration lie in the use of a forward projection model. We side-step these challenges by first proposing a solver to calibrate the parameters in terms of a back-projection model and then regress the parameters for a target forward model. These steps are incorporated in a robust estimation framework to cope with outlying detections. Extensive experiments demonstrate that our approach is very reliable and returns the most accurate calibration parameters as measured on the downstream task of absolute pose estimation on test sets. The code is released at https://github.com/ylochman/babelcalib.},
  archive   = {C_ICCV},
  author    = {Yaroslava Lochman and Kostiantyn Liepieshov and Jianhui Chen and Michal Perdoch and Christopher Zach and James Pritts},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01497},
  pages     = {15233-15242},
  title     = {BabelCalib: A universal approach to calibrating central cameras},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VSAC: Efficient and accurate estimator for h and
f. <em>ICCV</em>, 15223–15232. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present VSAC, a RANSAC-type robust estimator with a number of novelties. It benefits from the introduction of the concept of independent inliers that improves significantly the efficacy of the dominant plane handling and, also, allows near error-free rejection of incorrect models, without false positives. The local optimization process and its application is improved so that it is run on average only once. Further technical improvements include adaptive sequential hypothesis verification and efficient model estimation via Gaussian elimination. Experiments on four standard datasets show that VSAC is significantly faster than all its predecessors and runs on average in 1-2 ms, on a CPU. It is two orders of magnitude faster and yet as precise as MAGSAC++, the currently most accurate estimator of two-view geometry. In the repeated runs on EVD, HPatches, PhotoTourism, and Kusvod2 datasets, it never failed.},
  archive   = {C_ICCV},
  author    = {Maksym Ivashechkin and Daniel Barath and Jiří Matas},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01496},
  pages     = {15223-15232},
  title     = {VSAC: Efficient and accurate estimator for h and f},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). From goals, waypoints &amp; paths to long term human
trajectory forecasting. <em>ICCV</em>, 15213–15222. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human trajectory forecasting is an inherently multi-modal problem. Uncertainty in future trajectories stems from two sources: (a) sources that are known to the agent but unknown to the model, such as long term goals and (b) sources that are unknown to both the agent and the model, such as the intent of other agents and irreducible randomness in decisions. We propose to factorize this uncertainty into its epistemic and aleatoric sources. We model the epistemic uncertainty through multimodality in long term goals and the aleatoric uncertainty through multimodality in way-points and paths. To exemplify this dichotomy, we also propose a novel long term trajectory forecasting setting, with prediction horizons up to a minute, up to an order of magnitude longer than prior works. Finally, we present Y-net, a scene compliant trajectory forecasting network that exploits the proposed epistemic and aleatoric structure for diverse trajectory predictions across long prediction horizons. Y-net significantly improves previous state-of-the-art performance on both (a) The short prediction horizon setting on the Stanford Drone (31.7\% in FDE) and ETH/UCY datasets (7.4\% in FDE) and (b) The proposed long horizon setting on the re-purposed Stanford Drone and Intersection Drone datasets.},
  archive   = {C_ICCV},
  author    = {Karttikeya Mangalam and Yang An and Harshayu Girase and Jitendra Malik},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01495},
  pages     = {15213-15222},
  title     = {From goals, waypoints &amp; paths to long term human trajectory forecasting},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). End-to-end urban driving by imitating a reinforcement
learning coach. <em>ICCV</em>, 15202–15212. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {End-to-end approaches to autonomous driving commonly rely on expert demonstrations. Although humans are good drivers, they are not good coaches for end-to-end algorithms that demand dense on-policy supervision. On the contrary, automated experts that leverage privileged information can efficiently generate large scale on-policy and off-policy demonstrations. However, existing automated experts for urban driving make heavy use of hand-crafted rules and perform suboptimally even on driving simulators, where ground-truth information is available. To ad-dress these issues, we train a reinforcement learning expert that maps bird’s-eye view images to continuous low-level actions. While setting a new performance upper-bound on CARLA, our expert is also a better coach that provides in-formative supervision signals for imitation learning agents to learn from. Supervised by our reinforcement learning coach, a baseline end-to-end agent with monocular camera-input achieves expert-level performance. Our end-to-end agent achieves a 78\% success rate while generalizing to a new town and new weather on the NoCrash-dense bench-mark and state-of-the-art performance on the more challenging CARLA LeaderBoard.},
  archive   = {C_ICCV},
  author    = {Zhejun Zhang and Alexander Liniger and Dengxin Dai and Fisher Yu and Luc Van Gool},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01494},
  pages     = {15202-15212},
  title     = {End-to-end urban driving by imitating a reinforcement learning coach},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Globally optimal and efficient manhattan frame estimation by
delimiting rotation search space. <em>ICCV</em>, 15193–15201. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A typical man-made structure can be abstracted as the Manhattan world assumption, in which notion is further represented as a Manhattan Frame (MF) defined by three orthogonal axes. The problem of MF estimation can be formulated as the solution of the rotation between the MF and the camera frame (called the &quot;MF rotation&quot;). However, the whole rotation space is quite redundant for solving the MF rotation, which is one of the main factors that disturb the computational efficiency of those methods associated with a rotation space search. This paper proves that the volume of the space that just contains all MF rotations (called the &quot;MFR space&quot;) is only 1 / 24 of that of the whole rotation space, and then an exact MFR space is delimited from the rotation space. Searching in the delimited MFR space, the MF estimation solved by a branch-and-bound (BnB) framework guarantees stability and efficiency simultaneously. Furthermore, the general rotation problems associated with a rotation space search are solved more efficiently. Experiments on synthetic and real datasets have successfully confirmed the validity of our approach.},
  archive   = {C_ICCV},
  author    = {Wuwei Ge and Yu Song and Baichao Zhang and Zehua Dong},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01493},
  pages     = {15193-15201},
  title     = {Globally optimal and efficient manhattan frame estimation by delimiting rotation search space},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Indoor scene generation from a collection of
semantic-segmented depth images. <em>ICCV</em>, 15183–15192. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a method for creating 3D indoor scenes with a generative model learned from a collection of semantic-segmented depth images captured from different unknown scenes. Given a room with a specified size, our method automatically generates 3D objects in a room from a randomly sampled latent code. Different from existing methods that represent an indoor scene with the type, location, and other properties of objects in the room and learn the scene layout from a collection of complete 3D indoor scenes, our method models each indoor scene as a 3D semantic scene volume and learns a volumetric generative adversarial network (GAN) from a collection of 2.5D partial observations of 3D scenes. To this end, we apply a differentiable projection layer to project the generated 3D semantic scene volumes into semantic-segmented depth images and design a new multiple-view discriminator for learning the complete 3D scene volume from 2.5D semantic-segmented depth images. Compared to existing methods, our method not only efficiently reduces the workload of modeling and acquiring 3D scenes for training, but also produces better object shapes and their detailed layouts in the scene. We evaluate our method with different indoor scene datasets and demonstrate the advantages of our method. We also extend our method for generating 3D indoor scenes from semantic-segmented depth images inferred from RGB images of real scenes. 1},
  archive   = {C_ICCV},
  author    = {Ming-Jia Yang and Yu-Xiao Guo and Bin Zhou and Xin Tong},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01492},
  pages     = {15183-15192},
  title     = {Indoor scene generation from a collection of semantic-segmented depth images},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Change is everywhere: Single-temporal supervised object
change detection in remote sensing imagery. <em>ICCV</em>, 15173–15182.
(<a href="https://doi.org/10.1109/ICCV48922.2021.01491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For high spatial resolution (HSR) remote sensing images, bitemporal supervised learning always dominates change detection using many pairwise labeled bitemporal images. However, it is very expensive and time-consuming to pairwise label large-scale bitemporal HSR remote sensing images. In this paper, we propose single-temporal supervised learning (STAR) for change detection from a new perspective of exploiting object changes in unpaired images as supervisory signals. STAR enables us to train a high-accuracy change detector only using unpaired labeled images and generalize to real-world bitemporal images. To evaluate the effectiveness of STAR, we design a simple yet effective change detector called ChangeStar, which can reuse any deep semantic segmentation architecture by the ChangeMixin module. The comprehensive experimental results show that ChangeStar outperforms the baseline with a large margin under single-temporal super-vision and achieves superior performance under bitemporal supervision. Code is available at https://github.com/Z-Zheng/ChangeStar.},
  archive   = {C_ICCV},
  author    = {Zhuo Zheng and Ailong Ma and Liangpei Zhang and Yanfei Zhong},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01491},
  pages     = {15173-15182},
  title     = {Change is everywhere: Single-temporal supervised object change detection in remote sensing imagery},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GRF: Learning a general radiance field for 3D representation
and rendering. <em>ICCV</em>, 15162–15172. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a simple yet powerful neural network that implicitly represents and renders 3D objects and scenes only from 2D observations. The network models 3D geometries as a general radiance field, which takes a set of 2D images with camera poses and intrinsics as input, constructs an internal representation for each point of the 3D space, and then renders the corresponding appearance and geometry of that point viewed from an arbitrary position. The key to our approach is to learn local features for each pixel in 2D images and to then project these features to 3D points, thus yielding general and rich point representations. We additionally integrate an attention mechanism to aggregate pixel features from multiple 2D views, such that visual occlusions are implicitly taken into account. Extensive experiments demonstrate that our method can generate high-quality and realistic novel views for novel objects, unseen categories and challenging real-world scenes.},
  archive   = {C_ICCV},
  author    = {Alex Trevithick and Bo Yang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01490},
  pages     = {15162-15172},
  title     = {GRF: Learning a general radiance field for 3D representation and rendering},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Geometry-based distance decomposition for monocular 3D
object detection. <em>ICCV</em>, 15152–15161. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Monocular 3D object detection is of great significance for autonomous driving but remains challenging. The core challenge is to predict the distance of objects in the absence of explicit depth information. Unlike regressing the distance as a single variable in most existing methods, we propose a novel geometry-based distance decomposition to recover the distance by its factors. The decomposition factors the distance of objects into the most representative and stable variables, i.e. the physical height and the projected visual height in the image plane. Moreover, the decomposition maintains the self-consistency between the two heights, leading to robust distance prediction when both predicted heights are inaccurate. The decomposition also enables us to trace the causes of the distance uncertainty for different scenarios. Such decomposition makes the distance prediction interpretable, accurate, and robust. Our method directly predicts 3D bounding boxes from RGB images with a compact architecture, making the training and inference simple and efficient. The experimental results show that our method achieves the state-of-the-art performance on the monocular 3D Object Detection and Bird’s Eye View tasks of the KITTI dataset, and can generalize to images with different camera intrinsics 1 .},
  archive   = {C_ICCV},
  author    = {Xuepeng Shi and Qi Ye and Xiaozhi Chen and Chuangrong Chen and Zhixiang Chen and Tae-Kyun Kim},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01489},
  pages     = {15152-15161},
  title     = {Geometry-based distance decomposition for monocular 3D object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Waypoint models for instruction-guided navigation in
continuous environments. <em>ICCV</em>, 15142–15151. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Little inquiry has explicitly addressed the role of action spaces in language-guided visual navigation – either in terms of its effect on navigation success or the efficiency with which a robotic agent could execute the resulting trajectory. Building on the recently released VLN-CE [24] setting for instruction following in continuous environments, we develop a class of language-conditioned waypoint prediction networks to examine this question. We vary the expressivity of these models to explore a spectrum between low-level actions and continuous waypoint prediction. We measure task performance and estimated execution time on a profiled LoCoBot [1] robot. We find more expressive models result in simpler, faster to execute trajectories, but lower-level actions can achieve better navigation metrics by approximating shortest paths better. Further, our models outperform prior work in VLN-CE and set a new state-of-the-art on the public leaderboard – increasing success rate by 4\% with our best model on this challenging task.},
  archive   = {C_ICCV},
  author    = {Jacob Krantz and Aaron Gokaslan and Dhruv Batra and Stefan Lee and Oleksandr Maksymets},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01488},
  pages     = {15142-15151},
  title     = {Waypoint models for instruction-guided navigation in continuous environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Active learning for lane detection: A knowledge distillation
approach. <em>ICCV</em>, 15132–15141. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Lane detection is a key task for autonomous driving vehicles. Currently, lane detection relies on a huge amount of annotated images, which is a heavy burden. Active learning has been proposed to reduce annotation in many computer vision tasks, but no effort has been made for lane detection. Through experiments, we find that existing active learning methods perform poorly for lane detection, and the reasons are twofold. On one hand, most methods evaluate data uncertainties based on entropy, which is undesirable in lane detection because it encourages to select images with very few lanes or even no lane at all. On the other hand, existing methods are not aware of the noise of lane annotations, which is caused by heavy occlusion and unclear lane marks. In this paper, we build a novel knowledge distillation framework and evaluate the uncertainty of images based on the knowledge learnt by the student model. We show that the proposed uncertainty metric overcomes the above two problems. To reduce data redundancy, we explore the influence sets of image samples, and propose a new diversity metric for data selection. Finally we incorporate the uncertainty and diversity metrics, and develop a greedy algorithm for data selection. The experiments show that our method achieves new state-of-the-art on the lane detection benchmarks. In addition, we extend this method to common 2D object detection and the results show that it is also effective.},
  archive   = {C_ICCV},
  author    = {Fengchao Peng and Chao Wang and Jianzhuang Liu and Zhen Yang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01487},
  pages     = {15132-15141},
  title     = {Active learning for lane detection: A knowledge distillation approach},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GridToPix: Training embodied agents with minimal
supervision. <em>ICCV</em>, 15121–15131. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While deep reinforcement learning (RL) promises freedom from hand-labeled data, great successes, especially for Embodied AI, require significant work to create supervision via carefully shaped rewards. Indeed, without shaped rewards, i.e., with only terminal rewards, present-day Embodied AI results degrade significantly across Embodied AI problems from single-agent Habitat-based PointGoal Navigation (SPL drops from 55 to 0) and two-agent AI2-THOR-based Furniture Moving (success drops from 58\% to 1\%) to three-agent Google Football-based 3 vs. 1 with Keeper (game score drops from 0.6 to 0.1). As training from shaped rewards doesn’t scale to more realistic tasks, the community needs to improve the success of training with terminal rewards. For this we propose GRIDTOPIX: 1) train agents with terminal rewards in gridworlds that generically mirror Embodied AI environments, i.e., they are independent of the task; 2) distill the learned policy into agents that reside in complex visual worlds. Despite learning from only terminal rewards with identical models and RL algorithms, GRIDTOPIX significantly improves results across tasks: from PointGoal Navigation (SPL improves from 0 to 64) and Furniture Moving (success improves from 1\% to 25\%) to football gameplay (game score improves from 0.1 to 0.6). GRIDTOPIX even helps to improve the results of shaped reward training.},
  archive   = {C_ICCV},
  author    = {Unnat Jain and Iou-Jen Liu and Svetlana Lazebnik and Aniruddha Kembhavi and Luca Weihs and Alexander Schwing},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01486},
  pages     = {15121-15131},
  title     = {GridToPix: Training embodied agents with minimal supervision},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical object-to-zone graph for object navigation.
<em>ICCV</em>, 15110–15120. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The goal of object navigation is to reach the expected objects according to visual information in the unseen environments. Previous works usually implement deep models to train an agent to predict actions in real-time. However, in the unseen environment, when the target object is not in egocentric view, the agent may not be able to make wise decisions due to the lack of guidance. In this paper, we propose a hierarchical object-to-zone (HOZ) graph to guide the agent in a coarse-to-fine manner, and an online-learning mechanism is also proposed to update HOZ according to the real-time observation in new environments. In particular, the HOZ graph is composed of scene nodes, zone nodes and object nodes. With the pre-learned HOZ graph, the real-time observation and the target goal, the agent can constantly plan an optimal path from zone to zone. In the estimated path, the next potential zone is regarded as sub-goal, which is also fed into the deep reinforcement learning model for action prediction. Our methods are evaluated on the AI2-Thor simulator. In addition to widely used evaluation metrics SR and SPL, we also propose a new evaluation metric of SAE that focuses on the effective action rate. Experimental results demonstrate the effectiveness and efficiency of our proposed method. The code is available at https://github.com/sx-zhang/HOZ.git.},
  archive   = {C_ICCV},
  author    = {Sixian Zhang and Xinhang Song and Yubing Bai and Weijie Li and Yakui Chu and Shuqiang Jiang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01485},
  pages     = {15110-15120},
  title     = {Hierarchical object-to-zone graph for object navigation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Social NCE: Contrastive learning of socially-aware motion
representations. <em>ICCV</em>, 15098–15109. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning socially-aware motion representations is at the core of recent advances in multi-agent problems, such as human motion forecasting and robot navigation in crowds. Despite promising progress, existing representations learned with neural networks still struggle to generalize in closed-loop predictions (e.g., output colliding trajectories). This issue largely arises from the non-i.i.d. nature of sequential prediction in conjunction with ill-distributed training data. Intuitively, if the training data only comes from human behaviors in safe spaces, i.e., from &quot;positive&quot; examples, it is difficult for learning algorithms to capture the notion of &quot;negative&quot; examples like collisions. In this work, we aim to address this issue by explicitly modeling negative examples through self-supervision: (i) we intro-duce a social contrastive loss that regularizes the extracted motion representation by discerning the ground-truth positive events from synthetic negative ones; (ii) we construct informative negative samples based on our prior knowledge of rare but dangerous circumstances. Our method substantially reduces the collision rates of recent trajectory forecasting, behavioral cloning and reinforcement learning algorithms, outperforming state-of-the-art methods on several benchmarks. Our code is available at https://github.com/vita-epfl/social-nce.},
  archive   = {C_ICCV},
  author    = {Yuejiang Liu and Qi Yan and Alexandre Alahi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01484},
  pages     = {15098-15109},
  title     = {Social NCE: Contrastive learning of socially-aware motion representations},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ID-reveal: Identity-aware DeepFake video detection.
<em>ICCV</em>, 15088–15097. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A major challenge in DeepFake forgery detection is that state-of-the-art algorithms are mostly trained to detect a specific fake method. As a result, these approaches show poor generalization across different types of facial manipulations, e.g., from face swapping to facial reenactment. To this end, we introduce ID-Reveal, a new approach that learns temporal facial features, specific of how a person moves while talking, by means of metric learning coupled with an adversarial training strategy. The advantage is that we do not need any training data of fakes, but only train on real videos. Moreover, we utilize high-level semantic features, which enables robustness to widespread and disruptive forms of post-processing. We perform a thorough experimental analysis on several publicly available benchmarks. Compared to state of the art, our method improves generalization and is more robust to low-quality videos, that are usually spread over social networks. In particular, we obtain an average improvement of more than 15\% in terms of accuracy for facial reenactment on high compressed videos.},
  archive   = {C_ICCV},
  author    = {Davide Cozzolino and Andreas Rössler and Justus Thies and Matthias Nießner and Luisa Verdoliva},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01483},
  pages     = {15088-15097},
  title     = {ID-reveal: Identity-aware DeepFake video detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Multi-expert adversarial attack detection in person
re-identification using context inconsistency. <em>ICCV</em>,
15077–15087. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The success of deep neural networks (DNNs) has promoted the widespread applications of person re-identification (ReID). However, ReID systems inherit the vulnerability of DNNs to malicious attacks of visually in-conspicuous adversarial perturbations. Detection of adversarial attacks is, therefore, a fundamental requirement for robust ReID systems. In this work, we propose a Multi-Expert Adversarial Attack Detection (MEAAD) approach to achieve this goal by checking context inconsistency, which is suitable for any DNN-based ReID systems. Specifically, three kinds of context inconsistencies caused by adversarial attacks are employed to learn a detector for distinguishing the perturbed examples, i.e., a) the embedding distances between a perturbed query person image and its top-K retrievals are generally larger than those between a benign query image and its top-K retrievals, b) the embedding distances among the top-K retrievals of a perturbed query image are larger than those of a benign query image, c) the top-K retrievals of a benign query image obtained with multiple expert ReID models tend to be consistent, which is not preserved when attacks are present. Extensive experiments on the Market1501 and DukeMTMC-ReID datasets show that, as the first adversarial attack detection approach for ReID, MEAAD effectively detects various adversarial attacks and achieves high ROC-AUC (over 97.5\%).},
  archive   = {C_ICCV},
  author    = {Xueping Wang and Shasha Li and Min Liu and Yaonan Wang and Amit K. Roy-Chowdhury},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01482},
  pages     = {15077-15087},
  title     = {Multi-expert adversarial attack detection in person re-identification using context inconsistency},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PASS: Protected attribute suppression system for mitigating
bias in face recognition. <em>ICCV</em>, 15067–15076. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Face recognition networks encode information about sensitive attributes while being trained for identity classification. Such encoding has two major issues: (a) it makes the face representations susceptible to privacy leakage (b) it appears to contribute to bias in face recognition. However, existing bias mitigation approaches generally require end-to-end training and are unable to achieve high verification accuracy. Therefore, we present a descriptor-based adversarial de-biasing approach called ‘Protected Attribute Suppression System (PASS)’. PASS can be trained on top of descriptors obtained from any previously trained high-performing network to classify identities and simultaneously reduce encoding of sensitive attributes. This eliminates the need for end-to-end training. As a component of PASS, we present a novel discriminator training strategy that discourages a network from encoding protected attribute information. We show the efficacy of PASS to reduce gender and skintone information in descriptors from SOTA face recognition networks like Arcface. As a result, PASS descriptors outperform existing baselines in reducing gender and skintone bias on the IJB-C dataset, while maintaining a high verification accuracy.},
  archive   = {C_ICCV},
  author    = {Prithviraj Dhar and Joshua Gleason and Aniket Roy and Carlos D. Castillo and Rama Chellappa},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01481},
  pages     = {15067-15076},
  title     = {PASS: Protected attribute suppression system for mitigating bias in face recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ensemble attention distillation for privacy-preserving
federated learning. <em>ICCV</em>, 15056–15066. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the problem of Federated Learning (FL) where numerous decentralized computational nodes collaborate with each other to train a centralized machine learning model without explicitly sharing their local data samples. Such decentralized training naturally leads to issues of imbalanced or differing data distributions among the local models and challenges in fusing them into a central model. Existing FL methods deal with these issues by either sharing local parameters or fusing models via online distillation. However, such a design leads to multiple rounds of inter-node communication resulting in substantial band-width consumption, while also increasing the risk of data leakage and consequent privacy issues. To address these problems, we propose a new distillation-based FL frame-work that can preserve privacy by design, while also consuming substantially less network communication resources when compared to the current methods. Our framework engages in inter-node communication using only publicly available and approved datasets, thereby giving explicit privacy control to the user. To distill knowledge among the various local models, our framework involves a novel ensemble distillation algorithm that uses both final prediction as well as model attention. This algorithm explicitly considers the diversity among various local nodes while also seeking consensus among them. This results in a comprehensive technique to distill knowledge from various decentralized nodes. We demonstrate the various aspects and the associated benefits of our FL framework through extensive experiments that produce state-of-the-art results on both classification and segmentation tasks on natural and medical images.},
  archive   = {C_ICCV},
  author    = {Xuan Gong and Abhishek Sharma and Srikrishna Karanam and Ziyan Wu and Terrence Chen and David Doermann and Arun Innanje},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01480},
  pages     = {15056-15066},
  title     = {Ensemble attention distillation for privacy-preserving federated learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive label noise cleaning with meta-supervision for deep
face recognition. <em>ICCV</em>, 15045–15055. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The training of a deep face recognition system usually faces the interference of label noise in the training data. However, it is difficult to obtain a high-precision cleaning model to remove these noises. In this paper, we propose an adaptive label noise cleaning algorithm based on meta-learning for face recognition datasets, which can learn the distribution of the data to be cleaned and make automatic adjustments based on class differences. It first learns re-liable cleaning knowledge from well-labeled noisy data, then gradually transfers it to the target data with meta-supervision to improve performance. A threshold adapter module is also proposed to address the drift problem in transfer learning methods. Extensive experiments clean two noisy in-the-wild face recognition datasets and show the effectiveness of the proposed method to reach state-of-the-art performance on the IJB-C face recognition benchmark.},
  archive   = {C_ICCV},
  author    = {Yaobin Zhang and Weihong Deng and Yaoyao Zhong and Jiani Hu and Xian Li and Dongyue Zhao and Dongchao Wen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01479},
  pages     = {15045-15055},
  title     = {Adaptive label noise cleaning with meta-supervision for deep face recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TransForensics: Image forgery localization with dense
self-attention. <em>ICCV</em>, 15035–15044. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Nowadays advanced image editing tools and technical skills produce tampered images more realistically, which can easily evade image forensic systems and make authenticity verification of images more difficult. To tackle this challenging problem, we introduce TransForensics, a novel image forgery localization method inspired by Transformers. The two major components in our framework are dense self-attention encoders and dense correction modules. The former is to model global context and all pairwise inter-actions between local patches at different scales, while the latter is used for improving the transparency of the hidden layers and correcting the outputs from different branches. Compared to previous traditional and deep learning methods, TransForensics not only can capture discriminative representations and obtain high-quality mask predictions but is also not limited by tampering types and patch sequence orders. By conducting experiments on main bench-marks, we show that TransForensics outperforms the state-of-the-art methods by a large margin.},
  archive   = {C_ICCV},
  author    = {Jing Hao and Zhixin Zhang and Shicai Yang and Di Xie and Shiliang Pu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01478},
  pages     = {15035-15044},
  title     = {TransForensics: Image forgery localization with dense self-attention},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploring temporal coherence for more general video face
forgery detection. <em>ICCV</em>, 15024–15034. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although current face manipulation techniques achieve impressive performance regarding quality and controllability, they are struggling to generate temporal coherent face videos. In this work, we explore to take full advantage of the temporal coherence for video face forgery detection. To achieve this, we propose a novel end-to-end framework, which consists of two major stages. The first stage is a fully temporal convolution network (FTCN). The key insight of FTCN is to reduce the spatial convolution kernel size to 1, while maintaining the temporal convolution kernel size un-changed. We surprisingly find this special design can benefit the model for extracting the temporal features as well as improve the generalization capability. The second stage is a Temporal Transformer network, which aims to explore the long-term temporal coherence. The proposed frame-work is general and flexible, which can be directly trained from scratch without any pre-training models or external datasets. Extensive experiments show that our framework outperforms existing methods and remains effective when applied to detect new sorts of face forgery videos.},
  archive   = {C_ICCV},
  author    = {Yinglin Zheng and Jianmin Bao and Dong Chen and Ming Zeng and Fang Wen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01477},
  pages     = {15024-15034},
  title     = {Exploring temporal coherence for more general video face forgery detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-supervised domain adaptation for forgery localization
of JPEG compressed images. <em>ICCV</em>, 15014–15023. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With wide applications of image editing tools, forged images (splicing, copy-move, removal and etc.) have been becoming great public concerns. Although existing image forgery localization methods could achieve fairly good results on several public datasets, most of them perform poorly when the forged images are JPEG compressed as they are usually done in social networks. To tackle this issue, in this paper, a self-supervised domain adaptation network, which is composed of a backbone network with Siamese architecture and a compression approximation network (ComNet), is proposed for JPEG-resistant image forgery localization. To improve the performance against JPEG compression, ComNet is customized to approximate the JPEG compression operation through self-supervised learning, generating JPEG-agent images with general JPEG compression characteristics. The backbone network is then trained with domain adaptation strategy to localize the tampering boundary and region, and alleviate the domain shift between uncompressed and JPEG-agent images. Extensive experimental results on several public datasets show that the proposed method outperforms or rivals to other state-of-the-art methods in image forgery localization, especially for JPEG compression with unknown QFs.},
  archive   = {C_ICCV},
  author    = {Yuan Rao and Jiangqun Ni},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01476},
  pages     = {15014-15023},
  title     = {Self-supervised domain adaptation for forgery localization of JPEG compressed images},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning self-consistency for deepfake detection.
<em>ICCV</em>, 15003–15013. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a new method to detect deepfake images using the cue of the source feature inconsistency within the forged images. It is based on the hypothesis that images’ distinct source features can be preserved and extracted after going through state-of-the-art deepfake generation processes. We introduce a novel representation learning approach, called pair-wise self-consistency learning (PCL), for training ConvNets to extract these source features and detect deepfake images. It is accompanied by a new image synthesis approach, called inconsistency image genera-tor (I2G), to provide richly annotated training data for PCL. Experimental results on seven popular datasets show that our models improve averaged AUC over the state of the art from 96.45\% to 98.05\% in the in-dataset evaluation and from 86.03\% to 92.18\% in the cross-dataset evaluation.},
  archive   = {C_ICCV},
  author    = {Tianchen Zhao and Xiang Xu and Mingze Xu and Hui Ding and Yuanjun Xiong and Wei Xia},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01475},
  pages     = {15003-15013},
  title     = {Learning self-consistency for deepfake detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TransReID: Transformer-based object re-identification.
<em>ICCV</em>, 14993–15002. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Extracting robust feature representation is one of the key challenges in object re-identification (ReID). Although convolution neural network (CNN)-based methods have achieved great success, they only process one local neighborhood at a time and suffer from information loss on details caused by convolution and downsampling operators (e.g. pooling and strided convolution). To overcome these limitations, we propose a pure transformer-based object ReID framework named TransReID. Specifically, we first encode an image as a sequence of patches and build a transformer-based strong baseline with a few critical improvements, which achieves competitive results on several ReID benchmarks with CNN-based methods. To further enhance the robust feature learning in the context of transformers, two novel modules are carefully designed. (i) The jigsaw patch module (JPM) is proposed to rearrange the patch embeddings via shift and patch shuffle operations which generates robust features with improved discrimination ability and more diversified coverage. (ii) The side information embeddings (SIE) is introduced to mitigate feature bias towards camera/view variations by plugging in learnable embeddings to incorporate these non-visual clues. To the best of our knowledge, this is the first work to adopt a pure transformer for ReID research. Experimental results of TransReID are superior promising, which achieve state-of-the-art performance on both person and vehicle ReID benchmarks. Code is available at https://github.com/heshuting555/TransReID.},
  archive   = {C_ICCV},
  author    = {Shuting He and Hao Luo and Pichao Wang and Fan Wang and Hao Li and Wei Jiang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01474},
  pages     = {14993-15002},
  title     = {TransReID: Transformer-based object re-identification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning bias-invariant representation by cross-sample
mutual information minimization. <em>ICCV</em>, 14982–14992. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep learning algorithms mine knowledge from the training data and thus would likely inherit the dataset’s bias information. As a result, the obtained model would generalize poorly and even mislead the decision process in real-life applications. We propose to remove the bias information misused by the target task with a crosssample adversarial debiasing (CSAD) method. CSAD explicitly extracts target and bias features disentangled from the latent representation generated by a feature extractor and then learns to discover and remove the correlation between the target and bias features. The correlation measurement plays a critical role in adversarial debiasing and is conducted by a cross-sample neural mutual information estimator. Moreover, we propose joint content and local structural representation learning to boost mutual information estimation for better performance. We conduct thorough experiments on publicly available datasets to validate the advantages of the proposed method over state-of-the-art approaches.},
  archive   = {C_ICCV},
  author    = {Wei Zhu and Haitian Zheng and Haofu Liao and Weijian Li and Jiebo Luo},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01473},
  pages     = {14982-14992},
  title     = {Learning bias-invariant representation by cross-sample mutual information minimization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BiaSwap: Removing dataset bias with bias-tailored swapping
augmentation. <em>ICCV</em>, 14972–14981. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep neural networks often make decisions based on the spurious correlations inherent in the dataset, failing to generalize in an unbiased data distribution. Although previous approaches pre-define the type of dataset bias to prevent the network from learning it, recognizing the bias type in the real dataset is often prohibitive. This paper proposes a novel bias-tailored augmentation-based approach, BiaSwap, for learning debiased representation without requiring supervision on the bias type. Assuming that the bias corresponds to the easy-to-learn attributes, we sort the training images based on how much a biased classifier can exploits them as shortcut and divide them into bias-guiding and bias-contrary samples in an unsupervised manner. Afterwards, we integrate the style-transferring module of the image translation model with the class activation maps of such biased classifier, which enables to primarily transfer the bias attributes learned by the classifier. Therefore, given the pair of bias-guiding and bias-contrary, BiaSwap generates the bias-swapped image which contains the bias attributes from the bias-contrary images, while preserving bias-irrelevant ones in the bias-guiding images. Given such augmented images, BiaSwap demonstrates the superiority in debiasing against the existing baselines over both synthetic and real-world datasets. Even without careful supervision on the bias, BiaSwap achieves a remarkable performance on both unbiased and bias-guiding samples, implying the improved generalization capability of the model.},
  archive   = {C_ICCV},
  author    = {Eungyeup Kim and Jihyeon Lee and Jaegul Choo},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01472},
  pages     = {14972-14981},
  title     = {BiaSwap: Removing dataset bias with bias-tailored swapping augmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Understanding and mitigating annotation bias in facial
expression recognition. <em>ICCV</em>, 14960–14971. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The performance of a computer vision model depends on the size and quality of its training data. Recent studies have unveiled previously-unknown composition biases in common image datasets which then lead to skewed model outputs, and have proposed methods to mitigate these biases. However, most existing works assume that human-generated annotations can be considered gold-standard and unbiased. In this paper, we reveal that this assumption can be problematic, and that special care should be taken to prevent models from learning such annotation biases. We focus on facial expression recognition and compare the label biases between lab-controlled and in-the-wild datasets. We demonstrate that many expression datasets contain significant annotation biases between genders, especially when it comes to the happy and angry expressions, and that traditional methods cannot fully mitigate such biases in trained models. To remove expression annotation bias, we propose an AU-Calibrated Facial Expression Recognition (AUCFER) framework that utilizes facial action units (AUs) and incorporates the triplet loss into the objective function. Experimental results suggest that the proposed method is more effective in removing expression annotation bias than existing techniques.},
  archive   = {C_ICCV},
  author    = {Yunliang Chen and Jungseock Joo},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01471},
  pages     = {14960-14971},
  title     = {Understanding and mitigating annotation bias in facial expression recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discover the unknown biased attribute of an image
classifier. <em>ICCV</em>, 14950–14959. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent works find that AI algorithms learn biases from data. Therefore, it is urgent and vital to identify biases in AI algorithms. However, the previous bias identification pipeline overly relies on human experts to conjecture potential biases (e.g., gender), which may neglect other underlying biases not realized by humans. To help human experts better find the AI algorithms’ biases, we study a new problem in this work – for a classifier that predicts a target attribute of the input image, discover its unknown biased attribute.To solve this challenging problem, we use a hyperplane in the generative model’s latent space to represent an image attribute; thus, the original problem is transformed to optimizing the hyperplane’s normal vector and offset. We propose a novel total-variation loss within this framework as the objective function and a new orthogonalization penalty as a constraint. The latter prevents trivial solutions in which the discovered biased attribute is identical with the target or one of the known-biased attributes. Extensive experiments on both disentanglement datasets and real-world datasets show that our method can discover biased attributes and achieve better disentanglement w.r.t. target attributes. Furthermore, the qualitative results show that our method can discover unnoticeable biased attributes for various object and scene classifiers, proving our method’s generalizability for detecting biased attributes in diverse domains of images.},
  archive   = {C_ICCV},
  author    = {Zhiheng Li and Chenliang Xu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01470},
  pages     = {14950-14959},
  title     = {Discover the unknown biased attribute of an image classifier},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ICE: Inter-instance contrastive encoding for unsupervised
person re-identification. <em>ICCV</em>, 14940–14949. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unsupervised person re-identification (ReID) aims at learning discriminative identity features without annotations. Recently, self-supervised contrastive learning has gained increasing attention for its effectiveness in unsupervised representation learning. The main idea of instance contrastive learning is to match a same instance in different augmented views. However, the relationship between different instances has not been fully explored in previous contrastive methods, especially for instance-level contrastive loss. To address this issue, we propose Interinstance Contrastive Encoding (ICE) that leverages interinstance pairwise similarity scores to boost previous classlevel contrastive ReID methods. We first use pairwise similarity ranking as one-hot hard pseudo labels for hard instance contrast, which aims at reducing intra-class variance. Then, we use similarity scores as soft pseudo labels to enhance the consistency between augmented and original views, which makes our model more robust to augmentation perturbations. Experiments on several large-scale person ReID datasets validate the effectiveness of our proposed unsupervised method ICE, which is competitive with even supervised methods. Code is made available at https://github.com/chenhao2345/ICE.},
  archive   = {C_ICCV},
  author    = {Hao Chen and Benoit Lagadec and François Bremond},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01469},
  pages     = {14940-14949},
  title     = {ICE: Inter-instance contrastive encoding for unsupervised person re-identification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards the unseen: Iterative text recognition by distilling
from errors. <em>ICCV</em>, 14930–14939. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual text recognition is undoubtedly one of the most extensively researched topics in computer vision. Great progress have been made to date, with the latest models starting to focus on the more practical &quot;in-the-wild&quot; setting. However, a salient problem still hinders practical deployment – prior state-of-arts mostly struggle with recognising unseen (or rarely seen) character sequences. In this paper, we put forward a novel framework to specifically tackle this “unseen” problem. Our framework is iterative in nature, in that it utilises predicted knowledge of character sequences from a previous iteration, to augment the main network in improving the next prediction. Key to our success is a unique cross-modal variational autoencoder to act as a feedback module, which is trained with the presence of textual error distribution data. This module importantly translates a discrete predicted character space, to a continuous affine transformation parameter space used to condition the visual feature map at next iteration. Experiments on common datasets have shown competitive performance over state-of-the-arts under the conventional setting. Most importantly, under the new disjoint setup where train-test labels are mutually exclusive, ours offers the best performance thus showcasing the capability of generalising onto unseen words (Figure 1 offers a summary).},
  archive   = {C_ICCV},
  author    = {Ayan Kumar Bhunia and Pinaki Nath Chowdhury and Aneeshan Sain and Yi-Zhe Song},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01468},
  pages     = {14930-14939},
  title     = {Towards the unseen: Iterative text recognition by distilling from errors},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint visual semantic reasoning: Multi-stage decoder for
text recognition. <em>ICCV</em>, 14920–14929. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although text recognition has significantly evolved over the years, state-of the-art (SOTA) models still struggle in the wild scenarios due to complex backgrounds, varying fonts, uncontrolled illuminations, distortions and other artifacts. This is because such models solely depend on visual information for text recognition, thus lacking semantic reasoning capabilities. In this paper, we argue that semantic information offers a complementary role in addition to visual only. More specifically, we additionally utilize semantic information by proposing a multi-stage multi-scale attentional decoder that performs joint visual-semantic reasoning. Our novelty lies in the intuition that for text recognition, prediction should be refined in a stage-wise manner. Therefore our key contribution is in designing a stage-wise unrolling attentional decoder where non-differentiability, invoked by discretely predicted character labels, needs to be bypassed for end-to-end training. While the first stage predicts using visual features, subsequent stages refine on-top of it using joint visual-semantic information. Additionally, we introduce multi-scale 2D attention along with dense and residual connections between different stages to deal with varying scales of character sizes, for better performance and faster convergence during training. Experimental results show our approach to outperform existing SOTA methods by a considerable margin.},
  archive   = {C_ICCV},
  author    = {Ayan Kumar Bhunia and Aneeshan Sain and Amandeep Kumar and Shuvozit Ghose and Pinaki Nath Chowdhury and Yi-Zhe Song},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01467},
  pages     = {14920-14929},
  title     = {Joint visual semantic reasoning: Multi-stage decoder for text recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning instance-level spatial-temporal patterns for person
re-identification. <em>ICCV</em>, 14910–14919. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Person re-identification (Re-ID) aims to match pedestrians under dis-joint cameras. Most Re-ID methods formulate it as visual representation learning and image search, and its accuracy is consequently affected greatly by the search space. Spatial-temporal information has been proven to be efficient to filter irrelevant negative samples and significantly improve Re-ID accuracy. However, existing spatial-temporal person Re-ID methods are still rough and do not exploit spatial-temporal information sufficiently. In this paper, we propose a novel Instance-level and Spatial-Temporal Disentangled Re-ID method (InSTD), to improve Re-ID accuracy. In our proposed framework, personalized information such as moving direction is explicitly considered to further narrow down the search space. Besides, the spatial-temporal transferring probability is disentangled from joint distribution to marginal distribution, so that outliers can also be well modeled. Abundant experimental analyses are presented, which demonstrates the superiority and provides more insights into our method. The proposed method achieves mAP of 90.8\% on Market-1501 and 89.1\% on DukeMTMC-reID, improving from the baseline 82.2\% and 72.7\%, respectively. Besides, in order to provide a better benchmark for person re-identification, we release a cleaned data list of DukeMTMC-reID with this paper: https://github.com/RenMin1991/cleaned-DukeMTMC-reID/},
  archive   = {C_ICCV},
  author    = {Min Ren and Lingxiao He and Xingyu Liao and Wu Liu and Yunlong Wang and Tieniu Tan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01466},
  pages     = {14910-14919},
  title     = {Learning instance-level spatial-temporal patterns for person re-identification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3D local convolutional neural networks for gait recognition.
<em>ICCV</em>, 14900–14909. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The goal of gait recognition is to learn the unique spatiotemporal pattern about the human body shape from its temporal changing characteristics. As different body parts behave differently during walking, it is intuitive to model the spatio-temporal patterns of each part separately. However, existing part-based methods equally divide the feature maps of each frame into fixed horizontal stripes to get local parts. It is obvious that these stripe partition-based methods cannot accurately locate the body parts. First, different body parts can appear at the same stripe (e.g., arms and the torso), and one part can appear at different stripes in different frames (e.g., hands). Second, different body parts possess different scales, and even the same part in different frames can appear at different locations and scales. Third, different parts also exhibit distinct movement patterns (e.g., at which frame the movement starts, the position change frequency, how long it lasts). To overcome these issues, we propose novel 3D local operations as a generic family of building blocks for 3D gait recognition backbones. The proposed 3D local operations support the extraction of local 3D volumes of body parts in a sequence with adaptive spatial and temporal scales, locations and lengths. In this way, the spatio-temporal patterns of the body parts are well learned from the 3D local neighborhood in partspecific scales, locations, frequencies and lengths. Experiments demonstrate that our 3D local convolutional neural networks achieve state-of-the-art performance on popular gait datasets. Code is available at: https://github.com/yellowtownhz/3DLocalCNN.},
  archive   = {C_ICCV},
  author    = {Zhen Huang and Dixiu Xue and Xu Shen and Xinmei Tian and Houqiang Li and Jianqiang Huang and Xian-Sheng Hua},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01465},
  pages     = {14900-14909},
  title     = {3D local convolutional neural networks for gait recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Calibrating concepts and operations: Towards symbolic
reasoning on real images. <em>ICCV</em>, 14890–14899. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While neural symbolic methods demonstrate impressive performance in visual question answering on synthetic images, their performance suffers on real images. We identify that the long-tail distribution of visual concepts and unequal importance of reasoning steps in real data are the two key obstacles that limit the models’ real-world potentials. To address these challenges, we propose a new paradigm, Calibrating Concepts and Operations (CCO), which enables neural symbolic models to capture underlying data characteristics and to reason with hierarchical importance. Specifically, we introduce an executor with learnable concept embedding magnitudes for handling distribution imbalance, and an operation calibrator for highlighting important operations and suppressing redundant ones.Our experiments show CCO substantially boosts the performance of neural symbolic methods on real images. By evaluating models on the real world dataset GQA, CCO helps the neural symbolic method NSCL outperforms its vanilla counterpart by 9.1\% (from 47.0\% to 56.1\%); this result also largely reduces the performance gap between symbolic and non-symbolic methods. Additionally, we create a perturbed test set for better understanding and analyzing model performance on real images. Code is available at https://lizw14.github.io/project/ccosr.},
  archive   = {C_ICCV},
  author    = {Zhuowan Li and Elias Stengel-Eskin and Yixiao Zhang and Cihang Xie and Quan Tran and Benjamin Van Durme and Alan Yuille},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01464},
  pages     = {14890-14899},
  title     = {Calibrating concepts and operations: Towards symbolic reasoning on real images},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SemIE: Semantically-aware image extrapolation.
<em>ICCV</em>, 14880–14889. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a semantically-aware novel paradigm to perform image extrapolation that enables the addition of new object instances. All previous methods are limited in their capability of extrapolation to merely extending the already existing objects in the image. However, our proposed approach focuses not only on (i) extending the already present objects but also on (ii) adding new objects in the extended region based on the context. To this end, for a given image, we first obtain an object segmentation map using a state-of-the-art semantic segmentation method. The, thus, obtained segmentation map is fed into a network to compute the extrapolated semantic segmentation and the corresponding panoptic segmentation maps. The input image and the obtained segmentation maps are further utilized to generate the final extrapolated image. We conduct experiments on Cityscapes and ADE20K-bedroom datasets and show that our method outperforms all baselines in terms of FID, and similarity in object co-occurrence statistics. Project url: https://semie-iccv.github.io/},
  archive   = {C_ICCV},
  author    = {Bholeshwar Khurana and Soumya Ranjan Dash and Abhishek Bhatia and Aniruddha Mahapatra and Hrituraj Singh and Kuldeep Kulkarni},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01463},
  pages     = {14880-14889},
  title     = {SemIE: Semantically-aware image extrapolation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LocalTrans: A multiscale local transformer network for
cross-resolution homography estimation. <em>ICCV</em>, 14870–14879. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cross-resolution image alignment is a key problem in multiscale gigapixel photography, which requires to estimate homography matrix using images with large resolution gap. Existing deep homography methods concatenate the input images or features, neglecting the explicit formulation of correspondences between them, which leads to degraded accuracy in cross-resolution challenges. In this paper, we consider the cross-resolution homography estimation as a multimodal problem, and propose a local transformer network embedded within a multiscale structure to explicitly learn correspondences between the multimodal inputs, namely, input images with different resolutions. The proposed local transformer adopts a local attention map specifically for each position in the feature. By combining the local transformer with the multiscale structure, the network is able to capture long-short range correspondences efficiently and accurately. Experiments on both the MS-COCO dataset and the real-captured cross-resolution dataset show that the proposed network outperforms existing state-of-the-art feature-based and deep-learning-based homography estimation methods, and is able to accurately align images under 10× resolution gap.},
  archive   = {C_ICCV},
  author    = {Ruizhi Shao and Gaochang Wu and Yuemei Zhou and Ying Fu and Lu Fang and Yebin Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01462},
  pages     = {14870-14879},
  title     = {LocalTrans: A multiscale local transformer network for cross-resolution homography estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Diverse image style transfer via invertible cross-space
mapping. <em>ICCV</em>, 14860–14869. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Image style transfer aims to transfer the styles of artworks onto arbitrary photographs to create novel artistic images. Although style transfer is inherently an underdetermined problem, existing approaches usually assume a deterministic solution, thus failing to capture the full distribution of possible outputs. To address this limitation, we propose a Diverse Image Style Transfer (DIST) framework which achieves significant diversity by enforcing an invertible cross-space mapping. Specifically, the framework consists of three branches: disentanglement branch, inverse branch, and stylization branch. Among them, the disentanglement branch factorizes artworks into content space and style space; the inverse branch encourages the invertible mapping between the latent space of input noise vectors and the style space of generated artistic images; the stylization branch renders the input content image with the style of an artist. Armed with these three branches, our approach is able to synthesize significantly diverse stylized images without loss of quality. We conduct extensive experiments and comparisons to evaluate our approach qualitatively and quantitatively. The experimental results demonstrate the effectiveness of our method.},
  archive   = {C_ICCV},
  author    = {Haibo Chen and Lei Zhao and Huiming Zhang and Zhizhong Wang and Zhiwen Zuo and Ailin Li and Wei Xing and Dongming Lu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01461},
  pages     = {14860-14869},
  title     = {Diverse image style transfer via invertible cross-space mapping},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image harmonization with transformer. <em>ICCV</em>,
14850–14859. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Image harmonization, aiming to make composite images look more realistic, is an important and challenging task. The composite, synthesized by combining foreground from one image with background from another image, inevitably suffers from the issue of inharmonious appearance caused by distinct imaging conditions, i.e., lights. Current solutions mainly adopt an encoder-decoder architecture with convolutional neural network (CNN) to capture the context of composite images, trying to understand what it looks like in the surrounding background near the foreground. In this work, we seek to solve image harmonization with Transformer, by leveraging its powerful ability of modeling long-range context dependencies, for adjusting foreground light to make it compatible with background light while keeping structure and semantics unchanged. We present the design of our harmonization Transformer frameworks without and with disentanglement, as well as comprehensive experiments and ablation study, demonstrating the power of Transformer and investigating the Transformer for vision. Our method achieves state-of-the-art performance on both image harmonization and image inpainting/enhancement, indicating its superiority. Our code and models are available at https://github.com/zhenglab/HarmonyTransformer.},
  archive   = {C_ICCV},
  author    = {Zonghui Guo and Dongsheng Guo and Haiyong Zheng and Zhaorui Gu and Bing Zheng and Junyu Dong},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01460},
  pages     = {14850-14859},
  title     = {Image harmonization with transformer},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Manifold alignment for semantically aligned style transfer.
<em>ICCV</em>, 14841–14849. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most existing style transfer methods follow the assumption that styles can be represented with global statistics (e.g., Gram matrices or covariance matrices), and thus address the problem by forcing the output and style images to have similar global statistics. An alternative is the assumption of local style patterns, where algorithms are designed to swap similar local features of content and style images. However, the limitation of these existing methods is that they neglect the semantic structure of the content image which may lead to corrupted content structure in the output. In this paper, we make a new assumption that image features from the same semantic region form a manifold and an image with multiple semantic regions follows a multi-manifold distribution. Based on this assumption, the style transfer problem is formulated as aligning two multi-manifold distributions and a Manifold Alignment based Style Transfer (MAST) framework is proposed. The proposed frame-work allows semantically similar regions between the output and the style image share similar style patterns. Moreover, the proposed manifold alignment method is flexible to allow user editing or using semantic segmentation maps as guidance for style transfer. To allow the method to be applicable to photorealistic style transfer, we propose a new adaptive weight skip connection network structure to preserve the content details. Extensive experiments verify the effectiveness of the proposed framework for both artistic and photorealistic style transfer. Code is available at https://github.com/NJUHuoJing/MAST.},
  archive   = {C_ICCV},
  author    = {Jing Huo and Shiyin Jin and Wenbin Li and Jing Wu and Yu-Kun Lai and Yinghuan Shi and Yang Gao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01459},
  pages     = {14841-14849},
  title     = {Manifold alignment for semantically aligned style transfer},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detection and continual learning of novel face presentation
attacks. <em>ICCV</em>, 14831–14840. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Advances in deep learning, combined with availability of large datasets, have led to impressive improvements in face presentation attack detection research. However, state-of-the-art face antispoofing systems are still vulnerable to novel types of attacks that are never seen during training. Moreover, even if such attacks are correctly detected, these systems lack the ability to adapt to newly encountered attacks. The post-training ability of continually detecting new types of attacks and self-adaptation to identify these attack types, after the initial detection phase, is highly appealing. In this paper, we enable a deep neural network to detect anomalies in the observed input data points as potential new types of attacks by suppressing the confidence-level of the network outside the training samples’ distribution. We then use experience replay to update the model to incorporate knowledge about new types of attacks without forgetting the past learned attack types. Experimental results are provided to demonstrate the effectiveness of the proposed method on two benchmark datasets as well as a newly introduced dataset which exhibits a large variety of attack types. 1},
  archive   = {C_ICCV},
  author    = {Mohammad Rostami and Leonidas Spinoulas and Mohamed Hussein and Joe Mathai and Wael Abd-Almageed},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01458},
  pages     = {14831-14840},
  title     = {Detection and continual learning of novel face presentation attacks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust watermarking for deep neural networks via bi-level
optimization. <em>ICCV</em>, 14821–14830. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep neural networks (DNNs) have become state-of-the-art in many application domains. The increasing complexity and cost for building these models demand means for protecting their intellectual property (IP). This paper presents a novel DNN framework that optimizes the robustness of the embedded watermarks. Our method is originated from DNN fault attacks. Different from prior end-to-end DNN watermarking approaches, we only modify a tiny subset of weights to embed the watermark, which also facilities better control of the model behaviors and enables larger rooms for optimizing the robustness of the watermarks.In this paper, built upon the above concept, we pro-pose a bi-level optimization framework where the inner loop phase optimizes the example-level problem to generate robust exemplars, while the outer loop phase proposes a masked adaptive optimization to achieve the robustness of the projected DNN models. Our method alternates the learning of the protected models and watermark exemplars across all phases, where watermark exemplars are not just data samples that could be optimized and adjusted instead. We verify the performance of the proposed methods over a wide range of datasets and DNN architectures. Various transformation attacks including fine-tuning, pruning and overwriting are used to evaluate the robustness.},
  archive   = {C_ICCV},
  author    = {Peng Yang and Yingjie Lao and Ping Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01457},
  pages     = {14821-14830},
  title     = {Robust watermarking for deep neural networks via bi-level optimization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Understanding and evaluating racial biases in image
captioning. <em>ICCV</em>, 14810–14820. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Image captioning is an important task for benchmarking visual reasoning and for enabling accessibility for people with vision impairments. However, as in many machine learning settings, social biases can influence image captioning in undesirable ways. In this work, we study bias propagation pathways within image captioning, focusing specifically on the COCO dataset. Prior work has analyzed gender bias in captions using automatically-derived gender labels; here we examine racial and intersectional biases using manual annotations. Our first contribution is in annotating the perceived gender and skin color of 28,315 of the depicted people after obtaining IRB approval. Using these annotations, we compare racial biases present in both manual and automatically-generated image captions. We demonstrate differences in caption performance, sentiment, and word choice between images of lighter versus darker-skinned people. Further, we find the magnitude of these differences to be greater in modern captioning systems compared to older ones, thus leading to concerns that without proper consideration and mitigation these differences will only become increasingly prevalent. Code and data is available at https://princetonvisualai.github.io/imagecaptioning-bias/.},
  archive   = {C_ICCV},
  author    = {Dora Zhao and Angelina Wang and Olga Russakovsky},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01456},
  pages     = {14810-14820},
  title     = {Understanding and evaluating racial biases in image captioning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Membership inference attacks are easier on difficult
problems. <em>ICCV</em>, 14800–14809. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Membership inference attacks (MIA) try to detect if data samples were used to train a neural network model, e.g. to detect copyright abuses. We show that models with higher dimensional input and output are more vulnerable to MIA, and address in more detail models for image translation and semantic segmentation, including medical image segmentation. We show that reconstruction-errors can lead to very effective MIA attacks as they are indicative of memorization. Unfortunately, reconstruction error alone is less effective at discriminating between non-predictable images used in training and easy to predict images that were never seen before. To overcome this, we propose using a novel predictability error that can be computed for each sample, and its computation does not require a training set. Our membership error, obtained by subtracting the predictability error from the reconstruction error, is shown to achieve high MIA accuracy on an extensive number of benchmarks. 1},
  archive   = {C_ICCV},
  author    = {Avital Shafran and Shmuel Peleg and Yedid Hoshen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01455},
  pages     = {14800-14809},
  title     = {Membership inference attacks are easier on difficult problems},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DisUnknown: Distilling unknown factors for disentanglement
learning. <em>ICCV</em>, 14790–14799. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Disentangling data into interpretable and independent factors is critical for controllable generation tasks. With the availability of labeled data, supervision can help enforce the separation of specific factors as expected. However, it is often expensive or even impossible to label every single factor to achieve fully-supervised disentanglement. In this paper, we adopt a general setting where all factors that are hard to label or identify are encapsulated as a single unknown factor. Under this setting, we propose a flexible weakly-supervised multi-factor disentanglement framework DisUnknown, which Distills Unknown factors for enabling multi-conditional generation regarding both labeled and unknown factors. Specifically, a two-stage training approach is adopted to first disentangle the unknown factor with an effective and robust training method, and then train the final generator with the proper disentanglement of all labeled factors utilizing the unknown distillation. To demonstrate the generalization capacity and scalability of our method, we evaluate it on multiple benchmark datasets qualitatively and quantitatively and further apply it to various real-world applications on complicated datasets.},
  archive   = {C_ICCV},
  author    = {Sitao Xiang and Yuming Gu and Pengda Xiang and Menglei Chai and Hao Li and Yajie Zhao and Mingming He},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01454},
  pages     = {14790-14799},
  title     = {DisUnknown: Distilling unknown factors for disentanglement learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint audio-visual deepfake detection. <em>ICCV</em>,
14780–14789. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deepfakes (&quot;deep learning&quot; + &quot;fake&quot;) are videos synthetically generated with AI algorithms. While they could be entertaining, they could also be misused for falsifying speeches and spreading misinformation. The process to create deepfakes involves both visual and auditory manipulations. Exploration on detecting visual deepfakes has produced a number of detection methods as well as datasets, while audio deepfakes (e.g. synthetic speech from text-to-speech or voice conversion systems) and the relationship between the video and audio modalities have been relatively neglected. In this work, we propose a novel visual / auditory deepfake joint detection task and show that exploiting the intrinsic synchronization between the visual and auditory modalities could benefit deepfake detection. Experiments demonstrate that the proposed joint detection framework outperforms independently trained models, and at the same time, yields superior generalization capability on unseen types of deepfakes.},
  archive   = {C_ICCV},
  author    = {Yipin Zhou and Ser-Nam Lim},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01453},
  pages     = {14780-14789},
  title     = {Joint audio-visual deepfake detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Painting from part. <em>ICCV</em>, 14759–14768. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies the problem of painting the whole image from part of it, namely painting from part or part-painting for short, involving both inpainting and outpainting. To address the challenge of taking full advantage of both information from local domain (part) and knowledge from global domain (dataset), we propose a novel part-painting method according to the observations of relationship between part and whole, which consists of three stages: part-noise restarting, part-feature repainting, and part-patch refining, to paint the whole image by leveraging both feature-level and patch-level part as well as powerful representation ability of generative adversarial network. Extensive ablation studies show efficacy of each stage, and our method achieves state-of-the-art performance on both inpainting and outpainting benchmarks with free-form parts, including our new mask dataset for irregular outpainting. Our code and dataset are available at https://github.com/zhenglab/partpainting.},
  archive   = {C_ICCV},
  author    = {Dongsheng Guo and Haoru Zhao and Yunhao Cheng and Haiyong Zheng and Zhaorui Gu and Bing Zheng},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01451},
  pages     = {14759-14768},
  title     = {Painting from part},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Benchmarking ultra-high-definition image super-resolution.
<em>ICCV</em>, 14749–14758. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Increasingly, modern mobile devices allow capturing images at Ultra-High-Definition (UHD) resolution, which includes 4K and 8K images. However, current single image super-resolution (SISR) methods focus on super-resolving images to ones with resolution up to high definition (HD) and ignore higher-resolution UHD images. To explore their performance on UHD images, in this paper, we first introduce two large-scale image datasets, UHDSR4K and UHDSR8K, to benchmark existing SISR methods. With 70,000 V100 GPU hours of training, we benchmark these methods on 4K and 8K resolution images under seven different settings to provide a set of baseline models. Moreover, we propose a baseline model, called Mesh Attention Network (MANet) for SISR. The MANet applies the attention mechanism in both different depths (horizontal) and different levels of receptive field (vertical). In this way, correlations among feature maps are learned, enabling the network to focus on more important features.},
  archive   = {C_ICCV},
  author    = {Kaihao Zhang and Dongxu Li and Wenhan Luo and Wenqi Ren and Björn Stenger and Wei Liu and Hongdong Li and Ming-Hsuan Yang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01450},
  pages     = {14749-14758},
  title     = {Benchmarking ultra-high-definition image super-resolution},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accelerating atmospheric turbulence simulation via learned
phase-to-space transform. <em>ICCV</em>, 14739–14748. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fast and accurate simulation of imaging through atmospheric turbulence is essential for developing turbulence mitigation algorithms. Recognizing the limitations of previous approaches, we introduce a new concept known as the phase-to-space (P2S) transform to significantly speed up the simulation. P2S is built upon three ideas: (1) reformulating the spatially varying convolution as a set of invariant convolutions with basis functions, (2) learning the basis function via the known turbulence statistics models, (3) implementing the P2S transform via a light-weight network that directly converts the phase representation to spatial representation. The new simulator offers 300× – 1000× speed up compared to the mainstream split-step simulators while preserving the essential turbulence statistics.},
  archive   = {C_ICCV},
  author    = {Zhiyuan Mao and Nicholas Chimitt and Stanley H. Chan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01449},
  pages     = {14739-14748},
  title     = {Accelerating atmospheric turbulence simulation via learned phase-to-space transform},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Click to move: Controlling video generation with sparse
motion. <em>ICCV</em>, 14729–14738. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces Click to Move (C2M), a novel framework for video generation where the user can control the motion of the synthesized video through mouse clicks specifying simple object trajectories of the key objects in the scene. Our model receives as input an initial frame, its corresponding segmentation map and the sparse motion vectors encoding the input provided by the user. It outputs a plausible video sequence starting from the given frame and with a motion that is consistent with user input. Notably, our proposed deep architecture incorporates a Graph Convolution Network (GCN) modelling the movements of all the objects in the scene in a holistic manner and effectively combining the sparse user motion information and image features. Experimental results show that C2M outperforms existing methods on two publicly available datasets, thus demonstrating the effectiveness of our GCN framework at modelling object interactions. The source code is publicly available at https://github.com/PierfrancescoArdino/C2M.},
  archive   = {C_ICCV},
  author    = {Pierfrancesco Ardino and Marco De Nadai and Bruno Lepri and Elisa Ricci and Stéphane Lathuilière},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01448},
  pages     = {14729-14738},
  title     = {Click to move: Controlling video generation with sparse motion},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pathdreamer: A world model for indoor navigation.
<em>ICCV</em>, 14718–14728. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {People navigating in unfamiliar buildings take advantage of myriad visual, spatial and semantic cues to efficiently achieve their navigation goals. Towards equipping computational agents with similar capabilities, we introduce Pathdreamer, a visual world model for agents navigating in novel indoor environments. Given one or more previous visual observations, Pathdreamer generates plausible high-resolution 360° visual observations (RGB, semantic segmentation and depth) for viewpoints that have not been visited, in buildings not seen during training. In regions of high uncertainty (e.g. predicting around corners, imagining the contents of an unseen room), Pathdreamer can predict diverse scenes, allowing an agent to sample multiple realistic outcomes for a given trajectory. We demonstrate that Pathdreamer encodes useful and accessible visual, spatial and semantic knowledge about human environments by using it in the downstream task of Vision-and-Language Navigation (VLN). Specifically, we show that planning ahead with Pathdreamer brings about half the benefit of looking ahead at actual observations from unobserved parts of the environment. We hope that Pathdreamer will help unlock model-based approaches to challenging embodied navigation tasks such as navigating to specified objects and VLN.},
  archive   = {C_ICCV},
  author    = {Jing Yu Koh and Honglak Lee and Yinfei Yang and Jason Baldridge and Peter Anderson},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01447},
  pages     = {14718-14728},
  title     = {Pathdreamer: A world model for indoor navigation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SLAMP: Stochastic latent appearance and motion prediction.
<em>ICCV</em>, 14708–14717. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion is an important cue for video prediction and often utilized by separating video content into static and dynamic components. Most of the previous work utilizing motion is deterministic but there are stochastic methods that can model the inherent uncertainty of the future. Existing stochastic models either do not reason about motion explicitly or make limiting assumptions about the static part. In this paper, we reason about appearance and motion in the video stochastically by predicting the future based on the motion history. Explicit reasoning about motion without history already reaches the performance of current stochastic models. The motion history further improves the results by allowing to predict consistent dynamics several frames into the future. Our model performs comparably to the state-of-the-art models on the generic video prediction datasets, however, significantly outperforms them on two challenging real-world autonomous driving datasets with complex motion and dynamic background.},
  archive   = {C_ICCV},
  author    = {Adil Kaan Akan and Erkut Erdem and Aykut Erdem and Fatma Güney},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01446},
  pages     = {14708-14717},
  title     = {SLAMP: Stochastic latent appearance and motion prediction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Point-based modeling of human clothing. <em>ICCV</em>,
14698–14707. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a new approach to human clothing modeling based on point clouds. Within this approach, we learn a deep model that can predict point clouds of various outfits, for various human poses, and for various human body shapes. Notably, outfits of various types and topologies can be handled by the same model. Using the learned model, we can infer the geometry of new outfits from as little as a single image, and perform outfit retargeting to new bodies in new poses. We complement our geometric model with appearance modeling that uses the point cloud geometry as a geometric scaffolding and employs neural point-based graphics to capture outfit appearance from videos and to re-render the captured outfits. We validate both geometric modeling and appearance modeling aspects of the proposed approach against recently proposed methods and establish the viability of point-based clothing modeling.},
  archive   = {C_ICCV},
  author    = {Ilya Zakharkin and Kirill Mazur and Artur Grigorev and Victor Lempitsky},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01445},
  pages     = {14698-14707},
  title     = {Point-based modeling of human clothing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). IPOKE: Poking a still image for controlled stochastic video
synthesis. <em>ICCV</em>, 14687–14697. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {How would a static scene react to a local poke? What are the effects on other parts of an object if you could locally push it? There will be distinctive movement, despite evident variations caused by the stochastic nature of our world. These outcomes are governed by the characteristic kinematics of objects that dictate their overall motion caused by a local interaction. Conversely, the movement of an object provides crucial information about its underlying distinctive kinematics and the interdependencies between its parts. This two-way relation motivates learning a bijective mapping between object kinematics and plausible future image sequences. Therefore, we propose iPOKE – invertible Prediction of Object Kinematics – that, conditioned on an initial frame and a local poke, allows to sample object kinematics and establishes a one-to-one correspondence to the corresponding plausible videos, thereby providing a controlled stochastic video synthesis. In contrast to previous works, we do not generate arbitrary realistic videos, but provide efficient control of movements, while still capturing the stochastic nature of our environment and the diversity of plausible outcomes it entails. Moreover, our approach can transfer kinematics onto novel object instances and is not confined to particular object classes. Our project page is available at https://bit.ly/3dJN4Lf.},
  archive   = {C_ICCV},
  author    = {Andreas Blattmann and Timo Milbich and Michael Dorkenwald and Björn Ommer},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01444},
  pages     = {14687-14697},
  title     = {IPOKE: Poking a still image for controlled stochastic video synthesis},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attention-based multi-reference learning for image
super-resolution. <em>ICCV</em>, 14677–14686. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a novel Attention-based Multi-Reference Super-resolution network (AMRSR) that, given a low-resolution image, learns to adaptively transfer the most similar texture from multiple reference images to the super-resolution output whilst maintaining spatial coherence. The use of multiple reference images together with attention-based sampling is demonstrated to achieve significantly improved performance over state-of-the-art reference super-resolution approaches on multiple benchmark datasets. Reference super-resolution approaches have recently been proposed to overcome the ill-posed problem of image super-resolution by providing additional information from a high-resolution reference image. Multi-reference super-resolution extends this approach by providing a more diverse pool of image features to overcome the inherent information deficit whilst maintaining memory efficiency. A novel hierarchical attention-based sampling approach is introduced to learn the similarity between low-resolution image features and multiple reference images based on a perceptual loss. Ablation demonstrates the contribution of both multi-reference and hierarchical attention-based sampling to overall performance. Perceptual and quantitative ground-truth evaluation demonstrates significant improvement in performance even when the reference images deviate significantly from the target image. The project website can be found at https://marcopesavento.github.io/AMRSR/},
  archive   = {C_ICCV},
  author    = {Marco Pesavento and Marco Volino and Adrian Hilton},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01443},
  pages     = {14677-14686},
  title     = {Attention-based multi-reference learning for image super-resolution},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic cross feature fusion for remote sensing
pansharpening. <em>ICCV</em>, 14667–14676. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep Convolution Neural Networks have been adopted for pansharpening and achieved state-of-the-art performance. However, most of the existing works mainly focus on single-scale feature fusion, which leads to failure in fully considering relationships of information between high-level semantics and low-level features, despite the network is deep enough. In this paper, we propose a dynamic cross feature fusion network (DCFNet) for pansharpening. Specifically, DCFNet contains multiple parallel branches, including a high-resolution branch served as the backbone, and the low-resolution branches progressively supplemented into the backbone. Thus our DCFNet can represent the overall information well. In order to enhance the relationships of inter-branches, dynamic cross feature transfers are embedded into multiple branches to obtain high-resolution representations. Then contextualized features will be learned to improve the fusion of information. Experimental results indicate that DCFNet significantly outperforms the prior arts in both quantitative indicators and visual qualities.},
  archive   = {C_ICCV},
  author    = {Xiao Wu and Ting-Zhu Huang and Liang-Jian Deng and Tian-Jing Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01442},
  pages     = {14667-14676},
  title     = {Dynamic cross feature fusion for remote sensing pansharpening},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural image compression via attentional multi-scale back
projection and frequency decomposition. <em>ICCV</em>, 14657–14666. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, neural image compression emerges as a rapidly developing topic in computer vision, where the state-of-the-art approaches now exhibit superior compression performance than their conventional counterparts. Despite the great progress, current methods still have limitations in preserving fine spatial details for optimal reconstruction, especially at low compression rates. We make three contributions in tackling this issue. First, we develop a novel back projection method with attentional and multi-scale feature fusion for augmented representation power. Our back projection method recalibrates the current estimation by establishing feedback connections between high-level and low-level attributes in an attentional and discriminative manner. Second, we propose to decompose the input image and separately process the distinct frequency components, whose derived latents are recombined using a novel dual attention module, so that details inside regions of interest could be explicitly manipulated. Third, we propose a novel training scheme for reducing the latent rounding residual. Experimental results show that, when measured in PSNR, our model reduces BD-rate by 9.88\% and 10.32\% over the state-of-the-art method, and 4.12\% and 4.32\% over the latest coding standard Versatile Video Coding (VVC) on the Kodak and CLIC2020 Professional Validation dataset, respectively. Our approach also produces more visually pleasant images when optimized for MS-SSIM. The significant improvement upon existing methods shows the effectiveness of our method in preserving and remedying spatial information for enhanced compression quality.},
  archive   = {C_ICCV},
  author    = {Ge Gao and Pei You and Rong Pan and Shunyuan Han and Yuanyuan Zhang and Yuchao Dai and Hojae Lee},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01441},
  pages     = {14657-14666},
  title     = {Neural image compression via attentional multi-scale back projection and frequency decomposition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep edge-aware interactive colorization against
color-bleeding effects. <em>ICCV</em>, 14647–14656. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep neural networks for automatic image colorization often suffer from the color-bleeding artifact, a problematic color spreading near the boundaries between adjacent objects. Such color-bleeding artifacts debase the reality of generated outputs, limiting the applicability of colorization models in practice. Although previous approaches have attempted to address this problem in an automatic manner, they tend to work only in limited cases where a high contrast of gray-scale values are given in an input image. Alternatively, leveraging user interactions would be a promising approach for solving this color-breeding artifacts. In this paper, we propose a novel edge-enhancing network for the regions of interest via simple user scribbles indicating where to enhance. In addition, our method requires a minimal amount of effort from users for their satisfactory enhancement. Experimental results demonstrate that our interactive edge-enhancing approach effectively improves the color-bleeding artifacts compared to the existing baselines across various datasets.},
  archive   = {C_ICCV},
  author    = {Eungyeup Kim and Sanghyeon Lee and Jeonghoon Park and Somi Choi and Choonghyun Seo and Jaegul Choo},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01440},
  pages     = {14647-14656},
  title     = {Deep edge-aware interactive colorization against color-bleeding effects},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unpaired learning for high dynamic range image tone mapping.
<em>ICCV</em>, 14637–14646. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {High dynamic range (HDR) photography is becoming increasingly popular and available by DSLR and mobile-phone cameras. While deep neural networks (DNN) have greatly impacted other domains of image manipulation, their use for HDR tone-mapping is limited due to the lack of a definite notion of ground-truth solution, which is needed for producing training data.In this paper we describe a new tone-mapping approach guided by the distinct goal of producing low dynamic range (LDR) renditions that best reproduce the visual characteristics of native LDR images. This goal enables the use of an unpaired adversarial training based on unrelated sets of HDR and LDR images, both of which are widely available and easy to acquire.In order to achieve an effective training under this minimal requirements, we introduce the following new steps and components: (i) a range-normalizing pre-process which estimates and applies a different level of curve-based compression, (ii) a loss that preserves the input content while allowing the network to achieve its goal, and (iii) the use of a more concise discriminator network, designed to promote the reproduction of low-level attributes native LDR possess.Evaluation of the resulting network demonstrates its ability to produce photo-realistic artifact-free tone-mapped images, and state-of-the-art performance on different image fidelity indices and visual distances.},
  archive   = {C_ICCV},
  author    = {Yael Vinker and Inbar Huberman-Spiegelglas and Raanan Fattal},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01439},
  pages     = {14637-14646},
  title     = {Unpaired learning for high dynamic range image tone mapping},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gait recognition via effective global-local feature
representation and local temporal aggregation. <em>ICCV</em>,
14628–14636. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Gait recognition is one of the most important biometric technologies and has been applied in many fields. Recent gait recognition frameworks represent each gait frame by descriptors extracted from either global appearances or local regions of humans. However, the representations based on global information often neglect the details of the gait frame, while local region based descriptors cannot capture the relations among neighboring regions, thus reducing their discriminativeness. In this paper, we propose a novel feature extraction and fusion framework to achieve discriminative feature representations for gait recognition. Towards this goal, we take advantage of both global visual information and local region details and develop a Global and Local Feature Extractor (GLFE). Specifically, our GLFE module is composed of our newly designed multiple global and local convolutional layers (GLConv) to ensemble global and local features in a principle manner. Furthermore, we present a novel operation, namely Local Temporal Aggregation (LTA), to further preserve the spatial information by reducing the temporal resolution to obtain higher spatial resolution. With the help of our GLFE and LTA, our method significantly improves the discriminativeness of our visual features, thus improving the gait recognition performance. Extensive experiments demonstrate that our proposed method outperforms state-of-the-art gait recognition methods on two popular datasets.},
  archive   = {C_ICCV},
  author    = {Beibei Lin and Shunli Zhang and Xin Yu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01438},
  pages     = {14628-14636},
  title     = {Gait recognition via effective global-local feature representation and local temporal aggregation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dressing in order: Recurrent person image generation for
pose transfer, virtual try-on and outfit editing. <em>ICCV</em>,
14618–14627. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We proposes a flexible person generation framework called Dressing in Order (DiOr), which supports 2D pose transfer, virtual try-on, and several fashion editing tasks. The key to DiOr is a novel recurrent generation pipeline to sequentially put garments on a person, so that trying on the same garments in different orders will result in different looks. Our system can produce dressing effects not achievable by existing work, including different interactions of garments (e.g., wearing a top tucked into the bottom or over it), as well as layering of multiple garments of the same type (e.g., jacket over shirt over t-shirt). DiOr explicitly encodes the shape and texture of each garment, enabling these elements to be edited separately. Joint training on pose transfer and inpainting helps with detail preservation and coherence of generated garments. Extensive evaluations show that DiOr outperforms other recent methods like ADGAN [28] in terms of output quality, and handles a wide range of editing functions for which there is no direct supervision.},
  archive   = {C_ICCV},
  author    = {Aiyu Cui and Daniel McKee and Svetlana Lazebnik},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01437},
  pages     = {14618-14627},
  title     = {Dressing in order: Recurrent person image generation for pose transfer, virtual try-on and outfit editing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bridging the gap between label- and reference-based
synthesis in multi-attribute image-to-image translation. <em>ICCV</em>,
14608–14617. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The image-to-image translation (I2TT) model takes a target label or a reference image as the input, and changes a source into the specified target domain. The two types of synthesis, either label- or reference-based, have substantial differences. Particularly, the label-based synthesis reflects the common characteristics of the target domain, and the reference-based shows the specific style similar to the reference. This paper intends to bridge the gap between them in the task of multi-attribute I2TT. We design the label- and reference-based encoding modules (LEM and REM) to compare the domain differences. They first transfer the source image and target label (or reference) into a common embedding space, by providing the opposite directions through the attribute difference vector. Then the two embeddings are simply fused together to form the latent code S rand (or S ref ), reflecting the domain style differences, which is injected into each layer of the generator by SPADE. To link LEM and REM, so that two types of results benefit each other, we encourage the two latent codes to be close, and set up the cycle consistency between the forward and backward translations on them. Moreover, the interpolation between the S rand and S ref is also used to synthesize an extra image. Experiments show that label- and reference-based synthesis are indeed mutually promoted, so that we can have the diverse results from LEM, and high quality results with the similar style of the reference. Code will be available at https://github.com/huangqiusheng/BridgeGAN.},
  archive   = {C_ICCV},
  author    = {Qiusheng Huang and Zhilin Zheng and Xueqi Hu and Li Sun and Qingli Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01436},
  pages     = {14608-14617},
  title     = {Bridging the gap between label- and reference-based synthesis in multi-attribute image-to-image translation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). StyleFormer: Real-time arbitrary style transfer via
parametric style composition. <em>ICCV</em>, 14598–14607. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we propose a new feed-forward arbitrary style transfer method, referred to as StyleFormer, which can simultaneously fulfill fine-grained style diversity and semantic content coherency. Specifically, our transformer-inspired feature-level stylization method consists of three modules: (a) the style bank generation module for sparse but compact parametric style pattern extraction, (b) the transformer-driven style composition module for content-guided global style composition, and (c) the parametric content modulation module for flexible but faithful stylization. The output stylized images are impressively coherent with the content structure, sensitive to the detailed style variations, but still holistically adhere to the style distributions from the style images. Qualitative and quantitative comparisons as well as comprehensive user studies demonstrate that our StyleFormer outperforms the existing SOTA methods in generating visually plausible stylization results with real-time efficiency.},
  archive   = {C_ICCV},
  author    = {Xiaolei Wu and Zhihao Hu and Lu Sheng and Dong Xu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01435},
  pages     = {14598-14607},
  title     = {StyleFormer: Real-time arbitrary style transfer via parametric style composition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Domain-aware universal style transfer. <em>ICCV</em>,
14589–14597. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Style transfer aims to reproduce content images with the styles from reference images. Existing universal style transfer methods successfully deliver arbitrary styles to original images either in an artistic or a photo-realistic way. However, the range of &quot;arbitrary style&quot; defined by existing works is bounded in the particular domain due to their structural limitation. Specifically, the degrees of content preservation and stylization are established according to a predefined target domain. As a result, both photo-realistic and artistic models have difficulty in performing the desired style transfer for the other domain. To overcome this limitation, we propose a unified architecture, Domain-aware Style Transfer Networks (DSTN) that transfer not only the style but also the property of domain (i.e., domainness) from a given reference image. To this end, we design a novel domainness indicator that captures the domainness value from the texture and structural features of reference images. Moreover, we introduce a unified framework with domainaware skip connection to adaptively transfer the stroke and palette to the input contents guided by the domainness indicator. Our extensive experiments validate that our model produces better qualitative results and outperforms previous methods in terms of proxy metrics on both artistic and photo-realistic stylizations. All codes and pre-trained weights are available at Kibeom-Hong/Domain-Aware-Style-Transfer.},
  archive   = {C_ICCV},
  author    = {Kibeom Hong and Seogkyu Jeon and Huan Yang and Jianlong Fu and Hyeran Byun},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01434},
  pages     = {14589-14597},
  title     = {Domain-aware universal style transfer},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Flow-guided video inpainting with scene templates.
<em>ICCV</em>, 14579–14588. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the problem of filling in missing spatiotemporal regions of a video. We provide a novel flow-based solution by introducing a generative model of images in relation to the scene (without missing regions) and mappings from the scene to images. We use the model to jointly infer the scene template, a 2D representation of the scene, and the mappings. This ensures consistency of the frame-to-frame flows generated to the underlying scene, reducing geometric distortions in flow based inpainting. The template is mapped to the missing regions in the video by a new (L 2 -L 1 ) interpolation scheme, creating crisp inpaintings and reducing common blur and distortion artifacts. We show on two benchmark datasets that our approach out-performs state-of-the-art quantitatively and in user studies. 1},
  archive   = {C_ICCV},
  author    = {Dong Lao and Peihao Zhu and Peter Wonka and Ganesh Sundaramoorthi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01433},
  pages     = {14579-14588},
  title     = {Flow-guided video inpainting with scene templates},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Training weakly supervised video frame interpolation with
events. <em>ICCV</em>, 14569–14578. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Event-based video frame interpolation is promising as event cameras capture dense motion signals that can greatly facilitate motion-aware synthesis. However, training existing frameworks for this task requires high frame-rate videos with synchronized events, posing challenges to collect real training data. In this work we show event-based frame interpolation can be trained without the need of high frame-rate videos. This is achieved via a novel weakly supervised framework that 1) corrects image appearance by extracting complementary information from events and 2) supplants motion dynamics modeling with attention mechanisms. For the latter we propose subpixel attention learning, which supports searching high-resolution correspondence efficiently on low-resolution feature grid. Though trained on low frame-rate videos, our framework outperforms existing models trained with full high frame-rate videos (and events) on both GoPro dataset and a new real event-based dataset. Codes, models and dataset will be made available at: https://github.com/YU-Zhiyang/WEVI.},
  archive   = {C_ICCV},
  author    = {Zhiyang Yu and Yu Zhang and Deyuan Liu and Dongqing Zou and Xijun Chen and Yebin Liu and Jimmy Ren},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01432},
  pages     = {14569-14578},
  title     = {Training weakly supervised video frame interpolation with events},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Internal video inpainting by implicit long-range
propagation. <em>ICCV</em>, 14559–14568. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel framework for video inpainting by adopting an internal learning strategy. Unlike previous methods that use optical flow for cross-frame context propagation to inpaint unknown regions, we show that this can be achieved implicitly by fitting a convolutional neural network to known regions. Moreover, to handle challenging sequences with ambiguous backgrounds or long-term occlusion, we design two regularization terms to preserve high-frequency details and long-term temporal consistency. Extensive experiments on the DAVIS dataset demonstrate that the proposed method achieves state-of-the-art inpainting quality quantitatively and qualitatively. We further extend the proposed method to another challenging task: learning to remove an object from a video giving a single object mask in only one frame in a 4K video. Our source code is available at https://tengfei-wang.github.io/Implicit-Internal-Video-Inpainting/.},
  archive   = {C_ICCV},
  author    = {Hao Ouyang and Tengfei Wang and Qifeng Chen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01431},
  pages     = {14559-14568},
  title     = {Internal video inpainting by implicit long-range propagation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards complete scene and regular shape for distortion
rectification by curve-aware extrapolation. <em>ICCV</em>, 14549–14558.
(<a href="https://doi.org/10.1109/ICCV48922.2021.01430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The wide-angle lens gains increasing attention since it can capture a wide field-of-view (FoV) scene. However, the obtained image is contaminated with radial distortion, making the scene not realistic. Previous distortion rectification methods rectify the image in a rectangle or invagination, failing to display the complete content and regular shape simultaneously. In this paper, we rethink the representation of rectification results and present a Rectification OutPainting (ROP) method, aiming to extrapolate the coherent semantics to the blank area and create a wider FoV beyond the original wide-angle lens. To address the specific challenges such as the variable painting region and curve boundary, a rectification module is designed to rectify the image with geometry supervision, and the extrapolated results are generated using a dual conditional expansion strategy. In terms of the spatially discounted correlation, a curve-aware correlation measurement is proposed to focus on the generated region to enforce the local consistency. To our knowledge, we are the first to tackle the challenging rectification via outpainting, and our curve-aware strategy can reach a rectification construction with complete content and regular shape. Extensive experiments well demonstrate the superiority of our ROP over other state-of-the-art solutions.},
  archive   = {C_ICCV},
  author    = {Kang Liao and Chunyu Lin and Yunchao Wei and Feng Li and Shangrong Yang and Yao Zhao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01430},
  pages     = {14549-14558},
  title     = {Towards complete scene and regular shape for distortion rectification by curve-aware extrapolation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parallel multi-resolution fusion network for image
inpainting. <em>ICCV</em>, 14539–14548. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Conventional deep image inpainting methods are based on auto-encoder architecture, in which the spatial details of images will be lost in the down-sampling process, leading to the degradation of generated results. Also, the structure information in deep layers and texture information in shallow layers of the auto-encoder architecture can not be well integrated. Differing from the conventional image inpainting architecture, we design a parallel multi-resolution inpainting network with multi-resolution partial convolution, in which low-resolution branches focus on the global structure while high-resolution branches focus on the local texture details. All these high- and low-resolution streams are in parallel and fused repeatedly with multi-resolution masked representation fusion so that the reconstructed images are semantically robust and textually plausible. Experimental results show that our method can effectively fuse structure and texture information, producing more realistic results than state-of-the-art methods.},
  archive   = {C_ICCV},
  author    = {Wentao Wang and Jianfu Zhang and Li Niu and Haoyu Ling and Xue Yang and Liqing Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01429},
  pages     = {14539-14548},
  title     = {Parallel multi-resolution fusion network for image inpainting},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). STRIVE: Scene text replacement in videos. <em>ICCV</em>,
14529–14538. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose replacing scene text in videos using deep style transfer and learned photometric transformations. Building on recent progress on still image text replacement, we present extensions that alter text while preserving the appearance and motion characteristics of the original video. Compared to the problem of still image text replacement, our method addresses additional challenges introduced by video, namely effects induced by changing lighting, motion blur, diverse variations in camera-object pose over time, and preservation of temporal consistency. We parse the problem into three steps. First, the text in all frames is normalized to a frontal pose using a spatio-temporal transformer network. Second, the text is replaced in a single reference frame using a state-of-art still-image text replacement method. Finally, the new text is transferred from the reference to remaining frames using a novel learned image transformation network that captures lighting and blur effects in a temporally consistent manner. Results on synthetic and challenging real videos show realistic text transfer, competitive quantitative and qualitative performance, and superior inference speed relative to alternatives. We introduce new synthetic and real-world datasets with paired text objects. To the best of our knowledge this is the first attempt at deep video text replacement.},
  archive   = {C_ICCV},
  author    = {Vijay Kumar B G and Jeyasri Subramanian and Varnith Chordia and Eugene Bart and Shaobo Fang and Kelly Guan and Raja Bala},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01428},
  pages     = {14529-14538},
  title     = {STRIVE: Scene text replacement in videos},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Asymmetric bilateral motion estimation for video frame
interpolation. <em>ICCV</em>, 14519–14528. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel video frame interpolation algorithm based on asymmetric bilateral motion estimation (ABME), which synthesizes an intermediate frame between two input frames. First, we predict symmetric bilateral motion fields to interpolate an anchor frame. Second, we estimate asymmetric bilateral motions fields from the anchor frame to the input frames. Third, we use the asymmetric fields to warp the input frames backward and reconstruct the intermediate frame. Last, to refine the intermediate frame, we develop a new synthesis network that generates a set of dynamic filters and a residual frame using local and global information. Experimental results show that the proposed algorithm achieves excellent performance on various datasets. The source codes and pretrained models are available at https://github.com/JunHeum/ABME.},
  archive   = {C_ICCV},
  author    = {Junheum Park and Chul Lee and Chang-Su Kim},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01427},
  pages     = {14519-14528},
  title     = {Asymmetric bilateral motion estimation for video frame interpolation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EgoRenderer: Rendering human avatars from egocentric camera
images. <em>ICCV</em>, 14508–14518. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present EgoRenderer, a system for rendering full-body neural avatars of a person captured by a wearable, egocentric fisheye camera that is mounted on a cap or a VR headset. Our system renders photorealistic novel views of the actor and her motion from arbitrary virtual camera locations. Rendering full-body avatars from such egocentric images come with unique challenges due to the top-down view and large distortions. We tackle these challenges by decomposing the rendering process into several steps, including texture synthesis, pose construction, and neural image translation. For texture synthesis, we propose Ego-DPNet, a neural network that infers dense correspondences between the input fisheye images and an underlying parametric body model, and to extract textures from egocentric inputs. In addition, to encode dynamic appearances, our approach also learns an implicit texture stack that captures detailed appearance variation across poses and viewpoints. For correct pose generation, we first estimate body pose from the egocentric view using a parametric model. We then synthesize an external free-viewpoint pose image by projecting the parametric model to the user-specified target viewpoint. We next combine the target pose image and the textures into a combined feature image, which is transformed into the output color image using a neural image translation network. Experimental evaluations show that EgoRenderer is capable of generating realistic free-viewpoint avatars of a person wearing an egocentric camera. Comparisons to several baselines demonstrate the advantages of our approach.},
  archive   = {C_ICCV},
  author    = {Tao Hu and Kripasindhu Sarkar and Lingjie Liu and Matthias Zwicker and Christian Theobalt},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01426},
  pages     = {14508-14518},
  title     = {EgoRenderer: Rendering human avatars from egocentric camera images},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Embedding novel views in a single JPEG image. <em>ICCV</em>,
14499–14507. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel approach for embedding novel views in a single JPEG image while preserving the perceptual fidelity of the modified JPEG image and the restored novel views. We adopt the popular novel view synthesis representation of multiplane images (MPIs). Our model first encodes 32 MPI layers (totally 128 channels) into a 3-channel JPEG image that can be decoded for MPIs to render novel views, with an embedding capacity of 1024 bits per pixel. We conducted experiments on public datasets with different novel view synthesis methods, and the results show that the proposed method can restore high-fidelity novel views from a slightly modified JPEG image. Furthermore, our method is robust to JPEG compression, color adjusting, and cropping. Our source code will be publicly available.},
  archive   = {C_ICCV},
  author    = {Yue Wu and Guotao Meng and Qifeng Chen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01425},
  pages     = {14499-14507},
  title     = {Embedding novel views in a single JPEG image},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning a sketch tensor space for image inpainting of
man-made scenes. <em>ICCV</em>, 14489–14498. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies the task of inpainting man-made scenes. It is very challenging due to the difficulty in preserving the visual patterns of images, such as edges, lines, and junctions. Especially, most previous works are failed to restore the object/building structures for images of man-made scenes. To this end, this paper proposes learning a Sketch Tensor (ST) space for inpainting man-made scenes. Such a space is learned to restore the edges, lines, and junctions in images, and thus makes reliable predictions of the holistic image structures. To facilitate the structure refinement, we propose a Multi-scale Sketch Tensor inpainting (MST) network, with a novel encoder-decoder structure. The encoder extracts lines and edges from the input images to project them into an ST space. From this space, the decoder is learned to restore the input images. Extensive experiments validate the efficacy of our model. Furthermore, our model can also achieve competitive performance in inpainting general nature images over the competitors.},
  archive   = {C_ICCV},
  author    = {Chenjie Cao and Yanwei Fu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01424},
  pages     = {14489-14498},
  title     = {Learning a sketch tensor space for image inpainting of man-made scenes},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). OSCAR-net: Object-centric scene graph attention for image
attribution. <em>ICCV</em>, 14479–14488. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Images tell powerful stories but cannot always be trusted. Matching images back to trusted sources (attribution) enables users to make a more informed judgment of the images they encounter online. We propose a robust image hashing algorithm to perform such matching. Our hash is sensitive to manipulation of subtle, salient visual details that can substantially change the story told by an image. Yet the hash is invariant to benign transformations (changes in quality, codecs, sizes, shapes, etc.) experienced by images during online redistribution. Our key contribution is OSCAR-Net 1 (Object-centric Scene Graph Attention for Image Attribution Network); a robust image hashing model inspired by recent successes of Transformers in the visual domain. OSCAR-Net constructs a scene graph representation that attends to fine-grained changes of every object’s visual appearance and their spatial relationships. The network is trained via contrastive learning on a dataset of original and manipulated images yielding a state of the art image hash for content fingerprinting that scales to millions of images.},
  archive   = {C_ICCV},
  author    = {Eric Nguyen and Tu Bui and Viswanathan Swaminathan and John Collomosse},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01423},
  pages     = {14479-14488},
  title     = {OSCAR-net: Object-centric scene graph attention for image attribution},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). XVFI: EXtreme video frame interpolation. <em>ICCV</em>,
14469–14478. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we firstly present a dataset (X4K1000FPS) of 4K videos of 1000 fps with the extreme motion to the research community for video frame interpolation (VFI), and propose an extreme VFI network, called XVFI-Net, that first handles the VFI for 4K videos with large motion. The XVFI-Net is based on a recursive multi-scale shared structure that consists of two cascaded modules for bidirectional optical flow learning between two input frames (BiOF-I) and for bidirectional optical flow learning from target to input frames (BiOF-T). The optical flows are stably approximated by a complementary flow reversal (CFR) proposed in BiOF-T module. During inference, the BiOF-I module can start at any scale of input while the BiOF-T module only operates at the original input scale so that the inference can be accelerated while maintaining highly accurate VFI performance. Extensive experimental results show that our XVFI-Net can successfully capture the essential information of objects with extremely large motions and complex textures while the state-of-the-art methods exhibit poor performance. Furthermore, our XVFI-Net framework also performs comparably on the previous lower resolution benchmark dataset, which shows a robustness of our algorithm as well. All source codes, pre-trained models, and proposed X4K1000FPS datasets are publicly available at https://github.com/JihyongOh/XVFI.},
  archive   = {C_ICCV},
  author    = {Hyeonjun Sim and Jihyong Oh and Munchurl Kim},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01422},
  pages     = {14469-14478},
  title     = {XVFI: EXtreme video frame interpolation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ELF-VC: Efficient learned flexible-rate video coding.
<em>ICCV</em>, 14459–14468. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While learned video codecs have demonstrated great promise, they have yet to achieve sufficient efficiency for practical deployment. In this work, we propose several novel ideas for learned video compression which allow for improved performance for the low-latency mode (I- and P-frames only) along with a considerable increase in computational efficiency. In this setting, for natural videos our approach compares favorably across the entire R-D curve under metrics PSNR, MS-SSIM and VMAF against all mainstream video standards (H.264, H.265, AV1) and all ML codecs. At the same time, our approach runs at least 5x faster and has fewer parameters than all ML codecs which report these figures.Our contributions include a flexible-rate framework allowing a single model to cover a large and dense range of bitrates, at a negligible increase in computation and parameter count; an efficient backbone optimized for ML-based codecs; and a novel in-loop flow prediction scheme which leverages prior information towards more efficient compression.We benchmark our method, which we call ELF-VC (Efficient, Learned and Flexible Video Coding) on popular video test sets UVG and MCL-JCV under metrics PSNR, MS-SSIM and VMAF. For example, on UVG under PSNR, it reduces the BD-rate by 44\% against H.264, 26\% against H.265, 15\% against AV1, and 35\% against the current best ML codec. At the same time, on an NVIDIA Titan V GPU our approach encodes/decodes VGA at 49/91 FPS, HD 720 at 19/35 FPS, and HD 1080 at 10/18 FPS.},
  archive   = {C_ICCV},
  author    = {Oren Rippel and Alexander G. Anderson and Kedar Tatwawadi and Sanjay Nair and Craig Lytle and Lubomir Bourdev},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01421},
  pages     = {14459-14468},
  title     = {ELF-VC: Efficient learned flexible-rate video coding},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Occlusion-aware video object inpainting. <em>ICCV</em>,
14448–14458. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Conventional video inpainting is neither object-oriented nor occlusion-aware, making it liable to obvious artifacts when large occluded object regions are inpainted. This paper presents occlusion-aware video object inpainting, which recovers both the complete shape and appearance for occluded objects in videos given their visible mask segmentation.To facilitate this new research, we construct the first large-scale video object inpainting benchmark YouTube-VOI to provide realistic occlusion scenarios with both occluded and visible object masks available. Our technical contribution VOIN jointly performs video object shape completion and occluded texture generation. In particular, the shape completion module models long-range object coherence while the flow completion module recovers accurate flow with sharp motion boundary, for propagating temporally-consistent texture to the same moving object across frames. For more realistic results, VOIN is optimized using both T-PatchGAN and a new spatio-temporal attention-based multi-class discriminator.Finally, we compare VOIN and strong baselines on YouTube-VOI. Experimental results clearly demonstrate the efficacy of our method including inpainting complex and dynamic objects. VOIN degrades gracefully with inaccurate input visible mask.},
  archive   = {C_ICCV},
  author    = {Lei Ke and Yu-Wing Tai and Chi-Keung Tang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01420},
  pages     = {14448-14458},
  title     = {Occlusion-aware video object inpainting},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Infinite nature: Perpetual view generation of natural scenes
from a single image. <em>ICCV</em>, 14438–14447. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce the problem of perpetual view generation— long-range generation of novel views corresponding to an arbitrarily long camera trajectory given a single image. This is a challenging problem that goes far beyond the capabilities of current view synthesis methods, which quickly degenerate when presented with large camera motions. Methods for video generation also have limited ability to produce long sequences and are often agnostic to scene geometry. We take a hybrid approach that integrates both geometry and image synthesis in an iterative ‘render, refine and repeat’ framework, allowing for long-range generation that cover large distances after hundreds of frames. Our approach can be trained from a set of monocular video sequences. We propose a dataset of aerial footage of coastal scenes, and compare our method with recent view synthesis and conditional video generation baselines, showing that it can generate plausible scenes for much longer time horizons over large camera trajectories compared to existing methods. Project page at https://infinite-nature.github.io.},
  archive   = {C_ICCV},
  author    = {Andrew Liu and Ameesh Makadia and Richard Tucker and Noah Snavely and Varun Jampani and Angjoo Kanazawa},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01419},
  pages     = {14438-14447},
  title     = {Infinite nature: Perpetual view generation of natural scenes from a single image},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Artificial fingerprinting for generative models: Rooting
deepfake attribution in training data. <em>ICCV</em>, 14428–14437. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Photorealistic image generation has reached a new level of quality due to the breakthroughs of generative adversarial networks (GANs). Yet, the dark side of such deepfakes, the malicious use of generated media, raises concerns about visual misinformation. While existing research work on deepfake detection demonstrates high accuracy, it is subject to advances in generation techniques and adversarial iterations on detection countermeasure techniques. Thus, we seek a proactive and sustainable solution on deepfake detection, that is agnostic to the evolution of generative models, by introducing artificial fingerprints into the models.Our approach is simple and effective. We first embed artificial fingerprints into training data, then validate a surprising discovery on the transferability of such fingerprints from training data to generative models, which in turn appears in the generated deepfakes. Experiments show that our fingerprinting solution (1) holds for a variety of cutting-edge generative models, (2) leads to a negligible side effect on generation quality, (3) stays robust against image-level and model-level perturbations, (4) stays hard to be detected by adversaries, and (5) converts deepfake detection and attribution into trivial tasks and outperforms the recent state-of-the-art baselines. Our solution closes the responsibility loop between publishing pre-trained generative model inventions and their possible misuses, which makes it independent of the current arms race.},
  archive   = {C_ICCV},
  author    = {Ning Yu and Vladislav Skripniuk and Sahar Abdelnabi and Mario Fritz},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01418},
  pages     = {14428-14437},
  title     = {Artificial fingerprinting for generative models: Rooting deepfake attribution in training data},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dual projection generative adversarial networks for
conditional image generation. <em>ICCV</em>, 14418–14427. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Conditional Generative Adversarial Networks (cGANs) extend the standard unconditional GAN framework to learning joint data-label distributions from samples, and have been established as powerful generative models capable of generating high-fidelity imagery. A challenge of training such a model lies in properly infusing class information into its generator and discriminator. For the discriminator, class conditioning can be achieved by either (1) directly incorporating labels as input or (2) involving labels in an auxiliary classification loss. In this paper, we show that the former directly aligns the class-conditioned fake-and-real data distributions P (image|class) (data matching), while the latter aligns data-conditioned class distributions P (class|image) (label matching). Although class separability does not directly translate to sample quality and becomes a burden if classification itself is intrinsically difficult, the discriminator cannot provide useful guidance for the generator if features of distinct classes are mapped to the same point and thus become inseparable. Motivated by this intuition, we propose a Dual Projection GAN (P2GAN) model that learns to balance between data matching and label matching. We then propose an improved cGAN model with Auxiliary Classification that directly aligns the fake and real conditionals P (class|image) by minimizing their f-divergence. Experiments on a synthetic Mixture of Gaussian (MoG) dataset and a variety of real-world datasets including CIFAR100, ImageNet, and VGGFace2 demonstrate the efficacy of our proposed models.},
  archive   = {C_ICCV},
  author    = {Ligong Han and Martin Renqiang Min and Anastasis Stathopoulos and Yu Tian and Ruijiang Gao and Asim Kadav and Dimitris Metaxas},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01417},
  pages     = {14418-14427},
  title     = {Dual projection generative adversarial networks for conditional image generation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Latent transformations via NeuralODEs for GAN-based image
editing. <em>ICCV</em>, 14408–14417. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in high-fidelity semantic image editing heavily rely on the presumably disentangled latent spaces of the state-of-the-art generative models, such as Style-GAN. Specifically, recent works show that it is possible to achieve decent controllability of attributes in face images via linear shifts along with latent directions. Several recent methods address the discovery of such directions, implicitly assuming that the state-of-the-art GANs learn the latent spaces with inherently linearly separable attribute distributions and semantic vector arithmetic properties.In our work, we show that nonlinear latent code manipulations realized as flows of a trainable Neural ODE are beneficial for many practical non-face image domains with more complex non-textured factors of variation. In particular, we investigate a large number of datasets with known attributes and demonstrate that certain attribute manipulations are challenging to obtain with linear shifts only.},
  archive   = {C_ICCV},
  author    = {Valentin Khrulkov and Leyla Mirvakhabova and Ivan Oseledets and Artem Babenko},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01416},
  pages     = {14408-14417},
  title     = {Latent transformations via NeuralODEs for GAN-based image editing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Collaging class-specific GANs for semantic image synthesis.
<em>ICCV</em>, 14398–14407. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a new approach for high resolution semantic image synthesis. It consists of one base image generator and multiple class-specific generators. The base generator generates high quality images based on a segmentation map. To further improve the quality of different objects, we create a bank of Generative Adversarial Networks (GANs) by separately training class-specific models. This has several benefits including – dedicated weights for each class; centrally aligned data for each model; additional training data from other sources, potential of higher resolution and quality; and easy manipulation of a specific object in the scene. Experiments show that our approach can generate high quality images in high resolution while having flexibility of object-level control by using class-specific generators.},
  archive   = {C_ICCV},
  author    = {Yuheng Li and Yijun Li and Jingwan Lu and Eli Shechtman and Yong Jae Lee and Krishna Kumar Singh},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01415},
  pages     = {14398-14407},
  title     = {Collaging class-specific GANs for semantic image synthesis},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EigenGAN: Layer-wise eigen-learning for GANs. <em>ICCV</em>,
14388–14397. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent studies on Generative Adversarial Network (GAN) reveal that different layers of a generative CNN hold different semantics of the synthesized images. However, few GAN models have explicit dimensions to control the semantic attributes represented in a specific layer. This paper proposes EigenGAN which is able to unsupervisedly mine interpretable and controllable dimensions from different generator layers. Specifically, EigenGAN embeds one linear subspace with orthogonal basis into each generator layer. Via generative adversarial training to learn a target distribution, these layer-wise subspaces automatically discover a set of &quot;eigen-dimensions&quot; at each layer corresponding to a set of semantic attributes or interpretable variations. By traversing the coefficient of a specific eigen-dimension, the generator can produce samples with continuous changes corresponding to a specific semantic attribute. Taking the human face for example, EigenGAN can discover controllable dimensions for high-level concepts such as pose and gender in the subspace of deep layers, as well as low-level concepts such as hue and color in the subspace of shallow layers. Moreover, in the linear case, we theoretically prove that our algorithm derives the principal components as PCA does. Codes can be found in https://github.com/LynnHo/EigenGAN-Tensorflow.},
  archive   = {C_ICCV},
  author    = {Zhenliang He and Meina Kan and Shiguang Shan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01414},
  pages     = {14388-14397},
  title     = {EigenGAN: Layer-wise eigen-learning for GANs},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HeadGAN: One-shot neural head synthesis and editing.
<em>ICCV</em>, 14378–14387. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent attempts to solve the problem of head reenactment using a single reference image have shown promising results. However, most of them either perform poorly in terms of photo-realism, or fail to meet the identity preservation problem, or do not fully transfer the driving pose and expression. We propose HeadGAN, a novel system that conditions synthesis on 3D face representations, which can be extracted from any driving video and adapted to the facial geometry of any reference image, disentangling identity from expression. We further improve mouth movements, by utilising audio features as a complementary input. The 3D face representation enables HeadGAN to be further used as an efficient method for compression and reconstruction and a tool for expression and pose editing.},
  archive   = {C_ICCV},
  author    = {Michail Christos Doukas and Stefanos Zafeiriou and Viktoriia Sharmanska},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01413},
  pages     = {14378-14387},
  title     = {HeadGAN: One-shot neural head synthesis and editing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Physics-based differentiable depth sensor simulation.
<em>ICCV</em>, 14367–14377. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Gradient-based algorithms are crucial to modern computer-vision and graphics applications, enabling learning-based optimization and inverse problems. For example, photorealistic differentiable rendering pipelines for color images have been proven highly valuable to applications aiming to map 2D and 3D domains. However, to the best of our knowledge, no effort has been made so far towards extending these gradient-based methods to the generation of depth (2.5D) images, as simulating structured-light depth sensors implies solving complex light transport and stereo-matching problems. In this paper, we introduce a novel end-to-end differentiable simulation pipeline for the generation of realistic 2.5D scans, built on physics-based 3D rendering and custom block-matching algorithms. Each module can be differentiated w.r.t. sensor and scene parameters; e.g., to automatically tune the simulation for new devices over some provided scans or to leverage the pipeline as a 3D-to-2.5D transformer within larger computer-vision applications. Applied to the training of deep-learning methods for various depth-based recognition tasks (classification, pose estimation, semantic segmentation), our simulation greatly improves the performance of the resulting models on real scans, thereby demonstrating the fidelity and value of its synthetic depth data compared to previous static simulations and learning-based domain adaptation schemes.},
  archive   = {C_ICCV},
  author    = {Benjamin Planche and Rajat Vikram Singh},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01412},
  pages     = {14367-14377},
  title     = {Physics-based differentiable depth sensor simulation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Towards vivid and diverse image colorization with
generative color prior. <em>ICCV</em>, 14357–14366. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Colorization has attracted increasing interest in recent years. Classic reference-based methods usually rely on external color images for plausible results. A large image database or online search engine is inevitably required for retrieving such exemplars. Recent deep-learning-based methods could automatically colorize images at a low cost. However, unsatisfactory artifacts and incoherent colors are always accompanied. In this work, we aim at recovering vivid colors by leveraging the rich and diverse color priors encapsulated in a pretrained Generative Adversarial Networks (GAN). Specifically, we first &quot;retrieve&quot; matched features (similar to exemplars) via a GAN encoder and then incorporate these features into the colorization process with feature modulations. Thanks to the powerful generative color prior and delicate designs, our method could produce vivid colors with a single forward pass. Moreover, it is highly convenient to obtain diverse results by modifying GAN latent codes. Our method also inherits the merit of interpretable controls of GANs and could attain controllable and smooth transitions by walking through GAN latent space. Extensive experiments and user studies demonstrate that our method achieves superior performance than previous works.},
  archive   = {C_ICCV},
  author    = {Yanze Wu and Xintao Wang and Yu Li and Honglun Zhang and Xun Zhao and Ying Shan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01411},
  pages     = {14357-14366},
  title     = {Towards vivid and diverse image colorization with generative color prior},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ILVR: Conditioning method for denoising diffusion
probabilistic models. <em>ICCV</em>, 14347–14356. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Denoising diffusion probabilistic models (DDPM) have shown remarkable performance in unconditional image generation. However, due to the stochasticity of the generative process in DDPM, it is challenging to generate images with the desired semantics. In this work, we propose Iterative Latent Variable Refinement (ILVR), a method to guide the generative process in DDPM to generate high-quality images based on a given reference image. Here, the refinement of the generative process in DDPM enables a single DDPM to sample images from various sets directed by the reference image. The proposed ILVR method generates high-quality images while controlling the generation. The controllability of our method allows adaptation of a single DDPM without any additional learning in various image generation tasks, such as generation from various downsampling factors, multi-domain image translation, paint-to-image, and editing with scribbles.},
  archive   = {C_ICCV},
  author    = {Jooyoung Choi and Sungwon Kim and Yonghyun Jeong and Youngjune Gwon and Sungroh Yoon},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01410},
  pages     = {14347-14356},
  title     = {ILVR: Conditioning method for denoising diffusion probabilistic models},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Geometry-free view synthesis: Transformers and no 3D priors.
<em>ICCV</em>, 14336–14346. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Is a geometric model required to synthesize novel views from a single image? Being bound to local convolutions, CNNs need explicit 3D biases to model geometric transformations. In contrast, we demonstrate that a transformer-based model can synthesize entirely novel views without any hand-engineered 3D biases. This is achieved by (i) a global attention mechanism for implicitly learning long-range 3D correspondences between source and target views, and (ii) a probabilistic formulation necessary to capture the ambiguity inherent in predicting novel views from a single image, thereby overcoming the limitations of previous approaches that are restricted to relatively small viewpoint changes. We evaluate various ways to integrate 3D priors into a transformer architecture. However, our experiments show that no such geometric priors are required and that the transformer is capable of implicitly learning 3D relationships between images. Furthermore, this approach outperforms the state of the art in terms of visual quality while covering the full distribution of possible realizations.},
  archive   = {C_ICCV},
  author    = {Robin Rombach and Patrick Esser and Björn Ommer},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01409},
  pages     = {14336-14346},
  title     = {Geometry-free view synthesis: Transformers and no 3D priors},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FastNeRF: High-fidelity neural rendering at 200FPS.
<em>ICCV</em>, 14326–14335. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent work on Neural Radiance Fields (NeRF) showed how neural networks can be used to encode complex 3D environments that can be rendered photorealistically from novel viewpoints. Rendering these images is very computationally demanding and recent improvements are still a long way from enabling interactive rates, even on high-end hardware. Motivated by scenarios on mobile and mixed reality devices, we propose FastNeRF, the first NeRF-based system capable of rendering high fidelity photorealistic images at 200Hz on a high-end consumer GPU. The core of our method is a graphics-inspired factorization that allows for (i) compactly caching a deep radiance map at each position in space, (ii) efficiently querying that map using ray directions to estimate the pixel values in the rendered image. Extensive experiments show that the proposed method is 3000 times faster than the original NeRF algorithm and at least an order of magnitude faster than existing work on accelerating NeRF, while maintaining visual quality and extensibility.},
  archive   = {C_ICCV},
  author    = {Stephan J. Garbin and Marek Kowalski and Matthew Johnson and Jamie Shotton and Julien Valentin},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01408},
  pages     = {14326-14335},
  title     = {FastNeRF: High-fidelity neural rendering at 200FPS},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). KiloNeRF: Speeding up neural radiance fields with thousands
of tiny MLPs. <em>ICCV</em>, 14315–14325. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {NeRF synthesizes novel views of a scene with unprecedented quality by fitting a neural radiance field to RGB images. However, NeRF requires querying a deep Multi-Layer Perceptron (MLP) millions of times, leading to slow rendering times, even on modern GPUs. In this paper, we demonstrate that real-time rendering is possible by utilizing thousands of tiny MLPs instead of one single large MLP. In our setting, each individual MLP only needs to represent parts of the scene, thus smaller and faster-to-evaluate MLPs can be used. By combining this divide-and-conquer strategy with further optimizations, rendering is accelerated by three orders of magnitude compared to the original NeRF model without incurring high storage costs. Further, using teacher-student distillation for training, we show that this speed-up can be achieved without sacrificing visual quality.},
  archive   = {C_ICCV},
  author    = {Christian Reiser and Songyou Peng and Yiyi Liao and Andreas Geiger},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01407},
  pages     = {14315-14325},
  title     = {KiloNeRF: Speeding up neural radiance fields with thousands of tiny MLPs},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural radiance flow for 4D view synthesis and video
processing. <em>ICCV</em>, 14304–14314. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a method, Neural Radiance Flow (NeRFlow), to learn a 4D spatial-temporal representation of a dynamic scene from a set of RGB images. Key to our approach is the use of a neural implicit representation that learns to capture the 3D occupancy, radiance, and dynamics of the scene. By enforcing consistency across different modalities, our representation enables multi-view rendering in diverse dynamic scenes, including water pouring, robotic interaction, and real images, outperforming state-of-the-art methods for spatial-temporal view synthesis. Our approach works even when being provided only a single monocular real video. We further demonstrate that the learned representation can serve as an implicit scene prior, enabling video processing tasks such as image super-resolution and de-noising without any additional supervision.},
  archive   = {C_ICCV},
  author    = {Yilun Du and Yinan Zhang and Hong-Xing Yu and Joshua B. Tenenbaum and Jiajun Wu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01406},
  pages     = {14304-14314},
  title     = {Neural radiance flow for 4D view synthesis and video processing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Animatable neural radiance fields for modeling dynamic human
bodies. <em>ICCV</em>, 14294–14303. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the challenge of reconstructing an animatable human model from a multi-view video. Some recent works have proposed to decompose a non-rigidly deforming scene into a canonical neural radiance field and a set of deformation fields that map observation-space points to the canonical space, thereby enabling them to learn the dynamic scene from images. However, they represent the deformation field as translational vector field or SE(3) field, which makes the optimization highly under-constrained. Moreover, these representations cannot be explicitly controlled by input motions. Instead, we introduce neural blend weight fields to produce the deformation fields. Based on the skeleton-driven deformation, blend weight fields are used with 3D human skeletons to generate observation-to-canonical and canonical-to-observation correspondences. Since 3D human skeletons are more observable, they can regularize the learning of deformation fields. Moreover, the learned blend weight fields can be combined with input skeletal motions to generate new deformation fields to animate the human model. Experiments show that our approach significantly outperforms recent human synthesis methods. The code and supplementary materials are available at https://zju3dv.github.io/animatable_nerf/.},
  archive   = {C_ICCV},
  author    = {Sida Peng and Junting Dong and Qianqian Wang and Shangzhan Zhang and Qing Shuai and Xiaowei Zhou and Hujun Bao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01405},
  pages     = {14294-14303},
  title     = {Animatable neural radiance fields for modeling dynamic human bodies},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unconstrained scene generation with locally conditioned
radiance fields. <em>ICCV</em>, 14284–14293. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We tackle the challenge of learning a distribution over complex, realistic, indoor scenes. In this paper, we introduce Generative Scene Networks (GSN), which learns to decompose scenes into a collection of many local radiance fields that can be rendered from a free moving camera. Our model can be used as a prior to generate new scenes, or to complete a scene given only sparse 2D observations. Recent work has shown that generative models of radiance fields can capture properties such as multi-view consistency and view-dependent lighting. However, these models are specialized for constrained viewing of single objects, such as cars or faces. Due to the size and complexity of realistic indoor environments, existing models lack the representational capacity to adequately capture them. Our decomposition scheme scales to larger and more complex scenes while preserving details and diversity, and the learned prior enables high-quality rendering from viewpoints that are significantly different from observed viewpoints. When compared to existing models, GSN produces quantitatively higher-quality scene renderings across several different scene datasets.},
  archive   = {C_ICCV},
  author    = {Terrance DeVries and Miguel Angel Bautista and Nitish Srivastava and Graham W. Taylor and Joshua M. Susskind},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01404},
  pages     = {14284-14293},
  title     = {Unconstrained scene generation with locally conditioned radiance fields},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reality transform adversarial generators for image splicing
forgery detection and localization. <em>ICCV</em>, 14274–14283. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When many forgery images become more and more realistic with help of image editing tools and convolutional neural networks (CNNs), authenticators need to improve their ability to verify these forgery images. The process of generating and detecting forgery images is the same as the principle of Generative Adversarial Networks (GANs). In this paper, since the retouching progress of forgery images requires to suppress the tampering artifacts and to keep the structural information, we consider this retouching progress as an image style transform, and then propose a fake-to-realistic transform generator G T . For detecting the tampered regions, a localization generator G M is proposed too, which is based on a multi-decoder-single-task strategy. By adversarial training two generators, the proposed α-learnable whitening and coloring transform α-learnable WCT) block in G T automatically suppress the tampering artifacts in the forgery images. Meanwhile, the detection and localization abilities of G M will be improved by learning the forgery images retouched by G T . The experiment results demonstrate that the proposed two generators in GAN can simulate confrontation between the faker and the authenticator well; the localization generator G M outperforms the state-of-the-art methods in splicing forgery detection and localization on four public datasets.},
  archive   = {C_ICCV},
  author    = {Xiuli Bi and Zhipeng Zhang and Bin Xiao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01403},
  pages     = {14274-14283},
  title     = {Reality transform adversarial generators for image splicing forgery detection and localization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised image generation with infinite generative
adversarial networks. <em>ICCV</em>, 14264–14273. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Image generation has been heavily investigated in computer vision, where one core research challenge is to generate images from arbitrarily complex distributions with little supervision. Generative Adversarial Networks (GANs) as an implicit approach have achieved great successes in this direction and therefore been employed widely. However, GANs are known to suffer from issues such as mode collapse, non-structured latent space, being unable to compute likelihoods, etc. In this paper, we propose a new unsupervised non-parametric method named mixture of infinite conditional GANs or MIC-GANs, to tackle several GAN issues together, aiming for image generation with parsimonious prior knowledge. Through comprehensive evaluations across different datasets, we show that MIC-GANs are effective in structuring the latent space and avoiding mode collapse, and outperform state-of-the-art methods. MICGANs are adaptive, versatile, and robust. They offer a promising solution to several well-known GAN issues. Code available:github.com/yinghdb/MICGANs.},
  archive   = {C_ICCV},
  author    = {Hui Ying and He Wang and Tianjia Shao and Yin Yang and Kun Zhou},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01402},
  pages     = {14264-14273},
  title     = {Unsupervised image generation with infinite generative adversarial networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semantically robust unpaired image translation for data with
unmatched semantics statistics. <em>ICCV</em>, 14253–14263. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many applications of unpaired image-to-image translation require the input contents to be preserved semantically during translations. Unaware of the inherently unmatched semantics distributions between source and target domains, existing distribution matching methods (i.e., GAN-based) can give undesired solutions. In particular, although producing visually reasonable outputs, the learned models usually flip the semantics of the inputs. To tackle this without using extra supervisions, we propose to enforce the translated outputs to be semantically invariant w.r.t. small perceptual variations of the inputs, a property we call &quot;semantic robustness&quot;. By optimizing a robustness loss w.r.t. multi-scale feature space perturbations of the inputs, our method effectively reduces semantics flipping and produces translations that outperform existing methods both quantitatively and qualitatively.},
  archive   = {C_ICCV},
  author    = {Zhiwei Jia and Bodi Yuan and Kangkang Wang and Hong Wu and David Clifford and Zhiqiang Yuan and Hao Su},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01401},
  pages     = {14253-14263},
  title     = {Semantically robust unpaired image translation for data with unmatched semantics statistics},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LatentCLR: A contrastive learning approach for unsupervised
discovery of interpretable directions. <em>ICCV</em>, 14243–14252. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent research has shown that it is possible to find interpretable directions in the latent spaces of pre-trained Generative Adversarial Networks (GANs). These directions enable controllable image generation and support a wide range of semantic editing operations, such as zoom or rotation. The discovery of such directions is often done in a supervised or semi-supervised manner and requires manual annotations which limits their use in practice. In comparison, unsupervised discovery allows finding subtle directions that are difficult to detect a priori. In this work, we propose a contrastive learning-based approach to discover semantic directions in the latent space of pre-trained GANs in a self-supervised manner. Our approach finds semantically meaningful dimensions compatible with state-of-the-art methods.},
  archive   = {C_ICCV},
  author    = {Oğuz Kaan Yüksel and Enis Simsar and Ezgi Gülperi Er and Pinar Yanardag},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01400},
  pages     = {14243-14252},
  title     = {LatentCLR: A contrastive learning approach for unsupervised discovery of interpretable directions},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toward spatially unbiased generative models. <em>ICCV</em>,
14233–14242. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent image generation models show remarkable generation performance. However, they mirror strong location preference in datasets, which we call spatial bias. Therefore, generators render poor samples at unseen locations and scales. We argue that the generators rely on their implicit positional encoding to render spatial content. From our observations, the generator’s implicit positional encoding is translation-variant, making the generator spatially biased. To address this issue, we propose injecting explicit positional encoding at each scale of the generator. By learning the spatially unbiased generator, we facilitate the robust use of generators in multiple tasks, such as GAN inversion, multi-scale generation, generation of arbitrary sizes and aspect ratios. Furthermore, we show that our method can also be applied to denoising diffusion probabilistic models. Our code is available at: https://github.com/jychoill8/toward_spatial_unbiased.},
  archive   = {C_ICCV},
  author    = {Jooyoung Choi and Jungbeom Lee and Yonghyun Jeong and Sungroh Yoon},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01399},
  pages     = {14233-14242},
  title     = {Toward spatially unbiased generative models},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cortical surface shape analysis based on alexandrov
polyhedra. <em>ICCV</em>, 14224–14232. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Shape analysis has been playing an important role in early diagnosis and prognosis of neurodegenerative diseases such as Alzheimer’s diseases (AD). However, obtaining effective shape representations remains challenging. This paper proposes to use the Alexandrov polyhedra as surface-based shape signatures for cortical morphometry analysis. Given a closed genus-0 surface, its Alexandrov polyhedron is a convex representation that encodes its intrinsic geometry information. We propose to compute the polyhedra via a novel spherical optimal transport (OT) computation. In our experiments, we observe that the Alexandrov polyhedra of cortical surfaces between pathology-confirmed AD and cognitively unimpaired individuals are significantly different. Moreover, we propose a visualization method by comparing local geometry differences across cortical surfaces. We show that the proposed method is effective in pinpointing regional cortical structural changes impacted by AD.},
  archive   = {C_ICCV},
  author    = {Min Zhang and Yang Guo and Na Lei and Zhou Zhao and Jianfeng Wu and Xiaoyin Xu and Yalin Wang and Xianfeng Gu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01398},
  pages     = {14224-14232},
  title     = {Cortical surface shape analysis based on alexandrov polyhedra},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Searching for controllable image restoration networks.
<em>ICCV</em>, 14214–14223. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel framework for controllable image restoration that can effectively restore multiple types and levels of degradation of a corrupted image. The proposed model, named TASNet, is automatically determined by our neural architecture search algorithm, which optimizes the efficiency-accuracy trade-off of the candidate model architectures. Specifically, we allow TASNet to share the early layers across different restoration tasks and adaptively adjust the remaining layers with respect to each task. The shared task-agnostic layers greatly improve the efficiency while the task-specific layers are optimized for restoration quality, and our search algorithm seeks for the best balance between the two. We also propose a new data sampling strategy to further improve the overall restoration performance. As a result, TASNet achieves significantly faster GPU latency and lower FLOPs compared to the existing state-of-the-art models, while also showing visually more pleasing outputs. The source code and pre-trained models are available at https://github.com/ghimhw/TASNet.},
  archive   = {C_ICCV},
  author    = {Heewon Kim and Sungyong Baik and Myungsub Choi and Janghoon Choi and Kyoung Mu Lee},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01397},
  pages     = {14214-14223},
  title     = {Searching for controllable image restoration networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SIGNET: Efficient neural representation for light fields.
<em>ICCV</em>, 14204–14213. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel neural representation for light field content that enables compact storage and easy local reconstruction with high fidelity. We use a fully-connected neural network to learn the mapping function between each light field pixel’s coordinates and its corresponding color values. Since neural networks that simply take in raw coordinates are unable to accurately learn data containing fine details, we present an input transformation strategy based on the Gegenbauer polynomials, which previously showed theoretical advantages over the Fourier basis. We conduct experiments that show our Gegenbauer-based design combined with sinusoidal activation functions leads to a better light field reconstruction quality than a variety of network designs, including those with Fourier-inspired techniques introduced by prior works. Moreover, our SInusoidal Gegenbauer NETwork, or SIGNET, can represent light field scenes more compactly than the state-of-the-art compression methods while maintaining a comparable reconstruction quality. SIGNET also innately allows random access to encoded light field pixels due to its functional design. We further demonstrate that SIGNET’s super-resolution capability without any additional training.},
  archive   = {C_ICCV},
  author    = {Brandon Yushan Feng and Amitabh Varshney},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01396},
  pages     = {14204-14213},
  title     = {SIGNET: Efficient neural representation for light fields},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modulated periodic activations for generalizable local
functional representations. <em>ICCV</em>, 14194–14203. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-Layer Perceptrons (MLPs) make powerful functional representations for sampling and reconstruction problems involving low-dimensional signals like images, shapes and light fields. Recent works have significantly improved their ability to represent high-frequency content by using periodic activations or positional encodings. This often came at the expense of generalization: modern methods are typically optimized for a single signal. We present a new representation that generalizes to multiple instances and achieves state-of-the-art fidelity. We use a dual-MLP architecture to encode the signals. A synthesis network creates a functional mapping from a low-dimensional input (e.g. pixel-position) to the output domain (e.g. RGB color). A modulation network maps a latent code corresponding to the target signal to parameters that modulate the periodic activations of the synthesis network. We also propose a local-functional representation which enables generalization. The signal’s domain is partitioned into a regular grid, with each tile represented by a latent code. At test time, the signal is encoded with high-fidelity by inferring (or directly optimizing) the latent code-book. Our approach produces generalizable functional representations of images, videos and shapes, and achieves higher reconstruction quality than prior works that are optimized for a single signal.},
  archive   = {C_ICCV},
  author    = {Ishit Mehta and Michaël Gharbi and Connelly Barnes and Eli Shechtman and Ravi Ramamoorthi and Manmohan Chandraker},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01395},
  pages     = {14194-14203},
  title     = {Modulated periodic activations for generalizable local functional representations},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural strokes: Stylized line drawing of 3D shapes.
<em>ICCV</em>, 14184–14193. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a model for producing stylized line drawings from 3D shapes. The model takes a 3D shape and a viewpoint as input, and outputs a drawing with textured strokes, with variations in stroke thickness, deformation, and color learned from an artist’s style. The model is fully differentiable. We train its parameters from a single training drawing of another 3D shape. We show that, in contrast to previous image-based methods, the use of a geometric representation of 3D shape and 2D strokes allows the model to transfer important aspects of shape and texture style while preserving contours. Our method outputs the resulting drawing in a vector representation, enabling richer downstream analysis or editing in interactive applications. Our code and dataset are available at our project page: www.github.com/DifanLiu/NeuralStrokes},
  archive   = {C_ICCV},
  author    = {Difan Liu and Matthew Fisher and Aaron Hertzmann and Evangelos Kalogerakis},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01394},
  pages     = {14184-14193},
  title     = {Neural strokes: Stylized line drawing of 3D shapes},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). From two to one: A new scene text recognizer with visual
language modeling network. <em>ICCV</em>, 14174–14183. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we abandon the dominant complex language model and rethink the linguistic learning process in the scene text recognition. Different from previous methods considering the visual and linguistic information in two separate structures, we propose a Visual Language Modeling Network (VisionLAN), which views the visual and linguistic information as a union by directly enduing the vision model with language capability. Specially, we introduce the text recognition of character-wise occluded feature maps in the training stage. Such operation guides the vision model to use not only the visual texture of characters, but also the linguistic information in visual context for recognition when the visual cues are confused (e.g. occlusion, noise, etc.). As the linguistic information is acquired along with visual features without the need of extra language model, Vision-LAN significantly improves the speed by 39\% and adaptively considers the linguistic information to enhance the visual features for accurate recognition. Furthermore, an Occlusion Scene Text (OST) dataset is proposed to evaluate the performance on the case of missing character-wise visual cues. The state of-the-art results on several benchmarks prove our effectiveness. Code and dataset are available at https://github.com/wangyuxin87/VisionLAN.},
  archive   = {C_ICCV},
  author    = {Yuxin Wang and Hongtao Xie and Shancheng Fang and Jing Wang and Shenggao Zhu and Yongdong Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01393},
  pages     = {14174-14183},
  title     = {From two to one: A new scene text recognizer with visual language modeling network},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image manipulation detection by multi-view multi-scale
supervision. <em>ICCV</em>, 14165–14173. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The key challenge of image manipulation detection is how to learn generalizable features that are sensitive to manipulations in novel data, whilst specific to prevent false alarms on authentic images. Current research emphasizes the sensitivity, with the specificity overlooked. In this paper we address both aspects by multi-view feature learning and multi-scale supervision. By exploiting noise distribution and boundary artifact surrounding tampered regions, the former aims to learn semantic-agnostic and thus more generalizable features. The latter allows us to learn from authentic images which are nontrivial to be taken into account by current semantic segmentation network based methods. Our thoughts are realized by a new network which we term MVSS-Net. Extensive experiments on five benchmark sets justify the viability of MVSS-Net for both pixel-level and image-level manipulation detection.},
  archive   = {C_ICCV},
  author    = {Xinru Chen and Chengbo Dong and Jiaqi Ji and Juan Cao and Xirong Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01392},
  pages     = {14165-14173},
  title     = {Image manipulation detection by multi-view multi-scale supervision},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unaligned image-to-image translation by learning to
reweight. <em>ICCV</em>, 14154–14164. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unsupervised image-to-image translation aims at learning the mapping from the source to target domain without using paired images for training. An essential yet restrictive assumption for unsupervised image translation is that the two domains are aligned, e.g., for the selfie2anime task, the anime (selfie) domain must contain only anime (selfie) face images that can be translated to some images in the other domain. Collecting aligned domains can be laborious and needs lots of attention. In this paper, we consider the task of image translation between two unaligned domains, which may arise for various possible reasons. To solve this problem, we propose to select images based on importance reweighting and develop a method to learn the weights and perform translation simultaneously and automatically. We compare the proposed method with state-of-the-art image translation approaches and present qualitative and quantitative results on different tasks with unaligned domains. Extensive empirical evidence demonstrates the usefulness of the proposed problem formulation and the superiority of our method.},
  archive   = {C_ICCV},
  author    = {Shaoan Xie and Mingming Gong and Yanwu Xu and Kun Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01391},
  pages     = {14154-14164},
  title     = {Unaligned image-to-image translation by learning to reweight},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CR-fill: Generative image inpainting with auxiliary
contextual reconstruction. <em>ICCV</em>, 14144–14153. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent deep generative inpainting methods use attention layers to allow the generator to explicitly borrow feature patches from the known region to complete a missing region. Due to the lack of supervision signals for the correspondence between missing regions and known regions, it may fail to find proper reference features, which often leads to artifacts in the results. Also, it computes pair-wise similarity across the entire feature map during inference bringing a significant computational overhead. To address this issue, we propose to teach such patch-borrowing behavior to an attention-free generator by joint training of an auxiliary contextual reconstruction task, which encourages the generated output to be plausible even when reconstructed by surrounding regions. The auxiliary branch can be seen as a learnable loss function, i.e. named as contextual reconstruction (CR) loss, where query-reference feature similarity and reference-based reconstructor are jointly optimized with the inpainting generator. The auxiliary branch ( i.e. CR loss) is required only during training, and only the inpainting generator is required during the inference. Experimental results demonstrate that the proposed inpainting model compares favourably against the state-of-the-art in terms of quantitative and visual performance. Code is available at https://github.com/zengxianyu/crfill.},
  archive   = {C_ICCV},
  author    = {Yu Zeng and Zhe Lin and Huchuan Lu and Vishal M. Patel},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01390},
  pages     = {14144-14153},
  title     = {CR-fill: Generative image inpainting with auxiliary contextual reconstruction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rethinking the truly unsupervised image-to-image
translation. <em>ICCV</em>, 14134–14143. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Every recent image-to-image translation model inherently requires either image-level (i.e. input-output pairs) or set-level (i.e. domain labels) supervision. However, even set-level supervision can be a severe bottleneck for data collection in practice. In this paper, we tackle image-to-image translation in a fully unsupervised setting, i.e., neither paired images nor domain labels. To this end, we propose a truly unsupervised image-to-image translation model (TUNIT) that simultaneously learns to separate image domains and translates input images into the estimated domains. Experimental results show that our model achieves comparable or even better performance than the set-level supervised model trained with full labels, generalizes well on various datasets, and is robust against the choice of hyperparameters (e.g. the preset number of pseudo domains). Furthermore, TUNIT can be easily extended to semi-supervised learning with a few labeled data.},
  archive   = {C_ICCV},
  author    = {Kyungjune Baek and Yunjey Choi and Youngjung Uh and Jaejun Yoo and Hyunjung Shim},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01389},
  pages     = {14134-14143},
  title     = {Rethinking the truly unsupervised image-to-image translation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Aligning latent and image spaces to connect the
unconnectable. <em>ICCV</em>, 14124–14133. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we develop a method to generate infinite high-resolution images with diverse and complex content. It is based on a perfectly equivariant patch-wise generator with synchronous interpolations in the image and latent spaces. Latent codes, when sampled, are positioned on the coordinate grid, and each pixel is computed from an interpolation of the neighboring codes. We modify the AdaIN mechanism to work in such a setup and train a GAN model to generate images positioned between any two latent vectors. At test time, this allows for generating infinitely large images of diverse scenes that transition naturally from one into another. Apart from that, we introduce LHQ: a new dataset of 90k high-resolution nature landscapes. We test the approach on LHQ, LSUN Tower and LSUN Bridge and outperform the baselines by at least 4 times in terms of quality and diversity of the produced infinite images. The project website is located at https://universome.github.io/alis.},
  archive   = {C_ICCV},
  author    = {Ivan Skorokhodov and Grigorii Sotnikov and Mohamed Elhoseiny},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01388},
  pages     = {14124-14133},
  title     = {Aligning latent and image spaces to connect the unconnectable},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image inpainting via conditional texture and structure dual
generation. <em>ICCV</em>, 14114–14123. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep generative approaches have recently made considerable progress in image inpainting by introducing structure priors. Due to the lack of proper interaction with image texture during structure reconstruction, however, current solutions are incompetent in handling the cases with large corruptions, and they generally suffer from distorted results. In this paper, we propose a novel two-stream network for image inpainting, which models the structure-constrained texture synthesis and texture-guided structure reconstruction in a coupled manner so that they better leverage each other for more plausible generation. Furthermore, to enhance the global consistency, a Bi-directional Gated Feature Fusion (Bi-GFF) module is designed to exchange and combine the structure and texture information and a Contextual Feature Aggregation (CFA) module is developed to refine the generated contents by region affinity learning and multi-scale feature aggregation. Qualitative and quantitative experiments on the CelebA, Paris StreetView and Places2 datasets demonstrate the superiority of the proposed method. Our code is available at https://github.com/Xiefan-Guo/CTSDG.},
  archive   = {C_ICCV},
  author    = {Xiefan Guo and Hongyu Yang and Di Huang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01387},
  pages     = {14114-14123},
  title     = {Image inpainting via conditional texture and structure dual generation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MVSNeRF: Fast generalizable radiance field reconstruction
from multi-view stereo. <em>ICCV</em>, 14104–14113. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present MVSNeRF, a novel neural rendering approach that can efficiently reconstruct neural radiance fields for view synthesis. Unlike prior works on neural radiance fields that consider per-scene optimization on densely captured images, we propose a generic deep neural network that can reconstruct radiance fields from only three nearby input views via fast network inference. Our approach leverages plane-swept cost volumes (widely used in multi-view stereo) for geometry-aware scene reasoning, and combines this with physically based volume rendering for neural radiance field reconstruction. We train our network on real objects in the DTU dataset, and test it on three different datasets to evaluate its effectiveness and generalizability. Our approach can generalize across scenes (even indoor scenes, completely different from our training scenes of objects) and generate realistic view synthesis results using only three input images, significantly outperforming concurrent works on generalizable radiance field reconstruction. Moreover, if dense images are captured, our estimated radiance field representation can be easily fine-tuned; this leads to fast per-scene reconstruction with higher rendering quality and substantially less optimization time than NeRF.},
  archive   = {C_ICCV},
  author    = {Anpei Chen and Zexiang Xu and Fuqiang Zhao and Xiaoshuai Zhang and Fanbo Xiang and Jingyi Yu and Hao Su},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01386},
  pages     = {14104-14113},
  title     = {MVSNeRF: Fast generalizable radiance field reconstruction from multi-view stereo},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). WaveFill: A wavelet-based generation network for image
inpainting. <em>ICCV</em>, 14094–14103. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Image inpainting aims to complete the missing or corrupted regions of images with realistic contents. The prevalent approaches adopt a hybrid objective of reconstruction and perceptual quality by using generative adversarial networks. However, the reconstruction loss and adversarial loss focus on synthesizing contents of different frequencies and simply applying them together often leads to inter-frequency conflicts and compromised inpainting. This paper presents WaveFill, a wavelet-based inpainting network that decomposes images into multiple frequency bands and fills the missing regions in each frequency band separately and explicitly. WaveFill decomposes images by using discrete wavelet transform (DWT) that preserves spatial information naturally. It applies L1 reconstruction loss to the decomposed low-frequency bands and adversarial loss to high-frequency bands, hence effectively mitigate inter-frequency conflicts while completing images in spatial domain. To address the inpainting inconsistency in different frequency bands and fuse features with distinct statistics, we design a novel normalization scheme that aligns and fuses the multi-frequency features effectively. Extensive experiments over multiple datasets show that WaveFill achieves superior image inpainting qualitatively and quantitatively.},
  archive   = {C_ICCV},
  author    = {Yingchen Yu and Fangneng Zhan and Shijian Lu and Jianxiong Pan and Feiying Ma and Xuansong Xie and Chunyan Miao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01385},
  pages     = {14094-14103},
  title     = {WaveFill: A wavelet-based generation network for image inpainting},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PixelSynth: Generating a 3D-consistent experience from a
single image. <em>ICCV</em>, 14084–14093. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advancements in differentiable rendering and 3D reasoning have driven exciting results in novel view synthesis from a single image. Despite realistic results, methods are limited to relatively small view change. In order to synthesize immersive scenes, models must also be able to extrapolate. We present an approach that fuses 3D reasoning with autoregressive modeling to outpaint large view changes in a 3D-consistent manner, enabling scene synthesis. We demonstrate considerable improvement in single-image large-angle view synthesis results compared to a variety of methods and possible variants across simulated and real datasets. In addition, we show increased 3D consistency compared to alternative accumulation methods.},
  archive   = {C_ICCV},
  author    = {Chris Rockwell and David F. Fouhey and Justin Johnson},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01384},
  pages     = {14084-14093},
  title     = {PixelSynth: Generating a 3D-consistent experience from a single image},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards discovery and attribution of open-world GAN
generated images. <em>ICCV</em>, 14074–14083. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the recent progress in Generative Adversarial Networks (GANs), it is imperative for media and visual forensics to develop detectors which can identify and attribute images to the model generating them. Existing works have shown to attribute images to their corresponding GAN sources with high accuracy. However, these works are limited to a closed set scenario, failing to generalize to GANs unseen during train time and are therefore, not scalable with a steady influx of new GANs. We present an iterative algorithm for discovering images generated from previously unseen GANs by exploiting the fact that all GANs leave distinct fingerprints on their generated images. Our algorithm consists of multiple components including network training, out-of-distribution detection, clustering, merge and refine steps. Through extensive experiments, we show that our algorithm discovers unseen GANs with high accuracy and also generalizes to GANs trained on unseen real datasets. We additionally apply our algorithm to attribution and discovery of GANs in an online fashion as well as to the more standard task of real/fake detection. Our experiments demonstrate the effectiveness of our approach to discover new GANs and can be used in an open-world setup.},
  archive   = {C_ICCV},
  author    = {Sharath Girish and Saksham Suri and Saketh Rambhatla and Abhinav Shrivastava},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01383},
  pages     = {14074-14083},
  title     = {Towards discovery and attribution of open-world GAN generated images},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GAN-control: Explicitly controllable GANs. <em>ICCV</em>,
14063–14073. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a framework for training GANs with explicit control over generated facial images. We are able to control the generated image by settings exact attributes such as age, pose, expression, etc. Most approaches for manipulating GAN-generated images achieve partial control by leveraging the latent space disentanglement properties, obtained implicitly after standard GAN training. Such methods are able to change the relative intensity of certain attributes, but not explicitly set their values. Recently proposed methods, designed for explicit control over human faces, harness morphable 3D face models (3DMM) to allow fine-grained control capabilities in GANs. Unlike these methods, our control is not constrained to 3DMM parameters and is extendable beyond the domain of human faces. Using contrastive learning, we obtain GANs with an explicitly disentangled latent space. This disentanglement is utilized to train control-encoders mapping human-interpretable inputs to suitable latent vectors, thus allowing explicit control. In the domain of human faces we demonstrate control over identity, age, pose, expression, hair color and illumination. We also demonstrate control capabilities of our framework in the domains of painted portraits and dog image generation. We demonstrate that our approach achieves state-of-the-art performance both qualitatively and quantitatively.},
  archive   = {C_ICCV},
  author    = {Alon Shoshan and Nadav Bhonker and Igor Kviatkovsky and Gérard Medioni},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01382},
  pages     = {14063-14073},
  title     = {GAN-control: Explicitly controllable GANs},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GANcraft: Unsupervised 3D neural rendering of minecraft
worlds. <em>ICCV</em>, 14052–14062. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present GANcraft, an unsupervised neural rendering framework for generating photorealistic images of large 3D block worlds such as those created in Minecraft. Our method takes a semantic block world as input, where each block is assigned a semantic label such as dirt, grass, or water. We represent the world as a continuous volumetric function and train our model to render view-consistent photorealistic images for a user-controlled camera. In the absence of paired ground truth real images for the block world, we devise a training technique based on pseudo-ground truth and adversarial training. This stands in contrast to prior work on neural rendering for view synthesis, which requires ground truth images to estimate scene geometry and view-dependent appearance. In addition to camera trajectory, GANcraft allows user control over both scene semantics and output style. Experimental results with comparison to strong baselines show the effectiveness of GANcraft on this novel task of photorealistic 3D block world synthesis. The project website is available at https://nvlabs.github.io/GANcraft/.},
  archive   = {C_ICCV},
  author    = {Zekun Hao and Arun Mallya and Serge Belongie and Ming-Yu Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01381},
  pages     = {14052-14062},
  title     = {GANcraft: Unsupervised 3D neural rendering of minecraft worlds},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Omni-GAN: On the secrets of cGANs and beyond. <em>ICCV</em>,
14041–14051. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The conditional generative adversarial network (cGAN) is a powerful tool of generating high-quality images, but existing approaches mostly suffer unsatisfying performance or the risk of mode collapse. This paper presents Omni-GAN, a variant of cGAN that reveals the devil in designing a proper discriminator for training the model. The key is to ensure that the discriminator receives strong supervision to perceive the concepts and moderate regularization to avoid collapse. Omni-GAN is easily implemented and freely integrated with off-the-shelf encoding methods (e.g., implicit neural representation, INR). Experiments validate the superior performance of Omni-GAN and Omni-INR-GAN in a wide range of image generation and restoration tasks. In particular, Omni-INR-GAN sets new records on the ImageNet dataset with impressive Inception scores of 262.85 and 343.22 for the image sizes of 128 and 256, respectively, surpassing the previous records by 100+ points. Moreover, leveraging the generator prior, Omni-INR-GAN can extrapolate low-resolution images to arbitrary resolution, even up to ×60+ higher resolution. Code is available 1 .},
  archive   = {C_ICCV},
  author    = {Peng Zhou and Lingxi Xie and Bingbing Ni and Cong Geng and Qi Tian},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01380},
  pages     = {14041-14051},
  title     = {Omni-GAN: On the secrets of cGANs and beyond},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sketch your own GAN. <em>ICCV</em>, 14030–14040. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Can a user create a deep generative model by sketching a single example? Traditionally, creating a GAN model has required the collection of a large-scale dataset of exemplars and specialized knowledge in deep learning. In contrast, sketching is possibly the most universally accessible way to convey a visual concept. In this work, we present a method, GAN Sketching, for rewriting GANs with one or more sketches, to make GANs training easier for novice users. In particular, we change the weights of an original GAN model according to user sketches. We encourage the model’s output to match the user sketches through a crossdomain adversarial loss. Furthermore, we explore different regularization methods to preserve the original model’s diversity and image quality. Experiments have shown that our method can mold GANs to match shapes and poses specified by sketches while maintaining realism and diversity. Finally, we demonstrate a few applications of the resulting GAN, including latent space interpolation and image editing.},
  archive   = {C_ICCV},
  author    = {Sheng-Yu Wang and David Bau and Jun-Yan Zhu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01379},
  pages     = {14030-14040},
  title     = {Sketch your own GAN},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FuseFormer: Fusing fine-grained information in transformers
for video inpainting. <em>ICCV</em>, 14020–14029. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Transformer, as a strong and flexible architecture for modelling long-range relations, has been widely explored in vision tasks. However, when used in video inpainting that requires fine-grained representation, existed method still suffers from yielding blurry edges in detail due to the hard patch splitting. Here we aim to tackle this problem by proposing FuseFormer, a Transformer model designed for video inpainting via fine-grained feature fusion based on novel Soft Split and Soft Composition operations. The soft split divides feature map into many patches with given overlapping interval. On the contrary, the soft composition operates by stitching different patches into a whole feature map where pixels in overlapping regions are summed up. These two modules are first used in tokenization before Transformer layers and de-tokenization after Transformer layers, for effective mapping between tokens and features. Therefore, sub-patch level information interaction is enabled for more effective feature propagation between neighboring patches, resulting in synthesizing vivid content for hole regions in videos. Moreover, in FuseFormer, we elaborately insert the soft composition and soft split into the feed-forward network, enabling the 1D linear layers to have the capability of modelling 2D structure. And, the sub-patch level feature fusion ability is further enhanced. In both quantitative and qualitative evaluations, our proposed FuseFormer surpasses state-of-the-art methods. We also conduct detailed analysis to examine its superiority. Code and pretrained models are available at https://github.com/ruiliu-ai/FuseFormer.},
  archive   = {C_ICCV},
  author    = {Rui Liu and Hanming Deng and Yangyi Huang and Xiaoyu Shi and Lewei Lu and Wenxiu Sun and Xiaogang Wang and Jifeng Dai and Hongsheng Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01378},
  pages     = {14020-14029},
  title     = {FuseFormer: Fusing fine-grained information in transformers for video inpainting},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-scale separable network for ultra-high-definition
video deblurring. <em>ICCV</em>, 14010–14019. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although recent research has witnessed a significant progress on the video deblurring task, these methods struggle to reconcile inference efficiency and visual quality simultaneously, especially on ultra-high-definition (UHD) videos (e.g., 4K resolution). To address the problem, we propose a novel deep model for fast and accurate UHD Video Deblurring (UHDVD). The proposed UHDVD is achieved by a separable-patch architecture, which collaborates with a multi-scale integration scheme to achieve a large receptive field without adding the number of generic convolutional layers and kernels. Additionally, we design a residual channel-spatial attention (RCSA) module to improve accuracy and reduce the depth of the network appropriately. The proposed UHDVD is the first real-time deblurring model for 4K videos at 35 fps. To train the proposed model, we build a new dataset comprised of 4K blurry videos and corresponding sharp frames using three different smartphones. Comprehensive experimental results show that our network performs favorably against the state-of-the-art methods on both the 4K dataset and public benchmarks in terms of accuracy, speed, and model size.},
  archive   = {C_ICCV},
  author    = {Senyou Deng and Wenqi Ren and Yanyang Yan and Tao Wang and Fenglong Song and Xiaochun Cao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01377},
  pages     = {14010-14019},
  title     = {Multi-scale separable network for ultra-high-definition video deblurring},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Instance-wise hard negative example generation for
contrastive learning in unpaired image-to-image translation.
<em>ICCV</em>, 14000–14009. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Contrastive learning shows great potential in unpaired image-to-image translation, but sometimes the translated results are in poor quality and the contents are not preserved consistently. In this paper, we uncover that the negative examples play a critical role in the performance of contrastive learning for image translation. The negative examples in previous methods are randomly sampled from the patches of different positions in the source image, which are not effective to push the positive examples close to the query examples. To address this issue, we present instance-wise hard Negative Example Generation for Contrastive learning in Unpaired image-to-image Translation (NEGCUT). Specifically, we train a generator to produce negative examples online. The generator is novel from two perspectives: 1) it is instance-wise which means that the generated examples are based on the input image, and 2) it can generate hard negative examples since it is trained with an adversarial loss. With the generator, the performance of unpaired image-to-image translation is significantly improved. Experiments on three benchmark datasets demonstrate that the proposed NEGCUT framework achieves state-of-the-art performance compared to previous methods.},
  archive   = {C_ICCV},
  author    = {Weilun Wang and Wengang Zhou and Jianmin Bao and Dong Chen and Houqiang Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01376},
  pages     = {14000-14009},
  title     = {Instance-wise hard negative example generation for contrastive learning in unpaired image-to-image translation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TransferI2I: Transfer learning for image-to-image
translation from small datasets. <em>ICCV</em>, 13990–13999. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Image-to-image (I2I) translation has matured in recent years and is able to generate high-quality realistic images. However, despite current success, it still faces important challenges when applied to small domains. Existing methods use transfer learning for I2I translation, but they still require the learning of millions of parameters from scratch. This drawback severely limits its application on small domains. In this paper, we propose a new transfer learning for I2I translation (TransferI2I). We decouple our learning process into the image generation step and the I2I translation step. In the first step we propose two novel techniques: source-target initialization and self-initialization of the adaptor layer. The former finetunes the pretrained generative model (e.g., StyleGAN) on source and target data. The latter allows to initialize all non-pretrained network parameters without the need of any data. These techniques provide a better initialization for the I2I translation step. In addition, we introduce an auxiliary GAN that further facilitates the training of deep I2I systems even from small datasets. In extensive experiments on three datasets, (Animal faces, Birds, and Foods), we show that we outperform existing methods and that mFID improves on several datasets with over 25 points. Our code is available at: https://github.com/yaxingwang/TransferI2I.},
  archive   = {C_ICCV},
  author    = {Yaxing Wang and Héctor Laria and Joost van de Weijer and Laura Lopez-Fuentes and Bogdan Raducanu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01375},
  pages     = {13990-13999},
  title     = {TransferI2I: Transfer learning for image-to-image translation from small datasets},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep halftoning with reversible binary pattern.
<em>ICCV</em>, 13980–13989. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing halftoning algorithms usually drop colors and fine details when dithering color images with binary dot patterns, which makes it extremely difficult to recover the original information. To dispense the recovery trouble in future, we propose a novel halftoning technique that converts a color image into binary halftone with full restorability to the original version. The key idea is to implicitly embed those previously dropped information into the halftone patterns. So, the halftone pattern not only serves to reproduce the image tone, maintain the blue-noise randomness, but also represents the color information and fine details. To this end, we exploit two collaborative convolutional neural networks (CNNs) to learn the dithering scheme, under a nontrivial self-supervision formulation. To tackle the flatness degradation issue of CNNs, we propose a novel noise incentive block (NIB) that can serve as a generic CNN plug-in for performance promotion. At last, we tailor a guiding-aware training scheme that secures the convergence direction as regulated. We evaluate the invertible halftones in multiple aspects, which evidences the effectiveness of our method.},
  archive   = {C_ICCV},
  author    = {Menghan Xia and Wenbo Hu and Xueting Liu and Tien-Tsin Wong},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01374},
  pages     = {13980-13989},
  title     = {Deep halftoning with reversible binary pattern},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning high-fidelity face texture completion without
complete face texture. <em>ICCV</em>, 13970–13979. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For face texture completion, previous methods typically use some complete textures captured by multiview imaging systems or 3D scanners for supervised learning. This paper deals with a new challenging problem - learning to complete invisible texture in a single face image without using any complete texture. We simply leverage a large corpus of face images of different subjects (e. g., FFHQ) to train a texture completion model in an unsupervised manner. To achieve this, we propose DSD-GAN, a novel deep neural network based method that applies two discriminators in UV map space and image space. These two discriminators work in a complementary manner to learn both facial structures and texture details. We show that their combination is essential to obtain high-fidelity results. Despite the network never sees any complete facial appearance, it is able to generate compelling full textures from single images.},
  archive   = {C_ICCV},
  author    = {Jongyoo Kim and Jiaolong Yang and Xin Tong},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01373},
  pages     = {13970-13979},
  title     = {Learning high-fidelity face texture completion without complete face texture},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Diagonal attention and style-based GAN for content-style
disentanglement in image generation and translation. <em>ICCV</em>,
13960–13969. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One of the important research topics in image generative models is to disentangle the spatial contents and styles for their separate control. Although StyleGAN can generate content feature vectors from random noises, the resulting spatial content control is primarily intended for minor spatial variations, and the disentanglement of global content and styles is by no means complete. Inspired by a mathematical understanding of normalization and attention, here we present a novel hierarchical adaptive Diagonal spatial ATtention (DAT) layers to separately manipulate the spatial contents from styles in a hierarchical manner. Using DAT and AdaIN, our method enables coarse-to-fine level disentanglement of spatial contents and styles. In addition, our generator can be easily integrated into the GAN inversion framework so that the content and style of translated images from multi-domain image translation tasks can be flexibly controlled. By using various datasets, we confirm that the proposed method not only outperforms the existing models in disentanglement scores, but also provides more flexible control over spatial features in the generated images.},
  archive   = {C_ICCV},
  author    = {Gihyun Kwon and Jong Chul Ye},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01372},
  pages     = {13960-13969},
  title     = {Diagonal attention and style-based GAN for content-style disentanglement in image generation and translation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Labels4Free: Unsupervised segmentation using StyleGAN.
<em>ICCV</em>, 13950–13959. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose an unsupervised segmentation framework for StyleGAN generated objects. We build on two main observations. First, the features generated by StyleGAN hold valuable information that can be utilized towards training segmentation networks. Second, the foreground and background can often be treated to be largely independent and be swapped across images to produce plausible composited images. For our solution, we propose to augment the StyleGAN2 generator architecture with a segmentation branch and to split the generator into a foreground and background network. This enables us to generate soft segmentation masks for the foreground object in an unsupervised fashion. On multiple object classes, we report comparable results against state-of-the-art supervised segmentation networks, while against the best unsupervised segmentation approach we demonstrate a clear improvement, both in qualitative and quantitative metricsProject Page : https:/rameenabdal.github.io/Labels4Free},
  archive   = {C_ICCV},
  author    = {Rameen Abdal and Peihao Zhu and Niloy J. Mitra and Peter Wonka},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01371},
  pages     = {13950-13959},
  title     = {Labels4Free: Unsupervised segmentation using StyleGAN},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DAE-GAN: Dynamic aspect-aware GAN for text-to-image
synthesis. <em>ICCV</em>, 13940–13949. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Text-to-image synthesis refers to generating an image from a given text description, the key goal of which lies in photo realism and semantic consistency. Previous methods usually generate an initial image with sentence embedding and then refine it with fine-grained word embedding. Despite the significant progress, the ‘aspect’ information (e.g., red eyes) contained in the text, referring to several words rather than a word that depicts ‘a particular part or feature of something’, is often ignored, which is highly helpful for synthesizing image details. How to make better utilization of aspect information in text-to-image synthesis still remains an unresolved challenge. To address this problem, in this paper, we propose a Dynamic Aspect-awarE GAN (DAE-GAN) that represents text information comprehensively from multiple granularities, including sentence-level, word-level, and aspect-level. Moreover, inspired by human learning behaviors, we develop a novel Aspect-aware Dynamic Re-drawer (ADR) for image refinement, in which an Attended Global Refinement (AGR) module and an Aspect-aware Local Refinement (ALR) module are alternately employed. AGR utilizes word-level embedding to globally enhance the previously generated image, while ALR dynamically employs aspect-level embedding to refine image details from a local perspective. Finally, a corresponding matching loss function is designed to ensure the text-image semantic consistency at different levels. Extensive experiments on two well-studied and publicly available datasets (i.e., CUB-200 and COCO) demonstrate the superiority and rationality of our method.},
  archive   = {C_ICCV},
  author    = {Shulan Ruan and Yong Zhang and Kun Zhang and Yanbo Fan and Fan Tang and Qi Liu and Enhong Chen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01370},
  pages     = {13940-13949},
  title     = {DAE-GAN: Dynamic aspect-aware GAN for text-to-image synthesis},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detail me more: Improving GAN’s photo-realism of complex
scenes. <em>ICCV</em>, 13930–13939. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generative models can synthesize photo-realistic images of a single object. For example, for human faces, algorithms learn to model the local shape and shading of the face components, i.e., changes in the brows, eyes, nose, mouth, jaw line, etc. This is possible because all faces have two brows, two eyes, a nose and a mouth, approximately in the same location. The modeling of complex scenes is however much more challenging because the scene components and their location vary from image to image. For example, living rooms contain a varying number of products belonging to many possible categories and locations, e.g., a lamp may or may not be present in an endless number of possible locations. In the present work, we propose to add a &quot;broker&quot; module in Generative Adversarial Networks (GAN) to solve this problem. The broker is tasked to mediate the use of multiple discriminators in the appropriate image locales. For example, if a lamp is detected or wanted in a specific area of the scene, the broker assigns a fine-grained lamp discriminator to that image patch. This allows the generator to learn the shape and shading models of the lamp. The resulting multi-fine-grained optimization problem is able to synthesize complex scenes with almost the same level of photo-realism as single object images. We demonstrate the generability of the proposed approach on several GAN algorithms (BigGAN, ProGAN, StyleGAN, StyleGAN2), image resolutions (256 2 to 1024 2 ), and datasets. Our approach yields significant improvements over state-of-the-art GAN algorithms.},
  archive   = {C_ICCV},
  author    = {Raghudeep Gadde and Qianli Feng and Aleix M Martinez},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01369},
  pages     = {13930-13939},
  title     = {Detail me more: Improving GAN’s photo-realism of complex scenes},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GAN inversion for out-of-range images with geometric
transformations. <em>ICCV</em>, 13921–13929. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For successful semantic editing of real images, it is critical for a GAN inversion method to find an in-domain latent code that aligns with the domain of a pre-trained GAN model. Unfortunately, such in-domain latent codes can be found only for in-range images that align with the training images of a GAN model. In this paper, we propose BDInvert, a novel GAN inversion approach to semantic editing of out- of-range images that are geometrically unaligned with the training images of a GAN model. To find a latent code that is semantically editable, BDInvert inverts an input out-of-range image into an alternative latent space than the original latent space. We also propose a regularized inversion method to find a solution that supports semantic editing in the alternative space. Our experiments show that BDInvert effectively supports semantic editing of out-of-range images with geometric transformations.},
  archive   = {C_ICCV},
  author    = {Kyoungkook Kang and Seongtae Kim and Sunghyun Cho},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01368},
  pages     = {13921-13929},
  title     = {GAN inversion for out-of-range images with geometric transformations},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Frequency domain image translation: More photo-realistic,
better identity-preserving. <em>ICCV</em>, 13910–13920. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Image-to-image translation has been revolutionized with GAN-based methods. However, existing methods lack the ability to preserve the identity of the source domain. As a result, synthesized images can often over-adapt to the reference domain, losing important structural characteristics and suffering from suboptimal visual quality. To solve these challenges, we propose a novel frequency domain image translation (FDIT) framework, exploiting frequency information for enhancing the image generation process. Our key idea is to decompose the image into low-frequency and high-frequency components, where the high-frequency feature captures object structure akin to the identity. Our training objective facilitates the preservation of frequency information in both pixel space and Fourier spectral space. We broadly evaluate FDIT across five large-scale datasets and multiple tasks including image translation and GAN inversion. Extensive experiments and ablations show that FDIT effectively preserves the identity of the source image, and produces photo-realistic images. FDIT establishes state-of-the-art performance, reducing the average FID score by 5.6\% compared to the previous best method.},
  archive   = {C_ICCV},
  author    = {Mu Cai and Hong Zhang and Huijuan Huang and Qichuan Geng and Yixuan Li and Gao Huang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01367},
  pages     = {13910-13920},
  title     = {Frequency domain image translation: More photo-realistic, better identity-preserving},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Focal frequency loss for image reconstruction and synthesis.
<em>ICCV</em>, 13899–13909. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Image reconstruction and synthesis have witnessed remarkable progress thanks to the development of generative models. Nonetheless, gaps could still exist between the real and generated images, especially in the frequency domain. In this study, we show that narrowing gaps in the frequency domain can ameliorate image reconstruction and synthesis quality further. We propose a novel focal frequency loss, which allows a model to adaptively focus on frequency components that are hard to synthesize by down-weighting the easy ones. This objective function is complementary to existing spatial losses, offering great impedance against the loss of important frequency information due to the inherent bias of neural networks. We demonstrate the versatility and effectiveness of focal frequency loss to improve popular models, such as VAE, pix2pix, and SPADE, in both perceptual quality and quantitative performance. We further show its potential on StyleGAN2. 1 , 2},
  archive   = {C_ICCV},
  author    = {Liming Jiang and Bo Dai and Wayne Wu and Chen Change Loy},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01366},
  pages     = {13899-13909},
  title     = {Focal frequency loss for image reconstruction and synthesis},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). From continuity to editability: Inverting GANs with
consecutive images. <em>ICCV</em>, 13890–13898. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing GAN inversion methods are stuck in a paradox that the inverted codes can either achieve high-fidelity reconstruction, or retain the editing capability. Having only one of them clearly cannot realize real image editing. In this paper, we resolve this paradox by introducing consecutive images (e.g., video frames or the same person with different poses) into the inversion process. The rationale behind our solution is that the continuity of consecutive images leads to inherent editable directions. This inborn property is used for two unique purposes: 1) regularizing the joint inversion process, such that each of the inverted codes is semantically accessible from one of the other and fastened in an editable domain; 2) enforcing inter-image coherence, such that the fidelity of each inverted code can be maximized with the complement of other images. Extensive experiments demonstrate that our alternative significantly outperforms state-of-the-art methods in terms of reconstruction fidelity and editability on both the real image dataset and synthesis dataset. Furthermore, our method provides the first support of video-based GAN inversion and an interesting application of unsupervised semantic transfer from consecutive images. Source code can be found at: https://github.com/cnnlstm/InvertingGANs_with_ConsecutiveImgs.},
  archive   = {C_ICCV},
  author    = {Yangyang Xu and Yong Du and Wenpeng Xiao and Xuemiao Xu and Shengfeng He},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01365},
  pages     = {13890-13898},
  title     = {From continuity to editability: Inverting GANs with consecutive images},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiple heads are better than one: Few-shot font generation
with multiple localized experts. <em>ICCV</em>, 13880–13889. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A few-shot font generation (FFG) method has to satisfy two objectives: the generated images should preserve the underlying global structure of the target character and present the diverse local reference style. Existing FFG methods aim to disentangle content and style either by extracting a universal representation style or extracting multiple component-wise style representations. However, previous methods either fail to capture diverse local styles or cannot be generalized to a character with unseen components, e.g., unseen language systems. To mitigate the issues, we propose a novel FFG method, named Multiple Localized Experts Few-shot Font Generation Network (MX-Font). MX-Font extracts multiple style features not explicitly conditioned on component labels, but automatically by multiple experts to represent different local concepts, e.g., left-side sub-glyph. Owing to the multiple experts, MX-Font can capture diverse local concepts and show the generalizability to unseen languages. During training, we utilize component labels as weak supervision to guide each expert to be specialized for different local concepts. We formulate the component assign problem to each expert as the graph matching problem, and solve it by the Hungarian algorithm. We also employ the independence loss and the content-style adversarial loss to impose the content-style disentanglement. In our experiments, MX-Font outperforms previous state-of-the-art FFG methods in the Chinese generation and cross-lingual, e.g., Chinese to Korean, generation. Source code is available at https://github.com/clovaai/mxfont.},
  archive   = {C_ICCV},
  author    = {Song Park and Sanghyuk Chun and Junbum Cha and Bado Lee and Hyunjung Shim},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01364},
  pages     = {13880-13889},
  title     = {Multiple heads are better than one: Few-shot font generation with multiple localized experts},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VariTex: Variational neural face textures. <em>ICCV</em>,
13870–13879. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep generative models can synthesize photorealistic images of human faces with novel identities. However, a key challenge to the wide applicability of such techniques is to provide independent control over semantically meaningful parameters: appearance, head pose, face shape, and facial expressions. In this paper, we propose VariTex - to the best of our knowledge the first method that learns a variational latent feature space of neural face textures, which allows sampling of novel identities. We combine this generative model with a parametric face model and gain explicit control over head pose and facial expressions. To generate complete images of human heads, we propose an additive decoder that adds plausible details such as hair. A novel training scheme enforces a pose-independent latent space and in consequence, allows learning a one-to-many mapping between latent codes and pose-conditioned exterior regions. The resulting method can generate geometrically consistent images of novel identities under fine-grained control over head pose, face shape, and facial expressions. This facilitates a broad range of downstream tasks, like sampling novel identities, changing the head pose, expression transfer, and more.},
  archive   = {C_ICCV},
  author    = {Marcel C. Bühler and Abhimitra Meka and Gengyan Li and Thabo Beeler and Otmar Hilliges},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01363},
  pages     = {13870-13879},
  title     = {VariTex: Variational neural face textures},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning generative models of textured 3D meshes from
real-world images. <em>ICCV</em>, 13859–13869. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in differentiable rendering have sparked an interest in learning generative models of textured 3D meshes from image collections. These models natively disentangle pose and appearance, enable downstream applications in computer graphics, and improve the ability of generative models to understand the concept of image formation. Although there has been prior work on learning such models from collections of 2D images, these approaches require a delicate pose estimation step that exploits annotated keypoints, thereby restricting their applicability to a few specific datasets. In this work, we propose a GAN framework for generating textured triangle meshes without relying on such annotations. We show that the performance of our approach is on par with prior work that relies on ground-truth keypoints, and more importantly, we demonstrate the generality of our method by setting new baselines on a larger set of categories from ImageNet–for which keypoints are not available–without any class-specific hyperparameter tuning. We release our code at https://github.com/dariopavllo/textured-3d-gan},
  archive   = {C_ICCV},
  author    = {Dario Pavllo and Jonas Kohler and Thomas Hofmann and Aurelien Lucchi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01362},
  pages     = {13859-13869},
  title     = {Learning generative models of textured 3D meshes from real-world images},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to stylize novel views. <em>ICCV</em>, 13849–13858.
(<a href="https://doi.org/10.1109/ICCV48922.2021.01361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We tackle a 3D scene stylization problem — generating stylized images of a scene from arbitrary novel views given a set of images of the same scene and a reference image of the desired style as inputs. Direct solution of combining novel view synthesis and stylization approaches lead to results that are blurry or not consistent across different views. We propose a point cloud-based method for consistent 3D scene stylization. First, we construct the point cloud by back-projecting the image features to the 3D space. Second, we develop point cloud aggregation modules to gather the style information of the 3D scene, and then modulate the features in the point cloud with a linear transformation matrix. Finally, we project the transformed features to 2D space to obtain the novel views. Experimental results on two diverse datasets of real-world scenes validate that our method generates consistent stylized novel view synthesis results against other alternative approaches.},
  archive   = {C_ICCV},
  author    = {Hsin-Ping Huang and Hung-Yu Tseng and Saurabh Saini and Maneesh Singh and Ming-Hsuan Yang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01361},
  pages     = {13849-13858},
  title     = {Learning to stylize novel views},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Structure-transformed texture-enhanced network for person
image synthesis. <em>ICCV</em>, 13839–13848. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pose-guided virtual try-on task aims to modify the fashion item based on pose transfer task. These two tasks that belong to person image synthesis have strong correlations and similarities. However, existing methods treat them as two individual tasks and do not explore correlations between them. Moreover, these two tasks are challenging due to large misalignment and occlusions, thus most of these methods are prone to generate unclear human body structure and blurry fine-grained textures. In this paper, we devise a structure-transformed texture-enhanced network to generate high-quality person images and construct the relationships between two tasks. It consists of two modules: structure-transformed renderer and texture-enhanced stylizer. The structure-transformed renderer is introduced to transform the source person structure to the target one, while the texture-enhanced stylizer is served to enhance detailed textures and controllably inject the fashion style founded on the structural transformation. With the two modules, our model can generate photorealistic person images in diverse poses and even with various fashion styles. Extensive experiments demonstrate that our approach achieves state-of-the-art results on two tasks.},
  archive   = {C_ICCV},
  author    = {Munan Xu and Yuanqi Chen and Shan Liu and Thomas H. Li and Ge Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01360},
  pages     = {13839-13848},
  title     = {Structure-transformed texture-enhanced network for person image synthesis},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3D human texture estimation from a single image with
transformers. <em>ICCV</em>, 13829–13838. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a Transformer-based framework for 3D human texture estimation from a single image. The proposed Transformer is able to effectively exploit the global information of the input image, overcoming the limitations of existing methods that are solely based on convolutional neural networks. In addition, we also propose a mask-fusion strategy to combine the advantages of the RGB-based and texture-flow-based models. We further introduce a part-style loss to help reconstruct high-fidelity colors without introducing unpleasant artifacts. Extensive experiments demonstrate the effectiveness of the proposed method against state-of-the-art 3D human texture estimation approaches both quantitatively and qualitatively. The project page is at https://www.mmlab-ntu.com/project/texformer.},
  archive   = {C_ICCV},
  author    = {Xiangyu Xu and Chen Change},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01359},
  pages     = {13829-13838},
  title     = {3D human texture estimation from a single image with transformers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Motion-aware dynamic architecture for efficient frame
interpolation. <em>ICCV</em>, 13819–13828. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Video frame interpolation aims to synthesize accurate intermediate frames given a low-frame-rate video. While the quality of the generated frames is increasingly getting better, state-of-the-art models have become more and more computationally expensive. However, local regions with small or no motion can be easily interpolated with simple models and do not require such heavy compute, whereas some regions may not be correct even after inference through a large model. Thus, we propose an effective framework that assigns varying amounts of computation for different regions. Our dynamic architecture first calculates the approximate motion magnitude to use as a proxy for the difficulty levels for each region, and decides the depth of the model and the scale of the input. Experimental results show that static regions pass through a smaller number of layers, while the regions with larger motion are downscaled for better motion reasoning. In doing so, we demonstrate that the proposed framework can significantly reduce the computation cost (FLOPs) while maintaining the performance, often up to 50\% when interpolating a 2K resolution video.},
  archive   = {C_ICCV},
  author    = {Myungsub Choi and Suyoung Lee and Heewon Kim and Kyoung Mu Lee},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01358},
  pages     = {13819-13828},
  title     = {Motion-aware dynamic architecture for efficient frame interpolation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learned spatial representations for few-shot talking-head
synthesis. <em>ICCV</em>, 13809–13818. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel approach for few-shot talking-head synthesis. While recent works in neural talking heads have produced promising results, they can still produce images that do not preserve the identity of the subject in source images. We posit this is a result of the entangled representation of each subject in a single latent code that models 3D shape information, identity cues, colors, lighting and even background details. In contrast, we propose to factorize the representation of a subject into its spatial and style components. Our method generates a target frame in two steps. First, it predicts a discrete and dense spatial layout for the target image. Second, an image generator utilizes the predicted layout for spatial denormalization and synthesizes the target frame. We experimentally show that this disentangled representation leads to a significant improvement over previous methods, both quantitatively and qualitatively.},
  archive   = {C_ICCV},
  author    = {Moustafa Meshry and Saksham Suri and Larry S. Davis and Abhinav Shrivastava},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01357},
  pages     = {13809-13818},
  title     = {Learned spatial representations for few-shot talking-head synthesis},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image synthesis from layout with locality-aware mask
adaption. <em>ICCV</em>, 13799–13808. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper is concerned with synthesizing images conditioned on a layout (a set of bounding boxes with object categories). Existing works construct a layout-mask-image pipeline. Object masks are generated separately and mapped to bounding boxes to form a whole semantic segmentation mask (layout-to-mask), with which a new image is generated (mask-to-image). However, overlapped boxes in layouts result in overlapped object masks, which reduces the mask clarity and causes confusion in image generation. We hypothesize the importance of generating clean and semantically clear semantic masks. The hypothesis is supported by the finding that the performance of state-of-the-art LostGAN decreases when input masks are tainted. Motivated by this hypothesis, we propose Locality-Aware Mask Adaption (LAMA) module to adapt overlapped or nearby object masks in the generation. Experimental results show our proposed model with LAMA outperforms existing approaches regarding visual fidelity and alignment with input layouts. On COCO-stuff in 256×256, our method improves the state-of-the-art FID score from 41.65 to 31.12 and the SceneFID from 22.00 to 18.64.},
  archive   = {C_ICCV},
  author    = {Zejian Li and Jingyu Wu and Immanuel Koh and Yongchuan Tang and Lingyun Sun},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01356},
  pages     = {13799-13808},
  title     = {Image synthesis from layout with locality-aware mask adaption},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FashionMirror: Co-attention feature-remapping virtual try-on
with sequential template poses. <em>ICCV</em>, 13789–13798. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Virtual try-on tasks have drawn increased attention. Prior arts focus on tackling this task via warping clothes and fusing the information at the pixel level with the help of semantic segmentation. However, conducting semantic segmentation is time-consuming and easily causes error accumulation over time. Besides, warping the information at the pixel level instead of the feature level limits the performance (e.g., unable to generate different views) and is unstable since it directly demonstrates the results even with a misalignment. In contrast, fusing information at the feature level can be further refined by the convolution to obtain the final results. Based on these assumptions, we propose a co-attention feature-remapping framework, namely FashionMirror, that generates the try-on results according to the driven-pose sequence in two stages. In the first stage, we consider the source human image and the target try-on clothes to predict the removed mask and the try-on clothing mask, which replaces the pre-processed semantic segmentation and reduces the inference time. In the second stage, we first remove the clothes on the source human via the removed mask and warp the clothing features conditioning on the try-on clothing mask to fit the next frame human. Meanwhile, we predict the optical flows from the consecutive 2D poses and warp the source human to the next frame at the feature level. Then, we enhance the clothing features and source human features in every frame to generate realistic try-on results with spatiotemporal smoothness. Both qualitative and quantitative results show that FashionMirror outperforms the state-of-the-art virtual try-on approaches.},
  archive   = {C_ICCV},
  author    = {Chieh-Yun Chen and Ling Lo and Pin-Jui Huang and Hong-Han Shuai and Wen-Huang Cheng},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01355},
  pages     = {13789-13798},
  title     = {FashionMirror: Co-attention feature-remapping virtual try-on with sequential template poses},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Talk-to-edit: Fine-grained facial editing via dialog.
<em>ICCV</em>, 13779–13788. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Facial editing is an important task in vision and graphics with numerous applications. However, existing works are incapable to deliver a continuous and fine-grained editing mode (e.g., editing a slightly smiling face to a big laughing one) with natural interactions with users. In this work, we propose Talk-to-Edit, an interactive facial editing framework that performs fine-grained attribute manipulation through dialog between the user and the system. Our key insight is to model a continual &quot;semantic field&quot; in the GAN latent space. 1) Unlike previous works that regard the editing as traversing straight lines in the latent space, here the fine-grained editing is formulated as finding a curving trajectory that respects fine-grained attribute landscape on the semantic field. 2) The curvature at each step is location-specific and determined by the input image as well as the users’ language requests. 3) To engage the users in a meaningful dialog, our system generates language feedback by considering both the user request and the current state of the semantic field.We also contribute CelebA-Dialog, a visual-language facial editing dataset to facilitate large-scale study. Specifically, each image has manually annotated fine-grained attribute annotations as well as template-based textual descriptions in natural language. Extensive quantitative and qualitative experiments demonstrate the superiority of our framework in terms of 1) the smoothness of fine-grained editing, 2) the identity/attribute preservation, and 3) the visual photorealism and dialog fluency. Notably, user study validates that our overall system is consistently favored by around 80\% of the participants. Our project page is https://www.mmlab-ntu.com/project/talkedit/.},
  archive   = {C_ICCV},
  author    = {Yuming Jiang and Ziqi Huang and Xingang Pan and Chen Change Loy and Ziwei Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01354},
  pages     = {13779-13788},
  title     = {Talk-to-edit: Fine-grained facial editing via dialog},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A latent transformer for disentangled face editing in images
and videos. <em>ICCV</em>, 13769–13778. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {High quality facial image editing is a challenging problem in the movie post-production industry, requiring a high degree of control and identity preservation. Previous works that attempt to tackle this problem may suffer from the entanglement of facial attributes and the loss of the person’s identity. Furthermore, many algorithms are limited to a certain task. To tackle these limitations, we propose to edit facial attributes via the latent space of a StyleGAN generator, by training a dedicated latent transformation network and incorporating explicit disentanglement and identity preservation terms in the loss function. We further introduce a pipeline to generalize our face editing to videos. Our model achieves a disentangled, controllable, and identity-preserving facial attribute editing, even in the challenging case of real (i.e., non-synthetic) images and videos. We conduct extensive experiments on image and video datasets and show that our model outperforms other state-of-the-art methods in visual quality and quantitative evaluation. Source codes are available at https://github.com/InterDigitalInc/latent-transformer.},
  archive   = {C_ICCV},
  author    = {Xu Yao and Alasdair Newson and Yann Gousseau and Pierre Hellier},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01353},
  pages     = {13769-13778},
  title     = {A latent transformer for disentangled face editing in images and videos},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning object-compositional neural radiance field for
editable scene rendering. <em>ICCV</em>, 13759–13768. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Implicit neural rendering techniques have shown promising results for novel view synthesis. However, existing methods usually encode the entire scene as a whole, which is generally not aware of the object identity and limits the ability to the high-level editing tasks such as moving or adding furniture. In this paper, we present a novel neural scene rendering system, which learns an object-compositional neural radiance field and produces realistic rendering with editing capability for a clustered and real-world scene. Specifically, we design a novel two-pathway architecture, in which the scene branch encodes the scene geometry and appearance, and the object branch encodes each standalone object conditioned on learnable object activation codes. To survive the training in heavily cluttered scenes, we propose a scene-guided training strategy to solve the 3D space ambiguity in the occluded regions and learn sharp boundaries for each object. Extensive experiments demonstrate that our system not only achieves competitive performance for static scene novel-view synthesis, but also produces realistic rendering for object-level editing.},
  archive   = {C_ICCV},
  author    = {Bangbang Yang and Yinda Zhang and Yinghao Xu and Yijin Li and Han Zhou and Hujun Bao and Guofeng Zhang and Zhaopeng Cui},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01352},
  pages     = {13759-13768},
  title     = {Learning object-compositional neural radiance field for editable scene rendering},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image shape manipulation from a single augmented training
sample. <em>ICCV</em>, 13749–13758. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present DeepSIM, a generative model for conditional image manipulation based on a single image. We find that extensive augmentation is key for enabling single image training, and incorporate the use of thin-plate-spline (TPS) as an effective augmentation. Our network learns to map between a primitive representation of the image to the image itself. The choice of a primitive representation has an impact on the ease and expressiveness of the manipulations and can be automatic (e.g. edges), manual (e.g. segmentation) or hybrid such as edges on top of segmentations. At manipulation time, our generator allows for making complex image changes by modifying the primitive input representation and mapping it through the network. Our method is shown to achieve remarkable performance on image manipulation tasks.},
  archive   = {C_ICCV},
  author    = {Yael Vinker and Eliahu Horwitz and Nir Zabari and Yedid Hoshen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01351},
  pages     = {13749-13758},
  title     = {Image shape manipulation from a single augmented training sample},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PIRenderer: Controllable portrait image generation via
semantic neural rendering. <em>ICCV</em>, 13739–13748. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generating portrait images by controlling the motions of existing faces is an important task of great consequence to social media industries. For easy use and intuitive control, semantically meaningful and fully disentangled parameters should be used as modifications. However, many existing techniques do not provide such fine-grained controls or use indirect editing methods i.e. mimic motions of other individuals. In this paper, a Portrait Image Neural Renderer (PIRenderer) is proposed to control the face motions with the parameters of three-dimensional morphable face models (3DMMs). The proposed model can generate photo-realistic portrait images with accurate movements according to intuitive modifications. Experiments on both direct and indirect editing tasks demonstrate the superiority of this model. Meanwhile, we further extend this model to tackle the audio-driven facial reenactment task by extracting sequential motions from audio inputs. We show that our model can generate coherent videos with convincing movements from only a single reference image and a driving audio stream. Our source code is available at https://github.com/RenYurui/PIRender.},
  archive   = {C_ICCV},
  author    = {Yurui Ren and Ge Li and Yuanqi Chen and Thomas H. Li and Shan Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01350},
  pages     = {13739-13748},
  title     = {PIRenderer: Controllable portrait image generation via semantic neural rendering},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image synthesis via semantic composition. <em>ICCV</em>,
13729–13738. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a novel approach to synthesize realistic images based on their semantic layouts. It hypothesizes that for objects with similar appearance, they share similar representation. Our method establishes dependencies between regions according to their appearance correlation, yielding both spatially variant and associated representations. Conditioning on these features, we propose a dynamic weighted network constructed by spatially conditional computation (with both convolution and normalization). More than preserving semantic distinctions, the given dynamic network strengthens semantic relevance, benefiting global structure and detail synthesis. We demonstrate that our method gives the compelling generation performance qualitatively and quantitatively with extensive experiments on benchmarks.},
  archive   = {C_ICCV},
  author    = {Yi Wang and Lu Qi and Ying-Cong Chen and Xiangyu Zhang and Jiaya Jia},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01349},
  pages     = {13729-13738},
  title     = {Image synthesis via semantic composition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Class semantics-based attention for action detection.
<em>ICCV</em>, 13719–13728. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Action localization networks are often structured as a feature encoder sub-network and a localization sub-network, where the feature encoder learns to transform an input video to features that are useful for the localization sub-network to generate reliable action proposals. While some of the encoded features may be more useful for generating action proposals, prior action localization approaches do not include any attention mechanism that enables the localization sub-network to attend more to the more important features. In this paper, we propose a novel attention mechanism, the Class Semantics-based Attention (CSA), that learns from the temporal distribution of semantics of action classes present in an input video to find the importance scores of the encoded features, which are used to provide attention to the more useful encoded features. We demonstrate on two popular action detection datasets that incorporating our novel attention mechanism provides considerable performance gains on competitive action detection models (e.g., around 6.2\% improvement over BMN action detection baseline to obtain 47.5\% mAP on the THUMOS-14 dataset), and a new state-of-the-art of 36.25\% mAP on the ActivityNet v1.3 dataset. Further, the CSA localization model family which includes BMN-CSA, was part of the second-placed submission at the 2021 ActivityNet action localization challenge. Our attention mechanism outperforms prior self-attention modules such as the squeeze-and-excitation in action detection task. We also observe that our attention mechanism is complementary to such self-attention modules in that performance improvements are seen when both are used together.},
  archive   = {C_ICCV},
  author    = {Deepak Sridhar and Niamul Quader and Srikanth Muralidharan and Yaoxin Li and Peng Dai and Juwei Lu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01348},
  pages     = {13719-13728},
  title     = {Class semantics-based attention for action detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CAG-QIL: Context-aware actionness grouping via q imitation
learning for online temporal action localization. <em>ICCV</em>,
13709–13718. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Temporal action localization has been one of the most popular tasks in video understanding, due to the importance of detecting action instances in videos. However, not much progress has been made on extending it to work in an on-line fashion, although many video related tasks can benefit by going online with the growing video streaming services. To this end, we introduce a new task called Online Temporal Action Localization (On-TAL), in which the goal is to immediately detect action instances from an untrimmed streaming video. The online setting makes the new task very challenging as the actionness decision for every frame has to be made without access to future frames and also because post-processing methods cannot be used to modify past action proposals. We propose a novel framework, Context-Aware Actionness Grouping (CAG) as a solution for On-TAL and train it with the imitation learning algorithm, which allows us to avoid sophisticated reward engineering. Evaluation of our work on THUMOS14 and Activitynet1.3 shows significant improvement over non-naive baselines, demonstrating the effectiveness of our approach. As a by-product, our method can also be used for the Online Detection of Action Start (ODAS), in which our method also outperforms previous state-of-the-art models.},
  archive   = {C_ICCV},
  author    = {Hyolim Kang and Kyungmin Kim and Yumin Ko and Seon Joo Kim},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01347},
  pages     = {13709-13718},
  title     = {CAG-QIL: Context-aware actionness grouping via q imitation learning for online temporal action localization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient action recognition via dynamic knowledge
propagation. <em>ICCV</em>, 13699–13708. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Efficient action recognition has become crucial to extend the success of action recognition to many real-world applications. Contrary to most existing methods, which mainly focus on selecting salient frames to reduce the computation cost, we focus more on making the most of the selected frames. To this end, we employ two networks of different capabilities that operate in tandem to efficiently recognize actions. Given a video, the lighter network processes more frames while the heavier one only processes a few. In order to enable the effective interaction between the two, we propose dynamic knowledge propagation based on a cross-attention mechanism. This is the main component of our framework that is essentially a student-teacher architecture, but as the teacher model continues to interact with the student model during inference, we call it a dynamic student-teacher framework. Through extensive experiments, we demonstrate the effectiveness of each component of our framework. Our method outperforms competing state-of-the-art methods on two video datasets: ActivityNet-v1.3 and Mini-Kinetics.},
  archive   = {C_ICCV},
  author    = {Hanul Kim and Mihir Jain and Jun-Tae Lee and Sungrack Yun and Fatih Porikli},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01346},
  pages     = {13699-13708},
  title     = {Efficient action recognition via dynamic knowledge propagation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TAM: Temporal adaptive module for video recognition.
<em>ICCV</em>, 13688–13698. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Video data is with complex temporal dynamics due to various factors such as camera motion, speed variation, and different activities. To effectively capture this diverse motion pattern, this paper presents a new temporal adaptive module (TAM) to generate video-specific temporal kernels based on its own feature map. TAM proposes a unique two-level adaptive modeling scheme by decoupling the dynamic kernel into a location sensitive importance map and a location invariant aggregation weight. The importance map is learned in a local temporal window to capture short-term information, while the aggregation weight is generated from a global view with a focus on long-term structure. TAM is a modular block and could be integrated into 2D CNNs to yield a powerful video architecture (TANet) with a very small extra computational cost. The extensive experiments on Kinetics-400 and Something-Something datasets demonstrate that our TAM outperforms other temporal modeling methods consistently, and achieves the state-of-the-art performance under the similar complexity. The code is available at https://github.com/liu-zhy/temporal-adaptive-module.},
  archive   = {C_ICCV},
  author    = {Zhaoyang Liu and Limin Wang and Wayne Wu and Chen Qian and Tong Lu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01345},
  pages     = {13688-13698},
  title     = {TAM: Temporal adaptive module for video recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Class-incremental learning for action recognition in videos.
<em>ICCV</em>, 13678–13687. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We tackle catastrophic forgetting problem in the context of class-incremental learning for video recognition, which has not been explored actively despite the popularity of continual learning. Our framework addresses this challenging task by introducing time-channel importance maps and exploiting the importance maps for learning the representations of incoming examples via knowledge distillation. We also incorporate a regularization scheme in our objective function, which encourages individual features obtained from different time steps in a video to be uncorrelated and eventually improves accuracy by alleviating catastrophic forgetting. We evaluate the proposed approach on brand-new splits of class-incremental action recognition benchmarks constructed upon the UCF101, HMDB51, and Something-Something V2 datasets, and demonstrate the effectiveness of our algorithm in comparison to the existing continual learning methods that are originally designed for image data.},
  archive   = {C_ICCV},
  author    = {Jaeyoo Park and Minsoo Kang and Bohyung Han},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01344},
  pages     = {13678-13687},
  title     = {Class-incremental learning for action recognition in videos},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Target adaptive context aggregation for video scene graph
generation. <em>ICCV</em>, 13668–13677. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper deals with a challenging task of video scene graph generation (VidSGG), which could serve as a structured video representation for high-level understanding tasks. We present a new detect-to-track paradigm for this task by decoupling the context modeling for relation prediction from the complicated low-level entity tracking. Specifically, we design an efficient method for frame-level VidSGG, termed as Target Adaptive Context Aggregation Network (TRACE), with a focus on capturing spatio-temporal context information for relation recognition. Our TRACE framework streamlines the VidSGG pipeline with a modular design, and presents two unique blocks of Hierarchical Relation Tree (HRTree) construction and Target-adaptive Context Aggregation. More specific, our HRTree first provides an adpative structure for organizing possible relation candidates efficiently, and guides context aggregation module to effectively capture spatio-temporal structure information. Then, we obtain a contextualized feature representation for each relation candidate and build a classification head to recognize its relation category. Finally, we provide a simple temporal association strategy to track TRACE detected results to yield the video-level VidSGG. We perform experiments on two VidSGG benchmarks: ImageNet-VidVRD and Action Genome, and the results demonstrate that our TRACE achieves the state-of-the-art performance. The code and models are made available at https://github.com/MCG-NJU/TRACE.},
  archive   = {C_ICCV},
  author    = {Yao Teng and Limin Wang and Zhifeng Li and Gangshan Wu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01343},
  pages     = {13668-13677},
  title     = {Target adaptive context aggregation for video scene graph generation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-modal multi-action video recognition. <em>ICCV</em>,
13658–13667. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-action video recognition is much more challenging due to the requirement to recognize multiple actions co-occurring simultaneously or sequentially. Modeling multi-action relations is beneficial and crucial to understand videos with multiple actions, and actions in a video are usually presented in multiple modalities. In this paper, we propose a novel multi-action relation model for videos, by leveraging both relational graph convolutional networks (GCNs) and video multi-modality. We first build multi-modal GCNs to explore modality-aware multi-action relations, fed by modality-specific action representation as node features, i.e., spatiotemporal features learned by 3D convolutional neural network (CNN), audio and textual embeddings queried from respective feature lexicons. We then joint both multi-modal CNN-GCN models and multi-modal feature representations for learning better relational action predictions. Ablation study, multi-action relation visualization, and boosts analysis, all show efficacy of our multi-modal multi-action relation modeling. Also our method achieves state-of-the-art performance on large-scale multi-action M-MiT benchmark. Our code is made publicly available at https://github.com/zhenglab/multi-action-video.},
  archive   = {C_ICCV},
  author    = {Zhensheng Shi and Ju Liang and Qianqian Li and Haiyong Zheng and Zhaorui Gu and Junyu Dong and Bing Zheng},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01342},
  pages     = {13658-13667},
  title     = {Multi-modal multi-action video recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GroupFormer: Group activity recognition with clustered
spatial-temporal transformer. <em>ICCV</em>, 13648–13657. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Group activity recognition is a crucial yet challenging problem, whose core lies in fully exploring spatial-temporal interactions among individuals and generating reasonable group representations. However, previous methods either model spatial and temporal information separately, or directly aggregate individual features to form group features. To address these issues, we propose a novel group activity recognition network termed GroupFormer. It captures spatial-temporal contextual information jointly to augment the individual and group representations effectively with a clustered spatial-temporal transformer. Specifically, our GroupFormer has three appealing advantages: (1) A tailor-modified Transformer, Clustered Spatial-Temporal Transformer, is proposed to enhance the individual representation and group representation. (2) It models the spatial and temporal dependencies integrally and utilizes decoders to build the bridge between the spatial and temporal information. (3) A clustered attention mechanism is utilized to dynamically divide individuals into multiple clusters for better learning activity-aware semantic representations. Moreover, experimental results show that the proposed framework outperforms state-of-the-art methods on the Volleyball dataset and Collective Activity dataset. Code is available at https://github.com/xueyee/GroupFormer},
  archive   = {C_ICCV},
  author    = {Shuaicheng Li and Qianggang Cao and Lingbo Liu and Kunlin Yang and Shinan Liu and Jun Hou and Shuai Yi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01341},
  pages     = {13648-13657},
  title     = {GroupFormer: Group activity recognition with clustered spatial-temporal transformer},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Video self-stitching graph network for temporal action
localization. <em>ICCV</em>, 13638–13647. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Temporal action localization (TAL) in videos is a challenging task, especially due to the large variation in action temporal scales. Short actions usually occupy a major proportion in the datasets, but tend to have the lowest performance. In this paper, we confront the challenge of short actions and propose a multi-level cross-scale solution dubbed as video self-stitching graph network (VSGN). We have two key components in VSGN: video self-stitching (VSS) and cross-scale graph pyramid network (xGPN). In VSS, we focus on a short period of a video and magnify it along the temporal dimension to obtain a larger scale. We stitch the original clip and its magnified counterpart in one input sequence to take advantage of the complementary properties of both scales. The xGPN component further exploits the cross-scale correlations by a pyramid of cross-scale graph networks, each containing a hybrid module to aggregate features from across scales as well as within the same scale. Our VSGN not only enhances the feature representations, but also generates more positive anchors for short actions and more short training samples. Experiments demonstrate that VSGN obviously improves the localization performance of short actions as well as achieving the state-of-the-art overall performance on THUMOS-14 and ActivityNet-v1.3. VSGN code is available at https://github.com/coolbay/VSGN.},
  archive   = {C_ICCV},
  author    = {Chen Zhao and Ali Thabet and Bernard Ghanem},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01340},
  pages     = {13638-13647},
  title     = {Video self-stitching graph network for temporal action localization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning action completeness from points for
weakly-supervised temporal action localization. <em>ICCV</em>,
13628–13637. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We tackle the problem of localizing temporal intervals of actions with only a single frame label for each action instance for training. Owing to label sparsity, existing work fails to learn action completeness, resulting in fragmentary action predictions. In this paper, we propose a novel framework, where dense pseudo-labels are generated to provide completeness guidance for the model. Concretely, we first select pseudo background points to supplement point-level action labels. Then, by taking the points as seeds, we search for the optimal sequence that is likely to contain complete action instances while agreeing with the seeds. To learn completeness from the obtained sequence, we introduce two novel losses that contrast action instances with background ones in terms of action score and feature similarity, respectively. Experimental results demonstrate that our completeness guidance indeed helps the model to locate complete action instances, leading to large performance gains especially under high IoU thresholds. Moreover, we demonstrate the superiority of our method over existing state-of-the-art methods on four benchmarks: THUMOS’14, GTEA, BEOID, and ActivityNet. Notably, our method even performs comparably to recent fully-supervised methods, at the 6× cheaper annotation cost. Our code is available at https://github.com/Pilhyeon.},
  archive   = {C_ICCV},
  author    = {Pilhyeon Lee and Hyeran Byun},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01339},
  pages     = {13628-13637},
  title     = {Learning action completeness from points for weakly-supervised temporal action localization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Elaborative rehearsal for zero-shot action recognition.
<em>ICCV</em>, 13618–13627. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The growing number of action classes has posed a new challenge for video understanding, making Zero-Shot Action Recognition (ZSAR) a thriving direction. The ZSAR task aims to recognize target (unseen) actions without training examples by leveraging semantic representations to bridge seen and unseen actions. However, due to the complexity and diversity of actions, it remains challenging to semantically represent action classes and transfer knowledge from seen data. In this work, we propose an ER-enhanced ZSAR model inspired by an effective human memory technique Elaborative Rehearsal (ER), which involves elaborating a new concept and relating it to known concepts. Specifically, we expand each action class as an Elaborative Description (ED) sentence, which is more discriminative than a class name and less costly than manual-defined attributes. Besides directly aligning class semantics with videos, we incorporate objects from the video as Elaborative Concepts (EC) to improve video semantics and generalization from seen actions to unseen actions. Our ER-enhanced ZSAR model achieves state-of-the-art results on three existing benchmarks. Moreover, we propose a new ZSAR evaluation protocol on the Kinetics dataset to overcome limitations of current benchmarks and first compare with few-shot learning baselines on this more realistic setting. Our codes and collected EDs are released at https://github.com/DeLightCMU/ElaborativeRehearsal.},
  archive   = {C_ICCV},
  author    = {Shizhe Chen and Dong Huang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01338},
  pages     = {13618-13627},
  title     = {Elaborative rehearsal for zero-shot action recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Selective feature compression for efficient activity
recognition inference. <em>ICCV</em>, 13608–13617. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most action recognition solutions rely on dense sampling to precisely cover the informative temporal clip. Extensively searching temporal region is expensive for a real-world application. In this work, we focus on improving the inference efficiency of current action recognition backbones on trimmed videos, and illustrate that an action model can accurately classify an action with a single pass over the video unlike the multi-clip sampling common with SOTA by learning to drop non-informative features. We present Selective Feature Compression (SFC), an action recognition inference strategy that greatly increases model inference efficiency without compromising accuracy. Different from previous works that compress kernel size and decrease the channel dimension, we propose to compress features along the spatio-temporal dimensions without the need to change backbone parameters. Our experiments on Kinetics-400, UCF101 and ActivityNet show that SFC is able to reduce inference speed by 6-7x and memory usage by 5-6x compared with the commonly used 30 crop dense sampling procedure, while also slightly improving Top1 Accuracy. We perform thorough quantitative and qualitative evaluation and show how our SFC learns to attend to important video regions for the task of action recognition.},
  archive   = {C_ICCV},
  author    = {Chunhui Liu and Xinyu Li and Hao Chen and Davide Modolo and Joseph Tighe},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01337},
  pages     = {13608-13617},
  title     = {Selective feature compression for efficient activity recognition inference},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning cross-modal contrastive features for video domain
adaptation. <em>ICCV</em>, 13598–13607. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning transferable and domain adaptive feature representations from videos is important for video-relevant tasks such as action recognition. Existing video domain adaptation methods mainly rely on adversarial feature alignment, which has been derived from the RGB image space. However, video data is usually associated with multi-modal information, e.g., RGB and optical flow, and thus it remains a challenge to design a better method that considers the cross-modal inputs under the cross-domain adaptation setting. To this end, we propose a unified framework for video domain adaptation, which simultaneously regularizes cross-modal and cross-domain feature representations. Specifically, we treat each modality in a domain as a view and leverage the contrastive learning technique with properly designed sampling strategies. As a result, our objectives regularize feature spaces, which originally lack the connection across modalities or have less alignment across domains. We conduct experiments on domain adaptive action recognition benchmark datasets, i.e., UCF, HMDB, and EPIC-Kitchens, and demonstrate the effectiveness of our components against state-of-the-art algorithms.},
  archive   = {C_ICCV},
  author    = {Donghyun Kim and Yi-Hsuan Tsai and Bingbing Zhuang and Xiang Yu and Stan Sclaroff and Kate Saenko and Manmohan Chandraker},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01336},
  pages     = {13598-13607},
  title     = {Learning cross-modal contrastive features for video domain adaptation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). D2-net: Weakly-supervised action localization via
discriminative embeddings and denoised activations. <em>ICCV</em>,
13588–13597. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work proposes a weakly-supervised temporal action localization framework, called D2-Net, which strives to temporally localize actions using video-level supervision. Our main contribution is the introduction of a novel loss formulation, which jointly enhances the discriminability of latent embeddings and robustness of the output temporal class activations with respect to foreground-background noise caused by weak supervision. The proposed formulation comprises a discriminative and a denoising loss term for enhancing temporal action localization. The discriminative term incorporates a classification loss and utilizes a top-down attention mechanism to enhance the separability of latent foreground-background embeddings. The denoising loss term explicitly addresses the foreground-background noise in class activations by simultaneously maximizing intra-video and inter-video mutual information using a bottom-up attention mechanism. As a result, activations in the foreground regions are emphasized whereas those in the background regions are suppressed, thereby leading to more robust predictions. Comprehensive experiments are performed on multiple benchmarks, including THUMOS14 and ActivityNet1.2. Our D2-Net performs favorably in comparison to the existing methods on all datasets, achieving gains as high as 2.3\% in terms of mAP at IoU=0.5 on THUMOS14. Source code is available at https://github.com/naraysa/D2-Net.},
  archive   = {C_ICCV},
  author    = {Sanath Narayan and Hisham Cholakkal and Munawar Hayat and Fahad Shahbaz Khan and Ming-Hsuan Yang and Ling Shao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01335},
  pages     = {13588-13597},
  title     = {D2-net: Weakly-supervised action localization via discriminative embeddings and denoised activations},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Assignment-space-based multi-object tracking and
segmentation. <em>ICCV</em>, 13578–13587. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-object tracking and segmentation (MOTS) is important for understanding dynamic scenes in video data. Existing methods perform well on multi-object detection and segmentation for independent video frames, but tracking of objects over time remains a challenge. MOTS methods formulate tracking locally, i.e., frame-by-frame, leading to sub-optimal results. Classical global methods on tracking operate directly on object detections, which leads to a combinatorial growth in the detection space. In contrast, we formulate a global method for MOTS over the space of assignments rather than detections: First, we find all top-k assignments of objects detected and segmented between any two consecutive frames and develop a structured prediction formulation to score assignment sequences across any number of consecutive frames. We use dynamic programming to find the global optimizer of this formulation in polynomial time. Second, we connect objects which reappear after having been out of view for some time. For this we formulate an assignment problem. On the challenging KITTI-MOTS and MOTSChallenge datasets, this achieves state-of-the-art results among methods which don’t use depth data.},
  archive   = {C_ICCV},
  author    = {Anwesa Choudhuri and Girish Chowdhary and Alexander G. Schwing},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01334},
  pages     = {13578-13587},
  title     = {Assignment-space-based multi-object tracking and segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hybrid video anomaly detection framework via
memory-augmented flow reconstruction and flow-guided frame prediction.
<em>ICCV</em>, 13568–13577. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose HF 2 -VAD, a Hybrid framework that integrates Flow reconstruction and Frame prediction seamlessly to handle Video Anomaly Detection. Firstly, we design the network of ML-MemAE-SC (Multi-Level Memory modules in an Autoencoder with Skip Connections) to memorize normal patterns for optical flow reconstruction so that abnormal events can be sensitively identified with larger flow reconstruction errors. More importantly, conditioned on the reconstructed flows, we then employ a Conditional Variational Autoencoder (CVAE), which captures the high correlation between video frame and optical flow, to predict the next frame given several previous frames. By CVAE, the quality of flow reconstruction essentially influences that of frame prediction. Therefore, poorly reconstructed optical flows of abnormal events further deteriorate the quality of the final predicted future frame, making the anomalies more detectable. Experimental results demonstrate the effectiveness of the proposed method. Code is available at https://github.com/LiUzHiAn/hf2vad.},
  archive   = {C_ICCV},
  author    = {Zhian Liu and Yongwei Nie and Chengjiang Long and Qing Zhang and Guiqing Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01333},
  pages     = {13568-13577},
  title     = {A hybrid video anomaly detection framework via memory-augmented flow reconstruction and flow-guided frame prediction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VidTr: Video transformer without convolutions.
<em>ICCV</em>, 13557–13567. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce Video Transformer (VidTr) with separable-attention for video classification. Comparing with commonly used 3D networks, VidTr is able to aggregate spatiotemporal information via stacked attentions and provide better performance with higher efficiency. We first introduce the vanilla video transformer and show that transformer module is able to perform spatio-temporal modeling from raw pixels, but with heavy memory usage. We then present VidTr which reduces the memory cost by 3.3× while keeping the same performance. To further optimize the model, we propose the standard deviation based topK pooling for attention (pool topK_std ), which reduces the computation by dropping non-informative features along temporal dimension. VidTr achieves state-of-the-art performance on five commonly used datasets with lower computational requirement, showing both the efficiency and effectiveness of our design. Finally, error analysis and visualization show that VidTr is especially good at predicting actions that require long-term temporal reasoning.},
  archive   = {C_ICCV},
  author    = {Yanyi Zhang and Xinyu Li and Chunhui Liu and Bing Shuai and Yi Zhu and Biagio Brattoli and Hao Chen and Ivan Marsic and Joseph Tighe},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01332},
  pages     = {13557-13567},
  title     = {VidTr: Video transformer without convolutions},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Channel augmented joint learning for visible-infrared
recognition. <em>ICCV</em>, 13547–13556. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a powerful channel augmented joint learning strategy for the visible-infrared recognition problem. For data augmentation, most existing methods directly adopt the standard operations designed for single-modality visible images, and thus do not fully consider the imagery properties in visible to infrared matching. Our basic idea is to homogenously generate color-irrelevant images by randomly exchanging the color channels. It can be seamlessly integrated into existing augmentation operations without modifying the network, consistently improving the robustness against color variations. Incorporated with a random erasing strategy, it further greatly enriches the diversity by simulating random occlusions. For cross-modality metric learning, we design an enhanced channel-mixed learning strategy to simultaneously handle the intra-and cross-modality variations with squared difference for stronger discriminability. Besides, a channel-augmented joint learning strategy is further developed to explicitly optimize the outputs of augmented images. Extensive experiments with insightful analysis on two visible-infrared recognition tasks show that the proposed strategies consistently improve the accuracy. Without auxiliary information, it improves the state-of-the-art Rank-1/mAP by 14.59\%/13.00\% on the large-scale SYSU-MM01 dataset.},
  archive   = {C_ICCV},
  author    = {Mang Ye and Weijian Ruan and Bo Du and Mike Zheng Shou},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01331},
  pages     = {13547-13556},
  title     = {Channel augmented joint learning for visible-infrared recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generating masks from boxes by mining spatio-temporal
consistencies in videos. <em>ICCV</em>, 13536–13546. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Segmenting objects in videos is a fundamental computer vision task. The current deep learning based paradigm offers a powerful, but data-hungry solution. However, current datasets are limited by the cost and human effort of annotating object masks in videos. This effectively limits the performance and generalization capabilities of existing video segmentation methods. To address this issue, we explore weaker form of bounding box annotations.We introduce a method for generating segmentation masks from per-frame bounding box annotations in videos. To this end, we propose a spatio-temporal aggregation module that effectively mines consistencies in the object and background appearance across multiple frames. We use our predicted accurate masks to train video object segmentation (VOS) networks for the tracking domain, where only manual bounding box annotations are available. The additional data provides substantially better generalization performance, leading to state-of-the-art results on standard tracking benchmarks. The code and models are available at https://github.com/visionml/pytracking.},
  archive   = {C_ICCV},
  author    = {Bin Zhao and Goutam Bhat and Martin Danelljan and Luc Van Gool and Radu Timofte},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01330},
  pages     = {13536-13546},
  title     = {Generating masks from boxes by mining spatio-temporal consistencies in videos},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to track objects from unlabeled videos.
<em>ICCV</em>, 13526–13535. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose to learn an Unsupervised Single Object Tracker (USOT) from scratch. We identify that three major challenges, i.e., moving object discovery, rich temporal variation exploitation, and online update, are the central causes of the performance bottleneck of existing unsupervised trackers. To narrow the gap between unsupervised trackers and supervised counterparts, we propose an effective unsupervised learning approach composed of three stages. First, we sample sequentially moving objects with unsupervised optical flow and dynamic programming, instead of random cropping. Second, we train a naive Siamese tracker from scratch using single-frame pairs. Third, we continue training the tracker with a novel cycle memory learning scheme, which is conducted in longer temporal spans and also enables our tracker to update online. Extensive experiments show that the proposed USOT learned from unlabeled videos performs well over the state-of-the-art unsupervised trackers by large margins, and on par with recent supervised deep trackers. Code is available at https://github.com/VISION-SJTU/USOT.},
  archive   = {C_ICCV},
  author    = {Jilai Zheng and Chao Ma and Houwen Peng and Xiaokang Yang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01329},
  pages     = {13526-13535},
  title     = {Learning to track objects from unlabeled videos},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MultiSports: A multi-person video dataset of
spatio-temporally localized sports actions. <em>ICCV</em>, 13516–13525.
(<a href="https://doi.org/10.1109/ICCV48922.2021.01328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Spatio-temporal action detection is an important and challenging problem in video understanding. The existing action detection benchmarks are limited in aspects of small numbers of instances in a trimmed video or low-level atomic actions. This paper aims to present a new multi-person dataset of spatio-temporal localized sports actions, coined as MultiSports. We first analyze the important ingredients of constructing a realistic and challenging dataset for spatio-temporal action detection by proposing three criteria: (1) multi-person scenes and motion dependent identification, (2) with well-defined boundaries, (3) relatively fine-grained classes of high complexity. Based on these guide-lines, we build the dataset of MultiSports v1.0 by selecting 4 sports classes, collecting 3200 video clips, and annotating 37701 action instances with 902k bounding boxes. Our datasets are characterized with important properties of high diversity, dense annotation, and high quality. Our Multi-Sports, with its realistic setting and detailed annotations, exposes the intrinsic challenges of spatio-temporal action detection. To benchmark this, we adapt several baseline methods to our dataset and give an indepth analysis on the action detection results in our dataset. We hope our MultiSports can serve as a standard benchmark for spatio-temporal action detection in the future. Our dataset website is at https://deeperaction.github.io/multisports/.},
  archive   = {C_ICCV},
  author    = {Yixuan Li and Lei Chen and Runyu He and Zhenzhi Wang and Gangshan Wu and Limin Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01328},
  pages     = {13516-13525},
  title     = {MultiSports: A multi-person video dataset of spatio-temporally localized sports actions},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Relaxed transformer decoders for direct action proposal
generation. <em>ICCV</em>, 13506–13515. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Temporal action proposal generation is an important and challenging task in video understanding, which aims at detecting all temporal segments containing action in-stances of interest. The existing proposal generation approaches are generally based on pre-defined anchor windows or heuristic bottom-up boundary matching strategies. This paper presents a simple and efficient framework (RTD-Net) for direct action proposal generation, by re-purposing a Transformer-alike architecture. To tackle the essential visual difference between time and space, we make three important improvements over the original transformer detection framework (DETR). First, to deal with slowness prior in videos, we replace the original Transformer en-coder with a boundary attentive module to better capture long-range temporal information. Second, due to the ambiguous temporal boundary and relatively sparse annotations, we present a relaxed matching scheme to relieve the strict criteria of single assignment to each groundtruth. Finally, we devise a three-branch head to further improve the proposal confidence estimation by explicitly predicting its completeness. Extensive experiments on THUMOS14 and ActivityNet-1.3 benchmarks demonstrate the effectiveness of RTD-Net, on both tasks of temporal action proposal generation and temporal action detection. Moreover, due to its simplicity in design, our framework is more efficient than previous proposal generation methods, without non-maximum suppression post-processing. The code and models are made available at https://github.com/MCG-NJU/RTD-Action.},
  archive   = {C_ICCV},
  author    = {Jing Tan and Jiaqi Tang and Limin Wang and Gangshan Wu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01327},
  pages     = {13506-13515},
  title     = {Relaxed transformer decoders for direct action proposal generation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enriching local and global contexts for temporal action
localization. <em>ICCV</em>, 13496–13505. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Effectively tackling the problem of temporal action localization (TAL) necessitates a visual representation that jointly pursues two confounding goals, i.e., fine-grained discrimination for temporal localization and sufficient visual invariance for action classification. We address this challenge by enriching both the local and global contexts in the popular two-stage temporal localization framework, where action proposals are first generated followed by action classification and temporal boundary regression. Our proposed model, dubbed ContextLoc, can be divided into three sub-networks: L-Net, G-Net and P-Net. L-Net enriches the local context via fine-grained modeling of snippet-level features, which is formulated as a query-and-retrieval process. G-Net enriches the global context via higher-level modeling of the video-level representation. In addition, we introduce a novel context adaptation module to adapt the global context to different proposals. P-Net further models the context-aware inter-proposal relations. We explore two existing models to be the P-Net in our experiments. The efficacy of our proposed method is validated by experimental results on the THUMOS14 (54.3\% at tIoU@0.5) and ActivityNet v1.3 (56.01\% at tIoU@0.5) datasets, which outperforms recent states of the art. Code is available at https://github.com/buxiangzhiren/ContextLoc.},
  archive   = {C_ICCV},
  author    = {Zixin Zhu and Wei Tang and Le Wang and Nanning Zheng and Gang Hua},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01326},
  pages     = {13496-13505},
  title     = {Enriching local and global contexts for temporal action localization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Anticipative video transformer. <em>ICCV</em>, 13485–13495.
(<a href="https://doi.org/10.1109/ICCV48922.2021.01325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose Anticipative Video Transformer (AVT), an end-to-end attention-based video modeling architecture that attends to the previously observed video in order to anticipate future actions. We train the model jointly to predict the next action in a video sequence, while also learning frame feature encoders that are predictive of successive future frames’ features. Compared to existing temporal aggregation strategies, AVT has the advantage of both maintaining the sequential progression of observed actions while still capturing long-range dependencies—both critical for the anticipation task. Through extensive experiments, we show that AVT obtains the best reported performance on four popular action anticipation benchmarks: EpicKitchens-55, EpicKitchens-100, EGTEA Gaze+, and 50-Salads; and it wins first place in the EpicKitchens-100 CVPR’21 challenge.},
  archive   = {C_ICCV},
  author    = {Rohit Girdhar and Kristen Grauman},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01325},
  pages     = {13485-13495},
  title     = {Anticipative video transformer},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The spatio-temporal poisson point process: A simple model
for the alignment of event camera data. <em>ICCV</em>, 13475–13484. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Event cameras, inspired by biological vision systems, provide a natural and data efficient representation of visual information. Visual information is acquired in the form of events that are triggered by local brightness changes. However, because most brightness changes are triggered by relative motion of the camera and the scene, the events recorded at a single sensor location seldom correspond to the same world point. To extract meaningful information from event cameras, it is helpful to register events that were triggered by the same underlying world point. In this work we propose a new model of event data that captures its natural spatio-temporal structure. We start by developing a model for aligned event data. That is, we develop a model for the data as though it has been perfectly registered already. In particular, we model the aligned data as a spatio-temporal Poisson point process. Based on this model, we develop a maximum likelihood approach to registering events that are not yet aligned. That is, we find transformations of the observed events that make them as likely as possible under our model. In particular we extract the camera rotation that leads to the best event alignment. We show new state of the art accuracy for rotational velocity estimation on the DAVIS 240C dataset [20]. In addition, our method is also faster and has lower computational complexity than several competing methods. Code: https://github.com/pbideau/Event-ST-PPP},
  archive   = {C_ICCV},
  author    = {Cheng Gu and Erik Learned-Miller and Daniel Sheldon and Guillermo Gallego and Pia Bideau},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01324},
  pages     = {13475-13484},
  title     = {The spatio-temporal poisson point process: A simple model for the alignment of event camera data},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Social fabric: Tubelet compositions for video relation
detection. <em>ICCV</em>, 13465–13474. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper strives to classify and detect the relationship between object tubelets appearing within a video as a 〈subject-predicate-object〉 triplet. Where existing works treat object proposals or tubelets as single entities and model their relations a posteriori, we propose to classify and detect predicates for pairs of object tubelets a priori. We also propose Social Fabric: an encoding that represents a pair of object tubelets as a composition of interaction primitives. These primitives are learned over all relations, resulting in a compact representation able to localize and classify relations from the pool of co-occurring object tubelets across all timespans in a video. The encoding enables our two-stage network. In the first stage, we train Social Fabric to suggest proposals that are likely interacting. We use the Social Fabric in the second stage to simultaneously finetune and predict predicate labels for the tubelets. Experiments demonstrate the benefit of early video relation modeling, our encoding and the two-stage architecture, leading to a new state-of-the-art on two benchmarks. We also show how the encoding enables query-by-primitive-example to search for spatio-temporal video relations. Code: https://github.com/shanshuo/Social-Fabric.},
  archive   = {C_ICCV},
  author    = {Shuo Chen and Zenglin Shi and Pascal Mettes and Cees G. M. Snoek},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01323},
  pages     = {13465-13474},
  title     = {Social fabric: Tubelet compositions for video relation detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discovering human interactions with large-vocabulary objects
via query and multi-scale detection. <em>ICCV</em>, 13455–13464. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we study the problem of human-object interaction (HOI) detection with large vocabulary object categories. Previous HOI studies are mainly conducted in the regime of limit object categories (e.g., 80 categories). Their solutions may face new difficulties in both object detection and interaction classification due to the increasing diversity of objects (e.g., 1000 categories). Different from previous methods, we formulate the HOI detection as a query problem. We propose a unified model to jointly discover the target objects and predict the corresponding interactions based on the human queries, thereby eliminating the need of using generic object detectors, extra steps to associate human-object instances, and multi-stream interaction recognition. This is achieved by a repurposed Transformer unit and a novel cascade detection over multi-scale feature maps. We observe that such a highly-coupled solution brings benefits for both object detection and interaction classification in a large vocabulary setting. To study the new challenges of the large vocabulary HOI detection, we assemble two datasets from the publicly available SWiG and 100 Days of Hands datasets. Experiments on these datasets validate that our proposed method can achieve a notable mAP improvement on HOI detection with a faster inference speed than existing one-stage HOI detectors. Our code is available at https://github.com/scwangdyd/large_vocabulary_hoi_detection.},
  archive   = {C_ICCV},
  author    = {Suchen Wang and Kim-Hui Yap and Henghui Ding and Jiyan Wu and Junsong Yuan and Yap-Peng Tan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01322},
  pages     = {13455-13464},
  title     = {Discovering human interactions with large-vocabulary objects via query and multi-scale detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HAA500: Human-centric atomic action dataset with curated
videos. <em>ICCV</em>, 13445–13454. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We contribute HAA500 1 , a manually annotated human-centric atomic action dataset for action recognition on 500 classes with over 591K labeled frames. To minimize ambiguities in action classification, HAA500 consists of highly diversified classes of fine-grained atomic actions, where only consistent actions fall under the same label, e.g., &quot;Baseball Pitching&quot; vs &quot;Free Throw in Basketball&quot;. Thus HAA500 is different from existing atomic action datasets, where coarse-grained atomic actions were labeled with coarse action-verbs such as &quot;Throw&quot;. HAA500 has been carefully curated to capture the precise movement of human figures with little class-irrelevant motions or spatiotemporal label noises.The advantages of HAA500 are fourfold: 1) human-centric actions with a high average of 69.7\% detectable joints for the relevant human poses; 2) high scalability since adding a new class can be done under 20–60 minutes; 3) curated videos capturing essential elements of an atomic action without irrelevant frames; 4) fine-grained atomic action classes. Our extensive experiments including cross-data validation using datasets collected in the wild demonstrate the clear benefits of human-centric and atomic characteristics of HAA500, which enable training even a baseline deep learning model to improve prediction by attending to atomic human poses. We detail the HAA500 dataset statistics and collection methodology and compare quantitatively with existing action recognition datasets.},
  archive   = {C_ICCV},
  author    = {Jihoon Chung and Cheng-Hsin Wuu and Hsuan-Ru Yang and Yu-Wing Tai and Chi-Keung Tang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01321},
  pages     = {13445-13454},
  title     = {HAA500: Human-centric atomic action dataset with curated videos},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Divide and conquer for single-frame temporal action
localization. <em>ICCV</em>, 13435–13444. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Single-frame temporal action localization (STAL) aims to localize actions in untrimmed videos with only one timestamp annotation for each action instance. Existing methods adopt the one-stage framework but couple the counting goal and the localization goal. This paper proposes a novel two-stage framework for the STAL task with the spirit of divide and conquer. The instance counting stage leverages the location supervision to determine the number of action instances and divide a whole video into multiple video clips, so that each video clip contains only one complete action instance; and the location estimation stage leverages the category supervision to localize the action instance in each video clip. To efficiently represent the action instance in each video clip, we introduce the proposal-based representation, and design a novel differentiable mask generator to enable the end-to-end training supervised by category labels. On THUMOS14, GTEA, and BEOID datasets, our method outperforms state-of-the-art methods by 3.5\%, 2.7\%, 4.8\% mAP on average. And extensive experiments verify the effectiveness of our method.},
  archive   = {C_ICCV},
  author    = {Chen Ju and Peisen Zhao and Siheng Chen and Ya Zhang and Yanfeng Wang and Qi Tian},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01320},
  pages     = {13435-13444},
  title     = {Divide and conquer for single-frame temporal action localization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning target candidate association to keep track of what
not to track. <em>ICCV</em>, 13424–13434. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The presence of objects that are confusingly similar to the tracked target, poses a fundamental challenge in appearance-based visual tracking. Such distractor objects are easily misclassified as the target itself, leading to eventual tracking failure. While most methods strive to suppress distractors through more powerful appearance models, we take an alternative approach.We propose to keep track of distractor objects in order to continue tracking the target. To this end, we introduce a learned association network, allowing us to propagate the identities of all target candidates from frame-to-frame. To tackle the problem of lacking ground-truth correspondences between distractor objects in visual tracking, we propose a training strategy that combines partial annotations with self-supervision. We conduct comprehensive experimental validation and analysis of our approach on several challenging datasets. Our tracker sets a new state-of-the-art on six benchmarks, achieving an AUC score of 67.1\% on LaSOT [21] and a +5.8\% absolute gain on the OxUvA long-term dataset [41]. The code and trained models are available at https://github.com/visionml/pytracking},
  archive   = {C_ICCV},
  author    = {Christoph Mayer and Martin Danelljan and Danda Pani Paudel and Luc Van Gool},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01319},
  pages     = {13424-13434},
  title     = {Learning target candidate association to keep track of what not to track},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Else-net: Elastic semantic network for continual action
recognition from skeleton data. <em>ICCV</em>, 13414–13423. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most of the state-of-the-art action recognition methods focus on offline learning, where the samples of all types of actions need to be provided at once. Here, we address continual learning of action recognition, where various types of new actions are continuously learned over time. This task is quite challenging, owing to the catastrophic forgetting problem stemming from the discrepancies between the previously learned actions and current new actions to be learned. Therefore, we propose Else-Net, a novel Elastic Semantic Network with multiple learning blocks to learn diversified human actions over time. Specifically, our Else-Net is able to automatically search and update the most relevant learning blocks w.r.t. the current new action, or explore new blocks to store new knowledge, preserving the unmatched ones to retain the knowledge of previously learned actions and alleviates forgetting when learning new actions. Moreover, even though different human actions may vary to a large extent as a whole, their local body parts can still share many homogeneous features. Inspired by this, our proposed Else-Net mines the shared knowledge of the decomposed human body parts from different actions, which benefits continual learning of actions. Experiments show that the proposed approach enables effective continual action recognition and achieves promising performance on two large-scale action recognition datasets.},
  archive   = {C_ICCV},
  author    = {Tianjiao Li and Qiuhong Ke and Hossein Rahmani and Rui En Ho and Henghui Ding and Jun Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01318},
  pages     = {13414-13423},
  title     = {Else-net: Elastic semantic network for continual action recognition from skeleton data},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Skeleton cloud colorization for unsupervised 3D action
representation learning. <em>ICCV</em>, 13403–13413. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Skeleton-based human action recognition has attracted increasing attention in recent years. However, most of the existing works focus on supervised learning which requiring a large number of annotated action sequences that are often expensive to collect. We investigate unsupervised representation learning for skeleton action recognition, and design a novel skeleton cloud colorization technique that is capable of learning skeleton representations from unlabeled skeleton sequence data. Specifically, we represent a skeleton action sequence as a 3D skeleton cloud and colorize each point in the cloud according to its temporal and spatial orders in the original (unannotated) skeleton sequence. Leveraging the colorized skeleton point cloud, we design an auto-encoder framework that can learn spatial-temporal features from the artificial color labels of skeleton joints effectively. We evaluate our skeleton cloud colorization approach with action classifiers trained under different configurations, including unsupervised, semi-supervised and fully-supervised settings. Extensive experiments on NTU RGB+D and NW-UCLA datasets show that the proposed method outperforms existing unsupervised and semi-supervised 3D action recognition methods by large margins, and it achieves competitive performance in supervised 3D action recognition as well.},
  archive   = {C_ICCV},
  author    = {Siyuan Yang and Jun Liu and Shijian Lu and Meng Hwa Er and Alex C. Kot},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01317},
  pages     = {13403-13413},
  title     = {Skeleton cloud colorization for unsupervised 3D action representation learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AdaSGN: Adapting joint number and model size for efficient
skeleton-based action recognition. <em>ICCV</em>, 13393–13402. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing methods for skeleton-based action recognition mainly focus on improving the recognition accuracy, whereas the efficiency of the model is rarely considered. Recently, there are some works trying to speed up the skeleton modeling by designing light-weight modules. However, in addition to the model size, the amount of the data involved in the calculation is also an important factor for the running speed, especially for the skeleton data where most of the joints are redundant or non-informative to identify a specific skeleton. Besides, previous works usually employ one fix-sized model for all the samples regardless of the difficulty of recognition, which wastes computations for easy samples. To address these limitations, a novel approach, called AdaSGN, is proposed in this paper, which can reduce the computational cost of the inference process by adaptively controlling the input number of the joints of the skeleton on-the-fly. Moreover, it can also adaptively select the optimal model size for each sample to achieve a better trade-off between the accuracy and the efficiency. We conduct extensive experiments on three challenging datasets, namely, NTU-60, NTU-120 and SHREC, to verify the superiority of the proposed approach, where AdaSGN achieves comparable or even higher performance with much lower GFLOPs compared with the baseline method.},
  archive   = {C_ICCV},
  author    = {Lei Shi and Yifan Zhang and Jian Cheng and Hanqing Lu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01316},
  pages     = {13393-13402},
  title     = {AdaSGN: Adapting joint number and model size for efficient skeleton-based action recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). AI choreographer: Music conditioned 3D dance generation
with AIST++. <em>ICCV</em>, 13381–13392. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present AIST++, a new multi-modal dataset of 3D dance motion and music, along with FACT, a Full-Attention Cross-modal Transformer network for generating 3D dance motion conditioned on music. The proposed AIST++ dataset contains 5.2 hours of 3D dance motion in 1408 sequences, covering 10 dance genres with multi-view videos with known camera poses—the largest dataset of this kind to our knowledge. We show that naively applying sequence models such as transformers to this dataset for the task of music conditioned 3D motion generation does not produce satisfactory 3D motion that is well correlated with the input music. We overcome these shortcomings by introducing key changes in its architecture design and supervision: FACT model involves a deep cross-modal transformer block with full-attention that is trained to predict N future motions. We empirically show that these changes are key factors in generating long sequences of realistic dance motion that are well-attuned to the input music. We conduct extensive experiments on AIST++ with user studies, where our method outperforms recent state-of-the-art methods both qualitatively and quantitatively. The code and the dataset can be found at: https://google.github.io/aichoreographer.},
  archive   = {C_ICCV},
  author    = {Ruilong Li and Shan Yang and David A. Ross and Angjoo Kanazawa},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01315},
  pages     = {13381-13392},
  title     = {AI choreographer: Music conditioned 3D dance generation with AIST++},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TRiPOD: Human trajectory and pose dynamics forecasting in
the wild. <em>ICCV</em>, 13370–13380. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Joint forecasting of human trajectory and pose dynamics is a fundamental building block of various applications ranging from robotics and autonomous driving to surveillance systems. Predicting body dynamics requires capturing subtle information embedded in the humans’ interactions with each other and with the objects present in the scene. In this paper, we propose a novel TRajectory and POse Dynamics (nicknamed TRiPOD) method based on graph attentional networks to model the human-human and human-object interactions both in the input space and the output space (decoded future output). The model is supplemented by a message passing interface over the graphs to fuse these different levels of interactions efficiently. Furthermore, to incorporate a real-world challenge, we propound to learn an indicator representing whether an estimated body joint is visible/invisible at each frame, e.g. due to occlusion or being outside the sensor field of view. Finally, we introduce a new benchmark for this joint task based on two challenging datasets (PoseTrack and 3DPW) and propose evaluation metrics to measure the effectiveness of predictions in the global space, even when there are invisible cases of joints. Our evaluation shows that TRiPOD outperforms all prior work and state-of-the-art specifically designed for each of the trajectory and pose forecasting tasks.},
  archive   = {C_ICCV},
  author    = {Vida Adeli and Mahsa Ehsanpour and Ian Reid and Juan Carlos Niebles and Silvio Savarese and Ehsan Adeli and Hamid Rezatofighi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01314},
  pages     = {13370-13380},
  title     = {TRiPOD: Human trajectory and pose dynamics forecasting in the wild},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GeomNet: A neural network based on riemannian geometries of
SPD matrix space and cholesky space for 3D skeleton-based interaction
recognition. <em>ICCV</em>, 13359–13369. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel method for representation and classification of two-person interactions from 3D skeleton sequences. The key idea of our approach is to use Gaussian distributions to capture statistics on ℝ n and those on the space of symmetric positive definite (SPD) matrices. The main challenge is how to parametrize those distributions. Towards this end, we develop methods for embedding Gaussian distributions in matrix groups based on the theory of Lie groups and Riemannian symmetric spaces. Our method relies on the Riemannian geometry of the underlying manifolds and has the advantage of encoding high-order statistics from 3D joint positions. We show that the proposed method achieves competitive results in two-person interaction recognition on three benchmarks for 3D human activity understanding.},
  archive   = {C_ICCV},
  author    = {Xuan Son Nguyen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01313},
  pages     = {13359-13369},
  title     = {GeomNet: A neural network based on riemannian geometries of SPD matrix space and cholesky space for 3D skeleton-based interaction recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Consistency-aware graph network for human interaction
understanding. <em>ICCV</em>, 13349–13358. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Compared with the progress made on human activity classification, much less success has been achieved on human interaction understanding (HIU). Apart from the latter task is much more challenging, the main cause is that recent approaches learn human interactive relations via shallow graphical models, which is inadequate to model complicated human interactions. In this paper, we propose a consistency-aware graph network, which combines the representative ability of graph network and the consistency-aware reasoning to facilitate HIU. Our network consists of three components, a backbone CNN to extract image features, a factor graph network to learn third-order interactive relations among participants, and a consistency-aware reasoning module to enforce labeling and grouping consistencies. Our key observation is that the consistency-aware-reasoning bias for HIU can be embedded into an energy, minimizing which delivers consistent predictions. An efficient mean-field inference algorithm is proposed, such that all modules of our network could be trained jointly in an end-to-end manner. Experimental results show that our approach achieves leading performance on three benchmarks. Code is available at https://git.io/CAGNet.},
  archive   = {C_ICCV},
  author    = {Zhenhua Wang and Jiajun Meng and Dongyan Guo and Jianhua Zhang and Javen Qinfeng Shi and Shengyong Chen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01312},
  pages     = {13349-13358},
  title     = {Consistency-aware graph network for human interaction understanding},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Channel-wise topology refinement graph convolution for
skeleton-based action recognition. <em>ICCV</em>, 13339–13348. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph convolutional networks (GCNs) have been widely used and achieved remarkable results in skeleton-based action recognition. In GCNs, graph topology dominates feature aggregation and therefore is the key to extracting representative features. In this work, we propose a novel Channel-wise Topology Refinement Graph Convolution (CTR-GC) to dynamically learn different topologies and effectively aggregate joint features in different channels for skeleton-based action recognition. The proposed CTR-GC models channel-wise topologies through learning a shared topology as a generic prior for all channels and refining it with channel-specific correlations for each channel. Our refinement method introduces few extra parameters and significantly reduces the difficulty of modeling channel-wise topologies. Furthermore, via reformulating graph convolutions into a unified form, we find that CTR-GC relaxes strict constraints of graph convolutions, leading to stronger representation capability. Combining CTR-GC with temporal modeling modules, we develop a powerful graph convolutional network named CTR-GCN which notably outperforms state-of-the-art methods on the NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets. 1},
  archive   = {C_ICCV},
  author    = {Yuxin Chen and Ziqi Zhang and Chunfeng Yuan and Bing Li and Ying Deng and Weiming Hu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01311},
  pages     = {13339-13348},
  title     = {Channel-wise topology refinement graph convolution for skeleton-based action recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Evidential deep learning for open set action recognition.
<em>ICCV</em>, 13329–13338. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In a real-world scenario, human actions are typically out of the distribution from training data, which requires a model to both recognize the known actions and reject the unknown. Different from image data, video actions are more challenging to be recognized in an open-set setting due to the uncertain temporal dynamics and static bias of human actions. In this paper, we propose a Deep Evidential Action Recognition (DEAR) method to recognize actions in an open testing set. Specifically, we formulate the action recognition problem from the evidential deep learning (EDL) perspective and propose a novel model calibration method to regularize the EDL training. Besides, to mitigate the static bias of video representation, we propose a plug-and-play module to debias the learned representation through contrastive learning. Experimental results show that our DEAR method achieves consistent performance gain on multiple mainstream action recognition models and benchmarks. Code and pre-trained models are available at https://www.rit.edu/actionlab/dear.},
  archive   = {C_ICCV},
  author    = {Wentao Bao and Qi Yu and Yu Kong},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01310},
  pages     = {13329-13338},
  title     = {Evidential deep learning for open set action recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Learn to match: Automatic matching network design for
visual tracking. <em>ICCV</em>, 13319–13328. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Siamese tracking has achieved groundbreaking performance in recent years, where the essence is the efficient matching operator cross-correlation and its variants. Besides the remarkable success, it is important to note that the heuristic matching network design relies heavily on expert experience. Moreover, we experimentally find that one sole matching operator is difficult to guarantee stable tracking in all challenging environments. Thus, in this work, we introduce six novel matching operators from the perspective of feature fusion instead of explicit similarity learning, namely Concatenation, Pointwise-Addition, Pairwise-Relation, FiLM, Simple-Transformer and Transductive-Guidance, to explore more feasibility on matching operator selection. The analyses reveal these operators’ selective adaptability on different environment degradation types, which inspires us to combine them to explore complementary features. To this end, we propose binary channel manipulation (BCM) to search for the optimal combination of these operators. BCM determines to retrain or discard one operator by learning its contribution to other tracking steps. By inserting the learned matching networks to a strong baseline tracker Ocean [47], our model achieves favorable gains by 67.2 → 71.4, 52.6 → 58.3, 70.3 → 76.0 success on OTB100, LaSOT, and TrackingNet, respectively. Notably, Our tracker, dubbed AutoMatch, uses less than half of training data/time than the baseline tracker, and runs at 50 FPS using PyTorch. Code and model are released at https://github.com/JudasDie/SOTS.},
  archive   = {C_ICCV},
  author    = {Zhipeng Zhang and Yihao Liu and Xiao Wang and Bing Li and Weiming Hu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01309},
  pages     = {13319-13328},
  title     = {Learn to match: Automatic matching network design for visual tracking},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-supervised 3D skeleton action representation learning
with motion consistency and continuity. <em>ICCV</em>, 13308–13318. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, self-supervised learning (SSL) has been proved very effective and it can help boost the performance in learning representations from unlabeled data in the image domain. Yet, very little is explored about its usefulness in 3D skeleton-based action recognition understanding. Directly applying existing SSL techniques for 3D skeleton learning, however, suffers from trivial solutions and imprecise representations. To tackle these drawbacks, we consider perceiving the consistency and continuity of motion at different playback speeds are two critical issues. To this end, we propose a novel SSL method to learn the 3D skeleton representation in an efficacious way. Specifically, by constructing a positive clip (speed-changed) and a negative clip (motion-broken) of the sampled action sequence, we encourage the positive pairs closer while pushing the negative pairs to force the network to learn the intrinsic dynamic motion consistency information. Moreover, to enhance the learning features, skeleton interpolation is further exploited to model the continuity of human skeleton data. To validate the effectiveness of the proposed method, extensive experiments are conducted on Kinetics, NTU60, NTU120, and PKUMMD datasets with several alternative network architectures. Experimental evaluations demonstrate the superiority of our approach and through which, we can gain significant performance improvement without using extra labeled data.},
  archive   = {C_ICCV},
  author    = {Yukun Su and Guosheng Lin and Qingyao Wu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01308},
  pages     = {13308-13318},
  title     = {Self-supervised 3D skeleton action representation learning with motion consistency and continuity},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatially conditioned graphs for detecting human–object
interactions. <em>ICCV</em>, 13299–13307. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the problem of detecting human–object interactions in images using graphical neural networks. Unlike conventional methods, where nodes send scaled but otherwise identical messages to each of their neighbours, we propose to condition messages between pairs of nodes on their spatial relationships, resulting in different messages going to neighbours of the same node. To this end, we explore various ways of applying spatial conditioning under a multi-branch structure. Through extensive experimentation we demonstrate the advantages of spatial conditioning for the computation of the adjacency structure, messages and the refined graph features. In particular, we empirically show that as the quality of the bounding boxes increases, their coarse appearance features contribute relatively less to the disambiguation of interactions compared to the spatial information. Our method achieves an mAP of 31.33\% on HICO-DET and 54.2\% on V-COCO, significantly outperforming state-of-the-art on fine-tuned detections.},
  archive   = {C_ICCV},
  author    = {Frederic Z. Zhang and Dylan Campbell and Stephen Gould},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01307},
  pages     = {13299-13307},
  title     = {Spatially conditioned graphs for detecting Human–Object interactions},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generating smooth pose sequences for diverse human motion
prediction. <em>ICCV</em>, 13289–13298. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent progress in stochastic motion prediction, i.e., predicting multiple possible future human motions given a single past pose sequence, has led to producing truly diverse future motions and even providing control over the motion of some body parts. However, to achieve this, the state-of-the-art method requires learning several mappings for diversity and a dedicated model for controllable motion prediction. In this paper, we introduce a unified deep generative network for both diverse and controllable motion prediction. To this end, we leverage the intuition that realistic human motions consist of smooth sequences of valid poses, and that, given limited data, learning a pose prior is much more tractable than a motion one. We therefore design a generator that predicts the motion of different body parts sequentially, and introduce a normalizing flow based pose prior, together with a joint angle loss, to achieve motion realism. Our experiments on two standard benchmark datasets, Human3.6M and HumanEva-I, demonstrate that our approach outperforms the state-of-the-art baselines in terms of both sample diversity and accuracy. The code is available at https://github.com/wei-mao-2019/gsps},
  archive   = {C_ICCV},
  author    = {Wei Mao and Miaomiao Liu and Mathieu Salzmann},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01306},
  pages     = {13289-13298},
  title     = {Generating smooth pose sequences for diverse human motion prediction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Motion prediction using trajectory cues. <em>ICCV</em>,
13279–13288. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Predicting human motion from a historical pose sequence is at the core of many applications in computer vision. Current state-of-the-art methods concentrate on learning motion contexts in the pose space, however, the high dimensionality and complex nature of human pose invoke inherent difficulties in extracting such contexts. In this paper, we instead advocate to model motion contexts in the joint trajectory space, as the trajectory of a joint is smooth, vectorial, and gives sufficient information to the model. Moreover, most existing methods consider only the dependencies between skeletal connected joints, disregarding prior knowledge and the hidden connections between geometrically separated joints. Motivated by this, we present a semi-constrained graph to explicitly encode skeletal connections and prior knowledge, while adaptively learn implicit dependencies between joints.We also explore the applications of our approach to a range of objects including human, fish, and mouse. Surprisingly, our method sets the new state-of-the-art performance on 4 different benchmark datasets, a remarkable highlight is that it achieves a 19.1\% accuracy improvement over current state-of-the-art in average. To facilitate future research, we have released our code at https://github.com/Pose-Group/MPT.},
  archive   = {C_ICCV},
  author    = {Zhenguang Liu and Pengxiang Su and Shuang Wu and Xuanjing Shen and Haipeng Chen and Yanbin Hao and Meng Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01305},
  pages     = {13279-13288},
  title     = {Motion prediction using trajectory cues},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-supervised 3D face reconstruction via conditional
estimation. <em>ICCV</em>, 13269–13278. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a conditional estimation (CEST) framework to learn 3D facial parameters from 2D single-view images by self-supervised training from videos. CEST is based on the process of analysis by synthesis, where the 3D facial parameters (shape, reflectance, viewpoint, and illumination) are estimated from the face image, and then recombined to reconstruct the 2D face image. In order to learn semantically meaningful 3D facial parameters without explicit access to their labels, CEST couples the estimation of different 3D facial parameters by taking their statistical dependency into account. Specifically, the estimation of any 3D facial parameter is not only conditioned on the given image, but also on the facial parameters that have already been derived. Moreover, the reflectance symmetry and consistency among the video frames are adopted to improve the disentanglement of facial parameters. Together with a novel strategy for incorporating the reflectance symmetry and consistency, CEST can be efficiently trained with in-the-wild video clips. Both qualitative and quantitative experiments demonstrate the effectiveness of CEST.},
  archive   = {C_ICCV},
  author    = {Yandong Wen and Weiyang Liu and Bhiksha Raj and Rita Singh},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01304},
  pages     = {13269-13278},
  title     = {Self-supervised 3D face reconstruction via conditional estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Likelihood-based diverse sampling for trajectory
forecasting. <em>ICCV</em>, 13259–13268. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Forecasting complex vehicle and pedestrian multi-modal distributions requires powerful probabilistic approaches. Normalizing flows (NF) have recently emerged as an attractive tool to model such distributions. However, a key drawback is that independent samples drawn from a flow model often do not adequately capture all the modes in the underlying distribution. We propose Likelihood-Based Diverse Sampling (LDS), a method for improving the quality and the diversity of trajectory samples from a pre-trained flow model. Rather than producing individual samples, LDS produces a set of trajectories in one shot. Given a pre-trained forecasting flow model, we train LDS using gradients from the model, to optimize an objective function that rewards high likelihood for individual trajectories in the predicted set, together with high spatial separation among trajectories. LDS outperforms state-of-art post-hoc neural diverse forecasting methods for various pre-trained flow models as well as conditional variational autoencoder (CVAE) models. Crucially, it can also be used for transductive trajectory forecasting, where the diverse forecasts are trained on-the-fly on unlabeled test examples. LDS is easy to implement, and we show that it offers a simple plug-in improvement over baselines on two challenging benchmarks. Code is at: https://github.com/JasonMa2016/LDS},
  archive   = {C_ICCV},
  author    = {Yecheng Jason Ma and Jeevana Priya Inala and Dinesh Jayaraman and Osbert Bastani},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01303},
  pages     = {13259-13268},
  title     = {Likelihood-based diverse sampling for trajectory forecasting},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Provably approximated point cloud registration.
<em>ICCV</em>, 13249–13258. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The goal of the alignment problem is to align a (given) point cloud P = {p 1 , ⋯,p n } to another (observed) point cloud Q = {q 1 , ⋯,q n }. That is, to compute a rotation matrix R ∈ ℝ 3×3 and a translation vector t ∈ ℝ 3 that minimize the sum of paired distances between every transformed point Rp i − t, to its corresponding point q i , over every i ∈ {1, ⋯,n}. A harder version is the registration problem, where the correspondence is unknown, and the minimum is also over all possible correspondence functions from P to Q. Algorithms such as the Iterative Closest Point (ICP) and its variants were suggested for these problems, but none yield a provable non-trivial approximation for the global optimum.We prove that there always exists a &quot;witness&quot; set of 3 pairs in P × Q that, via novel alignment algorithm, defines a constant factor approximation (in the worst case) to this global optimum. We then provide algorithms that recover this witness set and yield the first provable constant factor approximation for the: (i) alignment problem in O(n) expected time, and (ii) registration problem in polynomial time. Such small witness sets exist for many variants including points in d-dimensional space, outlier-resistant cost functions, and different correspondence types.Extensive experimental results on real and synthetic datasets show that, in practice, our approximation constants are close to 1 and our error is up to x10 times smaller than state-of-the-art algorithms.},
  archive   = {C_ICCV},
  author    = {Ibrahim Jubran and Alaa Maalouf and Ron Kimmel and Dan Feldman},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01302},
  pages     = {13249-13258},
  title     = {Provably approximated point cloud registration},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Square root marginalization for sliding-window bundle
adjustment. <em>ICCV</em>, 13240–13248. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we propose a novel square root sliding-window bundle adjustment suitable for real-time odometry applications. The square root formulation pervades three major aspects of our optimization-based sliding-window estimator: for bundle adjustment we eliminate landmark variables with nullspace projection; to store the marginalization prior we employ a matrix square root of the Hessian; and when marginalizing old poses we avoid forming normal equations and update the square root prior directly with a specialized QR decomposition. We show that the proposed square root marginalization is algebraically equivalent to the conventional use of Schur complement (SC) on the Hessian. Moreover, it elegantly deals with rank-deficient Jacobians producing a prior equivalent to SC with Moore–Penrose inverse. Our evaluation of visual and visual-inertial odometry on real-world datasets demonstrates that the proposed estimator is 36\% faster than the baseline. It furthermore shows that in single precision, conventional Hessian-based marginalization leads to numeric failures and reduced accuracy. We analyse numeric properties of the marginalization prior to explain why our square root form does not suffer from the same effect and therefore entails superior performance.},
  archive   = {C_ICCV},
  author    = {Nikolaus Demmel and David Schubert and Christiane Sommer and Daniel Cremers and Vladyslav Usenko},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01301},
  pages     = {13240-13248},
  title     = {Square root marginalization for sliding-window bundle adjustment},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Three steps to multimodal trajectory prediction: Modality
clustering, classification and synthesis. <em>ICCV</em>, 13230–13239.
(<a href="https://doi.org/10.1109/ICCV48922.2021.01300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multimodal prediction results are essential for trajectory prediction task as there is no single correct answer for the future. Previous frameworks can be divided into three categories: regression, generation and classification frameworks. However, these frameworks have weaknesses in different aspects so that they cannot model the multimodal prediction task comprehensively. In this paper, we present a novel insight along with a brand-new prediction framework by formulating multimodal prediction into three steps: modality clustering, classification and synthesis, and address the shortcomings of earlier frameworks. Exhaustive experiments on popular benchmarks have demonstrated that our proposed method surpasses state-of-the-art works even without introducing social and map information. Specifically, we achieve 19.2\% and 20.8\% improvement on ADE and FDE respectively on ETH/UCY dataset.},
  archive   = {C_ICCV},
  author    = {Jianhua Sun and Yuxuan Li and Hao-Shu Fang and Cewu Lu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01300},
  pages     = {13230-13239},
  title     = {Three steps to multimodal trajectory prediction: Modality clustering, classification and synthesis},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). M3D-VTON: A monocular-to-3D virtual try-on network.
<em>ICCV</em>, 13219–13229. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Virtual 3D try-on can provide an intuitive and realistic view for online shopping and has a huge potential commercial value. However, existing 3D virtual try-on methods mainly rely on annotated 3D human shapes and garment templates, which hinders their applications in practical scenarios. 2D virtual try-on approaches provide a faster alternative to manipulate clothed humans, but lack the rich and realistic 3D representation. In this paper, we propose a novel Monocular-to-3D Virtual Try-On Network (M3D-VTON) that builds on the merits of both 2D and 3D approaches. By integrating 2D information efficiently and learning a mapping that lifts the 2D representation to 3D, we make the first attempt to reconstruct a 3D try-on mesh only taking the target clothing and a person image as inputs. The proposed M3D-VTON includes three modules: 1) The Monocular Prediction Module (MPM) that estimates an initial full-body depth map and accomplishes 2D clothes-person alignment through a novel two-stage warping procedure; 2) The Depth Refinement Module (DRM) that refines the initial body depth to produce more detailed pleat and face characteristics; 3) The Texture Fusion Module (TFM) that fuses the warped clothing with the non-target body part to refine the results. We also construct a high-quality synthesized Monocular-to-3D virtual try-on dataset, in which each person image is associated with a front and a back depth map. Extensive experiments demonstrate that the proposed M3D-VTON can manipulate and reconstruct the 3D human body wearing the given clothing with compelling details and is more efficient than other 3D approaches. 1},
  archive   = {C_ICCV},
  author    = {Fuwei Zhao and Zhenyu Xie and Michael Kampffmeyer and Haoye Dong and Songfang Han and Tianxiang Zheng and Tao Zhang and Xiaodan Liang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01299},
  pages     = {13219-13229},
  title     = {M3D-VTON: A monocular-to-3D virtual try-on network},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PCAM: Product of cross-attention matrices for rigid
registration of point clouds. <em>ICCV</em>, 13209–13218. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Rigid registration of point clouds with partial overlaps is a longstanding problem usually solved in two steps: (a) finding correspondences between the point clouds; (b) filtering these correspondences to keep only the most reliable ones to estimate the transformation. Recently, several deep nets have been proposed to solve these steps jointly. We built upon these works and propose PCAM: a neural network whose key element is a pointwise product of crossattention matrices that permits to mix both low-level geometric and high-level contextual information to find point correspondences. These cross-attention matrices also permits the exchange of context information between the point clouds, at each layer, allowing the network construct better matching features within the overlapping regions. The experiments show that PCAM achieves state-of-the-art results among methods which, like us, solve steps (a) and (b) jointly via deepnets.},
  archive   = {C_ICCV},
  author    = {Anh-Quan Cao and Gilles Puy and Alexandre Boulch and Renaud Marlet},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01298},
  pages     = {13209-13218},
  title     = {PCAM: Product of cross-attention matrices for rigid registration of point clouds},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A general recurrent tracking framework without real data.
<em>ICCV</em>, 13199–13208. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent progress in multi-object tracking (MOT) has shown great significance of a robust scoring mechanism for potential tracks. However, the lack of available data in MOT makes it difficult to learn a general scoring mechanism. Multiple cues including appearance, motion and etc., are limitedly utilized in current manual scoring functions. In this paper, we propose a Multiple Nodes Tracking (MNT) framework that adapts to most trackers. Based on this framework, a Recurrent Tracking Unit (RTU) is designed to score potential tracks through long-term information. In addition, we present a method of generating simulated tracking data without real data to overcome the defect of limited available data in MOT. The experiments demonstrate that our simulated tracking data is effective for training RTU and achieves state-of-the-art performance on both MOT17 and MOT16 benchmarks. Meanwhile, RTU can be flexibly plugged into classic trackers such as DeepSORT and MHT, and makes remarkable improvements as well.},
  archive   = {C_ICCV},
  author    = {Shuai Wang and Hao Sheng and Yang Zhang and Yubin Wu and Zhang Xiong},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01297},
  pages     = {13199-13208},
  title     = {A general recurrent tracking framework without real data},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CAPTRA: CAtegory-level pose tracking for rigid and
articulated objects from point clouds. <em>ICCV</em>, 13189–13198. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we tackle the problem of category-level online pose tracking of objects from point cloud sequences. For the first time, we propose a unified framework that can handle 9DoF pose tracking for novel rigid object instances as well as per-part pose tracking for articulated objects from known categories. Here the 9DoF pose, comprising 6D pose and 3D size, is equivalent to a 3D amodal bounding box representation with free 6D pose. Given the depth point cloud at the current frame and the estimated pose from the last frame, our novel end-to-end pipeline learns to accurately update the pose. Our pipeline is composed of three modules: 1) a pose canonicalization module that normalizes the pose of the input depth point cloud; 2) RotationNet, a module that directly regresses small interframe delta rotations; and 3) CoordinateNet, a module that predicts the normalized coordinates and segmentation, enabling analytical computation of the 3D size and translation. Leveraging the small pose regime in the pose-canonicalized point clouds, our method integrates the best of both worlds by combining dense coordinate prediction and direct rotation regression, thus yielding an end-to-end differentiable pipeline optimized for 9DoF pose accuracy (without using non-differentiable RANSAC). Our extensive experiments demonstrate that our method achieves new state-of-the-art performance on category-level rigid object pose (NOCSREAL275 [29]) and articulated object pose benchmarks (SAPIEN [34], BMVC [18]) at the fastest FPS ∼ 12.},
  archive   = {C_ICCV},
  author    = {Yijia Weng and He Wang and Qiang Zhou and Yuzhe Qin and Yueqi Duan and Qingnan Fan and Baoquan Chen and Hao Su and Leonidas J. Guibas},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01296},
  pages     = {13189-13198},
  title     = {CAPTRA: CAtegory-level pose tracking for rigid and articulated objects from point clouds},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Box-aware feature enhancement for single object tracking on
point clouds. <em>ICCV</em>, 13179–13188. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current 3D single object tracking approaches track the target based on a feature comparison between the target template and the search area. However, due to the common occlusion in LiDAR scans, it is non-trivial to conduct accurate feature comparisons on severe sparse and incomplete shapes. In this work, we exploit the ground truth bounding box given in the first frame as a strong cue to enhance the feature description of the target object, enabling a more accurate feature comparison in a simple yet effective way. In particular, we first propose the BoxCloud, an informative and robust representation, to depict an object using the point-to-box relation. We further design an efficient box-aware feature fusion module, which leverages the aforementioned BoxCloud for reliable feature matching and embedding. Integrating the proposed general components into an existing model P2B [27], we construct a superior box-aware tracker (BAT) 1 . Experiments confirm that our proposed BAT outperforms the previous state-of-the-art by a large margin on both KITTI and NuScenes benchmarks, achieving a 12.8\% improvement in terms of precision while running ∼20\% faster.},
  archive   = {C_ICCV},
  author    = {Chaoda Zheng and Xu Yan and Jiantao Gao and Weibing Zhao and Wei Zhang and Zhen Li and Shuguang Cui},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01295},
  pages     = {13179-13188},
  title     = {Box-aware feature enhancement for single object tracking on point clouds},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Voxel-based network for shape completion by leveraging edge
generation. <em>ICCV</em>, 13169–13178. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep learning technique has yielded significant improvements in point cloud completion with the aim of completing missing object shapes from partial inputs. However, most existing methods fail to recover realistic structures due to over-smoothing of fine-grained details. In this paper, we develop a voxel-based network for point cloud completion by leveraging edge generation (VE-PCN). We first embed point clouds into regular voxel grids, and then generate complete objects with the help of the hallucinated shape edges. This decoupled architecture together with a multi-scale grid feature learning is able to generate more realistic on-surface details. We evaluate our model on the publicly available completion datasets and show that it outperforms existing state-of-the-art approaches quantitatively and qualitatively. Our source code is available at https://github.com/xiaogangw/VE-PCN.},
  archive   = {C_ICCV},
  author    = {Xiaogang Wang and Marcelo H Ang and Gim Hee Lee},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01294},
  pages     = {13169-13178},
  title     = {Voxel-based network for shape completion by leveraging edge generation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MEDIRL: Predicting the visual attention of drivers via
maximum entropy deep inverse reinforcement learning. <em>ICCV</em>,
13158–13168. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inspired by human visual attention, we propose a novel inverse reinforcement learning formulation using Maximum Entropy Deep Inverse Reinforcement Learning (MEDIRL) for predicting the visual attention of drivers in accident-prone situations. MEDIRL predicts fixation locations that lead to maximal rewards by learning a task-sensitive reward function from eye fixation patterns recorded from attentive drivers. Additionally, we introduce EyeCar, a new driver attention dataset in accident-prone situations. We conduct comprehensive experiments to evaluate our proposed model on three common benchmarks: (DR(eye)VE, BDD-A, DADA-2000), and our EyeCar dataset. Results indicate that MEDIRL outperforms existing models for predicting attention and achieves state-of-the-art performance. We present extensive ablation studies to provide more insights into different features of our proposed model. 1},
  archive   = {C_ICCV},
  author    = {Sonia Baee and Erfan Pakdamanian and Inki Kim and Lu Feng and Vicente Ordonez and Laura Barnes},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01293},
  pages     = {13158-13168},
  title     = {MEDIRL: Predicting the visual attention of drivers via maximum entropy deep inverse reinforcement learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unlimited neighborhood interaction for heterogeneous
trajectory prediction. <em>ICCV</em>, 13148–13157. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Understanding complex social interactions among agents is a key challenge for trajectory prediction. Most existing methods consider the interactions between pairwise traffic agents or in a local area, while the nature of interactions is unlimited, involving an uncertain number of agents and non-local areas simultaneously. Besides, they treat heterogeneous traffic agents the same, namely those among agents of different categories, while neglecting people’s diverse reaction patterns toward traffic agents in different categories. To address these problems, we propose a simple yet effective Unlimited Neighborhood Interaction Network (UNIN), which predicts trajectories of heterogeneous agents in multiple categories. Specifically, the proposed unlimited neighborhood interaction module generates the fused-features of all agents involved in an interaction simultaneously, which is adaptive to any number of agents and any range of interaction area. Meanwhile, a hierarchical graph attention module is proposed to obtain category-to-category interaction and agent-to-agent interaction. Finally, parameters of a Gaussian Mixture Model are estimated for generating the future trajectories. Extensive experimental results on benchmark datasets demonstrate a significant performance improvement of our method over the state-of-the-art methods.},
  archive   = {C_ICCV},
  author    = {Fang Zheng and Le Wang and Sanping Zhou and Wei Tang and Zhenxing Niu and Nanning Zheng and Gang Hua},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01292},
  pages     = {13148-13157},
  title     = {Unlimited neighborhood interaction for heterogeneous trajectory prediction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MG-GAN: A multi-generator model preventing
out-of-distribution samples in pedestrian trajectory prediction.
<em>ICCV</em>, 13138–13147. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pedestrian trajectory prediction is challenging due to its uncertain and multimodal nature. While generative adversarial networks can learn a distribution over future trajectories, they tend to predict out-of-distribution samples when the distribution of future trajectories is a mixture of multiple, possibly disconnected modes. To address this issue, we propose a multi-generator model for pedestrian trajectory prediction. Each generator specializes in learning a distribution over trajectories routing towards one of the primary modes in the scene, while a second network learns a categorical distribution over these generators, conditioned on the dynamics and scene input. This architecture allows us to effectively sample from specialized generators and to significantly reduce the out-of-distribution samples compared to single generator methods.},
  archive   = {C_ICCV},
  author    = {Patrick Dendorfer and Sven Elflein and Laura Leal-Taixé},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01291},
  pages     = {13138-13147},
  title     = {MG-GAN: A multi-generator model preventing out-of-distribution samples in pedestrian trajectory prediction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On exposing the challenging long tail in future prediction
of traffic actors. <em>ICCV</em>, 13127–13137. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Predicting the states of dynamic traffic actors into the future is important for autonomous systems to operate safely and efficiently. Remarkably, the most critical scenarios are much less frequent and more complex than the uncritical ones. Therefore, uncritical cases dominate the prediction. In this paper, we address specifically the challenging scenarios at the long tail of the dataset distribution. Our analysis shows that the common losses tend to place challenging cases sub-optimally in the embedding space. As a consequence, we propose to supplement the usual loss with a loss that places challenging cases closer to each other This triggers sharing information among challenging cases and learning specific predictive features. We show on four public datasets that this leads to improved performance on the challenging scenarios while the overall performance stays stable. The approach is agnostic w.r.t. the used network architecture, input modality or viewpoint, and can be integrated into existing solutions easily. Code is available at github.},
  archive   = {C_ICCV},
  author    = {Osama Makansi and Özgün Çiçek and Yassine Marrakchi and Thomas Brox},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01290},
  pages     = {13127-13137},
  title     = {On exposing the challenging long tail in future prediction of traffic actors},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating and exploiting the aleatoric uncertainty in
surface normal estimation. <em>ICCV</em>, 13117–13126. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Surface normal estimation from a single image is an important task in 3D scene understanding. In this paper, we address two limitations shared by the existing methods: the inability to estimate the aleatoric uncertainty and lack of detail in the prediction. The proposed network estimates the per-pixel surface normal probability distribution. We introduce a new parameterization for the distribution, such that its negative log-likelihood is the angular loss with learned attenuation. The expected value of the angular error is then used as a measure of the aleatoric uncertainty. We also present a novel decoder framework where pixel-wise multi-layer perceptrons are trained on a subset of pixels sampled based on the estimated uncertainty. The proposed uncertainty-guided sampling prevents the bias in training towards large planar surfaces and improves the quality of prediction, especially near object boundaries and on small structures. Experimental results show that the proposed method outperforms the state-of-the-art in ScanNet [4] and NYUv2 [33], and that the estimated uncertainty correlates well with the prediction error. Code is available at https://github.com/baegwangbin/surface_normal_uncertainty.},
  archive   = {C_ICCV},
  author    = {Gwangbin Bae and Ignas Budvytis and Roberto Cipolla},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01289},
  pages     = {13117-13126},
  title     = {Estimating and exploiting the aleatoric uncertainty in surface normal estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SLIM: Self-supervised LiDAR scene flow and motion
segmentation. <em>ICCV</em>, 13106–13116. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, several frameworks for self-supervised learning of 3D scene flow on point clouds have emerged. Scene flow inherently separates every scene into multiple moving agents and a large class of points following a single rigid sensor motion. However, existing methods do not leverage this property of the data in their self-supervised training routines which could improve and stabilize flow predictions. Based on the discrepancy between a robust rigid egomotion estimate and a raw flow prediction, we generate a self-supervised motion segmentation signal. The predicted motion segmentation, in turn, is used by our algorithm to attend to stationary points for aggregation of motion information in static parts of the scene. We learn our model end-to-end by backpropagating gradients through Kabsch’s algorithm and demonstrate that this leads to accurate egomotion which in turn improves the scene flow estimate. Using our method, we show state-of-the-art results across multiple scene flow metrics for different real-world datasets, showcasing the robustness and generalizability of this approach. We further analyze the performance gain when performing joint motion segmentation and scene flow in an ablation study. We also present a novel network architecture for 3D LiDAR scene flow which is capable of handling an order of magnitude more points during training than previously possible.},
  archive   = {C_ICCV},
  author    = {Stefan Andreas Baur and David Josef Emmerichs and Frank Moosmann and Peter Pinggera and Björn Ommer and Andreas Geiger},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01288},
  pages     = {13106-13116},
  title     = {SLIM: Self-supervised LiDAR scene flow and motion segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Motion basis learning for unsupervised deep homography
estimation with subspace projection. <em>ICCV</em>, 13097–13105. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce a new framework for unsupervised deep homography estimation. Our contributions are 3 folds. First, unlike previous methods that regress 4 offsets for a homography, we propose a homography flow representation, which can be estimated by a weighted sum of 8 pre-defined homography flow bases. Second, considering a homography contains 8 Degree-of-Freedoms (DOFs) that is much less than the rank of the network features, we propose a Low Rank Representation (LRR) block that reduces the feature rank, so that features corresponding to the dominant motions are retained while others are rejected. Last, we propose a Feature Identity Loss (FIL) to enforce the learned image feature warp-equivariant, meaning that the result should be identical if the order of warp operation and feature extraction is swapped. With this constraint, the unsupervised optimization is achieved more effectively and more stable features are learned. Extensive experiments are conducted to demonstrate the effectiveness of all the newly proposed components, and results show that our approach outperforms the state-of-the-art on the homography benchmark datasets both qualitatively and quantitatively. Code is available at https://github.com/megvii-research/BasesHomo},
  archive   = {C_ICCV},
  author    = {Nianjin Ye and Chuan Wang and Haoqiang Fan and Shuaicheng Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01287},
  pages     = {13097-13105},
  title     = {Motion basis learning for unsupervised deep homography estimation with subspace projection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient and differentiable shadow computation for inverse
problems. <em>ICCV</em>, 13087–13096. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Differentiable rendering has received increasing interest for image-based inverse problems. It can benefit traditional optimization-based solutions to inverse problems, but also allows for self-supervision of learning-based approaches for which training data with ground truth annotation is hard to obtain. However, existing differentiable renderers either do not model visibility of the light sources from the different points in the scene, responsible for shadows in the images, or are very slow which makes it difficult to train deep architectures over thousands of iterations. To this end, we propose an accurate yet efficient approach for differentiable visibility and soft shadow computation. Our approach is based on the spherical harmonics approximations of the scene illumination and visibility, where the occluding surface is approximated with spheres. This allows for a significantly more efficient shadow computation compared to methods based on ray tracing. As our formulation is differentiable, it can be used to solve inverse problems such as texture, illumination, rigid pose, and geometric deformation recovery from images using analysis-by-synthesis optimization.},
  archive   = {C_ICCV},
  author    = {Linjie Lyu and Marc Habermann and Lingjie Liu and Mallikarjun B R and Ayush Tewari and Christian Theobalt},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01286},
  pages     = {13087-13096},
  title     = {Efficient and differentiable shadow computation for inverse problems},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Geometric granularity aware pixel-to-mesh. <em>ICCV</em>,
13077–13086. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pixel-to-mesh has wide applications, especially in virtual or augmented reality, animation and game industry. However, existing mesh reconstruction models perform unsatisfactorily in local geometry details due to ignoring mesh topology information during learning. Besides, most methods are constrained by the initial template, which cannot reconstruct meshes of various genus. In this work, we propose a geometric granularity-aware pixel-to-mesh framework with a fidelity-selection-and-guarantee strategy, which explicitly addresses both challenges. First, a geometry structure extractor is proposed for detecting local high structured parts and capturing local spatial feature. Second, we apply it to facilitate pixel-to-mesh mapping and resolve coarse details problem caused by the neglect of structural information in previous practices. Finally, a mesh edit module is proposed to encourage non-zero genus topology to emergence by fine-grained topology modification and a patching algorithm is introduced to repair the non-closed boundaries. Extensive experimental results, both quantitatively and visually have demonstrated the high reconstruction fidelity achieved by the proposed framework.},
  archive   = {C_ICCV},
  author    = {Yue Shi and Bingbing Ni and Jinxian Liu and Dingyi Rong and Ye Qian and Wenjun Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01285},
  pages     = {13077-13086},
  title     = {Geometric granularity aware pixel-to-mesh},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Multiresolution deep implicit functions for 3D shape
representation. <em>ICCV</em>, 13067–13076. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce Multiresolution Deep Implicit Functions (MDIF), a hierarchical representation that can recover fine geometry detail, while being able to perform global operations such as shape completion. Our model represents a complex 3D shape with a hierarchy of latent grids, which can be decoded into different levels of detail and also achieve better accuracy. For shape completion, we propose latent grid dropout to simulate partial data in the latent space and therefore defer the completing functionality to the decoder side. This along with our multires design significantly improves the shape completion quality under decoder-only latent optimization. To the best of our knowledge, MDIF is the first deep implicit function model that can at the same time (1) represent different levels of detail and allow progressive decoding; (2) support both encoder-decoder inference and decoder-only latent optimization, and fulfill multiple applications; (3) perform detailed decoder-only shape completion. Experiments demonstrate its superior performance against prior art in various 3D reconstruction tasks.},
  archive   = {C_ICCV},
  author    = {Zhang Chen and Yinda Zhang and Kyle Genova and Sean Fanello and Sofien Bouaziz and Christian Häne and Ruofei Du and Cem Keskin and Thomas Funkhouser and Danhang Tang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01284},
  pages     = {13067-13076},
  title     = {Multiresolution deep implicit functions for 3D shape representation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Motion guided attention fusion to recognize interactions
from videos. <em>ICCV</em>, 13056–13066. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a dual-pathway approach for recognizing fine-grained interactions from videos. We build on the success of prior dual-stream approaches, but make a distinction between the static and dynamic representations of objects and their interactions explicit by introducing separate motion and object detection pathways. Then, using our new Motion-Guided Attention Fusion module, we fuse the bottom-up features in the motion pathway with features captured from object detections to learn the temporal aspects of an action. We show that our approach can generalize across appearance effectively and recognize actions where an actor interacts with previously unseen objects. We validate our approach using the compositional action recognition task from the Something-Something-v2 dataset where we outperform existing state-of-the-art methods. We also show that our method can generalize well to real world tasks by showing state-of-the-art performance on recognizing humans assembling various IKEA furniture on the IKEA-ASM dataset.},
  archive   = {C_ICCV},
  author    = {Tae Soo Kim and Jonathan Jones and Gregory D. Hager},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01283},
  pages     = {13056-13066},
  title     = {Motion guided attention fusion to recognize interactions from videos},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning self-similarity in space and time as generalized
motion for video action recognition. <em>ICCV</em>, 13045–13055. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Spatio-temporal convolution often fails to learn motion dynamics in videos and thus an effective motion representation is required for video understanding in the wild. In this paper, we propose a rich and robust motion representation based on spatio-temporal self-similarity (STSS). Given a sequence of frames, STSS represents each local region as similarities to its neighbors in space and time. By converting appearance features into relational values, it enables the learner to better recognize structural patterns in space and time. We leverage the whole volume of STSS and let our model learn to extract an effective motion representation from it. The proposed neural block, dubbed SELFY, can be easily inserted into neural architectures and trained end-to-end without additional supervision. With a sufficient volume of the neighborhood in space and time, it effectively captures long-term interaction and fast motion in the video, leading to robust action recognition. Our experimental analysis demonstrates its superiority over previous methods for motion modeling as well as its complementarity to spatio-temporal features from direct convolution. On the standard action recognition benchmarks, Something-Something-V1 &amp; V2, Diving-48, and FineGym, the proposed method achieves the state-of-the-art results.},
  archive   = {C_ICCV},
  author    = {Heeseung Kwon and Manjin Kim and Suha Kwak and Minsu Cho},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01282},
  pages     = {13045-13055},
  title     = {Learning self-similarity in space and time as generalized motion for video action recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning an augmented RGB representation with cross-modal
knowledge distillation for action detection. <em>ICCV</em>, 13033–13044.
(<a href="https://doi.org/10.1109/ICCV48922.2021.01281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In video understanding, most cross-modal knowledge distillation (KD) methods are tailored for classification tasks, focusing on the discriminative representation of the trimmed videos. However, action detection requires not only categorizing actions, but also localizing them in untrimmed videos. Therefore, transferring knowledge pertaining to temporal relations is critical for this task which is missing in the previous cross-modal KD frameworks. To this end, we aim at learning an augmented RGB representation for action detection, taking advantage of additional modalities at training time through KD. We propose a KD frame-work consisting of two levels of distillation. On one hand, atomic-level distillation encourages the RGB student to learn the sub-representation of the actions from the teacher in a contrastive manner. On the other hand, sequence-level distillation encourages the student to learn the temporal knowledge from the teacher, which consists of transferring the Global Contextual Relations and the Action Boundary Saliency. The result is an Augmented-RGB stream that can achieve competitive performance as the two-stream network while using only RGB at inference time. Extensive experimental analysis shows that our proposed distillation frame-work is generic and outperforms other popular cross-modal distillation methods in action detection task.},
  archive   = {C_ICCV},
  author    = {Rui Dai and Srijan Das and François Bremond},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01281},
  pages     = {13033-13044},
  title     = {Learning an augmented RGB representation with cross-modal knowledge distillation for action detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Object tracking by jointly exploiting frame and event
domain. <em>ICCV</em>, 13023–13032. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inspired by the complementarity between conventional frame-based and bio-inspired event-based cameras, we propose a multi-modal based approach to fuse visual cues from the frame- and event-domain to enhance the single object tracking performance, especially in degraded conditions (e.g., scenes with high dynamic range, low light, and fast-motion objects). The proposed approach can effectively and adaptively combine meaningful information from both domains. Our approach’s effectiveness is enforced by a novel designed cross-domain attention schemes, which can effectively enhance features based on self- and cross-domain attention schemes; The adaptiveness is guarded by a specially designed weighting scheme, which can adaptively balance the contribution of the two domains. To exploit event-based visual cues in single-object tracking, we construct a large-scale frame-event-based dataset, which we subsequently employ to train a novel frame-event fusion based model. Extensive experiments show that the proposed approach outperforms state-of-the-art frame-based tracking methods by at least 10.4\% and 11.9\% in terms of representative success rate and precision rate, respectively. Besides, the effectiveness of each key component of our approach is evidenced by our thorough ablation study.},
  archive   = {C_ICCV},
  author    = {Jiqing Zhang and Xin Yang and Yingkai Fu and Xiaopeng Wei and Baocai Yin and Bo Dong},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01280},
  pages     = {13023-13032},
  title     = {Object tracking by jointly exploiting frame and event domain},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Encoder-decoder with multi-level attention for 3D human
shape and pose estimation. <em>ICCV</em>, 13013–13022. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D human shape and pose estimation is the essential task for human motion analysis, which is widely used in many 3D applications. However, existing methods cannot simultaneously capture the relations at multiple levels, including spatial-temporal level and human joint level. Therefore they fail to make accurate predictions in some hard scenarios when there is cluttered background, occlusion, or extreme pose. To this end, we propose Multi-level Attention Encoder-Decoder Network (MAED), including a Spatial-Temporal Encoder (STE) and a Kinematic Topology Decoder (KTD) to model multi-level attentions in a unified framework. STE consists of a series of cascaded blocks based on Multi-Head Self-Attention, and each block uses two parallel branches to learn spatial and temporal attention respectively. Meanwhile, KTD aims at modeling the joint level attention. It regards pose estimation as a top-down hierarchical process similar to SMPL kinematic tree. With the training set of 3DPW, MAED outperforms previous state-of-the-art methods by 6.2, 7.2, and 2.4 mm of PA-MPJPE on the three widely used benchmarks 3DPW, MPI-INF-3DHP, and Human3.6M respectively. Our code is available at https://github.com/ziniuwan/maed.},
  archive   = {C_ICCV},
  author    = {Ziniu Wan and Zhengjia Li and Maoqing Tian and Jianbo Liu and Shuai Yi and Hongsheng Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01279},
  pages     = {13013-13022},
  title     = {Encoder-decoder with multi-level attention for 3D human shape and pose estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sketch2Mesh: Reconstructing and editing 3D shapes from
sketches. <em>ICCV</em>, 13003–13012. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reconstructing 3D shape from 2D sketches has long been an open problem because the sketches only provide very sparse and ambiguous information. In this paper, we use an encoder/decoder architecture for the sketch to mesh translation. When integrated into a user interface that provides camera parameters for the sketches, this enables us to leverage its latent parametrization to represent and refine a 3D mesh so that its projections match the external contours outlined in the sketch. We will show that this approach is easy to deploy, robust to style changes, and effective. Furthermore, it can be used for shape refinement given only single pen strokes.We compare our approach to state-of-the-art methods on sketches—both hand-drawn and synthesized—and demonstrate that we outperform them.},
  archive   = {C_ICCV},
  author    = {Benoit Guillard and Edoardo Remelli and Pierre Yvernay and Pascal Fua},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01278},
  pages     = {13003-13012},
  title     = {Sketch2Mesh: Reconstructing and editing 3D shapes from sketches},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SIMstack: A generative shape and instance model for
unordered object stacks. <em>ICCV</em>, 12992–13002. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {By estimating 3D shape and instances from a single view, we can capture information about an environment quickly, without the need for comprehensive scanning and multi-view fusion. Solving this task for composite scenes (such as object stacks) is challenging: occluded areas are not only ambiguous in shape but also in instance segmentation; multiple decompositions could be valid. We observe that physics constrains decomposition as well as shape in occluded regions and hypothesise that a latent space learned from scenes built under physics simulation can serve as a prior to better predict shape and instances in occluded regions. To this end we propose SIMstack, a depth-conditioned Variational Auto-Encoder (VAE), trained on a dataset of objects stacked under physics simulation. We formulate instance segmentation as a centre voting task which allows for class-agnostic detection and doesn’t require setting the maximum number of objects in the scene. At test time, our model can generate 3D shape and instance segmentation from a single depth view, probabilistically sampling proposals for the occluded region from the learned latent space. Our method has practical applications in providing robots some of the ability humans have to make rapid intuitive inferences of partially observed scenes. We demonstrate an application for precise (non-disruptive) object grasping of unknown objects from a single depth view.},
  archive   = {C_ICCV},
  author    = {Zoe Landgraf and Raluca Scona and Tristan Laidlow and Stephen James and Stefan Leutenegger and Andrew J. Davison},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01277},
  pages     = {12992-13002},
  title     = {SIMstack: A generative shape and instance model for unordered object stacks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A-SDF: Learning disentangled signed distance functions for
articulated shape representation. <em>ICCV</em>, 12981–12991. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent work has made significant progress on using implicit functions, as a continuous representation for 3D rigid object shape reconstruction. However, much less effort has been devoted to modeling general articulated objects. Compared to rigid objects, articulated objects have higher degrees of freedom, which makes it hard to generalize to unseen shapes. To deal with the large shape variance, we introduce Articulated Signed Distance Functions (A-SDF) to represent articulated shapes with a disentangled latent space, where we have separate codes for encoding shape and articulation. With this disentangled continuous representation, we demonstrate that we can control the articulation input and animate unseen instances with unseen joint angles. Furthermore, we propose a Test-Time Adaptation inference algorithm to adjust our model during inference. We demonstrate our model generalize well to out-of-distribution and unseen data, e.g., partial point clouds and real-world depth images. Project page with code: https://jitengmu.github.io/A-SDF/.},
  archive   = {C_ICCV},
  author    = {Jiteng Mu and Weichao Qiu and Adam Kortylewski and Alan Yuille and Nuno Vasconcelos and Xiaolong Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01276},
  pages     = {12981-12991},
  title     = {A-SDF: Learning disentangled signed distance functions for articulated shape representation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Planar surface reconstruction from sparse views.
<em>ICCV</em>, 12971–12980. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The paper studies planar surface reconstruction of indoor scenes from two views with unknown camera poses. While prior approaches have successfully created object-centric reconstructions of many scenes, they fail to exploit other structures, such as planes, which are typically the dominant components of indoor scenes. In this paper, we re-construct planar surfaces from multiple views, while jointly estimating camera pose. Our experiments demonstrate that our method is able to advance the state of the art of re-construction from sparse views, on challenging scenes from Matterport3D.},
  archive   = {C_ICCV},
  author    = {Linyi Jin and Shengyi Qian and Andrew Owens and David F. Fouhey},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01275},
  pages     = {12971-12980},
  title     = {Planar surface reconstruction from sparse views},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discovering 3D parts from image collections. <em>ICCV</em>,
12961–12970. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reasoning 3D shapes from 2D images is an essential yet challenging task, especially when only single-view images are at our disposal. While an object can have a complicated shape, individual parts are usually close to geometric primitives and thus are easier to model. Furthermore, parts provide a mid-level representation that is robust to appearance variations across objects in a particular category. In this work, we tackle the problem of 3D part discovery from only 2D image collections. Instead of relying on manually annotated parts for supervision, we propose a self-supervised approach, latent part discovery (LPD). Our key insight is to learn a novel part shape prior that allows each part to fit an object shape faithfully while constrained to have simple geometry. Extensive experiments on the synthetic ShapeNet, PartNet, and real-world Pascal 3D+ datasets show that our method discovers consistent object parts and achieves favorable reconstruction accuracy compared to the existing methods with the same level of supervision. Our project page with code is at https://chhankyao.github.io/lpd/.},
  archive   = {C_ICCV},
  author    = {Chun-Han Yao and Wei-Chih Hung and Varun Jampani and Ming-Hsuan Yang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01274},
  pages     = {12961-12970},
  title     = {Discovering 3D parts from image collections},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). THUNDR: Transformer-based 3D HUmaN reconstruction with
markers. <em>ICCV</em>, 12951–12960. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present THUNDR, a transformer-based deep neural network methodology to reconstruct the 3d pose and shape of people, given monocular RGB images. Key to our methodology is an intermediate 3d marker representation, where we aim to combine the predictive power of model-free-output architectures and the regularizing, anthropometrically-preserving properties of a statistical human surface model like GHUM—a recently introduced, expressive full body statistical 3d human model, trained end-to-end. Our novel transformer-based prediction pipeline can focus on image regions relevant to the task, supports self-supervised regimes, and ensures that solutions are consistent with human anthropometry. We show state-of-the-art results on Human3.6M and 3DPW, for both the fully-supervised and the self-supervised models, for the task of inferring 3d human shape, joint positions, and global translation. Moreover, we observe very solid 3d reconstruction performance for difficult human poses collected in the wild.},
  archive   = {C_ICCV},
  author    = {Mihai Zanfir and Andrei Zanfir and Eduard Gabriel Bazavan and William T. Freeman and Rahul Sukthankar and Cristian Sminchisescu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01273},
  pages     = {12951-12960},
  title     = {THUNDR: Transformer-based 3D HUmaN reconstruction with markers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Non-rigid neural radiance fields: Reconstruction and novel
view synthesis of a dynamic scene from monocular video. <em>ICCV</em>,
12939–12950. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present Non-Rigid Neural Radiance Fields (NR-NeRF), a reconstruction and novel view synthesis approach for general non-rigid dynamic scenes. Our approach takes RGB images of a dynamic scene as input (e.g., from a monocular video recording), and creates a high-quality space-time geometry and appearance representation. We show that a single handheld consumer-grade camera is sufficient to synthesize sophisticated renderings of a dynamic scene from novel virtual camera views, e.g. a ‘bullet-time’ video effect. NR-NeRF disentangles the dynamic scene into a canonical volume and its deformation. Scene deformation is implemented as ray bending, where straight rays are deformed non-rigidly. We also propose a novel rigidity network to better constrain rigid regions of the scene, leading to more stable results. The ray bending and rigidity network are trained without explicit supervision. Our formulation enables dense correspondence estimation across views and time, and compelling video editing applications such as motion exaggeration. Our code will be open sourced.},
  archive   = {C_ICCV},
  author    = {Edgar Tretschk and Ayush Tewari and Vladislav Golyanik and Michael Zollhöfer and Christoph Lassner and Christian Theobalt},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01272},
  pages     = {12939-12950},
  title     = {Non-rigid neural radiance fields: Reconstruction and novel view synthesis of a dynamic scene from monocular video},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CodeNeRF: Disentangled neural radiance fields for object
categories. <em>ICCV</em>, 12929–12938. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {CodeNeRF is an implicit 3D neural representation that learns the variation of object shapes and textures across a category and can be trained, from a set of posed images, to synthesize novel views of unseen objects. Unlike the original NeRF, which is scene specific, CodeNeRF learns to disentangle shape and texture by learning separate embeddings. At test time, given a single unposed image of an unseen object, CodeNeRF jointly estimates camera viewpoint, and shape and appearance codes via optimization. Unseen objects can be reconstructed from a single image, and then rendered from new viewpoints or their shape and texture edited by varying the latent codes. We conduct experiments on the SRN benchmark, which show that CodeNeRF generalises well to unseen objects and achieves on-par performance with methods that require known camera pose at test time. Our results on real-world images demonstrate that CodeNeRF can bridge the sim-to-real gap. Project page: https://github.com/wayne1123/code-nerf},
  archive   = {C_ICCV},
  author    = {Wonbong Jang and Lourdes Agapito},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01271},
  pages     = {12929-12938},
  title     = {CodeNeRF: Disentangled neural radiance fields for object categories},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mesh graphormer. <em>ICCV</em>, 12919–12928. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a graph-convolution-reinforced transformer, named Mesh Graphormer, for 3D human pose and mesh reconstruction from a single image. Recently both transformers and graph convolutional neural networks (GC-NNs) have shown promising progress in human mesh re-construction. Transformer-based approaches are effective in modeling non-local interactions among 3D mesh vertices and body joints, whereas GCNNs are good at exploiting neighborhood vertex interactions based on a pre-specified mesh topology. In this paper, we study how to combine graph convolutions and self-attentions in a transformer to model both local and global interactions. Experimental results show that our proposed method, Mesh Graphormer, significantly outperforms the previous state-of-the-art methods on multiple benchmarks, including Human3.6M, 3DPW, and FreiHAND datasets. Code and pre-trained models are available at https://github.com/microsoft/MeshGraphormer.},
  archive   = {C_ICCV},
  author    = {Kevin Lin and Lijuan Wang and Zicheng Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01270},
  pages     = {12919-12928},
  title     = {Mesh graphormer},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). I2UV-HandNet: Image-to-UV prediction network for accurate
and high-fidelity 3D hand mesh modeling. <em>ICCV</em>, 12909–12918. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reconstructing a high-precision and high-fidelity 3D human hand from a color image plays a central role in replicating a realistic virtual hand in human-computer interaction and virtual reality applications. The results of current methods are lacking in accuracy and fidelity due to various hand poses and severe occlusions. In this study, we propose an I2UV-HandNet model for accurate hand pose and shape estimation as well as 3D hand super-resolution reconstruction. Specifically, we present the first UV-based 3D hand shape representation. To recover a 3D hand mesh from an RGB image, we design an AffineNet to predict a UV position map from the input in an image-to-image translation fashion. To obtain a higher fidelity shape, we exploit an additional SRNet to transform the low-resolution UV map outputted by AffineNet into a high-resolution one. For the first time, we demonstrate the characterization capability of the UV-based hand shape representation. Our experi ments show that the proposed method achieves state-of-the-art performance on several challenging benchmarks.},
  archive   = {C_ICCV},
  author    = {Ping Chen and Yujin Chen and Dong Yang and Fangyin Wu and Qin Li and Qingpei Xia and Yong Tan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01269},
  pages     = {12909-12918},
  title     = {I2UV-HandNet: Image-to-UV prediction network for accurate and high-fidelity 3D hand mesh modeling},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DeepGaze IIE: Calibrated prediction in and out-of-domain for
state-of-the-art saliency modeling. <em>ICCV</em>, 12899–12908. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Since 2014 transfer learning has become the key driver for the improvement of spatial saliency prediction—however, with stagnant progress in the last 3-5 years. We conduct a large-scale transfer learning study which tests different ImageNet backbones, always using the same read out architecture and learning protocol adopted from DeepGaze II. By replacing the VGG19 backbone of DeepGaze II with ResNet50 features we improve the performance on saliency prediction from 78\% to 85\%. However, as we continue to test better ImageNet models as backbones—such as EfficientNetB5—we observe no additional improvement on saliency prediction. By analyzing the backbones further, we find that generalization to other datasets differs substantially, with models being consistently overconfident in their fixation predictions. We show that by combining multiple backbones in a principled manner a good confidence calibration on unseen datasets can be achieved. This new model &quot;DeepGaze IIE&quot; yields a significant leap in benchmark performance in and out-of-domain with a 15 percent point improvement over DeepGaze II to 93\% on MIT1003, marking a new state of the art on the MIT/Tuebingen Saliency Benchmark in all available metrics (AUC: 88.3\%, sAUC: 79.4\%, CC: 82.4\%).},
  archive   = {C_ICCV},
  author    = {Akis Linardos and Matthias Kümmerer and Ori Press and Matthias Bethge},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01268},
  pages     = {12899-12908},
  title     = {DeepGaze IIE: Calibrated prediction in and out-of-domain for state-of-the-art saliency modeling},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Context-sensitive temporal feature learning for gait
recognition. <em>ICCV</em>, 12889–12898. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although gait recognition has drawn increasing research attention recently, it remains challenging to learn discriminative temporal representation since the silhouette differences are quite subtle in spatial domain. Inspired by the observation that humans can distinguish gaits of different subjects by adaptively focusing on temporal sequences with different time scales, we propose a context-sensitive temporal feature learning (CSTL) network in this paper, which aggregates temporal features in three scales to obtain motion representation according to the temporal contextual information. Specifically, CSTL introduces relation modeling among multi-scale features to evaluate feature importances, based on which network adaptively enhances more important scale and suppresses less important scale. Besides that, we propose a salient spatial feature learning (SSFL) module to tackle the misalignment problem caused by temporal operation, e.g., temporal convolution. SSFL recombines a frame of salient spatial features by extracting the most discriminative parts across the whole sequence. In this way, we achieve adaptive temporal learning and salient spatial mining simultaneously. Extensive experiments conducted on two datasets demonstrate the state-of-the-art performance. On CASIA-B dataset, we achieve rank-1 accuracies of 98.0\%, 95.4\% and 87.0\% under normal walking, bag-carrying and coat-wearing conditions. On OU-MVLP dataset, we achieve rank-1 accuracy of 90.2\%. The source code will be published at https://github.com/OliverHxh/CSTL.},
  archive   = {C_ICCV},
  author    = {Xiaohu Huang and Duowang Zhu and Hao Wang and Xinggang Wang and Bo Yang and Botao He and Wenyu Liu and Bin Feng},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01267},
  pages     = {12889-12898},
  title     = {Context-sensitive temporal feature learning for gait recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PIAP-DF: Pixel-interested and anti person-specific facial
action unit detection net with discrete feedback learning.
<em>ICCV</em>, 12879–12888. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Facial Action Units (AUs) are of great significance in communication. Automatic AU detection can improve the understanding of psychological conditions and emotional status. Recently, several deep learning methods have been proposed to detect AUs automatically. However, several challenges, such as poor extraction of fine-grained and robust local AUs information, model overfitting on person-specific features, as well as the limitation of datasets with wrong labels, remain to be addressed. In this paper, we propose a joint strategy called PIAP-DF to solve these problems, which involves 1) a multi-stage Pixel-Interested learning method with pixel-level attention for each AU; 2) an Anti Person-Specific method aiming to eliminate features associated with any individual as much as possible; 3) a semi-supervised learning method with Discrete Feedback, designed to effectively utilize unlabeled data and mitigate the negative impacts of wrong labels. Experimental results on the two popular AU detection datasets BP4D and DISFA prove that PIAP-DF can be the new state-of-the-art method. Compared with the current best method, PIAP-DF improves the average F1 score by 3.2\% on BP4D and by 0.5\% on DISFA. All modules of PIAP-DF can be easily removed after training to obtain a lightweight model for practical application.},
  archive   = {C_ICCV},
  author    = {Yang Tang and Wangding Zeng and Dafei Zhao and Honggang Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01266},
  pages     = {12879-12888},
  title     = {PIAP-DF: Pixel-interested and anti person-specific facial action unit detection net with discrete feedback learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical memory matching network for video object
segmentation. <em>ICCV</em>, 12869–12878. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present Hierarchical Memory Matching Network (HMMN) for semi-supervised video object segmentation. Based on a recent memory-based method [33], we propose two advanced memory read modules that enable us to perform memory reading in multiple scales while exploiting temporal smoothness. We first propose a kernel guided memory matching module that replaces the non-local dense memory read, commonly adopted in previous memory-based methods. The module imposes the temporal smoothness constraint in the memory read, leading to accurate memory retrieval. More importantly, we introduce a hierarchical memory matching scheme and propose a top-k guided memory matching module in which memory read on a fine-scale is guided by that on a coarse-scale. With the module, we perform memory read in multiple scales efficiently and leverage both high-level semantic and low-level fine-grained memory features to predict detailed object masks. Our network achieves state-of-the-art performance on the validation sets of DAVIS 2016/2017 (90.8\% and 84.7\%) and YouTube-VOS 2018/2019 (82.6\% and 82.5\%), and test-dev set of DAVIS 2017 (78.6\%). The source code and model are available online: https://github.com/Hongje/HMMN.},
  archive   = {C_ICCV},
  author    = {Hongje Seong and Seoung Wug Oh and Joon-Young Lee and Seongwon Lee and Suhyeon Lee and Euntai Kim},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01265},
  pages     = {12869-12878},
  title     = {Hierarchical memory matching network for video object segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards interpretable deep networks for monocular depth
estimation. <em>ICCV</em>, 12859–12868. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep networks for Monocular Depth Estimation (MDE) have achieved promising performance recently and it is of great importance to further understand the interpretability of these networks. Existing methods attempt to provide post-hoc explanations by investigating visual cues, which may not explore the internal representations learned by deep networks. In this paper, we find that some hidden units of the network are selective to certain ranges of depth, and thus such behavior can be served as a way to interpret the internal representations. Based on our observations, we quantify the interpretability of a deep MDE network by the depth selectivity of its hidden units. Moreover, we then propose a method to train interpretable MDE deep networks without changing their original architectures, by assigning a depth range for each unit to select. Experimental results demonstrate that our method is able to enhance the interpretability of deep MDE networks by largely improving the depth selectivity of their units, while not harming or even improving the depth estimation accuracy. We further provide comprehensive analysis to show the reliability of selective units, the applicability of our method on different layers, models, and datasets, and a demonstration on analysis of model error. Source code and models are available at https://github.com/youzunzhi/InterpretableMDE.},
  archive   = {C_ICCV},
  author    = {Zunzhi You and Yi-Hsuan Tsai and Wei-Chen Chiu and Guanbin Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01264},
  pages     = {12859-12868},
  title     = {Towards interpretable deep networks for monocular depth estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GyroFlow: Gyroscope-guided unsupervised optical flow
learning. <em>ICCV</em>, 12849–12858. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing optical flow methods are erroneous in challenging scenes, such as fog, rain, and night because the basic optical flow assumptions such as brightness and gradient constancy are broken. To address this problem, we present an unsupervised learning approach that fuses gyroscope into optical flow learning. Specifically, we first convert gyroscope readings into motion fields named gyro field. Second, we design a self-guided fusion module to fuse the background motion extracted from the gyro field with the optical flow and guide the network to focus on motion details. To the best of our knowledge, this is the first deep learning-based framework that fuses gyroscope data and image content for optical flow learning. To validate our method, we propose a new dataset that covers regular and challenging scenes. Experiments show that our method outperforms the state-of-art methods in both regular and challenging scenes. Code and dataset are available at https://github.com/megvii-research/GyroFlow.},
  archive   = {C_ICCV},
  author    = {Haipeng Li and Kunming Luo and Shuaicheng Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01263},
  pages     = {12849-12858},
  title     = {GyroFlow: Gyroscope-guided unsupervised optical flow learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). VaPiD: A rapid vanishing point detector via learned
optimizers. <em>ICCV</em>, 12839–12848. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Being able to infer 3D structures from 2D images with geometric principles, vanishing points have been a well-recognized concept in 3D vision research. It has been widely used in autonomous driving, SLAM, and AR/VR for applications including road direction estimation, camera calibration, and camera pose estimation. Existing vanishing point detection methods often need to trade off between robustness, precision, and inference speed. In this paper, we introduce VaPiD, a novel neural network-based rapid Vanishing Point Detector that achieves unprecedented efficiency with learned vanishing point optimizers. The core of our method contains two components: a vanishing point proposal network that gives a set of vanishing point proposals as coarse estimations; and a neural vanishing point optimizer that iteratively optimizes the positions of the vanishing point proposals to achieve high-precision levels. Extensive experiments on both synthetic and real-world datasets show that our method provides competitive, if not better, performance as compared to the previous state-of-the-art vanishing point detection approaches, while being significantly faster.},
  archive   = {C_ICCV},
  author    = {Shichen Liu and Yichao Zhou and Yajie Zhao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01262},
  pages     = {12839-12848},
  title     = {VaPiD: A rapid vanishing point detector via learned optimizers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive surface normal constraint for depth estimation.
<em>ICCV</em>, 12829–12838. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel method for single image depth estimation using surface normal constraints. Existing depth estimation methods either suffer from the lack of geometric constraints, or are limited to the difficulty of reliably capturing geometric context, which leads to a bottleneck of depth estimation quality. We therefore introduce a simple yet effective method, named Adaptive Surface Normal (ASN) constraint, to effectively correlate the depth estimation with geometric consistency. Our key idea is to adaptively determine the reliable local geometry from a set of randomly sampled candidates to derive surface normal constraint, for which we measure the consistency of the geometric contextual features. As a result, our method can faithfully reconstruct the 3D geometry and is robust to local shape variations, such as boundaries, sharp corners and noises. We conduct extensive evaluations and comparisons using public datasets. The experimental results demonstrate our method outperforms the state-of-the-art methods and has superior efficiency and robustness. Codes are available at: https://github.com/xxlong0/ASNDepth},
  archive   = {C_ICCV},
  author    = {Xiaoxiao Long and Cheng Lin and Lingjie Liu and Wei Li and Christian Theobalt and Ruigang Yang and Wenping Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01261},
  pages     = {12829-12838},
  title     = {Adaptive surface normal constraint for depth estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SurfaceNet: Adversarial SVBRDF estimation from a single
image. <em>ICCV</em>, 12820–12828. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we present SurfaceNet, an approach for estimating spatially-varying bidirectional reflectance distribution function (SVBRDF) material properties from a single image. We pose the problem as an image translation task and propose a novel patch-based generative adversarial network (GAN) that is able to produce high-quality, high-resolution surface reflectance maps. The employment of the GAN paradigm has a twofold objective: 1) allowing the model to recover finer details than standard translation models; 2) reducing the domain shift between synthetic and real data distributions in an unsupervised way.An extensive evaluation, carried out on a public benchmark of synthetic and real images under different illumination conditions, shows that SurfaceNet largely outperforms existing SVBRDF reconstruction methods, both quantitatively and qualitatively. Furthermore, SurfaceNet exhibits a remarkable ability in generating high-quality maps from real samples without any supervision at training time.Source code available at https://github.com/perceivelab/surfacenet.},
  archive   = {C_ICCV},
  author    = {Giuseppe Vecchio and Simone Palazzo and Concetto Spampinato},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01260},
  pages     = {12820-12828},
  title     = {SurfaceNet: Adversarial SVBRDF estimation from a single image},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparse needlets for lighting estimation with spherical
transport loss. <em>ICCV</em>, 12810–12819. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate lighting estimation is challenging yet critical to many computer vision and computer graphics tasks such as high-dynamic-range (HDR) relighting. Existing approaches model lighting in either frequency domain or spatial domain which is insufficient to represent the complex lighting conditions in scenes and tends to produce inaccurate estimation. This paper presents NeedleLight, a new lighting estimation model that represents illumination with needlets and allows lighting estimation in both frequency domain and spatial domain jointly. An optimal thresholding function is designed to achieve sparse needlets which trims redundant lighting parameters and demonstrates superior localization properties for illumination representation. In addition, a novel spherical transport loss is designed based on optimal transport theory which guides to regress lighting representation parameters with consideration of the spatial information. Furthermore, we propose a new metric that is concise yet effective by directly evaluating the estimated illumination maps rather than rendered images. Extensive experiments show that NeedleLight achieves superior lighting estimation consistently across multiple evaluation metrics as compared with state-of-the-art methods.},
  archive   = {C_ICCV},
  author    = {Fangneng Zhan and Changgong Zhang and Wenbo Hu and Shijian Lu and Feiying Ma and Xuansong Xie and Ling Shao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01259},
  pages     = {12810-12819},
  title     = {Sparse needlets for lighting estimation with spherical transport loss},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards high fidelity monocular face reconstruction with
rich reflectance using self-supervised learning and ray tracing.
<em>ICCV</em>, 12799–12809. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robust face reconstruction from monocular image in general lighting conditions is challenging. Methods combining deep neural network encoders with differentiable rendering have opened up the path for very fast monocular reconstruction of geometry, lighting and reflectance. They can also be trained in self-supervised manner for increased robustness and better generalization. However, their differentiable rasterization-based image formation models, as well as underlying scene parameterization, limit them to Lambertian face reflectance and to poor shape details. More recently, ray tracing was introduced for monocular face reconstruction within a classic optimization-based framework and enables state-of-the art results. However, optimization-based approaches are inherently slow and lack robustness. In this paper, we build our work on the afore-mentioned approaches and propose a new method that greatly improves reconstruction quality and robustness in general scenes. We achieve this by combining a CNN encoder with a differentiable ray tracer, which enables us to base the reconstruction on much more advanced personalized diffuse and specular albedos, a more sophisticated illumination model and a plausible representation of self-shadows. This enables to take a big leap forward in reconstruction quality of shape, appearance and lighting even in scenes with difficult illumination. With consistent face attributes reconstruction, our method leads to practical applications such as relighting and self-shadows removal. Compared to state-of-the-art methods, our results show improved accuracy and validity of the approach.},
  archive   = {C_ICCV},
  author    = {Abdallah Dib and Cédric Thébault and Junghyun Ahn and Philippe-Henri Gosselin and Christian Theobalt and Louis Chevallier},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01258},
  pages     = {12799-12809},
  title     = {Towards high fidelity monocular face reconstruction with rich reflectance using self-supervised learning and ray tracing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive confidence thresholding for monocular depth
estimation. <em>ICCV</em>, 12788–12798. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-supervised monocular depth estimation has become an appealing solution to the lack of ground truth labels, but its reconstruction loss often produces over-smoothed results across object boundaries and is incapable of handling occlusion explicitly. In this paper, we propose a new approach to leverage pseudo ground truth depth maps of stereo images generated from self-supervised stereo matching methods. The confidence map of the pseudo ground truth depth map is estimated to mitigate performance degeneration by inaccurate pseudo depth maps. To cope with the prediction error of the confidence map itself, we also leverage the threshold network that learns the threshold dynamically conditioned on the pseudo depth maps. The pseudo depth labels filtered out by the thresholded confidence map are used to supervise the monocular depth network. Furthermore, we propose the probabilistic framework that refines the monocular depth map with the help of its uncertainty map through the pixel-adaptive convolution (PAC) layer. Experimental results demonstrate superior performance to state-of-the-art monocular depth estimation methods. Lastly, we exhibit that the proposed threshold learning can also be used to improve the performance of existing confidence estimation approaches.},
  archive   = {C_ICCV},
  author    = {Hyesong Choi and Hunsang Lee and Sunkyung Kim and Sunok Kim and Seungryong Kim and Kwanghoon Sohn and Dongbo Min},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01257},
  pages     = {12788-12798},
  title     = {Adaptive confidence thresholding for monocular depth estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DnD: Dense depth estimation in crowded dynamic indoor
scenes. <em>ICCV</em>, 12777–12787. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel approach for estimating depth from a monocular camera as it moves through complex and crowded indoor environments, e.g., a department store or a metro station. Our approach predicts absolute scale depth maps over the entire scene consisting of a static background and multiple moving people, by training on dynamic scenes. Since it is difficult to collect dense depth maps from crowded indoor environments, we design our training framework without requiring depths produced from depth sensing devices. Our network leverages RGB images and sparse depth maps generated from traditional 3D reconstruction methods to estimate dense depth maps. We use two constraints to handle depth for non-rigidly moving people without tracking their motion explicitly. We demonstrate that our approach offers consistent improvements over recent depth estimation methods on the NAVERLABS dataset, which includes complex and crowded scenes.},
  archive   = {C_ICCV},
  author    = {Dongki Jung and Jaehoon Choi and Yonghan Lee and Deokhwa Kim and Changick Kim and Dinesh Manocha and Donghwan Lee},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01256},
  pages     = {12777-12787},
  title     = {DnD: Dense depth estimation in crowded dynamic indoor scenes},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MonoIndoor: Towards good practice of self-supervised
monocular depth estimation for indoor environments. <em>ICCV</em>,
12767–12776. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-supervised depth estimation for indoor environments is more challenging than its outdoor counterpart in at least the following two aspects: (i) the depth range of indoor sequences varies a lot across different frames, making it difficult for the depth network to induce consistent depth cues, whereas the maximum distance in outdoor scenes mostly stays the same as the camera usually sees the sky; (ii) the indoor sequences contain much more rotational motions, which cause difficulties for the pose network, while the motions of outdoor sequences are pre-dominantly translational, especially for driving datasets such as KITTI. In this paper, special considerations are given to those challenges and a set of good practices are consolidated for improving the performance of self-supervised monocular depth estimation in indoor environments. The proposed method mainly consists of two novel modules, i.e., a depth factorization module and a residual pose estimation module, each of which is designed to respectively tackle the aforementioned challenges. The effectiveness of each module is shown through a carefully conducted ablation study and the demonstration of the state-of-the-art performance on three indoor datasets, i.e., EuRoC, NYUv2 and 7-Scenes.},
  archive   = {C_ICCV},
  author    = {Pan Ji and Runze Li and Bir Bhanu and Yi Xu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01255},
  pages     = {12767-12776},
  title     = {MonoIndoor: Towards good practice of self-supervised monocular depth estimation for indoor environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). R-MSFM: Recurrent multi-scale feature modulation for
monocular depth estimating. <em>ICCV</em>, 12757–12766. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose Recurrent Multi-Scale Feature Modulation (R-MSFM), a new deep network architecture for self-supervised monocular depth estimation. R-MSFM extracts per-pixel features, builds a multi-scale feature modulation module, and iteratively updates an inverse depth through a parameter-shared decoder at the fixed resolution. This architecture enables our R-MSFM to maintain semantically richer while spatially more precise representations and avoid the error propagation caused by the traditional U-Net-like coarse-to-fine architecture widely used in this domain, resulting in strong generalization and efficient parameter count. Experimental results demonstrate the superiority of our proposed R-MSFM both at model size and inference speed, and show the state-of-the-art results on the KITTI benchmark. Code is available at https://github.com/jsczzzk/R-MSFM},
  archive   = {C_ICCV},
  author    = {Zhongkai Zhou and Xinnan Fan and Pengfei Shi and Yuanxue Xin},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01254},
  pages     = {12757-12766},
  title     = {R-MSFM: Recurrent multi-scale feature modulation for monocular depth estimating},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Boosting monocular depth estimation with lightweight 3D
point fusion. <em>ICCV</em>, 12747–12756. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose enhancing monocular depth estimation by adding 3D points as depth guidance. Unlike existing depth completion methods, our approach performs well on extremely sparse and unevenly distributed point clouds, which makes it agnostic to the source of the 3D points. We achieve this by introducing a novel multi-scale 3D point fusion network that is both lightweight and efficient. We demonstrate its versatility on two different depth estimation problems where the 3D points have been acquired with conventional structure-from-motion and Li-DAR. In both cases, our network performs on par with state-of-the-art depth completion methods and achieves significantly higher accuracy when only a small number of points is used while being more compact in terms of the number of parameters. We show that our method outperforms some contemporary deep learning based multi-view stereo and structure-from-motion methods both in accuracy and in compactness.},
  archive   = {C_ICCV},
  author    = {Lam Huynh and Phong Nguyen and Jiří Matas and Esa Rahtu and Janne Heikkilä},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01253},
  pages     = {12747-12756},
  title     = {Boosting monocular depth estimation with lightweight 3D point fusion},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PX-NET: Simple and efficient pixel-wise training of
photometric stereo networks. <em>ICCV</em>, 12737–12746. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Retrieving accurate 3D reconstructions of objects from the way they reflect light is a very challenging task in computer vision. Despite more than four decades since the definition of the Photometric Stereo problem, most of the literature has had limited success when global illumination effects such as cast shadows, self-reflections and ambient light come into play, especially for specular surfaces. Recent approaches have leveraged the capabilities of deep learning in conjunction with computer graphics in order to cope with the need of a vast number of training data to invert the image irradiance equation and retrieve the geometry of the object. However, rendering global illumination effects is a slow process which can limit the amount of training data that can be generated.In this work we propose a novel pixel-wise training procedure for normal prediction by replacing the training data (observation maps) of globally rendered images with independent per-pixel generated data. We show that global physical effects can be approximated on the observation map domain and this simplifies and speeds up the data creation procedure. Our network, PX-NET, achieves state-of-the-art performance compared to other pixelwise methods on synthetic datasets, as well as the DiLiGenT real dataset on both dense and sparse light settings.},
  archive   = {C_ICCV},
  author    = {Fotios Logothetis and Ignas Budvytis and Roberto Mecca and Roberto Cipolla},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01252},
  pages     = {12737-12746},
  title     = {PX-NET: Simple and efficient pixel-wise training of photometric stereo networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised depth completion with calibrated backprojection
layers. <em>ICCV</em>, 12727–12736. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a deep neural network architecture to infer dense depth from an image and a sparse point cloud. It is trained using a video stream and corresponding synchronized sparse point cloud, as obtained from a LIDAR or other range sensor, along with the intrinsic calibration parameters of the camera. At inference time, the calibration of the camera, which can be different than the one used for training, is fed as an input to the network along with the sparse point cloud and a single image. A Calibrated Backprojection Layer backprojects each pixel in the image to three-dimensional space using the calibration matrix and a depth feature descriptor. The resulting 3D positional encoding is concatenated with the image descriptor and the previous layer output to yield the input to the next layer of the encoder. A decoder, exploiting skip-connections, produces a dense depth map. The resulting Calibrated Backprojection Network, or KBNet, is trained without supervision by minimizing the photometric reprojection error. KBNet imputes missing depth value based on the training set, rather than on generic regularization. We test KBNet on public depth completion benchmarks, where it outperforms the state of the art by 30\% indoor and 8\% outdoor when the same camera is used for training and testing. When the test camera is different, the improvement reaches 62\%.},
  archive   = {C_ICCV},
  author    = {Alex Wong and Stefano Soatto},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01251},
  pages     = {12727-12736},
  title     = {Unsupervised depth completion with calibrated backprojection layers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Self-supervised monocular depth estimation for all day
images using domain separation. <em>ICCV</em>, 12717–12726. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Remarkable results have been achieved by DCNN based self-supervised depth estimation approaches. However, most of these approaches can only handle either day-time or night-time images, while their performance degrades for all-day images due to large domain shift and the variation of illumination between day and night images. To relieve these limitations, we propose a domain-separated network for self-supervised depth estimation of all-day images. Specifically, to relieve the negative influence of disturbing terms (illumination, etc.), we partition the information of day and night image pairs into two complementary sub-spaces: private and invariant domains, where the former contains the unique information (illumination, etc.) of day and night images and the latter contains essential shared information (texture, etc.). Meanwhile, to guarantee that the day and night images contain the same information, the domain-separated network takes the day-time images and corresponding night-time images (generated by GAN) as input, and the private and invariant feature extractors are learned by orthogonality and similarity loss, where the domain gap can be alleviated, thus better depth maps can be expected. Meanwhile, the reconstruction and photometric losses are utilized to estimate complementary information and depth maps effectively. Experimental results demonstrate that our approach achieves state-of-the-art depth estimation results for all-day images on the challenging Oxford RobotCar dataset, proving the superiority of our proposed approach. Code and data split are available at https://github.com/LINA-lln/ADDS-DepthNet.},
  archive   = {C_ICCV},
  author    = {Lina Liu and Xibin Song and Mengmeng Wang and Yong Liu and Liangjun Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01250},
  pages     = {12717-12726},
  title     = {Self-supervised monocular depth estimation for all day images using domain separation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Can scale-consistent monocular depth be learned in a
self-supervised scale-invariant manner? <em>ICCV</em>, 12707–12716. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Geometric constraints are shown to enforce scale consistency and remedy the scale ambiguity issue in self-supervised monocular depth estimation. Meanwhile, scale-invariant losses focus on learning relative depth, leading to accurate relative depth prediction. To combine the best of both worlds, we learn scale-consistent self-supervised depth in a scale-invariant manner. Towards this goal, we present a scale-aware geometric (SAG) loss, which enforces scale consistency through point cloud alignment. Compared to prior arts, SAG loss takes relative scale into consideration during relative motion estimation, enabling more precise alignment and explicit supervision for scale inference. In addition, a novel two-stream architecture for depth estimation is designed, which disentangles scale from depth estimation and allows depth to be learned in a scale-invariant manner. The integration of SAG loss and two-stream network enables more consistent scale inference and more accurate relative depth estimation. Our method achieves state-of-the-art performance under both scale-invariant and scale-dependent evaluation settings.},
  archive   = {C_ICCV},
  author    = {Lijun Wang and Yifan Wang and Linzhao Wang and Yunlong Zhan and Ying Wang and Huchuan Lu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01249},
  pages     = {12707-12716},
  title     = {Can scale-consistent monocular depth be learned in a self-supervised scale-invariant manner?},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Holistic pose graph: Modeling geometric structure among
objects in a scene using graph inference for 3D object prediction.
<em>ICCV</em>, 12697–12706. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Due to the missing depth cues, it is essentially ambiguous to detect 3D objects from a single RGB image. Existing methods predict the 3D pose for each object independently or merely by combining local relationships within limited surroundings, but rarely explore the inherent geometric relationships from a global perspective. To address this issue, we argue that modeling geometric structure among objects in a scene is very crucial, and thus elaborately devise the Holistic Pose Graph (HPG) that explicitly integrates all geometric poses including the object pose treated as nodes and the relative pose treated as edges. The inference of the HPG uses GRU to encode the pose features from their corresponding regions in a single RGB image, and passes messages along the graph structure iteratively to improve the predicted poses. To further enhance the correspondence between the object pose and the relative pose, we propose a novel consistency loss to explicitly measure the deviations between them. Finally, we apply Holistic Pose Estimation (HPE) to jointly evaluate both the independent object pose and the relative pose. Our experiments on the SUN RGB-D dataset demonstrate that the proposed method provides a significant improvement on 3D object prediction.},
  archive   = {C_ICCV},
  author    = {Jiwei Xiao and Ruiping Wang and Xilin Chen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01248},
  pages     = {12697-12706},
  title     = {Holistic pose graph: Modeling geometric structure among objects in a scene using graph inference for 3D object prediction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 4DComplete: Non-rigid motion estimation beyond the
observable surface. <em>ICCV</em>, 12686–12696. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tracking non-rigidly deforming scenes using range sensors has numerous applications including computer vision, AR/VR, and robotics. However, due to occlusions and physical limitations of range sensors, existing methods only handle the visible surface, thus causing discontinuities and in-completeness in the motion field. To this end, we introduce 4DComplete, a novel data-driven approach that estimates the non-rigid motion for the unobserved geometry. 4DComplete takes as input a partial shape and motion observation, extracts 4D time-space embedding, and jointly infers the missing geometry and motion field using a sparse fully-convolutional network. For network training, we constructed a large-scale synthetic dataset called DeformingThings4D, which consists of 1,972 animation sequences spanning 31 different animals or humanoid categories with dense 4D annotation. Experiments show that 4DComplete 1) reconstructs high-resolution volumetric shape and motion field from a partial observation, 2) learns an entangled 4D feature representation that benefits both shape and motion estimation, 3) yields more accurate and natural deformation than classic non-rigid priors such as As-Rigid-As-Possible (ARAP) deformation, and 4) generalizes well to unseen objects in real-world sequences.},
  archive   = {C_ICCV},
  author    = {Yang Li and Hikari Takehara and Takafumi Taketomi and Bo Zheng and Matthias Nießner},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01247},
  pages     = {12686-12696},
  title     = {4DComplete: Non-rigid motion estimation beyond the observable surface},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NPMs: Neural parametric models for 3D deformable shapes.
<em>ICCV</em>, 12675–12685. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Parametric 3D models have enabled a wide variety of tasks in computer graphics and vision, such as modeling human bodies, faces, and hands. However, the construction of these parametric models is often tedious, as it requires heavy manual tweaking, and they struggle to represent additional complexity and details such as wrinkles or clothing. To this end, we propose Neural Parametric Models (NPMs), a novel, learned alternative to traditional, parametric 3D models, which does not require handcrafted, object-specific constraints. In particular, we learn to disentangle 4D dynamics into latent-space representations of shape and pose, leveraging the flexibility of recent developments in learned implicit functions. Crucially, once learned, our neural parametric models of shape and pose enable optimization over the learned spaces to fit to new observations, similar to the fitting of a traditional parametric model, e.g., SMPL. This enables NPMs to achieve a significantly more accurate and detailed representation of observed deformable sequences. We show that NPMs improve notably over both parametric and non-parametric state of the art in reconstruction and tracking of monocular depth sequences of clothed humans and hands. Latent-space interpolation as well as shape / pose transfer experiments further demonstrate the usefulness of NPMs. Code is publicly available at https://pablopalafox.github.io/npms.},
  archive   = {C_ICCV},
  author    = {Pablo Palafox and Aljaž Božič and Justus Thies and Matthias Nießner and Angela Dai},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01246},
  pages     = {12675-12685},
  title     = {NPMs: Neural parametric models for 3D deformable shapes},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NeRD: Neural reflectance decomposition from image
collections. <em>ICCV</em>, 12664–12674. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Decomposing a scene into its shape, reflectance, and illumination is a challenging but important problem in computer vision and graphics. This problem is inherently more challenging when the illumination is not a single light source under laboratory conditions but is instead an unconstrained environmental illumination. Though recent work has shown that implicit representations can be used to model the radiance field of an object, most of these techniques only enable view synthesis and not relighting. Additionally, evaluating these radiance fields is resource and time-intensive. We propose a neural reflectance decomposition (NeRD) technique that uses physically-based rendering to decompose the scene into spatially varying BRDF material properties. In contrast to existing techniques, our input images can be captured under different illumination conditions. In addition, we also propose techniques to convert the learned reflectance volume into a relightable textured mesh enabling fast real-time rendering with novel illuminations. We demonstrate the potential of the proposed approach with experiments on both synthetic and real datasets, where we are able to obtain high-quality relightable 3D assets from image collections. The datasets and code are available at the project page: https://markboss.me/publication/2021-nerd/.},
  archive   = {C_ICCV},
  author    = {Mark Boss and Raphael Braun and Varun Jampani and Jonathan T. Barron and Ce Liu and Hendrik P.A. Lensch},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01245},
  pages     = {12664-12674},
  title     = {NeRD: Neural reflectance decomposition from image collections},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Learning anchored unsigned distance functions with gradient
direction alignment for single-view garment reconstruction.
<em>ICCV</em>, 12654–12663. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While single-view 3D reconstruction has made significant progress benefiting from deep shape representations in recent years, garment reconstruction is still not solved well due to open surfaces, diverse topologies and complex geometric details. In this paper, we propose a novel learn-able Anchored Unsigned Distance Function (AnchorUDF) representation for 3D garment reconstruction from a single image. AnchorUDF represents 3D shapes by predicting unsigned distance fields (UDFs) to enable open garment surface modeling at arbitrary resolution. To capture diverse garment topologies, AnchorUDF not only computes pixel-aligned local image features of query points, but also leverages a set of anchor points located around the surface to enrich 3D position features for query points, which provides stronger 3D space context for the distance function. Furthermore, in order to obtain more accurate point projection direction at inference, we explicitly align the spatial gradient direction of AnchorUDF with the ground-truth direction to the surface during training. Extensive experiments on two public 3D garment datasets, i.e., MGN and Deep Fashion3D, demonstrate that AnchorUDF achieves the state-of-the-art performance on single-view garment reconstruction. Code is available at https://github.com/zhaofang0627/AnchorUDF.},
  archive   = {C_ICCV},
  author    = {Fang Zhao and Wenhao Wang and Shengcai Liao and Ling Shao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01244},
  pages     = {12654-12663},
  title     = {Learning anchored unsigned distance functions with gradient direction alignment for single-view garment reconstruction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). StructDepth: Leveraging the structural regularities for
self-supervised indoor depth estimation. <em>ICCV</em>, 12643–12653. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-supervised monocular depth estimation has achieved impressive performance on outdoor datasets. Its performance however degrades notably in indoor environments because of the lack of textures. Without rich textures, the photometric consistency is too weak to train a good depth network. Inspired by the early works on indoor modeling, we leverage the structural regularities exhibited in indoor scenes, to train a better depth network. Specifically, we adopt two extra supervisory signals for self-supervised training: 1) the Manhattan normal constraint and 2) the co-planar constraint. The Manhattan normal constraint enforces the major surfaces (the floor, ceiling, and walls) to be aligned with dominant directions. The co-planar constraint states that the 3D points be well fitted by a plane if they are located within the same planar region. To generate the supervisory signals, we adopt two components to classify the major surface normal into dominant directions and detect the planar regions on the fly during training. As the predicted depth becomes more accurate after more training epochs, the supervisory signals also improve and in turn feedback to obtain a better depth model. Through extensive experiments on indoor benchmark datasets, the results show that our network outperforms the state-of-the-art methods. The source code is available at https://github.com/SJTU-ViSYS/StructDepth.},
  archive   = {C_ICCV},
  author    = {Boying Li and Yuan Huang and Zeyu Liu and Danping Zou and Wenxian Yu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01243},
  pages     = {12643-12653},
  title     = {StructDepth: Leveraging the structural regularities for self-supervised indoor depth estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep implicit surface point prediction networks.
<em>ICCV</em>, 12633–12642. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep neural representations of 3D shapes as implicit functions have been shown to produce high fidelity models surpassing the resolution-memory trade-off faced by the explicit representations using meshes and point clouds. However, most such approaches focus on representing closed shapes. Unsigned distance function (UDF) based approaches have been proposed recently as a promising alternative to represent both open and closed shapes. However, since the gradients of UDFs vanish on the surface, it is challenging to estimate local (differential) geometric properties like the normals and tangent planes which are needed for many downstream applications in vision and graphics. There are additional challenges in computing these properties efficiently with a low-memory footprint. This paper presents a novel approach that models such surfaces using a new class of implicit representations called the closest surface-point (CSP) representation. We show that CSP allows us to represent complex surfaces of any topology (open or closed) with high fidelity. It also allows for accurate and efficient computation of local geometric properties. We further demonstrate that it leads to efficient implementation of downstream algorithms like sphere-tracing for rendering the 3D surface as well as to create explicit mesh-based representations. Extensive experimental evaluation on the ShapeNet dataset validate the above contributions with results surpassing the state-of-the-art. Code and data are available at https://sites.google.com/view/cspnet.},
  archive   = {C_ICCV},
  author    = {Rahul Venkatesh and Tejan Karmali and Sarthak Sharma and Aurobrata Ghosh and R. Venkatesh Babu and Lászlό A. Jeni and Maneesh Singh},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01242},
  pages     = {12633-12642},
  title     = {Deep implicit surface point prediction networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fine-grained semantics-aware representation enhancement for
self-supervised monocular depth estimation. <em>ICCV</em>, 12622–12632.
(<a href="https://doi.org/10.1109/ICCV48922.2021.01241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-supervised monocular depth estimation has been widely studied, owing to its practical importance and recent promising improvements. However, most works suffer from limited supervision of photometric consistency, especially in weak texture regions and at object boundaries. To overcome this weakness, we propose novel ideas to improve self-supervised monocular depth estimation by leveraging cross-domain information, especially scene semantics. We focus on incorporating implicit semantic knowledge into geometric representation enhancement and suggest two ideas: a metric learning approach that exploits the semantics-guided local geometry to optimize intermediate depth representations and a novel feature fusion module that judiciously utilizes cross-modality between two heterogeneous feature representations. We comprehensively evaluate our methods on the KITTI dataset and demonstrate that our method outperforms state-of-the-art methods. The source code is available at https://github.com/hyBlue/FSRE-Depth.},
  archive   = {C_ICCV},
  author    = {Hyunyoung Jung and Eunhyeok Park and Sungjoo Yoo},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01241},
  pages     = {12622-12632},
  title     = {Fine-grained semantics-aware representation enhancement for self-supervised monocular depth estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DeepPanoContext: Panoramic 3D scene understanding with
holistic scene context graph and relation-based optimization.
<em>ICCV</em>, 12612–12621. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Panorama images have a much larger field-of-view thus naturally encode enriched scene context information compared to standard perspective images, which however is not well exploited in the previous scene understanding methods. In this paper, we propose a novel method for panoramic 3D scene understanding which recovers the 3D room layout and the shape, pose, position, and semantic category for each object from a single full-view panorama image. In order to fully utilize the rich context information, we design a novel graph neural network based context model to predict the relationship among objects and room layout, and a differentiable relationship-based optimization module to optimize object arrangement with well-designed objective functions on-the-fly. Realizing the existing data are either with incomplete ground truth or overly-simplified scene, we present a new synthetic dataset with good diversity in room layout and furniture placement, and realistic image quality for total panoramic 3D scene understanding. Experiments demonstrate that our method outperforms existing methods on panoramic scene understanding in terms of both geometry accuracy and object arrangement. Code is available at https://chengzhag.github.io/publication/dpc.},
  archive   = {C_ICCV},
  author    = {Cheng Zhang and Zhaopeng Cui and Cai Chen and Shuaicheng Liu and Bing Zeng and Hujun Bao and Yinda Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01240},
  pages     = {12612-12621},
  title     = {DeepPanoContext: Panoramic 3D scene understanding with holistic scene context graph and relation-based optimization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bridging unsupervised and supervised depth from focus via
all-in-focus supervision. <em>ICCV</em>, 12601–12611. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Depth estimation is a long-lasting yet important task in computer vision. Most of the previous works try to estimate depth from input images and assume images are all-in-focus (AiF), which is less common in real-world applications. On the other hand, a few works take defocus blur into account and consider it as another cue for depth estimation. In this paper, we propose a method to estimate not only a depth map but an AiF image from a set of images with different focus positions (known as a focal stack). We design a shared architecture to exploit the relationship between depth and AiF estimation. As a result, the proposed method can be trained either supervisedly with ground truth depth, or unsupervisedly with AiF images as supervisory signals. We show in various experiments that our method outperforms the state-of-the-art methods both quantitatively and qualitatively, and also has higher efficiency in inference time.},
  archive   = {C_ICCV},
  author    = {Ning-Hsu Wang and Ren Wang and Yu-Lun Liu and Yu-Hao Huang and Yu-Lin Chang and Chia-Ping Chen and Kevin Jou},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01239},
  pages     = {12601-12611},
  title     = {Bridging unsupervised and supervised depth from focus via all-in-focus supervision},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Geometric deep neural network using rigid and non-rigid
transformations for human action recognition. <em>ICCV</em>,
12591–12600. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep Learning architectures, albeit successful in most computer vision tasks, were designed for data with an underlying Euclidean structure, which is not usually fulfilled since pre-processed data may lie on a non-linear space. In this paper, we propose a geometry aware deep learning approach using rigid and non rigid transformation optimization for skeleton-based action recognition. Skeleton sequences are first modeled as trajectories on Kendall’s shape space and then mapped to the linear tangent space. The resulting structured data are then fed to a deep learning architecture, which includes a layer that optimizes over rigid and non rigid transformations of the 3D skeletons, followed by a CNN-LSTM network. The assessment on two large scale skeleton datasets, namely NTU-RGB+D and NTU-RGB+D 120, has proven that the proposed approach outperforms existing geometric deep learning methods and exceeds recently published approaches with respect to the majority of configurations.},
  archive   = {C_ICCV},
  author    = {Rasha Friji and Hassen Drira and Faten Chaieb and Hamza Kchok and Sebastian Kurtek},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01238},
  pages     = {12591-12600},
  title     = {Geometric deep neural network using rigid and non-rigid transformations for human action recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toward realistic single-view 3D object reconstruction with
unsupervised learning from multiple images. <em>ICCV</em>, 12580–12590.
(<a href="https://doi.org/10.1109/ICCV48922.2021.01237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recovering the 3D structure of an object from a single image is a challenging task due to its ill-posed nature. One approach is to utilize the plentiful photos of the same object category to learn a strong 3D shape prior for the object. This approach has successfully been demonstrated by a recent work of Wu et al. (2020), which obtained impressive 3D reconstruction networks with unsupervised learning. However, their algorithm is only applicable to symmetric objects. In this paper, we eliminate the symmetry requirement with a novel unsupervised algorithm that can learn a 3D reconstruction network from a multi-image dataset. Our algorithm is more general and covers the symmetry-required scenario as a special case. Besides, we employ a novel albedo loss that improves the reconstructed details and realisticity. Our method surpasses the previous work in both quality and robustness, as shown in experiments on datasets of various structures, including single-view, multiview, image-collection, and video sets. Code is available at: https://github.com/VinAIResearch/LeMul.},
  archive   = {C_ICCV},
  author    = {Long-Nhat Ho and Anh Tuan Tran and Quynh Phung and Minh Hoai},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01237},
  pages     = {12580-12590},
  title     = {Toward realistic single-view 3D object reconstruction with unsupervised learning from multiple images},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Patch2CAD: Patchwise embedding learning for in-the-wild
shape retrieval from a single image. <em>ICCV</em>, 12569–12579. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D perception of object shapes from RGB image input is fundamental towards semantic scene understanding, grounding image-based perception in our spatially 3dimensional real-world environments. To achieve a mapping between image views of objects and 3D shapes, we leverage CAD model priors from existing large-scale databases, and propose a novel approach towards constructing a joint embedding space between 2D images and 3D CAD models in a patch-wise fashion – establishing correspondences between patches of an image view of an object and patches of CAD geometry. This enables part similarity reasoning for retrieving similar CADs to a new image view without exact matches in the database. Our patch embedding provides more robust CAD retrieval for shape estimation in our end-to-end estimation of CAD model shape and pose for detected objects in a single input image. Experiments on in-the-wild, complex imagery from ScanNet show that our approach is more robust than state of the art in real-world scenarios without any exact CAD matches.},
  archive   = {C_ICCV},
  author    = {Weicheng Kuo and Anelia Angelova and Tsung-Yi Lin and Angela Dai},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01236},
  pages     = {12569-12579},
  title     = {Patch2CAD: Patchwise embedding learning for in-the-wild shape retrieval from a single image},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). MINE: Towards continuous depth MPI with NeRF for novel view
synthesis. <em>ICCV</em>, 12558–12568. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose MINE to perform novel view synthesis and depth estimation via dense 3D reconstruction from a single image. Our approach is a continuous depth generalization of the Multiplane Images (MPI) by introducing the NEural radiance fields (NeRF). Given a single image as input, MINE predicts a 4-channel image (RGB and volume density) at arbitrary depth values to jointly reconstruct the camera frustum and fill in occluded contents. The reconstructed and inpainted frustum can then be easily rendered into novel RGB or depth views using differentiable rendering. Extensive experiments on RealEstate10K, KITTI and Flowers Light Fields show that our MINE outperforms state-of-the-art by a large margin in novel view synthesis. We also achieve competitive results in depth estimation on iBims-1 and NYU-v2 without annotated depth supervision. Our source code is available at https://github.com/vincentfung13/MINE.},
  archive   = {C_ICCV},
  author    = {Jiaxin Li and Zijian Feng and Qi She and Henghui Ding and Changhu Wang and Gim Hee Lee},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01235},
  pages     = {12558-12568},
  title     = {MINE: Towards continuous depth MPI with NeRF for novel view synthesis},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RetrievalFuse: Neural 3D scene reconstruction with a
database. <em>ICCV</em>, 12548–12557. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D reconstruction of large scenes is a challenging problem due to the high-complexity nature of the solution space, in particular for generative neural networks. In contrast to traditional generative learned models which encode the full generative process into a neural network and can struggle with maintaining local details at the scene level, we introduce a new method that directly leverages scene geometry from the training database. First, we learn to synthesize an initial estimate for a 3D scene, constructed by retrieving a top-k set of volumetric chunks from the scene database. These candidates are then refined to a final scene generation with an attention-based refinement that can effectively select the most consistent set of geometry from the candidates and combine them together to create an output scene, facilitating transfer of coherent structures and local detail from train scene geometry. We demonstrate our neural scene reconstruction with a database for the tasks of 3D super resolution and surface reconstruction from sparse point clouds, showing that our approach enables generation of more coherent, accurate 3D scenes, improving on average by over 8\% in IoU over state-of-the-art scene reconstruction.},
  archive   = {C_ICCV},
  author    = {Yawar Siddiqui and Justus Thies and Fangchang Ma and Qi Shan and Matthias Nießner and Angela Dai},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01234},
  pages     = {12548-12557},
  title     = {RetrievalFuse: Neural 3D scene reconstruction with a database},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). In-the-wild single camera 3D reconstruction through moving
water surfaces. <em>ICCV</em>, 12538–12547. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a method for reconstructing the 3D shape of underwater environments from a single, stationary camera placed above the water. We propose a novel differentiable framework, which, to our knowledge, is the first single-camera solution that is capable of simultaneously retrieving the structure of dynamic water surfaces and static underwater scene geometry in the wild. This framework integrates ray casting of Snell’s law at the refractive interface, multi-view triangulation and specially designed loss functions.Our method is calibration-free, and thus it is easy to collect data outdoors in uncontrolled environments. Experimental results show that our method is able to realize robust and quality reconstructions on a variety of scenes, both in a laboratory environment and in the wild, and even in a salt water environment. We believe the method is promising for applications in surveying and environmental monitoring.},
  archive   = {C_ICCV},
  author    = {Jinhui Xiong and Wolfgang Heidrich},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01233},
  pages     = {12538-12547},
  title     = {In-the-wild single camera 3D reconstruction through moving water surfaces},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3D building reconstruction from monocular remote sensing
images. <em>ICCV</em>, 12528–12537. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D building reconstruction from monocular remote sensing imagery is an important research problem and an economic solution to large-scale city modeling, compared with reconstruction from LiDAR data and multi-view imagery. However, several challenges such as the partial invisibility of building footprints and facades, the serious shadow effect, and the extreme variance of building height in large-scale areas, have restricted the existing monocular image based building reconstruction studies to certain application scenes, i.e., modeling simple low-rise buildings from near-nadir images. In this study, we propose a novel 3D building reconstruction method for monocular remote sensing images, which tackles the above difficulties, thus providing an appealing solution for more complicated scenarios. We design a multi-task building reconstruction network, named MTBR-Net, to learn the geometric property of oblique images, the key components of a 3D building model and their relations via four semantic-related and three offset-related tasks. The network outputs are further integrated by a prior knowledge based 3D model optimization method to produce the the final 3D building models. Results on a public 3D reconstruction dataset and a novel released dataset demonstrate that our method improves the height estimation performance by over 40\% and the segmentation F1-score by 2\% - 4\% compared with current state-of-the-art.},
  archive   = {C_ICCV},
  author    = {Weijia Li and Lingxuan Meng and Jinwang Wang and Conghui He and Gui-Song Xia and Dahua Lin},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01232},
  pages     = {12528-12537},
  title     = {3D building reconstruction from monocular remote sensing images},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning indoor inverse rendering with 3D spatially-varying
lighting. <em>ICCV</em>, 12518–12527. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we address the problem of jointly estimating albedo, normals, depth and 3D spatially-varying lighting from a single image. Most existing methods formulate the task as image-to-image translation, ignoring the 3D properties of the scene. However, indoor scenes contain complex 3D light transport where a 2D representation is insufficient. In this paper, we propose a unified, learning-based inverse rendering framework that formulates 3D spatially-varying lighting. Inspired by classic volume rendering techniques, we propose a novel Volumetric Spherical Gaussian representation for lighting, which parameterizes the exitant radiance of the 3D scene surfaces on a voxel grid. We design a physics-based differentiable renderer that utilizes our 3D lighting representation, and formulates the energy-conserving image formation process that enables joint training of all intrinsic properties with the re-rendering constraint. Our model ensures physically correct predictions and avoids the need for ground-truth HDR lighting which is not easily accessible. Experiments show that our method outperforms prior works both quantitatively and qualitatively, and is capable of producing photorealistic results for AR applications such as virtual object insertion even for highly specular objects.},
  archive   = {C_ICCV},
  author    = {Zian Wang and Jonah Philion and Sanja Fidler and Jan Kautz},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01231},
  pages     = {12518-12527},
  title     = {Learning indoor inverse rendering with 3D spatially-varying lighting},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Worldsheet: Wrapping the world in a 3D sheet for view
synthesis from a single image. <em>ICCV</em>, 12508–12517. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present Worldsheet, a method for novel view synthesis using just a single RGB image as input. The main insight is that simply shrink-wrapping a planar mesh sheet onto the input image, consistent with the learned intermediate depth, captures underlying geometry sufficient to generate photorealistic unseen views with large viewpoint changes. To operationalize this, we propose a novel differentiable texture sampler that allows our wrapped mesh sheet to be textured and rendered differentiably into an image from a target viewpoint. Our approach is category-agnostic, end-to-end trainable without using any 3D supervision, and requires a single image at test time. We also explore a simple extension by stacking multiple layers of Worldsheets to better handle occlusions. Worldsheet consistently outperforms prior state-of-the-art methods on single-image view synthesis across several datasets. Furthermore, this simple idea captures novel views surprisingly well on a wide range of high-resolution in-the-wild images, converting them into navigable 3D pop-ups. Video results and code are available at https://worldsheet.github.io.},
  archive   = {C_ICCV},
  author    = {Ronghang Hu and Nikhila Ravi and Alexander C. Berg and Deepak Pathak},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01230},
  pages     = {12508-12517},
  title     = {Worldsheet: Wrapping the world in a 3D sheet for view synthesis from a single image},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SLIDE: Single image 3D photography with soft layering and
depth-aware inpainting. <em>ICCV</em>, 12498–12507. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Single image 3D photography enables viewers to view a still image from novel viewpoints. Recent approaches combine monocular depth networks with inpainting networks to achieve compelling results. A drawback of these techniques is the use of hard depth layering, making them unable to model intricate appearance details such as thin hair-like structures. We present SLIDE, a modular and unified system for single image 3D photography that uses a simple yet effective soft layering strategy to better preserve appearance details in novel views. In addition, we propose a novel depth-aware training strategy for our inpainting module, better suited for the 3D photography task. The resulting SLIDE approach is modular, enabling the use of other components such as segmentation and matting for improved layering. At the same time, SLIDE uses an efficient layered depth formulation that only requires a single forward pass through the component networks to produce high quality 3D photos. Extensive experimental analysis on three view-synthesis datasets, in combination with user studies on in-the-wild image collections, demonstrate superior performance of our technique in comparison to existing strong baselines while being conceptually much simpler. Project page: https://varunjampani.github.io/slide},
  archive   = {C_ICCV},
  author    = {Varun Jampani and Huiwen Chang and Kyle Sargent and Abhishek Kar and Richard Tucker and Michael Krainin and Dominik Kaeser and William T. Freeman and David Salesin and Brian Curless and Ce Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01229},
  pages     = {12498-12507},
  title     = {SLIDE: Single image 3D photography with soft layering and depth-aware inpainting},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RFNet: Recurrent forward network for dense point cloud
completion. <em>ICCV</em>, 12488–12497. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Point cloud completion is an interesting and challenging task in 3D vision, aiming to recover complete shapes from sparse and incomplete point clouds. Existing learning-based methods often require vast computation cost to achieve excellent performance, which limits their practical applications. In this paper, we propose a novel Recurrent Forward Network (RFNet), which is composed of three modules: Recurrent Feature Extraction (RFE), Forward Dense Completion (FDC) and Raw Shape Protection (RSP). The RFE extracts multiple global features from the incomplete point clouds for different recurrent levels, and the FDC generates point clouds in a coarse-to-fine pipeline. The RSP introduces details from the original incomplete models to refine the completion results. Besides, we propose a Sampling Chamfer Distance to better capture the shapes of models and a new Balanced Expansion Constraint to restrict the expansion distances from coarse to fine. According to the experiments on ShapeNet and KITTI, our network can achieve the state-of-the-art with lower memory cost and faster convergence.},
  archive   = {C_ICCV},
  author    = {Tianxin Huang and Hao Zou and Jinhao Cui and Xuemeng Yang and Mengmeng Wang and Xiangrui Zhao and Jiangning Zhang and Yi Yuan and Yifan Xu and Yong Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01228},
  pages     = {12488-12497},
  title     = {RFNet: Recurrent forward network for dense point cloud completion},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PoinTr: Diverse point cloud completion with geometry-aware
transformers. <em>ICCV</em>, 12478–12487. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Point clouds captured in real-world applications are of-ten incomplete due to the limited sensor resolution, single viewpoint, and occlusion. Therefore, recovering the complete point clouds from partial ones becomes an indispensable task in many practical applications. In this paper, we present a new method that reformulates point cloud completion as a set-to-set translation problem and design a new model, called PoinTr that adopts a transformer encoder-decoder architecture for point cloud completion. By rep-resenting the point cloud as a set of unordered groups of points with position embeddings, we convert the point cloud to a sequence of point proxies and employ the transformers for point cloud generation. To facilitate transformers to better leverage the inductive bias about 3D geometric structures of point clouds, we further devise a geometry-aware block that models the local geometric relationships explicitly. The migration of transformers enables our model to better learn structural knowledge and preserve detailed information for point cloud completion. Furthermore, we propose two more challenging benchmarks with more diverse incomplete point clouds that can better reflect the real-world scenarios to promote future research. Experimental results show that our method outperforms state-of-the-art methods by a large margin on both the new bench-marks and the existing ones. Code is available at https://github.com/yuxumin/PoinTr.},
  archive   = {C_ICCV},
  author    = {Xumin Yu and Yongming Rao and Ziyi Wang and Zuyan Liu and Jiwen Lu and Jie Zhou},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01227},
  pages     = {12478-12487},
  title     = {PoinTr: Diverse point cloud completion with geometry-aware transformers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ME-PCN: Point completion conditioned on mask emptiness.
<em>ICCV</em>, 12468–12477. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Point completion refers to completing the missing geometries of an object from incomplete observations. Mainstream methods predict the missing shapes by decoding a global feature learned from the input point cloud, which often leads to deficient results in preserving topology consistency and surface details. In this work, we present MEPCN, a point completion network that leverages emptiness in 3D shape space. Given a single depth scan, previous methods often encode the occupied partial shapes while ignoring the empty regions (e.g. holes) in depth maps. In contrast, we argue that these ‘emptiness’ clues indicate shape boundaries that can be used to improve topology representation and detail granularity on surfaces. Specifically, our ME-PCN encodes both the occupied point cloud and the neighboring ‘empty points’. It estimates coarse-grained but complete and reasonable surface points in the first stage, followed by a refinement stage to produce fine-grained surface details. Comprehensive experiments verify that our ME-PCN presents better qualitative and quantitative performance against the state-of-the-art. Besides, we further prove that our ‘emptiness’ design is lightweight and easy to embed in existing methods, which shows consistent effectiveness in improving the CD and EMD scores.},
  archive   = {C_ICCV},
  author    = {Bingchen Gong and Yinyu Nie and Yiqun Lin and Xiaoguang Han and Yizhou Yu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01226},
  pages     = {12468-12477},
  title     = {ME-PCN: Point completion conditioned on mask emptiness},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CSG-stump: A learning friendly CSG-like representation for
interpretable shape parsing. <em>ICCV</em>, 12458–12467. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generating an interpretable and compact representation of 3D shapes from point clouds is an important and challenging problem. This paper presents CSG-Stump Net, an unsupervised end-to-end network for learning shapes from point clouds and discovering the underlying constituent modeling primitives and operations as well. At the core is a three-level structure called CSG-Stump, consisting of a complement layer at the bottom, an intersection layer in the middle, and a union layer at the top. CSG-Stump is proven to be equivalent to CSG in terms of representation, therefore inheriting the interpretable, compact and editable nature of CSG while freeing from CSG’s complex tree structures. Particularly, the CSG-Stump has a simple and regular structure, allowing neural networks to give outputs of a constant dimensionality, which makes itself deep-learning friendly. Due to these characteristics of CSG-Stump, CSG-Stump Net achieves superior results compared to previous CSG-based methods and generates much more appealing shapes, as confirmed by extensive experiments.},
  archive   = {C_ICCV},
  author    = {Daxuan Ren and Jianmin Zheng and Jianfei Cai and Jiatong Li and Haiyong Jiang and Zhongang Cai and Junzhe Zhang and Liang Pan and Mingyuan Zhang and Haiyu Zhao and Shuai Yi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01225},
  pages     = {12458-12467},
  title     = {CSG-stump: A learning friendly CSG-like representation for interpretable shape parsing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised learning of fine structure generation for 3D
point clouds by 2D projection matching. <em>ICCV</em>, 12446–12457. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning to generate 3D point clouds without 3D supervision is an important but challenging problem. Current solutions leverage various differentiable renderers to project the generated 3D point clouds onto a 2D image plane, and train deep neural networks using the per-pixel difference with 2D ground truth images. However, these solutions are still struggling to fully recover fine structures of 3D shapes, such as thin tubes or planes. To resolve this issue, we propose an unsupervised approach for 3D point cloud generation with fine structures. Specifically, we cast 3D point cloud learning as a 2D projection matching problem. Rather than using entire 2D silhouette images as a regular pixel supervision, we introduce structure adaptive sampling to randomly sample 2D points within the silhouettes as an irregular point supervision, which alleviates the consistency issue of sampling from different view angles. Our method pushes the neural network to generate a 3D point cloud whose 2D projections match the irregular point supervision from different view angles. Our 2D projection matching approach enables the neural network to learn more accurate structure information than using the per-pixel difference, especially for fine and thin 3D structures. Our method can recover fine 3D structures from 2D silhouette images at different resolutions, and is robust to different sampling methods and point number in irregular point supervision. Our method outperforms others under widely used benchmarks. Our code, data and models are available at http-s://github.com/chenchao15/2D_projectionjnatching.},
  archive   = {C_ICCV},
  author    = {Chao Chen and Zhizhong Han and Yu-Shen Liu and Matthias Zwicker},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01224},
  pages     = {12446-12457},
  title     = {Unsupervised learning of fine structure generation for 3D point clouds by 2D projection matching},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3DStyleNet: Creating 3D shapes with geometric and texture
style variations. <em>ICCV</em>, 12436–12445. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a method to create plausible geometric and texture style variations of 3D objects in the quest to democratize 3D content creation. Given a pair of textured source and target objects, our method predicts a part-aware affine transformation field that naturally warps the source shape to imitate the overall geometric style of the target. In addition, the texture style of the target is transferred to the warped source object with the help of a multi-view differentiable renderer. Our model, 3DSTYLENET, is composed of two sub-networks trained in two stages. First, the geometric style network is trained on a large set of untextured 3D shapes. Second, we jointly optimize our geometric style network and a pre-trained image style transfer network with losses defined over both the geometry and the rendering of the result. Given a small set of high-quality textured objects, our method can create many novel stylized shapes, resulting in effortless 3D content creation and style-ware data augmentation. We showcase our approach qualitatively on 3D content stylization, and provide user studies to validate the quality of our results. In addition, our method can serve as a valuable tool to create 3D data augmentations for computer vision tasks. Extensive quantitative analysis shows that 3DSTYLENET outperforms alternative data augmentation techniques for the downstream task of single-image 3D reconstruction.},
  archive   = {C_ICCV},
  author    = {Kangxue Yin and Jun Gao and Maria Shugrina and Sameh Khamis and Sanja Fidler},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01223},
  pages     = {12436-12445},
  title     = {3DStyleNet: Creating 3D shapes with geometric and texture style variations},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3DIAS: 3D shape reconstruction with implicit algebraic
surfaces. <em>ICCV</em>, 12426–12435. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D Shape representation has substantial effects on 3D shape reconstruction. Primitive-based representations approximate a 3D shape mainly by a set of simple implicit primitives, but the low geometrical complexity of the primitives limits the shape resolution. Moreover, setting a sufficient number of primitives for an arbitrary shape is challenging. To overcome these issues, we propose a constrained implicit algebraic surface as the primitive with few learnable coefficients and higher geometrical complexities and a deep neural network to produce these primitives. Our experiments demonstrate the superiorities of our method in terms of representation power compared to the state-of-the-art methods in single RGB image 3D shape reconstruction. Furthermore, we show that our method can semantically learn segments of 3D shapes in an unsupervised manner. The code is publicly available from this link.},
  archive   = {C_ICCV},
  author    = {Mohsen Yavartanoo and Jaeyoung Chung and Reyhaneh Neshatavar and Kyoung Mu Lee},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01222},
  pages     = {12426-12435},
  title     = {3DIAS: 3D shape reconstruction with implicit algebraic surfaces},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Sat2Vid: Street-view panoramic video synthesis from a
single satellite image. <em>ICCV</em>, 12416–12425. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel method for synthesizing both temporally and geometrically consistent street-view panoramic video from a single satellite image and camera trajectory. Existing cross-view synthesis approaches focus on images, while video synthesis in such a case has not yet received enough attention. For geometrical and temporal consistency, our approach explicitly creates a 3D point cloud representation of the scene and maintains dense 3D-2D correspondences across frames that reflect the geometric scene configuration inferred from the satellite view. As for synthesis in the 3D space, we implement a cascaded network architecture with two hourglass modules to generate point-wise coarse and fine features from semantics and per-class latent vectors, followed by projection to frames and an up-sampling module to obtain the final realistic video. By leveraging computed correspondences, the produced street-view video frames adhere to the 3D geometric scene structure and maintain temporal consistency. Qualitative and quantitative experiments demonstrate superior results compared to other state-of-the-art synthesis approaches that either lack temporal consistency or realistic appearance. To the best of our knowledge, our work is the first one to synthesize cross-view images to videos..},
  archive   = {C_ICCV},
  author    = {Zuoyue Li and Zhenqiang Li and Zhaopeng Cui and Rongjun Qin and Marc Pollefeys and Martin R. Oswald},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01221},
  pages     = {12416-12425},
  title     = {Sat2Vid: Street-view panoramic video synthesis from a single satellite image},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Structured outdoor architecture reconstruction by
exploration and classification. <em>ICCV</em>, 12407–12415. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an explore-and-classify framework for structured architectural reconstruction from an aerial image. Starting from a potentially imperfect building reconstruction by an existing algorithm, our approach 1) explores the space of building models by modifying the reconstruction via heuristic actions; 2) learns to classify the correctness of building models while generating classification labels based on the ground-truth; and 3) repeat. At test time, we iterate exploration and classification, seeking for a result with the best classification score. We evaluate the approach using initial reconstructions by two baselines and two state-of-the-art reconstruction algorithms. Qualitative and quantitative evaluations demonstrate that our approach consistently improves the reconstruction quality from every initial reconstruction.},
  archive   = {C_ICCV},
  author    = {Fuyang Zhang and Xiang Xu and Nelson Nauata and Yasutaka Furukawa},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01220},
  pages     = {12407-12415},
  title     = {Structured outdoor architecture reconstruction by exploration and classification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reconstructing hand-object interactions in the wild.
<em>ICCV</em>, 12397–12406. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of understanding hand-object interactions from 2D images in the wild. This requires reconstructing both the hand and the object in 3D, which is challenging because of the mutual occlusion between the hand and the object. In this paper we make two main contributions: (1) a novel reconstruction technique, RHO (Reconstructing Hands and Objects), which reconstructs 3D models of both the hand and the object leveraging the 2D image cues and 3D contact priors; (2) a dataset MOW (Manipulating Objects in the Wild) of 500 examples of hand-object interaction images that have been &quot;3Dfied&quot; with the help of the RHO technique. Overall our dataset contains 121 distinct object categories, with a much greater diversity of manipulation actions, than in previous datasets.},
  archive   = {C_ICCV},
  author    = {Zhe Cao and Ilija Radosavovic and Angjoo Kanazawa and Jitendra Malik},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01219},
  pages     = {12397-12406},
  title     = {Reconstructing hand-object interactions in the wild},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Single view physical distance estimation using human pose.
<em>ICCV</em>, 12386–12396. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a fully automated system that simultaneously estimates the camera intrinsics, the ground plane, and physical distances between people from a single RGB image or video captured by a camera viewing a 3-D scene from a fixed vantage point. To automate camera calibration and distance estimation, we leverage priors about human pose and develop a novel direct formulation for pose-based auto-calibration and distance estimation, which shows state-of-the-art performance on publicly available datasets. The proposed approach enables existing camera systems to measure physical distances without needing a dedicated calibration process or range sensors, and is applicable to a broad range of use cases such as social distancing and workplace safety. Furthermore, to enable evaluation and drive research in this area, we contribute to the publicly available MEVA dataset with additional distance annotations, resulting in &quot;MEVADA&quot; – an evaluation benchmark for the pose-based auto-calibration and distance estimation problem.},
  archive   = {C_ICCV},
  author    = {Xiaohan Fei and Henry Wang and Lin Lee Cheong and Xiangyu Zeng and Meng Wang and Joseph Tighe},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01218},
  pages     = {12386-12396},
  title     = {Single view physical distance estimation using human pose},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SO-pose: Exploiting self-occlusion for direct 6D pose
estimation. <em>ICCV</em>, 12376–12385. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Directly regressing all 6 degrees-of-freedom (6DoF) for the object pose (i.e. the 3D rotation and translation) in a cluttered environment from a single RGB image is a challenging problem. While end-to-end methods have recently demonstrated promising results at high efficiency, they are still inferior when compared with elaborate PnP/RANSAC-based approaches in terms of pose accuracy. In this work, we address this shortcoming by means of a novel reasoning about self-occlusion, in order to establish a two-layer representation for 3D objects which considerably enhances the accuracy of end-to-end 6D pose estimation. Our framework, named SO-Pose, takes a single RGB image as input and respectively generates 2D-3D correspondences as well as self-occlusion information harnessing a shared encoder and two separate decoders. Both outputs are then fused to directly regress the 6DoF pose parameters. Incorporating cross-layer consistencies that align correspondences, self-occlusion and 6D pose, we can further improve accuracy and robustness, surpassing or rivaling all other state-of-the-art approaches on various challenging datasets.},
  archive   = {C_ICCV},
  author    = {Yan Di and Fabian Manhardt and Gu Wang and Xiangyang Ji and Nassir Navab and Federico Tombari},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01217},
  pages     = {12376-12385},
  title     = {SO-pose: Exploiting self-occlusion for direct 6D pose estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EventHands: Real-time neural 3D hand pose estimation from an
event stream. <em>ICCV</em>, 12365–12375. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D hand pose estimation from monocular videos is a long-standing and challenging problem, which is now seeing a strong upturn. In this work, we address it for the first time using a single event camera, i.e., an asynchronous vision sensor reacting on brightness changes. Our EventHands approach has characteristics previously not demonstrated with a single RGB or depth camera such as high temporal resolution at low data throughputs and real-time performance at 1000 Hz. Due to the different data modality of event cameras compared to classical cameras, existing methods cannot be directly applied to and re-trained for event streams. We thus design a new neural approach which accepts a new event stream representation suitable for learning, which is trained on newly-generated synthetic event streams and can generalise to real data. Experiments show that EventHands outperforms recent monocular methods using a colour (or depth) camera in terms of accuracy and its ability to capture hand motions of unprecedented speed. Our method, the event stream simulator and the dataset are publicly available (see https://4dqv.mpi-inf.mpg.de/EventHands/).},
  archive   = {C_ICCV},
  author    = {Viktor Rudnev and Vladislav Golyanik and Jiayi Wang and Hans-Peter Seidel and Franziska Mueller and Mohamed Elgharib and Christian Theobalt},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01216},
  pages     = {12365-12375},
  title     = {EventHands: Real-time neural 3D hand pose estimation from an event stream},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Uncertainty-aware human mesh recovery from video by learning
part-based 3D dynamics. <em>ICCV</em>, 12355–12364. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite the recent success of 3D human reconstruction methods, recovering the accurate and smooth 3D human motion from video is still challenging. Designing a temporal model in the encoding stage is not sufficient enough to settle the trade-off problem between the per-frame accuracy and the motion smoothness. To address this problem, we approach some of the fundamental problems of 3D reconstruction tasks, simultaneously predicting 3D pose and 3D motion dynamics. First, we utilize the power of uncertainty to address the problem of multiple 3D configurations resulting in the same 2D projections. Second, we confirmed that dividing the body into local regions shows outstanding results for estimating 3D motion dynamics. In this paper, we propose (i) an encoder that makes two different estimations: a static feature that presents 2D pose feature as distribution and a dynamic feature that includes optical flow information and (ii) a decoder that divides the body into five different local regions to estimate the 3D motion dynamics of each region. We demonstrate how our method recovers the accurate and smooth motion and achieves the state-of-the-art results for both constrained and in-the-wild videos.},
  archive   = {C_ICCV},
  author    = {Gun-Hee Lee and Seong-Whan Lee},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01215},
  pages     = {12355-12364},
  title     = {Uncertainty-aware human mesh recovery from video by learning part-based 3D dynamics},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gravity-aware monocular 3D human-object reconstruction.
<em>ICCV</em>, 12345–12354. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes GraviCap, i.e., a new approach for joint markerless 3D human motion capture and object trajectory estimation from monocular RGB videos. We focus on scenes with objects partially observed during a free flight. In contrast to existing monocular methods, we can recover scale, object trajectories as well as human bone lengths in meters and the ground plane&#39;s orientation, thanks to the awareness of the gravity constraining object motions. Our objective function is parametrised by the object&#39;s initial velocity and position, gravity direction and focal length, and jointly optimised for one or several free flight episodes. The proposed human-object interaction constraints ensure geometric consistency of the 3D reconstructions and improved physical plausibility of human poses compared to the unconstrained case. We evaluate GraviCap on a new dataset with ground-truth annotations for persons and different objects undergoing free flights. In the experiments, our approach achieves state-of-the-art accuracy in 3D human motion capture on various metrics. We urge the reader to watch our supplementary video. Both the source code and the dataset are released; see http://4dqv.mpi-inf.mpg.de/GraviCap/.},
  archive   = {C_ICCV},
  author    = {Rishabh Dabral and Soshi Shimada and Arjun Jain and Christian Theobalt and Vladislav Golyanik},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01214},
  pages     = {12345-12354},
  title     = {Gravity-aware monocular 3D human-object reconstruction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pi-NAS: Improving neural architecture search by reducing
supernet training consistency shift. <em>ICCV</em>, 12334–12344. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently proposed neural architecture search (NAS) methods co-train billions of architectures in a supernet and estimate their potential accuracy using the network weights detached from the supernet. However, the ranking correlation between the architectures&#39; predicted accuracy and their actual capability is incorrect, which causes the existing NAS methods&#39; dilemma. We attribute this ranking correlation problem to the supernet training consistency shift, including feature shift and parameter shift. Feature shift is identified as dynamic input distributions of a hidden layer due to random path sampling. The input distribution dynamic affects the loss descent and finally affects architecture ranking. Parameter shift is identified as contradictory parameter updates for a shared layer lay in different paths in different training steps. The rapidly-changing parameter could not preserve architecture ranking. We address these two shifts simultaneously using a nontrivial supernet-∏ model, called ∏-NAS. Specifically, we employ a supernet-∏ model that contains cross-path learning to reduce the feature consistency shift between different paths. Meanwhile, we adopt a novel nontrivial mean teacher containing negative samples to overcome parameter shift and model collision. Furthermore, our ∏-NAS runs in an unsupervised manner, which can search for more transferable architectures. Extensive experiments on ImageNet and a wide range of downstream tasks (e.g., COCO 2017, ADE20K, and Cityscapes) demonstrate the effectiveness and universality of our ∏-NAS compared to supervised NAS. See Codes 1 .},
  archive   = {C_ICCV},
  author    = {Jiefeng Peng and Jiqi Zhang and Changlin Li and Guangrun Wang and Xiaodan Liang and Liang Lin},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01213},
  pages     = {12334-12344},
  title     = {Pi-NAS: Improving neural architecture search by reducing supernet training consistency shift},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). One-pass multi-view clustering for large-scale data.
<em>ICCV</em>, 12324–12333. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing non-negative matrix factorization based multi-view clustering algorithms compute multiple coefficient matrices respect to different data views, and learn a common consensus concurrently. The final partition is always obtained from the consensus with classical clustering techniques, such as k-means. However, the non-negativity constraint prevents from obtaining a more discriminative embedding. Meanwhile, this two-step procedure fails to unify multi-view matrix factorization with partition generation closely, resulting in unpromising performance. Therefore, we propose an one-pass multi-view clustering algorithm by removing the non-negativity constraint and jointly optimize the aforementioned two steps. In this way, the generated partition can guide multi-view matrix factorization to produce more purposive coefficient matrix which, as a feedback, improves the quality of partition. To solve the resultant optimization problem, we design an alternate strategy which is guaranteed to be convergent theoretically. Moreover, the proposed algorithm is free of parameter and of linear complexity, making it practical in applications. In addition, the proposed algorithm is compared with recent advances in literature on benchmarks, demonstrating its effectiveness, superiority and efficiency.},
  archive   = {C_ICCV},
  author    = {Jiyuan Liu and Xinwang Liu and Yuexiang Yang and Li Liu and Siqi Wang and Weixuan Liang and Jiangyong Shi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01212},
  pages     = {12324-12333},
  title     = {One-pass multi-view clustering for large-scale data},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Orthogonal projection loss. <em>ICCV</em>, 12313–12323. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep neural networks have achieved remarkable performance on a range of classification tasks, with softmax crossentropy (CE) loss emerging as the de-facto objective function. The CE loss encourages features of a class to have a higher projection score on the true class-vector compared to the negative classes. However, this is a relative constraint and does not explicitly force different class features to be well-separated. Motivated by the observation that ground-truth class representations in CE loss are orthogonal (one-hot encoded vectors), we develop a novel loss function termed ‘Orthogonal Projection Loss&#39; (OPL) which imposes orthogonality in the feature space. OPL augments the properties of CE loss and directly enforces inter-class separation alongside intra-class clustering in the feature space through orthogonality constraints on the mini-batch level. As compared to other alternatives of CE, OPL offers unique advantages e.g., no additional learnable parameters, does not require careful negative mining and is not sensitive to the batch size. Given the plug-and-play nature of OPL, we evaluate it on a diverse range of tasks including image recognition (CIFAR-100), large-scale classification (ImageNet), domain generalization (PACS) and few-shot learning (mini-ImageNet, CIFAR-FS, tiered-ImageNet and Meta-dataset) and demonstrate its effectiveness across the board. Furthermore, OPL offers better robustness against practical nuisances such as adversarial attacks and label noise. Code is available at: https://github.com/kahnchana/opl.},
  archive   = {C_ICCV},
  author    = {Kanchana Ranasinghe and Muzammal Naseer and Munawar Hayat and Salman Khan and Fahad Shahbaz Khan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01211},
  pages     = {12313-12323},
  title     = {Orthogonal projection loss},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AdvRush: Searching for adversarially robust neural
architectures. <em>ICCV</em>, 12302–12312. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep neural networks continue to awe the world with their remarkable performance. Their predictions, however, are prone to be corrupted by adversarial examples that are imperceptible to humans. Current efforts to improve the robustness of neural networks against adversarial examples are focused on developing robust training methods, which update the weights of a neural network in a more robust direction. In this work, we take a step beyond training of the weight parameters and consider the problem of designing an adversarially robust neural architecture with high intrinsic robustness. We propose AdvRush, a novel adversarial robustness-aware neural architecture search algorithm, based upon a finding that independent of the training method, the intrinsic robustness of a neural network can be represented with the smoothness of its input loss landscape. Through a regularizer that favors a candidate architecture with a smoother input loss landscape, AdvRush successfully discovers an adversarially robust neural architecture. Along with a comprehensive theoretical motivation for AdvRush, we conduct an extensive amount of experiments to demonstrate the efficacy of AdvRush on various benchmark datasets. Notably, on CIFAR-10, AdvRush achieves 55.91\% robust accuracy under FGSM attack after standard training and 50.04\% robust accuracy under AutoAttack after 7-step PGD adversarial training.},
  archive   = {C_ICCV},
  author    = {Jisoo Mok and Byunggook Na and Hyeokjun Choe and Sungroh Yoon},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01210},
  pages     = {12302-12312},
  title     = {AdvRush: Searching for adversarially robust neural architectures},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Learning latent architectural distribution in
differentiable neural architecture search via variational information
maximization. <em>ICCV</em>, 12292–12301. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing differentiable neural architecture search approaches simply assume the architectural distribution on each edge is independent of each other, which conflicts with the intrinsic properties of architecture. In this paper, we view the architectural distribution as the latent representation of specific data points. Then we propose Variational Information Maximization Neural Architecture Search (VIM-NAS) to leverage a simple yet effective convolutional neural network to model the latent representation, and optimize for a tractable variational lower bound to the mutual information between the data points and the latent representations. VIM-NAS automatically learns a nearly one-hot distribution from a continuous distribution with extremely fast convergence speed, e.g., converging with one epoch. Experimental results demonstrate VIM-NAS achieves state-of-the-art performance on various search spaces, including DARTS search space, NAS-Bench-1shot1, NAS-Bench-201, and simplified search spaces S1-S4. Specifically, VIM-NAS achieves a top-1 error rate of 2.45\% and 15.80\% within 10 minutes on CIFAR-10 and CIFAR-100, respectively, and a top-1 error rate of 24.0\% when transferred to ImageNet.},
  archive   = {C_ICCV},
  author    = {Yaoming Wang and Yuchen Liu and Wenrui Dai and Chenglin Li and Junni Zou and Hongkai Xiong},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01209},
  pages     = {12292-12301},
  title     = {Learning latent architectural distribution in differentiable neural architecture search via variational information maximization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive convolutions with per-pixel dynamic filter atom.
<em>ICCV</em>, 12282–12291. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Applying feature dependent network weights have been proved to be effective in many fields. However, in practice, restricted by the enormous size of model parameters and memory footprints, scalable and versatile dynamic convolutions with per-pixel adapted filters are yet to be fully explored. In this paper, we address this challenge by de-composing filters, adapted to each spatial position, over dynamic filter atoms generated by a light-weight network from local features. Adaptive receptive fields can be supported by further representing each filter atom over sets of pre-fixed multi-scale bases. As plug-and-play replacements to convolutional layers, the introduced adaptive convolutions with per-pixel dynamic atoms enable explicit modeling of intra-image variance, while avoiding heavy computation, parameters, and memory cost. Our method preserves the appealing properties of conventional convolutions as being translation-equivariant and parametrically efficient. We present experiments to show that, the proposed method delivers comparable or even better performance across tasks, and are particularly effective on handling tasks with significant intra-image variance.},
  archive   = {C_ICCV},
  author    = {Ze Wang and Zichen Miao and Jun Hu and Qiang Qiu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01208},
  pages     = {12282-12291},
  title     = {Adaptive convolutions with per-pixel dynamic filter atom},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unifying nonlocal blocks for neural networks. <em>ICCV</em>,
12272–12281. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The nonlocal-based blocks are designed for capturing long-range spatial-temporal dependencies in computer vision tasks. Although having shown excellent performance, they still lack the mechanism to encode the rich, structured information among elements in an image or video. In this paper, to theoretically analyze the property of these nonlocal-based blocks, we provide a new perspective to interpret them, where we view them as a set of graph filters generated on a fully-connected graph. Specifically, when choosing the Chebyshev graph filter, a unified formulation can be derived for explaining and analyzing the existing nonlocal-based blocks (e.g., nonlocal block, nonlocal stage, double attention block). Furthermore, by concerning the property of spectral, we propose an efficient and robust spectral nonlocal block, which can be more robust and flexible to catch long-range dependencies when inserted into deep neural networks than the existing nonlocal blocks. Experimental results demonstrate the clear-cut improvements and practical applicabilities of our method on image classification, action recognition, semantic segmentation, and person re-identification tasks. Code are available at https://github.com/zh460045050/SNL_ICCV2021.},
  archive   = {C_ICCV},
  author    = {Lei Zhu and Qi She and Duo Li and Yanye Lu and Xuejing Kang and Jie Hu and Changhu Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01207},
  pages     = {12272-12281},
  title     = {Unifying nonlocal blocks for neural networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). BossNAS: Exploring hybrid CNN-transformers with
block-wisely self-supervised neural architecture search. <em>ICCV</em>,
12261–12271. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A myriad of recent breakthroughs in hand-crafted neural architectures for visual recognition have highlighted the urgent need to explore hybrid architectures consisting of diversified building blocks. Meanwhile, neural architecture search methods are surging with an expectation to reduce human efforts. However, whether NAS methods can efficiently and effectively handle diversified search spaces with disparate candidates (e.g. CNNs and transformers) is still an open question. In this work, we present Block-wisely Self-supervised Neural Architecture Search (BossNAS), an unsupervised NAS method that addresses the problem of in-accurate architecture rating caused by large weight-sharing space and biased supervision in previous methods. More specifically, we factorize the search space into blocks and utilize a novel self-supervised training scheme, named ensemble bootstrapping, to train each block separately before searching them as a whole towards the population center. Additionally, we present HyTra search space, a fabric-like hybrid CNN-transformer search space with searchable down-sampling positions. On this challenging search space, our searched model, BossNet-T, achieves up to 82.5\% accuracy on ImageNet, surpassing EfficientNet by 2.4\% with comparable compute time. Moreover, our method achieves superior architecture rating accuracy with 0.78 and 0.76 Spearman correlation on the canonical MBConv search space with ImageNet and on NATS-Bench size search space with CIFAR-100, respectively, surpassing state-of-the-art NAS methods. 1},
  archive   = {C_ICCV},
  author    = {Changlin Li and Tao Tang and Guangrun Wang and Jiefeng Peng and Bing Wang and Xiaodan Liang and Xiaojun Chang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01206},
  pages     = {12261-12271},
  title     = {BossNAS: Exploring hybrid CNN-transformers with block-wisely self-supervised neural architecture search},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AutoFormer: Searching transformers for visual recognition.
<em>ICCV</em>, 12250–12260. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, pure transformer-based models have shown great potentials for vision tasks such as image classification and detection. However, the design of transformer networks is challenging. It has been observed that the depth, embedding dimension, and number of heads can largely affect the performance of vision transformers. Previous models configure these dimensions based upon manual crafting. In this work, we propose a new one-shot architecture search framework, namely AutoFormer, dedicated to vision transformer search. AutoFormer entangles the weights of different blocks in the same layers during supernet training. Benefiting from the strategy, the trained supernet allows thousands of subnets to be very well-trained. Specifically, the performance of these subnets with weights inherited from the supernet is comparable to those retrained from scratch. Besides, the searched models, which we refer to AutoFormers, surpass the recent state-of-the-arts such as ViT and DeiT. In particular, AutoFormer-tiny/small/base achieve 74.7\%/81.7\%/82.4\% top-1 accuracy on ImageNet with 5.7M/22.9M/53.7M parameters, respectively. Lastly, we verify the transferability of AutoFormer by providing the performance on downstream benchmarks and distillation experiments. Code and models are available at https://github.com/microsoft/Cream.},
  archive   = {C_ICCV},
  author    = {Minghao Chen and Houwen Peng and Jianlong Fu and Haibin Ling},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01205},
  pages     = {12250-12260},
  title     = {AutoFormer: Searching transformers for visual recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LeViT: A vision transformer in ConvNet’s clothing for faster
inference. <em>ICCV</em>, 12239–12249. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We design a family of image classification architectures that optimize the trade-off between accuracy and efficiency in a high-speed regime. Our work exploits recent findings in attention-based architectures, which are competitive on highly parallel processing hardware. We revisit principles from the extensive literature on convolutional neural networks to apply them to transformers, in particular activation maps with decreasing resolutions. We also introduce the attention bias, a new way to integrate positional information in vision transformers.As a result, we propose LeViT: a hybrid neural network for fast inference image classification. We consider different measures of efficiency on different hardware platforms, so as to best reflect a wide range of application scenarios. Our extensive experiments empirically validate our technical choices and show they are suitable to most architectures. Overall, LeViT significantly outperforms existing convnets and vision transformers with respect to the speed/accuracy tradeoff. For example, at 80\% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on CPU. We release the code at https://github.com/facebookresearch/LeViT.},
  archive   = {C_ICCV},
  author    = {Ben Graham and Alaaeldin El-Nouby and Hugo Touvron and Pierre Stock and Armand Joulin and Hervé Jégou and Matthijs Douze},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01204},
  pages     = {12239-12249},
  title     = {LeViT: A vision transformer in ConvNet’s clothing for faster inference},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Homogeneous architecture augmentation for neural predictor.
<em>ICCV</em>, 12229–12238. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Neural Architecture Search (NAS) can automatically design well-performed architectures of Deep Neural Networks (DNNs) for the tasks at hand. However, one bottleneck of NAS is the prohibitively computational cost largely due to the expensive performance evaluation. The neural predictors can directly estimate the performance without any training of the DNNs to be evaluated, thus have drawn increasing attention from researchers. Despite their popularity, they also suffer a severe limitation: the shortage of annotated DNN architectures for effectively training the neural predictors. In this paper, we proposed Homogeneous Architecture Augmentation for Neural Predictor (HAAP) of DNN architectures to address the issue aforementioned. Specifically, a homogeneous architecture augmentation algorithm is proposed in HAAP to generate sufficient training data taking the use of homogeneous representation. Furthermore, the one-hot encoding strategy is introduced into HAAP to make the representation of DNN architectures more effective. The experiments have been conducted on both NAS-Benchmark-101 and NAS-Bench-201 dataset. The experimental results demonstrate that the proposed HAAP algorithm outperforms the state of the arts compared, yet with much less training data. In addition, the ablation studies on both benchmark datasets have also shown the universality of the homogeneous architecture augmentation. Our code has been made available at https://github.com/lyq998/HAAP.},
  archive   = {C_ICCV},
  author    = {Yuqiao Liu and Yehui Tang and Yanan Sun},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01203},
  pages     = {12229-12238},
  title     = {Homogeneous architecture augmentation for neural predictor},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FairNAS: Rethinking evaluation fairness of weight sharing
neural architecture search. <em>ICCV</em>, 12219–12228. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One of the most critical problems in weight-sharing neural architecture search is the evaluation of candidate models within a predefined search space. In practice, a one-shot supernet is trained to serve as an evaluator. A faithful ranking certainly leads to more accurate searching results. However, current methods are prone to making misjudgments. In this paper, we prove that their biased evaluation is due to inherent unfairness in the supernet training. In view of this, we propose two levels of constraints: expectation fairness and strict fairness. Particularly, strict fairness ensures equal optimization opportunities for all choice blocks throughout the training, which neither overestimates nor underestimates their capacity. We demonstrate that this is crucial for improving the confidence of models’ ranking. Incorporating the one-shot supernet trained under the proposed fairness constraints with a multi-objective evolutionary search algorithm, we obtain various state-of-the-art models, e.g., FairNAS-A attains 77.5\% top-1 validation accuracy on ImageNet.},
  archive   = {C_ICCV},
  author    = {Xiangxiang Chu and Bo Zhang and Ruijun Xu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01202},
  pages     = {12219-12228},
  title     = {FairNAS: Rethinking evaluation fairness of weight sharing neural architecture search},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distilling optimal neural networks: Rapid search in diverse
spaces. <em>ICCV</em>, 12209–12218. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current state-of-the-art Neural Architecture Search (NAS) methods neither efficiently scale to multiple hardware platforms, nor handle diverse architectural search-spaces. To remedy this, we present DONNA (Distilling Optimal Neural Network Architectures), a novel pipeline for rapid, scalable and diverse NAS, that scales to many user scenarios. DONNA consists of three phases. First, an accuracy predictor is built using blockwise knowledge distillation from a reference model. This predictor enables searching across diverse networks with varying macro-architectural parameters such as layer types and attention mechanisms, as well as across micro-architectural parameters such as block repeats and expansion rates. Second, a rapid evolutionary search finds a set of pareto-optimal architectures for any scenario using the accuracy predictor and on-device measurements. Third, optimal models are quickly fine-tuned to training-from-scratch accuracy. DONNA is up to 100× faster than MNasNet in finding state-of-the-art architectures on-device. Classifying ImageNet, DONNA architectures are 20\% faster than EfficientNet-B0 and Mo-bileNetV2 on a Nvidia V100 GPU and 10\% faster with 0.5\% higher accuracy than MobileNetV2-1.4x on a Samsung S20 smartphone. In addition to NAS, DONNA is used for search-space extension and exploration, as well as hardware-aware model compression.},
  archive   = {C_ICCV},
  author    = {Bert Moons and Parham Noorzad and Andrii Skliar and Giovanni Mariani and Dushyant Mehta and Chris Lott and Tijmen Blankevoort},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01201},
  pages     = {12209-12218},
  title     = {Distilling optimal neural networks: Rapid search in diverse spaces},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Direct differentiable augmentation search. <em>ICCV</em>,
12199–12208. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Data augmentation has been an indispensable tool to improve the performance of deep neural networks, however the augmentation can hardly transfer among different tasks and datasets. Consequently, a recent trend is to adopt AutoML technique to learn proper augmentation policy without extensive hand-crafted tuning. In this paper, we propose an efficient differentiable search algorithm called Direct Differentiable Augmentation Search (DDAS). It exploits meta-learning with one-step gradient update and continuous relaxation to the expected training loss for efficient search. Our DDAS can achieve efficient augmentation search without relying on approximations such as Gumbel-Softmax or second order gradient approximation. To further reduce the adverse effect of improper augmentations, we organize the search space into a two level hierarchy, in which we first decide whether to apply augmentation, and then determine the specific augmentation policy. On standard image classification benchmarks, our DDAS achieves state-of-the-art performance and efficiency tradeoff while reducing the search cost dramatically, e.g. 0.15 GPU hours for CIFAR-10. In addition, we also use DDAS to search augmentation for object detection task and achieve comparable performance with AutoAugment [8], while being 1000× faster. Code will be released in https://github.com/zxcvfd13502/DDAS_code},
  archive   = {C_ICCV},
  author    = {Aoming Liu and Zehao Huang and Zhiwu Huang and Naiyan Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01200},
  pages     = {12199-12208},
  title     = {Direct differentiable augmentation search},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Product quantizer aware inverted index for scalable nearest
neighbor search. <em>ICCV</em>, 12190–12198. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The inverted index is one of the most commonly used structures for non-exhaustive nearest neighbor search on large-scale datasets. It allows a significant factor of acceleration by a reduced number of distance computations with only a small fraction of the database. In particular, the inverted index enables the product quantization (PQ) to learn their codewords in the residual vector space. The quantization error of the PQ can be substantially improved in such combination since the residual vector space is much more quantization-friendly thanks to their compact distribution compared to the original data. In this paper, we first raise an unremarked but crucial question; why the inverted index and the product quantizer are optimized separately even though they are closely related? For instance, changes on the inverted index distort the whole residual vector space. To address the raised question, we suggest a joint optimization of the coarse and fine quantizers by substituting the original objective of the coarse quantizer to end-to-end quantization distortion. Moreover, our method is generic and applicable to different combinations of coarse and fine quantizers such as inverted multi-index and optimized PQ.},
  archive   = {C_ICCV},
  author    = {Haechan Noh and Taeho Kim and Jae-Pil Heo},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01199},
  pages     = {12190-12198},
  title     = {Product quantizer aware inverted index for scalable nearest neighbor search},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vector neurons: A general framework for SO(3)-equivariant
networks. <em>ICCV</em>, 12180–12189. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Invariance and equivariance to the rotation group have been widely discussed in the 3D deep learning community for pointclouds. Yet most proposed methods either use complex mathematical tools that may limit their accessibility, or are tied to specific input data types and network architectures. In this paper, we introduce a general framework built on top of what we call Vector Neuron representations for creating SO (3) -equivariant neural networks for pointcloud processing. Extending neurons from 1D scalars to 3D vectors, our vector neurons enable a simple mapping of SO (3) actions to latent spaces thereby providing a framework for building equivariance in common neural operations – including linear layers, non-linearities, pooling, and normalizations. Due to their simplicity, vector neurons are versatile and, as we demonstrate, can be incorporated into diverse network architecture backbones, allowing them to process geometry inputs in arbitrary poses. Despite its simplicity, our method performs comparably well in accuracy and generalization with other more complex and specialized state-of-the-art methods on classification and segmentation tasks. We also show for the first time a rotation equivariant reconstruction network. Source code is available at https://github.com/FlyingGiraffe/vnn.},
  archive   = {C_ICCV},
  author    = {Congyue Deng and Or Litany and Yueqi Duan and Adrien Poulenard and Andrea Tagliasacchi and Leonidas Guibas},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01198},
  pages     = {12180-12189},
  title     = {Vector neurons: A general framework for SO(3)-equivariant networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robustness via cross-domain ensembles. <em>ICCV</em>,
12169–12179. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a method for making neural network predictions robust to shifts from the training data distribution. The proposed method is based on making predictions via a diverse set of cues (called ‘middle domains’) and ensembling them into one strong prediction. The premise of the idea is that predictions made via different cues respond differently to a distribution shift, hence one should be able to merge them into one robust final prediction. We perform the merging in a straightforward but principled manner based on the uncertainty associated with each prediction. The evaluations are performed using multiple tasks and datasets (Taskonomy, Replica, ImageNet, CIFAR) under a wide range of adversarial and non-adversarial distribution shifts which demonstrate the proposed method is considerably more robust than its standard learning counterpart, conventional deep ensembles, and several other baselines.},
  archive   = {C_ICCV},
  author    = {Teresa Yeo and Oğuzhan Fatih Kar and Amir Zamir},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01197},
  pages     = {12169-12179},
  title     = {Robustness via cross-domain ensembles},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vision transformers for dense prediction. <em>ICCV</em>,
12159–12168. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce dense prediction transformers, an architecture that leverages vision transformers in place of convolutional networks as a backbone for dense prediction tasks. We assemble tokens from various stages of the vision transformer into image-like representations at various resolutions and progressively combine them into full-resolution predictions using a convolutional decoder. The transformer backbone processes representations at a constant and relatively high resolution and has a global receptive field at every stage. These properties allow the dense prediction transformer to provide finer-grained and more globally coherent predictions when compared to fully-convolutional networks. Our experiments show that this architecture yields substantial improvements on dense prediction tasks, especially when a large amount of training data is available. For monocular depth estimation, we observe an improvement of up to 28\% in relative performance when compared to a state-of-the-art fully-convolutional network. When applied to semantic segmentation, dense prediction transformers set a new state of the art on ADE20K with 49.02\% mIoU. We further show that the architecture can be fine-tuned on smaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets the new state of the art. Our models are available at https://github.com/intel-isl/DPT.},
  archive   = {C_ICCV},
  author    = {René Ranftl and Alexey Bochkovskiy and Vladlen Koltun},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01196},
  pages     = {12159-12168},
  title     = {Vision transformers for dense prediction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Viewpoint invariant dense matching for visual
geolocalization. <em>ICCV</em>, 12149–12158. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we propose a novel method for image matching based on dense local features and tailored for visual geolocalization. Dense local features matching is robust against changes in illumination and occlusions, but not against viewpoint shifts which are a fundamental aspect of geolocalization. Our method, called GeoWarp, directly embeds invariance to viewpoint shifts in the process of extracting dense features. This is achieved via a trainable module which learns from the data an invariance that is meaningful for the task of recognizing places. We also devise a new self-supervised loss and two new weakly supervised losses to train this module using only unlabeled data and weak labels. GeoWarp is implemented efficiently as a re-ranking method that can be easily embedded into pre-existing visual geolocalization pipelines. Experimental validation on standard geolocalization benchmarks demonstrates that GeoWarp boosts the accuracy of state-of-the-art retrieval architectures. The code and trained models will be released upon acceptance of this paper.},
  archive   = {C_ICCV},
  author    = {Gabriele Berton and Carlo Masone and Valerio Paolicelli and Barbara Caputo},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01195},
  pages     = {12149-12158},
  title     = {Viewpoint invariant dense matching for visual geolocalization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bayesian triplet loss: Uncertainty quantification in image
retrieval. <em>ICCV</em>, 12138–12148. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Uncertainty quantification in image retrieval is crucial for downstream decisions, yet it remains a challenging and largely unexplored problem. Current methods for estimating uncertainties are poorly calibrated, computationally expensive, or based on heuristics. We present a new method that views image embeddings as stochastic features rather than deterministic features. Our two main contributions are (1) a likelihood that matches the triplet constraint and that evaluates the probability of an anchor being closer to a positive than a negative; and (2) a prior over the feature space that justifies the conventional l 2 normalization. To ensure computational efficiency, we derive a variational approximation of the posterior, called the Bayesian triplet loss, that produces state-of-the-art uncertainty estimates and matches the predictive performance of current state-of-the-art methods.},
  archive   = {C_ICCV},
  author    = {Frederik Warburg and Martin Jørgensen and Javier Civera and Søren Hauberg},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01194},
  pages     = {12138-12148},
  title     = {Bayesian triplet loss: Uncertainty quantification in image retrieval},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning attribute-driven disentangled representations for
interactive fashion retrieval. <em>ICCV</em>, 12127–12137. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Interactive retrieval for online fashion shopping provides the ability to change image retrieval results according to the user feedback. One common problem in interactive retrieval is that a specific user interaction (e.g., changing the color of a T-shirt) causes other aspects to change inadvertently (e.g., the retrieved item has a sleeve type different than the query). This is a consequence of existing methods learning visual representations that are semantically entangled in the embedding space, which limits the controllability of the retrieved results. We propose to leverage on the semantics of visual attributes to train convolutional networks that learn attribute-specific subspaces for each attribute to obtain disentangled representations. Thus operations, such as swapping out a particular attribute value for another, impact the attribute at hand and leave others untouched. We show that our model can be tailored to deal with different retrieval tasks while maintaining its disentanglement property. We obtain state-of-the-art performance on three interactive fashion retrieval tasks: attribute manipulation retrieval, conditional similarity retrieval, and outfit complementary item retrieval. Code and models are publicly available 1 .},
  archive   = {C_ICCV},
  author    = {Yuxin Hou and Eleonora Vig and Michael Donoser and Loris Bazzani},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01193},
  pages     = {12127-12137},
  title     = {Learning attribute-driven disentangled representations for interactive fashion retrieval},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Telling the what while pointing to the where: Multimodal
queries for image retrieval. <em>ICCV</em>, 12116–12126. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most existing image retrieval systems use text queries as a way for the user to express what they are looking for. However, fine-grained image retrieval often requires the ability to also express where in the image the content they are looking for is. The text modality can only cumbersomely express such localization preferences, whereas pointing is a more natural fit. In this paper, we propose an image retrieval setup with a new form of multimodal queries, where the user simultaneously uses both spoken natural language (the what) and mouse traces over an empty canvas (the where) to express the characteristics of the desired target image. We then describe simple modifications to an existing image retrieval model, enabling it to operate in this setup. Qualitative and quantitative experiments show that our model effectively takes this spatial guidance into account, and provides significantly more accurate retrieval results compared to text-only equivalent systems.},
  archive   = {C_ICCV},
  author    = {Soravit Changpinyo and Jordi Pont-Tuset and Vittorio Ferrari and Radu Soricut},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01192},
  pages     = {12116-12126},
  title     = {Telling the what while pointing to the where: Multimodal queries for image retrieval},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Video geo-localization employing geo-temporal feature
learning and GPS trajectory smoothing. <em>ICCV</em>, 12106–12115. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we address the problem of video geo-localization by proposing a Geo-Temporal Feature Learning (GTFL) Network to simultaneously learn the discriminative features for the query video frames and the gallery images for estimating the geo-spatial trajectory of a query video. Based on a transformer encoder architecture, our GTFL model encodes query and gallery data separately, via two dedicated branches. The proposed GPS Loss and Clip Triplet Loss exploit the geographical and temporal proximity between the frames and the clips to jointly learn the query and the gallery features. We also propose a deep learning approach to trajectory smoothing by predicting the outliers in the estimated GPS positions and learning the offsets to smooth the trajectory. We build a large dataset from four different regions of USA; New York, San Francisco, Berkeley and Bay Area using BDD driving videos as query, and by collecting corresponding Google StreetView (GSV) Images for gallery. Extensive evaluations of proposed method on this new dataset are provided . Code and dataset details is publicly available at https://github.com/kregmi/VTE.},
  archive   = {C_ICCV},
  author    = {Krishna Regmi and Mubarak Shah},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01191},
  pages     = {12106-12115},
  title     = {Video geo-localization employing geo-temporal feature learning and GPS trajectory smoothing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Face image retrieval with attribute manipulation.
<em>ICCV</em>, 12096–12105. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current face image retrieval solutions are limited, since they treat different facial attributes the same and cannot incorporate user’s preference for a subset of attributes in their search criteria. This paper introduces a new face image retrieval framework, where the input face query is augmented by both an adjustment vector that specifies the desired modifications to the facial attributes, and a preference vector that assigns different levels of importance to different attributes. For example, a user can ask for retrieving images similar to a query image, but with a different hair color, and no preference for absence/presence of eyeglasses in the results. To achieve this, we propose to disentangle the semantics, corresponding to various attributes, by learning a set of sparse and orthogonal basis vectors in the latent space of StyleGAN. Such basis vectors are then employed to decompose the dissimilarity between face images in terms of dissimilarity between their attributes, assign preference to the attributes, and adjust the attributes in the query. Enforcing sparsity on the basis vectors helps us to disentangle the latent space and adjust each attribute independently from other attributes, while enforcing orthogonality facilitates preference assignment and the dissimilarity decomposition. The effectiveness of our approach is illustrated by achieving state-of-the-art results for the face image retrieval task.},
  archive   = {C_ICCV},
  author    = {Alireza Zaeemzadeh and Shabnam Ghadar and Baldo Faieta and Zhe Lin and Nazanin Rahnavard and Mubarak Shah and Ratheesh Kalarot},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01190},
  pages     = {12096-12105},
  title     = {Face image retrieval with attribute manipulation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Instance-level image retrieval using reranking transformers.
<em>ICCV</em>, 12085–12095. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Instance-level image retrieval is the task of searching in a large database for images that match an object in a query image. To address this task, systems usually rely on a retrieval step that uses global image descriptors, and a subsequent step that performs domain-specific refinements or reranking by leveraging operations such as geometric verification based on local features. In this work, we propose Reranking Transformers (RRTs) as a general model to incorporate both local and global features to rerank the matching images in a supervised fashion and thus replace the relatively expensive process of geometric verification. RRTs are lightweight and can be easily parallelized so that reranking a set of top matching results can be performed in a single forward-pass. We perform extensive experiments on the Revisited Oxford and Paris datasets, and the Google Landmarks v2 dataset, showing that RRTs outperform previous reranking approaches while using much fewer local descriptors. Moreover, we demonstrate that, unlike existing approaches, RRTs can be optimized jointly with the feature extractor, which can lead to feature representations tailored to downstream tasks and further accuracy improvements. The code and trained models are publicly available at https://github.com/uvavision/RerankingTransformer.},
  archive   = {C_ICCV},
  author    = {Fuwen Tan and Jiangbo Yuan and Vicente Ordonez},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01189},
  pages     = {12085-12095},
  title     = {Instance-level image retrieval using reranking transformers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning specialized activation functions with the piecewise
linear unit. <em>ICCV</em>, 12075–12084. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The choice of activation functions is crucial for modern deep neural networks. Popular hand-designed activation functions like Rectified Linear Unit(ReLU) and its variants show promising performance in various tasks and models. Swish, the automatically discovered activation function, has been proposed and outperforms ReLU on many challenging datasets. However, it has two main drawbacks. First, the tree-based search space is highly discrete and restricted, which is difficult for searching. Second, the sample-based searching method is inefficient, making it infeasible to find specialized activation functions for each dataset or neural architecture. To tackle these drawbacks, we propose a new activation function called Piecewise Linear Unit(PWLU), which incorporates a carefully designed formulation and learning method. It can learn specialized activation functions and achieves SOTA performance on large-scale datasets like ImageNet and COCO. For example, on ImageNet classification dataset, PWLU improves 0.9\%/0.53\%/1.0\%/1.7\%/1.0\% top-1 accuracy over Swish for ResNet-18/ResNet-50/MobileNet-V2/MobileNetV3/EfficientNet-B0. PWLU is also easy to implement and efficient at inference, which can be widely applied in real-world applications.},
  archive   = {C_ICCV},
  author    = {Yucong Zhou and Zezhou Zhu and Zhao Zhong},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01188},
  pages     = {12075-12084},
  title     = {Learning specialized activation functions with the piecewise linear unit},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-supervised product quantization for deep unsupervised
image retrieval. <em>ICCV</em>, 12065–12074. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Supervised deep learning-based hash and vector quantization are enabling fast and large-scale image retrieval systems. By fully exploiting label annotations, they are achieving outstanding retrieval performances compared to the conventional methods. However, it is painstaking to assign labels precisely for a vast amount of training data, and also, the annotation process is error-prone. To tackle these issues, we propose the first deep unsupervised image retrieval method dubbed Self-supervised Product Quantization (SPQ) network, which is label-free and trained in a self-supervised manner. We design a Cross Quantized Contrastive learning strategy that jointly learns codewords and deep visual descriptors by comparing individually transformed images (views). Our method analyzes the image contents to extract descriptive features, allowing us to understand image representations for accurate retrieval. By conducting extensive experiments on benchmarks, we demonstrate that the proposed method yields state-of-the-art results even without supervised pretraining.},
  archive   = {C_ICCV},
  author    = {Young Kyun Jang and Nam Ik Cho},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01187},
  pages     = {12065-12074},
  title     = {Self-supervised product quantization for deep unsupervised image retrieval},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep symmetric network for underexposed image enhancement
with recurrent attentional learning. <em>ICCV</em>, 12055–12064. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Underexposed image enhancement is of importance in many research domains. In this paper, we take this problem as image feature transformation between the underexposed image and its paired enhanced version, and we propose a deep symmetric network for the issue. Our symmetric network adapts invertible neural networks (INN) for bidirectional feature learning between images, and to ensure the mutual propagation invertible we specifically construct two pairs of encoder-decoder with the same pretrained parameters. This invertible mechanism with bidirectional feature transformations enable us to both avoid colour bias and recover the content effectively for image enhancement. In addition, we propose a new recurrent residual-attention module (RRAM), where the recurrent learning network is designed to gradually perform the desired colour adjustments. Ablation experiments are executed to show the role of each component of our new architecture. We conduct a large number of experiments on two datasets to demonstrate that our method achieves the state-of-the-art effect in underexposed image enhancement. Code is available at https://www.shaopinglu.net/proj-iccv21/ImageEnhancement.html.},
  archive   = {C_ICCV},
  author    = {Lin Zhao and Shao-Ping Lu and Tao Chen and Zhenglu Yang and Ariel Shamir},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01186},
  pages     = {12055-12064},
  title     = {Deep symmetric network for underexposed image enhancement with recurrent attentional learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep relational metric learning. <em>ICCV</em>, 12045–12054.
(<a href="https://doi.org/10.1109/ICCV48922.2021.01185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a deep relational metric learning (DRML) framework for image clustering and retrieval. Most existing deep metric learning methods learn an embedding space with a general objective of increasing interclass distances and decreasing intraclass distances. However, the conventional losses of metric learning usually suppress intraclass variations which might be helpful to identify samples of unseen classes. To address this problem, we propose to adaptively learn an ensemble of features that characterizes an image from different aspects to model both interclass and intraclass distributions. We further employ a relational module to capture the correlations among each feature in the ensemble and construct a graph to represent an image. We then perform relational inference on the graph to integrate the ensemble and obtain a relation-aware embedding to measure the similarities. Extensive experiments on the widely-used CUB-200-2011, Cars196, and Stanford Online Products datasets demonstrate that our framework improves existing deep metric learning methods and achieves very competitive results. 1},
  archive   = {C_ICCV},
  author    = {Wenzhao Zheng and Borui Zhang and Jiwen Lu and Jie Zhou},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01185},
  pages     = {12045-12054},
  title     = {Deep relational metric learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Universal cross-domain retrieval: Generalizing across
classes and domains. <em>ICCV</em>, 12036–12044. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, for the first time, we address the problem of universal cross-domain retrieval, where the test data can belong to classes or domains which are unseen during training. Due to dynamically increasing number of categories and practical constraint of training on every possible domain, which requires large amounts of data, generalizing to both unseen classes and domains is important. Towards that goal, we propose SnMpNet (Semantic Neighbourhood and Mixture Prediction Network), which incorporates two novel losses to account for the unseen classes and domains encountered during testing. Specifically, we introduce a novel Semantic Neighborhood loss to bridge the knowledge gap between seen and unseen classes and ensure that the latent space embedding of the unseen classes is semantically meaningful with respect to its neighboring classes. We also introduce a mix-up based supervision at image-level as well as semantic-level of the data for training with the Mixture Prediction loss, which helps in efficient retrieval when the query belongs to an unseen domain. These losses are incorporated on the SE-ResNet50 backbone to obtain SnMpNet. Extensive experiments on two large-scale datasets, Sketchy Extended and DomainNet, and thorough comparisons with state-of-the-art justify the effectiveness of the proposed model.},
  archive   = {C_ICCV},
  author    = {Soumava Paul and Titir Dutta and Soma Biswas},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01184},
  pages     = {12036-12044},
  title     = {Universal cross-domain retrieval: Generalizing across classes and domains},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning by aligning: Visible-infrared person
re-identification using cross-modal correspondences. <em>ICCV</em>,
12026–12035. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the problem of visible-infrared person re-identification (VI-reID), that is, retrieving a set of person images, captured by visible or infrared cameras, in a cross-modal setting. Two main challenges in VI-reID are intraclass variations across person images, and cross-modal discrepancies between visible and infrared images. Assuming that the person images are roughly aligned, previous approaches attempt to learn coarse image- or rigid part-level person representations that are discriminative and generalizable across different modalities. However, the person images, typically cropped by off-the-shelf object detectors, are not necessarily well-aligned, which distract discriminative person representation learning. In this paper, we introduce a novel feature learning framework that addresses these problems in a unified way. To this end, we propose to exploit dense correspondences between cross-modal person images. This allows to address the cross-modal discrepancies in a pixel-level, suppressing modality-related features from person representations more effectively. This also encourages pixel-wise associations between cross-modal local features, further facilitating discriminative feature learning for VI-reID. Extensive experiments and analyses on standard VI-reID benchmarks demonstrate the effectiveness of our approach, which significantly outperforms the state of the art.},
  archive   = {C_ICCV},
  author    = {Hyunjong Park and Sanghoon Lee and Junghyup Lee and Bumsub Ham},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01183},
  pages     = {12026-12035},
  title     = {Learning by aligning: Visible-infrared person re-identification using cross-modal correspondences},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Video-based person re-identification with spatial and
temporal memory networks. <em>ICCV</em>, 12016–12025. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Video-based person re-identification (reID) aims to retrieve person videos with the same identity as a query person across multiple cameras. Spatial and temporal distractors in person videos, such as background clutter and partial occlusions over frames, respectively, make this task much more challenging than image-based person reID. We observe that spatial distractors appear consistently in a particular location, and temporal distractors show several patterns, e.g., partial occlusions occur in the first few frames, where such patterns provide informative cues for predicting which frames to focus on (i.e., temporal attentions). Based on this, we introduce a novel Spatial and Temporal Memory Networks (STMN). The spatial memory stores features for spatial distractors that frequently emerge across video frames, while the temporal memory saves attentions which are optimized for typical temporal patterns in person videos. We leverage the spatial and temporal memories to refine frame-level person representations and to aggregate the refined frame-level features into a sequence-level person representation, respectively, effectively handling spatial and temporal distractors in person videos. We also introduce a memory spread loss preventing our model from addressing particular items only in the memories. Experimental results on standard benchmarks, including MARS, DukeMTMC-VideoReID, and LSVID, demonstrate the effectiveness of our method.},
  archive   = {C_ICCV},
  author    = {Chanho Eom and Geon Lee and Junghyup Lee and Bumsub Ham},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01182},
  pages     = {12016-12025},
  title     = {Video-based person re-identification with spatial and temporal memory networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pyramid spatial-temporal aggregation for video-based person
re-identification. <em>ICCV</em>, 12006–12015. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Video-based person re-identification aims to associate the video clips of the same person across multiple non-overlapping cameras. Spatial-temporal representations can provide richer and complementary information between frames, which are crucial to distinguish the target person when occlusion occurs. This paper proposes a novel Pyramid Spatial-Temporal Aggregation (PSTA) framework to aggregate the frame-level features progressively and fuse the hierarchical temporal features into a final video-level representation. Thus, short-term and long-term temporal information could be well exploited by different hierarchies. Furthermore, a Spatial-Temporal Aggregation Module (STAM) is proposed to enhance the aggregation capability of PSTA. It mainly consists of two novel attention blocks: Spatial Reference Attention (SRA) and Temporal Reference Attention (TRA). SRA explores the spatial correlations within a frame to determine the attention weight of each location. While TRA extends SRA with the correlations between adjacent frames, temporal consistency information can be fully explored to suppress the interference features and strengthen the discriminative ones. Extensive experiments on several challenging benchmarks demonstrate the effectiveness of the proposed PSTA, and our full model reaches 91.5\% and 98.3\% Rank-1 accuracy on MARS and DukeMTMC-VID benchmarks. The source code is available at https://github.com/WangYQ9/VideoReID-PSTA.},
  archive   = {C_ICCV},
  author    = {Yingquan Wang and Pingping Zhang and Shang Gao and Xia Geng and Hu Lu and Dong Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01181},
  pages     = {12006-12015},
  title     = {Pyramid spatial-temporal aggregation for video-based person re-identification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ASMR: Learning attribute-based person search with adaptive
semantic margin regularizer. <em>ICCV</em>, 11996–12005. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Attribute-based person search is the task of finding person images that are best matched with a set of text attributes given as query. The main challenge of this task is the large modality gap between attributes and images. To reduce the gap, we present a new loss for learning cross-modal embeddings in the context of attribute-based person search. We regard a set of attributes as a category of people sharing the same traits. In a joint embedding space of the two modalities, our loss pulls images close to their person categories for modality alignment. More importantly, it pushes apart a pair of person categories by a margin determined adaptively by their semantic distance, where the distance metric is learned end-to-end so that the loss considers importance of each attribute when relating person categories. Our loss guided by the adaptive semantic margin leads to more discriminative and semantically well-arranged distributions of person images. As a consequence, it enables a simple embedding model to achieve state-of-the-art records on public benchmarks without bells and whistles.},
  archive   = {C_ICCV},
  author    = {Boseung Jeong and Jicheol Park and Suha Kwak},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01180},
  pages     = {11996-12005},
  title     = {ASMR: Learning attribute-based person search with adaptive semantic margin regularizer},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weakly supervised person search with region siamese
networks. <em>ICCV</em>, 11986–11995. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Supervised learning is dominant in person search, but it requires elaborate labeling of bounding boxes and identities. Large-scale labeled training data is often difficult to collect, especially for person identities. A natural question is whether a good person search model can be trained without the need of identity supervision. In this paper, we present a weakly supervised setting where only bounding box annotations are available. Based on this new setting, we provide an effective baseline model termed Region Siamese Networks (R-SiamNets). Towards learning useful representations for recognition in the absence of identity labels, we supervise the R-SiamNet with instance-level consistency loss and cluster-level contrastive loss. For instance-level consistency learning, the R-SiamNet is constrained to extract consistent features from each person region with or without out-of-region context. For cluster-level contrastive learning, we enforce the aggregation of closest instances and the separation of dissimilar ones in feature space. Extensive experiments validate the utility of our weakly supervised method. Our model achieves the rank-1 of 87.1\% and mAP of 86.0\% on CUHK-SYSU benchmark, which surpasses several fully supervised methods, such as OIM [36] and MGTS [4], by a clear margin. More promising performance can be reached by incorporating extra training data. We hope this work could encourage the future research in this field.},
  archive   = {C_ICCV},
  author    = {Chuchu Han and Kai Su and Dongdong Yu and Zehuan Yuan and Changxin Gao and Nong Sang and Yi Yang and Changhu Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01179},
  pages     = {11986-11995},
  title     = {Weakly supervised person search with region siamese networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PT-CapsNet: A novel prediction-tuning capsule network
suitable for deeper architectures. <em>ICCV</em>, 11976–11985. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Capsule Networks (CapsNets) create internal representations by parsing inputs into various instances at different resolution levels via a two-phase process – part-whole transformation and hierarchical component routing. Since both of these internal phases are computationally expensive, CapsNet have not found wider use. Existing variations of CapsNets mainly focus on performance comparison with the original CapsNet, and have not outperformed CNN-based models on complex tasks. To address the limitations of the existing CapsNet structures, we propose a novel Prediction-Tuning Capsule Network (PT-CapsNet), and also introduce fully connected PT-Capsules (FC-PT-Caps) and locally connected PT-Capsules (LC-PT-Caps). Different from existing CapsNet structures, our proposed model (i) allows the use of capsules for more difficult vision tasks and provides wider applicability; and (ii) provides better than or comparable performance to CNN-based baselines on these complex tasks. In our experiments, we show robustness to affine transformations, as well as the lightweight and scalability of PT-CapsNet via constructing larger and deeper networks and performing comparisons on classification, semantic segmentation and object detection tasks. The results show consistent performance improvement and significant parameter reduction compared to various baseline models. Code is available at https://github.com/Christinepan881/PT-CapsNet.git.},
  archive   = {C_ICCV},
  author    = {Chenbin Pan and Senem Velipasalar},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01178},
  pages     = {11976-11985},
  title     = {PT-CapsNet: A novel prediction-tuning capsule network suitable for deeper architectures},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EC-DARTS: Inducing equalized and consistent optimization
into DARTS. <em>ICCV</em>, 11966–11975. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Based on the relaxed search space, differential architecture search (DARTS) is efficient in searching for a high-performance architecture. However, the unbalanced competition among operations that have different trainable parameters causes the model collapse. Besides, the inconsistent structures in the search and retraining stages causes cross-stage evaluation to be unstable. In this paper, we call these issues as an operation gap and a structure gap in DARTS. To shrink these gaps, we propose to induce equalized and consistent optimization in differentiable architecture search (EC-DARTS). EC-DARTS decouples different operations based on their categories to optimize the operation weights so that the operation gap between them is shrinked. Besides, we introduce an induced structural transition to bridge the structure gap between the model structures in the search and retraining stages. Extensive experiments on CIFAR10 and ImageNet demonstrate the effectiveness of our method. Specifically, on CIFAR10, we achieve a test error of 2.39\%, while only 0.3 GPU days on NVIDIA TITAN V. On ImageNet, our method achieves a top-1 error of 23.6\% under the mobile setting.},
  archive   = {C_ICCV},
  author    = {Qinqin Zhou and Xiawu Zheng and Liujuan Cao and Bineng Zhong and Teng Xi and Gang Zhang and Errui Ding and Mingliang Xu and Rongrong Ji},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01177},
  pages     = {11966-11975},
  title     = {EC-DARTS: Inducing equalized and consistent optimization into DARTS},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inferring high-resolution traffic accident risk maps based
on satellite imagery and GPS trajectories. <em>ICCV</em>, 11957–11965.
(<a href="https://doi.org/10.1109/ICCV48922.2021.01176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traffic accidents cost about 3\% of the world’s GDP and are the leading cause of death in children and young adults. Accident risk maps are useful tools to monitor and mitigate accident risk. We present a technique to generate high-resolution (5 meters) accident risk maps. At this high resolution, accidents are sparse and risk estimation is limited by bias-variance trade-off. Prior accident risk maps either estimate low-resolution maps that are of low utility (high bias), or they use frequency-based estimation techniques that inaccurately predict where accidents actually happen (high variance). To improve this trade-off, we use an end-to-end deep architecture that can input satellite imagery, GPS trajectories, road maps and the history of accidents. Our evaluation on four metropolitan areas in the US with a total area of 7,488 km 2 shows that our technique outperform prior work in terms of resolution and accuracy.},
  archive   = {C_ICCV},
  author    = {Songtao He and Mohammad Amin Sadeghi and Sanjay Chawla and Mohammad Alizadeh and Hari Balakrishnan and Samuel Madden},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01176},
  pages     = {11957-11965},
  title     = {Inferring high-resolution traffic accident risk maps based on satellite imagery and GPS trajectories},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LIRA: Learnable, imperceptible and robust backdoor attacks.
<em>ICCV</em>, 11946–11956. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, machine learning models have demonstrated to be vulnerable to backdoor attacks, primarily due to the lack of transparency in black-box models such as deep neural networks. A third-party model can be poisoned such that it works adequately in normal conditions but behaves maliciously on samples with specific trigger patterns. However, the trigger injection function is manually defined in most existing backdoor attack methods, e.g., placing a small patch of pixels on an image or slightly deforming the image before poisoning the model. This results in a two-stage approach with a sub-optimal attack success rate and a lack of complete stealthiness under human inspection.In this paper, we propose a novel and stealthy backdoor attack framework, LIRA, which jointly learns the optimal, stealthy trigger injection function and poisons the model. We formulate such an objective as a non-convex, constrained optimization problem. Under this optimization framework, the trigger generator function will learn to manipulate the input with imperceptible noise to preserve the model performance on the clean data and maximize the attack success rate on the poisoned data. Then, we solve this challenging optimization problem with an efficient, two-stage stochastic optimization procedure. Finally, the proposed attack framework achieves 100\% success rates in several benchmark datasets, including MNIST, CIFAR10, GTSRB, and T-ImageNet, while simultaneously bypassing existing backdoor defense methods and human inspection.},
  archive   = {C_ICCV},
  author    = {Khoa Doan and Yingjie Lao and Weijie Zhao and Ping Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01175},
  pages     = {11946-11956},
  title     = {LIRA: Learnable, imperceptible and robust backdoor attacks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Building-GAN: Graph-conditioned architectural volumetric
design generation. <em>ICCV</em>, 11936–11945. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Volumetric design is the first and critical step for professional building design, where architects not only depict the rough 3D geometry of the building but also specify the programs to form a 2D layout on each floor. Though 2D layout generation for a single story has been widely studied, there is no developed method for multi-story buildings. This paper focuses on volumetric design generation conditioned on an input program graph. Instead of outputting dense 3D voxels, we propose a new 3D representation named voxel graph that is both compact and expressive for building geometries. Our generator is a cross-modal graph neural network that uses a pointer mechanism to connect the input program graph and the output voxel graph, and the whole pipeline is trained using the adversarial framework. The generated designs are evaluated qualitatively by a user study and quantitatively using three metrics: quality, diversity, and connectivity accuracy. We show that our model generates realistic 3D volumetric designs and outperforms previous methods and baselines.},
  archive   = {C_ICCV},
  author    = {Kai-Hung Chang and Chin-Yi Cheng and Jieliang Luo and Shingo Murata and Mehdi Nourbakhsh and Yoshito Tsuji},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01174},
  pages     = {11936-11945},
  title     = {Building-GAN: Graph-conditioned architectural volumetric design generation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stochastic transformer networks with linear competing units:
Application to end-to-end SL translation. <em>ICCV</em>, 11926–11935.
(<a href="https://doi.org/10.1109/ICCV48922.2021.01173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automating sign language translation (SLT) is a challenging real-world application. Despite its societal importance, though, research progress in the field remains rather poor. Crucially, existing methods that yield viable performance necessitate the availability of laborious to obtain gloss sequence groundtruth. In this paper, we attenuate this need, by introducing an end-to-end SLT model that does not entail explicit use of glosses; the model only needs text groundtruth. This is in stark contrast to existing end-to-end models that use gloss sequence groundtruth, either in the form of a modality that is recognized at an intermediate model stage, or in the form of a parallel output process, jointly trained with the SLT model. Our approach constitutes a Transformer network with a novel type of layers that combines: (i) local winner-takes-all (LWTA) layers with stochastic winner sampling, instead of conventional ReLU layers, (ii) stochastic weights with posterior distributions estimated via variational inference, and (iii) a weight compression technique at inference time that exploits estimated posterior variance to perform massive, almost lossless compression. We demonstrate that our approach can reach the currently best reported BLEU-4 score on the PHOENIX 2014T benchmark, but without making use of glosses for model training, and with a memory footprint reduced by more than 70\%.},
  archive   = {C_ICCV},
  author    = {Andreas Voskou and Konstantinos P. Panousis and Dimitrios Kosmopoulos and Dimitris N. Metaxas and Sotirios Chatzis},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01173},
  pages     = {11926-11935},
  title     = {Stochastic transformer networks with linear competing units: Application to end-to-end SL translation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rethinking spatial dimensions of vision transformers.
<em>ICCV</em>, 11916–11925. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vision Transformer (ViT) extends the application range of transformers from language processing to computer vision tasks as being an alternative architecture against the existing convolutional neural networks (CNN). Since the transformer-based architecture has been innovative for computer vision modeling, the design convention towards an effective architecture has been less studied yet. From the successful design principles of CNN, we investigate the role of spatial dimension conversion and its effectiveness on transformer-based architecture. We particularly attend to the dimension reduction principle of CNNs; as the depth increases, a conventional CNN increases channel dimension and decreases spatial dimensions. We empirically show that such a spatial dimension reduction is beneficial to a transformer architecture as well, and propose a novel Pooling-based Vision Transformer (PiT) upon the original ViT model. We show that PiT achieves the improved model capability and generalization performance against ViT. Throughout the extensive experiments, we further show PiT outperforms the baseline on several tasks such as image classification, object detection, and robustness evaluation. Source codes and ImageNet models are available at https://github.com/naver-ai/pit.},
  archive   = {C_ICCV},
  author    = {Byeongho Heo and Sangdoo Yun and Dongyoon Han and Sanghyuk Chun and Junsuk Choe and Seong Joon Oh},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01172},
  pages     = {11916-11925},
  title     = {Rethinking spatial dimensions of vision transformers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ALADIN: All layer adaptive instance normalization for
fine-grained style similarity. <em>ICCV</em>, 11906–11915. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present ALADIN (All Layer AdaIN); a novel architecture for searching images based on the similarity of their artistic style. Representation learning is critical to visual search, where distance in the learned search embedding reflects image similarity. Learning an embedding that discriminates fine-grained variations in style is hard, due to the difficulty of defining and labelling style. ALADIN takes a weakly supervised approach to learning a representation for fine-grained style similarity of digital artworks, leveraging BAM-FG, a novel large-scale dataset of user generated content groupings gathered from the web. ALADIN sets a new state of the art accuracy for style-based visual search over both coarse labelled style data (BAM) and BAM-FG; a new 2.62 million image dataset of 310,000 fine-grained style groupings also contributed by this work.},
  archive   = {C_ICCV},
  author    = {Dan Ruta and Saeid Motiian and Baldo Faieta and Zhe Lin and Hailin Jin and Alex Filipkowski and Andrew Gilbert and John Collomosse},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01171},
  pages     = {11906-11915},
  title     = {ALADIN: All layer adaptive instance normalization for fine-grained style similarity},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). HiT: Hierarchical transformer with momentum contrast for
video-text retrieval. <em>ICCV</em>, 11895–11905. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Video-Text Retrieval has been a hot research topic with the growth of multimedia data on the internet. Transformer for video-text learning has attracted increasing attention due to its promising performance. However, existing cross-modal transformer approaches typically suffer from two major limitations: 1) Exploitation of the transformer architecture where different layers have different feature characteristics is limited; 2) End-to-end training mechanism limits negative sample interactions in a mini-batch. In this paper, we propose a novel approach named Hierarchical Transformer (HiT) for video-text retrieval. HiT performs Hierarchical Cross-modal Contrastive Matching in both feature-level and semantic-level, achieving multi-view and comprehensive retrieval results. Moreover, inspired by MoCo, we propose Momentum Cross-modal Contrast for cross-modal learning to enable large-scale negative sample interactions on-the-fly, which contributes to the generation of more precise and discriminative representations. Experimental results on the three major Video-Text Retrieval benchmark datasets demonstrate the advantages of our method.},
  archive   = {C_ICCV},
  author    = {Song Liu and Haoqi Fan and Shengsheng Qian and Yiru Chen and Wenkui Ding and Zhongyuan Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01170},
  pages     = {11895-11905},
  title     = {HiT: Hierarchical transformer with momentum contrast for video-text retrieval},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Beyond road extraction: A dataset for map update using
aerial images. <em>ICCV</em>, 11885–11894. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The increasing availability of satellite and aerial imagery has sparked substantial interest in automatically updating street maps by processing aerial images. Until now, the community has largely focused on road extraction, where road networks are inferred from scratch from an aerial image. However, given that relatively high-quality maps exist in most parts of the world, in practice, inference approaches must be applied to update existing maps rather than infer new ones. With recent road extraction methods showing high accuracy, we argue that it is time to transition to the more practical map update task, where an existing map is updated by adding, removing, and shifting roads, without introducing errors in parts of the existing map that remain up-to-date. In this paper, we develop a new dataset called MUNO21 for the map update task, and show that it poses several new and interesting research challenges. We evaluate several state-of-the-art road extraction methods on MUNO21, and find that substantial further improvements in accuracy will be needed to realize automatic map update.},
  archive   = {C_ICCV},
  author    = {Favyen Bastani and Sam Madden},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01169},
  pages     = {11885-11894},
  title     = {Beyond road extraction: A dataset for map update using aerial images},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Clothing status awareness for long-term person
re-identification. <em>ICCV</em>, 11875–11884. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Long-Term person re-identification (LT-reID) exposes extreme challenges because of the longer time gaps between two recording footages where a person is likely to change clothing. There are two types of approaches for LT-reID: biometrics-based approach and data adaptation based approach. The former one is to seek clothing irrelevant biometric features. However, seeking high quality biometric feature is the main concern. The latter one adopts fine-tuning strategy by using data with significant clothing change. However, the performance is compromised when it is applied to cases without clothing change. This work argues that these approaches in fact are not aware of clothing status (i.e., change or no-change) of a pedestrian. Instead, they blindly assume all footages of a pedestrian have different clothes. To tackle this issue, a Regularization via Clothing Status Awareness Network (RCSANet) is proposed to regularize descriptions of a pedestrian by embedding the clothing status awareness. Consequently, the description can be enhanced to maintain the best ID discriminative feature while improving its robustness to real-world LT-reID where both clothing-change case and no-clothing-change case exist. Experiments show that RCSANet performs reasonably well on three LT-reID datasets.},
  archive   = {C_ICCV},
  author    = {Yan Huang and Qiang Wu and JingSong Xu and Yi Zhong and ZhaoXiang Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01168},
  pages     = {11875-11884},
  title     = {Clothing status awareness for long-term person re-identification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Learning to know where to see: A visibility-aware approach
for occluded person re-identification. <em>ICCV</em>, 11865–11874. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Person re-identification (ReID) has gained an impressive progress in recent years. However, the occlusion is still a common and challenging problem for recent ReID methods. Several mainstream methods utilize extra cues (e.g., human pose information) to distinguish human parts from obstacles to alleviate the occlusion problem. Although achieving inspiring progress, these methods severely rely on the fine-grained extra cues, and are sensitive to the estimation error in the extra cues. In this paper, we show that existing methods may degrade if the extra information is sparse or noisy. Thus we propose a simple yet effective method that is robust to sparse and noisy pose information. This is achieved by discretizing pose information to the visibility label of body parts, so as to suppress the influence of occluded regions. We show in our experiments that leveraging pose information in this way is more effective and robust. Besides, our method can be embedded into most person ReID models easily. Extensive experiments validate the effectiveness of our model on common occluded person ReID datasets.},
  archive   = {C_ICCV},
  author    = {Jinrui Yang and Jiawei Zhang and Fufu Yu and Xinyang Jiang and Mengdan Zhang and Xing Sun and Yingcong Chen and Wei-Shi Zheng},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01167},
  pages     = {11865-11874},
  title     = {Learning to know where to see: A visibility-aware approach for occluded person re-identification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Occluded person re-identification with single-scale global
representations. <em>ICCV</em>, 11855–11864. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Occluded person re-identification (ReID) aims at re-identifying occluded pedestrians from occluded or holistic images taken across multiple cameras. Current state-of-the-art (SOTA) occluded ReID models rely on some auxiliary modules, including pose estimation, feature pyramid and graph matching modules, to learn multi-scale and/or part-level features to tackle the occlusion challenges. This unfortunately leads to complex ReID models that (i) fail to generalize to challenging occlusions of diverse appearance, shape or size, and (ii) become ineffective in handling non-occluded pedestrians. However, real-world ReID applications typically have highly diverse occlusions and involve a hybrid of occluded and non-occluded pedestrians. To address these two issues, we introduce a novel ReID model that learns discriminative single-scale global-level pedestrian features by enforcing a novel exponentially sensitive yet bounded distance loss on occlusion-based augmented data. We show for the first time that learning single-scale global features without using these auxiliary modules is able to outperform the SOTA multi-scale and/or part-level feature-based models. Further, our simple model can achieve new SOTA performance in both occluded and non-occluded ReID, as shown by extensive results on three occluded and two general ReID benchmarks. Additionally, we create a large-scale occluded person ReID dataset with various occlusions in different scenes, which is significantly larger and contains more diverse occlusions and pedestrian dressings than existing occluded ReID datasets, providing a more faithful occluded ReID benchmark. The dataset is available at: https://git.io/OPReID},
  archive   = {C_ICCV},
  author    = {Cheng Yan and Guansong Pang and Jile Jiao and Xiao Bai and Xuetao Feng and Chunhua Shen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01166},
  pages     = {11855-11864},
  title     = {Occluded person re-identification with single-scale global representations},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). IDM: An intermediate domain module for domain adaptive
person re-ID. <em>ICCV</em>, 11844–11854. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unsupervised domain adaptive person re-identification (UDA re-ID) aims at transferring the labeled source domain’s knowledge to improve the model’s discriminability on the unlabeled target domain. From a novel perspective, we argue that the bridging between the source and target domains can be utilized to tackle the UDA re-ID task, and we focus on explicitly modeling appropriate intermediate domains to characterize this bridging. Specifically, we propose an Intermediate Domain Module (IDM) to generate intermediate domains’ representations on-the-fly by mixing the source and target domains’ hidden representations using two domain factors. Based on the &quot;shortest geodesic path&quot; definition, i.e., the intermediate domains along the shortest geodesic path between the two extreme domains can play a better bridging role, we propose two properties that these intermediate domains should satisfy. To ensure these two properties to better characterize appropriate intermediate domains, we enforce the bridge losses on intermediate domains’ prediction space and feature space, and enforce a diversity loss on the two domain factors. The bridge losses aim at guiding the distribution of appropriate intermediate domains to keep the right distance to the source and target domains. The diversity loss serves as a regularization to prevent the generated intermediate domains from being over-fitting to either of the source and target domains. Our proposed method outperforms the state-of-the-arts by a large margin in all the common UDA re-ID tasks, and the mAP gain is up to 7.7\% on the challenging MSMT17 benchmark. Code is available at https://github.com/SikaStar/IDM.},
  archive   = {C_ICCV},
  author    = {Yongxing Dai and Jun Liu and Yifan Sun and Zekun Tong and Chi Zhang and Ling-Yu Duan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01165},
  pages     = {11844-11854},
  title     = {IDM: An intermediate domain module for domain adaptive person re-ID},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The center of attention: Center-keypoint grouping via
attention for multi-person pose estimation. <em>ICCV</em>, 11833–11843.
(<a href="https://doi.org/10.1109/ICCV48922.2021.01164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce CenterGroup, an attention-based framework to estimate human poses from a set of identity-agnostic keypoints and person center predictions in an image. Our approach uses a transformer to obtain context-aware embeddings for all detected keypoints and centers and then applies multi-head attention to directly group joints into their corresponding person centers. While most bottom-up methods rely on non-learnable clustering at inference, CenterGroup uses a fully differentiable attention mechanism that we train end-to-end together with our keypoint detector. As a result, our method obtains state-of-the-art performance with up to 2.5x faster inference time than competing bottom-up approaches. Our code is available at https://github.com/dvl-tum/center-group},
  archive   = {C_ICCV},
  author    = {Guillem Brasó and Nikita Kister and Laura Leal-Taixé},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01164},
  pages     = {11833-11843},
  title     = {The center of attention: Center-keypoint grouping via attention for multi-person pose estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Memory-augmented dynamic neural relational inference.
<em>ICCV</em>, 11823–11832. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dynamic interacting systems are prevalent in vision tasks. These interactions are usually difficult to observe and measure directly, and yet understanding latent interactions is essential for performing inference tasks on dynamic systems like forecasting. Neural relational inference (NRI) techniques are thus introduced to explicitly estimate interpretable relations between the entities in the system for trajectory prediction. However, NRI assumes static relations; thus, dynamic neural relational inference (DNRI) was proposed to handle dynamic relations using LSTM. Unfortunately, the older information will be washed away when the LSTM updates the latent variable as a whole, which is why DNRI struggles with modeling long-term dependences and forecasting long sequences. This motivates us to propose a memory-augmented dynamic neural relational inference method, which maintains two associative memory pools: one for the interactive relations and the other for the individual entities. The two memory pools help retain useful relation features and node features for the estimation in the future steps. Our model dynamically estimates the relations by learning better embeddings and utilizing the long-range information stored in the memory. With the novel memory modules and customized structures, our memory-augmented DNRI can update and access the memory adaptively as required. The memory pools also serve as global latent variables across time to maintain detailed long-term temporal relations readily available for other components to use. Experiments on synthetic and real-world datasets show the effectiveness of the proposed method on modeling dynamic relations and forecasting complex trajectories.},
  archive   = {C_ICCV},
  author    = {Dong Gong and Zhen Zhang and Javen Qinfeng Shi and Anton Van Den Hengel},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01163},
  pages     = {11823-11832},
  title     = {Memory-augmented dynamic neural relational inference},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Occlude them all: Occlusion-aware attention network for
occluded person re-ID. <em>ICCV</em>, 11813–11822. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Person Re-Identification (ReID) has achieved remarkable performance along with the deep learning era. However, most approaches carry out ReID only based upon holistic pedestrian regions. In contrast, real-world scenarios involve occluded pedestrians, which provide partial visual appearances and destroy the ReID accuracy. A common strategy is to locate visible body parts by auxiliary model, which however suffers from significant domain gaps and data bias issues. To avoid such problematic models in occluded person ReID, we propose the Occlusion-Aware Mask Network (OAMN). In particular, we incorporate an attention-guided mask module, which requires guidance from labeled occlusion data. To this end, we propose a novel occlusion augmentation scheme that produces diverse and precisely labeled occlusion for any holistic dataset. The proposed scheme suits real-world scenarios better than existing schemes, which consider only limited types of occlusions. We also offer a novel occlusion unification scheme to tackle ambiguity information at the test phase. The above three components enable existing attention mechanisms to precisely capture body parts regardless of the occlusion. Comprehensive experiments on a variety of person ReID benchmarks demonstrate the superiority of OAMN over state-of-the-arts.},
  archive   = {C_ICCV},
  author    = {Peixian Chen and Wenfeng Liu and Pingyang Dai and Jianzhuang Liu and Qixiang Ye and Mingliang Xu and Qi’an Chen and Rongrong Ji},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01162},
  pages     = {11813-11822},
  title     = {Occlude them all: Occlusion-aware attention network for occluded person re-ID},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CM-NAS: Cross-modality neural architecture search for
visible-infrared person re-identification. <em>ICCV</em>, 11803–11812.
(<a href="https://doi.org/10.1109/ICCV48922.2021.01161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visible-Infrared person re-identification (VI-ReID) aims to match cross-modality pedestrian images, breaking through the limitation of single-modality person ReID in dark environment. In order to mitigate the impact of large modality discrepancy, existing works manually design various two-stream architectures to separately learn modality-specific and modality-sharable representations. Such a manual design routine, however, highly depends on massive experiments and empirical practice, which is time consuming and labor intensive. In this paper, we systematically study the manually designed architectures, and identify that appropriately separating Batch Normalization (BN) layers is the key to bring a great boost towards cross-modality matching. Based on this observation, the essential objective is to find the optimal separation scheme for each BN layer. To this end, we propose a novel method, named Cross-Modality Neural Architecture Search (CM-NAS). It consists of a BN-oriented search space in which the standard optimization can be fulfilled subject to the cross-modality task. Equipped with the searched architecture, our method outperforms state-of-the-art counter-parts in both two benchmarks, improving the Rank-1/mAP by 6.70\%/6.13\% on SYSU-MM01 and by 12.17\%/11.23\% on RegDB. Code is released at https://github.com/JDAI-CV/CM-NAS.},
  archive   = {C_ICCV},
  author    = {Chaoyou Fu and Yibo Hu and Xiang Wu and Hailin Shi and Tao Mei and Ran He},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01161},
  pages     = {11803-11812},
  title     = {CM-NAS: Cross-modality neural architecture search for visible-infrared person re-identification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Explainable person re-identification with attribute-guided
metric distillation. <em>ICCV</em>, 11793–11802. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite the great progress of person re-identification (ReID) with the adoption of Convolutional Neural Networks, current ReID models are opaque and only outputs a scalar distance between two persons. There are few methods providing users semantically understandable explanations for why two persons are the same one or not. In this paper, we propose a post-hoc method, named Attribute-guided Metric Distillation (AMD), to explain existing ReID models. This is the first method to explore attributes to answer: 1) what and where the attributes make two persons different, and 2) how much each attribute contributes to the difference. In AMD, we design a pluggable interpreter network for target models to generate quantitative contributions of attributes and visualize accurate attention maps of the most discriminative attributes. To achieve this goal, we propose a metric distillation loss by which the interpreter learns to decompose the distance of two persons into components of attributes with knowledge distilled from the target model. Moreover, we propose an attribute prior loss to make the interpreter generate attribute-guided attention maps and to eliminate biases caused by the imbalanced distribution of attributes. This loss can guide the interpreter to focus on the exclusive and discriminative attributes rather than the large-area but common attributes of two persons. Comprehensive experiments show that the interpreter can generate effective and intuitive explanations for varied models and generalize well under cross-domain settings. As a by-product, the accuracy of target models can be further improved with our interpreter. 1},
  archive   = {C_ICCV},
  author    = {Xiaodong Chen and Xinchen Liu and Wu Liu and Xiao-Ping Zhang and Yongdong Zhang and Tao Mei},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01160},
  pages     = {11793-11802},
  title     = {Explainable person re-identification with attribute-guided metric distillation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TransPose: Keypoint localization via transformer.
<em>ICCV</em>, 11782–11792. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While CNN-based models have made remarkable progress on human pose estimation, what spatial dependencies they capture to localize keypoints remains unclear. In this work, we propose a model called Trans-Pose, which introduces Transformer for human pose estimation. The attention layers built in Transformer enable our model to capture long-range relationships efficiently and also can reveal what dependencies the predicted key-points rely on. To predict keypoint heatmaps, the last attention layer acts as an aggregator, which collects contributions from image clues and forms maximum positions of keypoints. Such a heatmap-based localization approach via Transformer conforms to the principle of Activation Maximization [19]. And the revealed dependencies are image-specific and fine-grained, which also can provide evidence of how the model handles special cases, e.g., occlusion. The experiments show that TransPose achieves 75.8 AP and 75.0 AP on COCO validation and test-dev sets, while being more lightweight and faster than mainstream CNN architectures. The TransPose model also transfers very well on MPII benchmark, achieving superior performance on the test set when fine-tuned with small training costs. Code and pre-trained models are publicly available 1 .},
  archive   = {C_ICCV},
  author    = {Sen Yang and Zhibin Quan and Mu Nie and Wankou Yang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01159},
  pages     = {11782-11792},
  title     = {TransPose: Keypoint localization via transformer},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning with memory-based virtual classes for deep metric
learning. <em>ICCV</em>, 11772–11781. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The core of deep metric learning (DML) involves learning visual similarities in high-dimensional embedding space. One of the main challenges is to generalize from seen classes of training data to unseen classes of test data. Recent works have focused on exploiting past embeddings to increase the number of instances for the seen classes. Such methods achieve performance improvement via augmentation, while the strong focus on seen classes still remains. This can be undesirable for DML, where training and test data exhibit entirely different classes. In this work, we present a novel training strategy for DML called MemVir. Unlike previous works, MemVir memorizes both embedding features and class weights to utilize them as additional virtual classes. The exploitation of virtual classes not only utilizes augmented information for training but also alleviates a strong focus on seen classes for better generalization. Moreover, we embed the idea of curriculum learning by slowly adding virtual classes for a gradual increase in learning difficulty, which improves the learning stability as well as the final performance. MemVir can be easily applied to many existing loss functions without any modification. Extensive experimental results on famous benchmarks demonstrate the superiority of MemVir over state-of-the-art competitors. Code of MemVir is publicly available 1 .},
  archive   = {C_ICCV},
  author    = {Byungsoo Ko and Geonmo Gu and Han-Gyu Kim},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01158},
  pages     = {11772-11781},
  title     = {Learning with memory-based virtual classes for deep metric learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Product1M: Towards weakly supervised instance-level product
retrieval via cross-modal pretraining. <em>ICCV</em>, 11762–11771. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Nowadays, customer’s demands for E-commerce are more diversified, which introduces more complications to the product retrieval industry. Previous methods are either subject to single-modal input or perform supervised image-level product retrieval, thus fail to accommodate real-life scenarios where enormous weakly annotated multi-modal data are present. In this paper, we investigate a more realistic setting that aims to perform weakly-supervised multi-modal instance-level product retrieval among fine-grained product categories. To promote the study of this challenging task, we contribute Product1M, one of the largest multi-modal cosmetic datasets for real-world instance-level retrieval. Notably, Product1M contains over 1 million image-caption pairs and consists of two sample types, i.e., single-product and multi-product samples, which encompass a wide variety of cosmetics brands. In addition to the great diversity, Product1M enjoys several appealing characteristics including fine-grained categories, complex combinations, and fuzzy correspondence that well mimic the real-world scenes. Moreover, we propose a novel model named Cross-modal contrAstive Product Transformer for instance-level prodUct REtrieval (CAPTURE), that excels in capturing the potential synergy between multi-modal inputs via a hybrid-stream transformer in a self-supervised manner. CAPTURE generates discriminative instance features via masked multi-modal learning as well as cross-modal contrastive pretraining and it outperforms several SOTA cross-modal baselines. Extensive ablation studies well demonstrate the effectiveness and the generalization capacity of our model. Dataset and codes are available at https: //github.com/zhanxlin/Product1M.},
  archive   = {C_ICCV},
  author    = {Xunlin Zhan and Yangxin Wu and Xiao Dong and Yunchao Wei and Minlong Lu and Yichi Zhang and Hang Xu and Xiaodan Liang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01157},
  pages     = {11762-11771},
  title     = {Product1M: Towards weakly supervised instance-level product retrieval via cross-modal pretraining},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DOLG: Single-stage image retrieval with deep orthogonal
fusion of local and global features. <em>ICCV</em>, 11752–11761. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Image Retrieval is a fundamental task of obtaining images similar to the query one from a database. A common image retrieval practice is to firstly retrieve candidate images via similarity search using global image features and then re-rank the candidates by leveraging their local features. Previous learning-based studies mainly focus on either global or local image representation learning to tackle the retrieval task. In this paper, we abandon the two-stage paradigm and seek to design an effective single-stage solution by integrating local and global information inside images into compact image representations. Specifically, we propose a Deep Orthogonal Local and Global (DOLG) information fusion framework for end-to-end image retrieval. It attentively extracts representative local information with multi-atrous convolutions and self-attention at first. Components orthogonal to the global image representation are then extracted from the local information. At last, the orthogonal components are concatenated with the global representation as a complementary, and then aggregation is performed to generate the final representation. The whole framework is end-to-end differentiable and can be trained with image-level labels. Extensive experimental results validate the effectiveness of our solution and show that our model achieves state-of-the-art image retrieval performances on Revisited Oxford and Paris datasets. 1},
  archive   = {C_ICCV},
  author    = {Min Yang and Dongliang He and Miao Fan and Baorong Shi and Xuetong Xue and Fu Li and Errui Ding and Jizhou Huang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01156},
  pages     = {11752-11761},
  title     = {DOLG: Single-stage image retrieval with deep orthogonal fusion of local and global features},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ranking models in unlabeled new environments. <em>ICCV</em>,
11741–11751. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Consider a scenario where we are supplied with a number of ready-to-use models trained on a certain source domain and hope to directly apply the most appropriate ones to different target domains based on the models’ relative performance. Ideally we should annotate a validation set for model performance assessment on each new target environment, but such annotations are often very expensive. Under this circumstance, we introduce the problem of ranking models in unlabeled new environments. For this problem, we propose to adopt a proxy dataset that 1) is fully labeled and 2) well reflects the true model rankings in a given target environment, and use the performance rankings on the proxy sets as surrogates. We first select labeled datasets as the proxy. Specifically, datasets that are more similar to the unlabeled target domain are found to better preserve the relative performance rankings. Motivated by this, we further propose to search the proxy set by sampling images from various datasets that have similar distributions as the target. We analyze the problem and its solutions on the person re-identification (re-ID) task, for which sufficient datasets are publicly available, and show that a carefully constructed proxy set effectively captures relative performance ranking in new environments. Code is avalible at https://github.com/sxzrt/Proxy-Set.},
  archive   = {C_ICCV},
  author    = {Xiaoxiao Sun and Yunzhong Hou and Weijian Deng and Hongdong Li and Liang Zheng},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01155},
  pages     = {11741-11751},
  title     = {Ranking models in unlabeled new environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving robustness of facial landmark detection by
defending against adversarial attacks. <em>ICCV</em>, 11731–11740. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many recent developments in facial landmark detection have been driven by stacking model parameters or augmenting annotations. However, three subsequent challenges remain, including 1) an increase in computational overhead, 2) the risk of overfitting caused by increasing model parameters, and 3) the burden of labor-intensive annotation by humans. We argue that exploring the weaknesses of the detector so as to remedy them is a promising method of robust facial landmark detection. To achieve this, we propose a sample-adaptive adversarial training (SAAT) approach to interactively optimize an attacker and a detector, which improves facial landmark detection as a defense against sample-adaptive black-box attacks. By leveraging adversarial attacks, the proposed SAAT exploits adversarial perturbations beyond the handcrafted transformations to improve the detector. Specifically, an attacker generates adversarial perturbations to reflect the weakness of the detector. Then, the detector must improve its robustness to adversarial perturbations to defend against adversarial attacks. Moreover, a sample-adaptive weight is designed to balance the risks and benefits of augmenting adversarial examples to train the detector. We also introduce a masked face alignment dataset, Masked-300W, to evaluate our method. Experiments show that our SAAT performed comparably to existing state-of-the-art methods. The dataset and model are publicly available at https://github.com/zhuccly/SAAT.},
  archive   = {C_ICCV},
  author    = {Congcong Zhu and Xiaoqiang Li and Jide Li and Songmin Dai},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01154},
  pages     = {11731-11740},
  title     = {Improving robustness of facial landmark detection by defending against adversarial attacks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online knowledge distillation for efficient pose estimation.
<em>ICCV</em>, 11720–11730. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing state-of-the-art human pose estimation methods require heavy computational resources for accurate predictions. One promising technique to obtain an accurate yet lightweight pose estimator is knowledge distillation, which distills the pose knowledge from a powerful teacher model to a less-parameterized student model. However, existing pose distillation works rely on a heavy pre-trained estimator to perform knowledge transfer and require a complex two-stage learning procedure. In this work, we investigate a novel Online Knowledge Distillation framework by distilling Human Pose structure knowledge in a one-stage manner to guarantee the distillation efficiency, termed OKDHP. Specifically, OKDHP trains a single multi-branch network and acquires the predicted heatmaps from each, which are then assembled by a Feature Aggregation Unit (FAU) as the target heatmaps to teach each branch in reverse. Instead of simply averaging the heatmaps, FAU which consists of multiple parallel transformations with different receptive fields, leverages the multi-scale information, thus obtains target heatmaps with higher-quality. Specifically, the pixel-wise Kullback-Leibler (KL) divergence is utilized to mini-mize the discrepancy between the target heatmaps and the predicted ones, which enables the student network to learn the implicit keypoint relationship. Besides, an unbalanced OKDHP scheme is introduced to customize the student networks with different compression rates. The effectiveness of our approach is demonstrated by extensive experiments on two common benchmark datasets, MPII and COCO.},
  archive   = {C_ICCV},
  author    = {Zheng Li and Jingwen Ye and Mingli Song and Ying Huang and Zhigeng Pan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01153},
  pages     = {11720-11730},
  title     = {Online knowledge distillation for efficient pose estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DensePose 3D: Lifting canonical surface maps of articulated
objects to the third dimension. <em>ICCV</em>, 11709–11719. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We tackle the problem of monocular 3D reconstruction of articulated objects like humans and animals. We contribute DensePose 3D, a method that can learn such reconstructions in a weakly supervised fashion from 2D image annotations only. This is in stark contrast with previous deformable reconstruction methods that use parametric models such as SMPL pre-trained on a large dataset of 3D object scans. Because it does not require 3D scans, DensePose 3D can be used for learning a wide range of articulated categories such as different animal species. The method learns, in an end-to-end fashion, a soft partition of a given category-specific 3D template mesh into rigid parts together with a monocular reconstruction network that predicts the part motions such that they reproject correctly onto 2D DensePose-like surface annotations of the object. The decomposition of the object into parts is regularized by expressing part assignments as a combination of the smooth eigenfunctions of the Laplace-Beltrami operator. We show significant improvements compared to state-of-the-art nonrigid structure-from-motion baselines on both synthetic and real data on categories of humans and animals.},
  archive   = {C_ICCV},
  author    = {Roman Shapovalov and David Novotny and Benjamin Graham and Patrick Labatut and Andrea Vedaldi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01152},
  pages     = {11709-11719},
  title     = {DensePose 3D: Lifting canonical surface maps of articulated objects to the third dimension},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Motion adaptive pose estimation from compressed videos.
<em>ICCV</em>, 11699–11708. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human pose estimation from videos has many real-world applications. Existing methods focus on applying models with a uniform computation profile on fully decoded frames, ignoring the freely-available motion signals and motion-compensation residuals from the compressed stream. A novel model, called Motion Adaptive Pose Net is proposed to exploit the compressed streams to efficiently decode pose sequences from videos. The model incorporates a Motion Compensated ConvLSTM to propagate the spatially aligned features, along with an adaptive gate to dynamically determine if the computationally expensive features should be extracted from fully decoded frames to compensate the motion-warped features, solely based on the residual errors. Leveraging the informative yet readily available signals from compressed streams, we propagate the latent features through our Motion Adaptive Pose Net efficiently Our model outperforms the state-of-the-art models in pose-estimation accuracy on two widely used datasets with only around half of the computation complexity.},
  archive   = {C_ICCV},
  author    = {Zhipeng Fan and Jun Liu and Yao Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01151},
  pages     = {11699-11708},
  title     = {Motion adaptive pose estimation from compressed videos},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural-GIF: Neural generalized implicit functions for
animating people in clothing. <em>ICCV</em>, 11688–11698. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present Neural Generalized Implicit Functions (Neural-GIF), to animate people in clothing as a function of the body pose. Given a sequence of scans of a subject in various poses, we learn to animate the character for new poses. Existing methods have relied on template-based representations of the human body (or clothing). However such models usually have fixed and limited resolutions, require difficult data pre-processing steps and cannot be used with complex clothing. We draw inspiration from template-based methods, which factorize motion into articulation and nonrigid deformation, but generalize this concept for implicit shape learning to obtain a more flexible model. We learn to map every point in the space to a canonical space, where a learned deformation field is applied to model non-rigid effects, before evaluating the signed distance field. Our formulation allows the learning of complex and non-rigid deformations of clothing and soft tissue, without computing a template registration as it is common with current approaches. Neural-GIF can be trained on raw 3D scans and reconstructs detailed complex surface geometry and deformations. Moreover, the model can generalize to new poses. We evaluate our method on a variety of characters from different public datasets in diverse clothing styles and show significant improvements over baseline methods, quantitatively and qualitatively. We also extend our model to multiple shape setting. To stimulate further research, we will make the model, code and data publicly available at [1].},
  archive   = {C_ICCV},
  author    = {Garvita Tiwari and Nikolaos Sarafianos and Tony Tung and Gerard Pons-Moll},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01150},
  pages     = {11688-11698},
  title     = {Neural-GIF: Neural generalized implicit functions for animating people in clothing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards accurate alignment in real-time 3D hand-mesh
reconstruction. <em>ICCV</em>, 11678–11687. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D hand-mesh reconstruction from RGB images facilitates many applications, including augmented reality (AR). However, this requires not only real-time speed and accurate hand pose and shape but also plausible mesh-image alignment. While existing works already achieve promising results, meeting all three requirements is very challenging. This paper presents a novel pipeline by decoupling the hand-mesh reconstruction task into three stages: a joint stage to predict hand joints and segmentation; a mesh stage to predict a rough hand mesh; and a refine stage to fine-tune it with an offset mesh for mesh-image alignment. With careful design in the network structure and in the loss functions, we can promote high-quality finger-level mesh-image alignment and drive the models together to deliver real-time predictions. Extensive quantitative and qualitative results on benchmark datasets demonstrate that the quality of our results outperforms the state-of-the-art methods on hand-mesh/pose precision and hand-image alignment. In the end, we also showcase several real-time AR scenarios.},
  archive   = {C_ICCV},
  author    = {Xiao Tang and Tianyu Wang and Chi-Wing Fu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01149},
  pages     = {11678-11687},
  title     = {Towards accurate alignment in real-time 3D hand-mesh reconstruction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Full-body motion from a single head-mounted device:
Generating SMPL poses from partial observations. <em>ICCV</em>,
11667–11677. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The increased availability and maturity of head-mounted and wearable devices opens up opportunities for remote communication and collaboration. However, the signal streams provided by these devices (e.g., head pose, hand pose, and gaze direction) do not represent a whole person. One of the main open problems is therefore how to leverage these signals to build faithful representations of the user. In this paper, we propose a method based on variational autoencoders to generate articulated poses of a human skeleton based on noisy streams of head and hand pose. Our approach relies on a model of pose likelihood that is novel and theoretically well-grounded. We demonstrate on publicly available datasets that our method is effective even from very impoverished signals and investigate how pose prediction can be made more accurate and realistic.},
  archive   = {C_ICCV},
  author    = {Andrea Dittadi and Sebastian Dziadzio and Darren Cosker and Ben Lundell and Tom Cashman and Jamie Shotton},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01148},
  pages     = {11667-11677},
  title     = {Full-body motion from a single head-mounted device: Generating SMPL poses from partial observations},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DECA: Deep viewpoint-equivariant human pose estimation using
capsule autoencoders. <em>ICCV</em>, 11657–11666. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human Pose Estimation (HPE) aims at retrieving the 3D position of human joints from images or videos. We show that current 3D HPE methods suffer a lack of viewpoint equivariance, namely they tend to fail or perform poorly when dealing with viewpoints unseen at training time. Deep learning methods often rely on either scale-invariant, translation-invariant, or rotation-invariant operations, such as max-pooling. However, the adoption of such procedures does not necessarily improve viewpoint generalization, rather leading to more data-dependent methods. To tackle this issue, we propose a novel capsule autoencoder network with fast Variational Bayes capsule routing, named DECA. By modeling each joint as a capsule entity, combined with the routing algorithm, our approach can preserve the joints’ hierarchical and geometrical structure in the feature space, independently from the viewpoint. By achieving viewpoint equivariance, we drastically reduce the network data dependency at training time, resulting in an improved ability to generalize for unseen viewpoints. In the experimental validation, we outperform other methods on depth images from both seen and unseen viewpoints, both top-view, and front-view. In the RGB domain, the same network gives state-of-the-art results on the challenging viewpoint transfer task, also establishing a new framework for top-view HPE. The code can be found at https://github.com/mmlab-cv/DECA.},
  archive   = {C_ICCV},
  author    = {Nicola Garau and Niccolò Bisagno and Piotr Bródka and Nicola Conci},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01147},
  pages     = {11657-11666},
  title     = {DECA: Deep viewpoint-equivariant human pose estimation using capsule autoencoders},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). TravelNet: Self-supervised physically plausible hand motion
learning from monocular color images. <em>ICCV</em>, 11646–11656. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper aims to reconstruct physically plausible hand motion from monocular color images. Existing frame-by-frame estimating approaches can not guarantee the physical plausibility (e.g. penetration, jittering) directly. In this paper, we embed physical constraints on the per-frame estimated motions in both spatial and temporal space. Our key idea is to adopt a self-supervised learning strategy to train a novel encoder-decoder, named TravelNet, whose training motion data is prepared by the physics engine using discrete pose states. TravelNet captures key pose states from hand motion sequences as compact motion descriptors, inspired by the concept of keyframes in animation. Finally, it manages to extract those key states out of perturbations without manual annotations, and reconstruct the motions preserving details and physical plausibility. In the experiments, we show that the outputs of the TravelNet contain both finger synergism and time consistency. Through the proposed framework, hand motions can be accurately reconstructed and flexibly re-edited, which is superior to the state-of-the-art methods.},
  archive   = {C_ICCV},
  author    = {Zimeng Zhao and Xi Zhao and Yangang Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01146},
  pages     = {11646-11656},
  title     = {TravelNet: Self-supervised physically plausible hand motion learning from monocular color images},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3D human pose estimation with spatial and temporal
transformers. <em>ICCV</em>, 11636–11645. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Transformer architectures have become the model of choice in natural language processing and are now being introduced into computer vision tasks such as image classification, object detection, and semantic segmentation. However, in the field of human pose estimation, convolutional architectures still remain dominant. In this work, we present PoseFormer, a purely transformer-based approach for 3D human pose estimation in videos without convolutional architectures involved. Inspired by recent developments in vision transformers, we design a spatial-temporal transformer structure to comprehensively model the human joint relations within each frame as well as the temporal correlations across frames, then output an accurate 3D human pose of the center frame. We quantitatively and qualitatively evaluate our method on two popular and standard benchmark datasets: Human3.6M and MPI-INF-3DHP. Extensive experiments show that PoseFormer achieves state-of-the-art performance on both datasets. Code is available at https://github.com/zczcwh/PoseFormer},
  archive   = {C_ICCV},
  author    = {Ce Zheng and Sijie Zhu and Matias Mendieta and Taojiannan Yang and Chen Chen and Zhengming Ding},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01145},
  pages     = {11636-11645},
  title     = {3D human pose estimation with spatial and temporal transformers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A unified 3D human motion synthesis model via conditional
variational auto-encoder. <em>ICCV</em>, 11625–11635. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a unified and flexible framework to address the generalized problem of 3D motion synthesis that covers the tasks of motion prediction, completion, interpolation, and spatial-temporal recovery. Since these tasks have different input constraints and various fidelity and diversity requirements, most existing approaches only cater to a specific task or use different architectures to address various tasks. Here we propose a unified framework based on Conditional Variational Auto-Encoder (CVAE), where we treat any arbitrary input as a masked motion series. Notably, by considering this problem as a conditional generation process, we estimate a parametric distribution of the missing regions based on the input conditions, from which to sample and synthesize the full motion series. To further allow the flexibility of manipulating the motion style of the generated series, we design an Action-Adaptive Modulation (AAM) to propagate the given semantic guidance through the whole sequence. We also introduce a cross-attention mechanism to exploit distant relations among decoder and encoder features for better realism and global consistency. We conducted extensive experiments on Human 3.6M and CMU-Mocap. The results show that our method produces coherent and realistic results for various motion synthesis tasks, with the synthesized motions distinctly adapted by the given action labels.},
  archive   = {C_ICCV},
  author    = {Yujun Cai and Yiwei Wang and Yiheng Zhu and Tat-Jen Cham and Jianfei Cai and Junsong Yuan and Jun Liu and Chuanxia Zheng and Sijie Yan and Henghui Ding and Xiaohui Shen and Ding Liu and Nadia Magnenat Thalmann},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01144},
  pages     = {11625-11635},
  title     = {A unified 3D human motion synthesis model via conditional variational auto-encoder},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural TMDlayer: Modeling instantaneous flow of features via
SDE generators. <em>ICCV</em>, 11615–11624. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study how stochastic differential equation (SDE) based ideas can inspire new modifications to existing algorithms for a set of problems in computer vision. Loosely speaking, our formulation is related to both explicit and implicit strategies for data augmentation and group equivariance, but is derived from new results in the SDE literature on estimating infinitesimal generators of a class of stochastic processes. If and when there is nominal agreement between the needs of an application/task and the inherent properties and behavior of the types of processes that we can efficiently handle, we obtain a very simple and efficient plug-in layer that can be incorporated within any existing network architecture, with minimal modification and only a few additional parameters. We show promising experiments on a number of vision tasks including few shot learning, point cloud transformers and deep variational segmentation obtaining efficiency or performance improvements.},
  archive   = {C_ICCV},
  author    = {Zihang Meng and Vikas Singh and Sathya N. Ravi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01143},
  pages     = {11615-11624},
  title     = {Neural TMDlayer: Modeling instantaneous flow of features via SDE generators},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021d). Self-supervised transfer learning for hand mesh recovery
from binocular images. <em>ICCV</em>, 11606–11614. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traditional methods for RGB hand mesh recovery usually need to train a separate model for each dataset with the corresponding ground truth and are hardly adapted to new scenarios without the ground truth for supervision. To address the problem, we propose a self-supervised framework for hand mesh estimation, where we pre-learn hand priors from existing hand datasets and transfer the priors to new scenarios without any landmark annotations. The proposed approach takes binocular images as input and mainly relies on left-right consistency constraints including appearance consensus and shape consistency to train the model to estimate the hand mesh in new scenarios. We conduct experiments on the widely used stereo hand dataset, and the experimental results verify that our model can get comparable performance compared with state-of-the-art methods even without the corresponding landmark annotations. To further evaluate our model, we collect a large real binocular dataset. The experimental results on the collected real dataset also verify the effectiveness of our model qualitatively.},
  archive   = {C_ICCV},
  author    = {Zheng Chen and Sihan Wang and Yi Sun and Xiaohong Ma},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01142},
  pages     = {11606-11614},
  title     = {Self-supervised transfer learning for hand mesh recovery from binocular images},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep virtual markers for articulated 3D shapes.
<em>ICCV</em>, 11595–11605. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose deep virtual markers, a framework for estimating dense and accurate positional information for various types of 3D data. We design a concept and construct a framework that maps 3D points of 3D articulated models, like humans, into virtual marker labels. To realize the framework, we adopt a sparse convolutional neural network and classify 3D points of an articulated model into virtual marker labels. We propose to use soft labels for the classifier to learn rich and dense interclass relationships based on geodesic distance. To measure the localization accuracy of the virtual markers, we test FAUST challenge, and our result outperforms the state-of-the-art. We also observe outstanding performance on the generalizability test, unseen data evaluation, and different 3D data types (meshes and depth maps). We show additional applications using the estimated virtual markers, such as non-rigid registration, texture transfer, and realtime dense marker prediction from depth maps.},
  archive   = {C_ICCV},
  author    = {Hyomin Kim and Jungeon Kim and Jaewon Kam and Jaesik Park and Seungyong Lee},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01141},
  pages     = {11595-11605},
  title     = {Deep virtual markers for articulated 3D shapes},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Probabilistic modeling for human mesh recovery.
<em>ICCV</em>, 11585–11594. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper focuses on the problem of 3D human reconstruction from 2D evidence. Although this is an inherently ambiguous problem, the majority of recent works avoid the uncertainty modeling and typically regress a single estimate for a given input. In contrast to that, in this work, we propose to embrace the reconstruction ambiguity and we recast the problem as learning a mapping from the input to a distribution of plausible 3D poses. Our approach is based on the normalizing flows model and offers a series of advantages. For conventional applications, where a single 3D estimate is required, our formulation allows for efficient mode computation. Using the mode leads to performance that is comparable with the state of the art among deterministic unimodal regression models. Simultaneously, since we have access to the likelihood of each sample, we demonstrate that our model is useful in a series of downstream tasks, where we leverage the probabilistic nature of the prediction as a tool for more accurate estimation. These tasks include reconstruction from multiple uncalibrated views, as well as human model fitting, where our model acts as a powerful image-based prior for mesh recovery. Our results validate the importance of probabilistic modeling, and indicate state-of-the-art performance across a variety of settings. Code and models are available at: https://www.seas.upenn.edu/~nkolot/projects/prohmr.},
  archive   = {C_ICCV},
  author    = {Nikos Kolotouros and Georgios Pavlakos and Dinesh Jayaraman and Kostas Daniilidis},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01140},
  pages     = {11585-11594},
  title     = {Probabilistic modeling for human mesh recovery},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SNARF: Differentiable forward skinning for animating
non-rigid neural implicit shapes. <em>ICCV</em>, 11574–11584. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Neural implicit surface representations have emerged as a promising paradigm to capture 3D shapes in a continuous and resolution-independent manner. However, adapting them to articulated shapes is non-trivial. Existing approaches learn a backward warp field that maps deformed to canonical points. However, this is problematic since the backward warp field is pose dependent and thus requires large amounts of data to learn. To address this, we introduce SNARF, which combines the advantages of linear blend skinning (LBS) for polygonal meshes with those of neural implicit surfaces by learning a forward deformation field without direct supervision. This deformation field is defined in canonical, pose-independent, space, enabling generalization to unseen poses. Learning the deformation field from posed meshes alone is challenging since the correspondences of deformed points are defined implicitly and may not be unique under changes of topology. We propose a forward skinning model that finds all canonical correspondences of any deformed point using iterative root finding. We derive analytical gradients via implicit differentiation, enabling end-to-end training from 3D meshes with bone transformations. Compared to state-of-the-art neural implicit representations, our approach generalizes better to unseen poses while preserving accuracy. We demonstrate our method in challenging scenarios on (clothed) 3D humans in diverse and unseen poses.},
  archive   = {C_ICCV},
  author    = {Xu Chen and Yufeng Zheng and Michael J. Black and Otmar Hilliges and Andreas Geiger},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01139},
  pages     = {11574-11584},
  title     = {SNARF: Differentiable forward skinning for animating non-rigid neural implicit shapes},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TeachText: CrossModal generalized distillation for
text-video retrieval. <em>ICCV</em>, 11563–11573. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, considerable progress on the task of text-video retrieval has been achieved by leveraging large-scale pretraining on visual and audio datasets to construct powerful video encoders. By contrast, despite the natural symmetry, the design of effective algorithms for exploiting large-scale language pretraining remains under-explored. In this work, we are the first to investigate the design of such algorithms and propose a novel generalized distillation method, TeachText, which leverages complementary cues from multiple text encoders to provide an enhanced supervisory signal to the retrieval model. Moreover, we extend our method to video side modalities and show that we can effectively reduce the number of used modalities at test time without compromising performance. Our approach advances the state of the art on several video retrieval benchmarks by a significant margin and adds no computational overhead at test time. Last but not least, we show an effective application of our method for eliminating noise from retrieval datasets. Code and data can be found at https://www.robots.ox.ac.uk/˜vgg/research/teachtext/.},
  archive   = {C_ICCV},
  author    = {Ioana Croitoru and Simion-Vlad Bogolin and Marius Leordeanu and Hailin Jin and Andrew Zisserman and Samuel Albanie and Yang Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01138},
  pages     = {11563-11573},
  title     = {TeachText: CrossModal generalized distillation for text-video retrieval},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Support-set based cross-supervision for video grounding.
<em>ICCV</em>, 11553–11562. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current approaches for video grounding propose kinds of complex architectures to capture the video-text relations, and have achieved impressive improvements. However, it is hard to learn the complicated multi-modal relations by only architecture designing in fact. In this paper, we introduce a novel Support-set Based Cross-Supervision (Sscs) module which can improve existing methods during training phase without extra inference cost. The proposed Sscs module contains two main components, i.e., discriminative contrastive objective and generative caption objective. The contrastive objective aims to learn effective representations by contrastive learning, while the caption objective can train a powerful video encoder supervised by texts. Due to the co-existence of some visual entities in both ground-truth and background intervals, i.e. mutual exclusion, naively contrastive learning is unsuitable to video grounding. We address the problem by boosting the cross-supervision with the support-set concept, which collects visual information from the whole video and eliminates the mutual exclusion of entities. Combined with the original objectives, Sscs can enhance the abilities of multi-modal relation modeling for existing approaches. We extensively evaluate Sscs on three challenging datasets, and show that our method can improve current state-of-the-art methods by large margins, especially 6.35\% in terms of R1@0.5 on Charades-STA.},
  archive   = {C_ICCV},
  author    = {Xinpeng Ding and Nannan Wang and Shiwei Zhang and De Cheng and Xiaomeng Li and Ziyuan Huang and Mingqian Tang and Xinbo Gao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01137},
  pages     = {11553-11562},
  title     = {Support-set based cross-supervision for video grounding},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TACo: Token-aware cascade contrastive learning for
video-text alignment. <em>ICCV</em>, 11542–11552. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Contrastive learning has been widely used to train transformer-based vision-language models for video-text alignment and multi-modal representation learning. This paper presents a new algorithm called Token-Aware Cascade contrastive learning (TACo) that improves contrastive learning using two novel techniques. The first is the token-aware contrastive loss which is computed by taking into account the syntactic classes of words. This is motivated by the observation that for a video-text pair, the content words in the text, such as nouns and verbs, are more likely to be aligned with the visual contents in the video than the function words. Second, a cascade sampling method is applied to generate a small set of hard negative examples for efficient loss estimation for multi-modal fusion layers. To validate the effectiveness of TACo, in our experiments we finetune pretrained models for a set of downstream tasks including text-video retrieval (YouCook2, MSR-VTT and ActivityNet), video action step localization (CrossTask), video action segmentation (COIN). The results show that our models attain consistent improvements across different experimental settings over previous methods, set-ting new state-of-the-art on three public text-video retrieval benchmarks of YouCook2, MSR-VTT and ActivityNet.},
  archive   = {C_ICCV},
  author    = {Jianwei Yang and Yonatan Bisk and Jianfeng Gao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01136},
  pages     = {11542-11552},
  title     = {TACo: Token-aware cascade contrastive learning for video-text alignment},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Aligning subtitles in sign language videos. <em>ICCV</em>,
11532–11541. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The goal of this work is to temporally align asynchronous subtitles in sign language videos. In particular, we focus on sign-language interpreted TV broadcast data comprising (i) a video of continuous signing, and (ii) subtitles corresponding to the audio content. Previous work exploiting such weakly-aligned data only considered finding keyword-sign correspondences, whereas we aim to localise a complete subtitle text in continuous signing. We propose a Transformer architecture tailored for this task, which we train on manually annotated alignments covering over 15K subtitles that span 17.7 hours of video. We use BERT subtitle embeddings and CNN video representations learned for sign recognition to encode the two signals, which interact through a series of attention layers. Our model outputs frame-level predictions, i.e., for each video frame, whether it belongs to the queried subtitle or not. Through extensive evaluations, we show substantial improvements over existing alignment baselines that do not make use of subtitle text embeddings for learning. Our automatic alignment model opens up possibilities for advancing machine translation of sign languages via providing continuously synchronized video-text data.},
  archive   = {C_ICCV},
  author    = {Hannah Bull and Triantafyllos Afouras and Gül Varol and Samuel Albanie and Liliane Momeni and Andrew Zisserman},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01135},
  pages     = {11532-11541},
  title     = {Aligning subtitles in sign language videos},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual alignment constraint for continuous sign language
recognition. <em>ICCV</em>, 11522–11531. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vision-based Continuous Sign Language Recognition (CSLR) aims to recognize unsegmented signs from image streams. Overfitting is one of the most critical problems in CSLR training, and previous works show that the iterative training scheme can partially solve this problem while also costing more training time. In this study, we revisit the iterative training scheme in recent CSLR works and realize that sufficient training of the feature extractor is critical to solving the overfitting problem. Therefore, we propose a Visual Alignment Constraint (VAC) to enhance the feature extractor with alignment supervision. Specifically, the proposed VAC comprises two auxiliary losses: one focuses on visual features only, and the other enforces prediction alignment between the feature extractor and the alignment module. Moreover, we propose two metrics to reflect overfitting by measuring the prediction inconsistency between the feature extractor and the alignment module. Experimental results on two challenging CSLR datasets show that the proposed VAC makes CSLR networks end-to-end trainable and achieves competitive performance.},
  archive   = {C_ICCV},
  author    = {Yuecong Min and Aiming Hao and Xiujuan Chai and Xilin Chen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01134},
  pages     = {11522-11531},
  title     = {Visual alignment constraint for continuous sign language recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Physics-based human motion estimation and synthesis from
videos. <em>ICCV</em>, 11512–11521. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human motion synthesis is an important problem with applications in graphics, gaming and simulation environments for robotics. Existing methods require accurate motion capture data for training, which is costly to obtain. Instead, we propose a framework for training generative models of physically plausible human motion directly from monocular RGB videos, which are much more widely available. At the core of our method is a novel optimization formulation that corrects imperfect image-based pose estimations by enforcing physics constraints and reasons about contacts in a differentiable way. This optimization yields corrected 3D poses and motions, as well as their corresponding contact forces. Results show that our physically-corrected motions significantly outperform prior work on pose estimation. We can then use these to train a generative model to synthesize future motion. We demonstrate both qualitatively and quantitatively significantly improved motion estimation, synthesis quality and physical plausibility achieved by our method on the large scale Human3.6m dataset [12] as compared to prior kinematic and physics-based methods. By enabling learning of motion synthesis from video, our method paves the way for large-scale, realistic and diverse motion synthesis.},
  archive   = {C_ICCV},
  author    = {Kevin Xie and Tingwu Wang and Umar Iqbal and Yunrong Guo and Sanja Fidler and Florian Shkurti},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01133},
  pages     = {11512-11521},
  title     = {Physics-based human motion estimation and synthesis from videos},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Normalized human pose features for human action video
alignment. <em>ICCV</em>, 11501–11511. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel approach for extracting human pose features from human action videos. The goal is to let the pose features capture only the poses of the action while being invariant to other factors, including video back-grounds, the video subjects’ anthropometric characteristics and viewpoints. Such human pose features facilitate the comparison of pose similarity and can be used for down-stream tasks, such as human action video alignment and pose retrieval. The key to our approach is to first normalize the poses in the video frames by mapping the poses onto a pre-defined 3D skeleton to not only disentangle subject physical features, such as bone lengths and ratios, but also to unify global orientations of the poses. Then the normalized poses are mapped to a pose embedding space of high-level features, learned via unsupervised metric learning. We evaluate the effectiveness of our normalized features both qualitatively by visualizations, and quantitatively by a video alignment task on the Human3.6M dataset and an action recognition task on the Penn Action dataset.},
  archive   = {C_ICCV},
  author    = {Jingyuan Liu and Mingyi Shi and Qifeng Chen and Hongbo Fu and Chiew-Lan Tai},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01132},
  pages     = {11501-11511},
  title     = {Normalized human pose features for human action video alignment},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EM-POSE: 3D human pose estimation from sparse
electromagnetic trackers. <em>ICCV</em>, 11490–11500. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fully immersive experiences in AR/VR depend on re-constructing the full body pose of the user without restricting their motion. In this paper we study the use of body-worn electromagnetic (EM) field-based sensing for the task of 3D human pose reconstruction. To this end, we present a method to estimate SMPL parameters from 6-12 EM sensors. We leverage a customized wearable system consisting of wireless EM sensors measuring time-synchronized 6D poses at 120 Hz. To provide accurate poses even with little user instrumentation, we adopt a recently proposed hybrid framework, learned gradient descent (LGD), to iteratively estimate SMPL pose and shape from our input measurements. This allows us to harness powerful pose priors to cope with the idiosyncrasies of the input data and achieve accurate pose estimates. The proposed method uses AMASS to synthesize virtual EM-sensor data and we show that it generalizes well to a newly captured real dataset consisting of a total of 36 minutes of motion from 5 subjects. We achieve reconstruction errors as low as 31.8 mm and 13.3 degrees, outperforming both pure learning- and pure optimization-based methods. Code and data is available under https://ait.ethz.ch/projects/2021/em-pose.},
  archive   = {C_ICCV},
  author    = {Manuel Kaufmann and Yi Zhao and Chengcheng Tang and Lingling Tao and Christopher Twigg and Jie Song and Robert Wang and Otmar Hilliges},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01131},
  pages     = {11490-11500},
  title     = {EM-POSE: 3D human pose estimation from sparse electromagnetic trackers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Estimating egocentric 3D human pose in global space.
<em>ICCV</em>, 11480–11489. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Egocentric 3D human pose estimation using a single fisheye camera has become popular recently as it allows capturing a wide range of daily activities in unconstrained environments, which is difficult for traditional outside-in motion capture with external cameras. However, existing methods have several limitations. A prominent problem is that the estimated poses lie in the local coordinate system of the fisheye camera, rather than in the world coordinate system, which is restrictive for many applications. Furthermore, these methods suffer from limited accuracy and temporal instability due to ambiguities caused by the monocular setup and the severe occlusion in a strongly distorted egocentric perspective. To tackle these limitations, we present a new method for egocentric global 3D body pose estimation using a single head-mounted fish-eye camera. To achieve accurate and temporally stable global poses, a spatio-temporal optimization is performed over a sequence of frames by minimizing heatmap reprojection errors and enforcing local and global body motion priors learned from a mocap dataset. Experimental results show that our approach outperforms state-of-the-art methods both quantitatively and qualitatively.},
  archive   = {C_ICCV},
  author    = {Jian Wang and Lingjie Liu and Weipeng Xu and Kripasindhu Sarkar and Christian Theobalt},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01130},
  pages     = {11480-11489},
  title     = {Estimating egocentric 3D human pose in global space},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HuMoR: 3D human motion model for robust pose estimation.
<em>ICCV</em>, 11468–11479. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce HuMoR: a 3D Human Motion Model for Robust Estimation of temporal pose and shape. Though substantial progress has been made in estimating 3D human motion and shape from dynamic observations, recovering plausible pose sequences in the presence of noise and occlusions remains a challenge. For this purpose, we propose an expressive generative model in the form of a conditional variational autoencoder, which learns a distribution of the change in pose at each step of a motion sequence. Furthermore, we introduce a flexible optimization-based approach that leverages HuMoR as a motion prior to robustly estimate plausible pose and shape from ambiguous observations. Through extensive evaluations, we demonstrate that our model generalizes to diverse motions and body shapes after training on a large motion capture dataset, and enables motion reconstruction from multiple input modalities including 3D keypoints and RGB(-D) videos. See the project page at geometry.stanford.edu/projects/humor.},
  archive   = {C_ICCV},
  author    = {Davis Rempe and Tolga Birdal and Aaron Hertzmann and Jimei Yang and Srinath Sridhar and Leonidas J. Guibas},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01129},
  pages     = {11468-11479},
  title     = {HuMoR: 3D human motion model for robust pose estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modulated graph convolutional network for 3D human pose
estimation. <em>ICCV</em>, 11457–11467. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The graph convolutional network (GCN) has recently achieved promising performance of 3D human pose estimation (HPE) by modeling the relationship among body parts. However, most prior GCN approaches suffer from two main drawbacks. First, they share a feature transformation for each node within a graph convolution layer. This prevents them from learning different relations between different body joints. Second, the graph is usually defined according to the human skeleton and is suboptimal because human activities often exhibit motion patterns beyond the natural connections of body joints. To address these limitations, we introduce a novel Modulated GCN for 3D HPE. It consists of two main components: weight modulation and affinity modulation. Weight modulation learns different modulation vectors for different nodes so that the feature transformations of different nodes are disentangled while retaining a small model size. Affinity modulation adjusts the graph structure in a GCN so that it can model additional edges beyond the human skeleton. We investigate several affinity modulation methods as well as the impact of regularizations. Rigorous ablation study indicates both types of modulation improve performance with negligible overhead. Compared with state-of-the-art GCNs for 3D HPE, our approach either significantly reduces the estimation errors, e.g., by around 10\%, while retaining a small model size or drastically reduces the model size, e.g., from 4.22M to 0.29M (a 14.5× reduction), while achieving comparable performance. Results on two benchmarks show our Modulated GCN outperforms some recent states of the art. Our code is available at https://github.com/ZhimingZo/Modulated-GCN.},
  archive   = {C_ICCV},
  author    = {Zhiming Zou and Wei Tang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01128},
  pages     = {11457-11467},
  title     = {Modulated graph convolutional network for 3D human pose estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MSR-GCN: Multi-scale residual graph convolution networks for
human motion prediction. <em>ICCV</em>, 11447–11456. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human motion prediction is a challenging task due to the stochasticity and aperiodicity of future poses. Recently, graph convolutional network has been proven to be very effective to learn dynamic relations among pose joints, which is helpful for pose prediction. On the other hand, one can abstract a human pose recursively to obtain a set of poses at multiple scales. With the increase of the abstraction level, the motion of the pose becomes more stable, which benefits pose prediction too. In this paper, we propose a novel Multi-Scale Residual Graph Convolution Network (MSR-GCN) for human pose prediction task in the manner of end-to-end. The GCNs are used to extract features from fine to coarse scale and then from coarse to fine scale. The extracted features at each scale are then combined and decoded to obtain the residuals between the input and target poses. Intermediate supervisions are imposed on all the predicted poses, which enforces the network to learn more representative features. Our proposed approach is evaluated on two standard benchmark datasets, i.e., the Human3.6M dataset and the CMU Mocap dataset. Experimental results demonstrate that our method outperforms the state-of-the-art approaches. Code and pre-trained models are available at https://github.com/Droliven/MSRGCN.},
  archive   = {C_ICCV},
  author    = {Lingwei Dang and Yongwei Nie and Chengjiang Long and Qing Zhang and Guiqing Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01127},
  pages     = {11447-11456},
  title     = {MSR-GCN: Multi-scale residual graph convolution networks for human motion prediction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Revitalizing optimization for 3D human pose and shape
estimation: A sparse constrained formulation. <em>ICCV</em>,
11437–11446. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel sparse constrained formulation and from it derive a real-time optimization method for 3D human pose and shape estimation. Our optimization method, SCOPE (Sparse Constrained Optimization for 3D human Pose and shapE estimation), is orders of magnitude faster (avg. 4ms convergence) than existing optimization methods, while being mathematically equivalent to their dense unconstrained formulation under mild assumptions. We achieve this by exploiting the underlying sparsity and constraints of our formulation to efficiently compute the Gauss-Newton direction. We show that this computation scales linearly with the number of joints and measurements of a complex 3D human model, in contrast to prior work where it scales cubically due to their dense unconstrained formulation. Based on our optimization method, we present a real-time motion capture framework that estimates 3D human poses and shapes from a single image at over 30 FPS. In benchmarks against state-of-the-art methods on multiple public datasets, our framework outperforms other optimization methods and achieves competitive accuracy against regression methods. Project page with code and videos: https://sites.google.com/view/scope-human/.},
  archive   = {C_ICCV},
  author    = {Taosha Fan and Kalyan Vasudev Alwala and Donglai Xiang and Weipeng Xu and Todd Murphey and Mustafa Mukadam},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01126},
  pages     = {11437-11446},
  title     = {Revitalizing optimization for 3D human pose and shape estimation: A sparse constrained formulation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PyMAF: 3D human pose and shape regression with pyramidal
mesh alignment feedback loop. <em>ICCV</em>, 11426–11436. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Regression-based methods have recently shown promising results in reconstructing human meshes from monocular images. By directly mapping raw pixels to model parameters, these methods can produce parametric models in a feed-forward manner via neural networks. However, minor deviation in parameters may lead to noticeable mis-alignment between the estimated meshes and image evidences. To address this issue, we propose a Pyramidal Mesh Alignment Feedback (PyMAF) loop to leverage a feature pyramid and rectify the predicted parameters explicitly based on the mesh-image alignment status in our deep regressor. In PyMAF, given the currently predicted parameters, mesh-aligned evidences will be extracted from finer-resolution features accordingly and fed back for parameter rectification. To reduce noise and enhance the reliability of these evidences, an auxiliary pixel-wise supervision is imposed on the feature encoder, which provides mesh-image correspondence guidance for our network to preserve the most related information in spatial features. The efficacy of our approach is validated on several benchmarks, including Human3.6M, 3DPW, LSP, and COCO, where experimental results show that our approach consistently improves the mesh-image alignment of the reconstruction. The project page with code and video results can be found at https://hongwenzhang.github.io/pymaf.},
  archive   = {C_ICCV},
  author    = {Hongwen Zhang and Yating Tian and Xinchi Zhou and Wanli Ouyang and Yebin Liu and Limin Wang and Zhenan Sun},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01125},
  pages     = {11426-11436},
  title     = {PyMAF: 3D human pose and shape regression with pyramidal mesh alignment feedback loop},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning skeletal graph neural networks for hard 3D pose
estimation. <em>ICCV</em>, 11416–11425. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Various deep learning techniques have been proposed to solve the single-view 2D-to-3D pose estimation problem. While the average prediction accuracy has been improved significantly over the years, the performance on hard poses with depth ambiguity, self-occlusion, and complex or rare poses is still far from satisfactory. In this work, we target these hard poses and present a novel skeletal GNN learning solution. To be specific, we propose a hop-aware hierarchical channel-squeezing fusion layer to effectively extract relevant information from neighboring nodes while suppressing undesired noises in GNN learning. In addition, we propose a temporal-aware dynamic graph construction procedure that is robust and effective for 3D pose estimation. Experimental results on the Human3.6M dataset show that our solution achieves 10.3\% average prediction accuracy improvement and greatly improves on hard poses over state-of-the-art techniques. We further apply the proposed technique on the skeleton-based action recognition task and also achieve state-of-the-art performance. Our code is available at https://github.com/ailingzengzzz/Skeletal-GNN.},
  archive   = {C_ICCV},
  author    = {Ailing Zeng and Xiao Sun and Lei Yang and Nanxuan Zhao and Minhao Liu and Qiang Xu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01124},
  pages     = {11416-11425},
  title     = {Learning skeletal graph neural networks for hard 3D pose estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cherry-picking gradients: Learning low-rank embeddings of
visual data via differentiable cross-approximation. <em>ICCV</em>,
11406–11415. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose an end-to-end trainable framework that processes large-scale visual data tensors by looking at a fraction of their entries only. Our method combines a neural network encoder with a tensor train decomposition to learn a low-rank latent encoding, coupled with cross-approximation (CA) to learn the representation through a subset of the original samples. CA is an adaptive sampling algorithm that is native to tensor decompositions and avoids working with the full high-resolution data explicitly. Instead, it actively selects local representative samples that we fetch out-of-core and on demand. The required number of samples grows only logarithmically with the size of the input. Our implicit representation of the tensor in the network enables processing large grids that could not be otherwise tractable in their uncompressed form. The proposed approach is particularly useful for large-scale multidimensional grid data (e.g., 3D tomography), and for tasks that require context over a large receptive field (e.g., predicting the medical condition of entire organs). The code is available at https://github.com/aelphy/c-pic.},
  archive   = {C_ICCV},
  author    = {Mikhail Usvyatsov and Anastasia Makarova and Rafael Ballester-Ripoll and Maxim Rakhuba and Andreas Krause and Konrad Schindler},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01123},
  pages     = {11406-11415},
  title     = {Cherry-picking gradients: Learning low-rank embeddings of visual data via differentiable cross-approximation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Learning deep local features with multiple dynamic
attentions for large-scale image retrieval. <em>ICCV</em>, 11396–11405.
(<a href="https://doi.org/10.1109/ICCV48922.2021.01122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In image retrieval, learning local features with deep convolutional networks has been demonstrated effective to improve the performance. To discriminate deep local features, some research efforts turn to attention learning. However, existing attention-based methods only generate a single attention map for each image, which limits the exploration of diverse visual patterns. To this end, we propose a novel deep local feature learning architecture to simultaneously focus on multiple discriminative local patterns in an image. In our framework, we first adaptively reorganize the channels of activation maps for multiple heads. For each head, a new dynamic attention module is designed to learn the potential attentions. The whole architecture is trained as metric learning of weighted-sum-pooled global image features, with only image-level relevance label. After the architecture training, for each database image, we select local features based on their multi-head dynamic attentions, which are further indexed for efficient retrieval. Extensive experiments show the proposed method outperforms the state-of-the-art methods on the Revisited Oxford and Paris datasets. Besides, it typically achieves competitive results even using local features with lower dimensions. Code will be released at https://github.com/CHANWH/MDA.},
  archive   = {C_ICCV},
  author    = {Hui Wu and Min Wang and Wengang Zhou and Houqiang Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01122},
  pages     = {11396-11405},
  title     = {Learning deep local features with multiple dynamic attentions for large-scale image retrieval},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Single image 3D shape retrieval via cross-modal instance and
category contrastive learning. <em>ICCV</em>, 11385–11395. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we tackle the problem of single image-based 3D shape retrieval (IBSR), where we seek to find the most matched shape of a given single 2D image from a shape repository. Most of the existing works learn to embed 2D images and 3D shapes into a common feature space and perform metric learning using a triplet loss. Inspired by the great success in recent contrastive learning works on self-supervised representation learning, we propose a novel IBSR pipeline leveraging contrastive learning. We note that adopting such cross-modal contrastive learning between 2D images and 3D shapes into IBSR tasks is non-trivial and challenging: contrastive learning requires very strong data augmentation in constructed positive pairs to learn the feature invariance, whereas traditional metric learning works do not have this requirement. Moreover, object shape and appearance are entangled in 2D query images, thus making the learning task more difficult than contrasting single-modal data. To mitigate the challenges, we propose to use multi-view grayscale rendered images from the 3D shapes as a shape representation. We then introduce a strong data augmentation technique based on color transfer, which can significantly but naturally change the appearance of the query image, effectively satisfying the need for contrastive learning. Finally, we propose to incorporate a novel category-level contrastive loss that helps distinguish similar objects from different categories, in addition to classic instance-level contrastive loss. Our experiments demonstrate that our approach achieves the best performance on all the three popular IBSR benchmarks, including Pix3D, Stanford Cars, and Comp Cars, outperforming the previous state-of-the-art from 4\% - 15\% on retrieval accuracy.},
  archive   = {C_ICCV},
  author    = {Ming-Xian Lin and Jie Yang and He Wang and Yu-Kun Lai and Rongfei Jia and Binqiang Zhao and Lin Gao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01121},
  pages     = {11385-11395},
  title     = {Single image 3D shape retrieval via cross-modal instance and category contrastive learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weakly supervised text-based person re-identification.
<em>ICCV</em>, 11375–11384. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The conventional text-based person re-identification methods heavily rely on identity annotations. However, this labeling process is costly and time-consuming. In this paper, we consider a more practical setting called weakly supervised text-based person re-identification, where only the text-image pairs are available without the requirement of annotating identities during the training phase. To this end, we propose a Cross-Modal Mutual Training (CMMT) framework. Specifically, to alleviate the intra-class variations, a clustering method is utilized to generate pseudo labels for both visual and textual instances. To further re-fine the clustering results, CMMT provides a Mutual Pseudo Label Refinement module, which leverages the clustering results in one modality to refine that in the other modality constrained by the text-image pairwise relationship. Mean-while, CMMT introduces a Text-IoU Guided Cross-Modal Projection Matching loss to resolve the cross-modal matching ambiguity problem. A Text-IoU Guided Hard Sample Mining method is also proposed for learning discriminative textual-visual joint embeddings. We conduct extensive experiments to demonstrate the effectiveness of the proposed CMMT, and the results show that CMMT performs favorably against existing text-based person re-identification methods. Our code will be available at https://github.com/X-BrainLab/WS_Text-ReID.},
  archive   = {C_ICCV},
  author    = {Shizhen Zhao and Changxin Gao and Yuanjie Shao and Wei-Shi Zheng and Nong Sang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01120},
  pages     = {11375-11384},
  title     = {Weakly supervised text-based person re-identification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural architecture search for joint human parsing and pose
estimation. <em>ICCV</em>, 11365–11374. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human parsing and pose estimation are crucial for the understanding of human behaviors. Since these tasks are closely related, employing one unified model to perform two tasks simultaneously allows them to benefit from each other. However, since human parsing is a pixel-wise classification process while pose estimation is usually a regression task, it is non-trivial to extract discriminative features for both tasks while modeling their correlation in the joint learning fashion. Recent studies have shown that Neural Architecture Search (NAS) has the ability to allocate efficient feature connections for specific tasks automatically. With the spirit of NAS, we propose to search for an efficient network architecture (NPPNet) to tackle two tasks at the same time. On the one hand, to extract task-specific features for the two tasks and lay the foundation for the further searching of feature interaction, we propose to search their encoder-decoder architectures, respectively. On the other hand, to ensure two tasks fully communicate with each other, we propose to embed NAS units in both multi-scale feature interaction and high-level feature fusion to establish optimal connections between two tasks. Experimental results on both parsing and pose estimation benchmark datasets have demonstrated that the searched model achieves state-of-the-art performances on both tasks. 1},
  archive   = {C_ICCV},
  author    = {Dan Zeng and Yuhang Huang and Qian Bao and Junjie Zhang and Chi Su and Wu Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01119},
  pages     = {11365-11374},
  title     = {Neural architecture search for joint human parsing and pose estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stochastic scene-aware motion prediction. <em>ICCV</em>,
11354–11364. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A long-standing goal in computer vision is to capture, model, and realistically synthesize human behavior. Specifically, by learning from data, our goal is to enable virtual humans to navigate within cluttered indoor scenes and naturally interact with objects. Such embodied behavior has applications in virtual reality, computer games, and robotics, while synthesized behavior can be used as training data. The problem is challenging because real human motion is diverse and adapts to the scene. For example, a person can sit or lie on a sofa in many places and with varying styles. We must model this diversity to synthesize virtual humans that realistically perform human-scene interactions. We present a novel data-driven, stochastic motion synthesis method that models different styles of performing a given action with a target object. Our Scene-Aware Motion Prediction method (SAMP) generalizes to target objects of various geometries while enabling the character to navigate in cluttered scenes. To train SAMP, we collected MoCap data covering various sitting, lying down, walking, and running styles. We demonstrate SAMP on complex indoor scenes and achieve superior performance than existing solutions. Code and data are available for research at https://samp.is.tue.mpg.de.},
  archive   = {C_ICCV},
  author    = {Mohamed Hassan and Duygu Ceylan and Ruben Villegas and Jun Saito and Jimei Yang and Yi Zhou and Michael Black},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01118},
  pages     = {11354-11364},
  title     = {Stochastic scene-aware motion prediction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SemiHand: Semi-supervised hand pose estimation with
consistency. <em>ICCV</em>, 11344–11353. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present SemiHand, a semi-supervised framework for 3D hand pose estimation from monocular images. We pre-train the model on labelled synthetic data and fine-tune it on unlabelled real-world data by pseudo-labeling with consistency training. By design, we introduce data augmentation of differing difficulties, consistency regularizer, label correction and sample selection for RGB-based 3D hand pose estimation. In particular, by approximating the hand masks from hand poses, we propose a cross-modal consistency and leverage semantic predictions to guide the predicted poses. Meanwhile, we introduce pose registration as label correction to guarantee the biomechanical feasibility of hand bone lengths. Experiments show that our method achieves a favorable improvement on real-world datasets after fine-tuning.},
  archive   = {C_ICCV},
  author    = {Linlin Yang and Shicheng Chen and Angela Yao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01117},
  pages     = {11344-11353},
  title     = {SemiHand: Semi-supervised hand pose estimation with consistency},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interacting two-hand 3D pose and shape reconstruction from
single color image. <em>ICCV</em>, 11334–11343. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel deep learning framework to reconstruct 3D hand poses and shapes of two interacting hands from a single color image. Previous methods designed for single hand cannot be easily applied for the two hand scenario because of the heavy inter-hand occlusion and larger solution space. In order to address the occlusion and similar appearance between hands that may confuse the network, we design a hand pose-aware attention module to extract features associated to each individual hand respectively. We then leverage the two hand context presented in interaction to propose a context-aware cascaded refinement that improves the hand pose and shape accuracy of each hand conditioned on the context between interacting hands. Extensive experiments on the main benchmark datasets demonstrate that our method predicts accurate 3D hand pose and shape from single color image, and achieves the state-of-the-art performance. Code is available in project webpage https://baowenz.github.io/Intershape/.},
  archive   = {C_ICCV},
  author    = {Baowen Zhang and Yangang Wang and Xiaoming Deng and Yinda Zhang and Ping Tan and Cuixia Ma and Hongan Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01116},
  pages     = {11334-11343},
  title     = {Interacting two-hand 3D pose and shape reconstruction from single color image},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Learning motion priors for 4D human body capture in 3D
scenes. <em>ICCV</em>, 11323–11333. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recovering high-quality 3D human motion in complex scenes from monocular videos is important for many applications, ranging from AR/VR to robotics. However, capturing realistic human-scene interactions, while dealing with occlusions and partial views, is challenging; current approaches are still far from achieving compelling results. We address this problem by proposing LEMO: LEarning human MOtion priors for 4D human body capture. By leveraging the large-scale motion capture dataset AMASS [38], we introduce a novel motion smoothness prior, which strongly reduces the jitters exhibited by poses recovered over a sequence. Furthermore, to handle contacts and occlusions occurring frequently in body-scene interactions, we design a contact friction term and a contact-aware motion infiller obtained via per-instance self-supervised training. To prove the effectiveness of the proposed motion priors, we combine them into a novel pipeline for 4D human body capture in 3D scenes. With our pipeline, we demonstrate high-quality 4D human body capture, reconstructing smooth motions and physically plausible body-scene interactions. The code and data are available at https://sanweiliti.github.io/LEMO/LEMO.html.},
  archive   = {C_ICCV},
  author    = {Siwei Zhang and Yan Zhang and Federica Bogo and Marc Pollefeys and Siyu Tang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01115},
  pages     = {11323-11333},
  title     = {Learning motion priors for 4D human body capture in 3D scenes},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Contextually plausible and diverse 3D human motion
prediction. <em>ICCV</em>, 11313–11322. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We tackle the task of diverse 3D human motion prediction, that is, forecasting multiple plausible future 3D poses given a sequence of observed 3D poses. In this context, a popular approach consists of using a Conditional Variational Autoencoder (CVAE). However, existing approaches that do so either fail to capture the diversity in human motion, or generate diverse but semantically implausible continuations of the observed motion. In this paper, we address both of these problems by developing a new variational framework that accounts for both diversity and context of the generated future motion. To this end, and in contrast to existing approaches, we condition the sampling of the latent variable that acts as source of diversity on the representation of the past observation, thus encouraging it to carry relevant information. Our experiments demonstrate that our approach yields motions not only of higher quality while retaining diversity, but also that preserve the contextual information contained in the observed motion.},
  archive   = {C_ICCV},
  author    = {Sadegh Aliakbarian and Fatemeh Saleh and Lars Petersson and Stephen Gould and Mathieu Salzmann},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01114},
  pages     = {11313-11322},
  title     = {Contextually plausible and diverse 3D human motion prediction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The animation transformer: Visual correspondence via segment
matching. <em>ICCV</em>, 11303–11312. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual correspondence is a fundamental building block on the way to building assistive tools for hand-drawn animation. However, while a large body of work has focused on learning visual correspondences at the pixel-level, few approaches have emerged to learn correspondence at the level of line enclosures (segments) that naturally occur in hand-drawn animation. Exploiting this structure in animation has numerous benefits: it avoids the memory complexity of pixel attention over high resolution images and enables the use of real-world animation datasets that contain correspondence information at the level of per-segment colors. To that end, we propose the Animation Transformer (AnT) which uses a Transformer-based architecture to learn the spatial and visual relationships between segments across a sequence of images. By leveraging a forward match loss and a cycle consistency loss our approach attains excellent results compared to state-of-the-art pixel approaches on challenging datasets from real animation productions that lack ground-truth correspondence labels.},
  archive   = {C_ICCV},
  author    = {Evan Casey and Víctor Pérez and Zhuoru Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01113},
  pages     = {11303-11312},
  title     = {The animation transformer: Visual correspondence via segment matching},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). TokenPose: Learning keypoint tokens for human pose
estimation. <em>ICCV</em>, 11293–11302. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human pose estimation deeply relies on visual clues and anatomical constraints between parts to locate keypoints. Most existing CNN-based methods do well in visual representation, however, lacking in the ability to explicitly learn the constraint relationships between keypoints. In this paper, we propose a novel approach based on Token representation for human Pose estimation (TokenPose). In detail, each keypoint is explicitly embedded as a token to simultaneously learn constraint relationships and appearance cues from images. Extensive experiments show that the small and large TokenPose models are on par with state-of-the-art CNN-based counterparts while being more lightweight. Specifically, our TokenPose-S and TokenPose-L achieve 72.5 AP and 75.8 AP on COCO validation dataset respectively, with significant reduction in parameters (↓80.6\% ; ↓ 56.8\%) and GFLOPs (↓ 75.3\%; ↓24.7\%). Code is publicly available 1 .},
  archive   = {C_ICCV},
  author    = {Yanjie Li and Shoukui Zhang and Zhicheng Wang and Sen Yang and Wankou Yang and Shu-Tao Xia and Erjin Zhou},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01112},
  pages     = {11293-11302},
  title     = {TokenPose: Learning keypoint tokens for human pose estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-mutual distillation learning for continuous sign
language recognition. <em>ICCV</em>, 11283–11292. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, deep learning moves video-based Continuous Sign Language Recognition (CSLR) significantly forward. Currently, a typical network combination for CSLR includes a visual module, which focuses on spatial and short-temporal information, followed by a contextual module, which focuses on long-temporal information, and the Connectionist Temporal Classification (CTC) loss is adopted to train the network. However, due to the limitation of chain rules in back-propagation, the visual module is hard to adjust for seeking optimized visual features. As a result, it enforces that the contextual module focuses on contextual information optimization only rather than balancing efficient visual and contextual information. In this paper, we propose a Self-Mutual Knowledge Distillation (SMKD) method, which enforces the visual and contextual modules to focus on short-term and long-term information and enhances the discriminative power of both modules simultaneously. Specifically, the visual and contextual modules share the weights of their corresponding classifiers, and train with CTC loss simultaneously. Moreover, the spike phenomenon widely exists with CTC loss. Although it can help us choose a few of the key frames of a gloss, it does drop other frames in a gloss and makes the visual feature saturation in the early stage. A gloss segmentation is developed to relieve the spike phenomenon and decrease saturation in the visual module. We conduct experiments on two CSLR bench-marks: PHOENIX14 and PHOENIX14-T. Experimental results demonstrate the effectiveness of the SMKD.},
  archive   = {C_ICCV},
  author    = {Aiming Hao and Yuecong Min and Xilin Chen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01111},
  pages     = {11283-11292},
  title     = {Self-mutual distillation learning for continuous sign language recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Audio2Gestures: Generating diverse gestures from speech
audio with conditional variational autoencoders. <em>ICCV</em>,
11273–11282. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generating conversational gestures from speech audio is challenging due to the inherent one-to-many mapping be-tween audio and body motions. Conventional CNNs/RNNs assume one-to-one mapping, and thus tend to predict the average of all possible target motions, resulting in plain/boring motions during inference. In order to over-come this problem, we propose a novel conditional variational autoencoder (VAE) that explicitly models one-to-many audio-to-motion mapping by splitting the cross-modal latent code into shared code and motion-specific code. The shared code mainly models the strong correlation between audio and motion (such as the synchronized audio and motion beats), while the motion-specific code captures diverse motion information independent of the audio. However, splitting the latent code into two parts poses training difficulties for the VAE model. A mapping network facilitating random sampling along with other techniques including relaxed motion loss, bicycle constraint, and diversity loss are designed to better train the VAE. Experiments on both 3D and 2D motion datasets verify that our method generates more realistic and diverse motions than state-of-the-art methods, quantitatively and qualitatively. Finally, we demonstrate that our method can be readily used to generate motion sequences with user-specified motion clips on the timeline. Code and more results are at https://jingli513.github.io/audio2gestures.},
  archive   = {C_ICCV},
  author    = {Jing Li and Di Kang and Wenjie Pei and Xuefei Zhe and Ying Zhang and Zhenyu He and Linchao Bao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01110},
  pages     = {11273-11282},
  title     = {Audio2Gestures: Generating diverse gestures from speech audio with conditional variational autoencoders},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hand image understanding via deep multi-task learning.
<em>ICCV</em>, 11261–11272. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Analyzing and understanding hand information from multimedia materials like images or videos is important for many real world applications and remains active in research community. There are various works focusing on recovering hand information from single image, however, they usually solve a single task, for example, hand mask segmentation, 2D/3D hand pose estimation, or hand mesh reconstruction and perform not well in challenging scenarios. To further improve the performance of these tasks, we propose a novel Hand Image Understanding (HIU) framework to extract comprehensive information of the hand object from a single RGB image, by jointly considering the relationships between these tasks. To achieve this goal, a cascaded multi-task learning (MTL) backbone is designed to estimate the 2D heat maps, to learn the segmentation mask, and to generate the intermediate 3D information encoding, followed by a coarse-to-fine learning paradigm and a self-supervised learning strategy. Qualitative experiments demonstrate that our approach can recover reasonable mesh representations even in challenging situations. Quantitatively, our method significantly outperforms the state-of-the-art approaches on various widely-used datasets, in terms of diverse evaluation metrics https://github.com/MandyMo/HIU-DMTL.},
  archive   = {C_ICCV},
  author    = {Xiong Zhang and Hongsheng Huang and Jianchao Tan and Hongmin Xu and Cheng Yang and Guozhu Peng and Lei Wang and Ji Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01109},
  pages     = {11261-11272},
  title     = {Hand image understanding via deep multi-task learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning causal representation for training cross-domain
pose estimator via generative interventions. <em>ICCV</em>, 11250–11260.
(<a href="https://doi.org/10.1109/ICCV48922.2021.01108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D pose estimation has attracted increasing attention with the availability of high-quality benchmark datasets. However, prior works show that deep learning models tend to learn spurious correlations, which fail to generalize beyond the specific dataset they are trained on. In this work, we take a step towards training robust models for cross-domain pose estimation task, which brings together ideas from causal representation learning and generative adversarial networks. Specifically, this paper introduces a novel framework for causal representation learning which explicitly exploits the causal structure of the task. We consider changing domain as interventions on images under the data-generation process and steer the generative model to produce counterfactual features. This help the model learn transferable and causal relations across different domains. Our framework is able to learn with various types of unlabeled datasets. We demonstrate the efficacy of our proposed method on both human and hand pose estimation task. The experiment results show the proposed approach achieves state-of-the-art performance on most datasets for both domain adaptation and domain generalization settings.},
  archive   = {C_ICCV},
  author    = {Xiheng Zhang and Yongkang Wong and Xiaofei Wu and Juwei Lu and Mohan Kankanhalli and Xiangdong Li and Weidong Geng},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01108},
  pages     = {11250-11260},
  title     = {Learning causal representation for training cross-domain pose estimator via generative interventions},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HandFoldingNet: A 3D hand pose estimation network using
multiscale-feature guided folding of a 2D hand skeleton. <em>ICCV</em>,
11240–11249. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With increasing applications of 3D hand pose estimation in various human-computer interaction applications, convolution neural networks (CNNs) based estimation models have been actively explored. However, the existing models require complex architectures or redundant computational resources to trade with the acceptable accuracy. To tackle this limitation, this paper proposes HandFoldingNet, an accurate and efficient hand pose estimator that regresses the hand joint locations from the normalized 3D hand point cloud input. The proposed model utilizes a folding-based decoder that folds a given 2D hand skeleton into the corresponding joint coordinates. For higher estimation accuracy, folding is guided by multi-scale features, which include both global and joint-wise local features. Experimental results show that the proposed model outperforms the existing methods on three hand pose benchmark datasets with the lowest model parameter requirement. Code is available at https://github.com/cwc1260/HandFold.},
  archive   = {C_ICCV},
  author    = {Wencan Cheng and Jae Hyun Park and Jong Hwan Ko},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01107},
  pages     = {11240-11249},
  title     = {HandFoldingNet: A 3D hand pose estimation network using multiscale-feature guided folding of a 2D hand skeleton},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to regress bodies from images using differentiable
semantic rendering. <em>ICCV</em>, 11230–11239. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning to regress 3D human body shape and pose (e.g. SMPL parameters) from monocular images typically exploits losses on 2D keypoints, silhouettes, and/or part-segmentation when 3D training data is not available. Such losses, however, are limited because 2D keypoints do not supervise body shape and segmentations of people in clothing do not match projected minimally-clothed SMPL shapes. To exploit richer image information about clothed people, we introduce higher-level semantic information about clothing to penalize clothed and non-clothed regions of the human body differently. To do so, we train a body regressor using a novel &quot;Differentiable Semantic Rendering (DSR)&quot; loss. For Minimally-Clothed (MC) regions, we define the DSR-MC loss, which encourages a tight match between a rendered SMPL body and the minimally-clothed regions of the image. For clothed regions, we define the DSR-C loss to encourage the rendered SMPL body to be inside the clothing mask. To ensure end-to-end differentiable training, we learn a semantic clothing prior for SMPL vertices from thousands of clothed human scans. We perform extensive qualitative and quantitative experiments to evaluate the role of clothing semantics on the accuracy of 3D human pose and shape estimation. We outperform all previous state-of-the-art methods on 3DPW and Human3.6M and obtain on par results on MPI-INF-3DHP. Code and trained models are available for research at https://dsr.is.tue.mpg.de/.},
  archive   = {C_ICCV},
  author    = {Sai Kumar Dwivedi and Nikos Athanasiou and Muhammed Kocabas and Michael J. Black},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01106},
  pages     = {11230-11239},
  title     = {Learning to regress bodies from images using differentiable semantic rendering},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An empirical study of the collapsing problem in
semi-supervised 2D human pose estimation. <em>ICCV</em>, 11220–11229.
(<a href="https://doi.org/10.1109/ICCV48922.2021.01105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most semi-supervised learning models are consistency-based, which leverage unlabeled images by maximizing the similarity between different augmentations of an image. But when we apply them to human pose estimation that has extremely imbalanced class distribution, they often collapse and predict every pixel in unlabeled images as background. We find this is because the decision boundary passes the high-density areas of the minor class so more and more pixels are gradually misclassified as background. In this work, we present a surprisingly simple approach to drive the model to learn in the correct direction. For each image, it composes a pair of easy-hard augmentations and uses the more accurate predictions on the easy image to teach the network to learn pose information of the hard one. The accuracy superiority of teaching signals allows the network to be &quot;monotonically&quot; improved which effectively avoids collapsing. We apply our method to the state-of-the-art pose estimators and it further improves their performance on three public datasets.},
  archive   = {C_ICCV},
  author    = {Rongchang Xie and Chunyu Wang and Wenjun Zeng and Yizhou Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01105},
  pages     = {11220-11229},
  title     = {An empirical study of the collapsing problem in semi-supervised 2D human pose estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-supervised 3D hand pose estimation from monocular RGB
via contrastive learning. <em>ICCV</em>, 11210–11219. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Encouraged by the success of contrastive learning on image classification tasks, we propose a new self-supervised method for the structured regression task of 3D hand pose estimation. Contrastive learning makes use of unlabeled data for the purpose of representation learning via a loss formulation that encourages the learned feature representations to be invariant under any image transformation. For 3D hand pose estimation, it too is desirable to have invariance to appearance transformation such as color jitter. However, the task requires equivariance under affine transformations, such as rotation and translation. To address this issue, we propose an equivariant contrastive objective and demonstrate its effectiveness in the context of 3D hand pose estimation. We experimentally investigate the impact of invariant and equivariant contrastive objectives and show that learning equivariant features leads to better representations for the task of 3D hand pose estimation. Furthermore, we show that standard ResNets with sufficient depth, trained on additional unlabeled data, attain improvements of up to 14.5\% in PA-EPE on FreiHAND and thus achieves state-of-the-art performance without any task specific, specialized architectures. Code and models are available at https://ait.ethz.ch/projects/2021/PeCLR/},
  archive   = {C_ICCV},
  author    = {Adrian Spurr and Aneesh Dahiya and Xi Wang and Xucong Zhang and Otmar Hilliges},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01104},
  pages     = {11210-11219},
  title     = {Self-supervised 3D hand pose estimation from monocular RGB via contrastive learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical kinematic probability distributions for 3D
human shape and pose estimation from images in the wild. <em>ICCV</em>,
11199–11209. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the problem of 3D human body shape and pose estimation from an RGB image. This is often an ill-posed problem, since multiple plausible 3D bodies may match the visual evidence present in the input - particularly when the subject is occluded. Thus, it is desirable to estimate a distribution over 3D body shape and pose conditioned on the input image instead of a single 3D re-construction. We train a deep neural network to estimate a hierarchical matrix-Fisher distribution over relative 3D joint rotation matrices (i.e. body pose), which exploits the human body’s kinematic tree structure, as well as a Gaussian distribution over SMPL body shape parameters. To further ensure that the predicted shape and pose distributions match the visual evidence in the input image, we implement a differentiable rejection sampler to impose a reprojection loss between ground-truth 2D joint coordinates and samples from the predicted distributions, projected onto the image plane. We show that our method is competitive with the state-of-the-art in terms of 3D shape and pose metrics on the SSP-3D and 3DPW datasets, while also yielding a structured probability distribution over 3D body shape and pose, with which we can meaningfully quantify prediction uncertainty and sample multiple plausible 3D reconstructions to explain a given input image.},
  archive   = {C_ICCV},
  author    = {Akash Sengupta and Ignas Budvytis and Roberto Cipolla},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01103},
  pages     = {11199-11209},
  title     = {Hierarchical kinematic probability distributions for 3D human shape and pose estimation from images in the wild},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Space-time-separable graph convolutional network for pose
forecasting. <em>ICCV</em>, 11189–11198. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human pose forecasting is a complex structured-data sequence-modelling task, which has received increasing attention, also due to numerous potential applications. Research has mainly addressed the temporal dimension as time series and the interaction of human body joints with a kinematic tree or by a graph. This has decoupled the two aspects and leveraged progress from the relevant fields, but it has also limited the understanding of the complex structural joint spatio-temporal dynamics of the human pose.Here we propose a novel Space-Time-Separable Graph Convolutional Network (STS-GCN) for pose forecasting. For the first time, STS-GCN models the human pose dynamics only with a graph convolutional network (GCN), including the temporal evolution and the spatial joint interaction within a single-graph framework, which allows the cross-talk of motion and spatial correlations. Concurrently, STS-GCN is the first space-time-separable GCN: the space-time graph connectivity is factored into space and time affinity matrices, which bottlenecks the space-time cross-talk, while enabling full joint-joint and time-time correlations. Both affinity matrices are learnt end-to-end, which results in connections substantially deviating from the standard kinematic tree and the linear-time time series.In experimental evaluation on three complex, recent and large-scale benchmarks, Human3.6M [24], AMASS [34] and 3DPW [48], STS-GCN outperforms the state-of-the-art, surpassing the current best technique [35] by over 32\% in average at the most difficult long-term predictions, while only requiring 1.7\% of its parameters. We explain the results qualitatively and illustrate the graph interactions by the factored joint-joint and time-time learnt graph connections. Our source code is available at: https://github.com/FraLuca/STSGCN},
  archive   = {C_ICCV},
  author    = {Theodoros Sofianos and Alessio Sampieri and Luca Franco and Fabio Galasso},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01102},
  pages     = {11189-11198},
  title     = {Space-time-separable graph convolutional network for pose forecasting},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Probabilistic monocular 3D human pose estimation with
normalizing flows. <em>ICCV</em>, 11179–11188. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D human pose estimation from monocular images is a highly ill-posed problem due to depth ambiguities and occlusions. Nonetheless, most existing works ignore these ambiguities and only estimate a single solution. In contrast, we generate a diverse set of hypotheses that represents the full posterior distribution of feasible 3D poses. To this end, we propose a normalizing flow based method that exploits the deterministic 3D-to-2D mapping to solve the ambiguous inverse 2D-to-3D problem. Additionally, uncertain detections and occlusions are effectively modeled by incorporating uncertainty information of the 2D detector as condition. Further keys to success are a learned 3D pose prior and a generalization of the best-of-M loss. We evaluate our approach on the two benchmark datasets Human3.6M and MPI-INF-3DHP, outperforming all comparable methods in most metrics. The implementation is available on GitHub 1 .},
  archive   = {C_ICCV},
  author    = {Tom Wehrbein and Marco Rudolph and Bodo Rosenhahn and Bastian Wandt},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01101},
  pages     = {11179-11188},
  title     = {Probabilistic monocular 3D human pose estimation with normalizing flows},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). End-to-end detection and pose estimation of two interacting
hands. <em>ICCV</em>, 11169–11178. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Three dimensional hand pose estimation has reached a level of maturity, enabling real-world applications for single-hand cases. However, accurate estimation of the pose of two closely interacting hands still remains a challenge as in this case, one hand often occludes the other. We present a new algorithm that accurately estimates hand poses in such a challenging scenario. The crux of our algorithm lies in a framework that jointly trains the estimators of interacting hands, leveraging their inter-dependence. Further, we employ a GAN-type discriminator of interacting hand pose that helps avoid physically implausible configurations, e.g. intersecting fingers, and exploit the visibility of joints to improve intermediate 2D pose estimation. We incorporate them into a single model that learns to detect hands and estimate their pose based on a unified criterion of pose estimation accuracy. To our knowledge, this is the first attempt to build an end-to-end network that detects and estimates the pose of two closely interacting hands (as well as single hands). In the experiments with three datasets representing challenging real-world scenarios, our algorithm demonstrated significant and consistent performance improvements over state-of-the-arts.},
  archive   = {C_ICCV},
  author    = {Dong Uk Kim and Kwang In Kim and Seungryul Baek},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01100},
  pages     = {11169-11178},
  title     = {End-to-end detection and pose estimation of two interacting hands},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Monocular, one-stage, regression of multiple 3D people.
<em>ICCV</em>, 11159–11168. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper focuses on the regression of multiple 3D people from a single RGB image. Existing approaches predominantly follow a multi-stage pipeline that first detects people in bounding boxes and then independently regresses their 3D body meshes. In contrast, we propose to Regress all meshes in a One-stage fashion for Multiple 3D People (termed ROMP). The approach is conceptually simple, bounding box-free, and able to learn a per-pixel representation in an end-to-end manner. Our method simultaneously predicts a Body Center heatmap and a Mesh Parameter map, which can jointly describe the 3D body mesh on the pixel level. Through a body-center-guided sampling process, the body mesh parameters of all people in the image are easily extracted from the Mesh Parameter map. Equipped with such a fine-grained representation, our one-stage framework is free of the complex multi-stage process and more robust to occlusion. Compared with state-of-the-art methods, ROMP achieves superior performance on the challenging multi-person benchmarks, including 3DPW and CMU Panoptic. Experiments on crowded/occluded datasets demonstrate the robustness under various types of occlusion. The code, released at https://github.com/Arthur151/ROMP, is the first real-time implementation of monocular multi-person 3D mesh regression.},
  archive   = {C_ICCV},
  author    = {Yu Sun and Qian Bao and Wu Liu and Yili Fu and Michael J. Black and Tao Mei},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01099},
  pages     = {11159-11168},
  title     = {Monocular, one-stage, regression of multiple 3D people},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Camera distortion-aware 3D human pose estimation in video
with optimization-based meta-learning. <em>ICCV</em>, 11149–11158. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing 3D human pose estimation algorithms trained on distortion-free datasets suffer performance drop when applied to new scenarios with a specific camera distortion. In this paper, we propose a simple yet effective model for 3D human pose estimation in video that can quickly adapt to any distortion environment by utilizing MAML, a representative optimization-based meta-learning algorithm. We consider a sequence of 2D keypoints in a particular distortion as a single task of MAML. However, due to the absence of a large-scale dataset in a distorted environment, we propose an efficient method to generate synthetic distorted data from undistorted 2D keypoints. For the evaluation, we assume two practical testing situations depending on whether a motion capture sensor is available or not. In particular, we propose Inference Stage Optimization using bone-length symmetry and consistency. Extensive evaluation shows that our proposed method successfully adapts to various degrees of distortion in the testing phase and outperforms the existing state-of-the-art approaches. The proposed method is useful in practice because it does not require camera calibration and additional computations in a testing set-up. Code is available at https://github.com/hanbyel0105/CamDistHumanPose3D.},
  archive   = {C_ICCV},
  author    = {Hanbyel Cho and Yooshin Cho and Jaemyung Yu and Junmo Kim},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01098},
  pages     = {11149-11158},
  title     = {Camera distortion-aware 3D human pose estimation in video with optimization-based meta-learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Shape-aware multi-person pose estimation from multi-view
images. <em>ICCV</em>, 11138–11148. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we contribute a simple yet effective approach for estimating 3D poses of multiple people from multi-view images. Our proposed coarse-to-fine pipeline first aggregates noisy 2D observations from multiple camera views into 3D space and then associates them into individual instances based on a confidence-aware majority voting technique. The final pose estimates are attained from a novel optimization scheme which links high-confidence multi-view 2D observations and 3D joint candidates. More-over, a statistical parametric body model such as SMPL is leveraged as a regularizing prior for these 3D joint candidates. Specifically, both 3D poses and SMPL parameters are optimized jointly in an alternating fashion. Here the parametric models help in correcting implausible 3D pose estimates and filling in missing joint detections while updated 3D poses in turn guide obtaining better SMPL estimations. By linking 2D and 3D observations, our method is both accurate and generalizes to different data sources because it better decouples the final 3D pose from the interperson constellation and is more robust to noisy 2D detections. We systematically evaluate our method on public datasets and achieve state-of-the-art performance. The code and video will be available on the project page: https://ait.ethz.ch/projects/2021/multi-human-pose/.},
  archive   = {C_ICCV},
  author    = {Zijian Dong and Jie Song and Xu Chen and Chen Guo and Otmar Hilliges},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01097},
  pages     = {11138-11148},
  title     = {Shape-aware multi-person pose estimation from multi-view images},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph-based 3D multi-person pose estimation using multi-view
images. <em>ICCV</em>, 11128–11137. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies the task of estimating the 3D human poses of multiple persons from multiple calibrated camera views. Following the top-down paradigm, we decompose the task into two stages, i.e. person localization and pose estimation. Both stages are processed in coarse-to-fine manners. And we propose three task-specific graph neural networks for effective message passing. For 3D person localization, we first use Multi-view Matching Graph Module (MMG) to learn the cross-view association and recover coarse human proposals. The Center Refinement Graph Module (CRG) further refines the results via flexible point-based prediction. For 3D pose estimation, the Pose Regression Graph Module (PRG) learns both the multi-view geometry and structural relations between human joints. Our approach achieves state-of-the-art performance on CMU Panoptic and Shelf datasets with significantly lower computation complexity.},
  archive   = {C_ICCV},
  author    = {Size Wu and Sheng Jin and Wentao Liu and Lei Bai and Chen Qian and Dong Liu and Wanli Ouyang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01096},
  pages     = {11128-11137},
  title     = {Graph-based 3D multi-person pose estimation using multi-view images},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning realistic human reposing using cyclic
self-supervision with 3D shape, pose, and appearance consistency.
<em>ICCV</em>, 11118–11127. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Synthesizing images of a person in novel poses from a single image is a highly ambiguous task. Most existing approaches require paired training images; i.e. images of the same person with the same clothing in different poses. However, obtaining sufficiently large datasets with paired data is challenging and costly. Previous methods that forego paired supervision lack realism. We propose a self-supervised framework named SPICE (Self-supervised Person Image CrEation) that closes the image quality gap with supervised methods. The key insight enabling self-supervision is to exploit 3D information about the human body in several ways. First, the 3D body shape must remain unchanged when reposing. Second, representing body pose in 3D enables reasoning about self occlusions. Third, 3D body parts that are visible before and after reposing, should have similar appearance features. Once trained, SPICE takes an image of a person and generates a new image of that person in a new target pose. SPICE achieves state-of-the-art performance on the DeepFashion dataset, improving the FID score from 29.9 to 7.8 compared with previous unsupervised methods, and with performance similar to the state-of-the-art supervised method (6.4). SPICE also generates temporally coherent videos given an input image and a sequence of poses, despite being trained on static images only.},
  archive   = {C_ICCV},
  author    = {Soubhik Sanyal and Betty Mohler and Alex Vorobiov and Larry Davis and Timo Bolkart and Javier Romero and Matthew Loper and Michael J. Black},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01095},
  pages     = {11118-11127},
  title     = {Learning realistic human reposing using cyclic self-supervision with 3D shape, pose, and appearance consistency},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PARE: Part attention regressor for 3D human body estimation.
<em>ICCV</em>, 11107–11117. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite significant progress, we show that state of the art 3D human pose and shape estimation methods remain sensitive to partial occlusion and can produce dramatically wrong predictions although much of the body is observable. To address this, we introduce a soft attention mechanism, called the Part Attention REgressor (PARE), that learns to predict body-part-guided attention masks. We observe that state-of-the-art methods rely on global feature representations, making them sensitive to even small occlusions. In contrast, PARE’s part-guided attention mechanism overcomes these issues by exploiting information about the visibility of individual body parts while leveraging information from neighboring body-parts to predict occluded parts. We show qualitatively that PARE learns sensible attention masks, and quantitative evaluation confirms that PARE achieves more accurate and robust reconstruction results than existing approaches on both occlusion-specific and standard benchmarks. The code and data are available for research purposes at https://pare.is.tue.mpg.de/},
  archive   = {C_ICCV},
  author    = {Muhammed Kocabas and Chun-Hao P. Huang and Otmar Hilliges and Michael J. Black},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01094},
  pages     = {11107-11117},
  title     = {PARE: Part attention regressor for 3D human body estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SOMA: Solving optical marker-based MoCap automatically.
<em>ICCV</em>, 11097–11106. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Marker-based optical motion capture (mocap) is the &quot;gold standard&quot; method for acquiring accurate 3D human motion in computer vision, medicine, and graphics. The raw output of these systems are noisy and incomplete 3D points or short tracklets of points. To be useful, one must associate these points with corresponding markers on the captured subject; i.e. &quot;labelling&quot;. Given these labels, one can then &quot;solve&quot; for the 3D skeleton or body surface mesh. Commercial auto-labeling tools require a specific calibration procedure at capture time, which is not possible for archival data. Here we train a novel neural network called SOMA, which takes raw mocap point clouds with varying numbers of points, labels them at scale without any calibration data, independent of the capture technology, and requiring only minimal human intervention. Our key insight is that, while labeling point clouds is highly ambiguous, the 3D body provides strong constraints on the solution that can be exploited by a learning-based method. To enable learning, we generate massive training sets of simulated noisy and ground truth mocap markers animated by 3D bodies from AMASS. SOMA exploits an architecture with stacked self-attention elements to learn the spatial structure of the 3D body and an optimal transport layer to constrain the assignment (labeling) problem while rejecting outliers. We extensively evaluate SOMA both quantitatively and qualitatively. SOMA is more accurate and robust than existing state of the art research methods and can be applied where commercial systems cannot. We automatically label over 8 hours of archival mocap data across 4 different datasets captured using various technologies and output SMPL-X body models. The model and data is released for research purposes at https://soma.is.tue.mpg.de/.},
  archive   = {C_ICCV},
  author    = {Nima Ghorbani and Michael J. Black},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01093},
  pages     = {11097-11106},
  title     = {SOMA: Solving optical marker-based MoCap automatically},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hand-object contact consistency reasoning for human grasps
generation. <em>ICCV</em>, 11087–11096. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While predicting robot grasps with parallel jaw grippers have been well studied and widely applied in robot manipulation tasks, the study on natural human grasp generation with a multi-finger hand remains a very challenging problem. In this paper, we propose to generate human grasps given a 3D object in the world. Our key observation is that it is crucial to model the consistency between the hand contact points and object contact regions. That is, we encourage the prior hand contact points to be close to the object surface and the object common contact regions to be touched by the hand at the same time. Based on the hand-object contact consistency, we design novel objectives in training the human grasp generation model and also a new self-supervised task which allows the grasp generation network to be adjusted even during test time. Our experiments show significant improvement in human grasp generation over state-of-the-art approaches by a large margin. More interestingly, by optimizing the model during test time with the self-supervised task, it helps achieve larger gain on unseen and out-of-domain objects. Project page: https://hwjiang1510.github.io/GraspTTA/.},
  archive   = {C_ICCV},
  author    = {Hanwen Jiang and Shaowei Liu and Jiashun Wang and Xiaolong Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01092},
  pages     = {11087-11096},
  title     = {Hand-object contact consistency reasoning for human grasps generation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CPF: Learning a contact potential field to model the
hand-object interaction. <em>ICCV</em>, 11077–11086. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modeling the hand-object (HO) interaction not only requires estimation of the HO pose, but also pays attention to the contact due to their interaction. Significant progress has been made in estimating hand and object separately with deep learning methods, simultaneous HO pose estimation and contact modeling has not yet been fully explored. In this paper, we present an explicit contact representation namely Contact Potential Field (CPF), and a learning-fitting hybrid framework namely MIHO to Modeling the Interaction of Hand and Object. In CPF, we treat each contacting HO vertex pair as a spring-mass system. Hence the whole system forms a potential field with minimal elastic energy at the grasp position. Extensive experiments on the two commonly used benchmarks have demonstrated that our method can achieve state-of-the-art in several reconstruction metrics, and allow us to produce more physically plausible HO pose even when the ground-truth exhibits severe interpenetration or disjointedness. Our code is available at https://github.com/lixiny/CPF.},
  archive   = {C_ICCV},
  author    = {Lixin Yang and Xinyu Zhan and Kailin Li and Wenqiang Xu and Jiefeng Li and Cewu Lu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01091},
  pages     = {11077-11086},
  title     = {CPF: Learning a contact potential field to model the hand-object interaction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SignBERT: Pre-training of hand-model-aware representation
for sign language recognition. <em>ICCV</em>, 11067–11076. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hand gesture serves as a critical role in sign language. Current deep-learning-based sign language recognition (SLR) methods may suffer insufficient interpretability and overfitting due to limited sign data sources. In this paper, we introduce the first self-supervised pre-trainable SignBERT with incorporated hand prior for SLR. Sign-BERT views the hand pose as a visual token, which is derived from an off-the-shelf pose extractor. The visual tokens are then embedded with gesture state, temporal and hand chirality information. To take full advantage of available sign data sources, SignBERT first performs self-supervised pre-training by masking and reconstructing visual tokens. Jointly with several mask modeling strategies, we attempt to incorporate hand prior in a model-aware method to better model hierarchical context over the hand sequence. Then with the prediction head added, SignBERT is fine-tuned to perform the downstream SLR task. To validate the effectiveness of our method on SLR, we perform extensive experiments on four public benchmark datasets, i.e., NMFs-CSL, SLR500, MSASL and WLASL. Experiment results demonstrate the effectiveness of both self-supervised learning and imported hand prior. Furthermore, we achieve state-of-the-art performance on all benchmarks with a notable gain.},
  archive   = {C_ICCV},
  author    = {Hezhen Hu and Weichao Zhao and Wengang Zhou and Yuechen Wang and Houqiang Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01090},
  pages     = {11067-11076},
  title     = {SignBERT: Pre-training of hand-model-aware representation for sign language recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Speech drives templates: Co-speech gesture synthesis with
learned templates. <em>ICCV</em>, 11057–11066. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Co-speech gesture generation is to synthesize a gesture sequence that not only looks real but also matches with the input speech audio. Our method generates the movements of a complete upper body, including arms, hands, and the head. Although recent data-driven methods achieve great success, challenges still exist, such as limited variety, poor fidelity, and lack of objective metrics. Motivated by the fact that the speech cannot fully determine the gesture, we design a method that learns a set of gesture template vectors to model the latent conditions, which relieve the ambiguity. For our method, the template vector determines the general appearance of a generated gesture sequence, while the speech audio drives subtle movements of the body, both indispensable for synthesizing a realistic gesture sequence. Due to the intractability of an objective metric for gesturespeech synchronization, we adopt the lip-sync error as a proxy metric to tune and evaluate the synchronization ability of our model. Extensive experiments show the superiority of our method in both objective and subjective evaluations on fidelity and synchronization. 1},
  archive   = {C_ICCV},
  author    = {Shenhan Qian and Zhi Tu and Yihao Zhi and Wen Liu and Shenghua Gao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01089},
  pages     = {11057-11066},
  title     = {Speech drives templates: Co-speech gesture synthesis with learned templates},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Removing the bias of integral pose regression.
<em>ICCV</em>, 11047–11056. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Heatmap-based detection methods are dominant for 2D human pose estimation even though regression is more intuitive. The introduction of the integral regression method, which, architecture-wise uses an implicit heatmap, brings the two approaches even closer together. This begs the question – does detection really outperform regression? In this paper, we investigate the difference in supervision between the heatmap-based detection and integral regression, as this is the key remaining difference between the two approaches. In the process, we discover an underlying bias behind integral pose regression that arises from taking the expectation after the softmax function. To counter the bias, we present a compensation method which we find to improve integral regression accuracy on all 2D pose estimation benchmarks. We further propose a simple combined detection and bias-compensated regression method that considerably outperforms state-of-the-art baselines with few added components.},
  archive   = {C_ICCV},
  author    = {Kerui Gu and Linlin Yang and Angela Yao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01088},
  pages     = {11047-11056},
  title     = {Removing the bias of integral pose regression},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Keypoint communities. <em>ICCV</em>, 11037–11046. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a fast bottom-up method that jointly detects over 100 keypoints on humans or objects, also referred to as human/object pose estimation. We model all keypoints belonging to a human or an object –the pose– as a graph and leverage insights from community detection to quantify the independence of keypoints. We use a graph centrality measure to assign training weights to different parts of a pose. Our proposed measure quantifies how tightly a keypoint is connected to its neighborhood. Our experiments show that our method outperforms all previous methods for human pose estimation with fine-grained keypoint annotations on the face, the hands and the feet with a total of 133 keypoints. We also show that our method generalizes to car poses.},
  archive   = {C_ICCV},
  author    = {Duncan Zauss and Sven Kreiss and Alexandre Alahi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01087},
  pages     = {11037-11046},
  title     = {Keypoint communities},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ARCH++: Animation-ready clothed human reconstruction
revisited. <em>ICCV</em>, 11026–11036. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present ARCH++, an image-based method to reconstruct 3D avatars with arbitrary clothing styles. Our reconstructed avatars are animation-ready and highly realistic, in both the visible regions from input views and the unseen regions. While prior work shows great promise of reconstructing animatable clothed humans with various topologies, we observe that there exist fundamental limitations resulting in sub-optimal reconstruction quality. In this paper, we revisit the major steps of image-based avatar reconstruction and address the limitations with ARCH++. First, we introduce an end-to-end point based geometry encoder to better describe the semantics of the underlying 3D human body, in replacement of previous hand-crafted features. Second, in order to address the occupancy ambiguity caused by topological changes of clothed humans in the canonical pose, we propose a co-supervising framework with cross-space consistency to jointly estimate the occupancy in both the posed and canonical spaces. Last, we use image-to-image translation networks to further refine detailed geometry and texture on the reconstructed surface, which improves the fidelity and consistency across arbitrary viewpoints. In the experiments, we demonstrate improvements over the state of the art on both public benchmarks and user studies in reconstruction quality and realism.},
  archive   = {C_ICCV},
  author    = {Tong He and Yuanlu Xu and Shunsuke Saito and Stefano Soatto and Tony Tung},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01086},
  pages     = {11026-11036},
  title     = {ARCH++: Animation-ready clothed human reconstruction revisited},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SPEC: Seeing people in the wild with an estimated camera.
<em>ICCV</em>, 11015–11025. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Due to the lack of camera parameter information for in-the-wild images, existing 3D human pose and shape (HPS) estimation methods make several simplifying assumptions: weak-perspective projection, large constant focal length, and zero camera rotation. These assumptions often do not hold and we show, quantitatively and qualitatively, that they cause errors in the reconstructed 3D shape and pose. To address this, we introduce SPEC, the first in-the-wild 3D HPS method that estimates the perspective camera from a single image and employs this to reconstruct 3D human bodies more accurately. First, we train a neural network to estimate the field of view, camera pitch, and roll given an input image. We employ novel losses that improve the calibration accuracy over previous work. We then train a novel network that concatenates the camera calibration to the image features and uses these together to regress 3D body shape and pose. SPEC is more accurate than the prior art on the standard benchmark (3DPW) as well as two new datasets with more challenging camera views and varying focal lengths. Specifically, we create a new photorealistic synthetic dataset (SPEC-SYN) with ground truth 3D bodies and a novel in-the-wild dataset (SPEC-MTP) with calibration and high-quality reference bodies. Code and datasets are available for research purposes at https://spec.is.tue.mpg.de/.},
  archive   = {C_ICCV},
  author    = {Muhammed Kocabas and Chun-Hao P Huang and Joachim Tesch and Lea Müller and Otmar Hilliges and Michael J. Black},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01085},
  pages     = {11015-11025},
  title     = {SPEC: Seeing people in the wild with an estimated camera},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Human pose regression with residual log-likelihood
estimation. <em>ICCV</em>, 11005–11014. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Heatmap-based methods dominate in the field of human pose estimation by modelling the output distribution through likelihood heatmaps. In contrast, regression-based methods are more efficient but suffer from inferior performance. In this work, we explore maximum likelihood estimation (MLE) to develop an efficient and effective regression-based method. From the perspective of MLE, adopting different regression losses is making different assumptions about the output density function. A density function closer to the true distribution leads to a better regression performance. In light of this, we propose a novel regression paradigm with Residual Log-likelihood Estimation (RLE) to capture the underlying output distribution. Concretely, RLE learns the change of the distribution instead of the unreferenced underlying distribution to facilitate the training process. With the proposed reparameterization design, our method is compatible with off-the-shelf flow models. The proposed method is effective, efficient and flexible. We show its potential in various human pose estimation tasks with comprehensive experiments. Compared to the conventional regression paradigm, regression with RLE bring 12.4 mAP improvement on MSCOCO without any test-time overhead. Moreover, for the first time, especially on multi-person pose estimation, our regression method is superior to the heatmap-based methods. Our code is available at https://github.com/Jeff-sjtu/res-loglikelihood-regression.},
  archive   = {C_ICCV},
  author    = {Jiefeng Li and Siyuan Bian and Ailing Zeng and Can Wang and Bo Pang and Wentao Liu and Cewu Lu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01084},
  pages     = {11005-11014},
  title     = {Human pose regression with residual log-likelihood estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised 3D pose estimation for hierarchical dance video
recognition. <em>ICCV</em>, 10995–11004. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dance experts often view dance as a hierarchy of information, spanning low-level (raw images, image sequences), mid-levels (human poses and bodypart movements), and high-level (dance genre). We propose a Hierarchical Dance Video Recognition framework (HDVR). HDVR estimates 2D pose sequences, tracks dancers, and then simultaneously estimates corresponding 3D poses and 3D-to-2D imaging parameters, without requiring ground truth for 3D poses. Unlike most methods that work on a single person, our tracking works on multiple dancers, under occlusions. From the estimated 3D pose sequence, HDVR extracts body part movements, and therefrom dance genre. The resulting hierarchical dance representation is explainable to experts. To overcome noise and interframe correspondence ambiguities, we enforce spatial and temporal motion smoothness and photometric continuity over time. We use an LSTM network to extract 3D movement subsequences from which we recognize dance genre. For experiments, we have identified 154 movement types, of 16 body parts, and assembled a new University of Illinois Dance (UID) Dataset, containing 1143 video clips of 9 genres covering 30 hours, annotated with movement and genre labels. Our experimental results demonstrate that our algorithms outperform the state-of-the-art 3D pose estimation methods, which also enhances our dance recognition performance.},
  archive   = {C_ICCV},
  author    = {Xiaodan Hu and Narendra Ahuja},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01083},
  pages     = {10995-11004},
  title     = {Unsupervised 3D pose estimation for hierarchical dance video recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Egocentric pose estimation from human vision span.
<em>ICCV</em>, 10986–10994. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Estimating camera wearer&#39;s body pose from an egocentric view (egopose) is a vital task in augmented and virtual reality. Existing approaches either use a narrow field of view front facing camera that barely captures the wearer, or an extended head-mounted top-down camera for maximal wearer visibility. In this paper, we tackle the egopose estimation from a more natural human vision span, where camera wearer can be seen in the peripheral view and depending on the head pose the wearer may become invisible or has a limited partial view. This is a realistic visual field for user-centric wearable devices like glasses which have front facing wide angle cameras. Existing solutions are not appropriate for this setting, and so, we propose a novel deep learning system taking advantage of both the dynamic features from camera SLAM and the body shape imagery. We compute 3D head pose, 3D body pose, the figure/ground separation, all at the same time while explicitly enforcing a certain geometric consistency across pose attributes. We further show that this system can be trained robustly with lots of existing mocap data so we do not have to collect and annotate large new datasets. Lastly, our system estimates egopose in real time and on the fly while maintaining high accuracy.},
  archive   = {C_ICCV},
  author    = {Hao Jiang and Vamsi Krishna Ithapu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01082},
  pages     = {10986-10994},
  title     = {Egocentric pose estimation from human vision span},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EventHPE: Event-based 3D human pose and shape estimation.
<em>ICCV</em>, 10976–10985. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Event camera is an emerging imaging sensor for capturing dynamics of moving objects as events, which motivates our work in estimating 3D human pose and shape from the event signals. Events, on the other hand, have their unique challenges: rather than capturing static body postures, the event signals are best at capturing local motions. This leads us to propose a two-stage deep learning approach, called EventHPE. The first-stage, FlowNet, is trained by unsupervised learning to infer optical flow from events. Both events and optical flow are closely related to human body dynamics, which are fed as input to the ShapeNet in the second stage, to estimate 3D human shapes. To mitigate the discrepancy between image-based flow (optical flow) and shape-based flow (vertices movement of human body shape), a novel flow coherence loss is introduced by exploiting the fact that both flows are originated from the identical human motion. An in-house event-based 3D human dataset is curated that comes with 3D pose and shape annotations, which is by far the largest one to our knowledge. Empirical evaluations on DHP19 dataset and our in-house dataset demonstrate the effectiveness of our approach.},
  archive   = {C_ICCV},
  author    = {Shihao Zou and Chuan Guo and Xinxin Zuo and Sen Wang and Pengyu Wang and Xiaoqin Hu and Shoushun Chen and Minglun Gong and Li Cheng},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01081},
  pages     = {10976-10985},
  title     = {EventHPE: Event-based 3D human pose and shape estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Action-conditioned 3D human motion synthesis with
transformer VAE. <em>ICCV</em>, 10965–10975. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We tackle the problem of action-conditioned generation of realistic and diverse human motion sequences. In contrast to methods that complete, or extend, motion sequences, this task does not require an initial pose or sequence. Here we learn an action-aware latent representation for human motions by training a generative variational autoencoder (VAE). By sampling from this latent space and querying a certain duration through a series of positional encodings, we synthesize variable-length motion sequences conditioned on a categorical action. Specifically, we design a Transformer-based architecture, ACTOR, for encoding and decoding a sequence of parametric SMPL human body models estimated from action recognition datasets. We evaluate our approach on the NTU RGB+D, HumanAct12 and UESTC datasets and show improvements over the state of the art. Furthermore, we present two use cases: improving action recognition through adding our synthesized data to training, and motion denoising. Code and models are available on our project page [53].},
  archive   = {C_ICCV},
  author    = {Mathis Petrovich and Michael J. Black and Gül Varol},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01080},
  pages     = {10965-10975},
  title     = {Action-conditioned 3D human motion synthesis with transformer VAE},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The power of points for modeling humans in clothing.
<em>ICCV</em>, 10954–10964. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Currently it requires an artist to create 3D human avatars with realistic clothing that can move naturally. Despite progress on 3D scanning and modeling of human bodies, there is still no technology that can easily turn a static scan into an animatable avatar. Automating the creation of such avatars would enable many applications in games, social networking, animation, and AR/VR to name a few. The key problem is one of representation. Standard 3D meshes are widely used in modeling the minimally-clothed body but do not readily capture the complex topology of clothing. Recent interest has shifted to implicit surface models for this task but they are computationally heavy and lack compatibility with existing 3D tools. What is needed is a 3D representation that can capture varied topology at high resolution and that can be learned from data. We argue that this representation has been with us all along — the point cloud. Point clouds have properties of both implicit and explicit representations that we exploit to model 3D garment geometry on a human body. We train a neural network with a novel local clothing geometric feature to represent the shape of different outfits. The network is trained from 3D point clouds of many types of clothing, on many bodies, in many poses, and learns to model pose-dependent clothing deformations. The geometry feature can be optimized to fit a previously unseen scan of a person in clothing, enabling the scan to be reposed realistically. Our model demonstrates superior quantitative and qualitative results in both multi-outfit modeling and unseen outfit animation. The code is available for research purposes at https://qianlim.github.io/POP.},
  archive   = {C_ICCV},
  author    = {Qianli Ma and Jinlong Yang and Siyu Tang and Michael J. Black},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01079},
  pages     = {10954-10964},
  title     = {The power of points for modeling humans in clothing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BioFors: A large biomedical image forensics dataset.
<em>ICCV</em>, 10943–10953. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Research in media forensics has gained traction to combat the spread of misinformation. However, most of this research has been directed towards content generated on social media. Biomedical image forensics is a related problem, where manipulation or misuse of images reported in biomedical research documents is of serious concern. The problem has failed to gain momentum beyond an academic discussion due to an absence of benchmark datasets and standardized tasks. In this paper we present BioFors 1 – the first dataset for benchmarking common biomedical image manipulations. BioFors comprises 47,805 images extracted from 1,031 open-source research papers. Images in BioFors are divided into four categories – Microscopy, Blot/Gel, FACS and Macroscopy. We also propose three tasks for forensic analysis – external duplication detection, internal duplication detection and cut/sharp-transition detection. We benchmark BioFors on all tasks with suitable state-of-the-art algorithms. Our results and analysis show that existing algorithms developed on common computer vision datasets are not robust when applied to biomedical images, validating that more research is required to address the unique challenges of biomedical image forensics.},
  archive   = {C_ICCV},
  author    = {Ekraam Sabir and Soumyaroop Nandi and Wael AbdAlmageed and Prem Natarajan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01078},
  pages     = {10943-10953},
  title     = {BioFors: A large biomedical image forensics dataset},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FloW: A dataset and benchmark for floating waste detection
in inland waters. <em>ICCV</em>, 10933–10942. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Marine debris is severely threatening the marine lives and causing sustained pollution to the whole ecosystem. To prevent the wastes from getting into the ocean, it is helpful to clean up the floating wastes in inland waters using the autonomous cleaning devices like unmanned surface vehicles. The cleaning efficiency relies on a high-accurate and robust object detection system. However, the small size of the target, the strong light reflection over water surface, and the reflection of other objects on bank-side all bring challenges to the vision-based object detection system. To promote the practical application for autonomous floating wastes cleaning, we present FloW † , the first dataset for floating waste detection in inland water areas. The dataset consists of an image sub-dataset FloW-Img and a multimodal sub-dataset FloW-RI which contains synchronized millimeter wave radar data and images. Accurate annotations for images and radar data are provided, supporting floating waste detection strategies based on image, radar data, and the fusion of two sensors. We perform several baseline experiments on our dataset, including vision-based and radar-based detection methods. The results show that, the detection accuracy is relatively low and floating waste detection still remains a challenging task.},
  archive   = {C_ICCV},
  author    = {Yuwei Cheng and Jiannan Zhu and Mengxin Jiang and Jie Fu and Changsong Pang and Peidong Wang and Kris Sankaran and Olawale Onabola and Yimin Liu and Dianbo Liu and Yoshua Bengio},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01077},
  pages     = {10933-10942},
  title     = {FloW: A dataset and benchmark for floating waste detection in inland waters},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BV-person: A large-scale dataset for bird-view person
re-identification. <em>ICCV</em>, 10923–10932. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Person Re-IDentification (ReID) aims at re-identifying persons from non-overlapping cameras. Existing person ReID studies focus on horizontal-view ReID tasks, in which the person images are captured by the cameras from a (nearly) horizontal view. In this work we introduce a new ReID task, bird-view person ReID, which aims at searching for a person in a gallery of horizontal-view images with the query images taken from a bird&#39;s-eye view, i.e., an elevated view of an object from above. The task is important because there are a large number of video surveillance cameras capturing persons from such an elevated view at public places. However, it is a challenging task in that the images from the bird view (i) provide limited person appearance information and (ii) have a large discrepancy compared to the persons in the horizontal view. We aim to facilitate the development of person ReID from this line by introducing a large-scale real-world dataset for this task. The proposed dataset, named BV-Person, contains 114k images of 18k identities in which nearly 20k images of 7.4k identities are taken from the bird&#39;s-eye view. We further introduce a novel model for this new ReID task. Large-scale experiments are performed to evaluate our model and 11 current state-of-the-art ReID models on BV-Person to establish performance benchmarks from multiple perspectives. The empirical results show that our model consistently and substantially outperforms the state-of-the-art models on all five datasets derived from BV-Person. Our model also achieves state-of-the-art performance on two general ReID datasets. The BV-Person dataset is available at: https://git.io/BVPerson},
  archive   = {C_ICCV},
  author    = {Cheng Yan and Guansong Pang and Lei Wang and Jile Jiao and Xuetao Feng and Chunhua Shen and Jingjing Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01076},
  pages     = {10923-10932},
  title     = {BV-person: A large-scale dataset for bird-view person re-identification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3D-FRONT: 3D furnished rooms with layOuts and semaNTics.
<em>ICCV</em>, 10913–10922. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce 3D-FRONT (3D Furnished Rooms with layOuts and semaNTics), a new, large-scale, and comprehensive repository of synthetic indoor scenes highlighted by professionally designed layouts and a large number of rooms populated by high-quality textured 3D models with style compatibility. From layout semantics down to texture details of individual objects, our dataset is freely available to the academic community and beyond. Currently, 3D-FRONT contains 6,813 CAD houses, where 18,968 rooms diversely furnished by 3D objects, far surpassing all publicly available scene datasets. The 13,151 furniture objects all come with high-quality textures. While the floorplans and layout designs (i.e., furniture arrangements) are directly sourced from professional creations, the interior designs in terms of furniture styles, color, and textures have been carefully curated based on a recommender system we develop to attain consistent styles as expert designs. Furthermore, we release Trescope, a light-weight rendering tool, to support benchmark rendering of 2D images and annotations from 3D-FRONT. We demonstrate two applications, interior scene synthesis and texture synthesis, that are especially tailored to the strengths of our new dataset.},
  archive   = {C_ICCV},
  author    = {Huan Fu and Bowen Cai and Lin Gao and Ling-Xiao Zhang and Jiaming Wang and Cao Li and Qixun Zeng and Chengyue Sun and Rongfei Jia and Binqiang Zhao and Hao Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01075},
  pages     = {10913-10922},
  title     = {3D-FRONT: 3D furnished rooms with layOuts and semaNTics},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards real-world x-ray security inspection: A high-quality
benchmark and lateral inhibition module for prohibited items detection.
<em>ICCV</em>, 10903–10912. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Prohibited items detection in X-ray images often plays an important role in protecting public safety, which often deals with color-monotonous and luster-insufficient objects, resulting in unsatisfactory performance. Till now, there have been rare studies touching this topic due to the lack of specialized high-quality datasets. In this work, we first present a High-quality X-ray (HiXray) security inspection image dataset, which contains 102,928 common prohibited items of 8 categories. It is the largest dataset of high quality for prohibited items detection, gathered from the real-world airport security inspection and annotated by professional security inspectors. Besides, for accurate prohibited item detection, we further propose the Lateral Inhibition Module (LIM) inspired by the fact that humans recognize these items by ignoring irrelevant information and focusing on identifiable characteristics, especially when objects are overlapped with each other. Specifically, LIM, the elaborately designed flexible additional module, suppresses the noisy information flowing maximumly by the Bidirectional Propagation (BP) module and activates the most identifiable charismatic, boundary, from four directions by Boundary Activation (BA) module. We evaluate our method extensively on HiXray and OPIXray and the results demonstrate that it outperforms SOTA detection methods. 1},
  archive   = {C_ICCV},
  author    = {Renshuai Tao and Yanlu Wei and Xiangjian Jiang and Hainan Li and Haotong Qin and Jiakai Wang and Yuqing Ma and Libo Zhang and Xianglong Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01074},
  pages     = {10903-10912},
  title     = {Towards real-world X-ray security inspection: A high-quality benchmark and lateral inhibition module for prohibited items detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hypersim: A photorealistic synthetic dataset for holistic
indoor scene understanding. <em>ICCV</em>, 10892–10902. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For many fundamental scene understanding tasks, it is difficult or impossible to obtain per-pixel ground truth labels from real images. We address this challenge by introducing Hypersim, a photorealistic synthetic dataset for holistic indoor scene understanding. To create our dataset, we leverage a large repository of synthetic scenes created by professional artists, and we generate 77,400 images of 461 indoor scenes with detailed per-pixel labels and corresponding ground truth geometry. Our dataset: (1) relies exclusively on publicly available 3D assets; (2) includes complete scene geometry, material information, and lighting information for every scene; (3) includes dense per-pixel semantic instance segmentations and complete camera information for every image; and (4) factors every image into diffuse reflectance, diffuse illumination, and a non-diffuse residual term that captures view-dependent lighting effects.We analyze our dataset at the level of scenes, objects, and pixels, and we analyze costs in terms of money, computation time, and annotation effort. Remarkably, we find that it is possible to generate our entire dataset from scratch, for roughly half the cost of training a popular open-source natural language processing model. We also evaluate sim-to-real transfer performance on two real-world scene understanding tasks – semantic segmentation and 3D shape prediction – where we find that pre-training on our dataset significantly improves performance on both tasks, and achieves state-of-the-art performance on the most challenging Pix3D test set. All of our rendered image data, as well as all the code we used to generate our dataset and perform our experiments, is available online.},
  archive   = {C_ICCV},
  author    = {Mike Roberts and Jason Ramapuram and Anurag Ranjan and Atulit Kumar and Miguel Angel Bautista and Nathan Paczan and Russ Webb and Joshua M. Susskind},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01073},
  pages     = {10892-10902},
  title     = {Hypersim: A photorealistic synthetic dataset for holistic indoor scene understanding},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Common objects in 3D: Large-scale learning and evaluation of
real-life 3D category reconstruction. <em>ICCV</em>, 10881–10891. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traditional approaches for learning 3D object categories have been predominantly trained and evaluated on synthetic datasets due to the unavailability of real 3D-annotated category-centric data. Our main goal is to facilitate advances in this field by collecting real-world data in a magnitude similar to the existing synthetic counterparts. The principal contribution of this work is thus a large-scale dataset, called Common Objects in 3D, with real multi-view images of object categories annotated with camera poses and ground truth 3D point clouds. The dataset contains a total of 1.5 million frames from nearly 19,000 videos capturing objects from 50 MS-COCO categories and, as such, it is significantly larger than alternatives both in terms of the number of categories and objects.We exploit this new dataset to conduct one of the first large-scale &quot;in-the-wild&quot; evaluations of several new-view-synthesis and category-centric 3D reconstruction methods. Finally, we contribute NerFormer - a novel neural rendering method that leverages the powerful Transformer to reconstruct an object given a small number of its views.},
  archive   = {C_ICCV},
  author    = {Jeremy Reizenstein and Roman Shapovalov and Philipp Henzler and Luca Sbordone and Patrick Labatut and David Novotny},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01072},
  pages     = {10881-10891},
  title     = {Common objects in 3D: Large-scale learning and evaluation of real-life 3D category reconstruction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). UltraPose: Synthesizing dense pose with 1 billion points by
human-body decoupling 3D model. <em>ICCV</em>, 10871–10880. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recovering dense human poses from images plays a critical role in establishing an image-to-surface correspondence between RGB images and the 3D surface of the human body, serving the foundation of rich real-world applications, such as virtual humans, monocular-to-3d reconstruction. However, the popular DensePose-COCO dataset relies on a sophisticated manual annotation system, leading to severe limitations in acquiring the denser and more accurate annotated pose resources. In this work, we introduce a new 3D human-body model with a series of decoupled parameters that could freely control the generation of the body. Furthermore, we build a data generation system based on this decoupling 3D model, and construct an ultra dense synthetic benchmark UltraPose, containing around 1.3 billion corresponding points. Compared to the existing manually annotated DensePose-COCO dataset, the synthetic UltraPose has ultra dense image-to-surface correspondences without annotation cost and error. Our proposed UltraPose provides the largest benchmark and data resources for lifting the model capability in predicting more accurate dense poses. To promote future researches in this field, we also propose a transformer-based method to model the dense correspondence between 2D and 3D worlds. The proposed model trained on synthetic UltraPose can be applied to real-world scenarios, indicating the effectiveness of our benchmark and model. 1},
  archive   = {C_ICCV},
  author    = {Haonan Yan and Jiaqi Chen and Xujie Zhang and Shengkai Zhang and Nianhong Jiao and Xiaodan Liang and Tianxiang Zheng},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01071},
  pages     = {10871-10880},
  title     = {UltraPose: Synthesizing dense pose with 1 billion points by human-body decoupling 3D model},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SynFace: Face recognition with synthetic data.
<em>ICCV</em>, 10860–10870. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the recent success of deep neural networks, remarkable progress has been achieved on face recognition. However, collecting large-scale real-world training data for face recognition has turned out to be challenging, especially due to the label noise and privacy issues. Meanwhile, existing face recognition datasets are usually collected from web images, lacking detailed annotations on attributes (e.g., pose and expression), so the influences of different attributes on face recognition have been poorly investigated. In this paper, we address the above-mentioned issues in face recognition using synthetic face images, i.e., SynFace. Specifically, we first explore the performance gap between recent state-of-the-art face recognition models trained with synthetic and real face images. We then analyze the underlying causes behind the performance gap, e.g., the poor intraclass variations and the domain gap between synthetic and real face images. Inspired by this, we devise the SynFace with identity mixup (IM) and domain mixup (DM) to mitigate the above performance gap, demonstrating the great potentials of synthetic data for face recognition. Furthermore, with the controllable face synthesis model, we can easily manage different factors of synthetic face generation, including pose, expression, illumination, the number of identities, and samples per identity. Therefore, we also perform a systematically empirical analysis on synthetic face images to provide some insights on how to effectively utilize synthetic data for face recognition.},
  archive   = {C_ICCV},
  author    = {Haibo Qiu and Baosheng Yu and Dihong Gong and Zhifeng Li and Wei Liu and Dacheng Tao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01070},
  pages     = {10860-10870},
  title     = {SynFace: Face recognition with synthetic data},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). StereOBJ-1M: Large-scale stereo image dataset for 6D object
pose estimation. <em>ICCV</em>, 10850–10859. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a large-scale stereo RGB image object pose estimation dataset named the StereOBJ-1M dataset. The dataset is designed to address challenging cases such as object transparency, translucency, and specular reflection, in addition to the common challenges of occlusion, symmetry, and variations in illumination and environments. In order to collect data of sufficient scale for modern deep learning models, we propose a novel method for efficiently annotating pose data in a multi-view fashion that allows data capturing in complex and flexible environments. Fully annotated with 6D object poses, our dataset contains over 396K frames and over 1.5M annotations of 18 objects recorded in 183 scenes constructed in 11 different environments. The 18 objects include 8 symmetric objects, 7 transparent objects, and 8 reflective objects. We benchmark two state-of-the-art pose estimation frameworks on StereOBJ-1M as baselines for future work. We also propose a novel object-level pose optimization method for computing 6D pose from keypoint predictions in multiple images.},
  archive   = {C_ICCV},
  author    = {Xingyu Liu and Shun Iwase and Kris M. Kitani},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01069},
  pages     = {10850-10859},
  title     = {StereOBJ-1M: Large-scale stereo image dataset for 6D object pose estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to track with object permanence. <em>ICCV</em>,
10840–10849. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tracking by detection, the dominant approach for online multi-object tracking, alternates between localization and association steps. As a result, it strongly depends on the quality of instantaneous observations, often failing when objects are not fully visible. In contrast, tracking in humans is underlined by the notion of object permanence: once an object is recognized, we are aware of its physical existence and can approximately localize it even under full occlusions. In this work, we introduce an end-to-end trainable approach for joint object detection and tracking that is capable of such reasoning. We build on top of the recent CenterTrack architecture, which takes pairs of frames as input, and extend it to videos of arbitrary length. To this end, we augment the model with a spatio-temporal, recurrent memory module, allowing it to reason about object locations and identities in the current frame using all the previous history. It is, however, not obvious how to train such an approach. We study this question on a new, large-scale, synthetic dataset for multi-object tracking, which provides ground truth annotations for invisible objects, and propose several approaches for supervising tracking behind occlusions. Our model, trained jointly on synthetic and real data, outperforms the state of the art on KITTI and MOT17 datasets thanks to its robustness to occlusions.},
  archive   = {C_ICCV},
  author    = {Pavel Tokmakov and Jie Li and Wolfram Burgard and Adrien Gaidon},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01068},
  pages     = {10840-10849},
  title     = {Learning to track with object permanence},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MOTSynth: How can synthetic data help pedestrian detection
and tracking? <em>ICCV</em>, 10829–10839. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep learning-based methods for video pedestrian detection and tracking require large volumes of training data to achieve good performance. However, data acquisition in crowded public environments raises data privacy concerns – we are not allowed to simply record and store data without the explicit consent of all participants. Furthermore, the annotation of such data for computer vision applications usually requires a substantial amount of manual effort, especially in the video domain. Labeling instances of pedestrians in highly crowded scenarios can be challenging even for human annotators and may introduce errors in the training data. In this paper, we study how we can advance different aspects of multi-person tracking using solely synthetic data. To this end, we generate MOTSynth, a large, highly diverse synthetic dataset for object detection and tracking using a rendering game engine. Our experiments show that MOTSynth can be used as a replacement for real data on tasks such as pedestrian detection, re-identification, segmentation, and tracking.},
  archive   = {C_ICCV},
  author    = {Matteo Fabbri and Guillem Brasó and Gianluca Maugeri and Orcun Cetintas and Riccardo Gasparini and Aljoša Ošep and Simone Calderara and Laura Leal-Taixé and Rita Cucchiara},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01067},
  pages     = {10829-10839},
  title     = {MOTSynth: How can synthetic data help pedestrian detection and tracking?},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to adversarially blur visual object tracking.
<em>ICCV</em>, 10819–10828. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion blur caused by the moving of the object or camera during the exposure can be a key challenge for visual object tracking, affecting tracking accuracy significantly. In this work, we explore the robustness of visual object trackers against motion blur from a new angle, i.e., adversarial blur attack (ABA). Our main objective is to online transfer input frames to their natural motion-blurred counterparts while misleading the state-of-the-art trackers during the tracking process. To this end, we first design the motion blur synthesizing method for visual tracking based on the generation principle of motion blur, considering the motion information and the light accumulation process. With this synthetic method, we propose optimization-based ABA (OP-ABA) by iteratively optimizing an adversarial objective function against the tracking w.r.t. the motion and light accumulation parameters. The OP-ABA is able to produce natural adversarial examples but the iteration can cause heavy time cost, making it unsuitable for attacking real-time trackers. To alleviate this issue, we further propose one-step ABA (OS-ABA) where we design and train a joint adversarial motion and accumulation predictive network (JAMANet) with the guidance of OP-ABA, which is able to efficiently estimate the adversarial motion and accumulation parameters in a one-step way. The experiments on four popular datasets (e.g., OTB100, VOT2018, UAV123, and LaSOT) demonstrate that our methods are able to cause significant accuracy drops on four state-of-the-art trackers with high transferability. Please find the source code at https://github.com/tsingqguo/ABA},
  archive   = {C_ICCV},
  author    = {Qing Guo and Ziyi Cheng and Felix Juefei-Xu and Lei Ma and Xiaofei Xie and Yang Liu and Jianjun Zhao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01066},
  pages     = {10819-10828},
  title     = {Learning to adversarially blur visual object tracking},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Wanderlust: Online continual object detection in the real
world. <em>ICCV</em>, 10809–10818. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Online continual learning from data streams in dynamic environments is a critical direction in the computer vision field. However, realistic benchmarks and fundamental studies in this line are still missing. To bridge the gap, we present a new online continual object detection benchmark with an egocentric video dataset, Objects Around Krishna (OAK). OAK adopts the KrishnaCAM videos, an ego-centric video stream collected over nine months by a graduate student. OAK provides exhaustive bounding box annotations of 80 video snippets (~17.5 hours) for 105 object categories in outdoor scenes. The emergence of new object categories in our benchmark follows a pattern similar to what a single person might see in their day-to-day life. The dataset also captures the natural distribution shifts as the person travels to different places. These egocentric long running videos provide a realistic playground for continual learning algorithms, especially in online embodied settings. We also introduce new evaluation metrics to evaluate the model performance and catastrophic forgetting and provide baseline studies for online continual object detection. We believe this benchmark will pose new exciting challenges for learning from non-stationary data in continual learning. The OAK dataset and the associated benchmark are released at https://oakdata.github.io/.},
  archive   = {C_ICCV},
  author    = {Jianren Wang and Xin Wang and Yue Shang-Guan and Abhinav Gupta},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01065},
  pages     = {10809-10818},
  title     = {Wanderlust: Online continual object detection in the real world},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ORBIT: A real-world few-shot dataset for teachable object
recognition. <em>ICCV</em>, 10798–10808. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Object recognition has made great advances in the last decade, but predominately still relies on many high-quality training examples per object category. In contrast, learning new objects from only a few examples could enable many impactful applications from robotics to user personalization. Most few-shot learning research, however, has been driven by benchmark datasets that lack the high variation that these applications will face when deployed in the real-world. To close this gap, we present the ORBIT dataset and benchmark, grounded in the real-world application of teachable object recognizers for people who are blind/low-vision. The dataset contains 3,822 videos of 486 objects recorded by people who are blind/low-vision on their mobile phones. The benchmark reflects a realistic, highly challenging recognition problem, providing a rich playground to drive research in robustness to few-shot, high-variation conditions. We set the benchmark’s first state-of-the-art and show there is massive scope for further innovation, holding the potential to impact a broad range of real-world vision applications including tools for the blind/low-vision community. We release the dataset at https://doi.org/10.25383/city.14294597 and benchmark code at https://github.com/microsoft/ORBIT-Dataset.},
  archive   = {C_ICCV},
  author    = {Daniela Massiceti and Luisa Zintgraf and John Bronskill and Lida Theodorou and Matthew Tobias Harris and Edward Cutrell and Cecily Morrison and Katja Hofmann and Simone Stumpf},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01064},
  pages     = {10798-10808},
  title     = {ORBIT: A real-world few-shot dataset for teachable object recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Separable flow: Learning motion cost volumes for optical
flow estimation. <em>ICCV</em>, 10787–10797. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Full-motion cost volumes play a central role in current state-of-the-art optical flow methods. However, constructed using simple feature correlations, they lack the ability to encapsulate prior, or even non-local knowledge. This creates artifacts in poorly constrained ambiguous regions, such as occluded and textureless areas. We propose a separable cost volume module, a drop-in replacement to correlation cost volumes, that uses non-local aggregation layers to exploit global context cues and prior knowledge, in order to disambiguate motions in these regions. Our method leads both the now standard Sintel and KITTI optical flow benchmarks in terms of accuracy, and is also shown to generalize better from synthetic to real data.},
  archive   = {C_ICCV},
  author    = {Feihu Zhang and Oliver J. Woodford and Victor Prisacariu and Philip H. S. Torr},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01063},
  pages     = {10787-10797},
  title     = {Separable flow: Learning motion cost volumes for optical flow estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). End-to-end video instance segmentation via spatial-temporal
graph neural networks. <em>ICCV</em>, 10777–10786. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Video instance segmentation is a challenging task that extends image instance segmentation to the video domain. Existing methods either rely only on single-frame information for the detection and segmentation subproblems or handle tracking as a separate post-processing step, which limit their capability to fully leverage and share useful spatial-temporal information for all the subproblems. In this paper, we propose a novel graph-neural-network (GNN) based method to handle the aforementioned limitation. Specifically, graph nodes representing instance features are used for detection and segmentation while graph edges representing instance relations are used for tracking. Both inter and intra-frame information is effectively propagated and shared via graph updates and all the subproblems (i.e. detection, segmentation and tracking) are jointly optimized in an unified framework. The performance of our method shows great improvement on the YoutubeVIS validation dataset compared to existing methods and achieves 36.5\% AP with a ResNet-50 backbone, operating at 22 FPS.},
  archive   = {C_ICCV},
  author    = {Tao Wang and Ning Xu and Kean Chen and Weiyao Lin},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01062},
  pages     = {10777-10786},
  title     = {End-to-end video instance segmentation via spatial-temporal graph neural networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Omnidata: A scalable pipeline for making multi-task
mid-level vision datasets from 3D scans. <em>ICCV</em>, 10766–10776. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a pipeline to parametrically sample and render static multi-task vision datasets from comprehensive 3D scans from the real-world. In addition to enabling interesting lines of research, we show the tooling and generated data suffice to train robust vision models. Familiar architectures trained on a generated starter dataset reached state-of-the-art performance on multiple common vision tasks and benchmarks, despite having seen no benchmark or non-pipeline data. The depth estimation network outperforms MiDaS and the surface normal estimation network is the first to achieve human-level performance for in-the-wild surface normal estimation—at least according to one metric on the OASIS benchmark. The Dockerized pipeline with CLI, the (mostly python) code, PyTorch dataloaders for the generated data, the generated starter dataset, download scripts and other utilities are all available ${\color{Magenta}through}\;{\color{Magenta}our}\;{\color{Magenta}project}\;{\color{Magenta}website}$.},
  archive   = {C_ICCV},
  author    = {Ainaz Eftekhar and Alexander Sax and Jitendra Malik and Amir Zamir},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01061},
  pages     = {10766-10776},
  title     = {Omnidata: A scalable pipeline for making multi-task mid-level vision datasets from 3D scans},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unidentified video objects: A benchmark for dense,
open-world segmentation. <em>ICCV</em>, 10756–10765. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current state-of-the-art object detection and segmentation methods work well under the closed-world assumption. This closed-world setting assumes that the list of object categories is available during training and deployment. However, many real-world applications require detecting or segmenting novel objects, i.e., object categories never seen during training. In this paper, we present, UVO (Unidentified Video Objects), a new benchmark for openworld class-agnostic object segmentation in videos. Besides shifting the focus to the open-world setup, UVO is significantly larger, providing approximately 6 times more videos compared with DAVIS, and 7 times more mask (instance) annotations per video compared with YouTube-VO(I)S. UVO is also more challenging as it includes many videos with crowded scenes and complex background motions. We also demonstrated that UVO can be used for other applications, such as object tracking and super-voxel segmentation. We believe that UVO is a versatile testbed for researchers to develop novel approaches for open-world class-agnostic object segmentation, and inspires new research directions towards a more comprehensive video understanding beyond classification and detection.},
  archive   = {C_ICCV},
  author    = {Weiyao Wang and Matt Feiszli and Heng Wang and Du Tran},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01060},
  pages     = {10756-10765},
  title     = {Unidentified video objects: A benchmark for dense, open-world segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ACDC: The adverse conditions dataset with correspondences
for semantic driving scene understanding. <em>ICCV</em>, 10745–10755.
(<a href="https://doi.org/10.1109/ICCV48922.2021.01059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Level 5 autonomy for self-driving cars requires a robust visual perception system that can parse input images under any visual condition. However, existing semantic segmentation datasets are either dominated by images captured under normal conditions or are small in scale. To address this, we introduce ACDC, the Adverse Conditions Dataset with Correspondences for training and testing semantic segmentation methods on adverse visual conditions. ACDC consists of a large set of 4006 images which are equally distributed between four common adverse conditions: fog, nighttime, rain, and snow. Each adverse-condition image comes with a high-quality fine pixel-level semantic annotation, a corresponding image of the same scene taken under normal conditions, and a binary mask that distinguishes between intra-image regions of clear and uncertain semantic content. Thus, ACDC supports both standard semantic segmentation and the newly introduced uncertainty-aware semantic segmentation. A detailed empirical study demonstrates the challenges that the adverse domains of ACDC pose to state-of-the-art supervised and unsupervised approaches and indicates the value of our dataset in steering future progress in the field. Our dataset and benchmark are publicly available.},
  archive   = {C_ICCV},
  author    = {Christos Sakaridis and Dengxin Dai and Luc Van Gool},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01059},
  pages     = {10745-10755},
  title     = {ACDC: The adverse conditions dataset with correspondences for semantic driving scene understanding},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic surface function networks for clothed human bodies.
<em>ICCV</em>, 10734–10744. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel method for temporal coherent reconstruction and tracking of clothed humans. Given a monocular RGB-D sequence, we learn a person-specific body model which is based on a dynamic surface function network. To this end, we explicitly model the surface of the person using a multi-layer perceptron (MLP) which is embedded into the canonical space of the SMPL body model. With classical forward rendering, the represented surface can be rasterized using the topology of a template mesh. For each surface point of the template mesh, the MLP is evaluated to predict the actual surface location. To handle pose-dependent deformations, the MLP is conditioned on the SMPL pose parameters. We show that this surface representation as well as the pose parameters can be learned in a self-supervised fashion using the principle of analysis-by-synthesis and differentiable rasterization. As a result, we are able to reconstruct a temporally coherent mesh sequence from the input data. The underlying surface representation can be used to synthesize new animations of the reconstructed person including pose-dependent deformations.},
  archive   = {C_ICCV},
  author    = {Andrei Burov and Matthias Nießner and Justus Thies},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01058},
  pages     = {10734-10744},
  title     = {Dynamic surface function networks for clothed human bodies},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). KoDF: A large-scale korean DeepFake detection dataset.
<em>ICCV</em>, 10724–10733. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A variety of effective face-swap and face-reenactment methods have been publicized in recent years, democratizing the face synthesis technology to a great extent. Videos generated as such have come to be called deepfakes with a negative connotation, for various social problems they have caused. Facing the emerging threat of deepfakes, we have built the Korean DeepFake Detection Dataset (KoDF), a large-scale collection of synthesized and real videos focused on Korean subjects. In this paper, we provide a detailed description of methods used to construct the dataset, experimentally show the discrepancy between the distributions of KoDF and existing deepfake detection datasets, and underline the importance of using multiple datasets for real-world generalization. KoDF is publicly available at https://moneybrain-research.github.io/kodf in its entirety (i.e. real clips, synthesized clips, clips with adversarial attack, and metadata).},
  archive   = {C_ICCV},
  author    = {Patrick Kwon and Jaeseong You and Gyuhyeon Nam and Sungwoo Park and Gyeongsu Chae},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01057},
  pages     = {10724-10733},
  title     = {KoDF: A large-scale korean DeepFake detection dataset},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Transparent object tracking benchmark. <em>ICCV</em>,
10714–10723. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual tracking has achieved considerable progress in recent years. However, current research in the field mainly focuses on tracking of opaque objects, while little attention is paid to transparent object tracking. In this paper, we make the first attempt in exploring this problem by proposing a Transparent Object Tracking Benchmark (TOTB). Specifically, TOTB consists of 225 videos (86K frames) from 15 diverse transparent object categories. Each sequence is manually labeled with axis-aligned bounding boxes. To the best of our knowledge, TOTB is the first benchmark dedicated to transparent object tracking. In order to understand how existing trackers perform and to provide comparison for future research on TOTB, we extensively evaluate 25 state-of-the-art tracking algorithms. The evaluation results exhibit that more efforts are needed to improve transparent object tracking. Besides, we observe some nontrivial findings from the evaluation that are discrepant with some common beliefs in opaque object tracking. For example, we find that deeper features are not always good for improvements. Moreover, to encourage future research, we introduce a novel tracker, named TransATOM, which leverages transparency features for tracking and surpasses all 25 evaluated approaches by a large margin. By releasing TOTB, we expect to facilitate future research and application of transparent object tracking in both the academia and industry. The TOTB and evaluation results as well as TransATOM are available at https: //hengfan2010.github.io/projects/TOTB/.},
  archive   = {C_ICCV},
  author    = {Heng Fan and Halady Akhilesha Miththanthaya and Harshit Harshit and Siranjiv Ramana Rajan and Xiaoqiong Liu and Zhilin Zou and Yuewei Lin and Haibin Ling},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01056},
  pages     = {10714-10723},
  title     = {Transparent object tracking benchmark},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DepthTrack: Unveiling the power of RGBD tracking.
<em>ICCV</em>, 10705–10713. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {RGBD (RGB plus depth) object tracking is gaining momentum as RGBD sensors have become popular in many application fields such as robotics. However, the best RGBD trackers are extensions of the state-of-the-art deep RGB trackers. They are trained with RGB data and the depth channel is used as a sidekick for subtleties such as occlusion detection. This can be explained by the fact that there are no sufficiently large RGBD datasets to 1) train &quot;deep depth trackers&quot; and to 2) challenge RGB trackers with sequences for which the depth cue is essential. This work introduces a new RGBD tracking dataset - Depth-Track - that has twice as many sequences (200) and scene types (40) than in the largest existing dataset, and three times more objects (90). In addition, the average length of the sequences (1473), the number of deformable objects (16) and the number of annotated tracking attributes (15) have been increased. Furthermore, by running the SotA RGB and RGBD trackers on DepthTrack, we propose a new RGBD tracking baseline, namely DeT, which reveals that deep RGBD tracking indeed benefits from genuine training data. The code and dataset is available at https://github.com/xiaozai/DeT.},
  archive   = {C_ICCV},
  author    = {Song Yan and Jinyu Yang and Jani Käpylä and Feng Zheng and Aleš Leonardis and Joni-Kristian Kämäräinen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01055},
  pages     = {10705-10713},
  title     = {DepthTrack: Unveiling the power of RGBD tracking},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cloud transformers: A universal approach to point cloud
processing tasks. <em>ICCV</em>, 10695–10704. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a new versatile building block for deep point cloud processing architectures that is equally suited for diverse tasks. This building block combines the ideas of spatial transformers and multi-view convolutional networks with the efficiency of standard convolutional layers in two and three-dimensional dense grids. The new block operates via multiple parallel heads, whereas each head differentiably rasterizes feature representations of individual points into a low-dimensional space, and then uses dense convolution to propagate information across points. The results of the processing of individual heads are then combined together resulting in the update of point features. Using the new block, we build architectures for both discriminative (point cloud segmentation, point cloud classification) and generative (point cloud inpainting and image-based point cloud reconstruction) tasks. The resulting architectures achieve state-of-the-art performance for these tasks, demonstrating the versatility of the new block for point cloud processing.},
  archive   = {C_ICCV},
  author    = {Kirill Mazur and Victor Lempitsky},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01054},
  pages     = {10695-10704},
  title     = {Cloud transformers: A universal approach to point cloud processing tasks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Low-shot validation: Active importance sampling for
estimating classifier performance on rare categories. <em>ICCV</em>,
10685–10694. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For machine learning models trained with limited labeled training data, validation stands to become the main bottleneck to reducing overall annotation costs. We propose a statistical validation algorithm that accurately estimates the F-score of binary classifiers for rare categories, where finding relevant examples to evaluate on is particularly challenging. Our key insight is that simultaneous calibration and importance sampling enables accurate estimates even in the low-sample regime (&lt; 300 samples). Critically, we also derive an accurate single-trial estimator of the variance of our method and demonstrate that this estimator is empirically accurate at low sample counts, enabling a practitioner to know how well they can trust a given low-sample estimate. When validating state-of-the-art semi-supervised models on ImageNet and iNatural-ist2017, our method achieves the same estimates of model performance with up to 10× fewer labels than competing approaches. In particular, we can estimate model F1 scores with a variance of 0.005 using as few as 100 labels.},
  archive   = {C_ICCV},
  author    = {Fait Poms and Vishnu Sarukkai and Ravi Teja Mullapudi and Nimit S. Sohoni and William R. Mark and Deva Ramanan and Kayvon Fatahalian},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01053},
  pages     = {10685-10694},
  title     = {Low-shot validation: Active importance sampling for estimating classifier performance on rare categories},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Lifelong infinite mixture model based on knowledge-driven
dirichlet process. <em>ICCV</em>, 10675–10684. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent research efforts in lifelong learning propose to grow a mixture of models to adapt to an increasing number of tasks. The proposed methodology shows promising results in overcoming catastrophic forgetting. However, the theory behind these successful models is still not well understood. In this paper, we perform the theoretical analysis for lifelong learning models by deriving the risk bounds based on the discrepancy distance between the probabilistic representation of data generated by the model and that corresponding to the target dataset. Inspired by the theoretical analysis, we introduce a new lifelong learning approach, namely the Lifelong Infinite Mixture (LIMix) model, which can automatically expand its network architectures or choose an appropriate component to adapt its parameters for learning a new task, while preserving its previously learnt information. We propose to incorporate the knowledge by means of Dirichlet processes by using a gating mechanism which computes the dependence between the knowledge learnt previously and stored in each component, and a new set of data. Besides, we train a compact Student model which can accumulate cross-domain representations over time and make quick inferences. The code is available at https://github.com/dtuzi123/Lifelong-infinite-mixture-model.},
  archive   = {C_ICCV},
  author    = {Fei Ye and Adrian G. Bors},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01052},
  pages     = {10675-10684},
  title     = {Lifelong infinite mixture model based on knowledge-driven dirichlet process},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning with privileged tasks. <em>ICCV</em>, 10665–10674.
(<a href="https://doi.org/10.1109/ICCV48922.2021.01051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-objective multi-task learning aims to boost the performance of all tasks by leveraging their correlation and conflict appropriately. Nevertheless, in real practice, users may have preference for certain tasks, and other tasks simply serve as privileged or auxiliary tasks to assist the training of target tasks. The privileged tasks thus possess less or even no priority in the final task assessment by users. Motivated by this, we propose a privileged multiple descent algorithm to arbitrate the learning of target tasks and privileged tasks. Concretely, we introduce a privileged parameter so that the optimization direction does not necessarily follow the gradient from the privileged tasks, but concentrates more on the target tasks. Besides, we also encourage a priority parameter for the target tasks to control the potential distraction of optimization direction from the privileged tasks. In this way, the optimization direction can be more aggressively determined by weighting the gradients among target and privileged tasks, and thus highlight more the performance of target tasks under the unified multi-task learning context. Extensive experiments on synthetic and real-world datasets indicate that our method can achieve versatile Pareto solutions under varying preference for the target tasks.},
  archive   = {C_ICCV},
  author    = {Yuru Song and Zan Lou and Shan You and Erkun Yang and Fei Wang and Chen Qian and Changshui Zhang and Xiaogang Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01051},
  pages     = {10665-10674},
  title     = {Learning with privileged tasks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Lipschitz continuity guided knowledge distillation.
<em>ICCV</em>, 10655–10664. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Knowledge distillation has become one of the most important model compression techniques by distilling knowledge from larger teacher networks to smaller student ones. Although great success has been achieved by prior distillation methods via delicately designing various types of knowledge, they overlook the functional properties of neural networks, which makes the process of applying those techniques to new tasks unreliable and non-trivial. To alleviate such problem, in this paper, we initially leverage Lipschitz continuity to better represent the functional characteristic of neural networks and guide the knowledge distillation process. In particular, we propose a novel Lipschitz Continuity Guided Knowledge Distillation framework to faithfully distill knowledge by minimizing the distance between two neural networks’ Lipschitz constants, which enables teacher networks to better regularize student networks and improve the corresponding performance. We derive an explainable approximation algorithm with an explicit theoretical derivation to address the NP-hard problem of calculating the Lipschitz constant. Experimental results have shown that our method outperforms other benchmarks over several knowledge distillation tasks (e.g., classification, segmentation and object detection) on CIFAR-100, ImageNet, and PASCAL VOC datasets. Our code is available at https://github.com/42Shawn/LONDON/tree/master.},
  archive   = {C_ICCV},
  author    = {Yuzhang Shang and Bin Duan and Ziliang Zong and Liqiang Nie and Yan Yan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01050},
  pages     = {10655-10664},
  title     = {Lipschitz continuity guided knowledge distillation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Kernel methods in hyperbolic spaces. <em>ICCV</em>,
10645–10654. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Embedding data in hyperbolic spaces has proven beneficial for many advanced machine learning applications such as image classification and word embeddings. However, working in hyperbolic spaces is not without difficulties as a result of its curved geometry (e.g., computing the Frechet mean of a set of points requires an iterative algorithm). Furthermore, in Euclidean spaces, one can resort to kernel machines that not only enjoy rich theoretical properties but that can also lead to superior representational power (e.g., infinite-width neural networks). In this paper, we introduce positive definite kernel functions for hyperbolic spaces. This brings in two major advantages, 1. kernelization will pave the way to seamlessly benefit from kernel machines in conjunction with hyperbolic embeddings, and 2. the rich structure of the Hilbert spaces associated with kernel machines enables us to simplify various operations involving hyperbolic data. That said, identifying valid kernel functions on curved spaces is not straightforward and is indeed considered an open problem in the learning community. Our work addresses this gap and develops several valid positive definite kernels in hyperbolic spaces, including the universal ones (e.g., RBF). We comprehensively study the proposed kernels on a variety of challenging tasks including few-shot learning, zero-shot learning, person reidentification and knowledge distillation, showing the superiority of the kernelization for hyperbolic representations.},
  archive   = {C_ICCV},
  author    = {Pengfei Fang and Mehrtash Harandi and Lars Petersson},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01049},
  pages     = {10645-10654},
  title     = {Kernel methods in hyperbolic spaces},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DiagViB-6: A diagnostic benchmark suite for vision models in
the presence of shortcut and generalization opportunities.
<em>ICCV</em>, 10635–10644. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Common deep neural networks (DNNs) for image classification have been shown to rely on shortcut opportunities (SO) in the form of predictive and easy-to-represent visual factors. This is known as shortcut learning and leads to impaired generalization. In this work, we show that common DNNs also suffer from shortcut learning when predicting only basic visual object factors of variation (FoV) such as shape, color, or texture. We argue that besides shortcut opportunities, generalization opportunities (GO) are also an inherent part of real-world vision data and arise from partial independence between predicted classes and FoVs. We also argue that it is necessary for DNNs to exploit GO to overcome shortcut learning. Our core contribution is to introduce the Diagnostic Vision Benchmark suite DiagViB-6, which includes datasets and metrics to study a network’s shortcut vulnerability and generalization capability for six independent FoV. In particular, DiagViB-6 allows controlling the type and degree of SO and GO in a dataset. We benchmark a wide range of popular vision architectures and show that they can exploit GO only to a limited extent.},
  archive   = {C_ICCV},
  author    = {Elias Eulig and Piyapat Saranrittichai and Chaithanya Kumar Mummadi and Kilian Rambach and William Beluch and Xiahan Shi and Volker Fischer},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01048},
  pages     = {10635-10644},
  title     = {DiagViB-6: A diagnostic benchmark suite for vision models in the presence of shortcut and generalization opportunities},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Do different deep metric learning losses lead to similar
learned features? <em>ICCV</em>, 10624–10634. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent studies have shown that many deep metric learning loss functions perform very similarly under the same experimental conditions. One potential reason for this unexpected result is that all losses let the network focus on similar image regions or properties. In this paper, we investigate this by conducting a two-step analysis to extract and compare the learned visual features of the same model architecture trained with different loss functions: First, we compare the learned features on the pixel level by correlating saliency maps of the same input images. Second, we compare the clustering of embeddings for several image properties, e.g. object color or illumination. To provide independent control over these properties, photo-realistic 3D car renders similar to images in the Cars196 dataset are generated. In our analysis, we compare 14 pretrained models from a recent study and find that, even though all models perform similarly, different loss functions can guide the model to learn different features. We especially find differences between classification and ranking based losses. Our analysis also shows that some seemingly irrelevant properties can have significant influence on the resulting embedding. We encourage researchers from the deep metric learning community to use our methods to get insights into the features learned by their proposed methods.},
  archive   = {C_ICCV},
  author    = {Konstantin Kobs and Michael Steininger and Andrzej Dulny and Andreas Hotho},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01047},
  pages     = {10624-10634},
  title     = {Do different deep metric learning losses lead to similar learned features?},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LoOp: Looking for optimal hard negative embeddings for deep
metric learning. <em>ICCV</em>, 10614–10623. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep metric learning has been effectively used to learn distance metrics for different visual tasks like image retrieval, clustering, etc. In order to aid the training process, existing methods either use a hard mining strategy to extract the most informative samples or seek to generate hard synthetics using an additional network. Such approaches face different challenges and can lead to biased embeddings in the former case, and (i) harder optimization (ii) slower training speed (iii) higher model complexity in the latter case. In order to overcome these challenges, we propose a novel approach that looks for optimal hard negatives (LoOp) in the embedding space, taking full advantage of each tuple by calculating the minimum distance between a pair of positives and a pair of negatives. Unlike mining-based methods, our approach considers the entire space between pairs of embeddings to calculate the optimal hard negatives. Extensive experiments combining our approach and representative metric learning losses reveal a significant boost in performance on three benchmark datasets 1 .},
  archive   = {C_ICCV},
  author    = {Bhavya Vasudeva and Puneesh Deora and Saumik Bhattacharya and Umapada Pal and Sukalpa Chanda},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01046},
  pages     = {10614-10623},
  title     = {LoOp: Looking for optimal hard negative embeddings for deep metric learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Contrastive learning for label efficient semantic
segmentation. <em>ICCV</em>, 10603–10613. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collecting labeled data for the task of semantic segmentation is expensive and time-consuming, as it requires dense pixel-level annotations. While recent Convolutional Neural Network (CNN) based semantic segmentation approaches have achieved impressive results by using large amounts of labeled training data, their performance drops significantly as the amount of labeled data decreases. This happens because deep CNNs trained with the de facto cross-entropy loss can easily overfit to small amounts of labeled data. To address this issue, we propose a simple and effective contrastive learning-based training strategy in which we first pretrain the network using a pixel-wise, label-based contrastive loss, and then fine-tune it using the cross-entropy loss. This approach increases intra-class compactness and inter-class separability, thereby resulting in a better pixel classifier. We demonstrate the effectiveness of the proposed training strategy using the Cityscapes and PASCAL VOC 2012 segmentation datasets. Our results show that pretraining with the proposed contrastive loss results in large performance gains (more than 20\% absolute improvement in some settings) when the amount of labeled data is limited. In many settings, the proposed contrastive pretraining strategy, which does not use any additional data, is able to match or outperform the widely-used ImageNet pretraining strategy that uses more than a million additional labeled images.},
  archive   = {C_ICCV},
  author    = {Xiangyun Zhao and Raviteja Vemulapalli and Philip Andrew Mansfield and Boqing Gong and Bradley Green and Lior Shapira and Ying Wu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01045},
  pages     = {10603-10613},
  title     = {Contrastive learning for label efficient semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Von mises–fisher loss: An exploration of embedding
geometries for supervised learning. <em>ICCV</em>, 10592–10602. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent work has argued that classification losses utilizing softmax cross-entropy are superior not only for fixed-set classification tasks, but also by outperforming losses developed specifically for open-set tasks including few-shot learning and retrieval. Softmax classifiers have been studied using different embedding geometries—Euclidean, hyperbolic, and spherical—and claims have been made about the superiority of one or another, but they have not been systematically compared with careful controls. We conduct an empirical investigation of embedding geometry on soft-max losses for a variety of fixed-set classification and image retrieval tasks. An interesting property observed for the spherical losses lead us to propose a probabilistic classifier based on the von Mises–Fisher distribution, and we show that it is competitive with state-of-the-art methods while producing improved out-of-the-box calibration. We provide guidance regarding the trade-offs between losses and how to choose among them.},
  archive   = {C_ICCV},
  author    = {Tyler R. Scott and Andrew C. Gallagher and Michael C. Mozer},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01044},
  pages     = {10592-10602},
  title     = {Von Mises–Fisher loss: An exploration of embedding geometries for supervised learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Webly supervised fine-grained recognition: Benchmark
datasets and an approach. <em>ICCV</em>, 10582–10591. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning from the web can ease the extreme dependence of deep learning on large-scale manually labeled datasets. Especially for fine-grained recognition, which targets at distinguishing subordinate categories, it will significantly reduce the labeling costs by leveraging free web data. Despite its significant practical and research value, the webly supervised fine-grained recognition problem is not extensively studied in the computer vision community, largely due to the lack of high-quality datasets. To fill this gap, in this paper we construct two new benchmark webly supervised fine-grained datasets, termed WebFG-496 and WebiNat-5089, respectively. In concretely, WebFG-496 consists of three sub-datasets containing a total of 53,339 web training images with 200 species of birds (Web-bird), 100 types of aircrafts (Web-aircraft), and 196 models of cars (Web-car). For WebiNat-5089, it contains 5089 sub-categories and more than 1.1 million web training images, which is the largest webly supervised fine-grained dataset ever. As a minor contribution, we also propose a novel webly supervised method (termed &quot;Peer-learning&quot;) for benchmarking these datasets. Comprehensive experimental results and analyses on two new benchmark datasets demonstrate that the proposed method achieves superior performance over the competing baseline models and states-of-the-art. Our benchmark datasets and the source codes of Peer-learning have been made available at https://github.com/NUST-Machine-Intelligence-Laboratory/weblyFG-dataset.},
  archive   = {C_ICCV},
  author    = {Zeren Sun and Yazhou Yao and Xiu-Shen Wei and Yongshun Zhang and Fumin Shen and Jianxin Wu and Jian Zhang and Heng Tao Shen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01043},
  pages     = {10582-10591},
  title     = {Webly supervised fine-grained recognition: Benchmark datasets and an approach},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weakly supervised representation learning with coarse
labels. <em>ICCV</em>, 10573–10581. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the development of computational power and techniques for data collection, deep learning demonstrates a superior performance over most existing algorithms on visual benchmark data sets. Many efforts have been devoted to studying the mechanism of deep learning. One important observation is that deep learning can learn the discriminative patterns from raw materials directly in a task-dependent manner. Therefore, the representations obtained by deep learning outperform hand-crafted features significantly. However, for some real-world applications, it is too expensive to collect the task-specific labels, such as visual search in online shopping. Compared to the limited availability of these task-specific labels, their coarse-class labels are much more affordable, but representations learned from them can be suboptimal for the target task. To mitigate this challenge, we propose an algorithm to learn the fine-grained patterns for the target task, when only its coarse-class labels are available. More importantly, we provide a theoretical guarantee for this. Extensive experiments on real-world data sets demonstrate that the proposed method can significantly improve the performance of learned representations on the target task, when only coarse-class information is available for training.},
  archive   = {C_ICCV},
  author    = {Yuanhong Xu and Qi Qian and Hao Li and Rong Jin and Juhua Hu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01042},
  pages     = {10573-10581},
  title     = {Weakly supervised representation learning with coarse labels},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Focus on the positives: Self-supervised learning for
biodiversity monitoring. <em>ICCV</em>, 10563–10572. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the problem of learning self-supervised representations from unlabeled image collections. Unlike existing approaches that attempt to learn useful features by maximizing similarity between augmented versions of each input image or by speculatively picking negative samples, we instead also make use of the natural variation that occurs in image collections that are captured using static monitoring cameras. To achieve this, we exploit readily available context data that encodes information such as the spatial and temporal relationships between the input images. We are able to learn representations that are surprisingly effective for downstream supervised classification, by first identifying high probability positive pairs at training time, i.e. those images that are likely to depict the same visual concept. For the critical task of global biodiversity monitoring, this results in image features that can be adapted to challenging visual species classification tasks with limited human supervision. We present results on four different camera trap image collections, across three different families of self-supervised learning methods, and show that careful image selection at training time results in superior performance compared to existing baselines such as conventional self-supervised training and transfer learning.},
  archive   = {C_ICCV},
  author    = {Omiros Pantazis and Gabriel J. Brostow and Kate E. Jones and Oisin Mac Aodha},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01041},
  pages     = {10563-10572},
  title     = {Focus on the positives: Self-supervised learning for biodiversity monitoring},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Partner-assisted learning for few-shot image classification.
<em>ICCV</em>, 10553–10562. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Few-shot Learning has been studied to mimic human visual capabilities and learn effective models without the need of exhaustive human annotation. Even though the idea of meta-learning for adaptation has dominated the few-shot learning methods, how to train a feature extractor is still a challenge. In this paper, we focus on the design of training strategy to obtain an elemental representation such that the prototype of each novel class can be estimated from a few labeled samples. We propose a two-stage training scheme, Partner-Assisted Learning (PAL), which first trains a Partner Encoder to model pair-wise similarities and extract features serving as soft-anchors, and then trains a Main Encoder by aligning its outputs with soft-anchors while attempting to maximize classification performance. Two alignment constraints from logit-level and feature-level are designed individually. For each few-shot task, we perform prototype classification. Our method consistently outperforms the state-of-the-art methods on four benchmarks. Detailed ablation studies of PAL are provided to justify the selection of each component involved in training.},
  archive   = {C_ICCV},
  author    = {Jiawei Ma and Hanchen Xie and Guangxing Han and Shih-Fu Chang and Aram Galstyan and Wael Abd-Almageed},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01040},
  pages     = {10553-10562},
  title     = {Partner-assisted learning for few-shot image classification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Space-time crop &amp; attend: Improving cross-modal video
representation learning. <em>ICCV</em>, 10540–10552. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The quality of the image representations obtained from self-supervised learning depends strongly on the type of data augmentations used in the learning formulation. Recent papers have ported these methods from still images to videos and found that leveraging both audio and video signals yields strong gains; however, they did not find that spatial augmentations such as cropping, which are very important for still images, work as well for videos. In this paper, we improve these formulations in two ways unique to the spatio-temporal aspect of videos. First, for space, we show that spatial augmentations such as cropping do work well for videos too, but that previous implementations, due to the high processing and memory cost, could not do this at a scale sufficient for it to work well. To address this issue, we first introduce Feature Crop, a method to simulate such augmentations much more efficiently directly in feature space. Second, we show that as opposed to naïve average pooling, the use of transformer-based attention improves performance significantly, and is well suited for processing feature crops. Combining both of our discoveries into a new method, Space-Time Crop &amp; Attend (STiCA) we achieve state-of-the-art performance across multiple video-representation learning benchmarks. In particular, we achieve new state-of-the-art accuracies of 67.0\% on HMDB-51 and 93.1\% on UCF-101 when pre-training on Kinetics-400. Code and pretrained models are available 1 .},
  archive   = {C_ICCV},
  author    = {Mandela Patrick and Po-Yao Huang and Ishan Misra and Florian Metze and Andrea Vedaldi and Yuki M. Asano and João Henriques},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01039},
  pages     = {10540-10552},
  title     = {Space-time crop &amp; attend: Improving cross-modal video representation learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Personalized image semantic segmentation. <em>ICCV</em>,
10529–10539. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Semantic segmentation models trained on public datasets have achieved great success in recent years. However, these models didn’t consider the personalization issue of segmentation though it is important in practice. In this paper, we address the problem of personalized image segmentation. The objective is to generate more accurate segmentation results on unlabeled personalized images by investigating the data’s personalized traits. To open up future research in this area, we collect a large dataset containing various users’ personalized images called PSS (Personalized Semantic Segmentation). We also survey some recent researches related to this problem and report their performance on our dataset. Furthermore, by observing the correlation among a user’s personalized images, we propose a baseline method that incorporates the inter-image context when segmenting certain images. Extensive experiments show that our method outperforms the existing methods on the proposed dataset. The code and the PSS dataset are available at https://mmcheng.net/pss/.},
  archive   = {C_ICCV},
  author    = {Yu Zhang and Chang-Bin Zhang and Peng-Tao Jiang and Ming-Ming Cheng and Feng Mao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01038},
  pages     = {10529-10539},
  title     = {Personalized image semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Region similarity representation learning. <em>ICCV</em>,
10519–10528. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present Region Similarity Representation Learning (ReSim), a new approach to self-supervised representation learning for localization-based tasks such as object detection and segmentation. While existing work has largely focused on solely learning global representations for an entire image, ReSim learns both regional representations for localization as well as semantic image-level representations. ReSim operates by sliding a fixed-sized window across the overlapping area between two views (e.g., image crops), aligning these areas with their corresponding convolutional feature map regions, and then maximizing the feature similarity across views. As a result, ReSim learns spatially and semantically consistent feature representation throughout the convolutional feature maps of a neural network. A shift or scale of an image region, e.g., a shift or scale of an object, has a corresponding change in the feature maps; this allows downstream tasks to leverage these representations for localization. Through object detection, instance segmentation, and dense pose estimation experiments, we illustrate how ReSim learns representations which significantly improve the localization and classification performance compared to a competitive MoCo-v2 baseline: $ + 2.7AP_{75}^{bb}VOC$, $ + 1.1AP_{75}^{bb}COCO$, and +1.9 AP mk Cityscapes. Code and pre-trained models are released at: https://github.com/Tete-Xiao/ReSim},
  archive   = {C_ICCV},
  author    = {Tete Xiao and Colorado J Reed and Xiaolong Wang and Kurt Keutzer and Trevor Darrell},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01037},
  pages     = {10519-10528},
  title     = {Region similarity representation learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Impact of aliasing on generalization in deep convolutional
networks. <em>ICCV</em>, 10509–10518. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate the impact of aliasing on generalization in Deep Convolutional Networks and show that data augmentation schemes alone are unable to prevent it due to structural limitations in widely used architectures. Drawing insights from frequency analysis theory, we take a closer look at ResNet and EfficientNet architectures and review the trade-off between aliasing and information loss in each of their major components. We show how to mitigate aliasing by inserting non-trainable low-pass filters at key locations, particularly where networks lack the capacity to learn them. These simple architectural changes lead to substantial improvements in generalization on i.i.d. and even more on out-of-distribution conditions, such as image classification under natural corruptions on ImageNet-C [11] and few-shot learning on Meta-Dataset [26]. State-of-the art results are achieved on both datasets without introducing additional trainable parameters and using the default hyper-parameters of open source codebases.},
  archive   = {C_ICCV},
  author    = {Cristina Vasconcelos and Hugo Larochelle and Vincent Dumoulin and Rob Romijnders and Nicolas Le Roux and Ross Goroshin},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01036},
  pages     = {10509-10518},
  title     = {Impact of aliasing on generalization in deep convolutional networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Poly-NL: Linear complexity non-local layers with 3rd order
polynomials. <em>ICCV</em>, 10498–10508. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Spatial self-attention layers, in the form of Non-Local blocks, introduce long-range dependencies in Convolutional Neural Networks by computing pairwise similarities among all possible positions. Such pairwise functions underpin the effectiveness of non-local layers, but also determine a complexity that scales quadratically with respect to the input size both in space and time. This is a severely limiting factor that practically hinders the applicability of non-local blocks to even moderately sized inputs. Previous works focused on reducing the complexity by modifying the underlying matrix operations, however in this work we aim to retain full expressiveness of non-local layers while keeping complexity linear. We overcome the efficiency limitation of non-local blocks by framing them as special cases of 3rd order polynomial functions. This fact enables us to formulate novel fast Non-Local blocks, capable of reducing the complexity from quadratic to linear with no loss in performance, by replacing any direct computation of pairwise similarities with element-wise multiplications. The proposed method, which we dub as &quot;Poly-NL&quot;, is competitive with state-of-the-art performance across image recognition, instance segmentation, and face detection tasks, while having considerably less computational overhead.},
  archive   = {C_ICCV},
  author    = {Francesca Babiloni and Ioannis Marras and Filippos Kokkinos and Jiankang Deng and Grigorios Chrysos and Stefanos Zafeiriou},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01035},
  pages     = {10498-10508},
  title     = {Poly-NL: Linear complexity non-local layers with 3rd order polynomials},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Not all operations contribute equally: Hierarchical
operation-adaptive predictor for neural architecture search.
<em>ICCV</em>, 10488–10497. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph-based predictors have recently shown promising results on neural architecture search (NAS). Despite their efficiency, current graph-based predictors treat all operations equally, resulting in biased topological knowledge of cell architectures. Intuitively, not all operations are equally significant during forwarding propagation when aggregating information from these operations to another operation. To address the above issue, we propose a Hierarchical Operation-adaptive Predictor (HOP) for NAS. HOP contains an operation-adaptive attention module (OAM) to capture the diverse knowledge between operations by learning the relative significance of operations in cell architectures during aggregation over iterations. In addition, a cell-hierarchical gated module (CGM) further refines and enriches the obtained topological knowledge of cell architectures, by integrating cell information from each iteration of OAM. The experimental results compared with state-of-the-art predictors demonstrate the capability of our proposed HOP.},
  archive   = {C_ICCV},
  author    = {Ziye Chen and Yibing Zhan and Baosheng Yu and Mingming Gong and Bo Du},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01034},
  pages     = {10488-10497},
  title     = {Not all operations contribute equally: Hierarchical operation-adaptive predictor for neural architecture search},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). High-resolution optical flow from 1D attention and
correlation. <em>ICCV</em>, 10478–10487. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Optical flow is inherently a 2D search problem, and thus the computational complexity grows quadratically with respect to the search window, making large displacements matching infeasible for high-resolution images. In this paper, we take inspiration from Transformers and propose a new method for high-resolution optical flow estimation with significantly less computation. Specifically, a 1D attention operation is first applied in the vertical direction of the target image, and then a simple 1D correlation in the horizontal direction of the attended image is able to achieve 2D correspondence modeling effect. The directions of attention and correlation can also be exchanged, resulting in two 3D cost volumes that are concatenated for optical flow estimation. The novel 1D formulation empowers our method to scale to very high-resolution input images while maintaining competitive performance. Extensive experiments on Sintel, KITTI and real-world 4K (2160 × 3840) resolution images demonstrated the effectiveness and superiority of our proposed method. Code and models are available at https://github.com/haofeixu/flow1d.},
  archive   = {C_ICCV},
  author    = {Haofei Xu and Jiaolong Yang and Jianfei Cai and Juyong Zhang and Xin Tong},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01033},
  pages     = {10478-10487},
  title     = {High-resolution optical flow from 1D attention and correlation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploring simple 3D multi-object tracking for autonomous
driving. <em>ICCV</em>, 10468–10477. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D multi-object tracking in LiDAR point clouds is a key ingredient for self-driving vehicles. Existing methods are predominantly based on the tracking-by-detection pipeline and inevitably require a heuristic matching step for the detection association. In this paper, we present SimTrack to simplify the hand-crafted tracking paradigm by proposing an end-to-end trainable model for joint detection and tracking from raw point clouds. Our key design is to predict the first-appear location of each object in a given snippet to get the tracking identity and then update the location based on motion estimation. In the inference, the heuristic matching step can be completely waived by a simple read-off operation. SimTrack integrates the tracked object association, newborn object detection, and dead track killing in a single unified model. We conduct extensive evaluations on two large-scale datasets: nuScenes and Waymo Open Dataset. Experimental results reveal that our simple approach compares favorably with the state-of-the-art methods while ruling out the heuristic matching rules.},
  archive   = {C_ICCV},
  author    = {Chenxu Luo and Xiaodong Yang and Alan Yuille},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01032},
  pages     = {10468-10477},
  title     = {Exploring simple 3D multi-object tracking for autonomous driving},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Point-set distances for learning representations of 3D point
clouds. <em>ICCV</em>, 10458–10467. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning an effective representation of 3D point clouds requires a good metric to measure the discrepancy between two 3D point sets, which is non-trivial due to their irregularity. Most of the previous works resort to using the Chamfer discrepancy or Earth Mover’s distance, but those metrics are either ineffective in measuring the differences between point clouds or computationally expensive. In this paper, we conduct a systematic study with extensive experiments on distance metrics for 3D point clouds. From this study, we propose to use sliced Wasserstein distance and its variants for learning representations of 3D point clouds. In addition, we introduce a new algorithm to estimate sliced Wasserstein distance that guarantees that the estimated value is close enough to the true one. Experiments show that the sliced Wasserstein distance and its variants allow the neural network to learn a more efficient representation compared to the Chamfer discrepancy. We demonstrate the efficiency of the sliced Wasserstein metric and its variants on several tasks in 3D computer vision including training a point cloud autoencoder, generative modeling, transfer learning, and point cloud registration.},
  archive   = {C_ICCV},
  author    = {Trung Nguyen and Quang-Hieu Pham and Tam Le and Tung Pham and Nhat Ho and Binh-Son Hua},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01031},
  pages     = {10458-10467},
  title     = {Point-set distances for learning representations of 3D point clouds},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SGMNet: Learning rotation-invariant point cloud
representations via sorted gram matrix. <em>ICCV</em>, 10448–10457. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, various works that attempted to introduce rotation invariance to point cloud analysis have devised point-pair features, such as angles and distances. In these methods, however, the point-pair is only comprised of the center point and its adjacent points in a vicinity, which may bring information loss to the local feature representation. In this paper, we instead connect each point densely with all other points in a local neighborhood to compose the point-pairs. Specifically, we present a simple but effective local feature representation, called sorted Gram matrix(SGM), which is not only invariant to arbitrary rotations, but also models the pair-wise relationship of all the points in a neighbor-hood. In more detail, we utilize vector inner product to model distance- and angle-information between two points, and in a local patch it naturally forms a Gram matrix. In order to guarantee permutation invariance, we sort the correlation value in Gram matrix for each point, therefore this geometric feature names sorted Gram matrix. Furthermore, we mathematically prove that the Gram matrix is rotation-invariant and sufficient to model the inherent structure of a point cloud patch. We then use SGM as features in convolution, which can be readily integrated as a drop-in module into any point-based networks. Finally, we evaluated the proposed method on two widely used datasets, and it outperforms previous state-of-the-arts on both shape classification and part segmentation tasks by a large margin.},
  archive   = {C_ICCV},
  author    = {Jianyun Xu and Xin Tang and Yushi Zhu and Jie Sun and Shiliang Pu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01030},
  pages     = {10448-10457},
  title     = {SGMNet: Learning rotation-invariant point cloud representations via sorted gram matrix},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Temporally-coherent surface reconstruction via
metric-consistent atlases. <em>ICCV</em>, 10438–10447. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a method for the unsupervised reconstruction of a temporally-coherent sequence of surfaces from a sequence of time-evolving point clouds, yielding dense, semantically meaningful correspondences between all keyframes. We represent the reconstructed surface as an atlas, using a neural network. Using canonical correspondences defined via the atlas, we encourage the reconstruction to be as isometric as possible across frames, leading to semantically-meaningful reconstruction. Through experiments and comparisons, we empirically show that our method achieves results that exceed that state of the art in the accuracy of unsupervised correspondences and accuracy of surface reconstruction.},
  archive   = {C_ICCV},
  author    = {Jan Bednarik and Vladimir G. Kim and Siddhartha Chaudhuri and Shaifali Parashar and Mathieu Salzmann and Pascal Fua and Noam Aigerman},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01029},
  pages     = {10438-10447},
  title     = {Temporally-coherent surface reconstruction via metric-consistent atlases},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning spatio-temporal transformer for visual tracking.
<em>ICCV</em>, 10428–10437. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a new tracking architecture with an encoder-decoder transformer as the key component. The encoder models the global spatio-temporal feature dependencies between target objects and search regions, while the decoder learns a query embedding to predict the spatial positions of the target objects. Our method casts object tracking as a direct bounding box prediction problem, without using any proposals or predefined anchors. With the encoder-decoder transformer, the prediction of objects just uses a simple fully-convolutional network, which estimates the corners of objects directly. The whole method is end-to-end, does not need any postprocessing steps such as cosine window and bounding box smoothing, thus largely simplifying existing tracking pipelines. The proposed tracker achieves state-of-the-art performance on multiple challenging short-term and long-term benchmarks, while running at real-time speed, being 6× faster than Siam R-CNN [54]. Code and models are open-sourced at https://github.com/researchmm/Stark.},
  archive   = {C_ICCV},
  author    = {Bin Yan and Houwen Peng and Jianlong Fu and Dong Wang and Huchuan Lu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01028},
  pages     = {10428-10437},
  title     = {Learning spatio-temporal transformer for visual tracking},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PARTS: Unsupervised segmentation with slots, attention and
independence maximization. <em>ICCV</em>, 10419–10427. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {From an early age, humans perceive the visual world as composed of coherent objects with distinctive properties such as shape, size, and color. There is great interest in building models that are able to learn similar structure, ideally in an unsupervised manner. Learning such structure from complex 3D scenes that include clutter, occlusions, interactions, and camera motion is still an open challenge. We present a model that is able to segment visual scenes from complex 3D environments into distinct objects, learn disentangled representations of individual objects, and form consistent and coherent predictions of future frames, in a fully unsupervised manner. Our model (named PARTS) builds on recent approaches that utilize iterative amortized inference and transition dynamics for deep generative models. We achieve dramatic improvements in performance by introducing several novel contributions. We introduce a recurrent slot-attention like encoder which allows for top-down influence during inference. We argue that when inferring scene structure from image sequences it is better to use a fixed prior which is shared across the sequence rather than an auto-regressive prior as often used in prior work. We demonstrate our model’s success on three different video datasets (the popular benchmark CLEVRER; a simulated 3D Playroom environment; and a real-world Robotics Arm dataset). Finally, we analyze the contributions of the various model components and the representations learned by the model.},
  archive   = {C_ICCV},
  author    = {Daniel Zoran and Rishabh Kabra and Alexander Lerchner and Danilo J. Rezende},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01027},
  pages     = {10419-10427},
  title     = {PARTS: Unsupervised segmentation with slots, attention and independence maximization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Motion-augmented self-training for video recognition at
smaller scale. <em>ICCV</em>, 10409–10418. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The goal of this paper is to self-train a 3D convolutional neural network on an unlabeled video collection for deployment on small-scale video collections. As smaller video datasets benefit more from motion than appearance, we strive to train our network using optical flow, but avoid its computation during inference. We propose the first motion-augmented self-training regime, we call MotionFit. We start with supervised training of a motion model on a small, and labeled, video collection. With the motion model we generate pseudo-labels for a large unlabeled video collection, which enables us to transfer knowledge by learning to predict these pseudo-labels with an appearance model. Moreover, we introduce a multi-clip loss as a simple yet efficient way to improve the quality of the pseudo-labeling, even without additional auxiliary tasks. We also take into consideration the temporal granularity of videos during self-training of the appearance model, which was missed in previous works. As a result we obtain a strong motion-augmented representation model suited for video downstream tasks like action recognition and clip retrieval. On small-scale video datasets, MotionFit outperforms alternatives for knowledge transfer by 5\%-8\%, video-only self-supervision by 1\%-7\% and semi-supervised learning by 9\%-18\% using the same amount of class labels.},
  archive   = {C_ICCV},
  author    = {Kirill Gavrilyuk and Mihir Jain and Ilia Karmanov and Cees G. M. Snoek},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01026},
  pages     = {10409-10418},
  title     = {Motion-augmented self-training for video recognition at smaller scale},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ViewNet: Unsupervised viewpoint estimation from conditional
generation. <em>ICCV</em>, 10398–10408. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Understanding the 3D world without supervision is currently a major challenge in computer vision as the annotations required to supervise deep networks for tasks in this domain are expensive to obtain on a large scale. In this paper, we address the problem of unsupervised viewpoint estimation. We formulate this as a self-supervised learning task, where image reconstruction provides the supervision needed to predict the camera viewpoint. Specifically, we make use of pairs of images of the same object at training time, from unknown viewpoints, to self-supervise training by combining the viewpoint information from one image with the appearance information from the other. We demonstrate that using a perspective spatial transformer allows efficient viewpoint learning, outperforming existing unsupervised approaches on synthetic data, and obtains competitive results on the challenging PASCAL3D+ dataset.},
  archive   = {C_ICCV},
  author    = {Octave Mariotti and Oisin Mac Aodha and Hakan Bilen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01025},
  pages     = {10398-10408},
  title     = {ViewNet: Unsupervised viewpoint estimation from conditional generation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Curious representation learning for embodied intelligence.
<em>ICCV</em>, 10388–10397. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-supervised representation learning has achieved remarkable success in recent years. By subverting the need for supervised labels, such approaches are able to utilize the numerous unlabeled images that exist on the Internet and in photographic datasets. Yet to build truly intelligent agents, we must construct representation learning algorithms that can learn not only from datasets but also learn from environments. An agent in a natural environment will not typically be fed curated data. Instead, it must explore its environment to acquire the data it will learn from. We propose a framework, curious representation learning (CRL), which jointly learns a reinforcement learning policy and a visual representation model. The policy is trained to maximize the error of the representation learner, and in doing so is incentivized to explore its environment. At the same time, the learned representation becomes stronger and stronger as the policy feeds it ever harder data to learn from. Our learned representations enable promising transfer to downstream navigation tasks, performing better than or comparably to ImageNet pretraining without using any supervision at all. In addition, despite being trained in simulation, our learned representations can obtain interpretable results on real images. Code is available at https://yilundu.github.io/crl/.},
  archive   = {C_ICCV},
  author    = {Yilun Du and Chuang Gan and Phillip Isola},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01024},
  pages     = {10388-10397},
  title     = {Curious representation learning for embodied intelligence},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BuildingNet: Learning to label 3D buildings. <em>ICCV</em>,
10377–10387. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce BuildingNet: (a) a large-scale dataset of 3D building models whose exteriors are consistently labeled, and (b) a graph neural network that labels building meshes by analyzing spatial and structural relations of their geometric primitives. To create our dataset, we used crowdsourcing combined with expert guidance, resulting in 513K annotated mesh primitives, grouped into 292K semantic part components across 2K building models. The dataset covers several building categories, such as houses, churches, skyscrapers, town halls, libraries, and castles. We include a benchmark for evaluating mesh and point cloud labeling. Buildings have more challenging structural complexity compared to objects in existing benchmarks (e.g., ShapeNet, PartNet), thus, we hope that our dataset can nurture the development of algorithms that are able to cope with such large-scale geometric data for both vision and graphics tasks e.g., 3D semantic segmentation, part-based generative models, correspondences, texturing, and analysis of point cloud data acquired from real-world buildings. Finally, we show that our mesh-based graph neural network significantly improves performance over several baselines for labeling 3D meshes. Our project page www.buildingnet.org includes our dataset and code.},
  archive   = {C_ICCV},
  author    = {Pratheba Selvaraju and Mohamed Nabail and Marios Loizou and Maria Maslioukova and Melinos Averkiou and Andreas Andreou and Siddhartha Chaudhuri and Evangelos Kalogerakis},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01023},
  pages     = {10377-10387},
  title     = {BuildingNet: Learning to label 3D buildings},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distilling holistic knowledge with graph neural networks.
<em>ICCV</em>, 10367–10376. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Knowledge Distillation (KD) aims at transferring knowledge from a larger well-optimized teacher network to a smaller learnable student network. Existing KD methods have mainly considered two types of knowledge, namely the individual knowledge and the relational knowledge. However, these two types of knowledge are usually modeled independently while the inherent correlations between them are largely ignored. It is critical for sufficient student network learning to integrate both individual knowledge and relational knowledge while reserving their inherent correlation. In this paper, we propose to distill the novel holistic knowledge based on an attributed graph constructed among instances. The holistic knowledge is represented as a unified graph-based embedding by aggregating individual knowledge from relational neighborhood samples with graph neural networks, the student network is learned by distilling the holistic knowledge in a contrastive manner. Extensive experiments and ablation studies are conducted on benchmark datasets, the results demonstrate the effectiveness of the proposed method. The code has been published in https://github.com/wyc-ruiker/HKD},
  archive   = {C_ICCV},
  author    = {Sheng Zhou and Yucheng Wang and Defang Chen and Jiawei Chen and Xin Wang and Can Wang and Jiajun Bu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01022},
  pages     = {10367-10376},
  title     = {Distilling holistic knowledge with graph neural networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). RANK-NOSH: Efficient predictor-based architecture search
via non-uniform successive halving. <em>ICCV</em>, 10357–10366. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Predictor-based algorithms have achieved remarkable performance in the Neural Architecture Search (NAS) tasks. However, these methods suffer from high computation costs, as training the performance predictor usually requires training and evaluating hundreds of architectures from scratch. Previous works along this line mainly focus on reducing the number of architectures required to fit the predictor. In this work, we tackle this challenge from a different perspective - improve search efficiency by cutting down the computation budget of architecture training. We propose NOn-uniform Successive Halving (NOSH), a hierarchical scheduling algorithm that terminates the training of underperforming architectures early to avoid wasting budget. To effectively leverage the non-uniform supervision signals produced by NOSH, we formulate predictor-based architecture search as learning to rank with pairwise comparisons. The resulting method - RANK-NOSH, reduces the search budget by ~ 5× while achieving competitive or even better performance than previous state-of-the-art predictor-based methods on various spaces and datasets.},
  archive   = {C_ICCV},
  author    = {Ruochen Wang and Xiangning Chen and Minhao Cheng and Xiaocheng Tang and Cho-Jui Hsieh},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01021},
  pages     = {10357-10366},
  title     = {RANK-NOSH: Efficient predictor-based architecture search via non-uniform successive halving},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adversarial unsupervised domain adaptation with conditional
and label shift: Infer, align and iterate. <em>ICCV</em>, 10347–10356.
(<a href="https://doi.org/10.1109/ICCV48922.2021.01020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we propose an adversarial unsupervised domain adaptation (UDA) method under inherent conditional and label shifts, in which we aim to align the distributions w.r.t. both p(x|y) and p(y). Since labels are inaccessible in a target domain, conventional adversarial UDA methods assume that p(y) is invariant across domains and rely on aligning p(x) as an alternative to the p(x|y) alignment. To address this, we provide a thorough theoretical and empirical analysis of the conventional adversarial UDA methods under both conditional and label shifts, and propose a novel and practical alternative optimization scheme for adversarial UDA. Specifically, we infer the marginal p(y) and align p(x|y) iteratively at the training stage, and precisely align the posterior p(y|x) at the testing stage. Our experimental results demonstrate its effectiveness on both classification and segmentation UDA and partial UDA.},
  archive   = {C_ICCV},
  author    = {Xiaofeng Liu and Zhenhua Guo and Site Li and Fangxu Xing and Jane You and C.-C. Jay Kuo and Georges El Fakhri and Jonghye Woo},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01020},
  pages     = {10347-10356},
  title     = {Adversarial unsupervised domain adaptation with conditional and label shift: Infer, align and iterate},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Refining activation downsampling with SoftPool.
<em>ICCV</em>, 10337–10346. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Convolutional Neural Networks (CNNs) use pooling to decrease the size of activation maps. This process is crucial to increase the receptive fields and to reduce computational requirements of subsequent convolutions. An important feature of the pooling operation is the minimization of information loss, with respect to the initial activation maps, without a significant impact on the computation and memory overhead. To meet these requirements, we propose SoftPool: a fast and efficient method for exponentially weighted activation downsampling. Through experiments across a range of architectures and pooling methods, we demonstrate that SoftPool can retain more information in the reduced activation maps. This refined downsampling leads to improvements in a CNN’s classification accuracy. Experiments with pooling layer substitutions on ImageNet1K show an increase in accuracy over both original architectures and other pooling methods. We also test SoftPool on video datasets for action recognition. Again, through the direct replacement of pooling layers, we observe consistent performance improvements while computational loads and memory requirements remain limited 1 .},
  archive   = {C_ICCV},
  author    = {Alexandros Stergiou and Ronald Poppe and Grigorios Kalliatakis},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01019},
  pages     = {10337-10346},
  title     = {Refining activation downsampling with SoftPool},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Warp consistency for unsupervised learning of dense
correspondences. <em>ICCV</em>, 10326–10336. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The key challenge in learning dense correspondences lies in the lack of ground-truth matches for real image pairs. While photometric consistency losses provide unsupervised alternatives, they struggle with large appearance changes, which are ubiquitous in geometric and semantic matching tasks. Moreover, methods relying on synthetic training pairs often suffer from poor generalisation to real data.We propose Warp Consistency, an unsupervised learning objective for dense correspondence regression. Our objective is effective even in settings with large appearance and view-point changes. Given a pair of real images, we first construct an image triplet by applying a randomly sampled warp to one of the original images. We derive and analyze all flow-consistency constraints arising between the triplet. From our observations and empirical results, we design a general unsupervised objective employing two of the derived constraints. We validate our warp consistency loss by training three recent dense correspondence networks for the geometric and semantic matching tasks. Our approach sets a new stateof-the-art on several challenging benchmarks, including MegaDepth, RobotCar and TSS. Code and models are at github.com/PruneTruong/DenseMatching.},
  archive   = {C_ICCV},
  author    = {Prune Truong and Martin Danelljan and Fisher Yu and Luc Van Gool},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01018},
  pages     = {10326-10336},
  title     = {Warp consistency for unsupervised learning of dense correspondences},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Instance similarity learning for unsupervised feature
representation. <em>ICCV</em>, 10316–10325. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose an instance similarity learning (ISL) method for unsupervised feature representation. Conventional methods assign close instance pairs in the feature space with high similarity, which usually leads to wrong pairwise relationship for large neighborhoods because the Euclidean distance fails to depict the true semantic similarity on the feature manifold. On the contrary, our method mines the feature manifold in an unsupervised manner, through which the semantic similarity among instances is learned in order to obtain discriminative representations. Specifically, we employ the Generative Adversarial Networks (GAN) to mine the underlying feature manifold, where the generated features are applied as the proxies to progressively explore the feature manifold so that the semantic similarity among instances is acquired as reliable pseudo supervision. Extensive experiments on image classification demonstrate the superiority of our method compared with the state-of-the-art methods. The code is available at https://github.com/ZiweiWangTHU/ISL.git.},
  archive   = {C_ICCV},
  author    = {Ziwei Wang and Yunsong Wang and Ziyi Wu and Jiwen Lu and Jie Zhou},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01017},
  pages     = {10316-10325},
  title     = {Instance similarity learning for unsupervised feature representation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mean shift for self-supervised learning. <em>ICCV</em>,
10306–10315. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most recent self-supervised learning (SSL) algorithms learn features by contrasting between instances of images or by clustering the images and then contrasting between the image clusters. We introduce a simple mean-shift algorithm that learns representations by grouping images together without contrasting between them or adopting much of prior on the structure or number of the clusters. We simply &quot;shift&quot; the embedding of each image to be close to the &quot;mean&quot; of the neighbors of its augmentation. Since the closest neighbor is always another augmentation of the same image, our model will be identical to BYOL when using only one nearest neighbor instead of 5 used in our experiments. Our model achieves 72.4\% on ImageNet linear evaluation with ResNet50 at 200 epochs outperforming BYOL. Also, our method outperforms the SOTA by a large margin when using weak augmentations only, facilitating adoption of SSL for other modalities. Our code is available here: https://github.com/UMBCvision/MSF},
  archive   = {C_ICCV},
  author    = {Soroush Abbasi Koohpayegani and Ajinkya Tejankar and Hamed Pirsiavash},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01016},
  pages     = {10306-10315},
  title     = {Mean shift for self-supervised learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rethinking preventing class-collapsing in metric learning
with margin-based losses. <em>ICCV</em>, 10296–10305. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Metric learning seeks perceptual embeddings where visually similar instances are close and dissimilar instances are apart, but learned representations can be sub-optimal when the distribution of intra-class samples is diverse and distinct sub-clusters are present. Although theoretically with optimal assumptions, margin-based losses such as the triplet loss and margin loss have a diverse family of solutions. We theoretically prove and empirically show that under reasonable noise assumptions, margin-based losses tend to project all samples of a class with various modes onto a single point in the embedding space, resulting in class collapse that usually renders the space ill-sorted for classification or retrieval. To address this problem, we propose a simple modification to the embedding losses such that each sample selects its nearest same-class counterpart in a batch as the positive element in the tuple. This allows for the presence of multiple sub-clusters within each class. The adaptation can be integrated into a wide range of metric learning losses. Our method demonstrates clear benefits on various fine-grained image retrieval datasets over a variety of existing losses; qualitative retrieval results show that samples with similar visual patterns are indeed closer in the embedding space.},
  archive   = {C_ICCV},
  author    = {Elad Levi and Tete Xiao and Xiaolong Wang and Trevor Darrell},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01015},
  pages     = {10296-10305},
  title     = {Rethinking preventing class-collapsing in metric learning with margin-based losses},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving contrastive learning by visualizing feature
transformation. <em>ICCV</em>, 10286–10295. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Contrastive learning, which aims at minimizing the distance between positive pairs while maximizing that of negative ones, has been widely and successfully applied in unsupervised feature learning, where the design of positive and negative (pos/neg) pairs is one of its keys. In this paper, we attempt to devise a feature-level data manipulation, differing from data augmentation, to enhance the generic contrastive self-supervised learning. To this end, we first design a visualization scheme for pos/neg score 1 distribution, which enables us to analyze, interpret and understand the learning process. To our knowledge, this is the first attempt of its kind. More importantly, leveraging this tool, we gain some significant observations, which inspire our novel Feature Transformation proposals including the extrapolation of positives. This operation creates harder positives to boost the learning because hard positives enable the model to be more view-invariant. Besides, we propose the interpolation among negatives, which provides diversified negatives and makes the model more discriminative. It is the first attempt to deal with both challenges simultaneously. Experiment results show that our proposed Feature Transformation can improve at least 6.0\% accuracy on ImageNet-100 over MoCo baseline, and about 2.0\% accuracy on ImageNet-1K over the MoCoV2 baseline. Transferring to the downstream tasks successfully demonstrate our model is less task-bias. Visualization tools and codes: https://github.com/DTennant/CL-Visualizing-Feature-Transformation.},
  archive   = {C_ICCV},
  author    = {Rui Zhu and Bingchen Zhao and Jingen Liu and Zhenglong Sun and Chang Wen Chen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01014},
  pages     = {10286-10295},
  title     = {Improving contrastive learning by visualizing feature transformation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Video annotation for visual tracking via selection and
refinement. <em>ICCV</em>, 10276–10285. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep learning based visual trackers entail offline pre-training on large volumes of video datasets with accurate bounding box annotations that are labor-expensive to achieve. We present a new framework to facilitate bounding box annotations for video sequences, which investigates a selection-and-refinement strategy to automatically improve the preliminary annotations generated by tracking algorithms. A temporal assessment network (T-Assess Net) is proposed which is able to capture the temporal coherence of target locations and select reliable tracking results by measuring their quality. Meanwhile, a visual-geometry refinement network (VG-Refine Net) is also designed to further enhance the selected tracking results by considering both target appearance and temporal geometry constraints, allowing inaccurate tracking results to be corrected. The combination of the above two networks provides a principled approach to ensure the quality of automatic video annotation. Experiments on large scale tracking benchmarks demonstrate that our method can deliver highly accurate bounding box annotations and significantly reduce human labor by 94.0\%, yielding an effective means to further boost tracking performance with augmented training data.},
  archive   = {C_ICCV},
  author    = {Kenan Dai and Jie Zhao and Lijun Wang and Dong Wang and Jianhua Li and Huchuan Lu and Xuesheng Qian and Xiaoyun Yang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01013},
  pages     = {10276-10285},
  title     = {Video annotation for visual tracking via selection and refinement},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Benchmark platform for ultra-fine-grained visual
categorization beyond human performance. <em>ICCV</em>, 10265–10275. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep learning methods have achieved remarkable success in fine-grained visual categorization. Such successful categorization at sub-ordinate level, e.g., different animal or plant species, however relies heavily on the visual differences that human can observe and the ground-truths are labelled on the basis of such human visual observation. In contrast, few research has been done for visual categorization at the ultra-fine-grained level, i.e., a granularity where even human experts can hardly identify the visual differences or are not yet able to give affirmative labels by inferring observed pattern differences. This paper reports our efforts towards mitigating this research gap. We introduce the ultra-fine-grained (UFG) image dataset, a large collection of 47,114 images from 3,526 categories. All the images in the proposed UFG image dataset are grouped into categories with different confirmed cultivar names. In addition, we perform an extensive evaluation of state-of-the-art fine-grained classification methods on the proposed UFG image dataset as comparative baselines. The proposed UFG image dataset and evaluation protocols is intended to serve as a benchmark platform that can advance research of visual classification from approaching human performance to beyond human ability, via facilitating benchmark data of artificial intelligence (AI) not to be limited by the labels of human intelligence (HI). The dataset is available online at https://githuh.com/XiaohanYu-GU/Ultra-FGVC.},
  archive   = {C_ICCV},
  author    = {Xiaohan Yu and Yang Zhao and Yongsheng Gao and Xiaohui Yuan and Shengwu Xiong},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01012},
  pages     = {10265-10275},
  title     = {Benchmark platform for ultra-fine-grained visual categorization beyond human performance},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ACAV100M: Automatic curation of large-scale datasets for
audio-visual video representation learning. <em>ICCV</em>, 10254–10264.
(<a href="https://doi.org/10.1109/ICCV48922.2021.01011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The natural association between visual observations and their corresponding sound provides powerful self-supervisory signals for learning video representations, which makes the ever-growing amount of online videos an attractive source of training data. However, large portions of online videos contain irrelevant audio-visual signals because of edited/overdubbed audio, and models trained on such uncurated videos have shown to learn suboptimal representations. Therefore, existing self-supervised approaches rely on datasets with predetermined taxonomies of semantic concepts, where there is a high chance of audio-visual correspondence. Unfortunately, constructing such datasets require labor intensive manual annotation and/or verification, which severely limits the utility of online videos for large-scale learning. In this work, we present an automatic dataset curation approach based on subset optimization where the objective is to maximize the mutual information between audio and visual channels in videos. We demonstrate that our approach finds videos with high audio-visual correspondence and show that self-supervised models trained on our data achieve competitive performances compared to models trained on existing manually curated datasets. The most significant benefit of our approach is scalability: We release ACAV100M that contains 100 million videos with high audio-visual correspondence, ideal for self-supervised video representation learning.},
  archive   = {C_ICCV},
  author    = {Sangho Lee and Jiwan Chung and Youngjae Yu and Gunhee Kim and Thomas Breuel and Gal Chechik and Yale Song},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01011},
  pages     = {10254-10264},
  title     = {ACAV100M: Automatic curation of large-scale datasets for audio-visual video representation learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Active learning for deep object detection via probabilistic
modeling. <em>ICCV</em>, 10244–10253. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Active learning aims to reduce labeling costs by selecting only the most informative samples on a dataset. Few existing works have addressed active learning for object detection. Most of these methods are based on multiple models or are straightforward extensions of classification methods, hence estimate an image’s informativeness using only the classification head. In this paper, we propose a novel deep active learning approach for object detection. Our approach relies on mixture density networks that estimate a probabilistic distribution for each localization and classification head’s output. We explicitly estimate the aleatoric and epistemic uncertainty in a single forward pass of a single model. Our method uses a scoring function that aggregates these two types of uncertainties for both heads to obtain every image’s informativeness score. We demonstrate the efficacy of our approach in PASCAL VOC and MS-COCO datasets. Our approach outperforms single-model based methods and performs on par with multi-model based methods at a fraction of the computing cost. Code is available at https://github.com/NVlabs/AL-MDN.},
  archive   = {C_ICCV},
  author    = {Jiwoong Choi and Ismail Elezi and Hyuk-Jae Lee and Clement Farabet and Jose M. Alvarez},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01010},
  pages     = {10244-10253},
  title     = {Active learning for deep object detection via probabilistic modeling},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-supervised pretraining of 3D features on any
point-cloud. <em>ICCV</em>, 10232–10243. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pretraining on large labeled datasets is a prerequisite to achieve good performance in many computer vision tasks like image recognition, video understanding etc. However, pretraining is not widely used for 3D recognition tasks where state-of-the-art methods train models from scratch. A primary reason is the lack of large annotated datasets because 3D data labelling is time-consuming. Recent work shows that self-supervised learning is useful to pretrain models in 3D but requires multi-view data and point correspondences. We present a simple self-supervised pretraining method that can work with single-view depth scans acquired by varied sensors, without 3D registration and point correspondences. We pretrain standard point cloud and voxel based model architectures, and show that joint pretraining further improves performance. We evaluate our models on 9 benchmarks for object detection, semantic segmentation, and object classification, where they achieve state-of-the-art results. Most notably, we set a new state-of-the-art for object detection on ScanNet (69.0\% mAP) and SUNRGBD (63.5\% mAP). Our pretrained models are label efficient and improve performance for classes with few examples.},
  archive   = {C_ICCV},
  author    = {Zaiwei Zhang and Rohit Girdhar and Armand Joulin and Ishan Misra},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01009},
  pages     = {10232-10243},
  title     = {Self-supervised pretraining of 3D features on any point-cloud},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning conditional knowledge distillation for
degraded-reference image quality assessment. <em>ICCV</em>, 10222–10231.
(<a href="https://doi.org/10.1109/ICCV48922.2021.01008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An important scenario for image quality assessment (IQA) is to evaluate image restoration (IR) algorithms. The state-of-the-art approaches adopt a full-reference paradigm that compares restored images with their corresponding pristine-quality images. However, pristine-quality images are usually unavailable in blind image restoration tasks and real-world scenarios. In this paper, we propose a practical solution named degraded-reference IQA (DR-IQA), which exploits the inputs of IR models, degraded images, as references. Specifically, we extract reference information from degraded images by distilling knowledge from pristine-quality images. The distillation is achieved through learning a reference space, where various degraded images are encouraged to share the same feature statistics with pristine-quality images. And the reference space is optimized to capture deep image priors that are useful for quality assessment. Note that pristine-quality images are only used during training. Our work provides a powerful and differentiable metric for blind IRs, especially for GAN-based methods. Extensive experiments show that our results can even be close to the performance of full-reference settings.},
  archive   = {C_ICCV},
  author    = {Heliang Zheng and Huan Yang and Jianlong Fu and Zheng-Jun Zha and Jiebo Luo},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01008},
  pages     = {10222-10231},
  title     = {Learning conditional knowledge distillation for degraded-reference image quality assessment},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Understanding robustness of transformers for image
classification. <em>ICCV</em>, 10211–10221. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep Convolutional Neural Networks (CNNs) have long been the architecture of choice for computer vision tasks. Recently, Transformer-based architectures like Vision Transformer (ViT) have matched or even surpassed ResNets for image classification. However, details of the Transformer architecture –such as the use of non-overlapping patches– lead one to wonder whether these networks are as robust. In this paper, we perform an extensive study of a variety of different measures of robustness of ViT models and compare the findings to ResNet baselines. We investigate robustness to input perturbations as well as robustness to model perturbations. We find that when pre-trained with a sufficient amount of data, ViT models are at least as robust as the ResNet counterparts on a broad range of perturbations. We also find that Transformers are robust to the removal of almost any single layer, and that while activations from later layers are highly correlated with each other, they nevertheless play an important role in classification.},
  archive   = {C_ICCV},
  author    = {Srinadh Bhojanapalli and Ayan Chakrabarti and Daniel Glasner and Daliang Li and Thomas Unterthiner and Andreas Veit},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01007},
  pages     = {10211-10221},
  title     = {Understanding robustness of transformers for image classification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Temporal-wise attention spiking neural networks for event
streams classification. <em>ICCV</em>, 10201–10210. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {How to effectively and efficiently deal with spatio-temporal event streams, where the events are generally sparse and non-uniform and have the μs temporal resolution, is of great value and has various real-life applications. Spiking neural network (SNN), as one of the brain-inspired event-triggered computing models, has the potential to extract effective spatio-temporal features from the event streams. However, when aggregating individual events into frames with a new higher temporal resolution, existing SNN models do not attach importance to that the serial frames have different signal-to-noise ratios since event streams are sparse and non-uniform. This situation interferes with the performance of existing SNNs. In this work, we propose a temporal-wise attention SNN (TA-SNN) model to learn frame-based representation for processing event streams. Concretely, we extend the attention concept to temporal-wise input to judge the significance of frames for the final decision at the training stage, and discard the irrelevant frames at the inference stage. We demonstrate that TA-SNN models improve the accuracy of event streams classification tasks. We also study the impact of multiple-scale temporal resolutions for frame-based representation. Our approach is tested on three different classification tasks: gesture recognition, image classification, and spoken digit recognition. We report the state-of-the-art results on these tasks, and get the essential improvement of accuracy (almost 19\%) for gesture recognition with only 60 ms.},
  archive   = {C_ICCV},
  author    = {Man Yao and Huanhuan Gao and Guangshe Zhao and Dingheng Wang and Yihan Lin and Zhaoxu Yang and Guoqi Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01006},
  pages     = {10201-10210},
  title     = {Temporal-wise attention spiking neural networks for event streams classification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving robustness against common corruptions with
frequency biased models. <em>ICCV</em>, 10191–10200. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {CNNs perform remarkably well when the training and test distributions are i.i.d, but unseen image corruptions can cause a surprisingly large drop in performance. In various real scenarios, unexpected distortions, such as random noise, compression artefacts or weather distortions are common phenomena. Improving performance on corrupted images must not result in degraded i.i.d performance – a challenge faced by many state-of-the-art robust approaches. Image corruption types have different characteristics in the frequency spectrum and would benefit from a targeted type of data augmentation, which, however, is often unknown during training. In this paper, we introduce a mixture of two expert models specializing in high and low-frequency robustness, respectively. Moreover, we propose a new regularization scheme that minimizes the total variation (TV) of convolution feature-maps to increase high-frequency robustness. The approach improves on corrupted images without degrading in-distribution performance. We demonstrate this on ImageNet-C and also for real-world corruptions on an automotive dataset, both for object classification and object detection.},
  archive   = {C_ICCV},
  author    = {Tonmoy Saikia and Cordelia Schmid and Thomas Brox},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01005},
  pages     = {10191-10200},
  title     = {Improving robustness against common corruptions with frequency biased models},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Improve unsupervised pretraining for few-label transfer.
<em>ICCV</em>, 10181–10190. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unsupervised pretraining has achieved great success and many recent works have shown unsupervised pretraining can achieve comparable or even slightly better transfer performance than supervised pretraining on downstream target datasets. But in this paper, we find this conclusion may not hold when the target dataset has very few labeled samples for finetuning, i.e., few-label transfer. We analyze the possible reason from the clustering perspective: 1) The clustering quality of target samples is of great importance to few-label transfer; 2) Though contrastive learning is essential to learn how to cluster, its clustering quality is still inferior to supervised pretraining due to lack of label supervision. Based on the analysis, we interestingly discover that only involving some unlabeled target domain into the unsupervised pretraining can improve the clustering quality, subsequently reducing the transfer performance gap with supervised pretraining. This finding also motivates us to propose a new progressive few-label transfer algorithm for real applications, which aims to maximize the transfer performance under a limited annotation budget. To support our analysis and proposed method, we conduct extensive experiments on nine different target datasets. Experimental results show our proposed method can significantly boost the few-label transfer performance of unsupervised pretraining.},
  archive   = {C_ICCV},
  author    = {Suichan Li and Dongdong Chen and Yinpeng Chen and Lu Yuan and Lei Zhang and Qi Chu and Bin Liu and Nenghai Yu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01004},
  pages     = {10181-10190},
  title     = {Improve unsupervised pretraining for few-label transfer},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-supervised representation learning from flow
equivariance. <em>ICCV</em>, 10171–10180. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-supervised representation learning is able to learn semantically meaningful features; however, much of its recent success relies on multiple crops of an image with very few objects. Instead of learning view-invariant representation from simple images, humans learn representations in a complex world with changing scenes by observing object movement, deformation, pose variation and ego motion. Motivated by this ability, we present a new self-supervised learning representation framework that can be directly deployed on a video stream of complex scenes with many moving objects. Our framework features a simple flow equivariance objective that encourages the network to predict the features of another frame by applying a flow transformation to the features of the current frame. Our representations, learned from high-resolution raw video, can be readily used for downstream tasks on static images. Readout experiments on challenging semantic segmentation, instance segmentation, and object detection benchmarks show that we are able to outperform representations obtained from previous state-of-the-art methods including SimCLR [6] and BYOL [18].},
  archive   = {C_ICCV},
  author    = {Yuwen Xiong and Mengye Ren and Wenyuan Zeng and Raquel Urtasun Waabi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01003},
  pages     = {10171-10180},
  title     = {Self-supervised representation learning from flow equivariance},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Geography-aware self-supervised learning. <em>ICCV</em>,
10161–10170. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Contrastive learning methods have significantly narrowed the gap between supervised and unsupervised learning on computer vision tasks. In this paper, we explore their application to geo-located datasets, e.g. remote sensing, where unlabeled data is often abundant but labeled data is scarce. We first show that due to their different characteristics, a non-trivial gap persists between contrastive and supervised learning on standard benchmarks. To close the gap, we propose novel training methods that exploit the spatio-temporal structure of remote sensing data. We leverage spatially aligned images over time to construct temporal positive pairs in contrastive learning and geo-location to design pre-text tasks. Our experiments show that our proposed method closes the gap between contrastive and supervised learning on image classification, object detection and semantic segmentation for remote sensing. Moreover, we demonstrate that the proposed method can also be applied to geo-tagged ImageNet images, improving downstream performance on various tasks.},
  archive   = {C_ICCV},
  author    = {Kumar Ayush and Burak Uzkent and Chenlin Meng and Kumar Tanmay and Marshall Burke and David Lobell and Stefano Ermon},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01002},
  pages     = {10161-10170},
  title     = {Geography-aware self-supervised learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Temporal knowledge consistency for unsupervised visual
representation learning. <em>ICCV</em>, 10150–10160. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The instance discrimination paradigm has become dominant in unsupervised learning. It always adopts a teacher-student framework, in which the teacher provides embedded knowledge as a supervision signal for the student. The student learns meaningful representations by enforcing instance spatial consistency with the views from the teacher. However, the outputs of the teacher can vary dramatically on the same instance during different training stages, introducing unexpected noise and leading to catastrophic forgetting caused by inconsistent objectives. In this paper, we first integrate instance temporal consistency into current instance discrimination paradigms, and propose a novel and strong algorithm named Temporal Knowledge Consistency (TKC). Specifically, our TKC dynamically ensembles the knowledge of temporal teachers and adaptively selects useful information according to its importance to learning instance temporal consistency. Experimental result shows that TKC can learn better visual representations on both ResNet and AlexNet on linear evaluation protocol while transfer well to downstream tasks. All experiments suggest the good effectiveness and generalization of our method. Code will be made available.},
  archive   = {C_ICCV},
  author    = {Weixin Feng and Yuanjiang Wang and Lihua Ma and Ye Yuan and Chi Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01001},
  pages     = {10150-10160},
  title     = {Temporal knowledge consistency for unsupervised visual representation learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-supervised visual representations learning by
contrastive mask prediction. <em>ICCV</em>, 10140–10149. (<a
href="https://doi.org/10.1109/ICCV48922.2021.01000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Advanced self-supervised visual representation learning methods rely on the instance discrimination (ID) pretext task. We point out that the ID task has an implicit semantic consistency (SC) assumption, which may not hold in unconstrained datasets. In this paper, we propose a novel contrastive mask prediction (CMP) task for visual representation learning and design a mask contrast (MaskCo) framework to implement the idea. MaskCo contrasts region-level features instead of view-level features, which makes it possible to identify the positive sample without any assumptions. To solve the domain gap between masked and unmasked features, we design a dedicated mask prediction head in MaskCo. This module is shown to be the key to the success of the CMP. We evaluated MaskCo on training datasets beyond ImageNet and compare its performance with MoCo V2 [4]. Results show that MaskCo achieves comparable performance with MoCo V2 using ImageNet training dataset, but demonstrates a stronger performance across a range of downstream tasks when COCO or Conceptual Captions are used for training. MaskCo provides a promising alternative to the ID-based methods for self-supervised learning in the wild.},
  archive   = {C_ICCV},
  author    = {Yucheng Zhao and Guangting Wang and Chong Luo and Wenjun Zeng and Zheng-Jun Zha},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.01000},
  pages     = {10140-10149},
  title     = {Self-supervised visual representations learning by contrastive mask prediction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Contrastive learning of image representations with
cross-video cycle-consistency. <em>ICCV</em>, 10129–10139. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent works have advanced the performance of self-supervised representation learning by a large margin. The core among these methods is intra-image invariance learning. Two different transformations of one image instance are considered as a positive sample pair, where various tasks are designed to learn invariant representations by comparing the pair. Analogically, for video data, representations of frames from the same video are trained to be closer than frames from other videos, i.e. intra-video invariance. However, cross-video relation has barely been explored for visual representation learning. Unlike intra-video invariance, ground-truth labels of cross-video relation is usually unavailable without human labors. In this paper, we propose a novel contrastive learning method which explores the cross-video relation by using cycle-consistency for general image representation learning. This allows to collect positive sample pairs across different video instances, which we hypothesize will lead to higher-level semantics. We validate our method by transferring our image representation to multiple downstream tasks including visual object tracking, image classification, and action recognition. We show significant improvement over state-of-the-art contrastive learning methods. Project page is available at https://happywu.github.io/cycle_contrast_video.},
  archive   = {C_ICCV},
  author    = {Haiping Wu and Xiaolong Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00999},
  pages     = {10129-10139},
  title     = {Contrastive learning of image representations with cross-video cycle-consistency},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). H2O: Two hands manipulating objects for first person
interaction recognition. <em>ICCV</em>, 10118–10128. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a comprehensive framework for egocentric interaction recognition using markerless 3D annotations of two hands manipulating objects. To this end, we propose a method to create a unified dataset for egocentric 3D interaction recognition. Our method produces annotations of the 3D pose of two hands and the 6D pose of the manipulated objects, along with their interaction labels for each frame. Our dataset, called H2O (2 Hands and Objects), provides synchronized multi-view RGB-D images, interaction labels, object classes, ground-truth 3D poses for left &amp; right hands, 6D object poses, ground-truth camera poses, object meshes and scene point clouds. To the best of our knowledge, this is the first benchmark that enables the study of first-person actions with the use of the pose of both left and right hands manipulating objects and presents an unprecedented level of detail for egocentric 3D interaction recognition. We further propose the method to predict interaction classes by estimating the 3D pose of two hands and the 6D pose of the manipulated objects, jointly from RGB images. Our method models both inter- and intra-dependencies between both hands and objects by learning the topology of a graph convolutional network that predicts interactions. We show that our method facilitated by this dataset establishes a strong baseline for joint hand-object pose estimation and achieves state-of-the-art accuracy for first person interaction recognition.},
  archive   = {C_ICCV},
  author    = {Taein Kwon and Bugra Tekin and Jan Stühmer and Federica Bogo and Marc Pollefeys},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00998},
  pages     = {10118-10128},
  title     = {H2O: Two hands manipulating objects for first person interaction recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FloorPlanCAD: A large-scale CAD drawing dataset for panoptic
symbol spotting. <em>ICCV</em>, 10108–10117. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Access to large and diverse computer-aided design (CAD) drawings is critical for developing symbol spotting algorithms. In this paper, we present FloorPlan-CAD, a large-scale real-world CAD drawing dataset containing over 10,000 floor plans, ranging from residential to commercial buildings. CAD drawings in the dataset are all represented as vector graphics, which enable us to provide line-grained annotations of 30 object categories. Equipped by such annotations, we introduce the task of panoptic symbol spotting, which requires to spot not only instances of countable things, but also the semantic of uncountable stuff. Aiming to solve this task, we propose a novel method by combining Graph Convolutional Networks (GCNs) with Convolutional Neural Networks (CNNs), which captures both non-Euclidean and Euclidean features and can be trained end-to-end. The proposed CNN-GCN method achieved state-of-the-art (SOTA) performance on the task of semantic symbol spotting, and help us build a baseline network for the panoptic symbol spotting task. Our contributions are three-fold: 1) to the best of our knowledge, the presented CAD drawing dataset is the first of its kind; 2) the panoptic symbol spotting task considers the spotting of both thing instances and stuff semantic as one recognition problem; and 3) we presented a baseline solution to the panoptic symbol spotting task based on a novel CNN-GCN method, which achieved SOTA performance on semantic symbol spotting. We believe that these contributions will boost research in related areas. The dataset and code is publicly available at https://floorplancad.github.io/.},
  archive   = {C_ICCV},
  author    = {Zhiwen Fan and Lingjie Zhu and Honghua Li and Xiaohao Chen and Siyu Zhu and Ping Tan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00997},
  pages     = {10108-10117},
  title     = {FloorPlanCAD: A large-scale CAD drawing dataset for panoptic symbol spotting},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). OpenForensics: Large-scale challenging dataset for
multi-face forgery detection and segmentation in-the-wild.
<em>ICCV</em>, 10097–10107. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The proliferation of deepfake media is raising concerns among the public and relevant authorities. It has become essential to develop countermeasures against forged faces in social media. This paper presents a comprehensive study on two new countermeasure tasks: multi-face forgery detection and segmentation in-the-wild. Localizing forged faces among multiple human faces in unrestricted natural scenes is far more challenging than the traditional deepfake recognition task. To promote these new tasks, we have created the first large-scale dataset posing a high level of challenges that is designed with face-wise rich annotations explicitly for face forgery detection and segmentation, namely Open-Forensics. With its rich annotations, our OpenForensics dataset has great potentials for research in both deepfake prevention and general human face detection. We have also developed a suite of benchmarks for these tasks by conducting an extensive evaluation of state-of-the-art instance detection and segmentation methods on our newly constructed dataset in various scenarios.},
  archive   = {C_ICCV},
  author    = {Trung-Nghia Le and Huy H. Nguyen and Junichi Yamagishi and Isao Echizen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00996},
  pages     = {10097-10107},
  title     = {OpenForensics: Large-scale challenging dataset for multi-face forgery detection and segmentation in-the-wild},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LaLaLoc: Latent layout localisation in dynamic, unvisited
environments. <em>ICCV</em>, 10087–10096. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present LaLaLoc to localise in environments without the need for prior visitation, and in a manner that is robust to large changes in scene appearance, such as a full rearrangement of furniture. Specifically, LaLaLoc performs localisation through latent representations of room layout. LaLaLoc learns a rich embedding space shared between RGB panoramas and layouts inferred from a known floor plan that encodes the structural similarity between locations. Further, LaLaLoc introduces direct, cross-modal pose optimisation in its latent space. Thus, LaLaLoc enables fine-grained pose estimation in a scene without the need for prior visitation, as well as being robust to dynamics, such as a change in furniture configuration. We show that in a domestic environment LaLaLoc is able to accurately localise a single RGB panorama image to within 8.3cm, given only a floor plan as a prior.},
  archive   = {C_ICCV},
  author    = {Henry Howard-Jenkins and Jose-Raul Ruiz-Sarmiento and Victor Adrian Prisacariu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00995},
  pages     = {10087-10096},
  title     = {LaLaLoc: Latent layout localisation in dynamic, unvisited environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SketchAA: Abstract representation for abstract sketches.
<em>ICCV</em>, 10077–10086. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {What makes free-hand sketches appealing for humans lies with its capability as a universal tool to depict the visual world. Such flexibility at human ease, however, introduces abstract renderings that pose unique challenges to computer vision models. In this paper, we propose a purpose-made sketch representation for human sketches. The key intuition is that such representation should be abstract at design, so to accommodate the abstract nature of sketches. This is achieved by interpreting sketch abstraction on two levels: appearance and structure. We abstract sketch structure as a pre-defined coarse-to-fine visual block hierarchy, and average visual features within each block to model appearance abstraction. We then discuss three general strategies on how to exploit feature synergy across different levels of this abstraction hierarchy. The superiority of explicitly abstracting sketch representation is empirically validated on a number of sketch analysis tasks, including sketch recognition, fine-grained sketch-based image retrieval, and generative sketch healing. Our simple design not only yields strong results on all said tasks, but also offers intuitive feature granularity control to tailor for various downstream tasks. Code will be made publicly available.},
  archive   = {C_ICCV},
  author    = {Lan Yang and Kaiyue Pang and Honggang Zhang and Yi-Zhe Song},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00994},
  pages     = {10077-10086},
  title     = {SketchAA: Abstract representation for abstract sketches},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient visual pretraining with contrastive detection.
<em>ICCV</em>, 10066–10076. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-supervised pretraining has been shown to yield powerful representations for transfer learning. These performance gains come at a large computational cost however, with state-of-the-art methods requiring an order of magnitude more computation than supervised pretraining. We tackle this computational bottleneck by introducing a new self-supervised objective, contrastive detection, which tasks representations with identifying object-level features across augmentations. This objective extracts a rich learning signal per image, leading to state-of-the-art transfer accuracy on a variety of downstream tasks, while requiring up to 10× less pretraining. In particular, our strongest ImageNet-pretrained model performs on par with SEER, one of the largest self-supervised systems to date, which uses 1000× more pretraining data. Finally, our objective seamlessly handles pretraining on more complex images such as those in COCO, closing the gap with supervised transfer learning from COCO to PASCAL.},
  archive   = {C_ICCV},
  author    = {Olivier J. Hénaff and Skanda Koppula and Jean-Baptiste Alayrac and Aaron van den Oord and Oriol Vinyals and João Carreira},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00993},
  pages     = {10066-10076},
  title     = {Efficient visual pretraining with contrastive detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rethinking self-supervised correspondence learning: A video
frame-level similarity perspective. <em>ICCV</em>, 10055–10065. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning a good representation for space-time correspondence is the key for various computer vision tasks, including tracking object bounding boxes and performing video object pixel segmentation. To learn generalizable representation for correspondence in large-scale, a variety of self-supervised pretext tasks are proposed to explicitly perform object-level or patch-level similarity learning. Instead of following the previous literature, we propose to learn correspondence using Video Frame-level Similarity (VFS) learning, i.e, simply learning from comparing video frames. Our work is inspired by the recent success in image-level contrastive learning and similarity learning for visual recognition. Our hypothesis is that if the representation is good for recognition, it requires the convolutional features to find correspondence between similar objects or parts. Our experiments show surprising results that VFS surpasses state-of-the-art self-supervised approaches for both OTB visual object tracking and DAVIS video object segmentation. We perform detailed analysis on what matters in VFS and reveals new properties on image and frame level similarity learning. Project page with code: https://jerryxu.net/VFS.},
  archive   = {C_ICCV},
  author    = {Jiarui Xu and Xiaolong Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00992},
  pages     = {10055-10065},
  title     = {Rethinking self-supervised correspondence learning: A video frame-level similarity perspective},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Divide and contrast: Self-supervised learning from uncurated
data. <em>ICCV</em>, 10043–10054. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-supervised learning holds promise in leveraging large amounts of unlabeled data, however much of its progress has thus far been limited to highly curated pre-training data such as ImageNet. We explore the effects of contrastive learning from larger, less-curated image datasets such as YFCC, and find there is indeed a large difference in the resulting representation quality. We hypothesize that this curation gap is due to a shift in the distribution of image classes—which is more diverse and heavy-tailed—resulting in less relevant negative samples to learn from. We test this hypothesis with a new approach, Divide and Contrast (DnC), which alternates between contrastive learning and clustering-based hard negative mining. When pretrained on less curated datasets, DnC greatly improves the performance of self-supervised learning on downstream tasks, while remaining competitive with the current state-of-the-art on curated datasets.},
  archive   = {C_ICCV},
  author    = {Yonglong Tian and Olivier J. Hénaff and Aäron van den Oord},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00991},
  pages     = {10043-10054},
  title     = {Divide and contrast: Self-supervised learning from uncurated data},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised semantic segmentation by contrasting object
mask proposals. <em>ICCV</em>, 10032–10042. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Being able to learn dense semantic representations of images without supervision is an important problem in computer vision. However, despite its significance, this problem remains rather unexplored, with a few exceptions that considered unsupervised semantic segmentation on small-scale datasets with a narrow visual domain. In this paper, we make a first attempt to tackle the problem on datasets that have been traditionally utilized for the supervised case. To achieve this, we introduce a two-step framework that adopts a predetermined mid-level prior in a contrastive optimization objective to learn pixel embeddings. This marks a large deviation from existing works that relied on proxy tasks or end-to-end clustering. Additionally, we argue about the importance of having a prior that contains information about objects, or their parts, and discuss several possibilities to obtain such a prior in an unsupervised manner.Experimental evaluation shows that our method comes with key advantages over existing works. First, the learned pixel embeddings can be directly clustered in semantic groups using K-Means on PASCAL. Under the fully unsupervised setting, there is no precedent in solving the semantic segmentation task on such a challenging benchmark. Second, our representations can improve over strong baselines when transferred to new datasets, e.g. COCO and DAVIS. The code is available 1 .},
  archive   = {C_ICCV},
  author    = {Wouter Van Gansbeke and Simon Vandenhende and Stamatios Georgoulis and Luc Van Gool},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00990},
  pages     = {10032-10042},
  title     = {Unsupervised semantic segmentation by contrasting object mask proposals},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weakly supervised contrastive learning. <em>ICCV</em>,
10022–10031. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unsupervised visual representation learning has gained much attention from the computer vision community because of the recent achievement of contrastive learning. Most of the existing contrastive learning frameworks adopt the instance discrimination as the pretext task, which treating every single instance as a different class. However, such method will inevitably cause class collision problems, which hurts the quality of the learned representation. Motivated by this observation, we introduced a weakly supervised contrastive learning framework (WCL) to tackle this issue. Specifically, our proposed framework is based on two projection heads, one of which will perform the regular instance discrimination task. The other head will use a graph-based method to explore similar samples and generate a weak label, then perform a supervised contrastive learning task based on the weak label to pull the similar images closer. We further introduced a K-Nearest Neighbor based multi-crop strategy to expand the number of positive samples. Extensive experimental results demonstrate WCL improves the quality of self-supervised representations across different datasets. Notably, we get a new state-of-the-art result for semi-supervised learning. With only 1\% and 10\% labeled examples, WCL achieves 65\% and 72\% ImageNet Top-1 Accuracy using ResNet50, which is even higher than SimCLRv2 with ResNet101.},
  archive   = {C_ICCV},
  author    = {Mingkai Zheng and Fei Wang and Shan You and Chen Qian and Changshui Zhang and Xiaogang Wang and Chang Xu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00989},
  pages     = {10022-10031},
  title     = {Weakly supervised contrastive learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rethinking and improving relative position encoding for
vision transformer. <em>ICCV</em>, 10013–10021. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Relative position encoding (RPE) is important for transformer to capture sequence ordering of input tokens. General efficacy has been proven in natural language processing. However, in computer vision, its efficacy is not well studied and even remains controversial, e.g., whether relative position encoding can work equally well as absolute position? In order to clarify this, we first review existing relative position encoding methods and analyze their pros and cons when applied in vision transformers. We then propose new relative position encoding methods dedicated to 2D images, called image RPE (iRPE). Our methods consider directional relative distance modeling as well as the interactions between queries and relative position embeddings in self-attention mechanism. The proposed iRPE methods are simple and lightweight. They can be easily plugged into transformer blocks. Experiments demonstrate that solely due to the proposed encoding methods, DeiT [21] and DETR [1] obtain up to 1.5\% (top-1 Acc) and 1.3\% (mAP) stable improvements over their original versions on ImageNet and COCO respectively, without tuning any extra hyperparameters such as learning rate and weight decay. Our ablation and analysis also yield interesting findings, some of which run counter to previous understanding. Code and models are open-sourced at https://github.com/microsoft/Cream/tree/main/iRPE.},
  archive   = {C_ICCV},
  author    = {Kan Wu and Houwen Peng and Minghao Chen and Jianlong Fu and Hongyang Chao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00988},
  pages     = {10013-10021},
  title     = {Rethinking and improving relative position encoding for vision transformer},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). InSeGAN: A generative approach to segmenting identical
instances in depth images. <em>ICCV</em>, 10003–10012. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present InSeGAN, an unsupervised 3D generative adversarial network (GAN) for segmenting (nearly) identical instances of rigid objects in depth images. Using an analysis-by-synthesis approach, we design a novel GAN architecture to synthesize a multiple-instance depth image with independent control over each instance. InSeGAN takes in a set of code vectors (e.g., random noise vectors), each encoding the 3D pose of an object that is represented by a learned implicit object template. The generator has two distinct modules. The first module, the instance feature generator, uses each encoded pose to transform the implicit template into a feature map representation of each object instance. The second module, the depth image renderer, aggregates all of the single-instance feature maps output by the first module and generates a multiple-instance depth image. A discriminator distinguishes the generated multiple-instance depth images from the distribution of true depth images. To use our model for instance segmentation, we propose an instance pose encoder that learns to take in a generated depth image and reproduce the pose code vectors for all of the object instances. To evaluate our approach, we introduce a new synthetic dataset, &quot;Insta-10,&quot; consisting of 100,000 depth images, each with 5 instances of an object from one of 10 classes. Our experiments on Insta-10, as well as on real-world noisy depth images, show that InSeGAN achieves state-of-the-art performance, often outperforming prior methods by large margins.},
  archive   = {C_ICCV},
  author    = {Anoop Cherian and Gonçalo Dias Pais and Siddarth Jain and Tim K. Marks and Alan Sullivan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00987},
  pages     = {10003-10012},
  title     = {InSeGAN: A generative approach to segmenting identical instances in depth images},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Swin transformer: Hierarchical vision transformer using
shifted windows. <em>ICCV</em>, 9992–10002. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at https://github.com/microsoft/Swin-Transformer.},
  archive   = {C_ICCV},
  author    = {Ze Liu and Yutong Lin and Yue Cao and Han Hu and Yixuan Wei and Zheng Zhang and Stephen Lin and Baining Guo},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00986},
  pages     = {9992-10002},
  title     = {Swin transformer: Hierarchical vision transformer using shifted windows},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Field convolutions for surface CNNs. <em>ICCV</em>,
9981–9991. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel surface convolution operator acting on vector fields that is based on a simple observation: instead of combining neighboring features with respect to a single coordinate parameterization defined at a given point, we have every neighbor describe the position of the point within its own coordinate frame. This formulation combines intrinsic spatial convolution with parallel transport in a scattering operation while placing no constraints on the filters themselves, providing a definition of convolution that commutes with the action of isometries, has increased descriptive potential, and is robust to noise and other nuisance factors. The result is a rich notion of convolution which we call field convolution, well-suited for CNNs on surfaces. Field convolutions are flexible, straight-forward to incorporate into surface learning frameworks, and their highly discriminating nature has cascading effects throughout the learning pipeline. Using simple networks constructed from residual field convolution blocks, we achieve state-of-the-art results on standard benchmarks in fundamental geometry processing tasks, such as shape classification, segmentation, correspondence, and sparse matching.},
  archive   = {C_ICCV},
  author    = {Thomas W. Mitchel and Vladimir G. Kim and Michael Kazhdan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00985},
  pages     = {9981-9991},
  title     = {Field convolutions for surface CNNs},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). T-SVDNet: Exploring high-order prototypical correlations for
multi-source domain adaptation. <em>ICCV</em>, 9971–9980. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most existing domain adaptation methods focus on adaptation from only one source domain, however, in practice there are a number of relevant sources that could be leveraged to help improve performance on target domain. We propose a novel approach named T-SVDNet to address the task of Multi-source Domain Adaptation (MDA), which is featured by incorporating Tensor Singular Value Decomposition (T-SVD) into a neural network’s training pipeline. Overall, high-order correlations among multiple domains and categories are fully explored so as to better bridge the domain gap. Specifically, we impose Tensor-Low-Rank (TLR) constraint on a tensor obtained by stacking up a group of prototypical similarity matrices, aiming at capturing consistent data structure across different domains. Furthermore, to avoid negative transfer brought by noisy source data, we propose a novel uncertainty-aware weighting strategy to adaptively assign weights to different source domains and samples based on the result of uncertainty estimation. Extensive experiments conducted on public benchmarks demonstrate the superiority of our model in addressing the task of MDA compared to state-of-the-art methods. Code is available at https://github.com/lslrh/T-SVDNet.},
  archive   = {C_ICCV},
  author    = {Ruihuang Li and Xu Jia and Jianzhong He and Shuaijun Chen and Qinghua Hu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00984},
  pages     = {9971-9980},
  title     = {T-SVDNet: Exploring high-order prototypical correlations for multi-source domain adaptation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Co-scale conv-attentional image transformers. <em>ICCV</em>,
9961–9970. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present Co-scale conv-attentional image Transformers (CoaT), a Transformer-based image classifier equipped with co-scale and conv-attentional mechanisms. First, the co-scale mechanism maintains the integrity of Transformers’ encoder branches at individual scales, while allowing representations learned at different scales to effectively communicate with each other; we design a series of serial and parallel blocks to realize the co-scale mechanism. Second, we devise a conv-attentional mechanism by realizing a relative position embedding formulation in the factorized attention module with an efficient convolution-like implementation. CoaT empowers image Transformers with enriched multi-scale and contextual modeling capabilities. On ImageNet, relatively small CoaT models attain superior classification results compared with similar-sized convolutional neural networks and image/vision Transformers. The effectiveness of CoaT’s backbone is also illustrated on object detection and instance segmentation, demonstrating its applicability to downstream computer vision tasks.},
  archive   = {C_ICCV},
  author    = {Weijian Xu and Yifan Xu and Tyler Chang and Zhuowen Tu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00983},
  pages     = {9961-9970},
  title     = {Co-scale conv-attentional image transformers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Time-equivariant contrastive video representation learning.
<em>ICCV</em>, 9950–9960. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a novel self-supervised contrastive learning method to learn representations from unlabelled videos. Existing approaches ignore the specifics of input distortions, e.g., by learning invariance to temporal transformations. Instead, we argue that video representation should preserve video dynamics and reflect temporal manipulations of the input. Therefore, we exploit novel constraints to build representations that are equivariant to temporal transformations and better capture video dynamics. In our method, relative temporal transformations between augmented clips of a video are encoded in a vector and contrasted with other transformation vectors. To support temporal equivariance learning, we additionally propose the self-supervised classification of two clips of a video into 1. overlapping 2. ordered, or 3. unordered. Our experiments show that time-equivariant representations achieve state-of-the-art results in video retrieval and action recognition benchmarks on UCF101, HMDB51, and Diving48.},
  archive   = {C_ICCV},
  author    = {Simon Jenni and Hailin Jin},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00982},
  pages     = {9950-9960},
  title     = {Time-equivariant contrastive video representation learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modelling neighbor relation in joint space-time graph for
video correspondence learning. <em>ICCV</em>, 9940–9949. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a self-supervised method for learning reliable visual correspondence from unlabeled videos. We formulate the correspondence as finding paths in a joint space-time graph, where nodes are grid patches sampled from frames, and are linked by two type of edges: (i) neighbor relations that determine the aggregation strength from intra-frame neighbors in space, and (ii) similarity relations that indicate the transition probability of inter-frame paths across time. Leveraging the cycle-consistency in videos, our contrastive learning objective discriminates dynamic objects from both their neighboring views and temporal views. Compared with prior works, our approach actively explores the neighbor relations of central instances to learn a latent association between center-neighbor pairs (e.g., &quot;hand – arm&quot;) across time, thus improving the instance discrimination. Without fine-tuning, our learned representation outperforms the state-of-the-art self-supervised methods on a variety of visual tasks including video object propagation, part propagation, and pose keypoint tracking. Our self-supervised method also surpasses some fully supervised algorithms designed for the specific tasks.},
  archive   = {C_ICCV},
  author    = {Zixu Zhao and Yueming Jin and Pheng-Ann Heng},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00981},
  pages     = {9940-9949},
  title     = {Modelling neighbor relation in joint space-time graph for video correspondence learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Contrasting contrastive self-supervised representation
learning pipelines. <em>ICCV</em>, 9929–9939. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the past few years, we have witnessed remarkable breakthroughs in self-supervised representation learning. Despite the success and adoption of representations learned through this paradigm, much is yet to be understood about how different training methods and datasets influence performance on downstream tasks. In this paper, we analyze contrastive approaches as one of the most successful and popular variants of self-supervised representation learning. We perform this analysis from the perspective of the training algorithms, pre-training datasets and end tasks. We examine over 700 training experiments including 30 encoders, 4 pre-training datasets and 20 diverse downstream tasks. Our experiments address various questions regarding the performance of self-supervised models compared to their supervised counterparts, current benchmarks used for evaluation, and the effect of the pre-training data on end task performance. Our Visual Representation Benchmark (ViRB) is available at: https://github.com/allenai/virb.},
  archive   = {C_ICCV},
  author    = {Klemen Kotar and Gabriel Ilharco and Ludwig Schmidt and Kiana Ehsani and Roozbeh Mottaghi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00980},
  pages     = {9929-9939},
  title     = {Contrasting contrastive self-supervised representation learning pipelines},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning compatible embeddings. <em>ICCV</em>, 9919–9928.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Achieving backward compatibility when rolling out new models can highly reduce costs or even bypass feature re-encoding of existing gallery images for in-production visual retrieval systems. Previous related works usually leverage losses used in knowledge distillation which can cause performance degradations or not guarantee compatibility. To address these issues, we propose a general framework called Learning Compatible Embeddings (LCE) which is applicable for both cross model compatibility and compatible training in direct/forward/backward manners. Our compatibility is achieved by aligning class centers between models directly or via a transformation, and restricting more compact intra-class distributions for the new model. Experiments are conducted in extensive scenarios such as changes of training dataset, loss functions, network architectures as well as feature dimensions, and demonstrate that LCE efficiently enables model compatibility with marginal sacrifices of accuracies. The code will be available at https://github.com/IrvingMeng/LCE.},
  archive   = {C_ICCV},
  author    = {Qiang Meng and Chixiang Zhang and Xiaoqiang Xu and Feng Zhou},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00979},
  pages     = {9919-9928},
  title     = {Learning compatible embeddings},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Clustering by maximizing mutual information across views.
<em>ICCV</em>, 9908–9918. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel framework for image clustering that incorporates joint representation learning and clustering. Our method consists of two heads that share the same backbone network - a &quot;representation learning&quot; head and a &quot;clustering&quot; head. The &quot;representation learning&quot; head captures fine-grained patterns of objects at the instance level which serve as clues for the &quot;clustering&quot; head to extract coarse-grain information that separates objects into clusters. The whole model is trained in an end-to-end manner by minimizing the weighted sum of two sample-oriented contrastive losses applied to the outputs of the two heads. To ensure that the contrastive loss corresponding to the &quot;clustering&quot; head is optimal, we introduce a novel critic function called &quot;log-of-dot-product&quot;. Extensive experimental results demonstrate that our method significantly outperforms state-of-the-art single-stage clustering methods across a variety of image datasets, improving over the best baseline by about 5-7\% in accuracy on CIFAR10/20, STL10, and ImageNet-Dogs. Further, the &quot;two-stage&quot; variant of our method also achieves better results than baselines on three challenging ImageNet subsets.},
  archive   = {C_ICCV},
  author    = {Kien Do and Truyen Tran and Svetha Venkatesh},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00978},
  pages     = {9908-9918},
  title     = {Clustering by maximizing mutual information across views},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Learning better visual data similarities via new grouplet
non-euclidean embedding. <em>ICCV</em>, 9898–9907. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many computer vision problems, it is desired to learn the effective visual data similarity such that the prediction accuracy can be enhanced. Deep Metric Learning (DML) methods have been actively studied to measure the data similarity. Pair-based and proxy-based losses are the two major paradigms in DML. However, pair-wise methods involve expensive training costs, while proxy-based methods are less accurate in characterizing the relationships between data points. In this paper, we provide a hybrid grouplet paradigm, which inherits the accurate pair-wise relationship in pair-based methods and the efficient training in proxy-based methods. Our method also equips a non-Euclidean space to DML, which employs a hierarchical representation manifold. More specifically, we propose a unified graph perspective — different DML methods learn different local connecting patterns between data points. Based on the graph interpretation, we construct a flexible subset of data points, dubbed grouplet. Our grouplet doesn’t require explicit pair-wise relationships, instead, we encode the data relationships in an optimal transport problem regarding the proxies, and solve this problem via a differentiable implicit layer to automatically determine the relationships. Extensive experimental results show that our method significantly outperforms state-of-the-art baselines on several benchmarks. The ablation studies also verify the effectiveness of our method.},
  archive   = {C_ICCV},
  author    = {Yanfu Zhang and Lei Luo and Wenhan Xian and Heng Huang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00977},
  pages     = {9898-9907},
  title     = {Learning better visual data similarities via new grouplet non-euclidean embedding},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep matching prior: Test-time optimization for dense
correspondence. <em>ICCV</em>, 9887–9897. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Conventional techniques to establish dense correspondences across visually or semantically similar images focused on designing a task-specific matching prior, which is difficult to model in general. To overcome this, recent learning-based methods have attempted to learn a good matching prior within a model itself on large training data. The performance improvement was apparent, but the need for sufficient training data and intensive learning hinders their applicability. Moreover, using the fixed model at test time does not account for the fact that a pair of images may require their own prior, thus providing limited performance and poor generalization to unseen images.In this paper, we show that an image pair-specific prior can be captured by solely optimizing the untrained matching networks on an input pair of images. Tailored for such test-time optimization for dense correspondence, we present a residual matching network and a confidence-aware contrastive loss to guarantee a meaningful convergence. Experiments demonstrate that our framework, dubbed Deep Matching Prior (DMP), is competitive, or even outperforms, against the latest learning-based methods on several benchmarks for geometric matching and semantic matching, even though it requires neither large training data nor intensive learning. With the networks pre-trained, DMP attains state-of-the-art performance on all benchmarks.},
  archive   = {C_ICCV},
  author    = {Sunghwan Hong and Seungryong Kim},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00976},
  pages     = {9887-9897},
  title     = {Deep matching prior: Test-time optimization for dense correspondence},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On equivariant and invariant learning of object landmark
representations. <em>ICCV</em>, 9877–9886. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Given a collection of images, humans are able to discover landmarks by modeling the shared geometric structure across instances. This idea of geometric equivariance has been widely used for the unsupervised discovery of object landmark representations. In this paper, we develop a simple and effective approach by combining instance-discriminative and spatially-discriminative contrastive learning. We show that when a deep network is trained to be invariant to geometric and photometric transformations, representations emerge from its intermediate layers that are highly predictive of object landmarks. Stacking these across layers in a &quot;hypercolumn&quot; and projecting them using spatially-contrastive learning further improves their performance on matching and few-shot landmark regression tasks. We also present a unified view of existing equivariant and invariant representation learning approaches through the lens of contrastive learning, shedding light on the nature of invariances learned. Experiments on standard benchmarks for landmark learning, as well as a new challenging one we propose, show that the proposed approach surpasses prior state-of-the-art.},
  archive   = {C_ICCV},
  author    = {Zezhou Cheng and Jong-Chyi Su and Subhransu Maji},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00975},
  pages     = {9877-9886},
  title     = {On equivariant and invariant learning of object landmark representations},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards interpretable deep metric learning with structural
matching. <em>ICCV</em>, 9867–9876. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {How do the neural networks distinguish two images? It is of critical importance to understand the matching mechanism of deep models for developing reliable intelligent systems for many risky visual applications such as surveillance and access control. However, most existing deep metric learning methods match the images by comparing feature vectors, which ignores the spatial structure of images and thus lacks interpretability. In this paper, we present a deep interpretable metric learning (DIML) method for more transparent embedding learning. Unlike conventional metric learning methods based on feature vector comparison, we propose a structural matching strategy that explicitly aligns the spatial embeddings by computing an optimal matching flow between feature maps of the two images. Our method enables deep models to learn metrics in a more human-friendly way, where the similarity of two images can be decomposed to several part-wise similarities and their contributions to the overall similarity. Our method is model-agnostic, which can be applied to off-the-shelf backbone networks and metric learning methods. We evaluate our method on three major benchmarks of deep metric learning including CUB200-2011, Cars196, and Stanford Online Products, and achieve substantial improvements over popular metric learning methods with better interpretability. Code is available at https://github.com/wl-zhao/DIML.},
  archive   = {C_ICCV},
  author    = {Wenliang Zhao and Yongming Rao and Ziyi Wang and Jiwen Lu and Jie Zhou},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00974},
  pages     = {9867-9876},
  title     = {Towards interpretable deep metric learning with structural matching},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Track without appearance: Learn box and tracklet embedding
with local and global motion patterns for vehicle tracking.
<em>ICCV</em>, 9856–9866. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vehicle tracking is an essential task in the multi-object tracking (MOT) field. A distinct characteristic in vehicle tracking is that the trajectories of vehicles are fairly smooth in both the world coordinate and the image coordinate. Hence, models that capture motion consistencies are of high necessity. However, tracking with the standalone motion-based trackers is quite challenging because targets could get lost easily due to limited information, detection error and occlusion. Leveraging appearance information to assist object re-identification could resolve this challenge to some extent. However, doing so requires extra computation while appearance information is sensitive to occlusion as well. In this paper, we try to explore the significance of motion patterns for vehicle tracking without appearance information. We propose a novel approach that tackles the association issue for long-term tracking with the exclusive fully-exploited motion information. We address the tracklet embedding issue with the proposed reconstruct-to-embed strategy based on deep graph convolutional neural networks (GCN). Comprehensive experiments on the KITTI-car tracking dataset and UA-Detrac dataset show that the proposed method, though without appearance information, could achieve competitive performance with the state-of-the-art (SOTA) trackers. The source code will be available at https://github.com/GaoangW/LGMTracker.},
  archive   = {C_ICCV},
  author    = {Gaoang Wang and Renshu Gu and Zuozhu Liu and Weijie Hu and Mingli Song and Jenq-Neng Hwang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00973},
  pages     = {9856-9866},
  title     = {Track without appearance: Learn box and tracklet embedding with local and global motion patterns for vehicle tracking},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Saliency-associated object tracking. <em>ICCV</em>,
9846–9855. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most existing trackers based on deep learning perform tracking in a holistic strategy, which aims to learn deep representations of the whole target for localizing the target. It is arduous for such methods to track targets with various appearance variations. To address this limitation, another type of methods adopts a part-based tracking strategy which divides the target into equal patches and tracks all these patches in parallel. The target state is inferred by summarizing the tracking results of these patches. A potential limitation of such trackers is that not all patches are equally informative for tracking. Some patches that are not discriminative may have adverse effects. In this paper, we propose to track the salient local parts of the target that are discriminative for tracking. In particular, we propose a fine-grained saliency mining module to capture the local saliencies. Further, we design a saliency-association modeling module to associate the captured saliencies together to learn effective correlation representations between the exemplar and the search image for state estimation. Extensive experiments on five diverse datasets demonstrate that the proposed method performs favorably against state-of-the-art trackers.},
  archive   = {C_ICCV},
  author    = {Zikun Zhou and Wenjie Pei and Xin Li and Hongpeng Wang and Feng Zheng and Zhenyu He},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00972},
  pages     = {9846-9855},
  title     = {Saliency-associated object tracking},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High-performance discriminative tracking with transformers.
<em>ICCV</em>, 9836–9845. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {End-to-end discriminative trackers improve the state of the art significantly, yet the improvement in robustness and efficiency is restricted by the conventional discriminative model, i.e., least-squares based regression. In this paper, we present DTT, a novel single-object discriminative tracker, based on an encoder-decoder Transformer architecture. By self- and encoder-decoder attention mechanisms, our approach is able to exploit the rich scene information in an end-to-end manner, effectively removing the need for hand-designed discriminative models. In online tracking, given a new test frame, dense prediction is performed at all spatial positions. Not only location, but also bounding box of the target object is obtained in a robust fashion, streamlining the discriminative tracking pipeline. DTT is conceptually simple and easy to implement. It yields state-of-the-art performance on four popular benchmarks including GOT-10k, LaSOT, NfS, and TrackingNet while running at over 50 FPS, confirming its effectiveness and efficiency. We hope DTT may provide a new perspective for single-object visual tracking.},
  archive   = {C_ICCV},
  author    = {Bin Yu and Ming Tang and Linyu Zheng and Guibo Zhu and Jinqiao Wang and Hao Feng and Xuetao Feng and Hanqing Lu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00971},
  pages     = {9836-9845},
  title     = {High-performance discriminative tracking with transformers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CrowdDriven: A new challenging dataset for outdoor visual
localization. <em>ICCV</em>, 9825–9835. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual localization is the problem of estimating the position and orientation from which a given image (or a sequence of images) is taken in a known scene. It is an important part of a wide range of computer vision and robotics applications, from self-driving cars to augmented/virtual reality systems. Visual localization techniques should work reliably and robustly under a wide range of conditions, including seasonal, weather, illumination and man-made changes. Recent benchmarking efforts model this by providing images under different conditions, and the community has made rapid progress on these datasets since their inception. However, they are limited to a few geographical regions and often recorded with a single device. We propose a new benchmark for visual localization in outdoor scenes, using crowd-sourced data to cover a wide range of geographical regions and camera devices with a focus on the failure cases of current algorithms. Experiments with state-of-the-art localization approaches show that our dataset is very challenging, with all evaluated methods failing on its hardest parts. As part of the dataset release, we provide the tooling used to generate it, enabling efficient and effective 2D correspondence annotation to obtain reference poses.},
  archive   = {C_ICCV},
  author    = {Ara Jafarzadeh and Manuel López Antequera and Pau Gargallo and Yubin Kuang and Carl Toft and Fredrik Kahl and Torsten Sattler},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00970},
  pages     = {9825-9835},
  title     = {CrowdDriven: A new challenging dataset for outdoor visual localization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visio-temporal attention for multi-camera multi-target
association. <em>ICCV</em>, 9814–9824. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the task of Re-Identification (Re-ID) in multi-target multi-camera (MTMC) tracking where we track multiple pedestrians using multiple overlapping uncalibrated (unknown pose) cameras. Since the videos are temporally synchronized and spatially overlapping, we can see a person from multiple views and associate their trajectory across cameras. In order to find the correct association between pedestrians visible from multiple views during the same time window, we extract a visual feature from a tracklet (sequence of pedestrian images) that encodes its similarity and dissimilarity to all other candidate tracklets. We propose a inter-tracklet (person to person) attention mechanism that learns a representation for a target tracklet while taking into account other tracklets across multiple views. Furthermore, to encode the gait and motion of a person, we introduce second intra-tracklet (person-specific) attention module with position embeddings. This second module employs a transformer encoder to learn a feature from a sequence of features over one tracklet. Experimental results on WILDTRACK and our new dataset ‘ConstructSite’ confirm the superiority of our model over state-of-the-art ReID methods (5\% and 10\% performance gain respectively) in the context of uncalibrated MTMC tracking. While our model is designed for overlapping cameras, we also obtain state-of-the-art results on two other benchmark datasets (MARS and DukeMTMC) with non-overlapping cameras.},
  archive   = {C_ICCV},
  author    = {Yu-Jhe Li and Xinshuo Weng and Yan Xu and Kris Kitani},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00969},
  pages     = {9814-9824},
  title     = {Visio-temporal attention for multi-camera multi-target association},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Human trajectory prediction via counterfactual analysis.
<em>ICCV</em>, 9804–9813. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Forecasting human trajectories in complex dynamic environments plays a critical role in autonomous vehicles and intelligent robots. Most existing methods learn to predict future trajectories by behavior clues from history trajectories and interaction clues from environments. However, the inherent bias between training and deployment environments is ignored. Hence, we propose a counterfactual analysis method for human trajectory prediction to investigate the causality between the predicted trajectories and input clues and alleviate the negative effects brought by the environment bias. We first build a causal graph for trajectory forecasting with history trajectory, future trajectory, and the environment interactions. Then, we cut off the inference from environment to trajectory by constructing the counterfactual intervention on the trajectory itself. Finally, we compare the factual and counterfactual trajectory clues to alleviate the effects of environment bias and highlight the trajectory clues. Our counterfactual analysis is a plug-and-play module that can be applied to any baseline prediction methods including RNN- and CNN-based ones. We show that our method achieves consistent improvement for different baselines and obtains the state-of-the-art results on public pedestrian trajectory forecasting benchmarks. 1},
  archive   = {C_ICCV},
  author    = {Guangyi Chen and Junlong Li and Jiwen Lu and Jie Zhou},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00968},
  pages     = {9804-9813},
  title     = {Human trajectory prediction via counterfactual analysis},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AgentFormer: Agent-aware transformers for socio-temporal
multi-agent forecasting. <em>ICCV</em>, 9793–9803. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Predicting accurate future trajectories of multiple agents is essential for autonomous systems but is challenging due to the complex interaction between agents and the uncertainty in each agent’s future behavior. Forecasting multi-agent trajectories requires modeling two key dimensions: (1) time dimension, where we model the influence of past agent states over future states; (2) social dimension, where we model how the state of each agent affects others. Most prior methods model these two dimensions separately, e.g., first using a temporal model to summarize features over time for each agent independently and then modeling the interaction of the summarized features with a social model. This approach is suboptimal since independent feature encoding over either the time or social dimension can result in a loss of information. Instead, we would prefer a method that allows an agent’s state at one time to directly affect another agent’s state at a future time. To this end, we propose a new Transformer, termed AgentFormer, that simultaneously models the time and social dimensions. The model leverages a sequence representation of multi-agent trajectories by flattening trajectory features across time and agents. Since standard attention operations disregard the agent identity of each element in the sequence, AgentFormer uses a novel agent-aware attention mechanism that preserves agent identities by attending to elements of the same agent differently than elements of other agents. Based on AgentFormer, we propose a stochastic multi-agent trajectory prediction model that can attend to features of any agent at any previous timestep when inferring an agent’s future position. The latent intent of all agents is also jointly modeled, allowing the stochasticity in one agent’s behavior to affect other agents. Extensive experiments show that our method significantly improves the state of the art on well-established pedestrian and autonomous driving datasets.},
  archive   = {C_ICCV},
  author    = {Ye Yuan and Xinshuo Weng and Yanglan Ou and Kris Kitani},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00967},
  pages     = {9793-9803},
  title     = {AgentFormer: Agent-aware transformers for socio-temporal multi-agent forecasting},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LOKI: Long term and key intentions for trajectory
prediction. <em>ICCV</em>, 9783–9792. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in trajectory prediction have shown that explicit reasoning about agents’ intent is important to accurately forecast their motion. However, the current research activities are not directly applicable to intelligent and safety critical systems. This is mainly because very few public datasets are available, and they only consider pedestrian-specific intents for a short temporal horizon from a restricted egocentric view. To this end, we propose LOKI (LOng term and Key Intentions), a novel large-scale dataset that is designed to tackle joint trajectory and intention prediction for heterogeneous traffic agents (pedestrians and vehicles) in an autonomous driving setting. The LOKI dataset is created to discover several factors that may affect intention, including i) agent’s own will, ii) social interactions, iii) environmental constraints, and iv) contextual information. We also propose a model that jointly performs trajectory and intention prediction, showing that recurrently reasoning about intention can assist with trajectory prediction. We show our method outperforms state-of-the-art trajectory prediction methods by upto 27\% and also provide a baseline for frame-wise intention estimation. The dataset is available at https://usa.honda-ri.com/loki},
  archive   = {C_ICCV},
  author    = {Harshayu Girase and Haiming Gang and Srikanth Malla and Jiachen Li and Akira Kanehara and Karttikeya Mangalam and Chiho Choi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00966},
  pages     = {9783-9792},
  title     = {LOKI: Long term and key intentions for trajectory prediction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learn-to-race: A multimodal control environment for
autonomous racing. <em>ICCV</em>, 9773–9782. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing research on autonomous driving primarily focuses on urban driving, which is insufficient for characterising the complex driving behaviour underlying high-speed racing. At the same time, existing racing simulation frameworks struggle in capturing realism, with respect to visual rendering, vehicular dynamics, and task objectives, inhibiting the transfer of learning agents to real-world contexts. We introduce a new environment, where agents Learn-to-Race (L2R) in simulated competition-style racing, using multimodal information—from virtual cameras to a comprehensive array of inertial measurement sensors. Our environment, which includes a simulator and an interfacing training framework, accurately models vehicle dynamics and racing conditions. In this paper, we release the Arrival simulator for autonomous racing. Next, we propose the L2R task with challenging metrics, inspired by learning-to-drive challenges, Formula-style racing, and multimodal trajectory prediction for autonomous driving. Additionally, we provide the L2R framework suite, facilitating simulated racing on high-precision models of real-world tracks. Finally, we provide an official L2R task dataset of expert demonstrations, as well as a series of baseline experiments and reference implementations. We make all code available: https://github.com/learn-to-race/l2r.},
  archive   = {C_ICCV},
  author    = {James Herman and Jonathan Francis and Siddha Ganju and Bingqing Chen and Anirudh Koul and Abhinav Gupta and Alexey Skabelkin and Ivan Zhukov and Max Kumskoy and Eric Nyberg},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00965},
  pages     = {9773-9782},
  title     = {Learn-to-race: A multimodal control environment for autonomous racing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Unsupervised point cloud pre-training via occlusion
completion. <em>ICCV</em>, 9762–9772. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We describe a simple pre-training approach for point clouds. It works in three steps: 1. Mask all points occluded in a camera view; 2. Learn an encoder-decoder model to reconstruct the occluded points; 3. Use the encoder weights as initialisation for downstream point cloud tasks. We find that even when we pre-train on a single dataset (ModelNet40), this method improves accuracy across different datasets and encoders, on a wide range of downstream tasks. Specifically, we show that our method outperforms previous pre-training methods in object classification, and both part-based and semantic segmentation tasks. We study the pre-trained features and find that they lead to wide downstream minima, have high transformation invariance, and have activations that are highly correlated with part labels. Code and data are available at: https://github.com/hansen7/OcCo},
  archive   = {C_ICCV},
  author    = {Hanchen Wang and Qi Liu and Xiangyu Yue and Joan Lasenby and Matt J. Kusner},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00964},
  pages     = {9762-9772},
  title     = {Unsupervised point cloud pre-training via occlusion completion},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to estimate hidden motions with global motion
aggregation. <em>ICCV</em>, 9752–9761. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Occlusions pose a significant challenge to optical flow algorithms that rely on local evidences. We consider an occluded point to be one that is imaged in the reference frame but not in the next, a slight overloading of the standard definition since it also includes points that move out-of-frame. Estimating the motion of these points is extremely difficult, particularly in the two-frame setting. Previous work relies on CNNs to learn occlusions, without much success, or requires multiple frames to reason about occlusions using temporal smoothness. In this paper, we argue that the occlusion problem can be better solved in the two-frame case by modelling image self-similarities. We introduce a global motion aggregation module, a transformer-based approach to find long-range dependencies between pixels in the first image, and perform global aggregation on the corresponding motion features. We demonstrate that the optical flow estimates in the occluded regions can be significantly improved without damaging the performance in non-occluded regions. This approach obtains new state-of-the-art results on the challenging Sintel dataset, improving the average end-point error by 13.6\% on Sintel Final and 13.7\% on Sintel Clean. At the time of submission, our method ranks first on these benchmarks among all published and unpublished approaches. Code is available at https://github.com/zacjiang/GMA.},
  archive   = {C_ICCV},
  author    = {Shihao Jiang and Dylan Campbell and Yao Lu and Hongdong Li and Richard Hartley},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00963},
  pages     = {9752-9761},
  title     = {Learning to estimate hidden motions with global motion aggregation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). X-world: Accessibility, vision, and autonomy meet.
<em>ICCV</em>, 9742–9751. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An important issue facing vision-based intelligent systems today is the lack of accessibility-aware development. A main reason for this issue is the absence of any large-scale, standardized vision benchmarks that incorporate relevant tasks and scenarios related to people with disabilities. This lack of representation hinders even preliminary analysis with respect to underlying pose, appearance, and occlusion characteristics of diverse pedestrians. What is the impact of significant occlusion from a wheelchair on instance segmentation quality? How can interaction with mobility aids, e.g., a long and narrow walking cane, be recognized robustly? To begin addressing such questions, we introduce X-World, an accessibility-centered development environment for vision-based autonomous systems. We tackle inherent data scarcity by leveraging a simulation environment to spawn dynamic agents with various mobility aids. The simulation supports generation of ample amounts of finely annotated, multi-modal data in a safe, cheap, and privacy-preserving manner. Our analysis highlights novel challenges introduced by our benchmark and tasks, as well as numerous opportunities for future developments. We further broaden our analysis using a complementary real-world evaluation benchmark of in-situ navigation by pedestrians with disabilities. Our contributions provide an initial step towards widespread deployment of vision-based agents that can perceive and model the interaction needs of diverse people with disabilities.},
  archive   = {C_ICCV},
  author    = {Jimuyang Zhang and Minglan Zheng and Matthew Boyd and Eshed Ohn-Bar},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00962},
  pages     = {9742-9751},
  title     = {X-world: Accessibility, vision, and autonomy meet},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hierarchical variational neural uncertainty model for
stochastic video prediction. <em>ICCV</em>, 9731–9741. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Predicting the future frames of a video is a challenging task, in part due to the underlying stochastic real-world phenomena. Prior approaches to solve this task typically estimate a latent prior characterizing this stochasticity, however do not account for the predictive uncertainty of the (deep learning) model. Such approaches often derive the training signal from the mean-squared error (MSE) between the generated frame and the ground truth, which can lead to sub-optimal training, especially when the predictive uncertainty is high. Towards this end, we introduce Neural Uncertainty Quantifier (NUQ) - a stochastic quantification of the model’s predictive uncertainty, and use it to weigh the MSE loss. We propose a hierarchical, variational framework to derive NUQ in a principled manner using a deep, Bayesian graphical model. Our experiments on three benchmark stochastic video prediction datasets show that our proposed framework trains more effectively compared to the state-of-the-art models (especially when the training sets are small), while demonstrating better video generation quality and diversity against several evaluation metrics.},
  archive   = {C_ICCV},
  author    = {Moitreya Chatterjee and Narendra Ahuja and Anoop Cherian},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00961},
  pages     = {9731-9741},
  title     = {A hierarchical variational neural uncertainty model for stochastic video prediction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dissecting image crops. <em>ICCV</em>, 9721–9730. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The elementary operation of cropping underpins nearly every computer vision system, ranging from data augmentation and translation invariance to computational photography and representation learning. This paper investigates the subtle traces introduced by this operation. For example, despite refinements to camera optics, lenses will leave behind certain clues, notably chromatic aberration and vignetting. Photographers also leave behind other clues relating to image aesthetics and scene composition. We study how to detect these traces, and investigate the impact that cropping has on the image distribution. While our aim is to dissect the fundamental impact of spatial crops, there are also a number of practical implications to our work, such as revealing faulty photojournalism and equipping neural network researchers with a better understanding of shortcut learning. Code is available at https://github.com/basilevh/dissecting-image-crops.},
  archive   = {C_ICCV},
  author    = {Basile Van Hoorick and Carl Vondrick},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00960},
  pages     = {9721-9730},
  title     = {Dissecting image crops},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Video autoencoder: Self-supervised disentanglement of static
3D structure and motion. <em>ICCV</em>, 9710–9720. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A video autoencoder is proposed for learning disentangled representations of 3D structure and camera pose from videos in a self-supervised manner. Relying on temporal continuity in videos, our work assumes that the 3D scene structure in nearby video frames remains static. Given a sequence of video frames as input, the video autoencoder extracts a disentangled representation of the scene including: (i) a temporally-consistent deep voxel feature to represent the 3D structure and (ii) a 3D trajectory of camera pose for each frame. These two representations will then be re-entangled for rendering the input video frames. This video autoencoder can be trained directly using a pixel re-construction loss, without any ground truth 3D or camera pose annotations. The disentangled representation can be applied to a range of tasks, including novel view synthesis, camera pose estimation, and video generation by motion following. We evaluate our method on several large-scale natural video datasets, and show generalization results on out-of-domain images. Project page with code: https://zlai0.github.io/VideoAutoencoder.},
  archive   = {C_ICCV},
  author    = {Zihang Lai and Sifei Liu and Alexei A. Efros and Xiaolong Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00959},
  pages     = {9710-9720},
  title     = {Video autoencoder: Self-supervised disentanglement of static 3D structure and motion},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Contact-aware retargeting of skinned motion. <em>ICCV</em>,
9700–9709. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a motion retargeting method that preserves self-contacts and prevents interpenetration. Self-contacts, such as when hands touch each other or the torso or the head, are important attributes of human body language and dynamics, yet existing methods do not model or preserve these contacts. Likewise, interpenetration, such as a hand passing into the torso, are a typical artifact of motion estimation methods. The input to our method is a human motion sequence and a target skeleton and character geometry. The method identifies self-contacts and ground contacts in the input motion, and optimizes the motion to apply to the output skeleton, while preserving these contacts and reducing interpenetration. We introduce a novel geometry-conditioned recurrent network with an encoder-space optimization strategy that achieves efficient retargeting while satisfying contact constraints. In experiments, our results quantitatively outperform previous methods and we conduct a user study where our retargeted motions are rated as higher-quality than those produced by recent works. We also show our method generalizes to motion estimated from human videos where we improve over previous works that produce noticeable interpenetration.},
  archive   = {C_ICCV},
  author    = {Ruben Villegas and Duygu Ceylan and Aaron Hertzmann and Jimei Yang and Jun Saito},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00958},
  pages     = {9700-9709},
  title     = {Contact-aware retargeting of skinned motion},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Large scale interactive motion forecasting for autonomous
driving: The waymo open motion dataset. <em>ICCV</em>, 9690–9699. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As autonomous driving systems mature, motion forecasting has received increasing attention as a critical requirement for planning. Of particular importance are interactive situations such as merges, unprotected turns, etc., where predicting individual object motion is not sufficient. Joint predictions of multiple objects are required for effective route planning. There has been a critical need for high-quality motion data that is rich in both interactions and annotation to develop motion planning models. In this work, we introduce the most diverse interactive motion dataset to our knowledge, and provide specific labels for interacting objects suitable for developing joint prediction models. With over 100,000 scenes, each 20 seconds long at 10 Hz, our new dataset contains more than 570 hours of unique data over 1750 km of roadways. It was collected by mining for interesting interactions between vehicles, pedestrians, and cyclists across six cities within the United States. We use a high-accuracy 3D auto-labeling system to generate high quality 3D bounding boxes for each road agent, and provide corresponding high definition 3D maps for each scene. Furthermore, we introduce a new set of metrics that provides a comprehensive evaluation of both single agent and joint agent interaction motion forecasting models. Finally, we provide strong baseline models for individual-agent prediction and joint-prediction. We hope that this new large-scale interactive motion dataset will provide new opportunities for advancing motion forecasting models.},
  archive   = {C_ICCV},
  author    = {Scott Ettinger and Shuyang Cheng and Benjamin Caine and Chenxi Liu and Hang Zhao and Sabeek Pradhan and Yuning Chai and Ben Sapp and Charles Qi and Yin Zhou and Zoey Yang and Aurélien Chouard and Pei Sun and Jiquan Ngiam and Vijay Vasudevan and Alexander McCauley and Jonathon Shlens and Dragomir Anguelov},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00957},
  pages     = {9690-9699},
  title     = {Large scale interactive motion forecasting for autonomous driving: The waymo open motion dataset},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Seeing dynamic scene in the dark: A high-quality video
dataset with mechatronic alignment. <em>ICCV</em>, 9680–9689. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Low-light video enhancement is an important task. Previous work is mostly trained on paired static images or videos. We compile a new dataset formed by our new strategy that contains high-quality spatially-aligned video pairs from dynamic scenes in low- and normal-light conditions. We built it using a mechatronic system to precisely control the dynamics during the video capture process, and further align the video pairs, both spatially and temporally, by identifying the system’s uniform motion stage. Besides the dataset, we propose an end-to-end framework, in which we design a self-supervised strategy to reduce noise, while enhancing the illumination based on the Retinex theory. Extensive experiments based on various metrics and large-scale user study demonstrate the value of our dataset and effectiveness of our method. The dataset and code are available at https://github.com/dvlab-research/SDSD.},
  archive   = {C_ICCV},
  author    = {Ruixing Wang and Xiaogang Xu and Chi-Wing Fu and Jiangbo Lu and Bei Yu and Jiaya Jia},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00956},
  pages     = {9680-9689},
  title     = {Seeing dynamic scene in the dark: A high-quality video dataset with mechatronic alignment},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). UVStyle-net: Unsupervised few-shot learning of 3D style
similarity measure for b-reps. <em>ICCV</em>, 9670–9679. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Boundary Representations (B-Reps) are the industry standard in 3D Computer Aided Design/Manufacturing (CAD/CAM) and industrial design due to their fidelity in representing stylistic details. However, they have been ignored in the 3D style research. Existing 3D style metrics typically operate on meshes or point clouds, and fail to account for end-user subjectivity by adopting fixed definitions of style, either through crowd-sourcing for style labels or hand-crafted features. We propose UVStyle-Net, a style similarity measure for B-Reps that leverages the style signals in the second order statistics of the activations in a pre-trained (unsupervised) 3D encoder, and learns their relative importance to a subjective end-user through few-shot learning. Our approach differs from all existing data-driven 3D style methods since it may be used in completely unsupervised settings, which is desirable given the lack of publicly available labeled B-Rep datasets. More importantly, the few-shot learning accounts for the inherent subjectivity associated with style. We show quantitatively that our proposed method with B-Reps is able to capture stronger style signals than alternative methods on meshes and point clouds despite its significantly greater computational efficiency. We also show it is able to generate meaningful style gradients with respect to the input shape, and that few-shot learning with as few as two positive examples selected by an end-user is sufficient to significantly improve the style measure. Finally, we demonstrate its efficacy on a large unlabeled public dataset of CAD models. Source code and data are available at github.com/AutodeskAILab/UVStyle-Net.},
  archive   = {C_ICCV},
  author    = {Peter Meltzer and Hooman Shayani and Amir Khasahmadi and Pradeep Kumar Jayaraman and Aditya Sanghi and Joseph Lambourne},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00955},
  pages     = {9670-9679},
  title     = {UVStyle-net: Unsupervised few-shot learning of 3D style similarity measure for B-reps},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning facial representations from the cycle-consistency
of face. <em>ICCV</em>, 9660–9669. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Faces manifest large variations in many aspects, such as identity, expression, pose, and face styling. Therefore, it is a great challenge to disentangle and extract these characteristics from facial images, especially in an unsupervised manner. In this work, we introduce cycle-consistency in facial characteristics as free supervisory signal to learn facial representations from unlabeled facial images. The learning is realized by superimposing the facial motion cycle-consistency and identity cycle-consistency constraints. The main idea of the facial motion cycle-consistency is that, given a face with expression, we can perform de-expression to a neutral face via the removal of facial motion and further perform re-expression to reconstruct back to the original face. The main idea of the identity cycle-consistency is to exploit both de-identity into mean face by depriving the given neutral face of its identity via feature re-normalization and re-identity into neutral face by adding the personal attributes to the mean face. At training time, our model learns to disentangle two distinct facial representations to be useful for performing cycle-consistent face reconstruction. At test time, we use the linear protocol scheme for evaluating facial representations on various tasks, including facial expression recognition and head pose regression. We also can directly apply the learnt facial representations to person recognition, frontalization and image-to-image translation. Our experiments show that the results of our approach is competitive with those of existing methods, demonstrating the rich and unique information embedded in the dis-entangled representations. Code is available at https://github.com/JiaRenChang/FaceCycle.},
  archive   = {C_ICCV},
  author    = {Jia-Ren Chang and Yong-Sheng Chen and Wei-Chen Chiu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00954},
  pages     = {9660-9669},
  title     = {Learning facial representations from the cycle-consistency of face},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint inductive and transductive learning for video object
segmentation. <em>ICCV</em>, 9650–9659. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Semi-supervised video object segmentation is a task of segmenting the target object in a video sequence given only a mask annotation in the first frame. The limited information available makes it an extremely challenging task. Most previous best-performing methods adopt matching-based transductive reasoning or online inductive learning. Nevertheless, they are either less discriminative for similar instances or insufficient in the utilization of spatio-temporal information. In this work, we propose to integrate transductive and inductive learning into a unified framework to exploit the complementarity between them for accurate and robust video object segmentation. The proposed approach consists of two functional branches. The transduction branch adopts a lightweight transformer architecture to aggregate rich spatio-temporal cues while the induction branch performs online inductive learning to obtain discriminative target information. To bridge these two diverse branches, a two-head label encoder is introduced to learn the suitable target prior for each of them. The generated mask encodings are further forced to be disentangled to better retain their complementarity. Extensive experiments on several prevalent benchmarks show that, without the need of synthetic training data, the proposed approach sets a series of new state-of-the-art records. Code is available at https://github.com/maoyunyao/JOINT.},
  archive   = {C_ICCV},
  author    = {Yunyao Mao and Ning Wang and Wengang Zhou and Houqiang Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00953},
  pages     = {9650-9659},
  title     = {Joint inductive and transductive learning for video object segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Do image classifiers generalize across time? <em>ICCV</em>,
9641–9649. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vision models notoriously flicker when applied to videos: they correctly recognize objects in some frames, but fail on perceptually similar, nearby frames. In this work, we systematically analyze the robustness of image classifiers to such temporal perturbations in videos. To do so, we construct two new datasets, ImageNet-Vid-Robust and YTBB-Robust, containing a total of 57,897 images grouped into 3,139 sets of perceptually similar images. Our datasets were derived from ImageNet-Vid and Youtube-BB, respectively, and thoroughly re-annotated by human experts for image similarity. We evaluate a diverse array of classifiers pre-trained on ImageNet and show a median classification accuracy drop of 16 and 10 points, respectively, on our two datasets. Additionally, we evaluate three detection models and show that natural perturbations induce both classification as well as localization errors, leading to a median drop in detection mAP of 14 points. Our analysis demonstrates that perturbations occurring naturally in videos pose a substantial and realistic challenge to deploying convolutional neural networks in environments that require both reliable and low-latency predictions.},
  archive   = {C_ICCV},
  author    = {Vaishaal Shankar and Achal Dave and Rebecca Roelofs and Deva Ramanan and Benjamin Recht and Ludwig Schmidt},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00952},
  pages     = {9641-9649},
  title     = {Do image classifiers generalize across time?},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Emerging properties in self-supervised vision transformers.
<em>ICCV</em>, 9630–9640. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) [16] that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3\% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder [26], multi-crop training [9], and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1\% top-1 on ImageNet in linear evaluation with ViT-Base.},
  archive   = {C_ICCV},
  author    = {Mathilde Caron and Hugo Touvron and Ishan Misra and Hervé Jegou and Julien Mairal and Piotr Bojanowski and Armand Joulin},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00951},
  pages     = {9630-9640},
  title     = {Emerging properties in self-supervised vision transformers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). An empirical study of training self-supervised vision
transformers. <em>ICCV</em>, 9620–9629. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper does not describe a novel method. Instead, it studies a straightforward, incremental, yet must-know baseline given the recent progress in computer vision: self-supervised learning for Vision Transformers (ViT). While the training recipes for standard convolutional networks have been highly mature and robust, the recipes for ViT are yet to be built, especially in the self-supervised scenarios where training becomes more challenging. In this work, we go back to basics and investigate the effects of several fundamental components for training self-supervised ViT. We observe that instability is a major issue that degrades accuracy, and it can be hidden by apparently good results. We reveal that these results are indeed partial failure, and they can be improved when training is made more stable. We benchmark ViT results in MoCo v3 and several other self-supervised frameworks, with ablations in various aspects. We discuss the currently positive evidence as well as challenges and open questions. We hope that this work will provide useful data points and experience for future research.},
  archive   = {C_ICCV},
  author    = {Xinlei Chen and Saining Xie and Kaiming He},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00950},
  pages     = {9620-9629},
  title     = {An empirical study of training self-supervised vision transformers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Concept generalization in visual representation learning.
<em>ICCV</em>, 9609–9619. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Measuring concept generalization, i.e., the extent to which models trained on a set of (seen) visual concepts can be leveraged to recognize a new set of (unseen) concepts, is a popular way of evaluating visual representations, especially in a self-supervised learning framework. Nonetheless, the choice of unseen concepts for such an evaluation is usually made arbitrarily, and independently from the seen concepts used to train representations, thus ignoring any semantic relationships between the two. In this paper, we argue that the semantic relationships between seen and unseen concepts affect generalization performance and propose ImageNet-CoG, 1 a novel benchmark on the ImageNet-21K (IN-21K) dataset that enables measuring concept generalization in a principled way. Our benchmark leverages expert knowledge that comes from WordNet in order to define a sequence of unseen IN-21K concept sets that are semantically more and more distant from the ImageNet-1K (IN-1K) subset, a ubiquitous training set. This allows us to benchmark visual representations learned on IN-1K out-of-the box. We conduct a large-scale study encompassing 31 convolution and transformer-based models and show how different architectures, levels of supervision, regularization techniques and use of web data impact the concept generalization performance.},
  archive   = {C_ICCV},
  author    = {Mert Bulent Sariyildiz and Yannis Kalantidis and Diane Larlus and Karteek Alahari},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00949},
  pages     = {9609-9619},
  title     = {Concept generalization in visual representation learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SelfReg: Self-supervised contrastive regularization for
domain generalization. <em>ICCV</em>, 9599–9608. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In general, an experimental environment for deep learning assumes that the training and the test dataset are sampled from the same distribution. However, in real-world situations, a difference in the distribution between two datasets, i.e. domain shift, may occur, which becomes a major factor impeding the generalization performance of the model. The research field to solve this problem is called domain generalization, and it alleviates the domain shift problem by extracting domain-invariant features explicitly or implicitly. In recent studies, contrastive learning-based domain generalization approaches have been proposed and achieved high performance. These approaches require sampling of the negative data pair. However, the performance of contrastive learning fundamentally depends on quality and quantity of negative data pairs. To address this issue, we propose a new regularization method for domain generalization based on contrastive learning, called self-supervised contrastive regularization (SelfReg). The proposed approach use only positive data pairs, thus it resolves various problems caused by negative pair sampling. Moreover, we propose a class-specific domain perturbation layer (CDPL), which makes it possible to effectively apply mixup augmentation even when only positive data pairs are used. The experimental results show that the techniques incorporated by SelfReg contributed to the performance in a compatible manner. In the recent benchmark, DomainBed, the proposed method shows comparable performance to the conventional state-of-the-art alternatives.},
  archive   = {C_ICCV},
  author    = {Daehee Kim and Youngjun Yoo and Seunghyun Park and Jinkyu Kim and Jaekoo Lee},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00948},
  pages     = {9599-9608},
  title     = {SelfReg: Self-supervised contrastive regularization for domain generalization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ISD: Self-supervised learning by iterative similarity
distillation. <em>ICCV</em>, 9589–9598. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, contrastive learning has achieved great results in self-supervised learning, where the main idea is to pull two augmentations of an image (positive pairs) closer compared to other random images (negative pairs). We argue that not all negative images are equally negative. Hence, we introduce a self-supervised learning algorithm where we use a soft similarity for the negative images rather than a binary distinction between positive and negative pairs. We iteratively distill a slowly evolving teacher model to the student model by capturing the similarity of a query image to some random images and transferring that knowledge to the student. Specifically, our method should handle unbalanced and unlabeled data better than existing contrastive learning methods, because the randomly chosen negative set might include many samples that are semantically similar to the query image. In this case, our method labels them as highly similar while standard contrastive methods label them as negatives. Our method achieves comparable results to the state-of-the-art models. Our code is available here: https://github.com/UMBCvision/ISD.},
  archive   = {C_ICCV},
  author    = {Ajinkya Tejankar and Soroush Abbasi Koohpayegani and Vipin Pillai and Paolo Favaro and Hamed Pirsiavash},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00947},
  pages     = {9589-9598},
  title     = {ISD: Self-supervised learning by iterative similarity distillation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On feature decorrelation in self-supervised learning.
<em>ICCV</em>, 9578–9588. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In self-supervised representation learning, a common idea behind most of the state-of-the-art approaches is to enforce the robustness of the representations to predefined augmentations. A potential issue of this idea is the existence of completely collapsed solutions (i.e., constant features), which are typically avoided implicitly by carefully chosen implementation details. In this work, we study a relatively concise framework containing the most common components from recent approaches. We verify the existence of complete collapse and discover another reachable collapse pattern that is usually overlooked, namely dimensional collapse. We connect dimensional collapse with strong correlations between axes and consider such connection as a strong motivation for feature decorrelation (i.e., standardizing the covariance matrix). The gains from feature decorrelation are verified empirically to highlight the importance and the potential of this insight.},
  archive   = {C_ICCV},
  author    = {Tianyu Hua and Wenxiao Wang and Zihui Xue and Sucheng Ren and Yue Wang and Hang Zhao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00946},
  pages     = {9578-9588},
  title     = {On feature decorrelation in self-supervised learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). With a little help from my friends: Nearest-neighbor
contrastive learning of visual representations. <em>ICCV</em>,
9568–9577. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-supervised learning algorithms based on instance discrimination train encoders to be invariant to pre-defined transformations of the same instance. While most methods treat different views of the same image as positives for a contrastive loss, we are interested in using positives from other instances in the dataset. Our method, Nearest-Neighbor Contrastive Learning of visual Representations (NNCLR), samples the nearest neighbors from the dataset in the latent space, and treats them as positives. This provides more semantic variations than pre-defined transformations.We find that using the nearest-neighbor as positive in contrastive losses improves performance significantly on ImageNet classification using ResNet-50 under the linear evaluation protocol, from 71.7\% to 75.6\%, outperforming previous state-of-the-art methods. On semi-supervised learning benchmarks we improve performance significantly when only 1\% ImageNet labels are available, from 53.8\% to 56.5\%. On transfer learning benchmarks our method outperforms state-of-the-art methods (including supervised learning with ImageNet) on 8 out of 12 downstream datasets. Furthermore, we demonstrate empirically that our method is less reliant on complex data augmentations. We see a relative reduction of only 2.1\% ImageNet Top-1 accuracy when we train using only random crops.},
  archive   = {C_ICCV},
  author    = {Debidatta Dwibedi and Yusuf Aytar and Jonathan Tompson and Pierre Sermanet and Andrew Zisserman},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00945},
  pages     = {9568-9577},
  title     = {With a little help from my friends: Nearest-neighbor contrastive learning of visual representations},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On compositions of transformations in contrastive
self-supervised learning. <em>ICCV</em>, 9557–9567. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the image domain, excellent representations can be learned by inducing invariance to content-preserving transformations via noise contrastive learning. In this paper, we generalize contrastive learning to a wider set of transformations, and their compositions, for which either invariance or distinctiveness is sought. We show that it is not immediately obvious how existing methods such as SimCLR can be extended to do so. Instead, we introduce a number of formal requirements that all contrastive formulations must satisfy, and propose a practical construction which satisfies these requirements. In order to maximise the reach of this analysis, we express all components of noise contrastive formulations as the choice of certain generalized transformations of the data (GDTs), including data sampling. We then consider videos as an example of data in which a large variety of transformations are applicable, accounting for the extra modalities – for which we analyze audio and text – and the dimension of time. We find that being invariant to certain transformations and distinctive to others is critical to learning effective video representations, improving the state-of-the-art for multiple benchmarks by a large margin, and even surpassing supervised pretraining. Code and pretrained models are available 1 .},
  archive   = {C_ICCV},
  author    = {Mandela Patrick and Yuki M. Asano and Polina Kuznetsova and Ruth Fong and João F. Henriques and Geoffrey Zweig and Andrea Vedaldi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00944},
  pages     = {9557-9567},
  title     = {On compositions of transformations in contrastive self-supervised learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Universal-prototype enhancing for few-shot object detection.
<em>ICCV</em>, 9547–9556. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Few-shot object detection (FSOD) aims to strengthen the performance of novel object detection with few labeled samples. To alleviate the constraint of few samples, enhancing the generalization ability of learned features for novel objects plays a key role. Thus, the feature learning process of FSOD should focus more on intrinsical object characteristics, which are invariant under different visual changes and therefore are helpful for feature generalization. Unlike previous attempts of the meta-learning paradigm, in this paper, we explore how to enhance object features with intrinsical characteristics that are universal across different object categories. We propose a new prototype, namely universal prototype, that is learned from all object categories. Besides the advantage of characterizing invariant characteristics, the universal prototypes alleviate the impact of unbalanced object categories. After enhancing object features with the universal prototypes, we impose a consistency loss to maximize the agreement between the enhanced features and the original ones, which is beneficial for learning invariant object characteristics. Thus, we develop a new framework of few-shot object detection with universal prototypes (F SOD up ) that owns the merit of feature generalization towards novel objects. Experimental results on PASCAL VOC and MS COCO show the effectiveness of F SOD up . Particularly, for the 1-shot case of VOC Split2, FSOD up outperforms the baseline by 6.8\% in terms of mAP.},
  archive   = {C_ICCV},
  author    = {Aming Wu and Yahong Han and Linchao Zhu and Yi Yang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00943},
  pages     = {9547-9556},
  title     = {Universal-prototype enhancing for few-shot object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SIGN: Spatial-information incorporated generative network
for generalized zero-shot semantic segmentation. <em>ICCV</em>,
9536–9546. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unlike conventional zero-shot classification, zero-shot semantic segmentation predicts a class label at the pixel level instead of the image level. When solving zero-shot semantic segmentation problems, the need for pixel-level prediction with surrounding context motivates us to incorporate spatial information using positional encoding. We improve standard positional encoding by introducing the concept of Relative Positional Encoding, which integrates spatial information at the feature level and can handle arbitrary image sizes. Furthermore, while self-training is widely used in zero-shot semantic segmentation to generate pseudo-labels, we propose a new knowledge-distillation-inspired self-training strategy, namely Annealed Self-Training, which can automatically assign different importance to pseudo-labels to improve performance. We systematically study the proposed Relative Positional Encoding and Annealed Self-Training in a comprehensive experimental evaluation, and our empirical results confirm the effectiveness of our method on three benchmark datasets.},
  archive   = {C_ICCV},
  author    = {Jiaxin Cheng and Soumyaroop Nandi and Prem Natarajan and Wael Abd-Almageed},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00942},
  pages     = {9536-9546},
  title     = {SIGN: Spatial-information incorporated generative network for generalized zero-shot semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Field-guide-inspired zero-shot learning. <em>ICCV</em>,
9526–9535. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern recognition systems require large amounts of supervision to achieve accuracy. Adapting to new domains requires significant data from experts, which is onerous and can become too expensive. Zero-shot learning requires an annotated set of attributes for a novel category. Annotating the full set of attributes for a novel category proves to be a tedious and expensive task in deployment. This is especially the case when the recognition domain is an expert domain. We introduce a new field-guide-inspired approach to zero-shot annotation where the learner model interactively asks for the most useful attributes that define a class. We evaluate our method on classification benchmarks with attribute annotations like CUB, SUN, and AWA2 and show that our model achieves the performance of a model with full annotations at the cost of significantly fewer number of annotations. Since the time of experts is precious, decreasing annotation cost can be very valuable for real-world deployment.},
  archive   = {C_ICCV},
  author    = {Utkarsh Mall and Bharath Hariharan and Kavita Bala},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00941},
  pages     = {9526-9535},
  title     = {Field-guide-inspired zero-shot learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploiting a joint embedding space for generalized zero-shot
semantic segmentation. <em>ICCV</em>, 9516–9525. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the problem of generalized zero-shot semantic segmentation (GZS3) predicting pixel-wise semantic labels for seen and unseen classes. Most GZS3 methods adopt a generative approach that synthesizes visual features of unseen classes from corresponding semantic ones (e.g., word2vec) to train novel classifiers for both seen and unseen classes. Although generative methods show decent performance, they have two limitations: (1) the visual features are biased towards seen classes; (2) the classifier should be retrained whenever novel unseen classes appear. We propose a discriminative approach to address these limitations in a unified framework. To this end, we leverage visual and semantic encoders to learn a joint embedding space, where the semantic encoder transforms semantic features to semantic prototypes that act as centers for visual features of corresponding classes. Specifically, we introduce boundary-aware regression (BAR) and semantic consistency (SC) losses to learn discriminative features. Our approach to exploiting the joint embedding space, together with BAR and SC terms, alleviates the seen bias problem. At test time, we avoid the retraining process by exploiting semantic prototypes as a nearest-neighbor (NN) classifier. To further alleviate the bias problem, we also propose an inference technique, dubbed Apollonius calibration (AC), that modulates the decision boundary of the NN classifier to the Apollonius circle adaptively. Experimental results demonstrate the effectiveness of our framework, achieving a new state of the art on standard benchmarks.},
  archive   = {C_ICCV},
  author    = {Donghyeon Baek and Youngmin Oh and Bumsub Ham},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00940},
  pages     = {9516-9525},
  title     = {Exploiting a joint embedding space for generalized zero-shot semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Universal representation learning from multiple domains for
few-shot classification. <em>ICCV</em>, 9506–9515. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we look at the problem of few-shot image classification that aims to learn a classifier for previously unseen classes and domains from few labeled samples. Recent methods use various adaptation strategies for aligning their visual representations to new domains or select the relevant ones from multiple domain-specific feature extractors. In this work, we present URL, which learns a single set of universal visual representations by distilling knowledge of multiple domain-specific networks after co-aligning their features with the help of adapters and centered kernel alignment. We show that the universal representations can be further refined for previously unseen domains by an efficient adaptation step in a similar spirit to distance learning methods. We rigorously evaluate our model in the recent Meta-Dataset benchmark and demonstrate that it significantly outperforms the previous methods while being more efficient.},
  archive   = {C_ICCV},
  author    = {Wei-Hong Li and Xialei Liu and Hakan Bilen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00939},
  pages     = {9506-9515},
  title     = {Universal representation learning from multiple domains for few-shot classification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Co2L: Contrastive continual learning. <em>ICCV</em>,
9496–9505. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent breakthroughs in self-supervised learning show that such algorithms learn visual representations that can be transferred better to unseen tasks than cross-entropy based methods which rely on task-specific supervision. In this paper, we found that the similar holds in the continual learning context: contrastively learned representations are more robust against the catastrophic forgetting than ones trained with the cross-entropy objective. Based on this novel observation, we propose a rehearsal-based continual learning algorithm that focuses on continually learning and maintaining transferable representations. More specifically, the proposed scheme (1) learns representations using the contrastive learning objective, and (2) preserves learned representations using a self-supervised distillation step. We conduct extensive experimental validations under popular benchmark image classification datasets, where our method sets the new state-of-the-art performance. Source code is available at https://github.com/chaht01/Co2L.},
  archive   = {C_ICCV},
  author    = {Hyuntak Cha and Jaeho Lee and Jinwoo Shin},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00938},
  pages     = {9496-9505},
  title     = {Co2L: Contrastive continual learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Solving inefficiency of self-supervised representation
learning. <em>ICCV</em>, 9485–9495. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-supervised learning (especially contrastive learning) has attracted great interest due to its huge potential in learning discriminative representations in an unsupervised manner. Despite the acknowledged successes, existing contrastive learning methods suffer from very low learning efficiency, e.g., taking about ten times more training epochs than supervised learning for comparable recognition accuracy. In this paper, we reveal two contradictory phenomena in contrastive learning that we call under-clustering and over-clustering problems, which are major obstacles to learning efficiency. Under-clustering means that the model cannot efficiently learn to discover the dissimilarity between inter-class samples when the negative sample pairs for contrastive learning are insufficient to differentiate all the actual object classes. Over-clustering implies that the model cannot efficiently learn features from excessive negative sample pairs, forcing the model to over-cluster samples of the same actual classes into different clusters. To simultaneously overcome these two problems, we propose a novel self-supervised learning framework using a truncated triplet loss. Precisely, we employ a triplet loss tending to maximize the relative distance between the positive pair and negative pairs to address the under-clustering problem; and we construct the negative pair by selecting a negative sample deputy from all negative samples to avoid the over-clustering problem, guaranteed by the Bernoulli Distribution model. We extensively evaluate our framework in several large-scale benchmarks (e.g., ImageNet, SYSU-30k, and COCO). The results demonstrate our model’s superiority (e.g., the learning efficiency) over the latest state-of-the-art methods by a clear margin. See Codes 1 .},
  archive   = {C_ICCV},
  author    = {Guangrun Wang and Keze Wang and Guangcong Wang and Philip H.S. Torr and Liang Lin},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00937},
  pages     = {9485-9495},
  title     = {Solving inefficiency of self-supervised representation learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributional robustness loss for long-tail learning.
<em>ICCV</em>, 9475–9484. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-world data is often unbalanced and long-tailed, but deep models struggle to recognize rare classes in the presence of frequent classes. To address unbalanced data, most studies try balancing the data, the loss, or the classifier to reduce classification bias towards head classes. Far less attention has been given to the latent representations learned with unbalanced data. We show that the feature extractor part of deep networks suffers greatly from this bias. We propose a new loss based on robustness theory, which encourages the model to learn high-quality representations for both head and tail classes. While the general form of the robustness loss may be hard to compute, we further derive an easy-to-compute upper bound that can be minimized efficiently. This procedure reduces representation bias towards head classes in the feature space and achieves new SOTA results on CIFAR100-LT, ImageNet-LT, and iNaturalist long-tail benchmarks. We find that training with robustness increases recognition accuracy of tail classes while largely maintaining the accuracy of head classes. The new robustness loss can be combined with various classifier balancing techniques and can be applied to representations at several layers of the deep model.},
  archive   = {C_ICCV},
  author    = {Dvir Samuel and Gal Chechik},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00936},
  pages     = {9475-9484},
  title     = {Distributional robustness loss for long-tail learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Learning from noisy data with robust representation
learning. <em>ICCV</em>, 9465–9474. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning from noisy data has attracted much attention, where most methods focus on label noise. In this work, we propose a new learning framework which simultaneously addresses three types of noise commonly seen in real-world data: label noise, out-of-distribution input, and input corruption. In contrast to most existing methods, we combat noise by learning robust representation. Specifically, we embed images into a low-dimensional subspace, and regularize the geometric structure of the subspace with robust contrastive learning, which includes an unsupervised consistency loss and a supervised mixup prototypical loss. We also propose a new noise cleaning method which leverages the learned representation to enforce a smoothness constraint on neighboring samples. Experiments on multiple benchmarks demonstrate state-of-the-art performance of our method and robustness of the learned representation. Code is available at https://github.com/salesforce/RRL/.},
  archive   = {C_ICCV},
  author    = {Junnan Li and Caiming Xiong and Steven C.H. Hoi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00935},
  pages     = {9465-9474},
  title     = {Learning from noisy data with robust representation learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). CoMatch: Semi-supervised learning with contrastive graph
regularization. <em>ICCV</em>, 9455–9464. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Semi-supervised learning has been an effective paradigm for leveraging unlabeled data to reduce the reliance on labeled data. We propose CoMatch, a new semi-supervised learning method that unifies dominant approaches and addresses their limitations. CoMatch jointly learns two representations of the training data, their class probabilities and low-dimensional embeddings. The two representations interact with each other to jointly evolve. The embeddings impose a smoothness constraint on the class probabilities to improve the pseudo-labels, whereas the pseudo-labels regularize the structure of the embeddings through graph-based contrastive learning. CoMatch achieves state-of-the-art performance on multiple datasets. It achieves substantial accuracy improvements on the label-scarce CIFAR-10 and STL-10. On ImageNet with 1\% labels, CoMatch achieves a top-1 accuracy of 66.0\%, outperforming FixMatch [32] by 12.6\%. Furthermore, CoMatch achieves better representation learning performance on downstream tasks, outperforming both supervised learning and self-supervised learning. Code and pre-trained models are available at https://github.com/salesforce/CoMatch/.},
  archive   = {C_ICCV},
  author    = {Junnan Li and Caiming Xiong and Steven C. H. Hoi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00934},
  pages     = {9455-9464},
  title     = {CoMatch: Semi-supervised learning with contrastive graph regularization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Meta-learning with task-adaptive loss function for few-shot
learning. <em>ICCV</em>, 9445–9454. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In few-shot learning scenarios, the challenge is to generalize and perform well on new unseen examples when only very few labeled examples are available for each task. Model-agnostic meta-learning (MAML) has gained the popularity as one of the representative few-shot learning methods for its flexibility and applicability to diverse problems. However, MAML and its variants often resort to a simple loss function without any auxiliary loss function or regularization terms that can help achieve better generalization. The problem lies in that each application and task may require different auxiliary loss function, especially when tasks are diverse and distinct. Instead of attempting to hand-design an auxiliary loss function for each application and task, we introduce a new meta-learning framework with a loss function that adapts to each task. Our proposed framework, named Meta-Learning with Task-Adaptive Loss Function (MeTAL), demonstrates the effectiveness and the flexibility across various domains, such as few-shot classification and few-shot regression.},
  archive   = {C_ICCV},
  author    = {Sungyong Baik and Janghoon Choi and Heewon Kim and Dohee Cho and Jaesik Min and Kyoung Mu Lee},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00933},
  pages     = {9445-9454},
  title     = {Meta-learning with task-adaptive loss function for few-shot learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Few-shot and continual learning with attentive independent
mechanisms. <em>ICCV</em>, 9435–9444. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep neural networks (DNNs) are known to perform well when deployed to test distributions that shares high similarity with the training distribution. Feeding DNNs with new data sequentially that were unseen in the training distribution has two major challenges — fast adaptation to new tasks and catastrophic forgetting of old tasks. Such difficulties paved way for the on-going research on few-shot learning and continual learning. To tackle these problems, we introduce Attentive Independent Mechanisms (AIM). We incorporate the idea of learning using fast and slow weights in conjunction with the decoupling of the feature extraction and higher-order conceptual learning of a DNN. AIM is designed for higher-order conceptual learning, modeled by a mixture of experts that compete to learn independent concepts to solve a new task. AIM is a modular component that can be inserted into existing deep learning frameworks. We demonstrate its capability for few-shot learning by adding it to SIB and trained on MiniImageNet and CIFAR-FS, showing significant improvement. AIM is also applied to ANML and OML trained on Omniglot, CIFAR-100 and MiniImageNet to demonstrate its capability in continual learning. Code made publicly available at https://github.com/huang50213/AIM-Fewshot-Continual.},
  archive   = {C_ICCV},
  author    = {Eugene Lee and Cheng-Han Huang and Chen-Yi Lee},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00932},
  pages     = {9435-9444},
  title     = {Few-shot and continual learning with attentive independent mechanisms},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Few-shot image classification: Just use a library of
pre-trained feature extractors and a simple classifier. <em>ICCV</em>,
9425–9434. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent papers have suggested that transfer learning can outperform sophisticated meta-learning methods for few-shot image classification. We take this hypothesis to its logical conclusion, and suggest the use of an ensemble of high-quality, pre-trained feature extractors for few-shot image classification. We show experimentally that a library of pre-trained feature extractors combined with a simple feed-forward network learned with an L2-regularizer can be an excellent option for solving cross-domain few-shot image classification. Our experimental results suggest that this simple approach far outperforms several well-established meta-learning algorithms.},
  archive   = {C_ICCV},
  author    = {Arkabandhu Chowdhury and Mingchao Jiang and Swarat Chaudhuri and Chris Jermaine},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00931},
  pages     = {9425-9434},
  title     = {Few-shot image classification: Just use a library of pre-trained feature extractors and a simple classifier},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Meta navigator: Search for a good adaptation policy for
few-shot learning. <em>ICCV</em>, 9415–9424. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Few-shot learning aims to adapt knowledge learned from previous tasks to novel tasks with only a limited amount of labeled data. Research literature on few-shot learning exhibits great diversity, while different algorithms often excel at different few-shot learning scenarios. It is therefore tricky to decide which learning strategies to use under different task conditions. Inspired by the recent success in Automated Machine Learning literature (AutoML), in this paper, we present Meta Navigator, a framework that attempts to solve the aforementioned limitation in few-shot learning by seeking a higher-level strategy and proffer to automate the selection from various few-shot learning designs. The goal of our work is to search for good parameter adaptation policies that are applied to different stages in the network for few-shot classification. We present a search space that covers many popular few-shot learning algorithms in the literature, and develop a differentiable searching and decoding algorithm based on meta-learning that supports gradient-based optimization. We demonstrate the effectiveness of our searching-based method on multiple benchmark datasets. Extensive experiments show that our approach significantly outperforms baselines and demonstrates performance advantages over many state-of-the-art methods.},
  archive   = {C_ICCV},
  author    = {Chi Zhang and Henghui Ding and Guosheng Lin and Ruibo Li and Changhu Wang and Chunhua Shen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00930},
  pages     = {9415-9424},
  title     = {Meta navigator: Search for a good adaptation policy for few-shot learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Boosting the generalization capability in cross-domain
few-shot learning via noise-enhanced supervised autoencoder.
<em>ICCV</em>, 9404–9414. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {State of the art (SOTA) few-shot learning (FSL) methods suffer significant performance drop in the presence of domain differences between source and target datasets. The strong discrimination ability on the source dataset does not necessarily translate to high classification accuracy on the target dataset. In this work, we address this cross-domain few-shot learning (CDFSL) problem by boosting the generalization capability of the model. Specifically, we teach the model to capture broader variations of the feature distributions with a novel noise-enhanced supervised autoencoder (NSAE). NSAE trains the model by jointly reconstructing inputs and predicting the labels of inputs as well as their reconstructed pairs. Theoretical analysis based on intra-class correlation (ICC) shows that the feature embeddings learned from NSAE have stronger discrimination and generalization abilities in the target domain. We also take advantage of NSAE structure and propose a two-step fine-tuning procedure that achieves better adaption and improves classification performance in the target domain. Extensive experiments and ablation studies are conducted to demonstrate the effectiveness of the proposed method. Experimental results show that our proposed method consistently outperforms SOTA methods under various conditions.},
  archive   = {C_ICCV},
  author    = {Hanwen Liang and Qiong Zhang and Peng Dai and Juwei Lu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00929},
  pages     = {9404-9414},
  title     = {Boosting the generalization capability in cross-domain few-shot learning via noise-enhanced supervised autoencoder},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Seasonal contrast: Unsupervised pre-training from uncurated
remote sensing data. <em>ICCV</em>, 9394–9403. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Remote sensing and automatic earth monitoring are key to solve global-scale challenges such as disaster prevention, land use monitoring, or tackling climate change. Although there exist vast amounts of remote sensing data, most of it remains unlabeled and thus inaccessible for supervised learning algorithms. Transfer learning approaches can reduce the data requirements of deep learning algorithms. However, most of these methods are pre-trained on ImageNet and their generalization to remote sensing imagery is not guaranteed due to the domain gap. In this work, we propose Seasonal Contrast (SeCo), an effective pipeline to leverage unlabeled data for in-domain pre-training of remote sensing representations. The SeCo pipeline is composed of two parts. First, a principled procedure to gather large-scale, unlabeled and uncurated remote sensing datasets containing images from multiple Earth locations at different timestamps. Second, a self-supervised algorithm that takes advantage of time and position invariance to learn transferable representations for remote sensing applications. We empirically show that models trained with SeCo achieve better performance than their ImageNet pre-trained counterparts and state-of-the-art self-supervised learning methods on multiple downstream tasks. The datasets and models in SeCo will be made public to facilitate transfer learning and enable rapid progress in remote sensing applications. 1},
  archive   = {C_ICCV},
  author    = {Oscar Mañas and Alexandre Lacoste and Xavier Giró-i-Nieto and David Vazquez and Pau Rodríguez},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00928},
  pages     = {9394-9403},
  title     = {Seasonal contrast: Unsupervised pre-training from uncurated remote sensing data},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Testing using privileged information by adapting features
with statistical dependence. <em>ICCV</em>, 9385–9393. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Given an imperfect predictor, we exploit additional features at test time to improve the predictions made, without retraining and without knowledge of the prediction function. This scenario arises if training labels or data are proprietary, restricted, or no longer available, or if training itself is prohibitively expensive. We assume that the additional features are useful if they exhibit strong statistical dependence to the underlying perfect predictor. Then, we empirically estimate and strengthen the statistical dependence between the initial noisy predictor and the additional features via manifold de-noising. As an example, we show that this approach leads to improvement in real-world visual attribute ranking.},
  archive   = {C_ICCV},
  author    = {Kwang In Kim and James Tompkin},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00927},
  pages     = {9385-9393},
  title     = {Testing using privileged information by adapting features with statistical dependence},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Densely guided knowledge distillation using multiple teacher
assistants. <em>ICCV</em>, 9375–9384. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the success of deep neural networks, knowledge distillation which guides the learning of a small student network from a large teacher network is being actively studied for model compression and transfer learning. However, few studies have been performed to resolve the poor learning issue of the student network when the student and teacher model sizes significantly differ. In this paper, we propose a densely guided knowledge distillation using multiple teacher assistants that gradually decreases the model size to efficiently bridge the large gap between the teacher and student networks. To stimulate more efficient learning of the student network, we guide each teacher assistant to every other smaller teacher assistants iteratively. Specifically, when teaching a smaller teacher assistant at the next step, the existing larger teacher assistants from the previous step are used as well as the teacher network. Moreover, we design stochastic teaching where, for each mini-batch, a teacher or teacher assistants are randomly dropped. This acts as a regularizer to improve the efficiency of teaching of the student network. Thus, the student can always learn salient distilled knowledge from the multiple sources. We verified the effectiveness of the proposed method for a classification task using CIFAR-10, CIFAR-100, and ImageNet. We also achieved significant performance improvements with various backbone architectures such as ResNet, WideResNet, and VGG. 1},
  archive   = {C_ICCV},
  author    = {Wonchul Son and Jaemin Na and Junyong Choi and Wonjun Hwang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00926},
  pages     = {9375-9384},
  title     = {Densely guided knowledge distillation using multiple teacher assistants},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rehearsal revealed: The limits and merits of revisiting
samples in continual learning. <em>ICCV</em>, 9365–9374. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning from non-stationary data streams and overcoming catastrophic forgetting still poses a serious challenge for machine learning research. Rather than aiming to improve state-of-the-art, in this work we provide insight into the limits and merits of rehearsal, one of continual learning’s most established methods. We hypothesize that models trained sequentially with rehearsal tend to stay in the same low-loss region after a task has finished, but are at risk of overfitting on its sample memory, hence harming generalization. We provide both conceptual and strong empirical evidence on three benchmarks for both behaviors, bringing novel insights into the dynamics of rehearsal and continual learning in general. Finally, we interpret important continual learning works in the light of our findings, allowing for a deeper understanding of their successes. 1},
  archive   = {C_ICCV},
  author    = {Eli Verwimp and Matthias De Lange and Tinne Tuytelaars},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00925},
  pages     = {9365-9374},
  title     = {Rehearsal revealed: The limits and merits of revisiting samples in continual learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Always be dreaming: A new approach for data-free
class-incremental learning. <em>ICCV</em>, 9354–9364. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern computer vision applications suffer from catastrophic forgetting when incrementally learning new concepts over time. The most successful approaches to alleviate this forgetting require extensive replay of previously seen data, which is problematic when memory constraints or data legality concerns exist. In this work, we consider the high-impact problem of Data-Free Class-Incremental Learning (DFCIL), where an incremental learning agent must learn new concepts over time without storing generators or training data from past tasks. One approach for DFCIL is to replay synthetic images produced by inverting a frozen copy of the learner’s classification model, but we show this approach fails for common class-incremental benchmarks when using standard distillation strategies. We diagnose the cause of this failure and propose a novel incremental distillation strategy for DFCIL, contributing a modified cross-entropy training and importance-weighted feature distillation, and show that our method results in up to a 25.1\% increase in final task accuracy (absolute difference) compared to SOTA DFCIL methods for common class-incremental benchmarks. Our method even outperforms several standard replay based methods which store a coreset of images. Our code is available at https://github.com/GT-RIPL/AlwaysBeDreaming-DFCIL},
  archive   = {C_ICCV},
  author    = {James Smith and Yen-Chang Hsu and Jonathan Balloch and Yilin Shen and Hongxia Jin and Zsolt Kira},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00924},
  pages     = {9354-9364},
  title     = {Always be dreaming: A new approach for data-free class-incremental learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MT-ORL: Multi-task occlusion relationship learning.
<em>ICCV</em>, 9344–9353. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Retrieving occlusion relation among objects in a single image is challenging due to sparsity of boundaries in image. We observe two key issues in existing works: firstly, lack of an architecture which can exploit the limited amount of coupling in the decoder stage between the two subtasks, namely occlusion boundary extraction and occlusion orientation prediction, and secondly, improper representation of occlusion orientation. In this paper, we propose a novel architecture called Occlusion-shared and Path-separated Network (OPNet), which solves the first issue by exploiting rich occlusion cues in shared high-level features and structured spatial information in task-specific low-level features. We then design a simple but effective orthogonal occlusion representation (OOR) to tackle the second issue. Our method surpasses the state-of-the-art methods by 6.1\%/8.3\% Boundary-AP and 6.5\%/10\% Orientation-AP on standard PIOD/BSDS ownership datasets. Code is available at https://github.com/fengpanhe/MT-ORL.},
  archive   = {C_ICCV},
  author    = {Panhe Feng and Qi She and Lei Zhu and Jiaxin Li and Lin Zhang and Zijian Feng and Changhu Wang and Chunpeng Li and Xuejing Kang and Anlong Ming},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00923},
  pages     = {9344-9353},
  title     = {MT-ORL: Multi-task occlusion relationship learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). STEM: An approach to multi-source domain adaptation with
guarantees. <em>ICCV</em>, 9332–9343. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-source Domain Adaptation (MSDA) is more practical but challenging than the conventional unsupervised domain adaptation due to the involvement of diverse multiple data sources. Two fundamental challenges of MSDA are: (i) how to deal with the diversity in the multiple source domains and (ii) how to cope with the data shift between the target domain and the source domains. In this paper, to address the first challenge, we propose a theoretical-guaranteed approach to combine domain experts locally trained on its own source domain to achieve a combined multi-source teacher that globally predicts well on the mixture of source domains. To address the second challenge, we propose to bridge the gap between the target domain and the mixture of source domains in the latent space via a generator or feature extractor. Together with bridging the gap in the latent space, we train a student to mimic the predictions of the teacher expert on both source and target examples. In addition, our approach is guaranteed with rigorous theory offered insightful justifications of how each component influences the transferring performance. Extensive experiments conducted on three benchmark datasets show that our proposed method achieves state-of-the-art performances to the best of our knowledge.},
  archive   = {C_ICCV},
  author    = {Van-Anh Nguyen and Tuan Nguyen and Trung Le and Quan Hung Tran and Dinh Phung},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00922},
  pages     = {9332-9343},
  title     = {STEM: An approach to multi-source domain adaptation with guarantees},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vector-decomposed disentanglement for domain-invariant
object detection. <em>ICCV</em>, 9322–9331. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To improve the generalization of detectors, for domain adaptive object detection (DAOD), recent advances mainly explore aligning feature-level distributions between the source and single-target domain, which may neglect the impact of domain-specific information existing in the aligned features. Towards DAOD, it is important to extract domain-invariant object representations. To this end, in this paper, we try to disentangle domain-invariant representations from domain-specific representations. And we propose a novel disentangled method based on vector decomposition. Firstly, an extractor is devised to separate domain-invariant representations from the input, which are used for extracting object proposals. Secondly, domain-specific representations are introduced as the differences between the input and domain-invariant representations. Through the difference operation, the gap between the domain-specific and domain-invariant representations is enlarged, which promotes domain-invariant representations to contain more domain-irrelevant information. In the experiment, we separately evaluate our method on the single- and compound-target case. For the single-target case, experimental results of four domain-shift scenes show our method obtains a significant performance gain over baseline methods. Moreover, for the compound-target case (i.e., the target is a compound of two different domains without domain labels), our method outperforms baseline methods by around 4\%, which demonstrates the effectiveness of our method.},
  archive   = {C_ICCV},
  author    = {Aming Wu and Rui Liu and Yahong Han and Linchao Zhu and Yi Yang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00921},
  pages     = {9322-9331},
  title     = {Vector-decomposed disentanglement for domain-invariant object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Partial video domain adaptation with partial adversarial
temporal attentive network. <em>ICCV</em>, 9312–9321. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Partial Domain Adaptation (PDA) is a practical and general domain adaptation scenario, which relaxes the fully shared label space assumption such that the source label space subsumes the target one. The key challenge of PDA is the issue of negative transfer caused by source-only classes. For videos, such negative transfer could be triggered by both spatial and temporal features, which leads to a more challenging Partial Video Domain Adaptation (PVDA) problem. In this paper, we propose a novel Partial Adversarial Temporal Attentive Network (PATAN) to address the PVDA problem by utilizing both spatial and temporal features for filtering source-only classes. Besides, PATAN constructs effective overall temporal features by attending to local temporal features that contribute more toward the class filtration process. We further introduce new benchmarks to facilitate research on PVDA problems, covering a wide range of PVDA scenarios. Empirical results demonstrate the state-of-the-art performance of our proposed PATAN across the multiple PVDA benchmarks. Code will be provided at: https://github.com/xuyu0010/PATAN.},
  archive   = {C_ICCV},
  author    = {Yuecong Xu and Jianfei Yang and Haozhi Cao and Zhenghua Chen and Qi Li and Kezhi Mao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00920},
  pages     = {9312-9321},
  title     = {Partial video domain adaptation with partial adversarial temporal attentive network},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards novel target discovery through open-set domain
adaptation. <em>ICCV</em>, 9302–9311. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Open-set domain adaptation (OSDA) considers that the target domain contains samples from novel categories unobserved in external source domain. Unfortunately, existing OSDA methods always ignore the demand for the information of unseen categories and simply recognize them as &quot;un-known&quot; set without further explanation. This motivates us to understand the unknown categories more specifically by exploring the underlying structures and recovering their interpretable semantic attributes. In this paper, we propose a novel framework to accurately identify the seen categories in target domain, and effectively recover the semantic attributes for unseen categories. Specifically, structure preserving partial alignment is developed to recognize the seen categories through domain-invariant feature learning. Attribute propagation over visual graph is designed to smoothly transit attributes from seen to unseen categories via visual-semantic mapping. Moreover, two new cross-domain benchmarks are constructed to evaluate the proposed framework in the novel and practical challenge. Experimental results on open-set recognition and semantic recovery demonstrate the superiority of the proposed method over other compared baselines.},
  archive   = {C_ICCV},
  author    = {Taotao Jing and Hongfu Liu and Zhengming Ding},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00919},
  pages     = {9302-9311},
  title     = {Towards novel target discovery through open-set domain adaptation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Me-momentum: Extracting hard confident examples from noisily
labeled data. <em>ICCV</em>, 9292–9301. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Examples that are close to the decision boundary—that we term hard examples, are essential to shape accurate classifiers. Extracting confident examples has been widely studied in the community of learning with noisy labels. However, it remains elusive how to extract hard confident examples from the noisy training data. In this paper, we propose a deep learning paradigm to solve this problem, which is built on the memorization effect of deep neural networks that they would first learn simple patterns, i.e., which are defined by these shared by multiple training examples. To extract hard confident examples that contain non-simple patterns and are entangled with the inaccurately labeled examples, we borrow the idea of momentum from physics. Specifically, we alternately update the confident examples and refine the classifier. Note that the extracted confident examples in the previous round can be exploited to learn a better classifier and that the better classifier will help identify better (and hard) confident examples. We call the approach the &quot;Momentum of Memorization&quot; (Me-Momentum). Empirical results on benchmark-simulated and real-world label-noise data illustrate the effectiveness of Me-Momentum for extracting hard confident examples, leading to better classification performance.},
  archive   = {C_ICCV},
  author    = {Yingbin Bai and Tongliang Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00918},
  pages     = {9292-9301},
  title     = {Me-momentum: Extracting hard confident examples from noisily labeled data},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021d). Energy-based open-world uncertainty modeling for confidence
calibration. <em>ICCV</em>, 9282–9291. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Confidence calibration is of great importance to the reliability of decisions made by machine learning systems. However, discriminative classifiers based on deep neural networks are often criticized for producing overconfident predictions that fail to reflect the true correctness likelihood of classification accuracy. We argue that such an inability to model uncertainty is mainly caused by the closed-world nature in softmax: a model trained by the cross-entropy loss will be forced to classify input into one of K pre-defined categories with high probability. To address this problem, we for the first time propose a novel K+1-way softmax formulation, which incorporates the modeling of open-world uncertainty as the extra dimension. To unify the learning of the original K-way classification task and the extra dimension that models uncertainty, we 1) propose a novel energy-based objective function, and moreover, 2) theoretically prove that optimizing such an objective essentially forces the extra dimension to capture the marginal data distribution. Extensive experiments show that our approach, Energy-based Open-World Softmax (EOW-Softmax), is superior to existing state-of-the-art methods in improving confidence calibration.},
  archive   = {C_ICCV},
  author    = {Yezhen Wang and Bo Li and Tong Che and Kaiyang Zhou and Ziwei Liu and Dongsheng Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00917},
  pages     = {9282-9291},
  title     = {Energy-based open-world uncertainty modeling for confidence calibration},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Localized simple multiple kernel k-means. <em>ICCV</em>,
9273–9281. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As a representative of multiple kernel clustering (MKC), simple multiple kernel k-means (SimpleMKKM) is recently put forward to boosting the clustering performance by optimally fusing a group of pre-specified kernel matrices. Despite achieving significant improvement in a variety of applications, we find out that SimpleMKKM could indiscriminately force all sample pairs to be equally aligned with the same ideal similarity. As a result, it does not sufficiently take the variation of samples into consideration, leading to unsatisfying clustering performance. To address these issues, this paper proposes a novel MKC algorithm with a &quot;local&quot; kernel alignment, which only requires that the similarity of a sample to its k-nearest neighbours be aligned with the ideal similarity matrix. Such an alignment helps the clustering algorithm to focus on closer sample pairs that shall stay together and avoids involving unreliable similarity evaluation for farther sample pairs. After that, we theoretically show that the objective of SimpleMKKM is a special case of this local kernel alignment criterion with normalizing each base kernel matrix. Based on this observation, the proposed localized SimpleMKKM can be readily implemented by existing SimpleMKKM package. Moreover, we conduct extensive experiments on several widely used benchmark datasets to evaluate the clustering performance of localized SimpleMKKM. The experimental results have demonstrated that our algorithm consistently outperforms the state-of-the-art ones, verifying the effectiveness of the proposed local kernel alignment criterion. The code of Localized SimpleMKKM is publicly available at: https://github.com/xinwangliu/LocalizedSMKKM.},
  archive   = {C_ICCV},
  author    = {Xinwang Liu and Sihang Zhou and Li Liu and Chang Tang and Siwei Wang and Jiyuan Liu and Yi Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00916},
  pages     = {9273-9281},
  title     = {Localized simple multiple kernel K-means},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A unified objective for novel class discovery.
<em>ICCV</em>, 9264–9272. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we study the problem of Novel Class Discovery (NCD). NCD aims at inferring novel object categories in an unlabeled set by leveraging from prior knowledge of a labeled set containing different, but related classes. Existing approaches tackle this problem by considering multiple objective functions, usually involving specialized loss terms for the labeled and the unlabeled samples respectively, and often requiring auxiliary regularization terms. In this paper we depart from this traditional scheme and introduce a UNified Objective function (UNO) for discovering novel classes, with the explicit purpose of favoring synergy between supervised and unsupervised learning. Using a multi-view self-labeling strategy, we generate pseudo-labels that can be treated homogeneously with ground truth labels. This leads to a single classification objective operating on both known and unknown classes. De-spite its simplicity, UNO outperforms the state of the art by a significant margin on several benchmarks (≈ +10\% on CIFAR-100 and +8\% on ImageNet). The project page is available at : https://ncd-uno.github.io.},
  archive   = {C_ICCV},
  author    = {Enrico Fini and Enver Sangineto and Stéphane Lathuilière and Zhun Zhong and Moin Nabi and Elisa Ricci},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00915},
  pages     = {9264-9272},
  title     = {A unified objective for novel class discovery},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Influence selection for active learning. <em>ICCV</em>,
9254–9263. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The existing active learning methods select the samples by evaluating the sample’s uncertainty or its effect on the diversity of labeled datasets based on different task-specific or model-specific criteria. In this paper, we propose the Influence Selection for Active Learning(ISAL) which selects the unlabeled samples that can provide the most positive influence on model performance. To obtain the influence of the unlabeled sample in the active learning scenario, we design the Untrained Unlabeled sample Influence Calculation(UUIC) to estimate the unlabeled sample’s expected gradient with which we calculate its influence. To prove the effectiveness of UUIC, we provide both theoretical and experimental analyses. Since the UUIC just depends on the model gradients, which can be obtained easily from any neural network, our active learning algorithm is task-agnostic and model-agnostic. ISAL achieves state-of-the-art performance in different active learning settings for different tasks with different datasets. Compared with previous methods, our method decreases the annotation cost at least by 12\%, 13\% and 16\% on CIFAR10, VOC2012 and COCO, respectively.},
  archive   = {C_ICCV},
  author    = {Zhuoming Liu and Hao Ding and Huaping Zhong and Weijia Li and Jifeng Dai and Conghui He},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00914},
  pages     = {9254-9263},
  title     = {Influence selection for active learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semi-supervised single-stage controllable GANs for
conditional fine-grained image generation. <em>ICCV</em>, 9244–9253. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Previous state-of-the-art deep generative models improve fine-grained image generation quality by designing hierarchical model structures and synthesizing images across multiple stages. The learning process is typically performed without any supervision in object categories. To address this issue, while at the same time to alleviate the level of complexity of both model design and training, we propose a Single-Stage Controllable GAN (SSCGAN) for conditional fine-grained image synthesis in a semi-supervised setting. Considering the fact that fine-grained object categories may have subtle distinctions and shared attributes, we take into account three factors of variation for generative modeling: class-independent content, cross-class attributes and class semantics, and associate them with different variables. To ensure disentanglement among the variables, we maximize mutual information between the class-independent variable and synthesized images, map real data to the latent space of a generator to perform consistency regularization of cross-class attributes, and incorporate class semantic-based regularization into a discriminator’s feature space. We show that the proposed approach delivers a single-stage controllable generator and high-fidelity synthesized images of fine-grained categories. SSC-GAN establishes state-of-the-art semi-supervised image synthesis results across multiple fine-grained datasets.},
  archive   = {C_ICCV},
  author    = {Tianyi Chen and Yi Liu and Yunfei Zhang and Si Wu and Yong Xu and Feng Liangbing and Hau San Wong},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00913},
  pages     = {9244-9253},
  title     = {Semi-supervised single-stage controllable GANs for conditional fine-grained image generation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Video pose distillation for few-shot, fine-grained sports
action recognition. <em>ICCV</em>, 9234–9243. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human pose is a useful feature for fine-grained sports action understanding. However, pose estimators are often unreliable when run on sports video due to domain shift and factors such as motion blur and occlusions. This leads to poor accuracy when downstream tasks, such as action recognition, depend on pose. End-to-end learning circumvents pose, but requires more labels to generalize.We introduce Video Pose Distillation (VPD), a weakly-supervised technique to learn features for new video domains, such as individual sports that challenge pose estimation. Under VPD, a student network learns to extract robust pose features from RGB frames in the sports video, such that, whenever pose is considered reliable, the features match the output of a pretrained teacher pose detector. Our strategy retains the best of both pose and end-to-end worlds, exploiting the rich visual patterns in raw video frames, while learning features that agree with the athletes’ pose and motion in the target video domain to avoid over-fitting to patterns unrelated to athletes’ motion.VPD features improve performance on few-shot, fine-grained action recognition, retrieval, and detection tasks in four real-world sports video datasets, without requiring additional ground-truth pose annotations.},
  archive   = {C_ICCV},
  author    = {James Hong and Matthew Fisher and Michaël Gharbi and Kayvon Fatahalian},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00912},
  pages     = {9234-9243},
  title     = {Video pose distillation for few-shot, fine-grained sports action recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Long short view feature decomposition via contrastive video
representation learning. <em>ICCV</em>, 9224–9233. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-supervised video representation methods typically focus on the representation of temporal attributes in videos. However, the role of stationary versus non-stationary attributes is less explored: Stationary features, which remain similar throughout the video, enable the prediction of video-level action classes. Non-stationary features, which represent temporally varying attributes, are more beneficial for downstream tasks involving more fine-grained temporal understanding, such as action segmentation. We argue that a single representation to capture both types of features is sub-optimal, and propose to decompose the representation space into stationary and non-stationary features via contrastive learning from long and short views, i.e. long video sequences and their shorter sub-sequences. Stationary features are shared between the short and long views, while non-stationary features aggregate the short views to match the corresponding long view. To empirically verify our approach, we demonstrate that our stationary features work particularly well on an action recognition downstream task, while our non-stationary features perform better on action segmentation. Furthermore, we analyse the learned representations and find that stationary features capture more temporally stable, static attributes, while non-stationary features encompass more temporally varying ones.},
  archive   = {C_ICCV},
  author    = {Nadine Behrmann and Mohsen Fayyaz and Juergen Gall and Mehdi Noroozi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00911},
  pages     = {9224-9233},
  title     = {Long short view feature decomposition via contrastive video representation learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-VAE: Learning disentangled view-common and
view-peculiar visual representations for multi-view clustering.
<em>ICCV</em>, 9214–9223. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-view clustering, a long-standing and important research problem, focuses on mining complementary information from diverse views. However, existing works often fuse multiple views’ representations or handle clustering in a common feature space, which may result in their entanglement especially for visual representations. To address this issue, we present a novel VAE-based multi-view clustering framework (Multi-VAE) by learning disentangled visual representations. Concretely, we define a view-common variable and multiple view-peculiar variables in the generative model. The prior of view-common variable obeys approximately discrete Gumbel Softmax distribution, which is introduced to extract the common cluster factor of multiple views. Meanwhile, the prior of view-peculiar variable follows continuous Gaussian distribution, which is used to represent each view’s peculiar visual factors. By controlling the mutual information capacity to disentangle the view-common and view-peculiar representations, continuous visual information of multiple views can be separated so that their common discrete cluster information can be effectively mined. Experimental results demonstrate that Multi-VAE enjoys the disentangled and explainable visual representations, while obtaining superior clustering performance compared with state-of-the-art methods.},
  archive   = {C_ICCV},
  author    = {Jie Xu and Yazhou Ren and Huayi Tang and Xiaorong Pu and Xiaofeng Zhu and Ming Zeng and Lifang He},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00910},
  pages     = {9214-9223},
  title     = {Multi-VAE: Learning disentangled view-common and view-peculiar visual representations for multi-view clustering},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph contrastive clustering. <em>ICCV</em>, 9204–9213. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, some contrastive learning methods have been proposed to simultaneously learn representations and clustering assignments, achieving significant improvements. However, these methods do not take the category information and clustering objective into consideration, thus the learned representations are not optimal for clustering and the performance might be limited. Towards this issue, we first propose a novel graph contrastive learning framework, and then apply it to the clustering task, resulting in the Graph Constrastive Clustering (GCC) method. Different from basic contrastive clustering that only assumes an image and its augmentation should share similar representation and clustering assignments, we lift the instance-level consistency to the cluster-level consistency with the assumption that samples in one cluster and their augmentations should all be similar. Specifically, on the one hand, we propose the graph Laplacian based contrastive loss to learn more discriminative and clustering-friendly features. On the other hand, we propose a novel graph-based contrastive learning strategy to learn more compact clustering assignments. Both of them incorporate the latent category information to reduce the intra-cluster variance as well as increase the inter-cluster variance. Experiments on six commonly used datasets demonstrate the superiority of our proposed approach over the state-of-the-art methods. 1},
  archive   = {C_ICCV},
  author    = {Huasong Zhong and Jianlong Wu and Chong Chen and Jianqiang Huang and Minghua Deng and Liqiang Nie and Zhouchen Lin and Xian-Sheng Hua},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00909},
  pages     = {9204-9213},
  title     = {Graph contrastive clustering},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Information-theoretic regularization for multi-source
domain adaptation. <em>ICCV</em>, 9194–9203. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Adversarial learning strategy has demonstrated remarkable performance in dealing with single-source Domain Adaptation (DA) problems, and it has recently been applied to Multi-source DA (MDA) problems. Although most existing MDA strategies rely on a multiple domain discriminator setting, its effect on the latent space representations has been poorly understood. Here we adopt an information-theoretic approach to identify and. resolve the potential adverse effect of the multiple domain discriminators on MDA: disintegration of domain-discriminative information, limited computational scalability, and a large variance in the gradient of the loss during training. We examine the above issues by situating adversarial DA in the context of information regularization. This also provides a theoretical justification for using a single and unified domain discriminator. Based on this idea, we implement a novel neural architecture called a Multi-source Information-regularized Adaptation Networks (MIAN). Large-scale experiments demonstrate that MIAN, despite its structural simplicity, reliably and significantly outperforms other state-of-the-art methods.},
  archive   = {C_ICCV},
  author    = {Geon Yeong Park and Sang Wan Lee},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00908},
  pages     = {9194-9203},
  title     = {Information-theoretic regularization for multi-source domain adaptation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Seeking similarities over differences: Similarity-based
domain alignment for adaptive object detection. <em>ICCV</em>,
9184–9193. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In order to robustly deploy object detectors across a wide range of scenarios, they should be adaptable to shifts in the input distribution without the need to constantly an-notate new data. This has motivated research in Unsupervised Domain Adaptation (UDA) algorithms for detection. UDA methods learn to adapt from labeled source domains to unlabeled target domains, by inducing alignment between detector features from source and target domains. Yet, there is no consensus on what features to align and how to do the alignment. In our work, we propose a framework that generalizes the different components commonly used by UDA methods laying the ground for an in-depth analysis of the UDA design space. Specifically, we propose a novel UDA algorithm, ViSGA, a direct implementation of our framework, that leverages the best design choices and introduces a simple but effective method to aggregate features at instance-level based on visual similarity before inducing group alignment via adversarial training. We show that both similarity-based grouping and adversarial training allows our model to focus on coarsely aligning feature groups, without being forced to match all instances across loosely aligned domains. Finally, we examine the applicability of ViSGA to the setting where labeled data are gathered from different sources. Experiments show that not only our method outperforms previous single-source approaches on Sim2Real and Adverse Weather, but also generalizes well to the multi-source setting.},
  archive   = {C_ICCV},
  author    = {Farzaneh Rezaeianaran and Rakshith Shetty and Rahaf Aljundi and Daniel Olmeda Reino and Shanshan Zhang and Bernt Schiele},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00907},
  pages     = {9184-9193},
  title     = {Seeking similarities over differences: Similarity-based domain alignment for adaptive object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploring robustness of unsupervised domain adaptation in
semantic segmentation. <em>ICCV</em>, 9174–9183. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent studies imply that deep neural networks are vulnerable to adversarial examples, i.e., inputs with a slight but intentional perturbation are incorrectly classified by the network. Such vulnerability makes it risky for some security-related applications (e.g., semantic segmentation in autonomous cars) and triggers tremendous concerns on the model reliability. For the first time, we comprehensively evaluate the robustness of existing UDA methods and propose a robust UDA approach. It is rooted in two observations: i) the robustness of UDA methods in semantic segmentation remains unexplored, which poses a security concern in this field; and ii) although commonly used self-supervision (e.g., rotation and jigsaw) benefits model robustness in classification and recognition tasks, they fail to provide the critical supervision signals that are essential in semantic segmentation. These observations motivate us to propose adversarial self-supervision UDA (or ASSUDA) that maximizes the agreement between clean images and their adversarial examples by a contrastive loss in the output space. Extensive empirical studies on commonly used benchmarks demonstrate that ASSUDA is resistant to adversarial attacks.},
  archive   = {C_ICCV},
  author    = {Jinyu Yang and Chunyuan Li and Weizhi An and Hehuan Ma and Yuzhi Guo and Yu Rong and Peilin Zhao and Junzhou Huang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00906},
  pages     = {9174-9183},
  title     = {Exploring robustness of unsupervised domain adaptation in semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tune it the right way: Unsupervised validation of domain
adaptation via soft neighborhood density. <em>ICCV</em>, 9164–9173. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unsupervised domain adaptation (UDA) methods can dramatically improve generalization on unlabeled target domains. However, optimal hyper-parameter selection is critical to achieving high accuracy and avoiding negative transfer. Supervised hyper-parameter validation is not possible without labeled target data, which raises the question: How can we validate unsupervised adaptation techniques in a realistic way? We first empirically analyze existing criteria and demonstrate that they are not very effective for tuning hyper-parameters. Intuitively, a well-trained source classifier should embed target samples of the same class nearby, forming dense neighborhoods in feature space. Based on this assumption, we propose a novel unsupervised validation criterion that measures the density of soft neighborhoods by computing the entropy of the similarity distribution between points. Our criterion is simpler than competing validation methods, yet more effective; it can tune hyper-parameters and the number of training iterations in both image classification and semantic segmentation models. The code used for the paper will be available at https://github.com/VisionLearningGroup/SND.},
  archive   = {C_ICCV},
  author    = {Kuniaki Saito and Donghyun Kim and Piotr Teterwak and Stan Sclaroff and Trevor Darrell and Kate Saenko},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00905},
  pages     = {9164-9173},
  title     = {Tune it the right way: Unsupervised validation of domain adaptation via soft neighborhood density},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Re-energizing domain discriminator with sample relabeling
for adversarial domain adaptation. <em>ICCV</em>, 9154–9163. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many unsupervised domain adaptation (UDA) methods exploit domain adversarial training to align the features to reduce domain gap, where a feature extractor is trained to fool a domain discriminator in order to have aligned feature distributions. The discrimination capability of the domain classifier w.r.t. the increasingly aligned feature distributions deteriorates as training goes on, thus cannot effectively further drive the training of feature extractor. In this work, we propose an efficient optimization strategy named Re-enforceable Adversarial Domain Adaptation (RADA) which aims to re-energize the domain discriminator during the training by using dynamic domain labels. Particularly, we relabel the well aligned target domain samples as source domain samples on the fly. Such relabeling makes the less separable distributions more separable, and thus leads to a more powerful domain classifier w.r.t. the new data distributions, which in turn further drives feature alignment. Extensive experiments on multiple UDA benchmarks demonstrate the effectiveness and superiority of our RADA.},
  archive   = {C_ICCV},
  author    = {Xin Jin and Cuiling Lan and Wenjun Zeng and Zhibo Chen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00904},
  pages     = {9154-9163},
  title     = {Re-energizing domain discriminator with sample relabeling for adversarial domain adaptation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). A style and semantic memory mechanism for domain
generalization. <em>ICCV</em>, 9144–9153. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mainstream state-of-the-art domain generalization algorithms tend to prioritize the assumption on semantic in-variance across domains. Meanwhile, the inherent intra-domain style invariance is usually underappreciated and put on the shelf. In this paper, we reveal that leveraging intra-domain style invariance is also of pivotal importance in improving the efficiency of domain generalization. We verify that it is critical for the network to be informative on what domain features are invariant and shared among in-stances, so that the network sharpens its understanding and improves its semantic discriminative ability. Correspondingly, we also propose a novel “jury” mechanism, which is particularly effective in learning useful semantic feature commonalities among domains. Our complete model called STEAM can be interpreted as a novel probabilistic graphical model, for which the implementation requires convenient constructions of two kinds of memory banks: semantic feature bank and style feature bank. Empirical results show that our proposed framework surpasses the state-of-the-art methods by clear margins.},
  archive   = {C_ICCV},
  author    = {Yang Chen and Yu Wang and Yingwei Pan and Ting Yao and Xinmei Tian and Tao Mei},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00903},
  pages     = {9144-9153},
  title     = {A style and semantic memory mechanism for domain generalization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The pursuit of knowledge: Discovering and localizing novel
categories using dual memory. <em>ICCV</em>, 9133–9143. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We tackle object category discovery, which is the problem of discovering and localizing novel objects in a large unlabeled dataset. While existing methods show results on datasets with less cluttered scenes and fewer object in-stances per image, we present our results on the challenging COCO dataset. Moreover, we argue that, rather than discovering new categories from scratch, discovery algorithms can benefit from identifying what is already known and focusing their attention on the unknown. We propose a method that exploits prior knowledge about certain object types to discover new categories by leveraging two memory modules, namely Working and Semantic memory. We show the performance of our detector on the COCO minival dataset to demonstrate its in-the-wild capabilities.},
  archive   = {C_ICCV},
  author    = {Sai Saketh Rambhatla and Rama Chellappa and Abhinav Shrivastava},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00902},
  pages     = {9133-9143},
  title     = {The pursuit of knowledge: Discovering and localizing novel categories using dual memory},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust object detection via instance-level temporal cycle
confusion. <em>ICCV</em>, 9123–9132. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Building reliable object detectors that are robust to domain shifts, such as various changes in context, viewpoint, and object appearances, is critical for real-world applications. In this work, we study the effectiveness of auxiliary self-supervised tasks to improve the out-of-distribution generalization of object detectors. Inspired by the principle of maximum entropy, we introduce a novel self-supervised task, instance-level temporal cycle confusion (CycConf), which operates on the region features of the object detectors. For each object, the task is to find the most different object proposals in the adjacent frame in a video and then cycle back to itself for self-supervision. CycConf encourages the object detector to explore invariant structures across instances under various motions, which leads to improved model robustness in unseen domains at test time. We observe consistent out-of-domain performance improvements when training object detectors in tandem with self-supervised tasks on various do-main adaptation benchmarks with static images (Cityscapes, Foggy Cityscapes, Sim10K) and large-scale video datasets (BDD100K and Waymo open data) 1 .},
  archive   = {C_ICCV},
  author    = {Xin Wang and Thomas E. Huang and Benlin Liu and Fisher Yu and Xiaolong Wang and Joseph E. Gonzalez and Trevor Darrell},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00901},
  pages     = {9123-9132},
  title     = {Robust object detection via instance-level temporal cycle confusion},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Knowledge mining and transferring for domain adaptive object
detection. <em>ICCV</em>, 9113–9122. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the thriving of deep learning, CNN-based object detectors have made great progress in the past decade. However, the domain gap between training and testing data leads to a prominent performance degradation and thus hinders their application in the real world. To alleviate this problem, Knowledge Transfer Network (KTNet) is proposed as a new paradigm for domain adaption. Specifically, KT-Net is constructed on a base detector with intrinsic knowledge mining and relational knowledge constraints. First, we design a foreground/background classifier shared by source domain and target domain to extract the common attribute knowledge of objects in different scenarios. Second, we model the relational knowledge graph and explicitly constrain the consistency of category correlation under source domain, target domain, as well as cross-domain conditions. As a result, the detector is guided to learn object-related and domain-independent representation. Extensive experiments and visualizations confirm that transferring object-specific knowledge can yield notable performance gains. The proposed KTNet achieves state-of-the-art results on three cross-domain detection benchmarks.},
  archive   = {C_ICCV},
  author    = {Kun Tian and Chenghao Zhang and Ying Wang and Shiming Xiang and Chunhong Pan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00900},
  pages     = {9113-9122},
  title     = {Knowledge mining and transferring for domain adaptive object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CDS: Cross-domain self-supervised pre-training.
<em>ICCV</em>, 9103–9112. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a two-stage pre-training approach that improves the generalization ability of standard single-domain pre-training. While standard pre-training on a single large dataset (such as ImageNet) can provide a good initial representation for transfer learning tasks, this approach may result in biased representations that impact the success of learning with new multi-domain data (e.g., different artistic styles) via methods like domain adaptation. We propose a novel pre-training approach called Cross-Domain Self-supervision (CDS), which directly employs unlabeled multi-domain data for downstream domain transfer tasks. Our approach uses self-supervision not only within a single domain but also across domains. In-domain instance discrimination is used to learn discriminative features on new data in a domain-adaptive manner, while cross-domain matching is used to learn domain-invariant features. We apply our method as a second pre-training step (after ImageNet pre-training), resulting in a significant target accuracy boost to diverse domain transfer tasks compared to standard one-stage pre-training.},
  archive   = {C_ICCV},
  author    = {Donghyun Kim and Kuniaki Saito and Tae-Hyun Oh and Bryan A. Plummer and Stan Sclaroff and Kate Saenko},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00899},
  pages     = {9103-9112},
  title     = {CDS: Cross-domain self-supervised pre-training},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-anchor active domain adaptation for semantic
segmentation. <em>ICCV</em>, 9092–9102. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unsupervised domain adaption has proven to be an effective approach for alleviating the intensive workload of manual annotation by aligning the synthetic source-domain data and the real-world target-domain samples. Unfortunately, mapping the target-domain distribution to the source-domain unconditionally may distort the essential structural information of the target-domain data. To this end, we firstly propose to introduce a novel multi-anchor based active learning strategy to assist domain adaptation regarding the semantic segmentation task. By innovatively adopting multiple anchors instead of a single centroid, the source domain can be better characterized as a multimodal distribution, thus more representative and complimentary samples are selected from the target domain. With little workload to manually annotate these active samples, the distortion of the target-domain distribution can be effectively alleviated, resulting in a large performance gain. The multi-anchor strategy is additionally employed to model the target-distribution. By regularizing the latent representation of the target samples compact around multiple anchors through a novel soft alignment loss, more precise segmentation can be achieved. Extensive experiments are conducted on public datasets to demonstrate that the proposed approach outperforms state-of-the-art methods significantly, along with thorough ablation study to verify the effectiveness of each component. The code will be released soon at https://github.com/munanning/MADA.},
  archive   = {C_ICCV},
  author    = {Munan Ning and Donghuan Lu and Dong Wei and Cheng Bian and Chenglang Yuan and Shuang Yu and Kai Ma and Yefeng Zheng},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00898},
  pages     = {9092-9102},
  title     = {Multi-anchor active domain adaptation for semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semantic concentration for domain adaptation. <em>ICCV</em>,
9082–9091. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Domain adaptation (DA) paves the way for label annotation and dataset bias issues by the knowledge transfer from a label-rich source domain to a related but unlabeled target domain. A mainstream of DA methods is to align the feature distributions of the two domains. However, the majority of them focus on the entire image features where irrelevant semantic information, e.g., the messy background, is inevitably embedded. Enforcing feature alignments in such case will negatively influence the correct matching of objects and consequently lead to the semantically negative transfer due to the confusion of irrelevant semantics. To tackle this issue, we propose Semantic Concentration for Domain Adaptation (SCDA), which encourages the model to concentrate on the most principal features via the pair-wise adversarial alignment of prediction distributions. Specifically, we train the classifier to class-wisely maximize the prediction distribution divergence of each sample pair, which enables the model to find the region with large differences among the same class of samples. Meanwhile, the feature extractor attempts to minimize that discrepancy, which suppresses the features of dissimilar regions among the same class of samples and accentuates the features of principal parts. As a general method, SCDA can be easily integrated into various DA methods as a regularizer to further boost their performance. Extensive experiments on the cross-domain benchmarks show the efficacy of SCDA.},
  archive   = {C_ICCV},
  author    = {Shuang Li and Mixue Xie and Fangrui Lv and Chi Harold Liu and Jian Liang and Chen Qin and Wei Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00897},
  pages     = {9082-9091},
  title     = {Semantic concentration for domain adaptation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Uncertainty-aware pseudo label refinery for domain adaptive
semantic segmentation. <em>ICCV</em>, 9072–9081. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unsupervised domain adaptation for semantic segmentation aims to assign the pixel-level labels for unlabeled target domain by transferring knowledge from the labeled source domain. A typical self-supervised learning approach generates pseudo labels from the source model and then re-trains the model to fit the target distribution. However, it suffers from noisy pseudo labels due to the existence of domain shift. Related works alleviate this problem by selecting high-confidence predictions, but uncertain classes with low confidence scores have rarely been considered. This informative uncertainty is essential to enhance feature representation and align source and target domains. In this paper, we propose a novel uncertainty-aware pseudo label refinery framework considering two crucial factors simultaneously. First, we progressively enhance the feature alignment model via the target-guided uncertainty rectifying framework. Second, we provide an uncertainty-aware pseudo label assignment strategy without any manually de-signed threshold to reduce the noisy labels. Extensive experiments demonstrate the effectiveness of our proposed approach and achieve state-of-the-art performance on two standard synthetic-2-real tasks.},
  archive   = {C_ICCV},
  author    = {Yuxi Wang and Junran Peng and Zhaoxiang Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00896},
  pages     = {9072-9081},
  title     = {Uncertainty-aware pseudo label refinery for domain adaptive semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dual path learning for domain adaptation of semantic
segmentation. <em>ICCV</em>, 9062–9071. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Domain adaptation for semantic segmentation enables to alleviate the need for large-scale pixel-wise annotations. Recently, self-supervised learning (SSL) with a combination of image-to-image translation shows great effectiveness in adaptive segmentation. The most common practice is to perform SSL along with image translation to well align a single domain (the source or target). However, in this single-domain paradigm, unavoidable visual inconsistency raised by image translation may affect subsequent learning. In this paper, based on the observation that domain adaptation frameworks performed in the source and target domain are almost complementary in terms of image translation and SSL, we propose a novel dual path learning (DPL) framework to alleviate visual inconsistency. Concretely, DPL contains two complementary and interactive single-domain adaptation pipelines aligned in source and target domain respectively. The inference of DPL is extremely simple, only one segmentation model in the target domain is employed. Novel technologies such as dual path image translation and dual path adaptive segmentation are proposed to make two paths promote each other in an interactive manner. Experiments on GTA5→Cityscapes and SYNTHIA→Cityscapes scenarios demonstrate the superiority of our DPL model over the state-of-the-art methods. The code and models are available at: https://github.com/royee182/DPL.},
  archive   = {C_ICCV},
  author    = {Yiting Cheng and Fangyun Wei and Jianmin Bao and Dong Chen and Fang Wen and Wenqiang Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00895},
  pages     = {9062-9071},
  title     = {Dual path learning for domain adaptation of semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-target adversarial frameworks for domain adaptation in
semantic segmentation. <em>ICCV</em>, 9052–9061. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we address the task of unsupervised domain adaptation (UDA) for semantic segmentation in presence of multiple target domains: The objective is to train a single model that can handle all these domains at test time. Such a multi-target adaptation is crucial for a variety of scenarios that real-world autonomous systems must handle. It is a challenging setup since one faces not only the domain gap between the labeled source set and the un-labeled target set, but also the distribution shifts existing within the latter among the different target domains. To this end, we introduce two adversarial frameworks: (i) multi-discriminator, which explicitly aligns each target domain to its counterparts, and (ii) multi-target knowledge transfer, which learns a target-agnostic model thanks to a multi-teacher/single-student distillation mechanism. The evaluation is done on four newly-proposed multi-target bench-marks for UDA in semantic segmentation. In all tested scenarios, our approaches consistently outperform baselines, setting competitive standards for the novel task.},
  archive   = {C_ICCV},
  author    = {Antoine Saporta and Tuan-Hung Vu and Matthieu Cord and Patrick Pérez},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00894},
  pages     = {9052-9061},
  title     = {Multi-target adversarial frameworks for domain adaptation in semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Meta-baseline: Exploring simple meta-learning for few-shot
learning. <em>ICCV</em>, 9042–9051. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Meta-learning has been the most common framework for few-shot learning in recent years. It learns the model from collections of few-shot classification tasks, which is believed to have a key advantage of making the training objective consistent with the testing objective. However, some recent works report that by training for whole-classification, i.e. classification on the whole label-set, it can get comparable or even better embedding than many meta-learning algorithms. The edge between these two lines of works has yet been underexplored, and the effectiveness of meta-learning in few-shot learning remains unclear. In this paper, we explore a simple process: meta-learning over a whole-classification pre-trained model on its evaluation metric. We observe this simple method achieves competitive performance to state-of-the-art methods on standard bench-marks. Our further analysis shed some light on understanding the trade-offs between the meta-learning objective and the whole-classification objective in few-shot learning. Our code is available at https://github.com/yinboc/few-shot-meta-baseline.},
  archive   = {C_ICCV},
  author    = {Yinbo Chen and Zhuang Liu and Huijuan Xu and Trevor Darrell and Xiaolong Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00893},
  pages     = {9042-9051},
  title     = {Meta-baseline: Exploring simple meta-learning for few-shot learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Coarsely-labeled data for better few-shot transfer.
<em>ICCV</em>, 9032–9041. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Few-shot learning is based on the premise that labels are expensive, especially when they are fine-grained and require expertise. But coarse labels might be easy to acquire and thus abundant. We present a representation learning approach - PAS that allows few-shot learners to leverage coarsely-labeled data available before evaluation. Inspired by self-training, we label the additional data using a teacher trained on the base dataset and filter the teacher’s prediction based on the coarse labels; a new student representation is then trained on the base dataset and the pseudo-labeled dataset. PAS is able to produce a representation that consistently and significantly outperforms the baselines in 3 different datasets. Code is available at https://github.com/cpphoo/PAS},
  archive   = {C_ICCV},
  author    = {Cheng Perng Phoo and Bharath Hariharan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00892},
  pages     = {9032-9041},
  title     = {Coarsely-labeled data for better few-shot transfer},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mixture-based feature space learning for few-shot image
classification. <em>ICCV</em>, 9021–9031. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce Mixture-based Feature Space Learning (MixtFSL) for obtaining a rich and robust feature representation in the context of few-shot image classification. Previous works have proposed to model each base class either with a single point or with a mixture model by relying on offline clustering algorithms. In contrast, we propose to model base classes with mixture models by simultaneously training the feature extractor and learning the mixture model parameters in an online manner. This results in a richer and more discriminative feature space which can be employed to classify novel examples from very few samples. Two main stages are proposed to train the MixtFSL model. First, the multimodal mixtures for each base class and the feature extractor parameters are learned using a combination of two loss functions. Second, the resulting network and mixture models are progressively refined through a leader-follower learning procedure, which uses the current estimate as a &quot;target&quot; network. This target network is used to make a consistent assignment of instances to mixture components, which increases performance and stabilizes training. The effectiveness of our end-to-end feature space learning approach is demonstrated with extensive experiments on four standard datasets and four backbones. Notably, we demon-strate that when we combine our robust representation with recent alignment-based approaches, we achieve new state-of-the-art results in the inductive setting, with an absolute accuracy for 5-shot classification of 82.45\% on miniImageNet, 88.20\% with tieredImageNet, and 60.70\% in FC100 using the ResNet-12 backbone.},
  archive   = {C_ICCV},
  author    = {Arman Afrasiyabi and Jean-François Lalonde and Christian Gagné},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00891},
  pages     = {9021-9031},
  title     = {Mixture-based feature space learning for few-shot image classification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the importance of distractors for few-shot
classification. <em>ICCV</em>, 9010–9020. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Few-shot classification aims at classifying categories of a novel task by learning from just a few (typically, 1 to 5) labelled examples. An effective approach to few-shot classification involves a prior model trained on a large-sample base domain, which is then finetuned over the novel few-shot task to yield generalizable representations. However, task-specific finetuning is prone to overfitting due to the lack of enough training examples. To alleviate this issue, we propose a new finetuning approach based on contrastive learning that reuses unlabelled examples from the base do-main in the form of distractors. Unlike the nature of unlabelled data used in prior works, distractors belong to classes that do not overlap with the novel categories. We demonstrate for the first time that inclusion of such distractors can significantly boost few-shot generalization. Our technical novelty includes a stochastic pairing of examples sharing the same category in the few-shot task and a weighting term that controls the relative influence of task-specific negatives and distractors. An important aspect of our finetuning objective is that it is agnostic to distractor labels and hence applicable to various base domain settings. More precisely, compared to state-of-the-art approaches, our method shows accuracy gains of up to 12\% in cross-domain and up to 5\% in unsupervised prior-learning settings. Our code is available at https://github.com/quantacode/Contrastive-Finetuning.git},
  archive   = {C_ICCV},
  author    = {Rajshekhar Das and Yu-Xiong Wang and José M. F. Moura},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00890},
  pages     = {9010-9020},
  title     = {On the importance of distractors for few-shot classification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generalized and incremental few-shot learning by explicit
learning and calibration without forgetting. <em>ICCV</em>, 9000–9009.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Both generalized and incremental few-shot learning have to deal with three major challenges: learning novel classes from only few samples per class, preventing catastrophic forgetting of base classes, and classifier calibration across novel and base classes. In this work we propose a three-stage framework that allows to explicitly and effectively address these challenges. While the first phase learns base classes with many samples, the second phase learns a calibrated classifier for novel classes from few samples while also preventing catastrophic forgetting. In the final phase, calibration is achieved across all classes. We evaluate the proposed framework on four challenging benchmark datasets for image and video few-shot classification and obtain state-of-the-art results for both generalized and incremental few shot learning.},
  archive   = {C_ICCV},
  author    = {Anna Kukleva and Hilde Kuehne and Bernt Schiele},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00889},
  pages     = {9000-9009},
  title     = {Generalized and incremental few-shot learning by explicit learning and calibration without forgetting},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive adversarial network for source-free domain
adaptation. <em>ICCV</em>, 8990–8999. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unsupervised Domain Adaptation solves knowledge transfer along with the coexistence of well-annotated source domain and unlabeled target instances. However, the source domain in many practical applications is not always accessible due to data privacy or the insufficient memory storage for small devices. This scenario defined as Source-free Domain Adaptation only allows accessing the well-trained source model for target learning. To address the challenge of source data unavailability, we develop an Adaptive Adversarial Network (A 2 Net) including three components. Specifically, the first one named Adaptive Adversarial Inference seeks a target-specific classifier to advance the recognition of samples which the provided source-specific classifier difficultly identifies. Then, the Contrastive Category-wise Matching module exploits the positive relation of every two target images to enforce the compactness of subspace for each category. Thirdly, Self-Supervised Rotation facilitates the model to learn additional semantics from target images by themselves. Extensive experiments on the popular cross-domain benchmarks verify the effectiveness of our proposed model on solving adaptation task without any source data.},
  archive   = {C_ICCV},
  author    = {Haifeng Xia and Handong Zhao and Zhengming Ding},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00888},
  pages     = {8990-8999},
  title     = {Adaptive adversarial network for source-free domain adaptation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). OVANet: One-vs-all network for universal domain adaptation.
<em>ICCV</em>, 8980–8989. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Universal Domain Adaptation (UNDA) aims to handle both domain-shift and category-shift between two datasets, where the main challenge is to transfer knowledge while rejecting &quot;unknown&quot; classes which are absent in the labeled source data but present in the unlabeled target data. Existing methods manually set a threshold to reject &quot;unknown&quot; samples based on validation or a pre-defined ratio of &quot;unknown&quot; samples, but this strategy is not practical. In this paper, we propose a method to learn the thresh-old using source samples and to adapt it to the target domain. Our idea is that a minimum inter-class distance in the source domain should be a good threshold to decide between &quot;known&quot; or &quot;unknown&quot; in the target. To learn the inter- and intra-class distance, we propose to train a one-vs-all classifier for each class using labeled source data. Then, we adapt the open-set classifier to the target domain by minimizing class entropy. The resulting framework is the simplest of all baselines of UNDA and is insensitive to the value of a hyper-parameter, yet outperforms baselines with a large margin. Implementation is available at https://github.com/VisionLearningGroup/OVANet.},
  archive   = {C_ICCV},
  author    = {Kuniaki Saito and Kate Saenko},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00887},
  pages     = {8980-8989},
  title     = {OVANet: One-vs-all network for universal domain adaptation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RDA: Robust domain adaptation via fourier adversarial
attacking. <em>ICCV</em>, 8968–8979. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unsupervised domain adaptation (UDA) involves a supervised loss in a labeled source domain and an unsupervised loss in an unlabeled target domain, which often faces more severe overfitting (than classical supervised learning) as the supervised source loss has clear domain gap and the unsupervised target loss is often noisy due to the lack of annotations. This paper presents RDA, a robust domain adaptation technique that introduces adversarial attacking to mitigate overfitting in UDA. We achieve robust domain adaptation by a novel Fourier adversarial attacking (FAA) method that allows large magnitude of perturbation noises but has minimal modification of image semantics, the former is critical to the effectiveness of its generated adversarial samples due to the existence of ‘domain gaps’. Specifically, FAA decomposes images into multiple frequency components (FCs) and generates adversarial samples by just perturbating certain FCs that capture little semantic information. With FAA-generated samples, the training can continue the ‘random walk’ and drift into an area with a flat loss landscape, leading to more robust domain adaptation. Extensive experiments over multiple domain adaptation tasks show that RDA can work with different computer vision tasks with superior performance.},
  archive   = {C_ICCV},
  author    = {Jiaxing Huang and Dayan Guan and Aoran Xiao and Shijian Lu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00886},
  pages     = {8968-8979},
  title     = {RDA: Robust domain adaptation via fourier adversarial attacking},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Generalized source-free domain adaptation. <em>ICCV</em>,
8958–8967. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Domain adaptation (DA) aims to transfer the knowledge learned from a source domain to an unlabeled target domain. Some recent works tackle source-free domain adaptation (SFDA) where only a source pre-trained model is available for adaptation to the target domain. However, those methods do not consider keeping source performance which is of high practical value in real world applications. In this paper, we propose a new domain adaptation paradigm called Generalized Source-free Domain Adaptation (G-SFDA), where the learned model needs to perform well on both the target and source domains, with only access to current unlabeled target data during adaptation. First, we propose local structure clustering (LSC), aiming to cluster the target features with its semantically similar neighbors, which successfully adapts the model to the target domain in the absence of source data. Second, we propose sparse domain attention (SDA), it produces a binary domain specific attention to activate different feature channels for different domains, meanwhile the domain attention will be utilized to regularize the gradient during adaptation to keep source information. In the experiments, for target performance our method is on par with or better than existing DA and SFDA methods, specifically it achieves state-of-the-art performance (85.4\%) on VisDA, and our method works well for all domains after adapting to single or multiple target domains. Code is available in https://github.com/Albert0147/G-SFDA.},
  archive   = {C_ICCV},
  author    = {Shiqi Yang and Yaxing Wang and Joost van de Weijer and Luis Herranz and Shangling Jui},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00885},
  pages     = {8958-8967},
  title     = {Generalized source-free domain adaptation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Active universal domain adaptation. <em>ICCV</em>,
8948–8957. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most unsupervised domain adaptation methods rely on rich prior knowledge about the source-target label set relationship, and they cannot recognize categories beyond the source classes, which limits their applicability in practical scenarios. This paper proposes a new paradigm for unsupervised domain adaptation, termed as Active Universal Domain Adaptation (AUDA), which removes all label set assumptions and aims for not only recognizing target samples from source classes but also inferring those from target-private classes by using active learning to annotate a small budget of target data. For AUDA, it is challenging to jointly adapt the model to the target domain and select informative target samples for annotations under a large domain gap and significant semantic shift. To address the problems, we propose an Active Universal Adaptation Network (AUAN). Specifically, we first introduce Adversarial and Diverse Curriculum Learning (ADCL), which progressively aligns source and target domains to classify whether target samples are from source classes. Then, we propose a Clustering Non-transferable Gradient Embedding (CNTGE) strategy, which utilizes the clues of transferability, diversity, and uncertainty to annotate target informative sample, making it possible to infer labels for target samples of target-private classes. Finally, we propose to jointly train ADCL and CNTGE with target supervision to promote domain adaptation and target-private class recognition. Extensive experiments demonstrate that the proposed AUDA model equipped with ADCL and CNTGE achieves significant results on four popular benchmarks.},
  archive   = {C_ICCV},
  author    = {Xinhong Ma and Junyu Gao and Changsheng Xu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00884},
  pages     = {8948-8957},
  title     = {Active universal domain adaptation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Confidence calibration for domain generalization under
covariate shift. <em>ICCV</em>, 8938–8947. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing calibration algorithms address the problem of covariate shift via unsupervised domain adaptation. However, these methods suffer from the following limitations: 1) they require unlabeled data from the target domain, which may not be available at the stage of calibration in real-world applications and 2) their performance depends heavily on the disparity between the distributions of the source and target domains. To address these two limitations, we present novel calibration solutions via domain generalization. Our core idea is to leverage multiple calibration domains to reduce the effective distribution disparity between the target and calibration domains for improved calibration transfer without needing any data from the target domain. We provide theoretical justification and empirical experimental results to demonstrate the effectiveness of our proposed algorithms. Compared against state-of-the-art calibration methods designed for domain adaptation, we observe a decrease of 8.86 percentage points in expected calibration error or, equivalently, an increase of 35 percentage points in improvement ratio for multi-class classification on the Office-Home dataset.},
  archive   = {C_ICCV},
  author    = {Yunye Gong and Xiao Lin and Yi Yao and Thomas G. Dietterich and Ajay Divakaran and Melinda Gervasio},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00883},
  pages     = {8938-8947},
  title     = {Confidence calibration for domain generalization under covariate shift},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Meta learning on a sequence of imbalanced domains with
difficulty awareness. <em>ICCV</em>, 8927–8937. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recognizing new objects by learning from a few labeled examples in an evolving environment is crucial to obtain excellent generalization ability for real-world machine learning systems. A typical setting across current meta learning algorithms assumes a stationary task distribution during meta training. In this paper, we explore a more practical and challenging setting where task distribution changes over time with domain shift. Particularly, we consider realistic scenarios where task distribution is highly imbalanced with domain labels unavailable in nature. We propose a kernel-based method for domain change detection and a difficulty-aware memory management mechanism that jointly considers the imbalanced domain size and domain importance to learn across domains continuously. Furthermore, we introduce an efficient adaptive task sampling method during meta training, which significantly reduces task gradient variance with theoretical guarantees. Finally, we propose a challenging benchmark with imbalanced domain sequences and varied domain difficulty. We have performed extensive evaluations on the proposed benchmark, demonstrating the effectiveness of our method.},
  archive   = {C_ICCV},
  author    = {Zhenyi Wang and Tiehang Duan and Le Fang and Qiuling Suo and Mingchen Gao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00882},
  pages     = {8927-8937},
  title     = {Meta learning on a sequence of imbalanced domains with difficulty awareness},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gradient distribution alignment certificates better
adversarial domain adaptation. <em>ICCV</em>, 8917–8926. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The latest heuristic for handling the domain shift in un-supervised domain adaptation tasks is to reduce the data distribution discrepancy using adversarial learning. Recent studies improve the conventional adversarial domain adaptation methods with discriminative information by integrating the classifier’s outputs into distribution divergence measurement. However, they still suffer from the equilibrium problem of adversarial learning in which even if the discriminator is fully confused, sufficient similarity between two distributions cannot be guaranteed. To overcome this problem, we propose a novel approach named feature gradient distribution alignment (FGDA) 1 . We demonstrate the rationale of our method both theoretically and empirically. In particular, we show that the distribution discrepancy can be reduced by constraining feature gradients of two domains to have similar distributions. Meanwhile, our method enjoys a theoretical guarantee that a tighter error upper bound for target samples can be obtained than that of conventional adversarial domain adaptation methods. By integrating the proposed method with existing adversarial domain adaptation models, we achieve state-of-the-art performance on two real-world benchmark datasets.},
  archive   = {C_ICCV},
  author    = {Zhiqiang Gao and Shufei Zhang and Kaizhu Huang and Qiufeng Wang and Chaoliang Zhong},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00881},
  pages     = {8917-8926},
  title     = {Gradient distribution alignment certificates better adversarial domain adaptation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Contrastive coding for active learning under class
distribution mismatch. <em>ICCV</em>, 8907–8916. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Active learning (AL) is successful based on the assumption that labeled and unlabeled data are obtained from the same class distribution. However, its performance deteriorates under class distribution mismatch, wherein the un-labeled data contain many samples out of the class distribution of labeled data. To effectively handle the problems under class distribution mismatch, we propose a contrastive coding based AL framework named CCAL. Unlike the existing AL methods that focus on selecting the most informative samples for annotating, CCAL extracts both semantic and distinctive features by contrastive learning and combines them in a query strategy to choose the most informative un-labeled samples with matched categories. Theoretically, we prove that the AL error of CCAL has a tight upper bound. Experimentally, we evaluate its performance on CIFAR10, CIFAR100, and an artificial cross-dataset that consists of five datasets; consequently, CCAL achieves state-of-the-art performance by a large margin with remarkably lower annotation cost. To the best of our knowledge, CCAL is the first work related to AL for class distribution mismatch.},
  archive   = {C_ICCV},
  author    = {Pan Du and Suyun Zhao and Hui Chen and Shuwen Chai and Hong Chen and Cuiping Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00880},
  pages     = {8907-8916},
  title     = {Contrastive coding for active learning under class distribution mismatch},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weak adaptation learning: Addressing cross-domain data
insufficiency with weak annotator. <em>ICCV</em>, 8897–8906. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Data quantity and quality are crucial factors for data-driven learning methods. In some target problem domains, there are not many data samples available, which could significantly hinder the learning process. While data from similar domains may be leveraged to help through domain adaptation, obtaining high-quality labeled data for those source domains themselves could be difficult or costly. To address such challenges on data insufficiency for classification problem in a target domain, we propose a weak adaptation learning (WAL) approach that leverages unlabeled data from a similar source domain, a low-cost weak annotator that produces labels based on task-specific heuristics, labeling rules, or other methods (albeit with inaccuracy), and a small amount of labeled data in the target domain. Our approach first conducts a theoretical analysis on the error bound of the trained classifier with respect to the data quantity and the performance of the weak annotator, and then introduces a multi-stage weak adaptation learning method to learn an accurate classifier by lowering the error bound. Our experiments demonstrate the effectiveness of our approach in learning an accurate classifier with limited labeled data in the target domain and unlabeled data in the source domain.},
  archive   = {C_ICCV},
  author    = {Shichao Xu and Lixu Wang and Yixuan Wang and Qi Zhu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00879},
  pages     = {8897-8906},
  title     = {Weak adaptation learning: Addressing cross-domain data insufficiency with weak annotator},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Deep co-training with task decomposition for
semi-supervised domain adaptation. <em>ICCV</em>, 8886–8896. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Semi-supervised domain adaptation (SSDA) aims to adapt models trained from a labeled source domain to a different but related target domain, from which unlabeled data and a small set of labeled data are provided. Current methods that treat source and target supervision without distinction overlook their inherent discrepancy, resulting in a source-dominated model that has not effectively use the target supervision. In this paper, we argue that the labeled target data needs to be distinguished for effective SSDA, and propose to explicitly decompose the SSDA task into two sub-tasks: a semi-supervised learning (SSL) task in the target domain and an unsupervised domain adaptation (UDA) task across domains. By doing so, the two sub-tasks can better leverage the corresponding supervision and thus yield very different classifiers. To integrate the strengths of the two classifiers, we apply the well established co-training framework, in which the two classifiers exchange their high confident predictions to iteratively &quot;teach each other&quot; so that both classifiers can excel in the target domain. We call our approach Deep Co-training with Task decomposition (DeCoTa). DeCoTa requires no adversarial training and is easy to implement. Moreover, DeCoTa is well founded on the theoretical condition of when co-training would succeed. As a result, DeCoTa achieves state-of-the-art results on several SSDA datasets, outperforming the prior art by a notable 4\% margin on DomainNet. Code is available at https://github.com/LoyoYang/DeCoTa.},
  archive   = {C_ICCV},
  author    = {Luyu Yang and Yan Wang and Mingfei Gao and Abhinav Shrivastava and Kilian Q. Weinberger and Wei-Lun Chao and Ser-Nam Lim},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00878},
  pages     = {8886-8896},
  title     = {Deep co-training with task decomposition for semi-supervised domain adaptation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Collaborative learning with disentangled features for
zero-shot domain adaptation. <em>ICCV</em>, 8876–8885. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Typical domain adaptation techniques aim to transfer the knowledge learned from a label-rich source domain to a label-scarce target domain in the same label space. However, it is often hard to get even the unlabeled target domain data of a task of interest. In such a case, we can capture the domain shift between the source domain and target domain from an unseen task and transfer it to the task of interest, which is known as zero-shot domain adaptation (ZSDA). Most of existing state-of-the-art methods for ZSDA attempted to generate target domain data. However, training such generative models causes significant computational overhead and is hardly optimized. In this paper, we propose a novel ZSDA method that learns a task-agnostic domain shift by collaborative training of domain-invariant semantic features and task-invariant domain features via adversarial learning. Meanwhile, the spatial attention map is learned from disentangled feature representations to selectively emphasize the domain-specific salient parts of the domain-invariant features. Experimental results show that our ZSDA method achieves state-of-the-art performance on several benchmarks.},
  archive   = {C_ICCV},
  author    = {Won Young Jhoo and Jae-Pil Heo},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00877},
  pages     = {8876-8885},
  title     = {Collaborative learning with disentangled features for zero-shot domain adaptation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). A simple feature augmentation for domain generalization.
<em>ICCV</em>, 8866–8875. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The topical domain generalization (DG) problem asks trained models to perform well on an unseen target domain with different data statistics from the source training domains. In computer vision, data augmentation has proven one of the most effective ways of better exploiting the source data to improve domain generalization. However, existing approaches primarily rely on image-space data augmentation, which requires careful augmentation design, and provides limited diversity of augmented data. We argue that feature augmentation is a more promising direction for DG. We find that an extremely simple technique of perturbing the feature embedding with Gaussian noise during training leads to a classifier with domain-generalization performance comparable to existing state of the art. To model more meaningful statistics reflective of cross-domain variability, we further estimate the full class-conditional feature covariance matrix iteratively during training. Subsequent joint stochastic feature augmentation provides an effective domain randomization method, perturbing features in the directions of intra-class/cross-domain variability. We verify our proposed method on three standard domain generalization benchmarks, Digit-DG, VLCS and PACS, and show it is outperforming or comparable to the state of the art in all setups, together with experimental analysis to illustrate how our method works towards training a robust generalisable model.},
  archive   = {C_ICCV},
  author    = {Pan Li and Da Li and Wei Li and Shaogang Gong and Yanwei Fu and Timothy M. Hospedales},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00876},
  pages     = {8866-8875},
  title     = {A simple feature augmentation for domain generalization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MDALU: Multi-source domain adaptation and label unification
with partial datasets. <em>ICCV</em>, 8856–8865. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One challenge of object recognition is to generalize to new domains, to more classes and/or to new modalities. This necessitates methods to combine and reuse existing datasets that may belong to different domains, have partial annotations, and/or have different data modalities. This paper formulates this as a multi-source domain adaptation and label unification problem, and proposes a novel method for it. Our method consists of a partially-supervised adaptation stage and a fully-supervised adaptation stage. In the former, partial knowledge is transferred from multiple source domains to the target domain and fused therein. Negative transfer between unmatching label spaces is mitigated via three new modules: domain attention, uncertainty maximization and attention-guided adversarial alignment. In the latter, knowledge is transferred in the unified label space after a label completion process with pseudolabels. Extensive experiments on three different tasks - image classification, 2D semantic image segmentation, and joint 2D-3D semantic segmentation - show that our method outperforms all competing methods significantly.},
  archive   = {C_ICCV},
  author    = {Rui Gong and Dengxin Dai and Yuhua Chen and Wen Li and Luc Van Gool},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00875},
  pages     = {8856-8865},
  title     = {MDALU: Multi-source domain adaptation and label unification with partial datasets},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised domain adaptive 3D detection with multi-level
consistency. <em>ICCV</em>, 8846–8855. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep learning-based 3D object detection has achieved unprecedented success with the advent of large-scale autonomous driving datasets. However, drastic performance degradation remains a critical challenge for cross-domain deployment. In addition, existing 3D domain adaptive detection methods often assume prior access to the target domain annotations, which is rarely feasible in the real world. To address this challenge, we study a more realistic setting, unsupervised 3D domain adaptive detection, which only utilizes source domain annotations. 1) We first comprehensively investigate the major underlying factors of the domain gap in 3D detection. Our key insight is that geometric mismatch is the key factor of domain shift. 2) Then, we propose a novel and unified framework, Multi-Level Consistency Network (MLC-Net), which employs a teacher-student paradigm to generate adaptive and reliable pseudo-targets. MLC-Net exploits point-, instance- and neural statistics-level consistency to facilitate cross-domain transfer. Extensive experiments demonstrate that MLC-Net out-performs existing state-of-the-art methods (including those using additional target domain information) on standard benchmarks. Notably, our approach is detector-agnostic, which achieves consistent gains on both single- and two-stage 3D detectors. Code will be released.},
  archive   = {C_ICCV},
  author    = {Zhipeng Luo and Zhongang Cai and Changqing Zhou and Gongjie Zhang and Haiyu Zhao and Shuai Yi and Shijian Lu and Hongsheng Li and Shanghang Zhang and Ziwei Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00874},
  pages     = {8846-8855},
  title     = {Unsupervised domain adaptive 3D detection with multi-level consistency},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-task self-training for learning general
representations. <em>ICCV</em>, 8836–8845. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite the fast progress in training specialized models for various tasks, learning a single general model that works well for many tasks is still challenging for computer vision. Here we introduce multi-task self-training (MuST), which harnesses the knowledge in independent specialized teacher models (e.g., ImageNet model on classification) to train a single general student model. Our approach has three steps. First, we train specialized teachers independently on labeled datasets. We then use the specialized teachers to label an unlabeled dataset to create a multi-task pseudo labeled dataset. Finally, the dataset, which now contains pseudo labels from teacher models trained on different datasets/tasks, is then used to train a student model with multi-task learning. We evaluate the feature representations of the student model on 6 vision tasks including image recognition (classification, detection, segmentation) and 3D geometry estimation (depth and surface normal estimation). MuST is scalable with unlabeled or partially labeled datasets and outperforms both specialized supervised models and self-supervised models when training on large scale datasets. Lastly, we show MuST can improve upon already strong checkpoints [23] trained with billions of examples. The results suggest self-training is a promising direction to aggregate labeled and unlabeled training data for learning general feature representations.},
  archive   = {C_ICCV},
  author    = {Golnaz Ghiasi and Barret Zoph and Ekin D. Cubuk and Quoc V. Le and Tsung-Yi Lin},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00873},
  pages     = {8836-8845},
  title     = {Multi-task self-training for learning general representations},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A broad study on the transferability of visual
representations with contrastive learning. <em>ICCV</em>, 8825–8835. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tremendous progress has been made in visual representation learning, notably with the recent success of self-supervised contrastive learning methods. Supervised contrastive learning has also been shown to outperform its cross-entropy counterparts by leveraging labels for choosing where to contrast. However, there has been little work to explore the transfer capability of contrastive learning to a different domain. In this paper, we conduct a comprehensive study on the transferability of learned representations of different contrastive approaches for linear evaluation, full-network transfer, and few-shot recognition on 12 downstream datasets from different domains, and object detection tasks on MSCOCO and VOC0712. The results show that the contrastive approaches learn representations that are easily transferable to a different downstream task. We further observe that the joint objective of self-supervised contrastive loss with cross-entropy/supervised-contrastive loss leads to better transferability of these models over their supervised counterparts. Our analysis reveals that the representations learned from the contrastive approaches contain more low/mid-level semantics than cross-entropy models, which enables them to quickly adapt to a new task. Our codes and models will be publicly available to facilitate future research on transferability of visual representations. 1},
  archive   = {C_ICCV},
  author    = {Ashraful Islam and Chun-Fu Chen and Rameswar Panda and Leonid Karlinsky and Richard Radke and Rogerio Feris},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00872},
  pages     = {8825-8835},
  title     = {A broad study on the transferability of visual representations with contrastive learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Composable augmentation encoding for video representation
learning. <em>ICCV</em>, 8814–8824. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We focus on contrastive methods for self-supervised video representation learning. A common paradigm in contrastive learning is to construct positive pairs by sampling different data views for the same instance, with different data instances as negatives. These methods implicitly assume a set of representational invariances to the view selection mechanism (e.g., sampling frames with temporal shifts), which may lead to poor performance on downstream tasks which violate these invariances (fine-grained video action recognition that would benefit from temporal information). To overcome this limitation, we propose an ‘augmentation aware’ contrastive learning framework, where we explicitly provide a sequence of augmentation parameterisations (such as the values of the time shifts used to create data views) as composable augmentation encodings (CATE) to our model when projecting the video representations for contrastive learning. We show that representations learned by our method encode valuable information about specified spatial or temporal augmentation, and in doing so also achieve state-of-the-art performance on a number of video benchmarks.},
  archive   = {C_ICCV},
  author    = {Chen Sun and Arsha Nagrani and Yonglong Tian and Cordelia Schmid},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00871},
  pages     = {8814-8824},
  title     = {Composable augmentation encoding for video representation learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Relational embedding for few-shot classification.
<em>ICCV</em>, 8802–8813. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose to address the problem of few-shot classification by meta-learning &quot;what to observe&quot; and &quot;where to attend&quot; in a relational perspective. Our method lever-ages relational patterns within and between images via self-correlational representation (SCR) and cross-correlational attention (CCA). Within each image, the SCR module transforms a base feature map into a self-correlation tensor and learns to extract structural patterns from the tensor. Between the images, the CCA module computes cross-correlation between two image representations and learns to produce co-attention between them. Our Relational Embedding Network (RENet) combines the two relational modules to learn relational embedding in an end-to-end manner. In experimental evaluation, it achieves consistent improvements over state-of-the-art methods on four widely used few-shot classification benchmarks of miniImageNet, tieredImageNet, CUB-200-2011, and CIFAR-FS.},
  archive   = {C_ICCV},
  author    = {Dahyun Kang and Heeseung Kwon and Juhong Min and Minsu Cho},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00870},
  pages     = {8802-8813},
  title     = {Relational embedding for few-shot classification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Variational feature disentangling for fine-grained few-shot
classification. <em>ICCV</em>, 8792–8801. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Data augmentation is an intuitive step towards solving the problem of few-shot classification. However, ensuring both discriminability and diversity in the augmented samples is challenging. To address this, we propose a feature disentanglement framework that allows us to augment features with randomly sampled intra-class variations while preserving their class-discriminative features. Specifically, we disentangle a feature representation into two components: one represents the intra-class variance and the other encodes the class-discriminative information. We assume that the intra-class variance induced by variations in poses, backgrounds, or illumination conditions is shared across all classes and can be modelled via a common distribution. Then we sample features repeatedly from the learned intra-class variability distribution and add them to the class-discriminative features to get the augmented features. Such a data augmentation scheme ensures that the augmented features inherit crucial class-discriminative features while exhibiting large intra-class variance. Our method significantly outperforms the state-of-the-art methods on multiple challenging fine-grained few-shot image classification benchmarks. Code is available at: https://github.com/cvlab-stonybrook/vfd-iccv21},
  archive   = {C_ICCV},
  author    = {Jingyi Xu and Hieu Le and Mingzhen Huang and ShahRukh Athar and Dimitris Samaras},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00869},
  pages     = {8792-8801},
  title     = {Variational feature disentangling for fine-grained few-shot classification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BAPA-net: Boundary adaptation and prototype alignment for
cross-domain semantic segmentation. <em>ICCV</em>, 8781–8791. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing cross-domain semantic segmentation methods usually focus on the overall segmentation results of whole objects but neglect the importance of object boundaries. In this work, we find that the segmentation performance can be considerably boosted if we treat object boundaries properly. For that, we propose a novel method called BAPA-Net, which is based on a convolutional neural network via Boundary Adaptation and Prototype Alignment, under the unsupervised domain adaptation setting. Specifically, we first construct additional images by pasting objects from source images to target images, and we develop a so-called boundary adaptation module to weigh each pixel based on its distance to the nearest boundary pixel of those pasted source objects. Moreover, we propose another prototype alignment module to reduce the domain mismatch by minimizing distances between the class prototypes of the source and target domains, where boundaries are removed to avoid domain confusion during prototype calculation. By integrating the boundary adaptation and prototype alignment, we are able to train a discriminative and domain-invariant model for cross-domain semantic segmentation. We conduct extensive experiments on the benchmark datasets of urban scenes (i.e., GTA5→Cityscapes and SYNTHIA→Cityscapes). And the promising results clearly show the effectiveness of our BAPA-Net method over existing state-of-the-art for cross-domain semantic segmentation. Our implementation is available at https://github.com/manmanjun/BAPA-Net.},
  archive   = {C_ICCV},
  author    = {Yahao Liu and Jinhong Deng and Xinchen Gao and Wen Li and Lixin Duan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00868},
  pages     = {8781-8791},
  title     = {BAPA-net: Boundary adaptation and prototype alignment for cross-domain semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Divide-and-assemble: Learning block-wise memory for
unsupervised anomaly detection. <em>ICCV</em>, 8771–8780. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reconstruction-based methods play an important role in unsupervised anomaly detection in images. Ideally, we expect a perfect reconstruction for normal samples and poor reconstruction for abnormal samples. Since the generalizability of deep neural networks is difficult to control, existing models such as autoencoder do not work well. In this work, we interpret the reconstruction of an image as a divide-and-assemble procedure. Surprisingly, by varying the granularity of division on feature maps, we are able to modulate the reconstruction capability of the model for both normal and abnormal samples. That is, finer granularity leads to better reconstruction, while coarser granularity leads to poorer reconstruction. With proper granularity, the gap between the reconstruction error of normal and abnormal samples can be maximized. The divide-and-assemble framework is implemented by embedding a novel multi-scale block-wise memory module into an autoencoder network. Besides, we introduce adversarial learning and explore the semantic latent representation of the discriminator, which improves the detection of subtle anomaly. We achieve state-of-the-art performance on the challenging MVTec AD dataset. Remarkably, we improve the vanilla autoencoder model by 10.1\% in terms of the AUROC score.},
  archive   = {C_ICCV},
  author    = {Jinlei Hou and Yingying Zhang and Qiaoyong Zhong and Di Xie and Shiliang Pu and Hong Zhou},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00867},
  pages     = {8771-8780},
  title     = {Divide-and-assemble: Learning block-wise memory for unsupervised anomaly detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Deep transport network for unsupervised video object
segmentation. <em>ICCV</em>, 8761–8770. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The popular unsupervised video object segmentation methods fuse the RGB frame and optical flow via a two-stream network. However, they cannot handle the distracting noises in each input modality, which may vastly deteriorate the model performance. We propose to establish the correspondence between the input modalities while suppressing the distracting signals via optimal structural matching. Given a video frame, we extract the dense local features from the RGB image and optical flow, and treat them as two complex structured representations. The Wasserstein distance is then employed to compute the global optimal flows to transport the features in one modality to the other, where the magnitude of each flow measures the extent of the alignment between two local features. To plug the structural matching into a two-stream network for end-to-end training, we factorize the input cost matrix into small spatial blocks and design a differentiable long-short Sinkhorn module consisting of a long-distant Sinkhorn layer and a short-distant Sinkhorn layer. We integrate the module into a dedicated two-stream network and dub our model TransportNet. Our experiments show that aligning motion-appearance yields the state-of-the-art results on the popular video object segmentation datasets.},
  archive   = {C_ICCV},
  author    = {Kaihua Zhang and Zicheng Zhao and Dong Liu and Qingshan Liu and Bo Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00866},
  pages     = {8761-8770},
  title     = {Deep transport network for unsupervised video object segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Domain-invariant disentangled network for generalizable
object detection. <em>ICCV</em>, 8751–8760. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the problem of domain generalizable object detection, which aims to learn a domain-invariant detector from multiple &quot;seen&quot; domains so that it can generalize well to other &quot;unseen&quot; domains. The generalization ability is crucial in practical scenarios especially when it is difficult to collect data. Compared to image classification, domain generalization in object detection has seldom been explored with more challenges brought by domain gaps on both image and instance levels. In this paper, we propose a novel generalizable object detection model, termed Domain-Invariant Disentangled Network (DIDN). In contrast to directly aligning multiple sources, we integrate a disentangled network into Faster R-CNN. By disentangling representations on both image and instance levels, DIDN is able to learn domain-invariant representations that are suitable for generalized object detection. Furthermore, we design a cross-level representation reconstruction to complement this two-level disentanglement so that informative object representations could be preserved. Extensive experiments are conducted on five benchmark datasets and the results demonstrate that our model achieves state-of-the-art performances on domain generalization for object detection.},
  archive   = {C_ICCV},
  author    = {Chuang Lin and Zehuan Yuan and Sicheng Zhao and Peize Sun and Changhu Wang and Jianfei Cai},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00865},
  pages     = {8751-8760},
  title     = {Domain-invariant disentangled network for generalizable object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PIT: Position-invariant transform for cross-FoV domain
adaptation. <em>ICCV</em>, 8741–8750. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cross-domain object detection and semantic segmentation have witnessed impressive progress recently. Existing approaches mainly consider the domain shift resulting from external environments including the changes of background, illumination or weather, while distinct camera intrinsic parameters appear commonly in different domains and their influence for domain adaptation has been very rarely explored. In this paper, we observe that the Field of View (FoV) gap induces noticeable instance appearance differences between the source and target domains. We further discover that the FoV gap between two domains impairs domain adaptation performance under both the FoV-increasing (source FoV &lt; target FoV) and FoV-decreasing cases. Motivated by the observations, we propose the Position-Invariant Transform (PIT) to better align images in different domains. We also introduce a reverse PIT for mapping the transformed/aligned images back to the original image space, and design a loss reweighting strategy to accelerate the training process. Our method can be easily plugged into existing cross-domain detection/segmentation frameworks, while bringing about negligible computational overhead. Extensive experiments demonstrate that our method can soundly boost the performance on both cross-domain object detection and segmentation for state-of-the-art techniques. Our code is available at https://github.com/sheepooo/PIT-Position-Invariant-Transform.},
  archive   = {C_ICCV},
  author    = {Qiqi Gu and Qianyu Zhou and Minghao Xu and Zhengyang Feng and Guangliang Cheng and Xuequan Lu and Jianping Shi and Lizhuang Ma},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00864},
  pages     = {8741-8750},
  title     = {PIT: Position-invariant transform for cross-FoV domain adaptation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Iterative label cleaning for transductive and
semi-supervised few-shot learning. <em>ICCV</em>, 8731–8740. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Few-shot learning amounts to learning representations and acquiring knowledge such that novel tasks may be solved with both supervision and data being limited. Improved performance is possible by transductive inference, where the entire test set is available concurrently, and semi-supervised learning, where more unlabeled data is available.Focusing on these two settings, we introduce a new algorithm that leverages the manifold structure of the labeled and unlabeled data distribution to predict pseudo-labels, while balancing over classes and using the loss value distribution of a limited-capacity classifier to select the cleanest labels, iteratively improving the quality of pseudo-labels. Our solution surpasses or matches the state of the art results on four benchmark datasets, namely miniImageNet, tieredImageNet, CUB and CIFAR-FS, while being robust over feature space pre-processing and the quantity of available data. The publicly available source code can be found in https://github.com/MichalisLazarou/iLPC},
  archive   = {C_ICCV},
  author    = {Michalis Lazarou and Tania Stathaki and Yannis Avrithis},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00863},
  pages     = {8731-8740},
  title     = {Iterative label cleaning for transductive and semi-supervised few-shot learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simpler is better: Few-shot semantic segmentation with
classifier weight transformer. <em>ICCV</em>, 8721–8730. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A few-shot semantic segmentation model is typically composed of a CNN encoder, a CNN decoder and a simple classifier (separating foreground and background pixels). Most existing methods meta-learn all three model components for fast adaptation to a new class. However, given that as few as a single support set image is available, effective model adaption of all three components to the new class is extremely challenging. In this work we propose to simplify the meta-learning task by focusing solely on the simplest component – the classifier, whilst leaving the en-coder and decoder to pre-training. We hypothesize that if we pretrain an off-the-shelf segmentation model over a set of diverse training classes with sufficient annotations, the encoder and decoder can capture rich discriminative features applicable for any unseen classes, rendering the sub-sequent meta-learning stage unnecessary. For the classifier meta-learning, we introduce a Classifier Weight Transformer (CWT) designed to dynamically adapt the support-set trained classifier’s weights to each query image in an inductive way. Extensive experiments on two standard bench-marks show that despite its simplicity, our method outperforms the state-of-the-art alternatives, often by a large margin. Code is available on https://github.com/zhiheLu/CWT-for-FSS.},
  archive   = {C_ICCV},
  author    = {Zhihe Lu and Sen He and Xiatian Zhu and Li Zhang and Yi-Zhe Song and Tao Xiang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00862},
  pages     = {8721-8730},
  title     = {Simpler is better: Few-shot semantic segmentation with classifier weight transformer},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Discriminative region-based multi-label zero-shot learning.
<em>ICCV</em>, 8711–8720. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-label zero-shot learning (ZSL) is a more realistic counter-part of standard single-label ZSL since several objects can co-exist in a natural image. However, the occurrence of multiple objects complicates the reasoning and requires region-specific processing of visual features to preserve their contextual cues. We note that the best existing multi-label ZSL method takes a shared approach towards attending to region features with a common set of attention maps for all the classes. Such shared maps lead to diffused attention, which does not discriminatively focus on relevant locations when the number of classes are large. Moreover, mapping spatially-pooled visual features to the class semantics leads to inter-class feature entanglement, thus hampering the classification. Here, we propose an alternate approach towards region-based discriminability-preserving multi-label zero-shot classification. Our approach maintains the spatial resolution to preserve region-level characteristics and utilizes a bi-level attention module (BiAM) to enrich the features by incorporating both region and scene context information. The enriched region-level features are then mapped to the class semantics and only their class predictions are spatially pooled to obtain image-level predictions, thereby keeping the multi-class features disentangled. Our approach sets a new state of the art on two large-scale multi-label zero-shot benchmarks: NUS-WIDE and Open Images. On NUS-WIDE, our approach achieves an absolute gain of 6.9\% mAP for ZSL, compared to the best published results. Source code is available at https://github.com/akshitac8/BiAM.},
  archive   = {C_ICCV},
  author    = {Sanath Narayan and Akshita Gupta and Salman Khan and Fahad Shahbaz Khan and Ling Shao and Mubarak Shah},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00861},
  pages     = {8711-8720},
  title     = {Discriminative region-based multi-label zero-shot learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mining latent classes for few-shot segmentation.
<em>ICCV</em>, 8701–8710. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Few-shot segmentation (FSS) aims to segment unseen classes given only a few annotated samples. Existing methods suffer the problem of feature undermining, i.e., potential novel classes are treated as background during training phase. Our method aims to alleviate this problem and enhance the feature embedding on latent novel classes. In our work, we propose a novel joint-training framework. Based on conventional episodic training on support-query pairs, we introduce an additional mining branch that exploits latent novel classes via transferable sub-clusters, and a new rectification technique on both background and fore-ground categories to enforce more stable prototypes. Over and above that, our transferable sub-cluster has the ability to leverage extra unlabeled data for further feature enhancement. Extensive experiments on two FSS benchmarks demonstrate that our method outperforms previous state-of-the-art by a large margin of 3.7\% mIOU on PASCAL-5 i and 7.0\% mIOU on COCO-20 i at the cost of 74\% fewer parameters and 2.5x faster inference speed. The source code is available at https://github.com/LiheYoung/MiningFSS.},
  archive   = {C_ICCV},
  author    = {Lihe Yang and Wei Zhuo and Lei Qi and Yinghuan Shi and Yang Gao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00860},
  pages     = {8701-8710},
  title     = {Mining latent classes for few-shot segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Semantics disentangling for generalized zero-shot learning.
<em>ICCV</em>, 8692–8700. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generalized zero-shot learning (GZSL) aims to classify samples under the assumption that some classes are not observable during training. To bridge the gap between the seen and unseen classes, most GZSL methods attempt to associate the visual features of seen classes with attributes or to generate unseen samples directly. Nevertheless, the visual features used in the prior approaches do not necessarily encode semantically related information that the shared attributes refer to, which degrades the model generalization to unseen classes. To address this issue, in this paper, we propose a novel semantics disentangling framework for the generalized zero-shot learning task (SDGZSL), where the visual features of unseen classes are firstly estimated by a conditional VAE and then factorized into semantic-consistent and semantic-unrelated latent vectors. In particular, a total correlation penalty is applied to guarantee the independence between the two factorized representations, and the semantic consistency of which is measured by the derived relation network. Extensive experiments conducted on four GZSL benchmark datasets have evidenced that the semantic-consistent features disentangled by the proposed SDGZSL are more generalizable in tasks of canonical and generalized zero-shot learning. Our source code is available at https://github.com/uqzhichen/SDGZSL.},
  archive   = {C_ICCV},
  author    = {Zhi Chen and Yadan Luo and Ruihong Qiu and Sen Wang and Zi Huang and Jingjing Li and Zheng Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00859},
  pages     = {8692-8700},
  title     = {Semantics disentangling for generalized zero-shot learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to hallucinate examples from extrinsic and
intrinsic supervision. <em>ICCV</em>, 8681–8691. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning to hallucinate additional examples has recently been shown as a promising direction to address few-shot learning tasks. This work investigates two important yet overlooked natural supervision signals for guiding the hallucination process – (i) extrinsic: classifiers trained on hallucinated examples should be close to strong classifiers that would be learned from a large amount of real examples; and (ii) intrinsic: clusters of hallucinated and real examples belonging to the same class should be pulled together, while simultaneously pushing apart clusters of hallucinated and real examples from different classes. We achieve (i) by introducing an additional mentor model on data-abundant base classes for directing the hallucinator, and achieve (ii) by performing contrastive learning between hallucinated and real examples. As a general, model-agnostic framework, our dual mentor-and self-directed (DMAS) hallucinator significantly improves few-shot learning performance on widely-used benchmarks in various scenarios.},
  archive   = {C_ICCV},
  author    = {Liangke Gui and Adrien Bardes and Ruslan Salakhutdinov and Alexander Hauptmann and Martial Hebert and Yu-Xiong Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00858},
  pages     = {8681-8691},
  title     = {Learning to hallucinate examples from extrinsic and intrinsic supervision},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Curvature generation in curved spaces for few-shot learning.
<em>ICCV</em>, 8671–8680. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Few-shot learning describes the challenging problem of recognizing samples from unseen classes given very few labeled examples. In many cases, few-shot learning is cast as learning an embedding space that assigns test samples to their corresponding class prototypes. Previous methods assume that data of all few-shot learning tasks comply with a fixed geometrical structure, mostly a Euclidean structure. Questioning this assumption that is clearly difficult to hold in real-world scenarios and incurs distortions to data, we propose to learn a task-aware curved embedding space by making use of the hyperbolic geometry. As a result, task-specific embedding spaces where suitable curvatures are generated to match the characteristics of data are constructed, leading to more generic embedding spaces. We then leverage on intra-class and inter-class context information in the embedding space to generate class prototypes for discriminative classification. We conduct a comprehensive set of experiments on inductive and transductive few-shot learning, demonstrating the benefits of our proposed method over existing embedding methods.},
  archive   = {C_ICCV},
  author    = {Zhi Gao and Yuwei Wu and Yunde Jia and Mehrtash Harandi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00857},
  pages     = {8671-8680},
  title     = {Curvature generation in curved spaces for few-shot learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DeFRCN: Decoupled faster r-CNN for few-shot object
detection. <em>ICCV</em>, 8661–8670. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Few-shot object detection, which aims at detecting novel objects rapidly from extremely few annotated examples of previously unseen classes, has attracted significant research interest in the community. Most existing approaches employ the Faster R-CNN as basic detection framework, yet, due to the lack of tailored considerations for data-scarce scenario, their performance is often not satisfactory. In this paper, we look closely into the conventional Faster R-CNN and analyze its contradictions from two orthogonal perspectives, namely multi-stage (RPN vs. RCNN) and multi-task (classification vs. localization). To resolve these issues, we propose a simple yet effective architecture, named Decoupled Faster R-CNN (DeFRCN). To be concrete, we extend Faster R-CNN by introducing Gradient Decoupled Layer for multistage decoupling and Prototypical Calibration Block for multi-task decoupling. The former is a novel deep layer with redefining the feature-forward operation and gradient-backward operation for decoupling its subsequent layer and preceding layer, and the latter is an offline prototype-based classification model with taking the proposals from detector as input and boosting the original classification scores with additional pairwise scores for calibration. Extensive experiments on multiple benchmarks show our framework is remarkably superior to other existing approaches and establishes a new state-of-the-art in few-shot literature 1 .},
  archive   = {C_ICCV},
  author    = {Limeng Qiao and Yuxuan Zhao and Zhiyuan Li and Xi Qiu and Jianan Wu and Chi Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00856},
  pages     = {8661-8670},
  title     = {DeFRCN: Decoupled faster R-CNN for few-shot object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pseudo-loss confidence metric for semi-supervised few-shot
learning. <em>ICCV</em>, 8651–8660. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Semi-supervised few-shot learning is developed to train a classifier that can adapt to new tasks with limited labeled data and a fixed quantity of unlabeled data. Most semi-supervised few-shot learning methods select pseudo-labeled data of unlabeled set by task-specific confidence estimation. This work presents a task-unified confidence estimation approach for semi-supervised few-shot learning, named pseudo-loss confidence metric (PLCM). It measures the data credibility by the loss distribution of pseudo-labels, which is synthetical considered multi-tasks. Specifically, pseudo-labeled data of different tasks are mapped to a unified metric space by mean of the pseudo-loss model, making it possible to learn the prior pseudo-loss distribution. Then, confidence of pseudo-labeled data is estimated according to the distribution component confidence of its pseudo-loss. Thus highly reliable pseudo-labeled data are selected to strengthen the classifier. Moreover, to overcome the pseudo-loss distribution shift and improve the effectiveness of classifier, we advance the multi-step training strategy coordinated with the class balance measures of class-apart selection and class weight. Experimental results on four popular benchmark datasets demonstrate that the proposed approach can effectively select pseudo-labeled data and achieve the state-of-the-art performance.},
  archive   = {C_ICCV},
  author    = {Kai Huang and Jie Geng and Wen Jiang and Xinyang Deng and Zhe Xu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00855},
  pages     = {8651-8660},
  title     = {Pseudo-loss confidence metric for semi-supervised few-shot learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Synthesized feature based few-shot class-incremental
learning on a mixture of subspaces. <em>ICCV</em>, 8641–8650. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Few-shot class incremental learning (FSCIL) aims to incrementally add sets of novel classes to a well-trained base model in multiple training sessions with the restriction that only a few novel instances are available per class. While learning novel classes, FSCIL methods gradually forget base (old) class training and overfit to a few novel class samples. Existing approaches have addressed this problem by computing the class prototypes from the visual or semantic word vector domain. In this paper, we propose addressing this problem using a mixture of subspaces. Subspaces define the cluster structure of the visual domain and help to describe the visual and semantic domain considering the overall distribution of the data. Additionally, we propose to employ a variational autoencoder (VAE) to generate synthesized visual samples for augmenting pseudo-feature while learning novel classes incrementally. The combined effect of the mixture of subspaces and synthesized features reduces the forgetting and overfitting problem of FSCIL. Extensive experiments on three image classification datasets show that our proposed method achieves competitive results compared to state-of-the-art methods.},
  archive   = {C_ICCV},
  author    = {Ali Cheraghian and Shafin Rahman and Sameera Ramasinghe and Pengfei Fang and Christian Simon and Lars Petersson and Mehrtash Harandi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00854},
  pages     = {8641-8650},
  title     = {Synthesized feature based few-shot class-incremental learning on a mixture of subspaces},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards alleviating the modeling ambiguity of unsupervised
monocular 3D human pose estimation. <em>ICCV</em>, 8631. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we study the ambiguity problem in the task of unsupervised 3D human pose estimation from 2D counterpart. On one hand, without explicit annotation, the scale of 3D pose is difficult to be accurately captured (scale ambiguity). On the other hand, one 2D pose might correspond to multiple 3D gestures, where the lifting procedure is inherently ambiguous (pose ambiguity). Previous methods generally use temporal constraints (e.g., constant bone length and motion smoothness) to alleviate the above issues. However, these methods commonly enforce the outputs to fulfill multiple training objectives simultaneously, which often lead to sub-optimal results. In contrast to the majority of previous works, we propose to split the whole problem into two sub-tasks, i.e., optimizing 2D input poses via a scale estimation module and then mapping optimized 2D pose to 3D counterpart via a pose lifting module. Furthermore, two temporal constraints are proposed to alleviate the scale and pose ambiguity respectively. These two modules are optimized via a iterative training scheme with corresponding temporal constraints, which effectively reduce the learning difficulty and lead to better performance. Results on the Human3.6M dataset demonstrate that our approach improves upon the prior art by 23.1\% and also outperforms several weakly supervised approaches that rely on 3D annotations. Our project is available at https://sites.google.com/view/ambiguity-aware-hpe.},
  archive   = {C_ICCV},
  author    = {Zhenbo Yu and Bingbing Ni and Jingwei Xu and Junjie Wang and Chenglong Zhao and Wenjun Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00853},
  pages     = {8631},
  title     = {Towards alleviating the modeling ambiguity of unsupervised monocular 3D human pose estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised layered image decomposition into object
prototypes. <em>ICCV</em>, 8620–8630. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an unsupervised learning framework for decomposing images into layers of automatically discovered object models. Contrary to recent approaches that model image layers with autoencoder networks, we represent them as explicit transformations of a small set of prototypical images. Our model has three main components: (i) a set of object prototypes in the form of learnable images with a transparency channel, which we refer to as sprites; (ii) differentiable parametric functions predicting occlusions and transformation parameters necessary to instantiate the sprites in a given image; (iii) a layered image formation model with occlusion for compositing these instances into complete images including background. By jointly learning the sprites and occlusion/transformation predictors to reconstruct images, our approach not only yields accurate layered image decompositions, but also identifies object categories and instance parameters. We first validate our approach by providing results on par with the state of the art on standard multiobject synthetic benchmarks (Tetrominoes, Multi-dSprites, CLEVR6). We then demonstrate the applicability of our model to real images in tasks that include clustering (SVHN, GTSRB), cosegmentation (Weizmann Horse) and object discovery from unfiltered social network images. To the best of our knowledge, our approach is the first layered image decomposition algorithm that learns an explicit and shared concept of object type, and is robust enough to be applied to real images.},
  archive   = {C_ICCV},
  author    = {Tom Monnier and Elliot Vincent and Jean Ponce and Mathieu Aubry},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00852},
  pages     = {8620-8630},
  title     = {Unsupervised layered image decomposition into object prototypes},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Intrinsic-extrinsic preserved GANs for unsupervised 3D pose
transfer. <em>ICCV</em>, 8610–8619. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the strength of deep generative models, 3D pose transfer regains intensive research interests in recent years. Existing methods mainly rely on a variety of constraints to achieve the pose transfer over 3D meshes, e.g., the need for manually encoding for shape and pose disentanglement. In this paper, we present an unsupervised approach to conduct the pose transfer between any arbitrate given 3D meshes. Specifically, a novel Intrinsic-Extrinsic Preserved Generative Adversarial Network (IEP-GAN) is presented for both intrinsic (i.e., shape) and extrinsic (i.e., pose) information preservation. Extrinsically, we propose a co-occurrence discriminator to capture the structural/pose invariance from distinct Laplacians of the mesh. Meanwhile, intrinsically, a local intrinsic-preserved loss is introduced to preserve the geodesic priors while avoiding heavy computations. At last, we show the possibility of using IEP-GAN to manipulate 3D human meshes in various ways, including pose transfer, identity swapping and pose interpolation with latent code vector arithmetic. The extensive experiments on various 3D datasets of humans, animals and hands qualitatively and quantitatively demonstrate the generality of our approach. Our proposed model produces better results and is substantially more efficient compared to recent state-of-the-art methods. Code is available: https://github.com/mikecheninoulu/Unsupervised_IEPGAN},
  archive   = {C_ICCV},
  author    = {Haoyu Chen and Hao Tang and Henglin Shi and Wei Peng and Nicu Sebe and Guoying Zhao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00851},
  pages     = {8610-8619},
  title     = {Intrinsic-extrinsic preserved GANs for unsupervised 3D pose transfer},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Skeleton2Mesh: Kinematics prior injected unsupervised human
mesh recovery. <em>ICCV</em>, 8599–8609. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we decouple unsupervised human mesh recovery into the well-studied problems of unsupervised 3D pose estimation, and human mesh recovery from estimated 3D skeletons, focusing on the latter task. The challenges of the latter task are two folds: (1) pose failure (i.e., pose mismatching – different skeleton definitions in dataset and SMPL , and pose ambiguity – endpoints have arbitrary joint angle configurations for the same 3D joint coordinates). (2) shape ambiguity (i.e., the lack of shape constraints on body configuration). To address these issues, we propose Skeleton2Mesh, a novel lightweight framework that recovers human mesh from a single image. Our Skeleton2Mesh contains three modules, i.e., Differentiable Inverse Kinematics (DIK), Pose Refinement (PR) and Shape Refinement (SR) modules. DIK is designed to transfer 3D rotation from estimated 3D skeletons, which relies on a minimal set of kinematics prior knowledge. Then PR and SR modules are utilized to tackle the pose ambiguity and shape ambiguity respectively. All three modules can be incorporated into Skeleton2Mesh seamlessly via an end-to-end manner. Furthermore, we utilize an adaptive joint regressor to alleviate the effects of skeletal topology from different datasets. Results on the Human3.6M dataset for human mesh recovery demonstrate that our method improves upon the previous unsupervised methods by 32.6\% under the same setting. Qualitative results on in-the-wild datasets exhibit that the recovered 3D meshes are natural, realistic. Our project is available at https://sites.google.com/view/skeleton2mesh.},
  archive   = {C_ICCV},
  author    = {Zhenbo Yu and Junjie Wang and Jingwei Xu and Bingbing Ni and Chenglong Zhao and Minsi Wang and Wenjun Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00850},
  pages     = {8599-8609},
  title     = {Skeleton2Mesh: Kinematics prior injected unsupervised human mesh recovery},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-supervised object detection via generative image
synthesis. <em>ICCV</em>, 8589–8598. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present SSOD – the first end-to-end analysis-by-synthesis framework with controllable GANs for the task of self-supervised object detection. We use collections of real-world images without bounding box annotations to learn to synthesize and detect objects. We leverage controllable GANs to synthesize images with pre-defined object properties and use them to train object detectors. We propose a tight end-to-end coupling of the synthesis and detection networks to optimally train our system. Finally, we also propose a method to optimally adapt SSOD to an intended target data without requiring labels for it. For the task of car detection, on the challenging KITTI and Cityscapes datasets, we show that SSOD outperforms the prior state-of-the-art purely image-based self-supervised object detection method Wetectron. Even without requiring any 3D CAD assets, it also surpasses the state-of-the-art rendering-based method Meta-Sim2. Our work advances the field of self-supervised object detection by introducing a successful new paradigm of using controllable GAN-based image synthesis for it and by significantly improving the baseline accuracy of the task. We open-source our code at https://github.com/NVlabs/SSOD.},
  archive   = {C_ICCV},
  author    = {Siva Karthik Mustikovela and Shalini De Mello and Aayush Prakash and Umar Iqbal and Sifei Liu and Thu Nguyen-Phuoc and Carsten Rother and Jan Kautz},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00849},
  pages     = {8589-8598},
  title     = {Self-supervised object detection via generative image synthesis},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Transporting causal mechanisms for unsupervised domain
adaptation. <em>ICCV</em>, 8579–8588. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing Unsupervised Domain Adaptation (UDA) literature adopts the covariate shift and conditional shift assumptions, which essentially encourage models to learn common features across domains. However, due to the lack of supervision in the target domain, they suffer from the semantic loss: the feature will inevitably lose non-discriminative semantics in source domain, which is however discriminative in target domain. We use a causal view—transportability theory [40]—to identify that such loss is in fact a confounding effect, which can only be removed by causal intervention. However, the theoretical solution provided by transportability is far from practical for UDA, because it requires the stratification and representation of the unobserved confounder that is the cause of the domain gap. To this end, we propose a practical solution: Transporting Causal Mechanisms (TCM), to identify the confounder stratum and representations by using the domain-invariant disentangled causal mechanisms, which are discovered in an unsupervised fashion. Our TCM is both theoretically and empirically grounded. Extensive experiments show that TCM achieves state-of-the-art performance on three challenging UDA benchmarks: ImageCLEF-DA, Office-Home, and VisDA-2017. Codes are available at https://github.com/yue-zhongqi/tcm.},
  archive   = {C_ICCV},
  author    = {Zhongqi Yue and Qianru Sun and Xian-Sheng Hua and Hanwang Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00848},
  pages     = {8579-8588},
  title     = {Transporting causal mechanisms for unsupervised domain adaptation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LabOR: Labeling only if required for domain adaptive
semantic segmentation. <em>ICCV</em>, 8568–8578. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unsupervised Domain Adaptation (UDA) for semantic segmentation has been actively studied to mitigate the domain gap between label-rich source data and unlabeled target data. Despite these efforts, UDA still has a long way to go to reach the fully supervised performance. To this end, we propose a Labeling Only if Required strategy, LabOR, where we introduce a human-in-the-loop approach to adaptively give scarce labels to points that a UDA model is uncertain about. In order to find the uncertain points, we generate an inconsistency mask using the proposed adaptive pixel selector and we label these segment-based regions to achieve near supervised performance with only a small fraction (about 2.2\%) ground truth points, which we call &quot;Segment based Pixel-Labeling (SPL).&quot; To further reduce the efforts of the human annotator, we also propose &quot;Point based Pixel-Labeling (PPL),&quot; which finds the most representative points for labeling within the generated inconsistency mask. This reduces efforts from 2.2\% segment label → 40 points label while minimizing performance degradation. Through extensive experimentation, we show the advantages of this new framework for domain adaptive semantic segmentation while minimizing human labor costs.},
  archive   = {C_ICCV},
  author    = {Inkyu Shin and Dong-Jin Kim and Jae Won Cho and Sanghyun Woo and Kwanyong Park and In So Kweon},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00847},
  pages     = {8568-8578},
  title     = {LabOR: Labeling only if required for domain adaptive semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). ECACL: A holistic framework for semi-supervised domain
adaptation. <em>ICCV</em>, 8558–8567. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies Semi-Supervised Domain Adaptation (SSDA), a practical yet under-investigated research topic that aims to learn a model of good performance using unlabeled samples and a few labeled samples in the target domain, with the help of labeled samples from a source domain. Several SSDA methods have been proposed recently, which however fail to fully exploit the value of the few labeled target samples. In this paper, we propose Enhanced Categorical Alignment and Consistency Learning (ECACL), a holistic SSDA framework that incorporates multiple mutually complementary domain alignment techniques. ECACL includes two categorical domain alignment techniques that achieve class-level alignment, a strong data augmentation based technique that enhances the model’s generalizability and a consistency learning based technique that forces the model to be robust with image perturbations. These techniques are applied on one or multiple of the three inputs (labeled source, unlabeled target, and labeled target) and align the domains from different perspectives. ECACL unifies them together and achieves fairly comprehensive domain alignments that are much better than the existing methods: For example, ECACL raises the state-of-the-art accuracy from 68.4 to 81.1 on VisDA2017 and from 45.5 to 53.4 on DomainNet for the 1-shot setting. Our code is available at https://github.com/kailigo/pacl.},
  archive   = {C_ICCV},
  author    = {Kai Li and Chang Liu and Handong Zhao and Yulun Zhang and Yun Fu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00846},
  pages     = {8558-8567},
  title     = {ECACL: A holistic framework for semi-supervised domain adaptation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adversarial robustness for unsupervised domain adaptation.
<em>ICCV</em>, 8548–8557. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Extensive Unsupervised Domain Adaptation (UDA) studies have shown great success in practice by learning transferable representations across a labeled source domain and an unlabeled target domain with deep models. However, current work focuses on improving the generalization ability of UDA models on clean examples without considering the adversarial robustness, which is crucial in real-world applications. Conventional adversarial training methods are not suitable for the adversarial robustness on the unlabeled target domain of UDA since they train models with adversarial examples generated by the supervised loss function. In this work, we propose to leverage intermediate representations learned by robust ImageNet models to improve the robustness of UDA models. Our method works by aligning the features of the UDA model with the robust features learned by ImageNet pre-trained models along with domain adaptation training. It utilizes both labeled and unlabeled domains and instills robustness without any adversarial intervention or label requirement during domain adaptation training. Our experimental results show that our method significantly improves adversarial robustness compared to the baseline while keeping clean accuracy on various UDA benchmarks.},
  archive   = {C_ICCV},
  author    = {Muhammad Awais and Fengwei Zhou and Hang Xu and Lanqing Hong and Ping Luo and Sung-Ho Bae and Zhenguo Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00845},
  pages     = {8548-8557},
  title     = {Adversarial robustness for unsupervised domain adaptation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SENTRY: Selective entropy optimization via committee
consistency for unsupervised domain adaptation. <em>ICCV</em>,
8538–8547. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many existing approaches for unsupervised domain adaptation (UDA) focus on adapting under only data distribution shift and offer limited success under additional cross-domain label distribution shift. Recent work based on self-training using target pseudolabels has shown promise, but on challenging shifts pseudolabels may be highly unreliable and using them for self-training may lead to error accumulation and domain misalignment. We propose Selective Entropy Optimization via Committee Consistency (SENTRY), a UDA algorithm that judges the reliability of a target instance based on its predictive consistency under a committee of random image transformations. Our algorithm then selectively minimizes predictive entropy to increase confidence on highly consistent target instances, while maximizing predictive entropy to reduce confidence on highly inconsistent ones. In combination with pseudolabel-based approximate target class balancing, our approach leads to significant improvements over the state-of-the-art on 27/31 domain shifts from standard UDA benchmarks as well as benchmarks designed to stress-test adaptation under label distribution shift. Our code is available at https://github.com/virajprabhu/SENTRY.},
  archive   = {C_ICCV},
  author    = {Viraj Prabhu and Shivam Khare and Deeksha Kartik and Judy Hoffman},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00844},
  pages     = {8538-8547},
  title     = {SENTRY: Selective entropy optimization via committee consistency for unsupervised domain adaptation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BiMaL: Bijective maximum likelihood approach to domain
adaptation in semantic scene segmentation. <em>ICCV</em>, 8528–8537. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Semantic segmentation aims to predict pixel-level labels. It has become a popular task in various computer vision applications. While fully supervised segmentation methods have achieved high accuracy on large-scale vision datasets, they are unable to generalize on a new test environment or a new domain well. In this work, we first introduce a new Unaligned Domain Score to measure the efficiency of a learned model on a new target domain in unsupervised manner. Then, we present the new Bijective Maximum Likelihood 1 (BiMaL) loss that is a generalized form of the Adversarial Entropy Minimization without any assumption about pixel independence. We have evaluated the proposed BiMaL on two domains. The proposed BiMaL approach consistently outperforms the SOTA methods on empirical experiments on &quot;SYNTHIA to Cityscapes&quot;, &quot;GTA5 to Cityscapes&quot;, and &quot;SYNTHIA to Vistas&quot;.},
  archive   = {C_ICCV},
  author    = {Thanh-Dat Truong and Chi Nhan Duong and Ngan Le and Son Lam Phung and Chase Rainwater and Khoa Luu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00843},
  pages     = {8528-8537},
  title     = {BiMaL: Bijective maximum likelihood approach to domain adaptation in semantic scene segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Geometric unsupervised domain adaptation for semantic
segmentation. <em>ICCV</em>, 8517–8527. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simulators can efficiently generate large amounts of labeled synthetic data with perfect supervision for hard-to-label tasks like semantic segmentation. However, they introduce a domain gap that severely hurts real-world performance. We propose to use self-supervised monocular depth estimation as a proxy task to bridge this gap and improve sim-to-real unsupervised domain adaptation (UDA). Our Geometric Unsupervised Domain Adaptation method (GUDA) 1 learns a domain-invariant representation via a multi-task objective combining synthetic semantic supervision with real-world geometric constraints on videos. GUDA establishes a new state of the art in UDA for semantic segmentation on three benchmarks, outperforming methods that use domain adversarial learning, self-training, or other self-supervised proxy tasks. Furthermore, we show that our method scales well with the quality and quantity of synthetic data while also improving depth prediction.},
  archive   = {C_ICCV},
  author    = {Vitor Guizilini and Jie Li and Rareş Ambruş and Adrien Gaidon},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00842},
  pages     = {8517-8527},
  title     = {Geometric unsupervised domain adaptation for semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards discriminative representation learning for
unsupervised person re-identification. <em>ICCV</em>, 8506–8516. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we address the problem of unsupervised domain adaptation for person re-ID where annotations are available for the source domain but not for target. Previous methods typically follow a two-stage optimization pipeline, where the network is first pre-trained on source and then fine-tuned on target with pseudo labels created by feature clustering. Such methods sustain two main limitations. (1) The label noise may hinder the learning of discriminative features for recognizing target classes. (2) The domain gap may hinder knowledge transferring from source to target. We propose three types of technical schemes to alleviate these issues. First, we propose a cluster-wise contrastive learning algorithm (CCL) by iterative optimization of feature learning and cluster refinery to learn noise-tolerant representations in the unsupervised manner. Second, we adopt a progressive domain adaptation (PDA) strategy to gradually mitigate the domain gap between source and target data. Third, we propose Fourier augmentation (FA) for further maximizing the class separability of re-ID models by imposing extra constraints in the Fourier space. We observe that these proposed schemes are capable of facilitating the learning of discriminative feature representations. Experiments demonstrate that our method consistently achieves notable improvements over the state-of-the-art unsupervised re-ID methods on multiple benchmarks, e.g., surpassing MMT largely by 8.1\%, 9.9\%, 11.4\% and 11.1\% mAP on the Market-to-Duke, Duke-to-Market, Market-to-MSMT and Duke-to-MSMT tasks, respectively.},
  archive   = {C_ICCV},
  author    = {Takashi Isobe and Dong Li and Lu Tian and Weihua Chen and Yi Shan and Shengjin Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00841},
  pages     = {8506-8516},
  title     = {Towards discriminative representation learning for unsupervised person re-identification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Domain adaptive semantic segmentation with self-supervised
depth estimation. <em>ICCV</em>, 8495–8505. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Domain adaptation for semantic segmentation aims to improve the model performance in the presence of a distribution shift between source and target domain. Leveraging the supervision from auxiliary tasks (such as depth estimation) has the potential to heal this shift because many visual tasks are closely related to each other. However, such a supervision is not always available. In this work, we leverage the guidance from self-supervised depth estimation, which is available on both domains, to bridge the domain gap. On the one hand, we propose to explicitly learn the task feature correlation to strengthen the target semantic predictions with the help of target depth estimation. On the other hand, we use the depth prediction discrepancy from source and target depth decoders to approximate the pixel-wise adaptation difficulty. The adaptation difficulty, inferred from depth, is then used to refine the target semantic segmentation pseudo-labels. The proposed method can be easily implemented into existing segmentation frameworks. We demonstrate the effectiveness of our approach on the benchmark tasks SYNTHIA-to-Cityscapes and GTA-to-Cityscapes, on which we achieve the new state-of-the-art performance of 55.0\% and 56.6\%, respectively. Our code is available at https://qin.ee/corda.},
  archive   = {C_ICCV},
  author    = {Qin Wang and Dengxin Dai and Lukas Hoyer and Luc Van Gool and Olga Fink},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00840},
  pages     = {8495-8505},
  title     = {Domain adaptive semantic segmentation with self-supervised depth estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Active domain adaptation via clustering uncertainty-weighted
embeddings. <em>ICCV</em>, 8485–8494. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generalizing deep neural networks to new target domains is critical to their real-world utility. In practice, it may be feasible to get some target data labeled, but to be cost-effective it is desirable to select a maximally-informative subset via active learning (AL). We study the problem of AL under a domain shift, called Active Domain Adaptation (Active DA). We demonstrate how existing AL approaches based solely on model uncertainty or diversity sampling are less effective for Active DA. We propose Clustering Uncertainty-weighted Embeddings (CLUE), a novel label acquisition strategy for Active DA that performs uncertainty-weighted clustering to identify target instances for labeling that are both uncertain under the model and diverse in feature space. CLUE consistently outperforms competing label acquisition strategies for Active DA and AL across learning settings on 6 diverse domain shifts for image classification. Our code is available at https://github.com/virajprabhu/CLUE.},
  archive   = {C_ICCV},
  author    = {Viraj Prabhu and Arjun Chandrasekaran and Kate Saenko and Judy Hoffman},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00839},
  pages     = {8485-8494},
  title     = {Active domain adaptation via clustering uncertainty-weighted embeddings},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hierarchical transformation-discriminating generative
model for few shot anomaly detection. <em>ICCV</em>, 8475–8484. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Anomaly detection, the task of identifying unusual samples in data, often relies on a large set of training samples. In this work, we consider the setting of few-shot anomaly detection in images, where only a few images are given at training. We devise a hierarchical generative model that captures the multi-scale patch distribution of each training image. We further enhance the representation of our model by using image transformations and optimize scale-specific patch-discriminators to distinguish between real and fake patches of the image, as well as between different transformations applied to those patches. The anomaly score is obtained by aggregating the patch-based votes of the correct transformation across scales and image regions. We demonstrate the superiority of our method on both the one-shot and few-shot settings, on the datasets of Paris, CIFAR10, MNIST and FashionMNIST as well as in the setting of defect detection on MVTec. In all cases, our method outperforms the recent baseline methods.},
  archive   = {C_ICCV},
  author    = {Shelly Sheynin and Sagie Benaim and Lior Wolf},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00838},
  pages     = {8475-8484},
  title     = {A hierarchical transformation-discriminating generative model for few shot anomaly detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised few-shot action recognition via
action-appearance aligned meta-adaptation. <em>ICCV</em>, 8464–8474. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present MetaUVFS as the first Unsupervised Meta-learning algorithm for Video Few-Shot action recognition. MetaUVFS leverages over 550K unlabeled videos to train a two-stream 2D and 3D CNN architecture via contrastive learning to capture the appearance-specific spatial and action-specific spatio-temporal video features respectively. MetaUVFS comprises a novel Action-Appearance Aligned Meta-adaptation (A3M) module that learns to focus on the action-oriented video features in relation to the appearance features via explicit few-shot episodic meta-learning over unsupervised hard-mined episodes. Our action-appearance alignment and explicit few-shot learner conditions the unsupervised training to mimic the downstream few-shot task, enabling MetaUVFS to significantly outperform all state-of-the-art unsupervised methods on few-shot benchmarks. Moreover, unlike previous few-shot action recognition methods that are supervised, MetaUVFS needs neither base-class labels nor a supervised pretrained backbone. Thus, we need to train MetaUVFS just once to perform competitively or sometimes even outperform state-of-the-art supervised methods on popular HMDB51, UCF101, and Kinetics100 few-shot datasets.},
  archive   = {C_ICCV},
  author    = {Jay Patravali and Gaurav Mittal and Ye Yu and Fuxin Li and Mei Chen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00837},
  pages     = {8464-8474},
  title     = {Unsupervised few-shot action recognition via action-appearance aligned meta-adaptation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interaction compass: Multi-label zero-shot learning of
human-object interactions via spatial relations. <em>ICCV</em>,
8452–8463. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of multi-label zero-shot recognition in which labels are in the form of human-object interactions (combinations of actions on objects), each image may contain multiple interactions and some interactions do not have training images. We propose a novel compositional learning framework that decouples interaction labels into separate action and object scores that incorporate the spatial compatibility between the two components. We combine these scores to efficiently recognize seen and unseen interactions. However, learning action-object spatial relations, in principle, requires bounding-box annotations, which are costly to gather. Moreover, it is not clear how to generalize spatial relations to unseen interactions. We address these challenges by developing a cross-attention mechanism that localizes objects from action locations and vice versa by predicting displacements between them, referred to as relational directions. During training, we estimate the relational directions as ones maximizing the scores of ground-truth interactions that guide predictions toward compatible action-object regions. By extensive experiments, we show the effectiveness of our framework, where we improve the state of the art by 2.6\% mAP score and 5.8\% recall score on HICO and Visual Genome datasets, respectively. 1},
  archive   = {C_ICCV},
  author    = {Dat Huynh and Ehsan Elhamifar},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00836},
  pages     = {8452-8463},
  title     = {Interaction compass: Multi-label zero-shot learning of human-object interactions via spatial relations},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LoFGAN: Fusing local representations for few-shot image
generation. <em>ICCV</em>, 8443–8451. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Given only a few available images for a novel unseen category, few-shot image generation aims to generate more data for this category. Previous works attempt to globally fuse these images by using adjustable weighted coefficients. However, there is a serious semantic misalignment between different images from a global perspective, making these works suffer from poor generation quality and diversity. To tackle this problem, we propose a novel Local-Fusion Generative Adversarial Network (LoFGAN) for fewshot image generation. Instead of using these available images as a whole, we first randomly divide them into a base image and several reference images. Next, LoFGAN matches local representations between the base and reference images based on semantic similarities, and replaces the local features with the closest related local features. In this way, LoFGAN can produce more realistic and diverse images at a more fine-grained level, and simultaneously enjoy the characteristic of semantic alignment. Furthermore, a local reconstruction loss is also proposed, which can provide better training stability and generation quality. We conduct extensive experiments on three datasets, which successfully demonstrates the effectiveness of our proposed method for few-shot image generation and downstream visual applications with limited data. Code is available at https://github.com/edward3862/LoFGAN-pytorch.},
  archive   = {C_ICCV},
  author    = {Zheng Gu and Wenbin Li and Jing Huo and Lei Wang and Yang Gao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00835},
  pages     = {8443-8451},
  title     = {LoFGAN: Fusing local representations for few-shot image generation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-mode modulator for multi-domain few-shot
classification. <em>ICCV</em>, 8433–8442. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most existing few-shot classification methods only consider generalization on one dataset (i.e., single-domain), failing to transfer across various seen and unseen domains. In this paper, we consider the more realistic multi-domain few-shot classification problem to investigate the cross-domain generalization. Two challenges exist in this new setting: (1) how to efficiently generate multi-domain feature representation, and (2) how to explore domain correlations for better cross-domain generalization. We propose a parameter-efficient multi-mode modulator to address both challenges. First, the modulator is designed to maintain multiple modulation parameters (one for each domain) in a single network, thus achieving single-network multi-domain representation. Given a particular domain, domain-aware features can be efficiently generated with the well-devised separative selection module and cooperative query module. Second, we further divide the modulation parameters into the domain-specific set and the domain-cooperative set to explore the intra-domain information and inter-domain correlations, respectively. The intra-domain information describes each domain independently to prevent negative interference. The inter-domain correlations guide information sharing among relevant domains to enrich their own representation. Moreover, unseen domains can utilize the correlations to obtain an adaptive combination of seen domains for extrapolation. We demonstrate that the proposed multi-mode modulator achieves state-of-the-art results on the challenging META-DATASET benchmark, especially for unseen test domains.},
  archive   = {C_ICCV},
  author    = {Yanbin Liu and Juho Lee and Linchao Zhu and Ling Chen and Humphrey Shi and Yi Yang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00834},
  pages     = {8433-8442},
  title     = {A multi-mode modulator for multi-domain few-shot classification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semi-supervised learning of visual features by
non-parametrically predicting view assignments with support samples.
<em>ICCV</em>, 8423–8432. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a novel method of learning by predicting view assignments with support samples (PAWS). The method trains a model to minimize a consistency loss, which ensures that different views of the same unlabeled instance are assigned similar pseudo-labels. The pseudo-labels are generated non-parametrically, by comparing the representations of the image views to those of a set of randomly sampled labeled images. The distance between the view representations and labeled representations is used to provide a weighting over class labels, which we interpret as a soft pseudo-label. By non-parametrically incorporating labeled samples in this way, PAWS extends the distance-metric loss used in self-supervised methods such as BYOL and SwAV to the semi-supervised setting. Despite the simplicity of the approach, PAWS outperforms other semi-supervised methods across architectures, setting a new state-of-the-art for a ResNet-50 on ImageNet trained with either 10\% or 1\% of the labels, reaching 75.5\% and 66.5\% top-1 respectively. than the previous best methods. PAWS requires 4× to 12× less training},
  archive   = {C_ICCV},
  author    = {Mahmoud Assran and Mathilde Caron and Ishan Misra and Piotr Bojanowski and Armand Joulin and Nicolas Ballas and Michael Rabbat},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00833},
  pages     = {8423-8432},
  title     = {Semi-supervised learning of visual features by non-parametrically predicting view assignments with support samples},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Task-aware part mining network for few-shot learning.
<em>ICCV</em>, 8413–8422. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Few-Shot Learning (FSL) aims at classifying samples into new unseen classes with only a handful of labeled samples available. However, most of the existing methods are based on the image-level pooled representation, yet ignore considerable local clues that are transferable across tasks. To address this issue, we propose an end-to-end Task-aware Part Mining Network (TPMN) by integrating an automatic part mining process into the metric-based model for FSL. The proposed TPMN model enjoys several merits. First, we design a meta filter learner to generate task-aware part filters based on the task embedding in a meta-learning way. The task-aware part filters can adapt to any individual task and automatically mine task-related local parts even for an unseen task. Second, an adaptive importance generator is proposed to identify key local parts and assign adaptive importance weights to different parts. To the best of our knowledge, this is the first work to automatically exploit the task-aware local parts in a meta-learning way for FSL. Extensive experimental results on four standard benchmarks demonstrate that the proposed model performs favorably against state-of-the-art FSL methods.},
  archive   = {C_ICCV},
  author    = {Jiamin Wu and Tianzhu Zhang and Yongdong Zhang and Feng Wu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00832},
  pages     = {8413-8422},
  title     = {Task-aware part mining network for few-shot learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning rare category classifiers on a tight labeling
budget. <em>ICCV</em>, 8403–8412. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many real-world ML deployments face the challenge of training a rare category model with a small labeling budget. In these settings, there is often access to large amounts of unlabeled data, therefore it is attractive to consider semi-supervised or active learning approaches to reduce human labeling effort. However, prior approaches make two assumptions that do not often hold in practice; (a) one has access to a modest amount of labeled data to bootstrap learning and (b) every image belongs to a common category of interest. In this paper, we consider the scenario where we start with as-little-as five labeled positives of a rare category and a large amount of unlabeled data of which 99.9\% of it is negatives. We propose an active semi-supervised method for building accurate models in this challenging setting. Our method leverages two key ideas: (a) Utilize human and machine effort where they are most effective; human labels are used to identify &quot;needle-in-a-haystack&quot; positives, while machine-generated pseudo-labels are used to identify negatives. (b) Adapt recently proposed representation learning techniques for handling extremely imbalanced human labeled data to iteratively train models with noisy machine labeled data. We compare our approach with prior active learning and semi-supervised approaches, demonstrating significant improvements in accuracy per unit labeling effort, particularly on a tight labeling budget.},
  archive   = {C_ICCV},
  author    = {Ravi Teja Mullapudi and Fait Poms and William R. Mark and Deva Ramanan and Kayvon Fatahalian},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00831},
  pages     = {8403-8412},
  title     = {Learning rare category classifiers on a tight labeling budget},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Transductive few-shot classification on the oblique
manifold. <em>ICCV</em>, 8392–8402. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Few-shot learning (FSL) attempts to learn with limited data. In this work, we perform the feature extraction in the Euclidean space and the geodesic distance metric on the Oblique Manifold (OM). Specially, for better feature extraction, we propose a non-parametric Region Self-attention with Spatial Pyramid Pooling (RSSPP), which realizes a trade-off between the generalization and the discriminative ability of the single image feature. Then, we embed the feature to OM as a point. Furthermore, we design an Oblique Distance-based Classifier (ODC) that achieves classification in the tangent spaces which better approximate OM locally by learnable tangency points. Finally, we introduce a new method for parameters initialization and a novel loss function in the transductive settings. Extensive experiments demonstrate the effectiveness of our algorithm and it outperforms state-of-the-art methods on the popular benchmarks: mini-ImageNet, tiered-ImageNet, and Caltech-UCSD Birds-200-2011 (CUB).},
  archive   = {C_ICCV},
  author    = {Guodong Qi and Huimin Yu and Zhaohui Lu and Shuzhao Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00830},
  pages     = {8392-8402},
  title     = {Transductive few-shot classification on the oblique manifold},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Binocular mutual learning for improving few-shot
classification. <em>ICCV</em>, 8382–8391. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most of the few-shot learning methods learn to transfer knowledge from datasets with abundant labeled data (i.e., the base set). From the perspective of class space on base set, existing methods either focus on utilizing all classes under a global view by normal pretraining, or pay more attention to adopt an episodic manner to train meta-tasks within few classes in a local view. However, the interaction of the two views is rarely explored. As the two views capture complementary information, we naturally think of the compatibility of them for achieving further performance gains. Inspired by the mutual learning paradigm and binocular parallax, we propose a unified framework, namely Binocular Mutual Learning (BML), which achieves the compatibility of the global view and the local view through both intraview and cross-view modeling. Concretely, the global view learns in the whole class space to capture rich inter-class relationships. Meanwhile, the local view learns in the local class space within each episode, focusing on matching positive pairs correctly. In addition, cross-view mutual interaction further promotes the collaborative learning and the implicit exploration of useful knowledge from each other. During meta-test, binocular embeddings are aggregated together to support decision-making, which greatly improve the accuracy of classification. Extensive experiments conducted on multiple benchmarks including cross-domain validation confirm the effectiveness of our method 1 .},
  archive   = {C_ICCV},
  author    = {Ziqi Zhou and Xi Qiu and Jiangtao Xie and Jianan Wu and Chi Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00829},
  pages     = {8382-8391},
  title     = {Binocular mutual learning for improving few-shot classification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DetCo: Unsupervised contrastive learning for object
detection. <em>ICCV</em>, 8372–8381. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present DetCo, a simple yet effective self-supervised approach for object detection. Unsupervised pre-training methods have been recently designed for object detection, but they are usually deficient in image classification, or the opposite. Unlike them, DetCo transfers well on downstream instance-level dense prediction tasks, while maintaining competitive image-level classification accuracy. The advantages are derived from (1) multi-level supervision to intermediate representations, (2) contrastive learning between global image and local patches. These two designs facilitate discriminative and consistent global and local representation at each level of feature pyramid, improving detection and classification, simultaneously.Extensive experiments on VOC, COCO, Cityscapes, and ImageNet demonstrate that DetCo not only outperforms recent methods on a series of 2D and 3D instance-level detection tasks, but also competitive on image classification. For example, on ImageNet classification, DetCo is 6.9\% and 5.0\% top-1 accuracy better than InsLoc and DenseCL, which are two contemporary works designed for object detection. Moreover, on COCO detection, DetCo is 6.9 AP better than SwAV with Mask R-CNN C4. Notably, DetCo largely boosts up Sparse R-CNN, a recent strong detector, from 45.0 AP to 46.5 AP (+1.5 AP), establishing a new SOTA on COCO.},
  archive   = {C_ICCV},
  author    = {Enze Xie and Jian Ding and Wenhai Wang and Xiaohang Zhan and Hang Xu and Peize Sun and Zhenguo Li and Ping Luo},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00828},
  pages     = {8372-8381},
  title     = {DetCo: Unsupervised contrastive learning for object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Shape self-correction for unsupervised point cloud
understanding. <em>ICCV</em>, 8362–8371. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We develop a novel self-supervised learning method named Shape Self-Correction for point cloud analysis. Our method is motivated by the principle that a good shape representation should be able to find distorted parts of a shape and correct them. To learn strong shape representations in an unsupervised manner, we first design a shape-disorganizing module to destroy certain local shape parts of an object. Then the destroyed shape and the normal shape are sent into a point cloud network to get representations, which are employed to segment points that belong to distorted parts and further reconstruct them to restore the shape to normal. To perform better in these two associated pretext tasks, the network is constrained to capture useful shape features from the object, which indicates that the point cloud network encodes rich geometric and contextual information. The learned feature extractor transfers well to downstream classification and segmentation tasks. Experimental results on ModelNet, ScanNet and ShapeNetPart demonstrate that our method achieves state-of-the-art performance among unsupervised methods. Our framework can be applied to a wide range of deep learning networks for point cloud analysis and we show experimentally that pre-training with our framework significantly boosts the performance of supervised models.},
  archive   = {C_ICCV},
  author    = {Ye Chen and Jinxian Liu and Bingbing Ni and Hang Wang and Jiancheng Yang and Ning Liu and Teng Li and Qi Tian},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00827},
  pages     = {8362-8371},
  title     = {Shape self-correction for unsupervised point cloud understanding},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online pseudo label generation by hierarchical cluster
dynamics for adaptive person re-identification. <em>ICCV</em>,
8351–8361. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Adaptive person re-identification (adaptive ReID) targets at transferring learned knowledge from the labeled source domain to the unlabeled target domain. Pseudo-label-based methods that alternatively generate pseudo labels and optimize the training model have demonstrated great effectiveness in this field. However, the generated pseudo labels are inaccurate and cannot reflect the true semantic meaning of the unlabeled samples. We consider such inaccuracy stems from both the lagged update of the pseudo labels as well as the simple criterion of the employed clustering method. To tackle the problem, we propose an online pseudo label generation by hierarchical cluster dynamics for adaptive ReID. In particular, hierarchical label banks are constructed for all the samples in the dataset, and we update the pseudo labels of the sample in each coming mini-batch, performing the model optimization and the label generation simultaneously. A new hierarchical cluster dynamics is built for the label update, where cluster merge and cluster split are driven by a possibility computed by the label propagation. Our method can achieve better pseudo labels and higher reid accuracy. Extensive experiments on Market-to-Duke, Duke-to-Market, MSMT-to-Market, MSMT-to-Duke, Market-to-MSMT, and Duke-to-MSMT verify the effectiveness of our proposed method.},
  archive   = {C_ICCV},
  author    = {Yi Zheng and Shixiang Tang and Guolong Teng and Yixiao Ge and Kaijian Liu and Jing Qin and Donglian Qi and Dapeng Chen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00826},
  pages     = {8351-8361},
  title     = {Online pseudo label generation by hierarchical cluster dynamics for adaptive person re-identification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised dense deformation embedding network for
template-free shape correspondence. <em>ICCV</em>, 8341–8350. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Shape correspondence from 3D deformation learning has attracted appealing academy interests recently. Nevertheless, current deep learning based methods require the supervision of dense annotations to learn per-point translations, which severely over-parameterize the deformation process. Moreover, they fail to capture local geometric details of original shape via global feature embedding. To address these challenges, we develop a new Unsupervised Dense Deformation Embedding Network (i.e., UD 2 E-Net), which learns to predict deformations between non-rigid shapes from dense local features. Since it is non-trivial to match deformation-variant local features for deformation prediction, we develop an Extrinsic-Intrinsic Autoencoder to first encode extrinsic geometric features from source into intrinsic coordinates in a shared canonical shape, with which the decoder then synthesizes corresponding target features. Moreover, a bounded maximum mean discrepancy loss is developed to mitigate the distribution divergence between the synthesized and original features. To learn natural deformation without dense supervision, we introduce a coarse parameterized deformation graph, for which a novel trace and propagation algorithm is proposed to improve both the quality and efficiency of the deformation. Our UD 2 E-Net outperforms state-of-the-art unsupervised methods by 24\% on Faust Inter challenge and even supervised methods by 13\% on Faust Intra challenge.},
  archive   = {C_ICCV},
  author    = {Ronghan Chen and Yang Cong and Jiahua Dong},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00825},
  pages     = {8341-8350},
  title     = {Unsupervised dense deformation embedding network for template-free shape correspondence},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Keep CALM and improve visual feature attribution.
<em>ICCV</em>, 8330–8340. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The class activation mapping, or CAM, has been the cornerstone of feature attribution methods for multiple vision tasks. Its simplicity and effectiveness have led to wide applications in the explanation of visual predictions and weakly-supervised localization tasks. However, CAM has its own shortcomings. The computation of attribution maps relies on ad-hoc calibration steps that are not part of the training computational graph, making it difficult for us to understand the real meaning of the attribution values. In this paper, we improve CAM by explicitly incorporating a la-tent variable encoding the location of the cue for recognition in the formulation, thereby subsuming the attribution map into the training computational graph. The resulting model, class activation latent mapping, or CALM, is trained with the expectation-maximization algorithm. Our experiments show that CALM identifies discriminative attributes for image classifiers more accurately than CAM and other visual attribution baselines. CALM also shows performance improvements over prior arts on the weakly-supervised object localization benchmarks. Our code is available at https://github.com/naver-ai/calm.},
  archive   = {C_ICCV},
  author    = {Jae Myung Kim and Junsuk Choe and Zeynep Akata and Seong Joon Oh},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00824},
  pages     = {8330-8340},
  title     = {Keep CALM and improve visual feature attribution},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The many faces of robustness: A critical analysis of
out-of-distribution generalization. <em>ICCV</em>, 8320–8329. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce four new real-world distribution shift datasets consisting of changes in image style, image blurriness, geographic location, camera operation, and more. With our new datasets, we take stock of previously proposed methods for improving out-of-distribution robustness and put them to the test. We find that using larger models and artificial data augmentations can improve robustness on real-world distribution shifts, contrary to claims in prior work. We find improvements in artificial robustness benchmarks can transfer to real-world distribution shifts, contrary to claims in prior work. Motivated by our observation that data augmentations can help with real-world distribution shifts, we also introduce a new data augmentation method which advances the state-of-the-art and outperforms models pre-trained with 1000× more labeled data. Overall we find that some methods consistently help with distribution shifts in texture and local image statistics, but these methods do not help with some other distribution shifts like geographic changes. Our results show that future research must study multiple distribution shifts simultaneously, as we demonstrate that no evaluated method consistently improves robustness.},
  archive   = {C_ICCV},
  author    = {Dan Hendrycks and Steven Basart and Norman Mu and Saurav Kadavath and Frank Wang and Evan Dorundo and Rahul Desai and Tyler Zhu and Samyak Parajuli and Mike Guo and Dawn Song and Jacob Steinhardt and Justin Gilmer},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00823},
  pages     = {8320-8329},
  title     = {The many faces of robustness: A critical analysis of out-of-distribution generalization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DRÆM – a discriminatively trained reconstruction embedding
for surface anomaly detection. <em>ICCV</em>, 8310–8319. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual surface anomaly detection aims to detect local image regions that significantly deviate from normal appearance. Recent surface anomaly detection methods rely on generative models to accurately reconstruct the normal areas and to fail on anomalies. These methods are trained only on anomaly-free images, and often require hand-crafted post-processing steps to localize the anomalies, which prohibits optimizing the feature extraction for maximal detection capability. In addition to reconstructive approach, we cast surface anomaly detection primarily as a discriminative problem and propose a discriminatively trained reconstruction anomaly embedding model (DRÆM). The proposed method learns a joint representation of an anomalous image and its anomaly-free reconstruction, while simultaneously learning a decision boundary between normal and anomalous examples. The method enables direct anomaly localization without the need for additional complicated post-processing of the network output and can be trained using simple and general anomaly simulations. On the challenging MVTec anomaly detection dataset, DRÆM outperforms the current state-of-the-art unsupervised methods by a large margin and even de-livers detection performance close to the fully-supervised methods on the widely used DAGM surface-defect detection dataset, while substantially outperforming them in localization accuracy. Code at github.com/VitjanZ/DRAEM.},
  archive   = {C_ICCV},
  author    = {Vitjan Zavrtanik and Matej Kristan and Danijel Skočaj},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00822},
  pages     = {8310-8319},
  title     = {DRÆM – a discriminatively trained reconstruction embedding for surface anomaly detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NAS-OoD: Neural architecture search for out-of-distribution
generalization. <em>ICCV</em>, 8300–8309. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances on Out-of-Distribution (OoD) generalization reveal the robustness of deep learning models against distribution shifts. However, existing works focus on OoD algorithms, such as invariant risk minimization, domain generalization, or stable learning, without considering the influence of deep model architectures on OoD generalization, which may lead to sub-optimal performance. Neural Architecture Search (NAS) methods search for architecture based on its performance on the training data, which may result in poor generalization for OoD tasks. In this work, we propose robust Neural Architecture Search for OoD generalization (NAS-OoD), which optimizes the architecture with respect to its performance on generated OoD data by gradient descent. Specifically, a data generator is learned to synthesize OoD data by maximizing losses computed by different neural architectures, while the goal for architecture search is to find the optimal architecture parameters that minimize the synthetic OoD data losses. The data generator and the neural architecture are jointly optimized in an end-to-end manner, and the minimax training process effectively discovers robust architectures that generalize well for different distribution shifts. Extensive experimental results show that NAS-OoD achieves superior performance on various OoD generalization benchmarks with deep models having a much fewer number of parameters. In addition, on a real industry dataset, the proposed NAS-OoD method reduces the error rate by more than 70\% compared with the state-of-the-art method, demonstrating the proposed method’s practicality for real applications.},
  archive   = {C_ICCV},
  author    = {Haoyue Bai and Fengwei Zhou and Lanqing Hong and Nanyang Ye and S.-H. Gary Chan and Zhenguo Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00821},
  pages     = {8300-8309},
  title     = {NAS-OoD: Neural architecture search for out-of-distribution generalization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Trash to treasure: Harvesting OOD data with cross-modal
matching for open-set semi-supervised learning. <em>ICCV</em>,
8290–8299. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Open-set semi-supervised learning (open-set SSL) investigates a challenging but practical scenario where out-of-distribution (OOD) samples are contained in the unlabeled data. While the mainstream technique seeks to completely filter out the OOD samples for semi-supervised learning (SSL), we propose a novel training mechanism that could effectively exploit the presence of OOD data for enhanced feature learning while avoiding its adverse impact on the SSL. We achieve this goal by first introducing a warm-up training that leverages all the unlabeled data, including both the in-distribution (ID) and OOD samples. Specifically, we perform a pretext task that enforces our feature extractor to obtain a high-level semantic understanding of the training images, leading to more discriminative features that can benefit the downstream tasks. Since the OOD samples are inevitably detrimental to SSL, we propose a novel cross-modal matching strategy to detect OOD samples. Instead of directly applying binary classification [39], we train the network to predict whether the data sample is matched to an assigned one-hot class label. The appeal of the proposed cross-modal matching over binary classification is the ability to generate a compatible feature space that aligns with the core classification task. Extensive experiments show that our approach substantially lifts the performance on open-set SSL and outperforms the state-of-the-art by a large margin.},
  archive   = {C_ICCV},
  author    = {Junkai Huang and Chaowei Fang and Weikai Chen and Zhenhua Chai and Xiaolin Wei and Pengxu Wei and Liang Lin and Guanbin Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00820},
  pages     = {8290-8299},
  title     = {Trash to treasure: Harvesting OOD data with cross-modal matching for open-set semi-supervised learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Semantically coherent out-of-distribution detection.
<em>ICCV</em>, 8281–8289. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current out-of-distribution (OOD) detection benchmarks are commonly built by defining one dataset as in-distribution (ID) and all others as OOD. However, these benchmarks unfortunately introduce some unwanted and impractical goals, e.g., to perfectly distinguish CIFAR dogs from ImageNet dogs, even though they have the same semantics and negligible covariate shifts. These unrealistic goals will result in an extremely narrow range of model capabilities, greatly limiting their use in real applications. To overcome these drawbacks, we re-design the benchmarks and propose the semantically coherent out-of-distribution detection (SC-OOD). On the SC-OOD benchmarks, existing methods suffer from large performance degradation, suggesting that they are extremely sensitive to low-level discrepancy between data sources while ignoring their inherent semantics. To develop an effective SC-OOD detection approach, we leverage an external unlabeled set and design a concise framework featured by unsupervised dual grouping (UDG) for the joint modeling of ID and OOD data. The proposed UDG can not only enrich the semantic knowledge of the model by exploiting unlabeled data in an unsupervised manner, but also distinguish ID/OOD samples to enhance ID classification and OOD detection tasks simultaneously. Extensive experiments demonstrate that our approach achieves the state-of-the-art performance on SC-OOD benchmarks. Code and benchmarks are provided on our project page: https://jingkang50.github.io/projects/scood.},
  archive   = {C_ICCV},
  author    = {Jingkang Yang and Haoqi Wang and Litong Feng and Xiaopeng Yan and Huabin Zheng and Wayne Zhang and Ziwei Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00819},
  pages     = {8281-8289},
  title     = {Semantically coherent out-of-distribution detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Task switching network for multi-task learning.
<em>ICCV</em>, 8271–8280. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce Task Switching Networks (TSNs), a task-conditioned architecture with a single unified encoder/decoder for efficient multi-task learning. Multiple tasks are performed by switching between them, performing one task at a time. TSNs have a constant number of parameters irrespective of the number of tasks. This scalable yet conceptually simple approach circumvents the overhead and intricacy of task-specific network components in existing works. In fact, we demonstrate for the first time that multi-tasking can be performed with a single task-conditioned decoder. We achieve this by learning task-specific conditioning parameters through a jointly trained task embedding network, encouraging constructive interaction between tasks. Experiments validate the effectiveness of our approach, achieving state-of-the-art results on two challenging multi-task benchmarks, PASCAL-Context and NYUD. Our analysis of the learned task embeddings further indicates a connection to task relationships studied in the recent literature.},
  archive   = {C_ICCV},
  author    = {Guolei Sun and Thomas Probst and Danda Pani Paudel and Nikola Popović and Menelaos Kanakis and Jagruti Patel and Dengxin Dai and Luc Van Gool},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00818},
  pages     = {8271-8280},
  title     = {Task switching network for multi-task learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online continual learning with natural distribution shifts:
An empirical study with visual data. <em>ICCV</em>, 8261–8270. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Continual learning is the problem of learning and retaining knowledge through time over multiple tasks and environments. Research has primarily focused on the incremental classification setting, where new tasks/classes are added at discrete time intervals. Such an &quot;offline&quot; setting does not evaluate the ability of agents to learn effectively and efficiently, since an agent can perform multiple learning epochs without any time limitation when a task is added. We argue that &quot;online&quot; continual learning, where data is a single continuous stream without task boundaries, enables evaluating both information retention and online learning efficacy. In online continual learning, each incoming small batch of data is first used for testing and then added to the training set, making the problem truly online. Trained models are later evaluated on historical data to assess information retention. We introduce a new benchmark for online continual visual learning that exhibits large scale and natural distribution shifts. Through a large-scale analysis, we identify critical and previously unobserved phenomena of gradient-based optimization in continual learning, and propose effective strategies for improving gradient-based online continual learning with real data. The source code and dataset are available in: https://github.com/IntelLabs/continuallearning.},
  archive   = {C_ICCV},
  author    = {Zhipeng Cai and Ozan Sener and Vladlen Koltun},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00817},
  pages     = {8261-8270},
  title     = {Online continual learning with natural distribution shifts: An empirical study with visual data},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Exploring inter-channel correlation for diversity-preserved
knowledge distillation. <em>ICCV</em>, 8251–8260. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Knowledge Distillation has shown very promising ability in transferring learned representation from the larger model (teacher) to the smaller one (student). Despite many efforts, prior methods ignore the important role of retaining inter-channel correlation of features, leading to the lack of capturing intrinsic distribution of the feature space and sufficient diversity properties of features in the teacher network. To solve the issue, we propose the novel Inter-Channel Correlation for Knowledge Distillation (ICKD), with which the diversity and homology of the feature space of the student network can align with that of the teacher network. The correlation between these two channels is interpreted as diversity if they are irrelevant to each other, otherwise homology. Then the student is required to mimic the correlation within its own embedding space. In addition, we introduce the grid-level inter-channel correlation, making it capable of dense prediction tasks. Extensive experiments on two vision tasks, including ImageNet classification and Pascal VOC segmentation, demonstrate the superiority of our ICKD, which consistently outperforms many existing methods, advancing the state-of-the-art in the fields of Knowledge Distillation. To our knowledge, we are the first method based on knowledge distillation boosts ResNet18 beyond 72\% Top-1 accuracy on ImageNet classification. Code is available at: https://github.com/ADLab-AutoDrive/ICKD.},
  archive   = {C_ICCV},
  author    = {Li Liu and Qingle Huang and Sihao Lin and Hongwei Xie and Bing Wang and Xiaojun Chang and Xiaodan Liang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00816},
  pages     = {8251-8260},
  title     = {Exploring inter-channel correlation for diversity-preserved knowledge distillation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CCT-net: Category-invariant cross-domain transfer for
medical single-to-multiple disease diagnosis. <em>ICCV</em>, 8240–8250.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A medical imaging model is usually explored for the diagnosis of a single disease. However, with the expanding demand for multi-disease diagnosis in clinical applications, multi-function solutions need to be investigated. Previous works proposed to either exploit different disease labels to conduct transfer learning through fine-tuning, or transfer knowledge across different domains with similar diseases. However, these methods still cannot address the real clinical challenge - a multi-disease model is required but annotations for each disease are not always available. In this paper, we introduce the task of transferring knowledge from single-disease diagnosis (source domain) to enhance multi-disease diagnosis (target domain). A category-invariant cross-domain transfer (CCT) method is proposed to address this single-to-multiple extension. First, for domain-specific task learning, we present a confidence weighted pooling (CWP) to obtain coarse heatmaps for different disease categories. Then, conditioned on these heatmaps, category-invariant feature refinement (CIFR) blocks are proposed to better localize discriminative semantic regions related to the corresponding diseases. The category-invariant characteristic enables transferability from the source domain to the target domain. We validate our method in two popular areas: extending diabetic retinopathy to identifying multiple ocular diseases, and extending glioma identification to the diagnosis of other brain tumors.},
  archive   = {C_ICCV},
  author    = {Yi Zhou and Lei Huang and Tao Zhou and Ling Shao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00815},
  pages     = {8240-8250},
  title     = {CCT-net: Category-invariant cross-domain transfer for medical single-to-multiple disease diagnosis},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Continual prototype evolution: Learning online from
non-stationary data streams. <em>ICCV</em>, 8230–8239. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Attaining prototypical features to represent class distributions is well established in representation learning. However, learning prototypes online from streaming data proves a challenging endeavor as they rapidly become outdated, caused by an ever-changing parameter space during the learning process. Additionally, continual learning does not assume the data stream to be stationary, typically resulting in catastrophic forgetting of previous knowledge. As a first, we introduce a system addressing both problems, where prototypes evolve continually in a shared latent space, enabling learning and prediction at any point in time. To facilitate learning, a novel objective function synchronizes the latent space with the continually evolving prototypes. In contrast to the major body of work in continual learning, data streams are processed in an online fashion without task information and can be highly imbalanced, for which we propose an efficient memory scheme. As an additional contribution, we propose the learner-evaluator framework that i) generalizes existing paradigms in continual learning, ii) introduces data incremental learning, and iii) models the bridge between continual learning and concept drift. We obtain state-of-the-art performance by a significant margin on eight benchmarks, including three highly imbalanced data streams. Code is publicly available. 1},
  archive   = {C_ICCV},
  author    = {Matthias De Lange and Tinne Tuytelaars},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00814},
  pages     = {8230-8239},
  title     = {Continual prototype evolution: Learning online from non-stationary data streams},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-supervised video representation learning with
meta-contrastive network. <em>ICCV</em>, 8219–8229. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-supervised learning has been successfully applied to pre-train video representations, which aims at efficient adaptation from pre-training domain to downstream tasks. Existing approaches merely leverage contrastive loss to learn instance-level discrimination. However, lack of category information will lead to hard-positive problem that constrains the generalization ability of this kind of methods. We find that the multi-task process of meta learning can provide a solution to this problem. In this paper, we propose a Meta-Contrastive Network (MCN), which combines the contrastive learning and meta learning, to enhance the learning ability of existing self-supervised approaches. Our method contains two training stages based on model-agnostic meta learning (MAML), each of which consists of a contrastive branch and a meta branch. Extensive evaluations demonstrate the effectiveness of our method. For two downstream tasks, i.e., video action recognition and video retrieval, MCN outperforms state-of-the-art approaches on UCF101 and HMDB51 datasets. To be more specific, with R(2+1)D backbone, MCN achieves Top-1 accuracies of 84.8\% and 54.5\% for video action recognition, as well as 52.5\% and 23.7\% for video retrieval.},
  archive   = {C_ICCV},
  author    = {Yuanze Lin and Xun Guo and Yan Lu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00813},
  pages     = {8219-8229},
  title     = {Self-supervised video representation learning with meta-contrastive network},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A simple baseline for semi-supervised semantic segmentation
with strong data augmentation. <em>ICCV</em>, 8209–8218. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, significant progress has been made on semantic segmentation. However, the success of supervised semantic segmentation typically relies on a large amount of labeled data, which is time-consuming and costly to obtain. Inspired by the success of semi-supervised learning methods for image classification, here we propose a simple yet effective semi-supervised learning framework for semantic segmentation. We demonstrate that the devil is in the details: a set of simple designs and training techniques can collectively improve the performance of semi-supervised semantic segmentation significantly. Previous works [3], [25] fail to effectively employ strong augmentation in pseudo-label learning, as the large distribution disparity caused by strong augmentation harms the batch nor-malization statistics. We design a new batch normalization, namely distribution-specific batch normalization (DSBN) to address this problem and show the importance of strong augmentation for semantic segmentation. Moreover, we design a self-correction loss, which is effective in terms of noise resistance. We conduct a series of ablation studies to show the effectiveness of each component. Our method achieves state-of-the-art results in the semi-supervised settings on the Cityscapes and Pascal VOC datasets.},
  archive   = {C_ICCV},
  author    = {Jianlong Yuan and Yifan Liu and Chunhua Shen and Zhibin Wang and Hao Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00812},
  pages     = {8209-8218},
  title     = {A simple baseline for semi-supervised semantic segmentation with strong data augmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semi-supervised semantic segmentation with pixel-level
contrastive learning from a class-wise memory bank. <em>ICCV</em>,
8199–8208. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents a novel approach for semi-supervised semantic segmentation. The key element of this approach is our contrastive learning module that enforces the segmentation network to yield similar pixel-level feature representations for same-class samples across the whole dataset. To achieve this, we maintain a memory bank which is continuously updated with relevant and high-quality feature vectors from labeled data. In an end-to-end training, the features from both labeled and unlabeled data are optimized to be similar to same-class samples from the memory bank. Our approach not only outperforms the current state-of-the-art for semi-supervised semantic segmentation but also for semi-supervised domain adaptation on well-known public benchmarks, with larger improvements on the most challenging scenarios, i.e., less available labeled data. Code is available at https://github.com/Shathe/SemiSeg-Contrastive},
  archive   = {C_ICCV},
  author    = {Iñigo Alonso and Alberto Sabater and David Ferstl and Luis Montesano and Ana C. Murillo},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00811},
  pages     = {8199-8208},
  title     = {Semi-supervised semantic segmentation with pixel-level contrastive learning from a class-wise memory bank},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). GistNet: A geometric structure transfer network for
long-tailed recognition. <em>ICCV</em>, 8189–8198. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The problem of long-tailed recognition, where the number of examples per class is highly unbalanced, is considered. It is hypothesized that the well known tendency of standard classifier training to overfit to popular classes can be exploited for effective transfer learning. Rather than eliminating this overfitting, e.g. by adopting popular class-balanced sampling methods, the learning algorithm should instead leverage this overfitting to transfer geometric information from popular to low-shot classes. A new classifier architecture, GistNet, is proposed to support this goal, using constellations of classifier parameters to encode the class geometry. A new learning algorithm is then proposed for GeometrIc Structure Transfer (GIST), with resort to a combination of loss functions that combine class-balanced and random sampling to guarantee that, while overfitting to the popular classes is restricted to geometric parameters, it is leveraged to transfer class geometry from popular to few-shot classes. This enables better generalization for few-shot classes without the need for the manual specification of class weights, or even the explicit grouping of classes into different types. Experiments on two popular long-tailed recognition datasets show that GistNet outperforms existing solutions to this problem.},
  archive   = {C_ICCV},
  author    = {Bo Liu and Haoxiang Li and Hao Kang and Gang Hua and Nuno Vasconcelos},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00810},
  pages     = {8189-8198},
  title     = {GistNet: A geometric structure transfer network for long-tailed recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parallel detection-and-segmentation learning for weakly
supervised instance segmentation. <em>ICCV</em>, 8178–8188. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Weakly supervised instance segmentation (WSIS) with only image-level labels has recently drawn much attention. To date, bottom-up WSIS methods refine discriminative cues from classifiers with sophisticated multi-stage training procedures, which also suffer from inconsistent object boundaries. And top-down WSIS methods are formulated as cascade detection-to-segmentation pipeline, in which the quality of segmentation learning heavily depends on pseudo masks generated from detectors. In this paper, we propose a unified parallel detection-and-segmentation learning (PDSL) framework to learn instance segmentation with only image-level labels, which draws inspiration from both top-down and bottom-up instance segmentation approaches. The detection module is the same as the typical design of any weakly supervised object detection, while the segmentation module leverages self-supervised learning to model class-agnostic foreground extraction, following by self-training to refine class-specific segmentation. We further design instance-activation correlation module to improve the coherence between detection and segmentation branches. Extensive experiments verify that the proposed method outperforms baselines and achieves the state-of-the-art results on PASCAL VOC and MS COCO.},
  archive   = {C_ICCV},
  author    = {Yunhang Shen and Liujuan Cao and Zhiwei Chen and Baochang Zhang and Chi Su and Yongjian Wu and Feiyue Huang and Rongrong Ji},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00809},
  pages     = {8178-8188},
  title     = {Parallel detection-and-segmentation learning for weakly supervised instance segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Frequency-aware spatiotemporal transformers for video
inpainting detection. <em>ICCV</em>, 8168–8177. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a Frequency-Aware Spatiotemporal Transformer (FAST) for video inpainting detection, which aims to simultaneously mine the traces of video in-painting from spatial, temporal, and frequency domains. Unlike existing deep video inpainting detection methods that usually rely on hand-designed attention modules and memory mechanism, our proposed FAST have innate global self-attention mechanisms to capture the long-range relations. While existing video inpainting methods usually exploit the spatial and temporal connections in a video, our method employs a spatiotemporal transformer framework to detect the spatial connections between patches and temporal dependency between frames. As the inpainted videos usually lack high frequency details, our proposed FAST synchronously exploits the frequency domain information with a specifically designed decoder. Extensive experimental results demonstrate that our approach achieves very competitive performance and generalizes well.},
  archive   = {C_ICCV},
  author    = {Bingyao Yu and Wanhua Li and Xiu Li and Jiwen Lu and Jie Zhou},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00808},
  pages     = {8168-8177},
  title     = {Frequency-aware spatiotemporal transformers for video inpainting detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Watch only once: An end-to-end video action detection
framework. <em>ICCV</em>, 8158–8167. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose an end-to-end pipeline, named Watch Once Only (WOO), for video action detection. Current methods either decouple video action detection task into separated stages of actor localization and action classification or train two separated models within one stage. In contrast, our approach solves the actor localization and action classification simultaneously in a unified network. The whole pipeline is significantly simplified by unifying the backbone network and eliminating many hand-crafted components. WOO takes a unified video backbone to simultaneously extract features for actor location and action classification. In addition, we introduce spatial-temporal action embeddings into our framework and design a spatial-temporal fusion module to obtain more discriminative features with richer information, which further boosts the action classification performance. Extensive experiments on AVA and JHMDB datasets show that WOO achieves state-of-the-art performance, while still reduces up to 16.7\% GFLOPs compared with existing methods. We hope our work can inspire re-thinking the convention of action detection and serve as a solid baseline for end-to-end action detection. Code is available at https://github.com/ShoufaChen/WOO.},
  archive   = {C_ICCV},
  author    = {Shoufa Chen and Peize Sun and Enze Xie and Chongjian Ge and Jiannan Wu and Lan Ma and Jiajun Shen and Ping Luo},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00807},
  pages     = {8158-8167},
  title     = {Watch only once: An end-to-end video action detection framework},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interactive prototype learning for egocentric action
recognition. <em>ICCV</em>, 8148–8157. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Egocentric video recognition is a challenging task that requires to identify both the actor’s motion and the active object that the actor interacts with. Recognizing the active object is particularly hard due to the cluttered background with distracting objects, the frequent field of view changes, severe occlusion, etc. To improve the active object classification, most existing methods use object detectors or human gaze information, which are computationally expensive or require labor-intensive annotations. To avoid these additional costs, we propose an end-to-end Interactive Prototype Learning (IPL) framework to learn better active object representations by leveraging the motion cues from the actor. First, we introduce a set of verb prototypes to disentangle active object features from distracting object features. Each prototype corresponds to a primary motion pattern of an egocentric action, offering a distinctive supervision signal for active object feature learning. Second, we design two interactive operations to enable the extraction of active object features, i.e., noun-to-verb assignment and verb-to-noun selection. These operations are parameter-efficient and can learn judicious location-aware features on top of 3D CNN backbones. We demonstrate that the IPL framework can generalize to different backbones and outperform the state-of-the-art on three large-scale egocentric video datasets, i.e., EPIC-KITCHENS-55, EPIC-KITCHENS-100 and EGTEA.},
  archive   = {C_ICCV},
  author    = {Xiaohan Wang and Linchao Zhu and Heng Wang and Yi Yang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00806},
  pages     = {8148-8157},
  title     = {Interactive prototype learning for egocentric action recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HighlightMe: Detecting highlights from human-centric videos.
<em>ICCV</em>, 8137–8147. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a domain- and user-preference-agnostic approach to detect highlightable excerpts from human-centric videos. Our method works on the graph-based representation of multiple observable human-centric modalities in the videos, such as poses and faces. We use an autoencoder network equipped with spatial-temporal graph convolutions to detect human activities and interactions based on these modalities. We train our network to map the activity- and interaction-based latent structural representations of the different modalities to per-frame highlight scores based on the representativeness of the frames. We use these scores to compute which frames to highlight and stitch contiguous frames to produce the excerpts. We train our network on the large-scale AVA-Kinetics action dataset and evaluate it on four benchmark video highlight datasets: DSH, TVSum, PHD 2 , and SumMe. We observe a 4–12\% improvement in the mean average precision of matching the human-annotated highlights over state-of-the-art methods in these datasets, without requiring any user-provided preferences or dataset-specific fine-tuning.},
  archive   = {C_ICCV},
  author    = {Uttaran Bhattacharya and Gang Wu and Stefano Petrangeli and Viswanathan Swaminathan and Dinesh Manocha},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00805},
  pages     = {8137-8147},
  title     = {HighlightMe: Detecting highlights from human-centric videos},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attention is not enough: Mitigating the distribution
discrepancy in asynchronous multimodal sequence fusion. <em>ICCV</em>,
8128–8136. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Videos flow as the mixture of language, acoustic, and vision modalities. A thorough video understanding needs to fuse time-series data of different modalities for prediction. Due to the variable receiving frequency for sequences from each modality, there usually exists inherent asynchrony across the collected multimodal streams. Towards an efficient multimodal fusion from asynchronous multimodal streams, we need to model the correlations between elements from different modalities. The recent Multimodal Transformer (MulT) approach extends the self-attention mechanism of the original Transformer network to learn the crossmodal dependencies between elements. However, the direct replication of self-attention will suffer from the distribution mismatch across different modality features. As a result, the learnt crossmodal dependencies can be unreliable. Motivated by this observation, this work proposes the Modality-Invariant Crossmodal Attention (MICA) approach towards learning crossmodal interactions over modality-invariant space in which the distribution mismatch between different modalities is well bridged. To this end, both the marginal distribution and the elements with high-confidence correlations are aligned over the common space of the query and key vectors which are computed from different modalities. Experiments on three standard benchmarks of multimodal video understanding clearly validate the superiority of our approach.},
  archive   = {C_ICCV},
  author    = {Tao Liang and Guosheng Lin and Lei Feng and Yan Zhang and Fengmao Lv},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00804},
  pages     = {8128-8136},
  title     = {Attention is not enough: Mitigating the distribution discrepancy in asynchronous multimodal sequence fusion},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TF-blender: Temporal feature blender for video object
detection. <em>ICCV</em>, 8118–8127. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Video objection detection is a challenging task because isolated video frames may encounter appearance deterioration, which introduces great confusion for detection. One of the popular solutions is to exploit the temporal information and enhance per-frame representation through aggregating features from neighboring frames. Despite achieving improvements in detection, existing methods focus on the selection of higher-level video frames for aggregation rather than modeling lower-level temporal relations to increase the feature representation. To address this limitation, we propose a novel solution named TF-Blender, which includes three modules: 1) Temporal relation models the relations between the current frame and its neigh-boring frames to preserve spatial information. 2). Feature adjustment enriches the representation of every neigh-boring feature map; 3) Feature blender combines outputs from the first two modules and produces stronger features for the later detection tasks. For its simplicity, TF-Blender can be effortlessly plugged into any detection network to improve detection behavior. Extensive evaluations on ImageNet VID and YouTube-VIS benchmarks indicate the performance guarantees of using TF-Blender on recent state-of-the-art methods. Code is available at https://github.com/goodproj13/TF-Blender.},
  archive   = {C_ICCV},
  author    = {Yiming Cui and Liqi Yan and Zhiwen Cao and Dongfang Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00803},
  pages     = {8118-8127},
  title     = {TF-blender: Temporal feature blender for video object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint visual and audio learning for video highlight
detection. <em>ICCV</em>, 8107–8117. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In video highlight detection, the goal is to identify the interesting moments within an unedited video. Although the audio component of the video provides important cues for highlight detection, the majority of existing efforts focus almost exclusively on the visual component. In this paper, we argue that both audio and visual components of a video should be modeled jointly to retrieve its best moments. To this end, we propose an audio-visual network for video highlight detection. At the core of our approach lies a bimodal attention mechanism, which captures the interaction between the audio and visual components of a video, and produces fused representations to facilitate highlight detection. Furthermore, we introduce a noise sentinel technique to adaptively discount a noisy visual or audio modality. Empirical evaluations on two benchmark datasets demonstrate the superior performance of our approach over the state-of-the-art methods.},
  archive   = {C_ICCV},
  author    = {Taivanbat Badamdorj and Mrigank Rochan and Yang Wang and Li Cheng},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00802},
  pages     = {8107-8117},
  title     = {Joint visual and audio learning for video highlight detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unified graph structured models for video understanding.
<em>ICCV</em>, 8097–8106. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate video understanding involves reasoning about the relationships between actors, objects and their environment, often over long temporal intervals. In this paper, we propose a message passing graph neural network that explicitly models these spatio-temporal relations and can use explicit representations of objects, when supervision is available, and implicit representations otherwise. Our formulation generalises previous structured models for video understanding, and allows us to study how different design choices in graph structure and representation affect the model’s performance. We demonstrate our method on two different tasks requiring relational reasoning in videos – spatio-temporal action detection on AVA and UCF101-24, and video scene graph classification on the recent Action Genome dataset – and achieve state-of-the-art results on all three datasets. Furthermore, we show quantitatively and qualitatively how our method is able to more effectively model relationships between relevant entities in the scene.},
  archive   = {C_ICCV},
  author    = {Anurag Arnab and Chen Sun and Cordelia Schmid},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00801},
  pages     = {8097-8106},
  title     = {Unified graph structured models for video understanding},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detecting human-object relationships in videos.
<em>ICCV</em>, 8086–8096. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study a crucial problem in video analysis: human-object relationship detection. The majority of previous approaches are developed only for the static image scenario, without incorporating the temporal dynamics so vital to contextualizing human-object relationships. We propose a model with Intra- and Inter-Transformers, enabling joint spatial and temporal reasoning on multiple visual concepts of objects, relationships, and human poses. We find that applying attention mechanisms among features distributed spatio-temporally greatly improves our understanding of human-object relationships. Our method is validated on two datasets, Action Genome and CAD-120-EVAR, and achieves state-of-the-art performance on both of them.},
  archive   = {C_ICCV},
  author    = {Jingwei Ji and Rishi Desai and Juan Carlos Niebles},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00800},
  pages     = {8086-8096},
  title     = {Detecting human-object relationships in videos},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ASCNet: Self-supervised video representation learning with
appearance-speed consistency. <em>ICCV</em>, 8076–8085. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study self-supervised video representation learning, which is a challenging task due to 1) lack of labels for explicit supervision; 2) unstructured and noisy visual information. Existing methods mainly use contrastive loss with video clips as the instances and learn visual representation by discriminating instances from each other, but they need a careful treatment of negative pairs by either relying on large batch sizes, memory banks, extra modalities or customized mining strategies, which inevitably includes noisy data. In this paper, we observe that the consistency between positive samples is the key to learn robust video representation. Specifically, we propose two tasks to learn appearance and speed consistency, respectively. The appearance consistency task aims to maximize the similarity between two clips of the same video with different playback speeds. The speed consistency task aims to maximize the similarity between two clips with the same playback speed but different appearance information. We show that optimizing the two tasks jointly consistently improves the performance on downstream tasks, e.g., action recognition and video retrieval. Remarkably, for action recognition on the UCF-101 dataset, we achieve 90.8\% accuracy without using any extra modalities or negative pairs for unsupervised pretraining, which outperforms the ImageNet supervised pretrained model. Codes and models will be available.},
  archive   = {C_ICCV},
  author    = {Deng Huang and Wenhao Wu and Weiwen Hu and Xu Liu and Dongliang He and Zhihua Wu and Xiangmiao Wu and Mingkui Tan and Errui Ding},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00799},
  pages     = {8076-8085},
  title     = {ASCNet: Self-supervised video representation learning with appearance-speed consistency},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weakly-supervised action segmentation and alignment via
transcript-aware union-of-subspaces learning. <em>ICCV</em>, 8065–8075.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the problem of learning to segment actions from weakly-annotated videos, i.e., videos accompanied by transcripts (ordered list of actions). We propose a framework in which we model actions with a union of low-dimensional subspaces, learn the subspaces using transcripts and refine video features that lend themselves to action subspaces. To do so, we design an architecture consisting of a Union-of-Subspaces Network, which is an ensemble of autoencoders, each modeling a low-dimensional action subspace and can capture variations of an action within and across videos. For learning, at each iteration, we generate positive and negative soft alignment matrices using the segmentations from the previous iteration, which we use for discriminative training of our model. To regularize the learning, we introduce a constraint loss that prevents imbalanced segmentations and enforces relatively similar duration of each action across videos. To have a real-time inference, we develop a hierarchical segmentation framework that uses subset selection to find representative transcripts and hierarchically align a test video with increasingly refined representative transcripts. Our experiments on three datasets show that our method improves the state-of-the-art action segmentation and alignment, while speeding up the inference time by a factor of 4 to 13. 1},
  archive   = {C_ICCV},
  author    = {Zijia Lu and Ehsan Elhamifar},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00798},
  pages     = {8065-8075},
  title     = {Weakly-supervised action segmentation and alignment via transcript-aware union-of-subspaces learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generic event boundary detection: A benchmark for event
segmentation. <em>ICCV</em>, 8055–8064. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel task together with a new benchmark for detecting generic, taxonomy-free event boundaries that segment a whole video into chunks. Conventional work in temporal video segmentation and action detection focuses on localizing pre-defined action categories and thus does not scale to generic videos. Cognitive Science has known since last century that humans consistently segment videos into meaningful temporal chunks. This segmentation happens naturally, without pre-defined event categories and without being explicitly asked to do so. Here, we repeat these cognitive experiments on mainstream CV datasets; with our novel annotation guideline which addresses the complexities of taxonomy-free event boundary annotation, we introduce the task of Generic Event Boundary Detection (GEBD) and the new benchmark Kinetics-GEBD. We view GEBD as an important stepping stone towards understanding the video as a whole, and believe it has been previously neglected due to a lack of proper task definition and annotations. Through experiment and human study we demonstrate the value of the annotations. Further, we benchmark supervised and un-supervised GEBD approaches on the TAPOS dataset and our Kinetics-GEBD. We release our annotations and baseline codes at CVPR’21 LOVEU Challenge: https://sites.google.com/view/loveucvpr21.},
  archive   = {C_ICCV},
  author    = {Mike Zheng Shou and Stan Weixian Lei and Weiyao Wang and Deepti Ghadiyaram and Matt Feiszli},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00797},
  pages     = {8055-8064},
  title     = {Generic event boundary detection: A benchmark for event segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Video object segmentation with dynamic memory networks and
adaptive object alignment. <em>ICCV</em>, 8045–8054. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel solution for object-matching based semi-supervised video object segmentation, where the target object masks in the first frame are provided. Existing object-matching based methods focus on the matching between the raw object features of the current frame and the first/previous frames. However, two issues are still not solved by these object-matching based methods. As the appearance of the video object changes drastically over time, 1) unseen parts/details of the object present in the current frame, resulting in incomplete annotation in the first annotated frame (e.g. view/scale changes). 2) even for the seen parts/details of the object in the current frame, their positions change relatively (e.g. pose changes/camera motion), leading to a misalignment for the object matching. To obtain the complete information of the target object, we propose a novel object-based dynamic memory network that exploits visual contents of all the past frames. To solve the misalignment problem caused by position changes of visual contents, we propose an adaptive object alignment module by incorporating a region translation function that aligns object proposals towards templates in the feature space. Our method achieves state-of-the-art results on latest benchmark datasets DAVIS 2017 ($\mathcal{J}$ of 81.4\% and $\mathcal{F}$ of 87.5\% on the validation set) and YouTube-VOS (the overall score of 82.7\% on the validation set) with a very efficient inference time (0.16 second/frame on DAVIS 2017 validation set). Code is available at: https://github.com/liang4sx/DMN-AOA.},
  archive   = {C_ICCV},
  author    = {Shuxian Liang and Xu Shen and Jianqiang Huang and Xian-Sheng Hua},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00796},
  pages     = {8045-8054},
  title     = {Video object segmentation with dynamic memory networks and adaptive object alignment},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Domain adaptive video segmentation via temporal consistency
regularization. <em>ICCV</em>, 8033–8044. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Video semantic segmentation is an essential task for the analysis and understanding of videos. Recent efforts largely focus on supervised video segmentation by learning from fully annotated data, but the learnt models often experience clear performance drop while applied to videos of a different domain. This paper presents DA-VSN, a domain adaptive video segmentation network that addresses domain gaps in videos by temporal consistency regularization (TCR) for consecutive frames of target-domain videos. DA-VSN consists of two novel and complementary designs. The first is cross-domain TCR that guides the prediction of target frames to have similar temporal consistency as that of source frames (learnt from annotated source data) via adversarial learning. The second is intra-domain TCR that guides unconfident predictions of target frames to have similar temporal consistency as confident predictions of target frames. Extensive experiments demonstrate the superiority of our proposed domain adaptive video segmentation network which outperforms multiple baselines consistently by large margins.},
  archive   = {C_ICCV},
  author    = {Dayan Guan and Jiaxing Huang and Aoran Xiao and Shijian Lu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00795},
  pages     = {8033-8044},
  title     = {Domain adaptive video segmentation via temporal consistency regularization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Crossover learning for fast online video instance
segmentation. <em>ICCV</em>, 8023–8032. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modeling temporal visual context across frames is critical for video instance segmentation (VIS) and other video understanding tasks. In this paper, we propose a fast on-line VIS model termed CrossVIS. For temporal information modeling in VIS, we present a novel crossover learning scheme that uses the instance feature in the current frame to pixel-wisely localize the same instance in other frames. Different from previous schemes, crossover learning does not require any additional network parameters for feature enhancement. By integrating with the instance segmentation loss, crossover learning enables efficient cross-frame instance-to-pixel relation learning and brings cost-free improvement during inference. Besides, a global balanced instance embedding branch is proposed for better and more stable online instance association. We conduct extensive experiments on three challenging VIS benchmarks, i.e., YouTube-VIS-2019, OVIS, and YouTube-VIS-2021 to evaluate our methods. CrossVIS achieves state-of-the-art online VIS performance and shows a decent trade-off between latency and accuracy. Code is available at https://github.com/hustvl/CrossVIS.},
  archive   = {C_ICCV},
  author    = {Shusheng Yang and Yuxin Fang and Xinggang Wang and Yu Li and Chen Fang and Ying Shan and Bin Feng and Wenyu Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00794},
  pages     = {8023-8032},
  title     = {Crossover learning for fast online video instance segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Searching for two-stream models in multivariate space for
video recognition. <em>ICCV</em>, 8013–8022. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Conventional video models rely on a single stream to capture the complex spatial-temporal features. Recent work on two-stream video models, such as SlowFast network and AssembleNet, prescribe separate streams to learn complementary features, and achieve stronger performance. However, manually designing both streams as well as the in-between fusion blocks is a daunting task, requiring to explore a tremendously large design space. Such manual exploration is time-consuming and often ends up with suboptimal architectures when computational resources are limited and the exploration is insufficient. In this work, we present a pragmatic neural architecture search approach, which is able to search for two-stream video models in giant spaces efficiently. We design a multivariate search space, including 6 search variables to capture a wide variety of choices in designing two-stream models. Furthermore, we propose a progressive search procedure, by searching for the architecture of individual streams, fusion blocks and attention blocks one after the other. We demonstrate two-stream models with significantly better performance can be automatically discovered in our design space. Our searched two-stream models, namely Auto-TSNet, consistently outperform other models on standard benchmarks. On Kinetics, compared with the SlowFast model, our Auto-TSNet-L model reduces FLOPS by nearly 11× while achieving the same accuracy 78.9\%. On Something-Something-V2, Auto- TSNet-M improves the accuracy by at least 2\% over other methods which use less than 50 GFLOPS per video.},
  archive   = {C_ICCV},
  author    = {Xinyu Gong and Heng Wang and Zheng Shou and Matt Feiszli and Zhangyang Wang and Zhicheng Yan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00793},
  pages     = {8013-8022},
  title     = {Searching for two-stream models in multivariate space for video recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Temporal action detection with multi-level supervision.
<em>ICCV</em>, 8002–8012. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Training temporal action detection in videos requires large amounts of labeled data, yet such annotation is expensive to collect. Incorporating unlabeled or weakly-labeled data to train action detection model could help reduce annotation cost. In this work, we first introduce the Semi-supervised Action Detection (SSAD) task with a mixture of labeled and unlabeled data and analyze different types of errors in the proposed SSAD baselines which are directly adapted from the semi-supervised classification literature. Identifying that the main source of error is action incompleteness (i.e., missing parts of actions), we alleviate it by designing an unsupervised foreground attention (UFA) module utilizing the conditional independence between foreground and background motion. Then we incorporate weakly-labeled data into SSAD and propose Omni-supervised Action Detection (OSAD) with three levels of supervision. To overcome the accompanying action-context confusion problem in OSAD baselines, an information bottleneck (IB) is designed to suppress the scene information in non-action frames while preserving the action information. We extensively benchmark against the baselines for SSAD and OSAD on our created data splits in THUMOS14 and ActivityNet1.2, and demonstrate the effectiveness of the proposed UFA and IB methods. Lastly, the benefit of our full OSAD-IB model under limited annotation budgets is shown by exploring the optimal annotation strategy for labeled, unlabeled and weakly-labeled data. 1},
  archive   = {C_ICCV},
  author    = {Baifeng Shi and Qi Dai and Judy Hoffman and Kate Saenko and Trevor Darrell and Huijuan Xu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00792},
  pages     = {8002-8012},
  title     = {Temporal action detection with multi-level supervision},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multimodal clustering networks for self-supervised learning
from unlabeled videos. <em>ICCV</em>, 7992–8001. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multimodal self-supervised learning is getting more and more attention as it allows not only to train large networks without human supervision but also to search and retrieve data across various modalities. In this context, this paper proposes a framework that, starting from a pre-trained backbone, learns a common multimodal embedding space that, in addition to sharing representations across different modalities, enforces a grouping of semantically similar instances. To this end, we extend the concept of instance-level contrastive learning with a multimodal clustering step in the training pipeline to capture semantic similarities across modalities. The resulting embedding space enables retrieval of samples across all modalities, even from unseen datasets and different domains. To evaluate our approach, we train our model on the HowTo100M dataset and evaluate its zero-shot retrieval capabilities in two challenging domains, namely text-to-video retrieval, and temporal action localization, showing state-of-the-art results on four different datasets.},
  archive   = {C_ICCV},
  author    = {Brian Chen and Andrew Rouditchenko and Kevin Duarte and Hilde Kuehne and Samuel Thomas and Angie Boggust and Rameswar Panda and Brian Kingsbury and Rogerio Feris and David Harwath and James Glass and Michael Picheny and Shih-Fu Chang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00791},
  pages     = {7992-8001},
  title     = {Multimodal clustering networks for self-supervised learning from unlabeled videos},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Foreground-action consistency network for weakly supervised
temporal action localization. <em>ICCV</em>, 7982–7991. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As a challenging task of high-level video understanding, weakly supervised temporal action localization has been attracting increasing attention. With only video annotations, most existing methods seek to handle this task with a localization-by-classification framework, which generally adopts a selector to select snippets of high probabilities of actions or namely the foreground. Nevertheless, the existing foreground selection strategies have a major limitation of only considering the unilateral relation from foreground to actions, which cannot guarantee the foreground-action consistency. In this paper, we present a framework named FAC-Net based on the I3D backbone, on which three branches are appended, named class-wise foreground classification branch, class-agnostic attention branch and multiple instance learning branch. First, our class-wise foreground classification branch regularizes the relation between actions and foreground to maximize the foreground-background separation. Besides, the class-agnostic attention branch and multiple instance learning branch are adopted to regularize the foreground-action consistency and help to learn a meaningful foreground classifier. Within each branch, we introduce a hybrid attention mechanism, which calculates multiple attention scores for each snippet, to focus on both discriminative and less-discriminative snippets to capture the full action boundaries. Experimental results on THUMOS14 and ActivityNet1.3 demonstrate the state-of-the-art performance of our method.},
  archive   = {C_ICCV},
  author    = {Linjiang Huang and Liang Wang and Hongsheng Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00790},
  pages     = {7982-7991},
  title     = {Foreground-action consistency network for weakly supervised temporal action localization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Enhancing self-supervised video representation learning via
multi-level feature optimization. <em>ICCV</em>, 7970–7981. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The crux of self-supervised video representation learning is to build general features from unlabeled videos. However, most recent works have mainly focused on high-level semantics and neglected lower-level representations and their temporal relationship which are crucial for general video understanding. To address these challenges, this paper proposes a multi-level feature optimization framework to improve the generalization and temporal modeling ability of learned video representations. Concretely, high-level features obtained from naive and prototypical contrastive learning are utilized to build distribution graphs, guiding the process of low-level and mid-level feature learning. We also devise a simple temporal modeling module from multi-level features to enhance motion pattern learning. Experiments demonstrate that multi-level feature optimization with the graph constraint and temporal modeling can greatly improve the representation ability in video understanding. Code is available$here$.},
  archive   = {C_ICCV},
  author    = {Rui Qian and Yuxi Li and Huabin Liu and John See and Shuangrui Ding and Xian Liu and Dian Li and Weiyao Lin},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00789},
  pages     = {7970-7981},
  title     = {Enhancing self-supervised video representation learning via multi-level feature optimization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PR-net: Preference reasoning for personalized video
highlight detection. <em>ICCV</em>, 7960–7969. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Personalized video highlight detection aims to shorten a long video to interesting moments according to a user’s preference, which has recently raised the community’s attention. Current methods regard the user’s history as holistic information to predict the user’s preference but negating the inherent diversity of the user’s interests, resulting in vague preference representation. In this paper, we propose a simple yet efficient preference reasoning framework (PR-Net) to explicitly take the diverse interests into account for frame-level highlight prediction. Specifically, distinct user-specific preferences for each input query frame are produced, presented as the similarity weighted sum of history highlights to the corresponding query frame. Next, distinct comprehensive preferences are formed by the user-specific preferences and a learnable generic preference for more overall highlight measurement. Lastly, the degree of highlight and non-highlight for each query frame is calculated as semantic similarity to its comprehensive and non-highlight preferences, respectively. Besides, to alleviate the ambiguity due to the incomplete annotation, a new bidirectional contrastive loss is proposed to ensure a compact and differentiable metric space. In this way, our method significantly outperforms state-of-the-art methods with a relative improvement of 12\% in mean accuracy precision.},
  archive   = {C_ICCV},
  author    = {Runnan Chen and Penghao Zhou and Wenzhe Wang and Nenglun Chen and Pai Peng and Xing Sun and Wenping Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00788},
  pages     = {7960-7969},
  title     = {PR-net: Preference reasoning for personalized video highlight detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cross-category video highlight detection via set-based
learning. <em>ICCV</em>, 7950–7959. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous highlight detection is crucial for enhancing the efficiency of video browsing on social media platforms. To attain this goal in a data-driven way, one may often face the situation where highlight annotations are not available on the target video category used in practice, while the supervision on another video category (named as source video category) is achievable. In such a situation, one can derive an effective highlight detector on target video category by transferring the highlight knowledge acquired from source video category to the target one. We call this problem cross-category video highlight detection, which has been rarely studied in previous works. For tackling such practical problem, we propose a Dual-Learner-based Video Highlight Detection (DL-VHD) framework. Under this framework, we first design a Set-based Learning module (SL-module) to improve the conventional pair-based learning by assessing the highlight extent of a video segment under a broader context. Based on such learning manner, we introduce two different learners to acquire the basic distinction of target category videos and the characteristics of highlight moments on source video category, respectively. These two types of highlight knowledge are further consolidated via knowledge distillation. Extensive experiments on three benchmark datasets demonstrate the superiority of the proposed SL-module, and the DL-VHD method outperforms five typical Unsupervised Domain Adaptation (UDA) algorithms on various cross-category highlight detection tasks. Our code is available at https://github.com/ChrisAllenMing/Cross_Category_Video_Highlight.},
  archive   = {C_ICCV},
  author    = {Minghao Xu and Hang Wang and Bingbing Ni and Riheng Zhu and Zhenbang Sun and Changhu Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00787},
  pages     = {7950-7959},
  title     = {Cross-category video highlight detection via set-based learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VideoLT: Large-scale long-tailed video recognition.
<em>ICCV</em>, 7940–7949. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Label distributions in real-world are oftentimes long-tailed and imbalanced, resulting in biased models towards dominant labels. While long-tailed recognition has been extensively studied for image classification tasks, limited effort has been made for the video domain. In this paper, we introduce VideoLT, a large-scale long-tailed video recognition dataset, as a step toward real-world video recognition. VideoLT contains 256,218 untrimmed videos, annotated into 1,004 classes with a long-tailed distribution. Through extensive studies, we demonstrate that state-of-the-art methods used for long-tailed image recognition do not perform well in the video domain due to the additional temporal dimension in videos. This motivates us to propose FrameStack, a simple yet effective method for long-tailed video recognition. In particular, FrameStack performs sampling at the frame-level in order to balance class distributions, and the sampling ratio is dynamically determined using knowledge derived from the network during training. Experimental results demonstrate that FrameStack can improve classification performance without sacrificing the overall accuracy. Code and dataset are available at: https://github.com/17Skye17/VideoLT.},
  archive   = {C_ICCV},
  author    = {Xing Zhang and Zuxuan Wu and Zejia Weng and Huazhu Fu and Jingjing Chen and Yu-Gang Jiang and Larry Davis},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00786},
  pages     = {7940-7949},
  title     = {VideoLT: Large-scale long-tailed video recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Temporal cue guided video highlight detection with low-rank
audio-visual fusion. <em>ICCV</em>, 7930–7939. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Video highlight detection plays an increasingly important role in social media content filtering, however, it remains highly challenging to develop automated video highlight detection methods because of the lack of temporal annotations (i.e., where the highlight moments are in long videos) for supervised learning. In this paper, we propose a novel weakly supervised method that can learn to detect highlights by mining video characteristics with video level annotations (topic tags) only. Particularly, we exploit audio-visual features to enhance video representation and take temporal cues into account for improving detection performance. Our contributions are threefold: 1) we propose an audio-visual tensor fusion mechanism that efficiently models the complex association between two modalities while reducing the gap of the heterogeneity between the two modalities; 2) we introduce a novel hierarchical temporal context encoder to embed local temporal clues in between neighboring segments; 3) finally, we alleviate the gradient vanishing problem theoretically during model optimization with attention-gated instance aggregation. Extensive experiments on two benchmark datasets (YouTube Highlights and TVSum) have demonstrated our method outperforms other state-of-the-art methods with remarkable improvements.},
  archive   = {C_ICCV},
  author    = {Qinghao Ye and Xiyue Shen and Yuan Gao and Zirui Wang and Qi Bi and Ping Li and Guang Yang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00785},
  pages     = {7930-7939},
  title     = {Temporal cue guided video highlight detection with low-rank audio-visual fusion},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Contrast and order representations for video self-supervised
learning. <em>ICCV</em>, 7919–7929. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies the problem of learning self-supervised representations on videos. In contrast to image modality that only requires appearance information on objects or scenes, video needs to further explore the relations between multiple frames/clips along the temporal dimension. However, the recent proposed contrastive-based self-supervised frameworks do not grasp such relations explicitly since they simply utilize two augmented clips from the same video and compare their distance without referring to their temporal relation. To address this, we present a contrast-and-order representation (CORP) framework for learning self-supervised video representations that can automatically capture both the appearance information within each frame and temporal information across different frames. In particular, given two video clips, our model first predicts whether they come from the same input video, and then predict the temporal ordering of the clips if they come from the same video. We also propose a novel decoupling attention method to learn symmetric similarity (contrast) and anti-symmetric patterns (order). Such design involves neither extra parameters nor computation, but can speed up the learning process and improve accuracy compared to the vanilla multi-head attention. We extensively validate the representation ability of our learned video features for the downstream action recognition task on Kinetics-400 and Something-something V2. Our method outperforms previous state-of-the-arts by a significant margin.},
  archive   = {C_ICCV},
  author    = {Kai Hu and Jie Shao and Yuan Liu and Bhiksha Raj and Marios Savvides and Zhiqiang Shen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00784},
  pages     = {7919-7929},
  title     = {Contrast and order representations for video self-supervised learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online-trained upsampler for deep low complexity video
compression. <em>ICCV</em>, 7909–7918. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep learning for image and video compression has demonstrated promising results both as a standalone technology and a hybrid combination with existing codecs. However, these systems still come with high computational costs. Deep learning models are typically applied directly in pixel space, making them expensive when resolutions become large.In this work, we propose an online-trained upsampler to augment an existing codec. The upsampler is a small neural network trained on an isolated group of frames. Its parameters are signalled to the decoder. This hybrid solution has a small scope of only 10s or 100s of frames and allows for a low complexity both on the encoding and the decoding side.Our algorithm works in offline and in zero-latency settings. Our evaluation employs the popular x265 codec on several high-resolution datasets ranging from Full HD to 8K. We demonstrate rate savings between 8.6\% and 27.5\% and provide ablation studies to show the impact of our design decisions. In comparison to similar works, our approach performs favourably.},
  archive   = {C_ICCV},
  author    = {Jan P. Klopp and Keng-Chi Liu and Shao-Yi Chien and Liang-Gee Chen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00783},
  pages     = {7909-7918},
  title     = {Online-trained upsampler for deep low complexity video compression},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Group-aware contrastive regression for action quality
assessment. <em>ICCV</em>, 7899–7908. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Assessing action quality is challenging due to the subtle differences between videos and large variations in scores. Most existing approaches tackle this problem by regressing a quality score from a single video, suffering a lot from the large inter-video score variations. In this paper, we show that the relations among videos can provide important clues for more accurate action quality assessment during both training and inference. Specifically, we reformulate the problem of action quality assessment as regressing the relative scores with reference to another video that has shared attributes (e.g., category and difficulty), instead of learning unreferenced scores. Following this formulation, we propose a new Contrastive Regression (CoRe) framework to learn the relative scores by pair-wise comparison, which highlights the differences between videos and guides the models to learn the key hints for assessment. In order to further exploit the relative information between two videos, we devise a group-aware regression tree to convert the conventional score regression into two easier sub-problems: coarse-to-fine classification and regression in small intervals. To demonstrate the effectiveness of CoRe, we conduct extensive experiments on three mainstream AQA datasets including AQA-7, MTL-AQA and JIGSAWS. Our approach outperforms previous methods by a large margin and establishes new state-of-the-art on all three benchmarks.},
  archive   = {C_ICCV},
  author    = {Xumin Yu and Yongming Rao and Wenliang Zhao and Jiwen Lu and Jie Zhou},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00782},
  pages     = {7899-7908},
  title     = {Group-aware contrastive regression for action quality assessment},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sensor-guided optical flow. <em>ICCV</em>, 7888–7898. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a framework to guide an optical flow network with external cues to achieve superior accuracy either on known or unseen domains. Given the availability of sparse yet accurate optical flow hints from an external source, these are injected to modulate the correlation scores computed by a state-of-the-art optical flow network and guide it towards more accurate predictions. Although no real sensor can provide sparse flow hints, we show how these can be obtained by combining depth measurements from active sensors with geometry and hand-crafted optical flow algorithms, leading to accurate enough hints for our purpose. Experimental results with a state-of-the-art flow network on standard benchmarks support the effectiveness of our framework, both in simulated and real conditions.},
  archive   = {C_ICCV},
  author    = {Matteo Poggi and Filippo Aleotti and Stefano Mattoccia},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00781},
  pages     = {7888-7898},
  title     = {Sensor-guided optical flow},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fooling LiDAR perception via adversarial trajectory
perturbation. <em>ICCV</em>, 7878–7887. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {LiDAR point clouds collected from a moving vehicle are functions of its trajectories, because the sensor motion needs to be compensated to avoid distortions. When autonomous vehicles are sending LiDAR point clouds to deep networks for perception and planning, could the motion compensation consequently become a wide-open backdoor in those networks, due to both the adversarial vulnerability of deep learning and GPS-based vehicle trajectory estimation that is susceptible to wireless spoofing? We demonstrate such possibilities for the first time: instead of directly attacking point cloud coordinates which requires tampering with the raw LiDAR readings, only adversarial spoofing of a self-driving car’s trajectory with small perturbations is enough to make safety-critical objects undetectable or detected with incorrect positions. Moreover, polynomial trajectory perturbation is developed to achieve a temporally-smooth and highly-imperceptible attack. Extensive experiments on 3D object detection have shown that such attacks not only lower the performance of the state-of-the-art detectors effectively, but also transfer to other detectors, raising a red flag for the community. The code is available on https://ai4ce.github.io/FLAT/.},
  archive   = {C_ICCV},
  author    = {Yiming Li and Congcong Wen and Felix Juefei-Xu and Chen Feng},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00780},
  pages     = {7878-7887},
  title     = {Fooling LiDAR perception via adversarial trajectory perturbation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). End-to-end unsupervised document image blind denoising.
<em>ICCV</em>, 7868–7877. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Removing noise from scanned pages is a vital step before their submission to optical character recognition (OCR) system. Most available image denoising methods are supervised where the pairs of noisy/clean pages are required. However, this assumption is rarely met in real settings. Besides, there is no single model that can remove various noise types from documents. Here, we propose a unified end-to-end unsupervised deep learning model, for the first time, that can effectively remove multiple types of noise, including salt &amp; pepper noise, blurred and/or faded text, as well as watermarks from documents at various levels of intensity. We demonstrate that the proposed model significantly improves the quality of scanned images and the OCR of the pages on several test datasets.},
  archive   = {C_ICCV},
  author    = {Mehrdad J Gangeh and Marcin Plata and Hamid R Motahari Nezhad and Nigel P Duffy},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00779},
  pages     = {7868-7877},
  title     = {End-to-end unsupervised document image blind denoising},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Removing adversarial noise in class activation feature
space. <em>ICCV</em>, 7858–7867. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep neural networks (DNNs) are vulnerable to adversarial noise. Pre-processing based defenses could largely remove adversarial noise by processing inputs. However, they are typically affected by the error amplification effect, especially in the front of continuously evolving attacks. To solve this problem, in this paper, we propose to remove adversarial noise by implementing a self-supervised adversarial training mechanism in a class activation feature space. To be specific, we first maximize the disruptions to class activation features of natural examples to craft adversarial examples. Then, we train a denoising model to minimize the distances between the adversarial examples and the natural examples in the class activation feature space. Empirical evaluations demonstrate that our method could significantly enhance adversarial robustness in comparison to previous state-of-the-art approaches, especially against unseen adversarial attacks and adaptive attacks.},
  archive   = {C_ICCV},
  author    = {Dawei Zhou and Nannan Wang and Chunlei Peng and Xinbo Gao and Xiaoyu Wang and Jun Yu and Tongliang Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00778},
  pages     = {7858-7867},
  title     = {Removing adversarial noise in class activation feature space},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Data-free universal adversarial perturbation and black-box
attack. <em>ICCV</em>, 7848–7857. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Universal adversarial perturbation (UAP), i.e. a single perturbation to fool the network for most images, is widely recognized as a more practical attack because the UAP can be generated beforehand and applied directly during the at-tack stage. One intriguing phenomenon regarding untargeted UAP is that most images are misclassified to a dominant label. This phenomenon has been reported in previous works while lacking a justified explanation, for which our work attempts to provide an alternative explanation. For a more practical universal attack, our investigation of untargeted UAP focuses on alleviating the dependence on the original training samples, from removing the need for sample labels to limiting the sample size. Towards strictly data-free untargeted UAP, our work proposes to exploit artificial Jigsaw images as the training samples, demonstrating competitive performance. We further investigate the possibility of exploiting the UAP for a data-free black-box attack which is arguably the most practical yet challenging threat model. We demonstrate that there exists optimization-free repetitive patterns which can successfully attack deep models. Code is available at https://bit.ly/3y0ZTIC.},
  archive   = {C_ICCV},
  author    = {Chaoning Zhang and Philipp Benz and Adil Karjauv and In So Kweon},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00777},
  pages     = {7848-7857},
  title     = {Data-free universal adversarial perturbation and black-box attack},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploiting multi-object relationships for detecting
adversarial attacks in complex scenes. <em>ICCV</em>, 7838–7847. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vision systems that deploy Deep Neural Networks (DNNs) are known to be vulnerable to adversarial examples. Recent research has shown that checking the intrinsic consistencies in the input data is a promising way to detect adversarial attacks (e.g., by checking the object co-occurrence relationships in complex scenes). However, existing approaches are tied to specific models and do not offer generalizability. Motivated by the observation that language descriptions of natural scene images have already captured the object co-occurrence relationships that can be learned by a language model, we develop a novel approach to perform context consistency checks using such language models. The distinguishing aspect of our approach is that it is independent of the deployed object detector and yet offers very high accuracy in terms of detecting adversarial examples in practical scenes with multiple objects. Experiments on the PASCAL VOC and MS COCO datasets show that our method can outperform state-of-the-art methods in detecting adversarial attacks.},
  archive   = {C_ICCV},
  author    = {Mingjun Yin and Shasha Li and Zikui Cai and Chengyu Song and M. Salman Asif and Amit K. Roy-Chowdhury and Srikanth V. Krishnamurthy},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00776},
  pages     = {7838-7847},
  title     = {Exploiting multi-object relationships for detecting adversarial attacks in complex scenes},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Naturalistic physical adversarial patch for object
detectors. <em>ICCV</em>, 7828–7837. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most prior works on physical adversarial attacks mainly focus on the attack performance but seldom enforce any restrictions over the appearance of the generated adversarial patches. This leads to conspicuous and attention-grabbing patterns for the generated patches which can be easily identified by humans. To address this issue, we pro-pose a method to craft physical adversarial patches for object detectors by leveraging the learned image manifold of a pretrained generative adversarial network (GAN) (e.g., BigGAN and StyleGAN) upon real-world images. Through sampling the optimal image from the GAN, our method can generate natural looking adversarial patches while maintaining high attack performance. With extensive experiments on both digital and physical domains and several independent subjective surveys, the results show that our proposed method produces significantly more realistic and natural looking patches than several state-of-the-art base-lines while achieving competitive attack performance. 1},
  archive   = {C_ICCV},
  author    = {Yu-Chih-Tuan Hu and Jun-Cheng Chen and Bo-Han Kung and Kai-Lung Hua and Daniel Stanley Tan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00775},
  pages     = {7828-7837},
  title     = {Naturalistic physical adversarial patch for object detectors},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the robustness of vision transformers to adversarial
examples. <em>ICCV</em>, 7818–7827. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in attention-based networks have shown that Vision Transformers can achieve state-of-the-art or near state-of-the-art results on many image classification tasks. This puts transformers in the unique position of being a promising alternative to traditional convolutional neural networks (CNNs). While CNNs have been carefully studied with respect to adversarial attacks, the same cannot be said of Vision Transformers. In this paper, we study the robustness of Vision Transformers to adversarial examples. Our analyses of transformer security is divided into three parts. First, we test the transformer under standard white-box and black-box attacks. Second, we study the transfer-ability of adversarial examples between CNNs and trans-formers. We show that adversarial examples do not readily transfer between CNNs and transformers. Based on this finding, we analyze the security of a simple ensemble defense of CNNs and transformers. By creating a new attack, the self-attention blended gradient attack, we show that such an ensemble is not secure under a white-box adversary. However, under a black-box adversary, we show that an ensemble can achieve unprecedented robustness without sacrificing clean accuracy. Our analysis for this work is done using six types of white-box attacks and two types of black-box attacks. Our study encompasses multiple Vision Transformers, Big Transfer Models and CNN architectures trained on CIFAR-10, CIFAR-100 and ImageNet.},
  archive   = {C_ICCV},
  author    = {Kaleel Mahmood and Rigel Mahmood and Marten van Dijk},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00774},
  pages     = {7818-7827},
  title     = {On the robustness of vision transformers to adversarial examples},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Integer-arithmetic-only certified robustness for quantized
neural networks. <em>ICCV</em>, 7808–7817. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Adversarial data examples have drawn significant attention from the machine learning and security communities. A line of work on tackling adversarial examples is certified robustness via randomized smoothing that can provide a theoretical robustness guarantee. However, such a mechanism usually uses floating-point arithmetic for calculations in inference and requires large memory footprints and daunting computational costs. These defensive models cannot run efficiently on edge devices nor be deployed on integer-only logical units such as Turing Tensor Cores or integer-only ARM processors. To overcome these challenges, we propose an integer randomized smoothing approach with quantization to convert any classifier into a new smoothed classifier, which uses integer-only arithmetic for certified robustness against adversarial perturbations. We prove a tight robustness guarantee under ℓ 2 -norm for the proposed approach. We show our approach can obtain a comparable accuracy and 4× ∼ 5× speedup over floating-point arithmetic certified robust methods on general-purpose CPUs and mobile devices on two distinct datasets (CIFAR-10 and Caltech-101).},
  archive   = {C_ICCV},
  author    = {Haowen Lin and Jian Lou and Li Xiong and Cyrus Shahabi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00773},
  pages     = {7808-7817},
  title     = {Integer-arithmetic-only certified robustness for quantized neural networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Batch normalization increases adversarial vulnerability and
decreases adversarial transferability: A non-robust feature perspective.
<em>ICCV</em>, 7798–7807. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Batch normalization (BN) has been widely used in modern deep neural networks (DNNs) due to improved convergence. BN is observed to increase the model accuracy while at the cost of adversarial robustness. There is an increasing interest in the ML community to understand the impact of BN on DNNs, especially related to the model robustness. This work attempts to understand the impact of BN on DNNs from a non-robust feature perspective. Straightforwardly, the improved accuracy can be attributed to the better utilization of useful features. It remains unclear whether BN mainly favors learning robust features (RFs) or non-robust features (NRFs). Our work presents empirical evidence that supports that BN shifts a model towards being more dependent on NRFs. To facilitate the analysis of such a feature robustness shift, we propose a framework for disentangling robust usefulness into robustness and usefulness. Extensive analysis under the proposed framework yields valuable insight on the DNN behavior regarding robustness, e.g. DNNs first mainly learn RFs and then NRFs. The insight that RFs transfer better than NRFs, further inspires simple techniques to strengthen transfer-based black-box attacks.},
  archive   = {C_ICCV},
  author    = {Philipp Benz and Chaoning Zhang and In So Kweon},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00772},
  pages     = {7798-7807},
  title     = {Batch normalization increases adversarial vulnerability and decreases adversarial transferability: A non-robust feature perspective},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Relating adversarially robust generalization to flat minima.
<em>ICCV</em>, 7787–7797. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Adversarial training (AT) has become the de-facto standard to obtain models robust against adversarial examples. However, AT exhibits severe robust overfitting: cross-entropy loss on adversarial examples, so-called robust loss, decreases continuously on training examples, while eventually increasing on test examples. In practice, this leads to poor robust generalization, i.e., adversarial robustness does not generalize well to new examples. In this paper, we study the relationship between robust generalization and flatness of the robust loss landscape in weight space, i.e., whether robust loss changes significantly when perturbing weights. To this end, we propose average- and worst-case metrics to measure flatness in the robust loss landscape and show a correlation between good robust generalization and flatness. For example, throughout training, flatness reduces significantly during overfitting such that early stopping effectively finds flatter minima in the robust loss landscape. Similarly, AT variants achieving higher adversarial robustness also correspond to flatter minima. This holds for many popular choices, e.g., AT-AWP, TRADES, MART, AT with self-supervision or additional unlabeled examples, as well as simple regularization techniques, e.g., AutoAugment, weight decay or label noise. For fair comparison across these approaches, our flatness measures are specifically designed to be scale-invariant and we conduct extensive experiments to validate our findings.},
  archive   = {C_ICCV},
  author    = {David Stutz and Matthias Hein and Bernt Schiele},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00771},
  pages     = {7787-7797},
  title     = {Relating adversarially robust generalization to flat minima},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Minimal adversarial examples for deep learning on 3D point
clouds. <em>ICCV</em>, 7777–7786. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With recent developments of convolutional neural net-works, deep learning for 3D point clouds has shown significant progress in various 3D scene understanding tasks, e.g., object recognition, semantic segmentation. In a safety-critical environment, it is however not well understood how such deep learning models are vulnerable to adversarial examples. In this work, we explore adversarial attacks for point cloud-based neural networks. We propose a unified formulation for adversarial point cloud generation that can generalise two different attack strategies. Our method generates adversarial examples by attacking the classification ability of point cloud-based networks while considering the perceptibility of the examples and ensuring the minimal level of point manipulations. Experimental results show that our method achieves the state-of-the-art performance with higher than 89\% and 90\% of attack success rate on synthetic and real-world data respectively, while manipulating only about 4\% of the total points.},
  archive   = {C_ICCV},
  author    = {Jaeyeon Kim and Binh-Son Hua and Duc Thanh Nguyen and Sai-Kit Yeung},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00770},
  pages     = {7777-7786},
  title     = {Minimal adversarial examples for deep learning on 3D point clouds},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Meta-attack: Class-agnostic and model-agnostic physical
adversarial attack. <em>ICCV</em>, 7767–7776. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern deep neural networks are often vulnerable to adversarial examples. Most exist attack methods focus on crafting adversarial examples in the digital domain, while only limited works study physical adversarial attack. However, it is more challenging to generate effective adversarial examples in the physical world due to many uncontrollable physical dynamics. Most current physical attack methods aim to generate robust physical adversarial examples by simulating all possible physical dynamics. When attacking new images or new DNN models, they require expensive manually efforts for simulating physical dynamics and considerable time for iteratively optimizing for each image. To tackle these issues, we propose a class-agnostic and model-agnostic physical adversarial attack model (Meta-Attack), which is able to not only generate robust physical adversarial examples by simulating color and shape distortions, but also generalize to attacking novel images and novel DNN models by accessing a few digital and physical images. To the best of our knowledge, this is the first work to formulate the physical attack as a few-shot learning problem. Here, the training task is redefined as the composition of a support set, a query set, and a target DNN model. Under the few-shot setting, we design a novel class-agnostic and model-agnostic meta-learning algorithm to enhance the generalization ability of our method. Extensive experimental results on two benchmark datasets with four challenging experimental settings verify the superior robustness and generalization of our method by comparing to state-of-the-art physical attack methods.},
  archive   = {C_ICCV},
  author    = {Weiwei Feng and Baoyuan Wu and Tianzhu Zhang and Yong Zhang and Yongdong Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00769},
  pages     = {7767-7776},
  title     = {Meta-attack: Class-agnostic and model-agnostic physical adversarial attack},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Consistency-sensitivity guided ensemble black-box
adversarial attacks in low-dimensional spaces. <em>ICCV</em>, 7758–7766.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Black-box attacks aim to generate adversarial noise to fail the victim deep neural network in the black box. The central task in black-box attack method design is to estimate and characterize the victim model in the high-dimensional model space based on feedback results of queries submitted to the victim network. The central performance goal is to minimize the number of queries needed for successful at-tack. Existing attack methods directly search and refine the adversarial noise in an extremely high-dimensional space, requiring hundreds or even thousands queries to the victim network. To address this challenge, we propose to explore a consistency and sensitivity guided ensemble attack (CSEA) method in a low-dimensional space. Specifically, we estimate the victim model in the black box using a learned linear composition of an ensemble of surrogate models with diversified network structures. Using random block masks on the input image, these surrogate models jointly construct and submit randomized and sparsified queries to the victim model. Based on these query results and guided by a consistency constraint, the surrogate models can be trained using a very small number of queries such that their learned composition is able to accurately approximate the victim model in the high-dimensional space. The randomized and sparsified queries also provide important information for us to construct an attack sensitivity map for the input image, with which the adversarial attack can be locally refined to further increase its success rate. Our extensive experimental results demonstrate that our proposed approach significantly reduces the number of queries to the victim network while maintaining very high success rates, outperforming existing black-box attack methods by large margins.},
  archive   = {C_ICCV},
  author    = {Jianhe Yuan and Zhihai He},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00768},
  pages     = {7758-7766},
  title     = {Consistency-sensitivity guided ensemble black-box adversarial attacks in low-dimensional spaces},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adversarial attacks on multi-agent communication.
<em>ICCV</em>, 7748–7757. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Growing at a fast pace, modern autonomous systems will soon be deployed at scale, opening up the possibility for cooperative multi-agent systems. Sharing information and distributing workloads allow autonomous agents to better perform tasks and increase computation efficiency. However, shared information can be modified to execute adversarial attacks on deep learning models that are widely employed in modern systems. Thus, we aim to study the robustness of such systems and focus on exploring adversarial at-tacks in a novel multi-agent setting where communication is done through sharing learned intermediate representations of neural networks. We observe that an indistinguishable adversarial message can severely degrade performance, but becomes weaker as the number of benign agents increases. Furthermore, we show that black-box transfer attacks are more difficult in this setting when compared to directly perturbing the inputs, as it is necessary to align the distribution of learned representations with domain adaptation. Our work studies robustness at the neural network level to con-tribute an additional layer of fault tolerance to modern security protocols for more secure multi-agent systems.},
  archive   = {C_ICCV},
  author    = {James Tu and Tsunhsuan Wang and Jingkang Wang and Sivabalan Manivasagam and Mengye Ren and Raquel Urtasun},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00767},
  pages     = {7748-7757},
  title     = {Adversarial attacks on multi-agent communication},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Reliably fast adversarial training via latent adversarial
perturbation. <em>ICCV</em>, 7738–7747. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While multi-step adversarial training is widely popular as an effective defense method against strong adversarial attacks, its computational cost is notoriously expensive, compared to standard training. Several single-step adversarial training methods have been proposed to mitigate the above-mentioned overhead cost; however, their performance is not sufficiently reliable depending on the optimization setting. To overcome such limitations, we deviate from the existing input-space-based adversarial training regime and propose a single-step latent adversarial training method (SLAT), which leverages the gradients of latent representation as the latent adversarial perturbation. We demonstrate that the ℓ 1 norm of feature gradients is implicitly regularized through the adopted latent perturbation, thereby recovering local linearity and ensuring reliable performance, compared to the existing single-step adversarial training methods. Because latent perturbation is based on the gradients of the latent representations which can be obtained for free in the process of input gradients computation, the proposed method costs roughly the same time as the fast gradient sign method. Experiment results demonstrate that the proposed method, despite its structural simplicity, outperforms state-of-the-art accelerated adversarial training methods.},
  archive   = {C_ICCV},
  author    = {Geon Yeong Park and Sang Wan Lee},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00766},
  pages     = {7738-7747},
  title     = {Reliably fast adversarial training via latent adversarial perturbation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Meta gradient adversarial attack. <em>ICCV</em>, 7728–7737.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, research on adversarial attacks has be-come a hot spot. Although current literature on the transfer-based adversarial attack has achieved promising results for improving the transferability to unseen black-box models, it still leaves a long way to go. Inspired by the idea of meta-learning, this paper proposes a novel architecture called Meta Gradient Adversarial Attack (MGAA), which is plug-and-play and can be integrated with any existing gradient-based attack method for improving the cross-model transferability. Specifically, we randomly sample multiple models from a model zoo to compose different tasks and iteratively simulate a white-box attack and a black-box attack in each task. By narrowing the gap between the gradient directions in white-box and black-box attacks, the transfer-ability of adversarial examples on the black-box setting can be improved. Extensive experiments on the CIFAR10 and ImageNet datasets show that our architecture outperforms the state-of-the-art methods for both black-box and white-box attack settings.},
  archive   = {C_ICCV},
  author    = {Zheng Yuan and Jie Zhang and Yunpei Jia and Chuanqi Tan and Tao Xue and Shiguang Shan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00765},
  pages     = {7728-7737},
  title     = {Meta gradient adversarial attack},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Augmented lagrangian adversarial attacks. <em>ICCV</em>,
7718–7727. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Adversarial attack algorithms are dominated by penalty methods, which are slow in practice, or more efficient distance-customized methods, which are heavily tailored to the properties of the distance considered. We propose a white-box attack algorithm to generate minimally perturbed adversarial examples based on Augmented Lagrangian principles. We bring several algorithmic modifications, which have a crucial effect on performance. Our attack enjoys the generality of penalty methods and the computational efficiency of distance-customized algorithms, and can be readily used for a wide set of distances. We compare our attack to state-of-the-art methods on three datasets and several models, and consistently obtain competitive performances with similar or lower computational complexity.},
  archive   = {C_ICCV},
  author    = {Jérôme Rony and Eric Granger and Marco Pedersoli and Ismail Ben Ayed},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00764},
  pages     = {7718-7727},
  title     = {Augmented lagrangian adversarial attacks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards understanding the generative capability of
adversarially robust classifiers. <em>ICCV</em>, 7708–7717. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, some works found an interesting phenomenon that adversarially robust classifiers can generate good images comparable to generative models. We investigate this phenomenon from an energy perspective and provide a novel explanation. We reformulate adversarial example generation, adversarial training, and image generation in terms of an energy function. We find that adversarial training contributes to obtaining an energy function that is flat and has low energy around the real data, which is the key for generative capability. Based on our new understanding, we further propose a better adversarial training method, Joint Energy Adversarial Training (JEAT), which can generate high-quality images and achieve new state-of-the-art robustness under a wide range of attacks. The Inception Score of the images (CIFAR-10) generated by JEAT is 8.80, much better than original robust classifiers (7.50). In particular, we find that the robustness of JEAT is better than other hybrid models.},
  archive   = {C_ICCV},
  author    = {Yao Zhu and Jiacheng Ma and Jiacheng Sun and Zewei Chen and Rongxin Jiang and Yaowu Chen and Zhenguo Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00763},
  pages     = {7708-7717},
  title     = {Towards understanding the generative capability of adversarially robust classifiers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ProFlip: Targeted trojan attack with progressive bit flips.
<em>ICCV</em>, 7698–7707. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The security of Deep Neural Networks (DNNs) is of great importance due to their employment in various safety-critical applications. DNNs are shown to be vulnerable against the Trojan attack that manipulates model parameters via poisoned training and gets activated by the pre-defined trigger during inference. In this work, we present ProFlip, the first targeted Trojan attack framework that can divert the prediction of the DNN to the target class by progressively identifying and flipping a small set of bits in model parameters. At its core, ProFlip consists of three key phases: (i) Determining significant neurons in the last layer; (ii) Generating an effective trigger pattern for the tar-get class; (iii) Identifying a sequence of susceptible bits of DNN parameters stored in the main memory (e.g., DRAM). After model deployment, the adversary can insert the Trojan by flipping the critical bits found by ProFlip using bit flip techniques such as Row Hammer or laser beams. As the result, the altered DNN predicts the target class when the trigger pattern is present in any inputs. We perform extensive evaluations of ProFlip on CIFAR10, SVHN, and ImageNet datasets with ResNet-18 and VGG-16 architectures. Empirical results show that, to reach an attack success rate (ASR) of over 94\%, ProFlip requires only 12 bit flips out of 88 million parameter bits for ResNet-18 with CIFAR-10, and 15 bit flips for ResNet-18 with ImageNet. Compared to the SOTA, ProFlip reduces the number of required bits flips by 28× ∼ 34× while reaching the same or higher ASR.},
  archive   = {C_ICCV},
  author    = {Huili Chen and Cheng Fu and Jishen Zhao and Farinaz Koushanfar},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00762},
  pages     = {7698-7707},
  title     = {ProFlip: Targeted trojan attack with progressive bit flips},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On generating transferable targeted perturbations.
<em>ICCV</em>, 7688–7697. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While the untargeted black-box transferability of adversarial perturbations has been extensively studied before, changing an unseen model’s decisions to a specific ‘targeted’ class remains a challenging feat. In this paper, we propose a new generative approach for highly transferable targeted perturbations (TTP). We note that the existing methods are less suitable for this task due to their reliance on class-boundary information that changes from one model to another, thus reducing transferability. In contrast, our approach matches the perturbed image ‘distribution’ with that of the target class, leading to high targeted transferability rates. To this end, we propose a new objective function that not only aligns the global distributions of source and target images, but also matches the local neighbourhood structure between the two domains. Based on the proposed objective, we train a generator function that can adaptively synthesize perturbations specific to a given input. Our generative approach is in-dependent of the source or target domain labels, while consistently performs well against state-of-the-art methods on a wide range of attack settings. As an example, we achieve 32.63\% target transferability from (an adversarially weak) VGG19 BN to (a strong) WideResNet on ImageNet val. set, which is 4× higher than the previous best generative attack and 16× better than instance-specific iterative attack. Code is available at: https://github.com/Muzammal-Naseer/TTP.},
  archive   = {C_ICCV},
  author    = {Muzammal Naseer and Salman Khan and Munawar Hayat and Fahad Shahbaz Khan and Fatih Porikli},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00761},
  pages     = {7688-7697},
  title     = {On generating transferable targeted perturbations},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parallel rectangle flip attack: A query-based black-box
attack against object detection. <em>ICCV</em>, 7677–7687. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Object detection has been widely used in many safety- critical tasks, such as autonomous driving. However, its vulnerability to adversarial examples has not been sufficiently studied, especially under the practical scenario of black-box attacks, where the attacker can only access the query feedback of predicted bounding-boxes and top- 1 scores returned by the attacked model. Compared with black-box attack to image classification, there are two main challenges in black-box attack to detection. Firstly, even if one bounding-box is successfully attacked, another sub- optimal bounding-box may be detected near the attacked bounding-box. Secondly, there are multiple bounding- boxes, leading to very high attack cost. To address these challenges, we propose a Parallel Rectangle Flip Attack (PRFA) via random search. We explain the difference between our method with other attacks in Fig. 1. Specifically, we generate perturbations in each rectangle patch to avoid sub-optimal detection near the attacked region. Besides, utilizing the observation that adversarial perturbations mainly locate around objects’ contours and critical points under white-box attacks, the search space of attacked rectangles is reduced to improve the attack efficiency. Moreover, we develop a parallel mechanism of attacking multiple rectangles simultaneously to further accelerate the attack process. Extensive experiments demonstrate that our method can effectively and efficiently attack various popular object detectors, including anchor-based and anchor- free, and generate transferable adversarial examples.},
  archive   = {C_ICCV},
  author    = {Siyuan Liang and Baoyuan Wu and Yanbo Fan and Xingxing Wei and Xiaochun Cao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00760},
  pages     = {7677-7687},
  title     = {Parallel rectangle flip attack: A query-based black-box attack against object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adversarial example detection using latent neighborhood
graph. <em>ICCV</em>, 7667–7676. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Detection of adversarial examples with high accuracy is critical for the security of deployed deep neural network-based models. We present the first graph-based adversarial detection method that constructs a Latent Neighborhood Graph (LNG) around an input example to determine if the input example is adversarial. Given an input example, selected reference adversarial and benign examples (represented as LNG nodes in Figure 1) are used to capture the local manifold in the vicinity of the input example. The LNG node connectivity parameters are optimized jointly with the parameters of a graph attention network in an end-to-end manner to determine the optimal graph topology for adversarial example detection. The graph attention network is used to determine if the LNG is derived from an adversarial or benign input example. Experimental evaluations on CIFAR-10, STL-10, and ImageNet datasets, using six adversarial attack methods, demonstrate that the proposed method outperforms state-of-the-art adversarial detection methods in white-box and gray-box settings. The proposed method is able to successfully detect adversarial examples crafted with small perturbations using unseen attacks.},
  archive   = {C_ICCV},
  author    = {Ahmed Abusnaina and Yuhang Wu and Sunpreet Arora and Yizhen Wang and Fei Wang and Hao Yang and David Mohaisen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00759},
  pages     = {7667-7676},
  title     = {Adversarial example detection using latent neighborhood graph},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sample efficient detection and classification of adversarial
attacks via self-supervised embeddings. <em>ICCV</em>, 7657–7666. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Adversarial robustness of deep models is pivotal in ensuring safe deployment in real world settings, but most modern defenses have narrow scope and expensive costs. In this paper, we propose a self-supervised method to detect adversarial attacks and classify them to their respective threat models, based on a linear model operating on the embed-dings from a pretrained self-supervised encoder. We use a SimCLR encoder in our experiments, since we show the SimCLR embedding distance is a good proxy for human perceptibility, enabling it to encapsulate many threat models at once. We call our method SimCat since it uses SimCLR encoder to catch and categorize various types of adversarial attacks, including ℓ p and non-ℓ p evasion attacks, as well as data poisonings. The simple nature of a linear classifier makes our method efficient in both time and sample complexity. For example, on SVHN, using only five pairs of clean and adversarial examples computed with a PGD-ℓ ∞ attack, SimCat’s detection accuracy is over 85\%. More-over, on ImageNet, using only 25 examples from each threat model, SimCat can classify eight different attack types such as PGD-ℓ 2 , PGD-ℓ ∞ , CW-ℓ 2 , PPGD, LPA, StAdv, ReColor, and JPEG-ℓ ∞ , with over 40\% accuracy. On STL10 data, we apply SimCat as a defense against poisoning attacks, such as BP, CP, FC, CLBD, HTBD, halving the success rate while using only twenty total poisons for training. We find that the detectors generalize well to unseen threat models. Lastly, we investigate the performance of our detection method under adaptive attacks and further boost its robustness against such attacks via adversarial training.},
  archive   = {C_ICCV},
  author    = {Mazda Moayeri and Soheil Feizi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00758},
  pages     = {7657-7666},
  title     = {Sample efficient detection and classification of adversarial attacks via self-supervised embeddings},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Just one moment: Structural vulnerability of deep action
recognition against one frame attack. <em>ICCV</em>, 7648–7656. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The video-based action recognition task has been extensively studied in recent years. In this paper, we study the structural vulnerability of deep learning-based action recognition models against the adversarial attack using the one frame attack that adds an inconspicuous perturbation to only a single frame of a given video clip. Our analysis shows that the models are highly vulnerable against the one frame attack due to their structural properties. Experiments demonstrate high fooling rates and inconspicuous characteristics of the attack. Furthermore, we show that strong universal one frame perturbations can be obtained under various scenarios. Our work raises the serious issue of adversarial vulnerability of the state-of-the-art action recognition models in various perspectives.},
  archive   = {C_ICCV},
  author    = {Jaehui Hwang and Jun-Hyuk Kim and Jun-Ho Choi and Jong-Seok Lee},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00757},
  pages     = {7648-7656},
  title     = {Just one moment: Structural vulnerability of deep action recognition against one frame attack},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AGKD-BML: Defense against adversarial attack by attention
guided knowledge distillation and bi-directional metric learning.
<em>ICCV</em>, 7638–7647. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While deep neural networks have shown impressive performance in many tasks, they are fragile to carefully de-signed adversarial attacks. We propose a novel adversarial training-based model by Attention Guided Knowledge Distillation and Bi-directional Metric Learning (AGKD-BML). The attention knowledge is obtained from a weight-fixed model trained on a clean dataset, referred to as a teacher model, and transferred to a model that is under training on adversarial examples (AEs), referred to as a student model. In this way, the student model is able to focus on the correct region, as well as correcting the intermediate features corrupted by AEs to eventually improve the model accuracy. Moreover, to efficiently regularize the representation in feature space, we propose a bidirectional metric learning. Specifically, given a clean image, it is first attacked to its most confusing class to get the forward AE. A clean image in the most confusing class is then randomly picked and attacked back to the original class to get the backward AE. A triplet loss is then used to shorten the representation distance between original image and its AE, while enlarge that between the forward and backward AEs. We conduct extensive adversarial robustness experiments on two widely used datasets with different attacks. Our proposed AGKD-BML model consistently outperforms the state-of-the-art approaches. The code of AGKD-BML will be available at: https://github.com/hongw579/AGKD-BML.},
  archive   = {C_ICCV},
  author    = {Hong Wang and Yuefan Deng and Shinjae Yoo and Haibin Ling and Yuewei Lin},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00756},
  pages     = {7638-7647},
  title     = {AGKD-BML: Defense against adversarial attack by attention guided knowledge distillation and bi-directional metric learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TkML-AP: Adversarial attacks to top-k multi-label learning.
<em>ICCV</em>, 7629–7637. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Top-k multi-label learning, which returns the top-k predicted labels from an input, has many practical applications such as image annotation, document analysis, and web search engine. However, the vulnerabilities of such algorithms with regards to dedicated adversarial perturbation attacks have not been extensively studied previously. In this work, we develop methods to create adversarial perturbations that can be used to attack top-k multi-label learning-based image annotation systems (T k ML-AP). Our methods explicitly consider the top-k ranking relation and are based on novel loss functions. Experimental evaluations on large-scale benchmark datasets including PASCAL VOC and MS COCO demonstrate the effectiveness of our methods in reducing the performance of state-of-the-art top-k multi-label learning methods, under both untargeted and targeted attacks.},
  archive   = {C_ICCV},
  author    = {Shu Hu and Lipeng Ke and Xin Wang and Siwei Lyu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00755},
  pages     = {7629-7637},
  title     = {TkML-AP: Adversarial attacks to top-k multi-label learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Feature importance-aware transferable adversarial attacks.
<em>ICCV</em>, 7619–7628. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Transferability of adversarial examples is of central importance for attacking an unknown model, which facilitates adversarial attacks in more practical scenarios, e.g., black-box attacks. Existing transferable attacks tend to craft adversarial examples by indiscriminately distorting features to degrade prediction accuracy in a source model without aware of intrinsic features of objects in the images. We argue that such brute-force degradation would introduce model-specific local optimum into adversarial examples, thus limiting the transferability. By contrast, we propose the Feature Importance-aware Attack (FIA), which disrupts important object-aware features that dominate model decisions consistently. More specifically, we obtain feature importance by introducing the aggregate gradient, which averages the gradients with respect to feature maps of the source model, computed on a batch of random transforms of the original clean image. The gradients will be highly correlated to objects of interest, and such correlation presents invariance across different models. Besides, the random transforms will preserve intrinsic features of objects and suppress model-specific information. Finally, the feature importance guides to search for adversarial examples to-wards disrupting critical features, achieving stronger transferability. Extensive experimental evaluation demonstrates the effectiveness and superior performance of the proposed FIA, i.e., improving the success rate by 9.5\% against normally trained models and 12.8\% against defense models as compared to the state-of-the-art transferable attacks. Code is available at: https://github.com/hcguoO0/FIA},
  archive   = {C_ICCV},
  author    = {Zhibo Wang and Hengchang Guo and Zhifei Zhang and Wenxin Liu and Zhan Qin and Kui Ren},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00754},
  pages     = {7619-7628},
  title     = {Feature importance-aware transferable adversarial attacks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Where are you heading? Dynamic trajectory prediction with
expert goal examples. <em>ICCV</em>, 7609–7618. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Goal-conditioned approaches recently have been found very useful to human trajectory prediction, when adequate goal estimates are provided. Yet, goal inference is difficult in itself and often incurs extra learning effort. We propose to predict pedestrian trajectories via the guidance of goal expertise, which can be obtained with modest expense through a novel goal-search mechanism on already seen training examples. There are three key contributions in our study. First, we devise a framework that exploits nearest examples for high-quality goal position inquiry. This approach naturally considers multi-modality, physical constraints, compatibility with existing methods and is nonparametric; it therefore does not require additional learning effort typical in goal inference. Second, we present an end-to-end trajectory predictor that can efficiently associate goal retrievals to past motion information and dynamically infer possible future trajectories. Third, with these two novel techniques in hand, we conduct a series of experiments on two broadly explored datasets (SDD and ETH/UCY) and show that our approach surpasses previous state-of-the-art performance by notable margins and reduces the need for additional parameters. Code can be found at our project page 1 .},
  archive   = {C_ICCV},
  author    = {He Zhao and Richard P. Wildes},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00753},
  pages     = {7609-7618},
  title     = {Where are you heading? dynamic trajectory prediction with expert goal examples},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). DRIVE: Deep reinforced accident anticipation with visual
explanation. <em>ICCV</em>, 7599–7608. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traffic accident anticipation aims to accurately and promptly predict the occurrence of a future accident from dashcam videos, which is vital for a safety-guaranteed self-driving system. To encourage an early and accurate decision, existing approaches typically focus on capturing the cues of spatial and temporal context before a future accident occurs. However, their decision-making lacks visual explanation and ignores the dynamic interaction with the environment. In this paper, we propose Deep ReInforced accident anticipation with Visual Explanation, named DRIVE. The method simulates both the bottom-up and top-down visual attention mechanism in a dashcam observation environment so that the decision from the pro-posed stochastic multi-task agent can be visually explained by attentive regions. Moreover, the proposed dense anticipation reward and sparse fixation reward are effective in training the DRIVE model with our improved reinforcement learning algorithm. Experimental results show that the DRIVE model achieves state-of-the-art performance on multiple real-world traffic accident datasets. Code and pre-trained model are available at https://www.rit.edu/actionlab/drive.},
  archive   = {C_ICCV},
  author    = {Wentao Bao and Qi Yu and Yu Kong},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00752},
  pages     = {7599-7608},
  title     = {DRIVE: Deep reinforced accident anticipation with visual explanation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robustness certification for point cloud models.
<em>ICCV</em>, 7588–7598. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The use of deep 3D point cloud models in safety-critical applications, such as autonomous driving, dictates the need to certify the robustness of these models to real-world trans-formations. This is technically challenging, as it requires a scalable verifier tailored to point cloud models that handles a wide range of semantic 3D transformations. In this work, we address this challenge and introduce 3DCertify, the first verifier able to certify the robustness of point cloud models. 3DCertify is based on two key insights: (i) a generic relaxation based on first-order Taylor approximations, applicable to any differentiable transformation, and (ii) a precise relaxation for global feature pooling, which is more complex than pointwise activations (e.g., ReLU or sigmoid) but commonly employed in point cloud models. We demonstrate the effectiveness of 3DCertify by performing an extensive evaluation on a wide range of 3D transformations (e.g., rotation, twisting) for both classification and part segmentation tasks. For example, we can certify robustness against rotations by ±60° for 95.7\% of point clouds, and our max pool relaxation increases certification by up to 15.6\%.},
  archive   = {C_ICCV},
  author    = {Tobias Lorenz and Anian Ruoss and Mislav Balunović and Gagandeep Singh and Martin Vechev},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00751},
  pages     = {7588-7598},
  title     = {Robustness certification for point cloud models},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A backdoor attack against 3D point cloud classifiers.
<em>ICCV</em>, 7577–7587. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vulnerability of 3D point cloud (PC) classifiers has become a grave concern due to the popularity of 3D sensors in safety-critical applications. Existing adversarial attacks against 3D PC classifiers are all test-time evasion (TTE) attacks that aim to induce test-time misclassifications using knowledge of the classifier. But since the victim classifier is usually not accessible to the attacker, the threat is largely diminished in practice, as PC TTEs typically have poor transferability. Here, we propose the first backdoor attack (BA) against PC classifiers. Originally proposed for images, BAs poison the victim classifier’s training set so that the classifier learns to decide to the attacker’s target class whenever the attacker’s backdoor pattern is present in a given input sample. Significantly, BAs do not require knowledge of the victim classifier. Different from image BAs, we propose to insert a cluster of points into a PC as a robust backdoor pattern customized for 3D PCs. Such clusters are also consistent with a physical attack (i.e., with a captured object in a scene). We optimize the cluster’s location using an independently trained surrogate classifier and choose the cluster’s local geometry to evade possible PC preprocessing and PC anomaly detectors (ADs). Experimentally, our BA achieves a uniformly high success rate (≥ 87\%) and shows evasiveness against state-of-the-art PC ADs. Code is available at https://github.com/zhenxianglance/PCBA.},
  archive   = {C_ICCV},
  author    = {Zhen Xiang and David J. Miller and Siheng Chen and Xi Li and George Kesidis},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00750},
  pages     = {7577-7587},
  title     = {A backdoor attack against 3D point cloud classifiers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Q-match: Iterative shape matching via quantum annealing.
<em>ICCV</em>, 7566–7576. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Finding shape correspondences can be formulated as an ${\mathcal{N}}{\mathcal{P}} - hard$ quadratic assignment problem (QAP) that becomes infeasible for shapes with high sampling density. A promising research direction is to tackle such quadratic optimization problems over binary variables with quantum annealing, which allows for some problems a more efficient search in the solution space. Unfortunately, enforcing the linear equality constraints in QAPs via a penalty significantly limits the success probability of such methods on currently available quantum hardware. To address this limitation, this paper proposes Q-Match, i.e., a new iterative quantum method for QAPs inspired by the α-expansion algorithm, which allows solving problems of an order of magnitude larger than current quantum methods. It implicitly enforces the QAP constraints by updating the current estimates in a cyclic fashion. Further, Q-Match can be applied iteratively, on a subset of well-chosen correspondences, al-lowing us to scale to real-world problems. Using the latest quantum annealer, the D-Wave Advantage, we evaluate the proposed method on a subset of QAPLIB as well as on isometric shape matching problems from the FAUST dataset.},
  archive   = {C_ICCV},
  author    = {Marcel Seelbach Benkner and Zorah Lähner and Vladislav Golyanik and Christof Wunderlich and Christian Theobalt and Michael Moeller},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00749},
  pages     = {7566-7576},
  title     = {Q-match: Iterative shape matching via quantum annealing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AdaMML: Adaptive multi-modal learning for efficient video
recognition. <em>ICCV</em>, 7556–7565. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-modal learning, which focuses on utilizing various modalities to improve the performance of a model, is widely used in video recognition. While traditional multi-modal learning offers excellent recognition results, its computational expense limits its impact for many real-world applications. In this paper, we propose an adaptive multi-modal learning framework, called AdaMML, that selects on-the-fly the optimal modalities for each segment conditioned on the input for efficient video recognition. Specifically, given a video segment, a multi-modal policy net-work is used to decide what modalities should be used for processing by the recognition model, with the goal of improving both accuracy and efficiency. We efficiently train the policy network jointly with the recognition model using standard back-propagation. Extensive experiments on four challenging diverse datasets demonstrate that our proposed adaptive approach yields 35\% − 55\% reduction in computation when compared to the traditional baseline that simply uses all the modalities irrespective of the in-put, while also achieving consistent improvements in accuracy over the state-of-the-art methods. Project page: https://rpand002.github.io/adamml.html.},
  archive   = {C_ICCV},
  author    = {Rameswar Panda and Chun-Fu Richard Chen and Quanfu Fan and Ximeng Sun and Kate Saenko and Aude Oliva and Rogerio Feris},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00748},
  pages     = {7556-7565},
  title     = {AdaMML: Adaptive multi-modal learning for efficient video recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). OadTR: Online action detection with transformers.
<em>ICCV</em>, 7545–7555. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most recent approaches for online action detection tend to apply Recurrent Neural Network (RNN) to capture long-range temporal structure. However, RNN suffers from non-parallelism and gradient vanishing, hence it is hard to be optimized. In this paper, we propose a new encoder-decoder framework based on Transformers, named OadTR, to tackle these problems. The encoder attached with a task token aims to capture the relationships and global inter-actions between historical observations. The decoder extracts auxiliary information by aggregating anticipated future clip representations. Therefore, OadTR can recognize current actions by encoding historical information and predicting future context simultaneously. We extensively evaluate the proposed OadTR on three challenging datasets: HDD, TVSeries, and THUMOS14. The experimental results show that OadTR achieves higher training and inference speeds than current RNN based approaches, and significantly outperforms the state-of-the-art methods in terms of both mAP and mcAP. Code is available at https://github.com/wangxiang1230/OadTR.},
  archive   = {C_ICCV},
  author    = {Xiang Wang and Shiwei Zhang and Zhiwu Qing and Yuanjie Shao and Zhengrong Zuo and Changxin Gao and Nong Sang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00747},
  pages     = {7545-7555},
  title     = {OadTR: Online action detection with transformers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tripartite information mining and integration for image
matting. <em>ICCV</em>, 7535–7544. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the development of deep convolutional neural networks, image matting has ushered in a new phase. Regarding the nature of image matting, most researches have focused on solutions for transition regions. However, we argue that many existing approaches are excessively focused on transition-dominant local fields and ignored the inherent coordination between global information and transition optimisation. In this paper, we propose the Tripartite Information Mining and Integration Network (TIMI-Net) to harmonize the coordination between global and local attributes formally. Specifically, we resort to a novel 3-branch encoder to accomplish comprehensive mining of the input information, which can supplement the neglected coordination between global and local fields. In order to achieve effective and complete interaction between such multi-branches information, we develop the Tripartite Information Integration (T I 2 ) Module to transform and integrate the interconnections between the different branches. In addition, we built a large-scale human matting dataset (Human-2K) to advance human image matting, which consists of 2100 high-precision human images (2000 images for training and 100 images for test). Finally, we con-duct extensive experiments to prove the performance of our proposed TIMI-Net, which demonstrates that our method performs favourably against the SOTA approaches on the alphamatting.com (Rank First), Composition-1K (MSE- 0.006, Grad-11.5), Distinctions-646 and our Human-2K. Also, we have developed an online evaluation website to perform natural image matting.},
  archive   = {C_ICCV},
  author    = {Yuhao Liu and Jiake Xie and Xiao Shi and Yu Qiao and Yujie Huang and Yong Tang and Xin Yang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00746},
  pages     = {7535-7544},
  title     = {Tripartite information mining and integration for image matting},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MultiSiam: Self-supervised multi-instance siamese
representation learning for autonomous driving. <em>ICCV</em>,
7526–7534. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous driving has attracted much attention over the years but turns out to be harder than expected, probably due to the difficulty of labeled data collection for model training. Self-supervised learning (SSL), which leverages unlabeled data only for representation learning, might be a promising way to improve model performance. Existing SSL methods, however, usually rely on the single-centric-object guarantee, which may not be applicable for multi-instance datasets such as street scenes. To alleviate this limitation, we raise two issues to solve: (1) how to define positive samples for cross-view consistency and (2) how to measure similarity in multi-instance circumstances. We first adopt an IoU threshold during random cropping to transfer global-inconsistency to local-consistency. Then, we propose two feature alignment methods to enable 2D feature maps for multi-instance similarity measurement. Addition-ally, we adopt intra-image clustering with self-attention for further mining intra-image similarity and translation-invariance. Experiments show that, when pre-trained on Waymo dataset, our method called Multi-instance Siamese Network (MultiSiam) remarkably improves generalization ability and achieves state-of-the-art transfer performance on autonomous driving benchmarks, including Cityscapes and BDD100K, while existing SSL counterparts like MoCo, MoCo-v2, and BYOL show significant performance drop. By pre-training on SODA10M, a large-scale autonomous driving dataset, MultiSiam exceeds the ImageNet pre-trained MoCo-v2, demonstrating the potential of domain-specific pre-training. Code will be available at https://github.com/KaiChen1998/MultiSiam.},
  archive   = {C_ICCV},
  author    = {Kai Chen and Lanqing Hong and Hang Xu and Zhenguo Li and Dit-Yan Yeung},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00745},
  pages     = {7526-7534},
  title     = {MultiSiam: Self-supervised multi-instance siamese representation learning for autonomous driving},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-supervised vessel segmentation via adversarial
learning. <em>ICCV</em>, 7516–7525. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vessel segmentation is critically essential for diagnosing a series of diseases, e.g., coronary artery disease and retinal disease. However, annotating vessel segmentation maps of medical images is notoriously challenging due to the tiny and complex vessel structures, leading to insufficient available annotated datasets for existing supervised methods and domain adaptation methods. The subtle structures and con-fusing background of medical images further suppress the efficacy of unsupervised methods. In this paper, we propose a self-supervised vessel segmentation method via adversarial learning. Our method learns vessel representations by training an attention-guided generator and a segmentation generator to simultaneously synthesize fake vessels and segment vessels out of coronary angiograms. To support the research, we also build the first X-ray angiography coronary vessel segmentation dataset, named XCAD. We evaluate our method extensively on multiple vessel segmentation datasets, including the XCAD dataset, the DRIVE dataset, and the STARE dataset. The experimental results show our method suppresses unsupervised methods significantly and achieves competitive performance compared with supervised methods and traditional methods.},
  archive   = {C_ICCV},
  author    = {Yuxin Ma and Yang Hua and Hanming Deng and Tao Song and Hao Wang and Zhengui Xue and Heng Cao and Ruhui Ma and Haibing Guan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00744},
  pages     = {7516-7525},
  title     = {Self-supervised vessel segmentation via adversarial learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Can shape structure features improve model robustness under
diverse adversarial settings? <em>ICCV</em>, 7506–7515. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent studies show that convolutional neural networks (CNNs) are vulnerable under various settings, including adversarial attacks, common corruptions, and backdoor attacks. Motivated by the findings that human visual sys-tem pays more attention to global structure (e.g., shapes) for recognition while CNNs are biased towards local texture features in images, in this work we aim to analyze whether &quot;edge features&quot; could improve the recognition robustness in these scenarios, and if so, to what extent? To answer these questions and systematically evaluate the global structure features, we focus on shape features and pro-pose two edge-enabled pipelines EdgeNetRob and Edge-GANRob, forcing the CNNs to rely more on edge features. Specifically, EdgeNetRob and EdgeGANRob first explicitly extract shape structure features from a given image via an edge detection algorithm. Then EdgeNetRob trains down-stream learning tasks directly on the extracted edge features, while EdgeGANRob reconstructs a new image by refilling the texture information with a trained generative adversarial network (GANs). To reduce the sensitivity of edge detection algorithms to perturbations, we additionally propose a robust edge detection approach Robust Canny based on vanilla Canny. Based on our evaluation, we find that EdgeNetRob can help boost model robustness under different attack scenarios at the cost of the clean model accuracy. EdgeGANRob, on the other hand, is able to improve the clean model accuracy compared to EdgeNetRob while preserving the robustness. This shows that given such edge features, how to leverage them matters for robustness, and it also depends on data properties. Our systematic studies on edge structure features under different settings will shed light on future robust feature exploration and optimization.},
  archive   = {C_ICCV},
  author    = {Mingjie Sun and Zichao Li and Chaowei Xiao and Haonan Qiu and Bhavya Kailkhura and Mingyan Liu and Bo Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00743},
  pages     = {7506-7515},
  title     = {Can shape structure features improve model robustness under diverse adversarial settings?},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). S3VAADA: Submodular subset selection for virtual adversarial
active domain adaptation. <em>ICCV</em>, 7496–7505. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unsupervised domain adaptation (DA) methods have focused on achieving maximal performance through aligning features from source and target domains without using labeled data in the target domain. Whereas, in the real-world scenario’s it might be feasible to get labels for a small proportion of target data. In these scenarios, it is important to select maximally-informative samples to label and find an effective way to combine them with the existing knowledge from source data. Towards achieving this, we propose S 3 VAADA which i) introduces a novel submodular criterion to select a maximally informative subset to label and ii) enhances a cluster-based DA procedure through novel improvements to effectively utilize all the available data for improving generalization on target. Our approach consistently outperforms the competing state-of-the-art approaches on datasets with varying degrees of domain shifts. The project page with additional details is available here: https://sites.google.com/iisc.ac.in/s3vaada-iccv2021/.},
  archive   = {C_ICCV},
  author    = {Harsh Rangwani and Arihant Jain and Sumukh K Aithal and R. Venkatesh Babu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00742},
  pages     = {7496-7505},
  title     = {S3VAADA: Submodular subset selection for virtual adversarial active domain adaptation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AdvDrop: Adversarial attack to DNNs by dropping information.
<em>ICCV</em>, 7486–7495. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human can easily recognize visual objects with lost information: even losing most details with only contour reserved, e.g. cartoon. However, in terms of visual perception of Deep Neural Networks (DNNs), the ability for recognizing abstract objects (visual objects with lost information) is still a challenge. In this work, we investigate this issue from an adversarial viewpoint: will the performance of DNNs decrease even for the images only losing a little information? Towards this end, we propose a novel adversarial attack, named AdvDrop, which crafts adversarial examples by dropping existing information of images. Previously, most adversarial attacks add extra disturbing information on clean images explicitly. Opposite to previous works, our proposed work explores the adversarial robustness of DNN models in a novel perspective by dropping imperceptible de-tails to craft adversarial examples. We demonstrate the effectiveness of AdvDrop by extensive experiments, and show that this new type of adversarial examples is more difficult to be defended by current defense systems.},
  archive   = {C_ICCV},
  author    = {Ranjie Duan and Yuefeng Chen and Dantong Niu and Yun Yang and A. K. Qin and Yuan He},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00741},
  pages     = {7486-7495},
  title     = {AdvDrop: Adversarial attack to DNNs by dropping information},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards robustness of deep neural networks via
regularization. <em>ICCV</em>, 7476–7485. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent studies have demonstrated the vulnerability of deep neural networks against adversarial examples. In-spired by the observation that adversarial examples often lie outside the natural image data manifold and the intrinsic dimension of image data is much smaller than its pixel space dimension, we propose to embed high-dimensional input images into a low-dimensional space and apply regularization on the embedding space to push the adversarial examples back to the manifold. The proposed framework is called Embedding Regularized Classifier (ER-Classifier), which improves the adversarial robustness of the classifier through embedding regularization. Besides improving classification accuracy against adversarial examples, the framework can be combined with detection methods to detect adversarial examples. Experimental results on several benchmark datasets show that, our proposed framework achieves good performance against strong adversarial at-tack methods.},
  archive   = {C_ICCV},
  author    = {Yao Li and Martin Renqiang Min and Thomas Lee and Wenchao Yu and Erik Kruus and Wei Wang and Cho-Jui Hsieh},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00740},
  pages     = {7476-7485},
  title     = {Towards robustness of deep neural networks via regularization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic divide-and-conquer adversarial training for robust
semantic segmentation. <em>ICCV</em>, 7466–7475. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Adversarial training is promising for improving robustness of deep neural networks towards adversarial perturbations, especially on the classification task. The effect of this type of training on semantic segmentation, contrarily, just commences. We make the initial attempt to explore the defense strategy on semantic segmentation by formulating a general adversarial training procedure that can per-form decently on both adversarial and clean samples. We propose a dynamic divide-and-conquer adversarial training (DDC-AT) strategy to enhance the defense effect, by set-ting additional branches in the target model during training, and dealing with pixels with diverse properties to-wards adversarial perturbation. Our dynamical division mechanism divides pixels into multiple branches automatically. Note all these additional branches can be abandoned during inference and thus leave no extra parameter and computation cost. Extensive experiments with various segmentation models are conducted on PASCAL VOC 2012 and Cityscapes datasets, in which DDC-AT yields satisfying performance under both white- and black-box at-tack. The code is available at https://github.com/dvlab-research/Robust-Semantic-Segmentation.},
  archive   = {C_ICCV},
  author    = {Xiaogang Xu and Hengshuang Zhao and Jiaya Jia},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00739},
  pages     = {7466-7475},
  title     = {Dynamic divide-and-conquer adversarial training for robust semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatio-temporal dynamic inference network for group activity
recognition. <em>ICCV</em>, 7456–7465. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Group activity recognition aims to understand the activity performed by a group of people. In order to solve it, modeling complex spatiotemporal interactions is the key. Previous methods are limited in reasoning on a predefined graph, which ignores the inherent person-specific interaction context. Moreover, they adopt inference schemes that are computationally expensive and easily result in the over-smoothing problem. In this paper, we manage to achieve spatio-temporal person-specific inferences by proposing Dynamic Inference Network (DIN), which composes of Dynamic Relation (DR) module and Dynamic Walk (DW) module. We firstly propose to initialize interaction fields on a primary spatio-temporal graph. Within each interaction field, we apply DR to predict the relation matrix and DW to predict the dynamic walk offsets in a joint-processing manner, thus forming a person-specific interaction graph. By updating features on the specific graph, a person can possess a global-level interaction field with a local initialization. Experiments indicate both modules’ effectiveness. Moreover, DIN 1 achieves significant improvement compared to previous state-of-the-art methods on two popular datasets under the same setting, while costing much less computation overhead of the reasoning module.},
  archive   = {C_ICCV},
  author    = {Hangjie Yuan and Dong Ni and Mang Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00738},
  pages     = {7456-7465},
  title     = {Spatio-temporal dynamic inference network for group activity recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interpolation-aware padding for 3D sparse convolutional
neural networks. <em>ICCV</em>, 7447–7455. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sparse voxel-based 3D convolutional neural networks (CNNs) are widely used for various 3D vision tasks. Sparse voxel-based 3D CNNs create sparse non-empty voxels from the 3D input and perform 3D convolution operations on them only. We propose a simple yet effective padding scheme — interpolation-aware padding to pad a few empty voxels adjacent to the non-empty voxels and involve them in the 3D CNN computation so that all neighboring voxels exist when computing point-wise features via the trilinear interpolation. For fine-grained 3D vision tasks where point-wise features are essential, like semantic segmentation and 3D detection, our network achieves higher prediction accuracy than the existing networks using the nearest neighbor interpolation or the normalized trilinear interpolation with the zero-padding or the octree-padding scheme. Through extensive compar-isons on various 3D segmentation and detection tasks, we demonstrate the superiority of 3D sparse CNNs with our padding scheme in conjunction with feature interpolation.},
  archive   = {C_ICCV},
  author    = {Yu-Qi Yang and Peng-Shuai Wang and Yang Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00737},
  pages     = {7447-7455},
  title     = {Interpolation-aware padding for 3D sparse convolutional neural networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CPFN: Cascaded primitive fitting networks for
high-resolution point clouds. <em>ICCV</em>, 7438–7446. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Representing human-made objects as a collection of base primitives has a long history in computer vision and reverse engineering. In the case of high-resolution point cloud scans, the challenge is to be able to detect both large primitives as well as those explaining the detailed parts. While the classical RANSAC approach requires case-specific parameter tuning, state-of-the-art networks are limited by memory consumption of their backbone modules such as PointNet++ [27], and hence fail to detect the fine-scale primitives. We present Cascaded Primitive Fitting Networks (CPFN) that relies on an adaptive patch sampling network to assemble detection results of global and local primitive detection networks. As a key enabler, we present a merging formulation that dynamically aggregates the primitives across global and local scales. Our evaluation demonstrates that CPFN improves the state-of-the-art SPFN performance by 13 − 14\% on high-resolution point cloud datasets and specifically improves the detection of fine-scale primitives by 20 − 22\%. Our code is available at: https://github.com/erictuanle/CPFN},
  archive   = {C_ICCV},
  author    = {Eric-Tuan Lê and Minhyuk Sung and Duygu Ceylan and Radomir Mech and Tamy Boubekeur and Niloy J. Mitra},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00736},
  pages     = {7438-7446},
  title     = {CPFN: Cascaded primitive fitting networks for high-resolution point clouds},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DRINet: A dual-representation iterative learning network for
point cloud segmentation. <em>ICCV</em>, 7427–7436. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel and flexible architecture for point cloud segmentation with dual-representation iterative learning. In point cloud processing, different representations have their own pros and cons. Thus, finding suitable ways to represent point cloud data structure while keeping its own internal physical property such as permutation and scale-invariant is a fundamental problem. Therefore, we propose our work, DRINet, which serves as the basic network structure for dual-representation learning with great flexibility at feature transferring and less computation cost, especially for large-scale point clouds. DRINet mainly consists of two modules called Sparse Point-Voxel Feature Extraction and Sparse Voxel-Point Feature Extraction. By utilizing these two modules iteratively, features can be propagated between two different representations. We further propose a novel multi-scale pooling layer for pointwise locality learning to improve context information propagation. Our network achieves state-of-the-art results for point cloud classification and segmentation tasks on several datasets while maintaining high runtime efficiency. For large-scale outdoor scenarios, our method outperforms state-of-the-art methods with a real-time inference time of 62ms per frame.},
  archive   = {C_ICCV},
  author    = {Maosheng Ye and Shuangjie Xu and Tongyi Cao and Qifeng Chen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00735},
  pages     = {7427-7436},
  title     = {DRINet: A dual-representation iterative learning network for point cloud segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Differentiable convolution search for point cloud
processing. <em>ICCV</em>, 7417–7426. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Exploiting convolutional neural networks for point cloud processing is quite challenging, due to the inherent irregular distribution and discrete shape representation of point clouds. To address these problems, many handcrafted convolution variants have sprung up in recent years. Though with elaborate design, these variants could be far from optimal in sufficiently capturing diverse shapes formed by discrete points. In this paper, we propose PointSeaConv, i.e., a novel differential convolution search paradigm on point clouds. It can work in a purely data-driven manner and thus is capable of auto-creating a group of suitable convolutions for geometric shape modeling. We also propose a joint optimization framework for simultaneous search of internal convolution and external architecture, and introduce epsilon-greedy algorithm to alleviate the effect of discretization error. As a result, PointSeaNet, a deep network that is sufficient to capture geometric shapes at both convolution level and architecture level, can be searched out for point cloud processing. Extensive experiments strongly evidence that our proposed PointSeaNet surpasses current handcrafted deep models on challenging benchmarks across multiple tasks with remarkable margins.},
  archive   = {C_ICCV},
  author    = {Xing Nie and Yongcheng Liu and Shaohong Chen and Jianlong Chang and Chunlei Huo and Gaofeng Meng and Qi Tian and Weiming Hu and Chunhong Pan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00734},
  pages     = {7417-7426},
  title     = {Differentiable convolution search for point cloud processing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scaling semantic segmentation beyond 1K classes on a single
GPU. <em>ICCV</em>, 7406–7416. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The state-of-the-art object detection and image classification methods can perform impressively on more than 9k classes. In contrast, the number of classes in semantic segmentation datasets is relatively limited. This is not surprising when the restrictions caused by the lack of labeled data and high computation demand for segmentation are considered. In this paper, we propose a novel training methodology to train and scale the existing semantic segmentation models for a large number of semantic classes without increasing the memory overhead. In our embedding-based scalable segmentation approach, we reduce the space complexity of the segmentation model’s output from O(C) to O(1), propose an approximation method for ground-truth class probability, and use it to compute cross-entropy loss. The proposed approach is general and can be adopted by any state-of-the-art segmentation model to gracefully scale it for any number of semantic classes with only one GPU. Our approach achieves similar, and in some cases, even better mIoU for Cityscapes, Pascal VOC, ADE20k, COCO-Stuff10k datasets when adopted to DeeplabV3+ model with different backbones. We demonstrate a clear benefit of our approach on a dataset with 1284 classes, bootstrapped from LVIS and COCO annotations, with almost three times better mIoU than the DeeplabV3+. Our source code is available at: https://github.com/shipra25jain/ESSNet.},
  archive   = {C_ICCV},
  author    = {Shipra Jain and Danda Pani Paudel and Martin Danelljan and Luc Van Gool},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00733},
  pages     = {7406-7416},
  title     = {Scaling semantic segmentation beyond 1K classes on a single GPU},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scribble-supervised semantic segmentation by uncertainty
reduction on neural representation and self-supervision on neural
eigenspace. <em>ICCV</em>, 7396–7405. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Scribble-supervised semantic segmentation has gained much attention recently for its promising performance without high-quality annotations. Due to the lack of supervision, confident and consistent predictions are usually hard to obtain. Typically, people handle these problems by either adopting an auxiliary task with the well-labeled dataset or incorporating a graphical model with additional requirements on scribble annotations. Instead, this work aims to achieve semantic segmentation by scribble annotations directly without extra information and other limitations. Specifically, we propose holistic operations, including minimizing entropy and a network embedded random walk on the neural representation to reduce uncertainty. Given the probabilistic transition matrix of a random walk, we further train the network with self-supervision on its neural eigenspace to impose consistency on predictions between related images. Comprehensive experiments and ablation studies verify the proposed approach, which demonstrates superiority over others; it is even comparable to some full-label supervised ones and works well when scribbles are randomly shrunk or dropped.},
  archive   = {C_ICCV},
  author    = {Zhiyi Pan and Peng Jiang and Yunhai Wang and Changhe Tu and Anthony G. Cohn},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00732},
  pages     = {7396-7405},
  title     = {Scribble-supervised semantic segmentation by uncertainty reduction on neural representation and self-supervision on neural eigenspace},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Weakly supervised segmentation of small buildings with
point labels. <em>ICCV</em>, 7386–7395. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most supervised image segmentation methods require delicate and time-consuming pixel-level labeling of building or objects, especially for small objects. In this paper, we present a weakly supervised segmentation network for aerial/satellite images, separately considering small and large objects. First, we propose a simple point labeling method for small objects, while large objects are fully labeled. Then, we present a segmentation network trained with a small object mask to separate small and large objects in the loss function. During training, we employ a memory bank to cope with the limited number of point labels. Experiments results with three public datasets demonstrate the feasibility of our approach.},
  archive   = {C_ICCV},
  author    = {Jae-Hun Lee and ChanYoung Kim and Sanghoon Sull},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00731},
  pages     = {7386-7395},
  title     = {Weakly supervised segmentation of small buildings with point labels},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A weakly supervised amodal segmenter with boundary
uncertainty estimation. <em>ICCV</em>, 7376–7385. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses weakly supervised amodal instance segmentation, where the goal is to segment both visible and occluded (amodal) object parts, while training provides only ground-truth visible (modal) segmentations. Following prior work, we use data manipulation to generate occlusions in training images and thus train a segmenter to predict amodal segmentations of the manipulated data. The resulting predictions on training images are taken as the pseudo-ground truth for the standard training of Mask-RCNN, which we use for amodal instance segmentation of test images. For generating the pseudo-ground truth, we specify a new Amodal Segmenter based on Boundary Uncertainty estimation (ASBU) and make two contributions. First, while prior work uses the occluder’s mask, our ASBU uses the occlusion boundary as input. Second, ASBU estimates an uncertainty map of the prediction. The estimated uncertainty regularizes learning such that lower segmentation loss is incurred on regions with high uncertainty. ASBU achieves significant performance improvement relative to the state of the art on the COCOA and KINS datasets in three tasks: amodal instance segmentation, amodal completion, and ordering recovery.},
  archive   = {C_ICCV},
  author    = {Khoi Nguyen and Sinisa Todorovic},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00730},
  pages     = {7376-7385},
  title     = {A weakly supervised amodal segmenter with boundary uncertainty estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph-BAS3Net: Boundary-aware semi-supervised segmentation
network with bilateral graph convolution. <em>ICCV</em>, 7366–7375. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Semi-supervised learning (SSL) algorithms have attracted much attentions in medical image segmentation by leveraging unlabeled data, which challenge in acquiring massive pixel-wise annotated samples. However, most of the existing SSLs neglected the geometric shape constraint in object, leading to unsatisfactory boundary and non-smooth of object. In this paper, we propose a novel boundary-aware semi-supervised medical image segmentation network, named Graph-BAS 3 Net, which incorporates the boundary information and learns duality constraints between semantics and geometrics in the graph domain. Specifically, the proposed method consists of two components: a multi-task learning framework BAS 3 Net and a graph-based cross-task module BGCM. The BAS 3 Net improves the existing GAN-based SSL by adding a boundary detection task, which encodes richer features of object shape and surface. Moreover, the BGCM further explores the co-occurrence relations between the semantics segmentation and boundary detection task, so that the network learns stronger semantic and geometric correspondences from both labeled and unlabeled data. Experimental results on the LiTS dataset and COVID-19 dataset confirm that our proposed Graph-BAS 3 Net outperforms the state-of-the-art methods in semi-supervised segmentation task.},
  archive   = {C_ICCV},
  author    = {Huimin Huang and Lanfen Lin and Yue Zhang and Yingying Xu and Jing Zheng and XiongWei Mao and Xiaohan Qian and Zhiyi Peng and Jianying Zhou and Yen-Wei Chen and Ruofeng Tong},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00729},
  pages     = {7366-7375},
  title     = {Graph-BAS3Net: Boundary-aware semi-supervised segmentation network with bilateral graph convolution},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic network quantization for efficient video inference.
<em>ICCV</em>, 7355–7365. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep convolutional networks have recently achieved great success in video recognition, yet their practical realization remains a challenge due to the large amount of computational resources required to achieve robust recognition. Motivated by the effectiveness of quantization for boosting efficiency, in this paper, we propose a dynamic network quantization framework, that selects optimal precision for each frame conditioned on the input for efficient video recognition. Specifically, given a video clip, we train a very lightweight network in parallel with the recognition network, to produce a dynamic policy indicating which numerical precision to be used per frame in recognizing videos. We train both networks effectively using standard backpropagation with a loss to achieve both competitive performance and resource efficiency required for video recognition. Extensive experiments on four challenging diverse benchmark datasets demonstrate that our proposed approach provides significant savings in computation and memory usage while outperforming the existing state-of-the-art methods. Project page: https://cs-people.bu.edu/sunxm/VideoIQ/project.html.},
  archive   = {C_ICCV},
  author    = {Ximeng Sun and Rameswar Panda and Chun-Fu Richard Chen and Aude Oliva and Rogerio Feris and Kate Saenko},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00728},
  pages     = {7355-7365},
  title     = {Dynamic network quantization for efficient video inference},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predictive feature learning for future segmentation
prediction. <em>ICCV</em>, 7345–7354. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Future segmentation prediction aims to predict the segmentation masks for unobserved future frames. Most existing works addressed it by directly predicting the intermediate features extracted by existing segmentation models. However, these segmentation features are learned to be local discriminative (with rich details) and are always of high resolution/dimension. Hence, the complicated spatiotemporal variations of these features are difficult to predict, which motivates us to learn a more predictive representation. In this work, we develop a novel framework called Predictive Feature Autoencoder. In the proposed framework, we construct an autoencoder which serves as a bridge between the segmentation features and the predictor. In the latent feature learned by the autoencoder, global structures are enhanced and local details are suppressed so that it is more predictive. In order to reduce the risk of vanishing the suppressed details during recurrent feature prediction, we further introduce a reconstruction constraint in the prediction module. Extensive experiments show the effectiveness of the proposed approach and our method outperforms state-of-the-arts by a considerable margin.},
  archive   = {C_ICCV},
  author    = {Zihang Lin and Jiangxin Sun and Jian-Fang Hu and Qizhi Yu and Jian-Huang Lai and Wei-Shi Zheng},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00727},
  pages     = {7345-7354},
  title     = {Predictive feature learning for future segmentation prediction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weakly supervised temporal anomaly segmentation with dynamic
time warping. <em>ICCV</em>, 7335–7344. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most recent studies on detecting and localizing temporal anomalies have mainly employed deep neural networks to learn the normal patterns of temporal data in an unsupervised manner. Unlike them, the goal of our work is to fully utilize instance-level (or weak) anomaly labels, which only indicate whether any anomalous events occurred or not in each instance of temporal data. In this paper, we present WETAS, a novel framework that effectively identifies anomalous temporal segments (i.e., consecutive time points) in an input instance. WETAS learns discriminative features from the instance-level labels so that it infers the sequential order of normal and anomalous segments within each instance, which can be used as a rough segmentation mask. Based on the dynamic time warping (DTW) alignment between the input instance and its segmentation mask, WETAS obtains the result of temporal segmentation, and simultaneously, it further enhances itself by using the mask as additional supervision. Our experiments show that WETAS considerably outperforms other baselines in terms of the localization of temporal anomalies, and also it provides more informative results than point-level detection methods.},
  archive   = {C_ICCV},
  author    = {Dongha Lee and Sehun Yu and Hyunjun Ju and Hwanjo Yu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00726},
  pages     = {7335-7344},
  title     = {Weakly supervised temporal anomaly segmentation with dynamic time warping},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Conditional diffusion for interactive segmentation.
<em>ICCV</em>, 7325–7334. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In click-based interactive segmentation, the mask extraction process is dictated by positive/negative user clicks; however, most existing methods do not fully exploit the user cues, requiring excessive numbers of clicks for satisfactory results. We propose Conditional Diffusion Network (CDNet), which propagates labeled representations from clicks to conditioned destinations with two levels of affinities: Feature Diffusion Module (FDM) spreads features from clicks to potential target regions with global similarity; Pixel Diffusion Module (PDM) diffuses the predicted logits of clicks within locally connected regions. Thus, the information inferred by user clicks could be generalized to proper destinations. In addition, we put forward Diversified Training (DT), which reduces the optimization ambiguity caused by click simulation. With FDM,PDM and DT, CDNet could better understand user’s intentions and make better predictions with limited interactions. CDNet achieves state-of-the-art performance on several benchmarks.},
  archive   = {C_ICCV},
  author    = {Xi Chen and Zhiyan Zhao and Feiwu Yu and Yilei Zhang and Manni Duan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00725},
  pages     = {7325-7334},
  title     = {Conditional diffusion for interactive segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised point cloud object co-segmentation by
co-contrastive learning and mutual attention sampling. <em>ICCV</em>,
7315–7324. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a new task, point cloud object co-segmentation, aiming to segment the common 3D objects in a set of point clouds. We formulate this task as an object point sampling problem, and develop two techniques, the mutual attention module and co-contrastive learning, to enable it. The proposed method employs two point samplers based on deep neural networks, the object sampler and the background sampler. The former targets at sampling points of common objects while the latter focuses on the rest. The mutual attention module explores point-wise correlation across point clouds. It is embedded in both samplers and can identify points with strong cross-cloud correlation from the rest. After extracting features for points selected by the two samplers, we optimize the networks by developing the co-contrastive loss, which minimizes feature discrepancy of the estimated object points while maximizing feature separation between the estimated object and back-ground points. Our method works on point clouds of an arbitrary object class. It is end-to-end trainable and does not need point-level annotations. It is evaluated on the ScanObjectNN and S3DIS datasets and achieves promising results. The source code will be available at https://github.com/jimmy15923/unsup_point_coseg.},
  archive   = {C_ICCV},
  author    = {Cheng-Kun Yang and Yung-Yu Chuang and Yen-Yu Lin},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00724},
  pages     = {7315-7324},
  title     = {Unsupervised point cloud object co-segmentation by co-contrastive learning and mutual attention sampling},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised segmentation incorporating shape prior via
generative adversarial networks. <em>ICCV</em>, 7304–7314. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an image segmentation algorithm that is developed in an unsupervised deep learning framework. The delineation of object boundaries often fails due to the nuisance factors such as illumination changes and occlusions. Thus, we initially propose an unsupervised image decomposition algorithm to obtain an intrinsic representation that is robust with respect to undesirable bias fields based on a multiplicative image model. The obtained intrinsic image is subsequently provided to an unsupervised segmentation procedure that is developed based on a piecewise smooth model. The segmentation model is further designed to incorporate a geometric constraint imposed in the generative adversarial network framework where the discrepancy between the distribution of partitioning functions and the distribution of prior shapes is minimized. We demonstrate the effectiveness and robustness of the proposed algorithm in particular with bias fields and occlusions using simple yet illustrative synthetic examples and a benchmark dataset for image segmentation.},
  archive   = {C_ICCV},
  author    = {Dahye Kim and Byung-Woo Hong},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00723},
  pages     = {7304-7314},
  title     = {Unsupervised segmentation incorporating shape prior via generative adversarial networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time instance segmentation with discriminative
orientation maps. <em>ICCV</em>, 7294–7303. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although instance segmentation has made considerable advancement over recent years, it’s still a challenge to design high accuracy algorithms with real-time performance. In this paper, we propose a real-time instance segmentation framework termed OrienMask. Upon the one-stage object detector YOLOv3, a mask head is added to predict some discriminative orientation maps, which are explicitly defined as spatial offset vectors for both foreground and background pixels. Thanks to the discrimination ability of orientation maps, masks can be recovered without the need for extra foreground segmentation. All instances that match with the same anchor size share a common orientation map. This special sharing strategy reduces the amortized memory utilization for mask predictions but without loss of mask granularity. Given the surviving box predictions after NMS, instance masks can be concurrently constructed from the corresponding orientation maps with low complexity. Owing to the concise design for mask representation and its effective integration with the anchor-based object detector, our method is qualified under real-time conditions while maintaining competitive accuracy. Experiments on COCO benchmark show that OrienMask achieves 34.8 mask AP at the speed of 42.7 fps evaluated with a single RTX 2080 Ti. The code is available at https://github.com/duwt/OrienMask.},
  archive   = {C_ICCV},
  author    = {Wentao Du and Zhiyu Xiang and Shuya Chen and Chengyu Qiao and Yiman Chen and Tingming Bai},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00722},
  pages     = {7294-7303},
  title     = {Real-time instance segmentation with discriminative orientation maps},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploring cross-image pixel contrast for semantic
segmentation. <em>ICCV</em>, 7283–7293. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current semantic segmentation methods focus only on mining &quot;local&quot; context, i.e., dependencies between pixels within individual images, by context-aggregation modules (e.g., dilated convolution, neural attention) or structure-aware optimization criteria (e.g., IoU-like loss). However, they ignore &quot;global&quot; context of the training data, i.e., rich semantic relations between pixels across different images. Inspired by recent advance in unsupervised contrastive representation learning, we propose a pixel-wise contrastive algorithm for semantic segmentation in the fully supervised setting. The core idea is to enforce pixel embeddings belonging to a same semantic class to be more similar than embeddings from different classes. It raises a pixel-wise metric learning paradigm for semantic segmentation, by explicitly exploring the structures of labeled pixels, which were rarely explored before. Our method can be effortlessly incorporated into existing segmentation frameworks without extra overhead during testing. We experimentally show that, with famous segmentation models (i.e., DeepLabV3, HRNet, OCR) and backbones (i.e., ResNet, HRNet), our method brings performance improvements across diverse datasets (i.e., Cityscapes, PASCAL-Context, COCO-Stuff, CamVid). We expect this work will encourage our community to rethink the current de facto training paradigm in semantic segmentation.},
  archive   = {C_ICCV},
  author    = {Wenguan Wang and Tianfei Zhou and Fisher Yu and Jifeng Dai and Ender Konukoglu and Luc Van Gool},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00721},
  pages     = {7283-7293},
  title     = {Exploring cross-image pixel contrast for semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Few-shot semantic segmentation with cyclic memory network.
<em>ICCV</em>, 7273–7282. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Few-shot semantic segmentation (FSS) is an important task for novel (unseen) object segmentation under the data-scarcity scenario. However, most FSS methods rely on unidirectional feature aggregation, e.g., from support prototypes to get the query prediction, and from high-resolution features to guide the low-resolution ones. This usually fails to fully capture the cross-resolution feature relationships and thus leads to inaccurate estimates of the query objects. To resolve the above dilemma, we propose a cyclic memory network (CMN) to directly learn to read abundant support information from all resolution features in a cyclic manner. Specifically, we first generate N pairs (key and value) of multi-resolution query features guided by the support feature and its mask. Next, we circularly take one pair of these features as the query to be segmented, and the rest N-1 pairs are written into an external memory accordingly, i.e., this leave-one-out process is conducted for N times. In each cycle, the query feature is updated by collaboratively matching its key and value with the memory, which can elegantly cover all the spatial locations from different resolutions. Furthermore, we incorporate the query feature re-adding and the query feature recursive updating mechanisms into the memory reading operation. CMN, equipped with these merits, can thus capture cross-resolution relationships and better handle the object appearance and scale variations in FSS. Experiments on PASCAL-5 i and COCO-20 i well validate the effectiveness of our model for FSS.},
  archive   = {C_ICCV},
  author    = {Guo-Sen Xie and Huan Xiong and Jie Liu and Yazhou Yao and Ling Shao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00720},
  pages     = {7273-7282},
  title     = {Few-shot semantic segmentation with cyclic memory network},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ECS-net: Improving weakly supervised semantic segmentation
by using connections between class activation maps. <em>ICCV</em>,
7263–7272. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Image-level weakly supervised semantic segmentation is a challenging task. As classification networks tend to capture notable object features and are insensitive to over-activation, class activation map (CAM) is too sparse and rough to guide segmentation network training. Inspired by the fact that erasing distinguishing features force networks to collect new ones from non-discriminative object regions, we using relationships between CAMs to propose a novel weakly supervised method. In this work, we apply these features, learned from erased images, as segmentation super-vision, driving network to study robust representation. In specifically, object regions obtained by CAM techniques are erased on images firstly. To provide other regions with seg-mentation supervision, Erased CAM Supervision Net (ECS-Net) generates pixel-level labels by predicting segmentation results of those processed images. We also design the rule of suppressing noise to select reliable labels. Our experiments on PASCAL VOC 2012 dataset show that without data annotations except for ground truth image-level labels, our ECS-Net achieves 67.6\% mIoU on test set and 66.6\% mIoU on val set, outperforming previous state-of-the-art methods.},
  archive   = {C_ICCV},
  author    = {Kunyang Sun and Haoqing Shi and Zhengming Zhang and Yongming Huang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00719},
  pages     = {7263-7272},
  title     = {ECS-net: Improving weakly supervised semantic segmentation by using connections between class activation maps},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pixel contrastive-consistent semi-supervised semantic
segmentation. <em>ICCV</em>, 7253–7262. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel semi-supervised semantic segmentation method which jointly achieves two desiderata of segmentation model regularities: the label-space consistency property between image augmentations and the feature-space contrastive property among different pixels. We lever-age the pixel-level ℓ 2 loss and the pixel contrastive loss for the two purposes respectively. To address the computational efficiency issue and the false negative noise issue involved in the pixel contrastive loss, we further introduce and investigate several negative sampling techniques. Extensive experiments demonstrate the state-of-the-art performance of our method (PC 2 Seg) with the DeepLab-v3+ architecture, in several challenging semi-supervised settings derived from the VOC, Cityscapes, and COCO datasets.},
  archive   = {C_ICCV},
  author    = {Yuanyi Zhong and Bodi Yuan and Hong Wu and Zhiqiang Yuan and Jian Peng and Yu-Xiong Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00718},
  pages     = {7253-7262},
  title     = {Pixel contrastive-consistent semi-supervised semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Segmenter: Transformer for semantic segmentation.
<em>ICCV</em>, 7242–7252. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Image segmentation is often ambiguous at the level of individual image patches and requires contextual information to reach label consensus. In this paper we introduce Segmenter, a transformer model for semantic segmentation. In contrast to convolution-based methods, our approach allows to model global context already at the first layer and throughout the network. We build on the recent Vision Transformer (ViT) and extend it to semantic segmentation. To do so, we rely on the output embeddings corresponding to image patches and obtain class labels from these embed-dings with a point-wise linear decoder or a mask trans-former decoder. We leverage models pre-trained for image classification and show that we can fine-tune them on moderate sized datasets available for semantic segmentation. The linear decoder allows to obtain excellent results already, but the performance can be further improved by a mask transformer generating class masks. We conduct an extensive ablation study to show the impact of the different parameters, in particular the performance is better for large models and small patch sizes. Segmenter attains excellent results for semantic segmentation. It outperforms the state of the art on both ADE20K and Pascal Context datasets and is competitive on Cityscapes.},
  archive   = {C_ICCV},
  author    = {Robin Strudel and Ricardo Garcia and Ivan Laptev and Cordelia Schmid},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00717},
  pages     = {7242-7252},
  title     = {Segmenter: Transformer for semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). From contexts to locality: Ultra-high resolution image
segmentation via locality-aware contextual correlation. <em>ICCV</em>,
7232–7241. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ultra-high resolution image segmentation has raised increasing interests in recent years due to its realistic applications. In this paper, we innovate the widely used high-resolution image segmentation pipeline, in which an ultrahigh resolution image is partitioned into regular patches for local segmentation and then the local results are merged into a high-resolution semantic mask. In particular, we introduce a novel locality-aware contextual correlation based segmentation model to process local patches, where the relevance between local patch and its various contexts are jointly and complementarily utilized to handle the semantic regions with large variations. Additionally, we present a contextual semantics refinement network that associates the local segmentation result with its contextual semantics, and thus is endowed with the ability of reducing boundary artifacts and refining mask contours during the generation of final high-resolution mask. Furthermore, in comprehensive experiments, we demonstrate that our model outperforms other state-of-the-art methods in public benchmarks. Our released codes are available at https://github.com/liqiokkk/FCtL.},
  archive   = {C_ICCV},
  author    = {Qi Li and Weixiang Yang and Wenxi Liu and Yuanlong Yu and Shengfeng He},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00716},
  pages     = {7232-7241},
  title     = {From contexts to locality: Ultra-high resolution image segmentation via locality-aware contextual correlation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Complementary patch for weakly supervised semantic
segmentation. <em>ICCV</em>, 7222–7231. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Weakly Supervised Semantic Segmentation (WSSS) based on image-level labels has been greatly advanced by exploiting the outputs of Class Activation Map (CAM) to generate the pseudo labels for semantic segmentation. However, CAM merely discovers seeds from a small number of regions, which may be insufficient to serve as pseudo masks for semantic segmentation. In this paper, we formulate the expansion of object regions in CAM as an increase in information. From the perspective of information theory, we propose a novel Complementary Patch (CP) Representation and prove that the information of the sum of the CAMs by a pair of input images with complementary hidden (patched) parts, namely CP Pair, is greater than or equal to the information of the baseline CAM. Therefore, a CAM with more information related to object seeds can be obtained by narrowing down the gap between the sum of CAMs generated by the CP Pair and the original CAM. We propose a CP Network (CPN) implemented by a triplet network and three regularization functions. To further improve the quality of the CAMs, we propose a Pixel-Region Correlation Module (PRCM) to augment the contextual in-formation by using object-region relations between the feature maps and the CAMs. Experimental results on the PAS-CAL VOC 2012 datasets show that our proposed method achieves a new state-of-the-art in WSSS, validating the effectiveness of our CP Representation and CPN.},
  archive   = {C_ICCV},
  author    = {Fei Zhang and Chaochen Gu and Chenyue Zhang and Yuchao Dai},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00715},
  pages     = {7222-7231},
  title     = {Complementary patch for weakly supervised semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mining contextual information beyond image for semantic
segmentation. <em>ICCV</em>, 7211–7221. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies the context aggregation problem in semantic image segmentation. The existing researches focus on improving the pixel representations by aggregating the contextual information within individual images. Though impressive, these methods neglect the significance of the representations of the pixels of the corresponding class beyond the input image. To address this, this paper proposes to mine the contextual information beyond individual images to further augment the pixel representations. We first set up a feature memory module, which is updated dynamically during training, to store the dataset-level representations of various categories. Then, we learn class probability distribution of each pixel representation under the supervision of the ground-truth segmentation. At last, the representation of each pixel is augmented by aggregating the dataset-level representations based on the corresponding class probability distribution. Furthermore, by utilizing the stored dataset-level representations, we also propose a representation consistent learning strategy to make the classification head better address intra-class compactness and inter-class dispersion. The proposed method could be effortlessly incorporated into existing segmentation frameworks (e.g., FCN, PSPNet, OCRNet and DeepLabV3) and brings consistent performance improvements. Mining contextual information beyond image allows us to report state-of-the-art performance on various benchmarks: ADE20K, LIP, Cityscapes and COCO-Stuff 1 .},
  archive   = {C_ICCV},
  author    = {Zhenchao Jin and Tao Gong and Dongdong Yu and Qi Chu and Jian Wang and Changhu Wang and Jie Shao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00714},
  pages     = {7211-7221},
  title     = {Mining contextual information beyond image for semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Boundary-sensitive pre-training for temporal localization in
videos. <em>ICCV</em>, 7200–7210. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many video analysis tasks require temporal localization for the detection of content changes. However, most existing models developed for these tasks are pre-trained on general video action classification tasks. This is due to large scale annotation of temporal boundaries in untrimmed videos being expensive. Therefore, no suitable datasets exist that enable pre-training in a manner sensitive to temporal boundaries. In this paper for the first time, we investigate model pre-training for temporal localization by introducing a novel boundary-sensitive pretext (BSP) task. Instead of relying on costly manual annotations of temporal boundaries, we propose to synthesize temporal boundaries in existing video action classification datasets. By defining different ways of synthesizing boundaries, BSP can then be simply conducted in a self-supervised manner via the classification of the boundary types. This enables the learning of video representations that are much more transferable to downstream temporal localization tasks. Extensive experiments show that the proposed BSP is superior and complementary to the existing action classification-based pre-training counterpart, and achieves new state-of-the-art performance on several temporal localization tasks. Please visit our website for more details https://frostinassiky.github.io/bsp.},
  archive   = {C_ICCV},
  author    = {Mengmeng Xu and Juan-Manuel Pérez-Rúa and Victor Escorcia and Brais Martínez and Xiatian Zhu and Li Zhang and Bernard Ghanem and Tao Xiang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00713},
  pages     = {7200-7210},
  title     = {Boundary-sensitive pre-training for temporal localization in videos},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiview pseudo-labeling for semi-supervised learning from
video. <em>ICCV</em>, 7189–7199. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a multiview pseudo-labeling approach to video learning, a novel framework that uses complementary views in the form of appearance and motion information for semi-supervised learning in video. The complementary views help obtain more reliable &quot;pseudo-labels&quot; on unlabeled video, to learn stronger video representations than from purely supervised data. Though our method capitalizes on multiple views, it nonetheless trains a model that is shared across appearance and motion input and thus, by design, incurs no additional computation overhead at inference time. On multiple video recognition datasets, our method substantially outperforms its supervised counterpart, and compares favorably to previous work on standard benchmarks in self-supervised video representation learning.},
  archive   = {C_ICCV},
  author    = {Bo Xiong and Haoqi Fan and Kristen Grauman and Christoph Feichtenhofer},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00712},
  pages     = {7189-7199},
  title     = {Multiview pseudo-labeling for semi-supervised learning from video},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cross-sentence temporal and semantic relations in video
activity localisation. <em>ICCV</em>, 7179–7188. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Video activity localisation has recently attained increasing attention due to its practical values in automatically localising the most salient visual segments corresponding to their language descriptions (sentences) from untrimmed and unstructured videos. For supervised model training, a temporal annotation of both the start and end time index of each video segment for a sentence (a video moment) must be given. This is not only very expensive but also sensitive to ambiguity and subjective annotation bias, a much harder task than image labelling. In this work, we develop a more accurate weakly-supervised solution by introducing Cross-Sentence Relations Mining (CRM) in video moment proposal generation and matching when only a paragraph description of activities without per-sentence temporal annotation is available. Specifically, we explore two cross-sentence relational constraints: (1) Temporal ordering and (2) semantic consistency among sentences in a paragraph description of video activities. Existing weakly-supervised techniques only consider within-sentence video segment correlations in training without considering cross-sentence paragraph context. This can mislead due to ambiguous expressions of individual sentences with visually indiscriminate video moment proposals in isolation. Experiments on two publicly available activity localisation datasets show the advantages of our approach over the state-of-the-art weakly supervised methods, especially so when the video activity descriptions become more complex.},
  archive   = {C_ICCV},
  author    = {Jiabo Huang and Yang Liu and Shaogang Gong and Hailin Jin},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00711},
  pages     = {7179-7188},
  title     = {Cross-sentence temporal and semantic relations in video activity localisation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ISNet: Integrate image-level and semantic-level context for
semantic segmentation. <em>ICCV</em>, 7169–7178. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Co-occurrent visual pattern makes aggregating contextual information a common paradigm to enhance the pixel representation for semantic image segmentation. The existing approaches focus on modeling the context from the perspective of the whole image, i.e., aggregating the image-level contextual information. Despite impressive, these methods weaken the significance of the pixel representations of the same category, i.e., the semantic-level contextual information. To address this, this paper proposes to augment the pixel representations by aggregating the image-level and semantic-level contextual information, respectively. First, an image-level context module is designed to capture the contextual information for each pixel in the whole image. Second, we aggregate the representations of the same category for each pixel where the category regions are learned under the supervision of the ground-truth segmentation. Third, we compute the similarities between each pixel representation and the image-level contextual information, the semantic-level contextual information, respectively. At last, a pixel representation is augmented by weighted aggregating both the image-level contextual information and the semantic-level contextual information with the similarities as the weights. Integrating the image-level and semantic-level context allows this paper to report state-of-the-art accuracy on four benchmarks, i.e., ADE20K, LIP, COCOStuff and Cityscapes 1 .},
  archive   = {C_ICCV},
  author    = {Zhenchao Jin and Bin Liu and Qi Chu and Nenghai Yu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00710},
  pages     = {7169-7178},
  title     = {ISNet: Integrate image-level and semantic-level context for semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-supervised video object segmentation by motion
grouping. <em>ICCV</em>, 7157–7168. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Animals have evolved highly functional visual systems to understand motion, assisting perception even under complex environments. In this paper, we work towards developing a computer vision system able to segment objects by exploiting motion cues, i.e. motion segmentation. To achieve this, we introduce a simple variant of the Transformer to segment optical flow frames into primary objects and the background, which can be trained in a self-supervised manner, i.e. without using any manual annotations. Despite using only optical flow, and no appearance information, as input, our approach achieves superior results compared to previous state-of-the-art self-supervised methods on public benchmarks (DAVIS2016, SegTrackv2, FBMS59), while being an order of magnitude faster. On a challenging camouflage dataset (MoCA), we significantly outperform other self-supervised approaches, and are competitive with the top supervised approach, highlighting the importance of motion cues and the potential bias towards appearance in existing video segmentation models.},
  archive   = {C_ICCV},
  author    = {Charig Yang and Hala Lamdouar and Erika Lu and Andrew Zisserman and Weidi Xie},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00709},
  pages     = {7157-7168},
  title     = {Self-supervised video object segmentation by motion grouping},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Cascade image matting with deformable graph refinement.
<em>ICCV</em>, 7147–7156. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Image matting refers to the estimation of the opacity of foreground objects. It requires correct contours and fine details of foreground objects for the matting results. To better accomplish human image matting tasks, we propose the Cascade Image Matting Network with Deformable Graph Refinement(CasDGR), which can automatically predict precise alpha mattes from single human images without any additional inputs. We adopt a network cascade architecture to perform matting from low-to-high resolution, which corresponds to coarse-to-fine optimization. We also introduce the Deformable Graph Refinement (DGR) module based on graph neural networks (GNNs) to overcome the limitations of convolutional neural networks (CNNs). The DGR module can effectively capture long-range relations and obtain more global and local information to help produce finer alpha mattes. We also reduce the computation complexity of the DGR module by dynamically predicting the neighbors and apply DGR module to higher–resolution features. Experimental results demonstrate the ability of our Cas-DGR to achieve state-of-the-art performance on synthetic datasets and produce good results on real human images.},
  archive   = {C_ICCV},
  author    = {Zijian Yu and Xuhui Li and Huijuan Huang and Wen Zheng and Li Chen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00708},
  pages     = {7147-7156},
  title     = {Cascade image matting with deformable graph refinement},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SOTR: Segmenting objects with transformers. <em>ICCV</em>,
7137–7146. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most recent transformer-based models show impressive performance on vision tasks, even better than Convolution Neural Networks (CNN). In this work, we present a novel, flexible, and effective transformer-based model for high-quality instance segmentation. The proposed method, Segmenting Objects with TRansformers (SOTR), simplifies the segmentation pipeline, building on an alternative CNN backbone appended with two parallel subtasks: (1) predicting per-instance category via transformer and (2) dynamically generating segmentation mask with the multi-level upsampling module. SOTR can effectively extract lower-level feature representations and capture long-range context dependencies by Feature Pyramid Network (FPN) and twin transformer, respectively. Meanwhile, compared with the original transformer, the proposed twin transformer is time- and resource-efficient since only a row and a column attention are involved to encode pixels. Moreover, SOTR is easy to be incorporated with various CNN backbones and transformer model variants to make considerable improvements for the segmentation accuracy and training convergence. Extensive experiments show that our SOTR performs well on the MS COCO dataset and surpasses state-of-the-art instance segmentation approaches. We hope our simple but strong framework could serve as a preferment baseline for instance-level recognition. Our code is available at https://github.com/easton-cau/SOTR.},
  archive   = {C_ICCV},
  author    = {Ruohao Guo and Dantong Niu and Liao Qu and Zhenbo Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00707},
  pages     = {7137-7146},
  title     = {SOTR: Segmenting objects with transformers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint topology-preserving and feature-refinement network for
curvilinear structure segmentation. <em>ICCV</em>, 7127–7136. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Curvilinear structure segmentation (CSS) is under semantic segmentation, whose applications include crack detection, aerial road extraction, and biomedical image segmentation. In general, geometric topology and pixel-wise features are two critical aspects of CSS. However, most semantic segmentation methods only focus on enhancing feature representations while existing CSS techniques emphasize preserving topology alone. In this paper, we present a Joint Topology-preserving and Feature-refinement Network (JTFN) that jointly models global topology and refined features based on an iterative feedback learning strategy. Specifically, we explore the structure of objects to help preserve corresponding topologies of predicted masks, thus design a reciprocative two-stream module for CSS and boundary detection. In addition, we introduce such topology-aware predictions as feedback guidance that refines attentive features by supplementing and enhancing saliencies. To the best of our knowledge, this is the first work that jointly addresses topology preserving and feature refinement for CSS. We evaluate JTFN on four datasets of diverse applications: Crack500, CrackTree200, Roads, and DRIVE. Results show that JTFN performs best in comparison with alternative methods. Code is available. 1},
  archive   = {C_ICCV},
  author    = {Mingfei Cheng and Kaili Zhao and Xuhong Guo and Yajing Xu and Jun Guo},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00706},
  pages     = {7127-7136},
  title     = {Joint topology-preserving and feature-refinement network for curvilinear structure segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Specialize and fuse: Pyramidal output representation for
semantic segmentation. <em>ICCV</em>, 7117–7126. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel pyramidal output representation to ensure parsimony with our &quot;specialize and fuse&quot; process for semantic segmentation. A pyramidal &quot;output&quot; representation consists of coarse-to-fine levels, where each level is &quot;specialize&quot; in a different class distribution (e.g., more stuff than things classes at coarser levels). Two types of pyramidal outputs (i.e., unity and semantic pyramid) are &quot;fused&quot; into the final semantic output, where the unity pyramid indicates unity-cells (i.e., all pixels in such cell share the same semantic label). The process ensures parsimony by predicting a relatively small number of labels for unity-cells (e.g., a large cell of grass) to build the final semantic output. In addition to the &quot;output&quot; representation, we design a coarse-to-fine contextual module to aggregate the &quot;features&quot; representation from different levels. We validate the effectiveness of each key module in our method through comprehensive ablation studies. Finally, our approach achieves state-of-the-art performance on three widely-used semantic segmentation datasets—ADE20K, COCO-Stuff, and Pascal-Context.},
  archive   = {C_ICCV},
  author    = {Chi-Wei Hsiao and Cheng Sun and Hwann-Tzong Chen and Min Sun},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00705},
  pages     = {7117-7126},
  title     = {Specialize and fuse: Pyramidal output representation for semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). How shift equivariance impacts metric learning for instance
segmentation. <em>ICCV</em>, 7108–7116. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Metric learning has received conflicting assessments concerning its suitability for solving instance segmentation tasks. It has been dismissed as theoretically flawed due to the shift equivariance of the employed CNNs and their respective inability to distinguish same-looking objects. Yet it has been shown to yield state of the art results for a variety of tasks, and practical issues have mainly been reported in the context of tile-and-stitch approaches, where discontinuities at tile boundaries have been observed. To date, neither of the reported issues have undergone thorough formal analysis. In our work, we contribute a comprehensive formal analysis of the shift equivariance properties of encoder-decoder-style CNNs, which yields a clear picture of what can and cannot be achieved with metric learning in the face of same-looking objects. In particular, we prove that a standard encoder-decoder network that takes d-dimensional images as input, with l pooling layers and pooling factor f, has the capacity to distinguish at most f dl same-looking objects, and we show that this upper limit can be reached. Furthermore, we show that to avoid discontinuities in a tile-and-stitch approach, assuming standard batch size 1, it is necessary to employ valid convolutions in combination with a training output window size strictly greater than f l , while at test-time it is necessary to crop tiles to size n • f l before stitching, with n ≥ 1. We complement these theoretical findings by discussing a number of insightful special cases for which we show empirical results on synthetic and real data.Code:https://github.com/Kainmueller-Lab/shift_equivariance_unet},
  archive   = {C_ICCV},
  author    = {Josef Lorenz Rumberger and Xiaoyan Yu and Peter Hirsch and Melanie Dohmen and Vanessa Emanuela Guarino and Ashkan Mokarian and Lisa Mais and Jan Funke and Dagmar Kainmueller},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00704},
  pages     = {7108-7116},
  title     = {How shift equivariance impacts metric learning for instance segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TempNet: Online semantic segmentation on large-scale point
cloud series. <em>ICCV</em>, 7098–7107. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Online semantic segmentation on a time series of point cloud frames is an essential task in autonomous driving. Existing models focus on single-frame segmentation, which cannot achieve satisfactory segmentation accuracy and offer unstably flicker among frames. In this paper, we propose a light-weight semantic segmentation framework for largescale point cloud series, called TempNet, which can improve both the accuracy and the stability of existing semantic segmentation models by combining a novel frame aggregation scheme. To be computational cost-efficient, feature extraction and aggregation are only conducted on a small portion of key frames via a temporal feature aggregation (TFA) network using an attentional pooling mechanism, and such enhanced features are propagated to the intermediate non-key frames. To avoid information loss from non-key frames, a partial feature update (PFU) network is designed to partially update the propagated features with the local features extracted on a non-key frame if a large disparity between the two is quickly assessed. As a result, consistent and information-rich features can be obtained for each frame. We implement TempNet on five state-of-the-art (SOTA) point cloud segmentation models and conduct extensive experiments on the SemanticKITTI dataset. Results demonstrate that TempNet outperforms SOTA competitors by wide margins with little extra computational cost.},
  archive   = {C_ICCV},
  author    = {Yunsong Zhou and Hongzi Zhu and Chunqin Li and Tiankai Cui and Shan Chang and Minyi Guo},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00703},
  pages     = {7098-7107},
  title     = {TempNet: Online semantic segmentation on large-scale point cloud series},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparse-to-dense feature matching: Intra and inter domain
cross-modal learning in domain adaptation for 3D semantic segmentation.
<em>ICCV</em>, 7088–7097. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Domain adaptation is critical for success when confronting with the lack of annotations in a new domain. As the huge time consumption of labeling process on 3D point cloud, domain adaptation for 3D semantic segmentation is of great expectation. With the rise of multi-modal datasets, large amount of 2D images are accessible besides 3D point clouds. In light of this, we propose to further leverage 2D data for 3D domain adaptation by intra and inter domain cross modal learning. As for intra-domain cross modal learning, most existing works sample the dense 2D pixel-wise features into the same size with sparse 3D point-wise features, resulting in the abandon of numerous useful 2D features. To address this problem, we propose Dynamic sparse-to-dense Cross Modal Learning (DsCML) to increase the sufficiency of multi-modality information interaction for domain adaptation. For inter-domain cross modal learning, we further advance Cross Modal Adversarial Learning (CMAL) on 2D and 3D data which contains different semantic content aiming to promote high-level modal complementarity. We evaluate our model under various multi-modality domain adaptation settings including day-to-night, country-to-country and dataset-to-dataset, brings large improvements over both uni-modal and multi-modal domain adaptation methods on all settings. Code is available at https://github.com/leolyj/DsCML},
  archive   = {C_ICCV},
  author    = {Duo Peng and Yinjie Lei and Wen Li and Pingping Zhang and Yulan Guo},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00702},
  pages     = {7088-7097},
  title     = {Sparse-to-dense feature matching: Intra and inter domain cross-modal learning in domain adaptation for 3D semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Persistent homology based graph convolution network for
fine-grained 3D shape segmentation. <em>ICCV</em>, 7078–7087. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fine-grained 3D segmentation is an important task in 3D object understanding, especially in applications such as intelligent manufacturing or parts analysis for 3D objects. However, many challenges involved in such problem are yet to be solved, such as i) interpreting the complex structures located in different regions for 3D objects; ii) capturing fine-grained structures with sufficient topology correctness. Current deep learning and graph machine learning methods fail to tackle such challenges and thus provide inferior performance in fine-grained 3D analysis. In this work, methods in topological data analysis are incorporated with geometric deep learning model for the task of fine-grained segmentation for 3D objects. We propose a novel neural network model called Persistent Homology based Graph Convolution Network (PHGCN), which i) integrates persistent homology into graph convolution network to capture multi-scale structural information that can accurately represent complex structures for 3D objects; ii) applies a novel Persistence Diagram Loss (ℒ PD ) that provides sufficient topology correctness for segmentation over the fine-grained structures. Extensive experiments on fine-grained 3D segmentation validate the effectiveness of the proposed PHGCN model and show significant improvements over current state-of-the-art methods.},
  archive   = {C_ICCV},
  author    = {Chi-Chong Wong and Chi-Man Vong},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00701},
  pages     = {7078-7087},
  title     = {Persistent homology based graph convolution network for fine-grained 3D shape segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ShapeConv: Shape-aware convolutional layer for indoor RGB-d
semantic segmentation. <em>ICCV</em>, 7068–7077. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {RGB-D semantic segmentation has attracted increasing attention over the past few years. Existing methods mostly employ homogeneous convolution operators to consume the RGB and depth features, ignoring their intrinsic differences. In fact, the RGB values capture the photometric appearance properties in the projected image space, while the depth feature encodes both the shape of a local geometry as well as the base (whereabout) of it in a larger context. Compared with the base, the shape probably is more inherent and has a stronger connection to the semantics, and thus is more critical for segmentation accuracy. Inspired by this observation, we introduce a Shape-aware Convolutional layer (ShapeConv) for processing the depth feature, where the depth feature is firstly decomposed into a shape-component and a base-component, next two learnable weights are introduced to cooperate with them independently, and finally a convolution is applied on the re-weighted combination of these two components. ShapeConv is model-agnostic and can be easily integrated into most CNNs to replace vanilla convolutional layers for semantic segmentation. Extensive experiments on three challenging indoor RGB-D semantic segmentation benchmarks, i.e., NYU-Dv2(-13,-40), SUN RGB-D, and SID, demonstrate the effectiveness of our ShapeConv when employing it over five popular architectures. Moreover, the performance of CNNs with ShapeConv is boosted without introducing any computation and memory increase in the inference phase. The reason is that the learnt weights for balancing the importance between the shape and base components in ShapeConv become constants in the inference phase, and thus can be fused into the following convolution, resulting in a network that is identical to one with vanilla convolutional layers.},
  archive   = {C_ICCV},
  author    = {Jinming Cao and Hanchao Leng and Dani Lischinski and Danny Cohen-Or and Changhe Tu and Yangyan Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00700},
  pages     = {7068-7077},
  title     = {ShapeConv: Shape-aware convolutional layer for indoor RGB-D semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AINet: Association implantation for superpixel segmentation.
<em>ICCV</em>, 7058–7067. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, some approaches are proposed to harness deep convolutional networks to facilitate superpixel segmentation. The common practice is to first evenly divide the image into a pre-defined number of grids and then learn to associate each pixel with its surrounding grids. However, simply applying a series of convolution operations with limited receptive fields can only implicitly perceive the relations between the pixel and its surrounding grids. Consequently, existing methods often fail to provide an effective context when inferring the association map. To remedy this issue, we propose a novel Association Implantation (AI) module to enable the network to explicitly capture the relations between the pixel and its surrounding grids. The proposed AI module directly implants the grid features to the surrounding of its corresponding central pixel, and conducts convolution on the padded window to adaptively transfer knowledge between them. With such an implantation operation, the network could explicitly harvest the pixel-grid level context, which is more in line with the target of superpixel segmentation comparing to the pixelwise relation. Furthermore, to pursue better boundary precision, we design a boundary-perceiving loss to help the network discriminate the pixels around boundaries in hidden feature level, which could benefit the subsequent inferring modules to accurately identify more boundary pixels. Extensive experiments on BSDS500 and NYUv2 datasets show that our method could achieve state-of-the-art performance. Code and pre-trained model are available at https://github.com/wangyxxjtu/AINet-ICCV2021.},
  archive   = {C_ICCV},
  author    = {Yaxiong Wang and Yunchao Wei and Xueming Qian and Li Zhu and Yi Yang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00699},
  pages     = {7058-7067},
  title     = {AINet: Association implantation for superpixel segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-mutating network for domain adaptive segmentation of
aerial images. <em>ICCV</em>, 7048–7057. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The domain-adaptive semantic segmentation of aerial images using a deep-learning technique is still challenging owing to the domain gaps between aerial images obtained in different areas. Currently, various convolutional neural network (CNN)-based domain adaptation methods have been developed to decrease the domain gaps. However, they still show poor performance for object segmentation when they are applied to images from other domains. In this paper, we propose a novel CNN-based self-mutating network (SMN), which can adaptively adjust the parameter values of convolutional filters as a response to the domain of an input image for better domain-adaptive segmentation. For the SMN, the parameter mutation technique was devised for adaptively changing parameters, and a parameterfluctuationtechniquewasdevelopedtorandomlyconvulsetheparameters. By adopting the parameter mutation and fluctuation, adaptive self-changing and fine-tuning of parameters can be realized for images from different domains, resulting in better prediction in domain-adaptive segmentation. Meanwhile, the results of the ablation study indicate that the SMN provided 11.19\% higher Intersection over Union values than other state-of-the-art methods, demonstrating its potential for the domain-adaptive segmentation of aerial images.},
  archive   = {C_ICCV},
  author    = {Kyungsu Lee and Haeyun Lee and Jae Youn Hwang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00698},
  pages     = {7048-7057},
  title     = {Self-mutating network for domain adaptive segmentation of aerial images},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Calibrated adversarial refinement for stochastic semantic
segmentation. <em>ICCV</em>, 7037–7047. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In semantic segmentation tasks, input images can often have more than one plausible interpretation, thus allowing for multiple valid labels. To capture such ambiguities, recent work has explored the use of probabilistic networks that can learn a distribution over predictions. However, these do not necessarily represent the empirical distribution accurately. In this work, we present a strategy for learning a calibrated predictive distribution over semantic maps, where the probability associated with each prediction reflects its ground truth correctness likelihood. To this end, we propose a novel two-stage, cascaded approach for calibrated adversarial refinement: (i) a standard segmentation network is trained with categorical cross entropy to predict a pixelwise probability distribution over semantic classes and (ii) an adversarially trained stochastic network is used to model the inter-pixel correlations to refine the output of the first network into coherent samples. Importantly, to calibrate the refinement network and prevent mode collapse, the expectation of the samples in the second stage is matched to the probabilities predicted in the first. We demonstrate the versatility and robustness of the approach by achieving state-of-the-art results on the multigrader LIDC dataset and on a modified Cityscapes dataset with injected ambiguities. In addition, we show that the core design can be adapted to other tasks requiring learning a calibrated predictive distribution by experimenting on a toy regression dataset. We provide an open source implementation of our method at https://github.com/EliasKassapis/CARSSS.},
  archive   = {C_ICCV},
  author    = {Elias Kassapis and Georgi Dikov and Deepak K. Gupta and Cedric Nugteren},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00697},
  pages     = {7037-7047},
  title     = {Calibrated adversarial refinement for stochastic semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generalize then adapt: Source-free domain adaptive semantic
segmentation. <em>ICCV</em>, 7026–7036. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unsupervised domain adaptation (DA) has gained substantial interest in semantic segmentation. However, almost all prior arts assume concurrent access to both labeled source and unlabeled target, making them unsuitable for scenarios demanding source-free adaptation. In this work 1 , we enable source-free DA by partitioning the task into two: a) source-only domain generalization and b) source-free target adaptation. Towards the former, we provide theoretical insights to develop a multi-head framework trained with a virtually extended multi-source dataset, aiming to balance generalization and specificity. Towards the latter, we utilize the multi-head framework to extract reliable target pseudo-labels for self-training. Additionally, we introduce a novel conditional prior-enforcing auto-encoder that discourages spatial irregularities, thereby enhancing the pseudo-label quality. Experiments on the standard GTA5→Cityscapes and SYNTHIA→Cityscapes benchmarks show our superiority even against the non-source-free prior-arts. Further, we show our compatibility with online adaptation enabling deployment in a sequentially changing environment.},
  archive   = {C_ICCV},
  author    = {Jogendra Nath Kundu and Akshay Kulkarni and Amit Singh and Varun Jampani and R. Venkatesh Babu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00696},
  pages     = {7026-7036},
  title     = {Generalize then adapt: Source-free domain adaptive semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). C3-SemiSeg: Contrastive semi-supervised segmentation via
cross-set learning and dynamic class-balancing. <em>ICCV</em>,
7016–7025. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The semi-supervised semantic segmentation methods utilize the unlabeled data to increase the feature discriminative ability to alleviate the burden of the annotated data. However, the dominant consistency learning diagram is limited by a) the misalignment between features from labeled and unlabeled data; b) treating each image and region separately without considering crucial semantic dependencies among classes. In this work, we introduce a novel C 3 -SemiSeg to improve consistency-based semi-supervised learning by exploiting better feature alignment under perturbations and enhancing the capability of discriminative feature cross images. Specifically, we first introduce a cross-set region-level data augmentation strategy to reduce the feature discrepancy between labeled data and unlabeled data. Cross-set pixel-wise contrastive learning is further integrated into the pipeline to facilitate feature representation ability. To stabilize training from the noisy label, we propose a dynamic confidence region selection strategy to focus on the high confidence region for loss calculation. We validate the proposed approach on Cityscapes and BDD100K dataset, which significantly outperforms other state-of-the-art semi-supervised semantic segmentation methods.},
  archive   = {C_ICCV},
  author    = {Yanning Zhou and Hang Xu and Wei Zhang and Bin Gao and Pheng-Ann Heng},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00695},
  pages     = {7016-7025},
  title     = {C3-SemiSeg: Contrastive semi-supervised segmentation via cross-set learning and dynamic class-balancing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RECALL: Replay-based continual learning in semantic
segmentation. <em>ICCV</em>, 7006–7015. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep networks allow to obtain outstanding results in semantic segmentation, however they need to be trained in a single shot with a large amount of data. Continual learning settings where new classes are learned in incremental steps and previous training data is no longer available are challenging due to the catastrophic forgetting phenomenon. Existing approaches typically fail when several incremental steps are performed or in presence of a distribution shift of the background class. We tackle these issues by recreating no longer available data for the old classes and outlining a content inpainting scheme on the background class. We propose two sources for replay data. The first resorts to a generative adversarial network to sample from the class space of past learning steps. The second relies on web-crawled data to retrieve images containing examples of old classes from online databases. In both scenarios no samples of past steps are stored, thus avoiding privacy concerns. Replay data are then blended with new samples during the incremental steps. Our approach, RECALL, outperforms state-of-the-art methods.},
  archive   = {C_ICCV},
  author    = {Andrea Maracani and Umberto Michieli and Marco Toldo and Pietro Zanuttigh},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00694},
  pages     = {7006-7015},
  title     = {RECALL: Replay-based continual learning in semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The surprising impact of mask-head architecture on novel
class segmentation. <em>ICCV</em>, 6995–7005. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Instance segmentation models today are very accurate when trained on large annotated datasets, but collecting mask annotations at scale is prohibitively expensive. We address the partially supervised instance segmentation problem in which one can train on (significantly cheaper) bounding boxes for all categories but use masks only for a subset of categories. In this work, we focus on a popular family of models which apply differentiable cropping to a feature map and predict a mask based on the resulting crop. Under this family, we study Mask R-CNN and discover that instead of its default strategy of training the mask-head with a combination of proposals and groundtruth boxes, training the mask-head with only groundtruth boxes dramatically improves its performance on novel classes. This training strategy also allows us to take advantage of alternative mask-head architectures, which we exploit by replacing the typical mask-head of 2-4 layers with significantly deeper off-the-shelf architectures (e.g. ResNet, Hourglass models). While many of these architectures perform similarly when trained in fully supervised mode, our main finding is that they can generalize to novel classes in dramatically different ways. We call this ability of mask-heads to generalize to unseen classes the strong mask generalization effect and show that without any specialty modules or losses, we can achieve state-of-the-art results in the partially supervised COCO instance segmentation benchmark. Finally, we demonstrate that our effect is general, holding across underlying detection methodologies (including anchor-based, anchor-free or no detector at all) and across different backbone networks. Code and pre-trained models are available at https://git.io/deepmac.},
  archive   = {C_ICCV},
  author    = {Vighnesh Birodkar and Zhichao Lu and Siyang Li and Vivek Rathod and Jonathan Huang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00693},
  pages     = {6995-7005},
  title     = {The surprising impact of mask-head architecture on novel class segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Context decoupling augmentation for weakly supervised
semantic segmentation. <em>ICCV</em>, 6984–6994. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Data augmentation is vital for deep learning neural networks. By providing massive training samples, it helps to improve the generalization ability of the model. Weakly supervised semantic segmentation (WSSS) is a challenging problem that has been deeply studied in recent years, conventional data augmentation approaches for WSSS usually employ geometrical transformations, random cropping and color jittering. However, merely increasing the same contextual semantic data does not bring much gain to the networks to distinguish the objects, e.g., the correct image-level classification of &quot;aeroplane&quot; may be not only due to the recognition of the object itself, but also its co-occurrence context like &quot;sky&quot;, which will cause the model to focus less on the object features. To this end, we present a Context Decoupling Augmentation (CDA) method, to change the inherent context in which the objects appear and thus drive the network to remove the dependence between object instances and contextual information. To validate the effectiveness of the proposed method, extensive experiments on PASCAL VOC 2012 and COCO datasets with several alternative network architectures demonstrate that CDA can boost various popular WSSS methods to the new state-of-the-art by a large margin. Code is available at https://github.com/suyukun666/CDA},
  archive   = {C_ICCV},
  author    = {Yukun Su and Ruizhou Sun and Guosheng Lin and Qingyao Wu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00692},
  pages     = {6984-6994},
  title     = {Context decoupling augmentation for weakly supervised semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unlocking the potential of ordinary classifier:
Class-specific adversarial erasing framework for weakly supervised
semantic segmentation. <em>ICCV</em>, 6974–6983. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Weakly supervised semantic segmentation (WSSS) using image-level classification labels usually utilizes the Class Activation Maps (CAMs) to localize objects of interest in images. While pointing out that CAMs only highlight the most discriminative regions of the classes of interest, adversarial erasing (AE) methods have been proposed to further explore the less discriminative regions. In this paper, we review the potential of the pre-trained classifier which is trained on the raw images. We experimentally verify that the ordinary classifier 1 already has the capability to activate the less discriminative regions if the most discriminative regions are erased to some extent. Based on that, we propose a class-specific AE-based framework that fully exploits the potential of an ordinary classifier. Our framework (1) adopts the ordinary classifier to notify the regions to be erased and (2) generates a class-specific mask for erasing by randomly sampling a single specific class to be erased (target class) among the existing classes on the image for obtaining more precise CAMs. Specifically, with the guidance of the ordinary classifier, the proposed CAMs Generation Network (CGNet) is enforced to generate a CAM of the target class while constraining the CAM not to intrude the object regions of the other classes. Along with the pseudo-labels refined from our CAMs, we achieve the state-of-the-art WSSS performance on both PASCAL VOC 2012 and MS-COCO dataset only with image-level supervision. The code is available at https://github.com/KAIST-vilab/OC-CSE.},
  archive   = {C_ICCV},
  author    = {Hyeokjun Kweon and Sung-Hoon Yoon and Hyeonseong Kim and Daehee Park and Kuk-Jin Yoon},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00691},
  pages     = {6974-6983},
  title     = {Unlocking the potential of ordinary classifier: Class-specific adversarial erasing framework for weakly supervised semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Leveraging auxiliary tasks with affinity learning for weakly
supervised semantic segmentation. <em>ICCV</em>, 6964–6973. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Semantic segmentation is a challenging task in the absence of densely labelled data. Only relying on class activation maps (CAM) with image-level labels provides deficient segmentation supervision. Prior works thus consider pre-trained models to produce coarse saliency maps to guide the generation of pseudo segmentation labels. However, the commonly used off-line heuristic generation process cannot fully exploit the benefits of these coarse saliency maps. Motivated by the significant inter-task correlation, we propose a novel weakly supervised multi-task framework termed as AuxSegNet, to leverage saliency detection and multi-label image classification as auxiliary tasks to improve the primary task of semantic segmentation using only image-level ground-truth labels. Inspired by their similar structured semantics, we also propose to learn a cross-task global pixellevel affinity map from the saliency and segmentation representations. The learned cross-task affinity can be used to refine saliency predictions and propagate CAM maps to provide improved pseudo labels for both tasks. The mutual boost between pseudo label updating and cross-task affinity learning enables iterative improvements on segmentation performance. Extensive experiments demonstrate the effectiveness of the proposed auxiliary learning network structure and the cross-task affinity learning method. The proposed approach achieves state-of-the-art weakly supervised segmentation performance on the challenging PASCAL VOC 2012 and MS COCO benchmarks. 1},
  archive   = {C_ICCV},
  author    = {Lian Xu and Wanli Ouyang and Mohammed Bennamoun and Farid Boussaid and Ferdous Sohel and Dan Xu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00690},
  pages     = {6964-6973},
  title     = {Leveraging auxiliary tasks with affinity learning for weakly supervised semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Prototypical matching and open set rejection for zero-shot
semantic segmentation. <em>ICCV</em>, 6954–6963. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The DCNN methods in addressing semantic segmentation demand vast amount of pixel-wise annotated training samples. In this work, we present zero-shot semantic segmentation, which aims to identify not only the seen classes contained in training but also the novel classes that have never been seen. We adopt a stringent inductive setting in which only the instances of seen classes are accessible during training. We propose an open-aware prototypical matching approach to accomplish the segmentation. The prototypical way extracts the visual representations by a set of prototypes, making it convenient and flexible to add new unseen classes. A prototype projection is trained to map the semantic representations towards prototypes based on seen instances, and will generate prototypes for unseen classes. Moreover, an open-set rejection is utilized to detect objects that do not belong to any seen classes, which greatly reduces the misclassification of unseen objects into seen classes due to the lack of seen training instances. We apply the framework on two segmentation datasets, Pascal VOC 2012 and Pascal Context, and achieve impressively state-of-the-art performance.},
  archive   = {C_ICCV},
  author    = {Hui Zhang and Henghui Ding},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00689},
  pages     = {6954-6963},
  title     = {Prototypical matching and open set rejection for zero-shot semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pseudo-mask matters in weakly-supervised semantic
segmentation. <em>ICCV</em>, 6944–6953. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most weakly supervised semantic segmentation (WSSS) methods follow the pipeline that generates pseudo-masks initially and trains the segmentation model with the pseudo-masks in fully supervised manner after. However, we find some matters related to the pseudo-masks, including high quality pseudo-masks generation from class activation maps (CAMs), and training with noisy pseudo-mask supervision. For these matters, we propose the following designs to push the performance to new state-of-art: (i) Coefficient of Variation Smoothing to smooth the CAMs adaptively; (ii) Proportional Pseudo-mask Generation to project the expanded CAMs to pseudo-mask based on a new metric indicating the importance of each class on each location, instead of the scores trained from binary classifiers. (iii) Pretended Under-Fitting strategy to suppress the influence of noise in pseudo-mask; (iv) Cyclic Pseudo-mask to boost the pseudo-masks during training of fully supervised semantic segmentation (FSSS). Experiments based on our methods achieve new state-of-art results on two changeling weakly supervised semantic segmentation datasets, pushing the mIoU to 70.0\% and 40.2\% on PAS-CAL VOC 2012 and MS COCO 2014 respectively. Codes including segmentation framework are released at https://github.com/Eli-YiLi/PMM},
  archive   = {C_ICCV},
  author    = {Yi Li and Zhanghui Kuang and Liyang Liu and Yimin Chen and Wayne Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00688},
  pages     = {6944-6953},
  title     = {Pseudo-mask matters in weakly-supervised semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-regulation for semantic segmentation. <em>ICCV</em>,
6933–6943. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we seek reasons for the two major failure cases in Semantic Segmentation (SS): 1) missing small objects or minor object parts, and 2) mislabeling minor parts of large objects as wrong classes. We have an interesting finding that Failure-1 is due to the underuse of detailed features and Failure-2 is due to the underuse of visual contexts. To help the model learn a better trade-off, we introduce several Self-Regulation (SR) losses for training SS neural networks. By &quot;self&quot;, we mean that the losses are from the model per se without using any additional data or supervision. By applying the SR losses, the deep layer features are regulated by the shallow ones to preserve more details; meanwhile, shallow layer classification logits are regulated by the deep ones to capture more semantics. We conduct extensive experiments on both weakly and fully supervised SS tasks, and the results show that our approach consistently surpasses the baselines. We also validate that SR losses are easy to implement in various state-of-the-art SS models, e.g., SPGNet [7] and OCRNet [62], incurring little computational overhead during training and none for testing 1 .},
  archive   = {C_ICCV},
  author    = {Dong Zhang and Hanwang Zhang and Jinhui Tang and Xian-Sheng Hua and Qianru Sun},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00687},
  pages     = {6933-6943},
  title     = {Self-regulation for semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hypercorrelation squeeze for few-shot segmenation.
<em>ICCV</em>, 6921–6932. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Few-shot semantic segmentation aims at learning to segment a target object from a query image using only a few annotated support images of the target class. This challenging task requires to understand diverse levels of visual cues and analyze fine-grained correspondence relations between the query and the support images. To address the problem, we propose Hypercorrelation Squeeze Networks (HSNet) that leverages multi-level feature correlation and efficient 4D convolutions. It extracts diverse features from different levels of intermediate convolutional layers and constructs a collection of 4D correlation tensors, i.e., hypercorrelations. Using efficient center-pivot 4D convolutions in a pyramidal architecture, the method gradually squeezes high-level semantic and low-level geometric cues of the hypercorrelation into precise segmentation masks in coarse-to-fine manner. The significant performance improvements on standard few-shot segmentation benchmarks of PASCAL-5 i , COCO-20 i , and FSS-1000 verify the efficacy of the proposed method.},
  archive   = {C_ICCV},
  author    = {Juhong Min and Dahyun Kang and Minsu Cho},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00686},
  pages     = {6921-6932},
  title     = {Hypercorrelation squeeze for few-shot segmenation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Re-distributing biased pseudo labels for semi-supervised
semantic segmentation: A baseline investigation. <em>ICCV</em>,
6910–6920. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While self-training has advanced semi-supervised semantic segmentation, it severely suffers from the long-tailed class distribution on real-world semantic segmentation datasets that make the pseudo-labeled data bias toward majority classes. In this paper, we present a simple and yet effective Distribution Alignment and Random Sampling (DARS) method to produce unbiased pseudo labels that match the true class distribution estimated from the labeled data. Besides, we also contribute a progressive data augmentation and labeling strategy to facilitate model training with pseudo-labeled data. Experiments on both Cityscapes and PASCAL VOC 2012 datasets demonstrate the effectiveness of our approach. Albeit simple, our method performs favorably in comparison with stateof-the-art approaches. Code will be available at https://github.com/CVMI-Lab/DARS.},
  archive   = {C_ICCV},
  author    = {Ruifei He and Jihan Yang and Xiaojuan Qi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00685},
  pages     = {6910-6920},
  title     = {Re-distributing biased pseudo labels for semi-supervised semantic segmentation: A baseline investigation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Seminar learning for click-level weakly supervised semantic
segmentation. <em>ICCV</em>, 6900–6909. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Annotation burden has become one of the biggest barriers to semantic segmentation. Approaches based on click-level annotations have therefore attracted increasing attention due to their superior trade-off between supervision and annotation cost. In this paper, we propose seminar learning, a new learning paradigm for semantic segmentation with click-level supervision. The fundamental rationale of seminar learning is to leverage the knowledge from different networks to compensate for insufficient information provided in click-level annotations. Mimicking a seminar, our seminar learning involves a teacher-student and a student-student module, where a student can learn from both skillful teachers and other students. The teacher-student module uses a teacher network based on the exponential moving average to guide the training of the student network. In the student-student module, heterogeneous pseudo-labels are proposed to bridge the transfer of knowledge among students to enhance each other’s performance. Experimental results demonstrate the effectiveness of seminar learning, which achieves the new state-of-the-art performance of 72.51\% (mIOU), surpassing previous methods by a large margin of up to 16.88\% on the Pascal VOC 2012 dataset.},
  archive   = {C_ICCV},
  author    = {Hongjun Chen and Jinbao Wang and Hong Cai Chen and Xiantong Zhen and Feng Zheng and Rongrong Ji and Ling Shao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00684},
  pages     = {6900-6909},
  title     = {Seminar learning for click-level weakly supervised semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Instances as queries. <em>ICCV</em>, 6890–6899. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present QueryInst, a new perspective for instance segmentation. QueryInst is a multi-stage end-to-end system that treats instances of interest as learnable queries, enabling query based object detectors, e.g., Sparse RCNN, to have strong instance segmentation performance. The attributes of instances such as categories, bounding boxes, instance masks, and instance association embeddings are represented by queries in a unified manner. In QueryInst, a query is shared by both detection and segmentation via dynamic convolutions and driven by parallellysupervised multi-stage learning. We conduct extensive experiments on three challenging benchmarks, i.e., COCO, CityScapes, and YouTube-VIS to evaluate the effectiveness of QueryInst in object detection, instance segmentation, and video instance segmentation tasks. For the first time, we demonstrate that a simple end-to-end query based framework can achieve the state-of-the-art performance in various instance-level recognition tasks. Code is available at https://github.com/hustvl/QueryInst.},
  archive   = {C_ICCV},
  author    = {Yuxin Fang and Shusheng Yang and Xinggang Wang and Yu Li and Chen Fang and Ying Shan and Bin Feng and Wenyu Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00683},
  pages     = {6890-6899},
  title     = {Instances as queries},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An elastica geodesic approach with convexity shape prior.
<em>ICCV</em>, 6880–6889. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The minimal geodesic models based on the Eikonal equations are capable of finding suitable solutions in various image segmentation scenarios. Existing geodesic-based segmentation approaches usually exploit the image features in conjunction with geometric regularization terms (such as curve length or elastica length) for computing geodesic paths. In this paper, we consider a more complicated problem: finding simple and closed geodesic curves which are imposed a convexity shape prior. The proposed approach relies on an orientation-lifting strategy, by which a planar curve can be mapped to an high-dimensional orientation space. The convexity shape prior serves as a constraint for the construction of local metrics. The geodesic curves in the lifted space then can be efficiently computed through the fast marching method. In addition, we introduce a way to incorporate region-based homogeneity features into the proposed geodesic model so as to solve the region-based segmentation issues with shape prior constraints.},
  archive   = {C_ICCV},
  author    = {Da Chen and Laurent D. Cohen and Jean-Marie Mirebeau and Xue-Cheng Tai},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00682},
  pages     = {6880-6889},
  title     = {An elastica geodesic approach with convexity shape prior},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Local temperature scaling for probability calibration.
<em>ICCV</em>, 6869–6879. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For semantic segmentation, label probabilities are often uncalibrated as they are typically only the by-product of a segmentation task. Intersection over Union (IoU) and Dice score are often used as criteria for segmentation success, while metrics related to label probabilities are not often explored. However, probability calibration approaches have been studied, which match probability outputs with experimentally observed errors. These approaches mainly focus on classification tasks, but not on semantic segmentation. Thus, we propose a learning-based calibration method that focuses on multi-label semantic segmentation. Specifically, we adopt a convolutional neural network to predict local temperature values for probability calibration. One advantage of our approach is that it does not change prediction accuracy, hence allowing for calibration as a postprocessing step. Experiments on the COCO, CamVid, and LPBA40 datasets demonstrate improved calibration performance for a range of different metrics. We also demonstrate the good performance of our method for multi-atlas brain segmentation from magnetic resonance images.},
  archive   = {C_ICCV},
  author    = {Zhipeng Ding and Xu Han and Peirong Liu and Marc Niethammer},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00681},
  pages     = {6869-6879},
  title     = {Local temperature scaling for probability calibration},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RINDNet: Edge detection for discontinuity in reflectance,
illumination, normal and depth. <em>ICCV</em>, 6859–6868. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As a fundamental building block in computer vision, edges can be categorised into four types according to the discontinuity in surface-Reflectance, Illumination, surface-Normal or Depth. While great progress has been made in detecting generic or individual types of edges, it remains under-explored to comprehensively study all four edge types together. In this paper, we propose a novel neural network solution, RINDNet, to jointly detect all four types of edges. Taking into consideration the distinct attributes of each type of edges and the relationship between them, RINDNet learns effective representations for each of them and works in three stages. In stage I, RINDNet uses a common backbone to extract features shared by all edges. Then in stage II it branches to prepare discriminative features for each edge type by the corresponding decoder. In stage III, an independent decision head for each type aggregates the features from previous stages to predict the initial results. Additionally, an attention module learns attention maps for all types to capture the underlying relations between them, and these maps are combined with initial results to generate the final edge detection results. For training and evaluation, we construct the first public benchmark, BSDS-RIND, with all four types of edges carefully annotated. In our experiments, RINDNet yields promising results in comparison with state-of-the-art methods. Additional analysis is presented in supplementary material.},
  archive   = {C_ICCV},
  author    = {Mengyang Pu and Yaping Huang and Qingji Guan and Haibin Ling},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00680},
  pages     = {6859-6868},
  title     = {RINDNet: Edge detection for discontinuity in reflectance, illumination, normal and depth},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Field of junctions: Extracting boundary structure at low
SNR. <em>ICCV</em>, 6849–6858. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a bottom-up model for simultaneously finding many boundary elements in an image, including contours, corners and junctions. The model explains boundary shape in each small patch using a ‘generalized M-junction’ comprising M angles and a freely-moving vertex. Images are analyzed using non-convex optimization to cooperatively find M + 2 junction values at every location, with spatial consistency being enforced by a novel regularizer that reduces curvature while preserving corners and junctions. The resulting ‘field of junctions’ is simultaneously a contour detector, corner/junction detector, and boundary-aware smoothing of regional appearance. Notably, its unified analysis of contours, corners, junctions and uniform regions allows it to succeed at high noise levels, where other methods for segmentation and boundary detection fail.},
  archive   = {C_ICCV},
  author    = {Dor Verbin and Todd Zickler},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00679},
  pages     = {6849-6858},
  title     = {Field of junctions: Extracting boundary structure at low SNR},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to cut by watching movies. <em>ICCV</em>,
6838–6848. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Video content creation keeps growing at an incredible pace; yet, creating engaging stories remains challenging and requires non-trivial video editing expertise. Many video editing components are astonishingly hard to automate primarily due to the lack of raw video materials. This paper focuses on a new task for computational video editing, namely the task of raking cut plausibility. Our key idea is to leverage content that has already been edited to learn fine-grained audiovisual patterns that trigger cuts. To do this, we first collected a data source of more than 10K videos, from which we extract more than 255K cuts. We devise a model that learns to discriminate between real and artificial cuts via contrastive learning. We set up a new task and a set of baselines to benchmark video cut generation. We observe that our proposed model outperforms the baselines by large margins. To demonstrate our model in real-world applications, we conduct human studies in a collection of unedited videos. The results show that our model does a better job at cutting than random and alternative baselines.},
  archive   = {C_ICCV},
  author    = {Alejandro Pardo and Fabian Caba Heilbron and Juan León Alcázar and Ali Thabet and Bernard Ghanem},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00678},
  pages     = {6838-6848},
  title     = {Learning to cut by watching movies},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). End-to-end dense video captioning with parallel decoding.
<em>ICCV</em>, 6827–6837. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dense video captioning aims to generate multiple associated captions with their temporal locations from the video. Previous methods follow a sophisticated &quot;localizethen-describe&quot; scheme, which heavily relies on numerous hand-crafted components. In this paper, we proposed a simple yet effective framework for end-to-end dense video captioning with parallel decoding (PDVC), by formulating the dense caption generation as a set prediction task. In practice, through stacking a newly proposed event counter on the top of a transformer decoder, the PDVC precisely segments the video into a number of event pieces under the holistic understanding of the video content, which effectively increases the coherence and readability of predicted captions. Compared with prior arts, the PDVC has several appealing advantages: (1) Without relying on heuristic non-maximum suppression or a recurrent event sequence selection network to remove redundancy, PDVC directly produces an event set with an appropriate size; (2) In contrast to adopting the two-stage scheme, we feed the enhanced representations of event queries into the localization head and caption head in parallel, making these two sub-tasks deeply interrelated and mutually promoted through the optimization; (3) Without bells and whistles, extensive experiments on ActivityNet Captions and YouCook2 show that PDVC is capable of producing high-quality captioning results, surpassing the state-of-the-art two-stage methods when its localization accuracy is on par with them. Code is available at https://github.com/ttengwang/PDVC.},
  archive   = {C_ICCV},
  author    = {Teng Wang and Ruimao Zhang and Zhichao Lu and Feng Zheng and Ran Cheng and Ping Luo},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00677},
  pages     = {6827-6837},
  title     = {End-to-end dense video captioning with parallel decoding},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ViViT: A video vision transformer. <em>ICCV</em>, 6816–6826.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present pure-transformer based models for video classification, drawing upon the recent success of such models in image classification. Our model extracts spatiotemporal tokens from the input video, which are then encoded by a series of transformer layers. In order to handle the long sequences of tokens encountered in video, we propose several, efficient variants of our model which factorise the spatial- and temporal-dimensions of the input. Although transformer-based models are known to only be effective when large training datasets are available, we show how we can effectively regularise the model during training and leverage pretrained image models to be able to train on comparatively small datasets. We conduct thorough ablation studies, and achieve state-of-the-art results on multiple video classification benchmarks including Kinetics 400 and 600, Epic Kitchens, Something-Something v2 and Moments in Time, outperforming prior methods based on deep 3D convolutional networks.},
  archive   = {C_ICCV},
  author    = {Anurag Arnab and Mostafa Dehghani and Georg Heigold and Chen Sun and Mario Lučić and Cordelia Schmid},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00676},
  pages     = {6816-6826},
  title     = {ViViT: A video vision transformer},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiscale vision transformers. <em>ICCV</em>, 6804–6815.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present Multiscale Vision Transformers (MViT) for video and image recognition, by connecting the seminal idea of multiscale feature hierarchies with transformer models. Multiscale Transformers have several channel-resolution scale stages. Starting from the input resolution and a small channel dimension, the stages hierarchically expand the channel capacity while reducing the spatial resolution. This creates a multiscale pyramid of features with early layers operating at high spatial resolution to model simple low-level visual information, and deeper layers at spatially coarse, but complex, high-dimensional features. We evaluate this fundamental architectural prior for modeling the dense nature of visual signals for a variety of video recognition tasks where it outperforms concurrent vision transformers that rely on large scale external pre-training and are 5-10× more costly in computation and parameters. We further remove the temporal dimension and apply our model for image classification where it outperforms prior work on vision transformers. Code is available at: https://github.com/facebookresearch/SlowFast.},
  archive   = {C_ICCV},
  author    = {Haoqi Fan and Bo Xiong and Karttikeya Mangalam and Yanghao Li and Zhicheng Yan and Jitendra Malik and Christoph Feichtenhofer},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00675},
  pages     = {6804-6815},
  title     = {Multiscale vision transformers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Where2Act: From pixels to actions for articulated 3D
objects. <em>ICCV</em>, 6793–6803. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One of the fundamental goals of visual perception is to allow agents to meaningfully interact with their environment. In this paper, we take a step towards that long-term goal – we extract highly localized actionable information related to elementary actions such as pushing or pulling for articulated objects with movable parts. For example, given a drawer, our network predicts that applying a pulling force on the handle opens the drawer. We propose, discuss, and evaluate novel network architectures that given image and depth data, predict the set of actions possible at each pixel, and the regions over articulated parts that are likely to move under the force. We propose a learning-from-interaction framework with an online data sampling strategy that allows us to train the network in simulation (SAPIEN) and generalizes across categories. Check the website for code and data release.},
  archive   = {C_ICCV},
  author    = {Kaichun Mo and Leonidas Guibas and Mustafa Mukadam and Abhinav Gupta and Shubham Tulsiani},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00674},
  pages     = {6793-6803},
  title     = {Where2Act: From pixels to actions for articulated 3D objects},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toward a visual concept vocabulary for GAN latent space.
<em>ICCV</em>, 6784–6792. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A large body of recent work has identified transformations in the latent spaces of generative adversarial networks (GANs) that consistently and interpretably transform generated images. But existing techniques for identifying these transformations rely on either a fixed vocabulary of prespecified visual concepts, or on unsupervised disentanglement techniques whose alignment with human judgments about perceptual salience is unknown. This paper introduces a new method for building open-ended vocabularies of primitive visual concepts represented in a GAN’s latent space. Our approach is built from three components: (1) automatic identification of perceptually salient directions based on their layer selectivity; (2) human annotation of these directions with free-form, compositional natural language descriptions; and (3) decomposition of these annotations into a visual concept vocabulary, consisting of distilled directions labeled with single words. Experiments show that concepts learned with our approach are reliable and composable—generalizing across classes, contexts, and observers, and enabling fine-grained manipulation of image style and content.},
  archive   = {C_ICCV},
  author    = {Sarah Schwettmann and Evan Hernandez and David Bau and Samuel Klein and Jacob Andreas and Antonio Torralba},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00673},
  pages     = {6784-6792},
  title     = {Toward a visual concept vocabulary for GAN latent space},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online multi-granularity distillation for GAN compression.
<em>ICCV</em>, 6773–6783. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generative Adversarial Networks (GANs) have witnessed prevailing success in yielding outstanding images, however, they are burdensome to deploy on resource-constrained devices due to ponderous computational costs and hulking memory usage. Although recent efforts on compressing GANs have acquired remarkable results, they still exist potential model redundancies and can be further compressed. To solve this issue, we propose a novel online multi-granularity distillation (OMGD) scheme to obtain lightweight GANs, which contributes to generating highfidelity images with low computational demands. We offer the first attempt to popularize single-stage online distillation for GAN-oriented compression, where the progressively promoted teacher generator helps to refine the discriminator-free based student generator. Complementary teacher generators and network layers provide comprehensive and multi-granularity concepts to enhance visual fidelity from diverse dimensions. Experimental results on four benchmark datasets demonstrate that OMGD successes to compress 40× MACs and 82.5× parameters on Pix2Pix and CycleGAN, without loss of image quality. It reveals that OMGD provides a feasible solution for the deployment of real-time image translation on resource-constrained devices. Our code and models are made public at: https://github.com/bytedance/OMGD},
  archive   = {C_ICCV},
  author    = {Yuxi Ren and Jie Wu and Xuefeng Xiao and Jianchao Yang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00672},
  pages     = {6773-6783},
  title     = {Online multi-granularity distillation for GAN compression},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scaling-up disentanglement for image translation.
<em>ICCV</em>, 6763–6772. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Image translation methods typically aim to manipulate a set of labeled attributes (given as supervision at training time e.g. domain label) while leaving the unlabeled attributes intact. Current methods achieve either: (i) disentanglement, which exhibits low visual fidelity and can only be satisfied where the attributes are perfectly uncorrelated. (ii) visually-plausible translations, which are clearly not disentangled. In this work, we propose OverLORD, a single framework for disentangling labeled and unlabeled attributes as well as synthesizing high-fidelity images, which is composed of two stages; (i) Disentanglement: Learning disentangled representations with latent optimization. Differently from previous approaches, we do not rely on adversarial training or any architectural biases. (ii) Synthesis: Training feed-forward encoders for inferring the learned attributes and tuning the generator in an adversarial manner to increase the perceptual quality. When the labeled and unlabeled attributes are correlated, we model an additional representation that accounts for the correlated attributes and improves disentanglement. We highlight that our flexible framework covers multiple settings as disentangling labeled attributes, pose and appearance, localized concepts, and shape and texture. We present significantly better disentanglement with higher translation quality and greater output diversity than state-of-the-art methods.},
  archive   = {C_ICCV},
  author    = {Aviv Gabbay and Yedid Hoshen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00671},
  pages     = {6763-6772},
  title     = {Scaling-up disentanglement for image translation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DeepCAD: A deep generative network for computer-aided design
models. <em>ICCV</em>, 6752–6762. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep generative models of 3D shapes have received a great deal of research interest. Yet, almost all of them generate discrete shape representations, such as voxels, point clouds, and polygon meshes. We present the first 3D generative model for a drastically different shape representation— describing a shape as a sequence of computer-aided design (CAD) operations. Unlike meshes and point clouds, CAD models encode the user creation process of 3D shapes, widely used in numerous industrial and engineering design tasks. However, the sequential and irregular structure of CAD operations poses significant challenges for existing 3D generative models. Drawing an analogy between CAD operations and natural language, we propose a CAD generative network based on the Transformer. We demonstrate the performance of our model for both shape autoencoding and random shape generation. To train our network, we create a new CAD dataset consisting of 178,238 models and their CAD construction sequences. We have made this dataset publicly available to promote future research on this topic.},
  archive   = {C_ICCV},
  author    = {Rundi Wu and Chang Xiao and Changxi Zheng},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00670},
  pages     = {6752-6762},
  title     = {DeepCAD: A deep generative network for computer-aided design models},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-class multi-instance count conditioned adversarial
image generation. <em>ICCV</em>, 6742–6751. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Image generation has rapidly evolved in recent years. Modern architectures for adversarial training allow to generate even high resolution images with remarkable quality. At the same time, more and more effort is dedicated towards controlling the content of generated images. In this paper, we take one further step in this direction and propose a conditional generative adversarial network (GAN) that generates images with a defined number of objects from given classes. This entails two fundamental abilities (1) being able to generate high-quality images given a complex constraint and (2) being able to count object instances per class in a given image. Our proposed model modularly extends the successful StyleGAN2 architecture with a count-based conditioning as well as with a regression subnetwork to count the number of generated objects per class during training. In experiments on three different datasets, we show that the proposed model learns to generate images according to the given multiple-class count condition even in the presence of complex backgrounds. In particular, we propose a new dataset, CityCount, which is derived from the Cityscapes street scenes dataset, to evaluate our approach in a challenging and practically relevant scenario. An implementation is available at https://github.com/boschresearch/MCCGAN.},
  archive   = {C_ICCV},
  author    = {Amrutha Saseendran and Kathrin Skubch and Margret Keuper},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00669},
  pages     = {6742-6751},
  title     = {Multi-class multi-instance count conditioned adversarial image generation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Harnessing the conditioning sensorium for improved image
translation. <em>ICCV</em>, 6732–6741. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-modal domain translation typically refers to synthesizing a novel image that inherits certain localized attributes from a ‘content’ image (e.g. layout, semantics, or geometry), and inherits everything else (e.g. texture, lighting, sometimes even semantics) from a ‘style’ image. The dominant approach to this task is attempting to learn disentangled ‘content’ and ‘style’ representations from scratch. However, this is not only challenging, but ill-posed, as what users wish to preserve during translation varies depending on their goals. Motivated by this inherent ambiguity, we define ‘content’ based on conditioning information extracted by off-the-shelf pre-trained models. We then train our style extractor and image decoder with an easy to optimize set of reconstruction objectives. The wide variety of high-quality pre-trained models available and simple training procedure makes our approach straightforward to apply across numerous domains and definitions of ‘content’. Additionally it offers intuitive control over which aspects of ’content’ are preserved across domains. We evaluate our method on traditional, well-aligned, datasets such as CelebA-HQ, and propose two novel datasets for evaluation on more complex scenes: ClassicTV and FFHQ-Wild. Our approach, Sensorium, enables higher quality domain translation for more complex scenes.},
  archive   = {C_ICCV},
  author    = {Cooper Nederhood and Nicholas Kolkin and Deqing Fu and Jason Salavon},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00668},
  pages     = {6732-6741},
  title     = {Harnessing the conditioning sensorium for improved image translation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). F-drop&amp;match: GANs with a dead zone in the
high-frequency domain. <em>ICCV</em>, 6723–6731. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generative adversarial networks built from deep convolutional neural networks (GANs) lack the ability to exactly replicate the high-frequency components of natural images. To alleviate this issue, we introduce two novel training techniques called frequency dropping (F-Drop) and frequency matching (F-Match). The key idea of F-Drop is to filter out unnecessary high-frequency components from the input images of the discriminators. This simple modification prevents the discriminators from being confused by perturbations of the high-frequency components. In addition, F- Drop makes the GANs focus on fitting in the low-frequency domain, in which there are the dominant components of natural images. F-Match minimizes the difference between real and fake images in the frequency domain for generating more realistic images. F-Match is implemented as a regularization term in the objective functions of the generators; it penalizes the batch mean error in the frequency domain. F-Match helps the generators to fit in the high-frequency domain filtered out by F-Drop to the real image. We experimentally demonstrate that the combination of F-Drop and F-Match improves the generative performance of GANs in both the frequency and spatial domain on multiple image benchmarks.},
  archive   = {C_ICCV},
  author    = {Shin&#39;ya Yamaguchi and Sekitoshi Kanai},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00667},
  pages     = {6723-6731},
  title     = {F-Drop&amp;Match: GANs with a dead zone in the high-frequency domain},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dual contrastive loss and attention for GANs. <em>ICCV</em>,
6711–6722. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generative Adversarial Networks (GANs) produce impressive results on unconditional image generation when powered with large-scale image datasets. Yet generated images are still easy to spot especially on datasets with high variance (e.g. bedroom, church). In this paper, we propose various improvements to further push the boundaries in image generation. Specifically, we propose a novel dual contrastive loss and show that, with this loss, discriminator learns more generalized and distinguishable representations to incentivize generation. In addition, we revisit attention and extensively experiment with different attention blocks in the generator. We find attention to be still an important module for successful image generation even though it was not used in the recent state-of-the-art models. Lastly, we study different attention architectures in the discriminator, and propose a reference attention mechanism. By combining the strengths of these remedies, we improve the compelling state-of-the-art Fréchet Inception Distance (FID) by at least 17.5\% on several benchmark datasets. We obtain even more significant improvements on compositional synthetic scenes (up to 47.5\% in FID).},
  archive   = {C_ICCV},
  author    = {Ning Yu and Guilin Liu and Aysegul Dundar and Andrew Tao and Bryan Catanzaro and Larry Davis and Mario Fritz},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00666},
  pages     = {6711-6722},
  title     = {Dual contrastive loss and attention for GANs},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Orthogonal jacobian regularization for unsupervised
disentanglement in image generation. <em>ICCV</em>, 6701–6710. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unsupervised disentanglement learning is a crucial issue for understanding and exploiting deep generative models. Recently, SeFa tries to find latent disentangled directions by performing SVD on the first projection of a pretrained GAN. However, it is only applied to the first layer and works in a post-processing way. Hessian Penalty minimizes the off-diagonal entries of the output’s Hessian matrix to facilitate disentanglement, and can be applied to multi-layers. However, it constrains each entry of output independently, making it not sufficient in disentangling the latent directions (e.g., shape, size, rotation, etc.) of spatially correlated variations. In this paper, we propose a simple Orthogonal Jacobian Regularization (OroJaR) to encourage deep generative model to learn disentangled representations. It simply encourages the variation of output caused by perturbations on different latent dimensions to be orthogonal, and the Jacobian with respect to the input is calculated to represent this variation. We show that our OroJaR also encourages the output’s Hessian matrix to be diagonal in an indirect manner. In contrast to the Hessian Penalty, our OroJaR constrains the output in a holistic way, making it very effective in disentangling latent dimensions corresponding to spatially correlated variations. Quantitative and qualitative experimental results show that our method is effective in disentangled and controllable image generation, and performs favorably against the state-of-the-art methods. Our code is available at https://github.com/csyxwei/OroJaR.},
  archive   = {C_ICCV},
  author    = {Yuxiang Wei and Yupeng Shi and Xiao Liu and Zhilong Ji and Yuan Gao and Zhongqin Wu and Wangmeng Zuo},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00665},
  pages     = {6701-6710},
  title     = {Orthogonal jacobian regularization for unsupervised disentanglement in image generation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ReStyle: A residual-based StyleGAN encoder via iterative
refinement. <em>ICCV</em>, 6691–6700. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, the power of unconditional image synthesis has significantly advanced through the use of Generative Adversarial Networks (GANs). The task of inverting an image into its corresponding latent code of the trained GAN is of utmost importance as it allows for the manipulation of real images, leveraging the rich semantics learned by the network. Recognizing the limitations of current inversion approaches, in this work we present a novel inversion scheme that extends current encoder-based inversion methods by introducing an iterative refinement mechanism. Instead of directly predicting the latent code of a given real image using a single pass, the encoder is tasked with predicting a residual with respect to the current estimate of the inverted latent code in a self-correcting manner. Our residual-based encoder, named ReStyle, attains improved accuracy compared to current state-of-the-art encoder-based methods with a negligible increase in inference time. We analyze the behavior of ReStyle to gain valuable insights into its iterative nature. We then evaluate the performance of our residual encoder and analyze its robustness compared to optimization-based inversion and state-of-the-art encoders. Code is available via our project page: https: //yuval-alaluf.github.io/restyle-encoder/},
  archive   = {C_ICCV},
  author    = {Yuval Alaluf and Or Patashnik and Daniel Cohen-Or},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00664},
  pages     = {6691-6700},
  title     = {ReStyle: A residual-based StyleGAN encoder via iterative refinement},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). When do GANs replicate? On the choice of dataset size.
<em>ICCV</em>, 6681–6690. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Do GANs replicate training images? Previous studies have shown that GANs do not seem to replicate training data without significant change in the training procedure. This leads to a series of research on the exact condition needed for GANs to overfit to the training data. Although a number of factors has been theoretically or empirically identified, the effect of dataset size and complexity on GANs replication is still unknown. With empirical evidence from BigGAN and StyleGAN2, on datasets CelebA, Flower and LSUN-bedroom, we show that dataset size and its complexity play an important role in GANs replication and perceptual quality of the generated images. We further quantify this relationship, discovering that replication percentage decays exponentially with respect to dataset size and complexity, with a shared decaying factor across GAN-dataset combinations. Meanwhile, the perceptual image quality follows a U-shape trend w.r.t dataset size. This finding leads to a practical tool for one-shot estimation on minimal dataset size to prevent GAN replication which can be used to guide datasets construction and selection.},
  archive   = {C_ICCV},
  author    = {Qianli Feng and Chenqi Guo and Fabian Benitez-Quiroz and Aleix Martinez},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00663},
  pages     = {6681-6690},
  title     = {When do GANs replicate? on the choice of dataset size},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generative layout modeling using constraint graphs.
<em>ICCV</em>, 6670–6680. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a new generative model for layout generation. We generate layouts in three steps. First, we generate the layout elements as nodes in a layout graph. Second, we compute constraints between layout elements as edges in the layout graph. Third, we solve for the final layout using constrained optimization. For the first two steps, we build on recent transformer architectures. The layout optimization implements the constraints efficiently. We show three practical contributions compared to the state of the art: our work requires no user input, produces higher quality layouts, and enables many novel capabilities for conditional layout generation.},
  archive   = {C_ICCV},
  author    = {Wamiq Para and Paul Guerrero and Tom Kelly and Leonidas Guibas and Peter Wonka},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00662},
  pages     = {6670-6680},
  title     = {Generative layout modeling using constraint graphs},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extending neural p-frame codecs for b-frame coding.
<em>ICCV</em>, 6660–6669. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While most neural video codecs address P-frame coding (predicting each frame from past ones), in this paper we address B-frame compression (predicting frames using both past and future reference frames). Our B-frame solution is based on the existing P-frame methods. As a result, B-frame coding capability can easily be added to an existing neural codec. The basic idea of our B-frame coding method is to interpolate the two reference frames to generate a single reference frame and then use it together with an existing P-frame codec to encode the input B-frame. Our studies show that the interpolated frame is a much better reference for the P-frame codec compared to using the previous frame as is usually done. Our results show that using the proposed method with an existing P-frame codec can lead to 28.5\% saving in bit-rate on the UVG dataset compared to the P-frame codec while generating the same video quality.},
  archive   = {C_ICCV},
  author    = {Reza Pourreza and Taco Cohen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00661},
  pages     = {6660-6669},
  title     = {Extending neural P-frame codecs for B-frame coding},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Searching for robustness: Loss learning for noisy
classification tasks. <em>ICCV</em>, 6650–6659. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a &quot;learning to learn&quot; approach for discovering white-box classification loss functions that are robust to label noise in the training data. We parameterise a flexible family of loss functions using Taylor polynomials, and apply evolutionary strategies to search for noise-robust losses in this space. To learn re-usable loss functions that can apply to new tasks, our fitness function scores their performance in aggregate across a range of training datasets and architectures. The resulting white-box loss provides a simple and fast &quot;plug-and-play&quot; module that enables effective label-noise-robust learning in diverse downstream tasks, without requiring a special training procedure or network architecture. The efficacy of our loss is demonstrated on a variety of datasets with both synthetic and real label noise, where we compare favourably to prior work.},
  archive   = {C_ICCV},
  author    = {Boyan Gao and Henry Gouk and Timothy M. Hospedales},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00660},
  pages     = {6650-6659},
  title     = {Searching for robustness: Loss learning for noisy classification tasks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evolving search space for neural architecture search.
<em>ICCV</em>, 6639–6649. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automation of neural architecture design has been a coveted alternative to human experts. Various search methods have been proposed aiming to find the optimal architecture in the search space. One would expect the search results to improve when the search space grows larger since it would potentially contain more performant candidates. Surprisingly, we observe that enlarging search space is unbeneficial or even detrimental to existing NAS methods such as DARTS, ProxylessNAS, and SPOS. This counterintuitive phenomenon suggests that enabling existing methods to large search space regimes is non-trivial. However, this problem is less discussed in the literature.We present a Neural Search-space Evolution (NSE) scheme, the first neural architecture search scheme designed especially for large space neural architecture search problems. The necessity of a well-designed search space with constrained size is a tacit consent in existing methods, and our NSE aims at minimizing such necessity. Specifically, the NSE starts with a search space subset, then evolves the search space by repeating two steps: 1) search an optimized space from the search space subset, 2) refill this subset from a large pool of operations that are not traversed. We further extend the flexibility of obtainable architectures by introducing a learnable multi-branch setting. With the proposed method, we achieve 77.3\% top-1 retrain accuracy on ImageNet with 333M FLOPs, which yielded a state-of-the-art performance among previous auto-generated architectures that do not involve knowledge distillation or weight pruning. When the latency constraint is adopted, our result also performs better than the previous best-performing mobile models with a 77.9\% Top-1 retrain accuracy. Code is available at https://github.com/orashi/NSENAS.},
  archive   = {C_ICCV},
  author    = {Yuanzheng Ci and Chen Lin and Ming Sun and Boyu Chen and Hongwen Zhang and Wanli Ouyang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00659},
  pages     = {6639-6649},
  title     = {Evolving search space for neural architecture search},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AdaAttN: Revisit attention mechanism in arbitrary neural
style transfer. <em>ICCV</em>, 6629–6638. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fast arbitrary neural style transfer has attracted widespread attention from academic, industrial and art communities due to its flexibility in enabling various applications. Existing solutions either attentively fuse deep style feature into deep content feature without considering feature distributions, or adaptively normalize deep content feature according to the style such that their global statistics are matched. Although effective, leaving shallow feature unexplored and without locally considering feature statistics, they are prone to unnatural output with unpleasing local distortions. To alleviate this problem, in this paper, we propose a novel attention and normalization module, named Adaptive Attention Normalization (AdaAttN), to adaptively perform attentive normalization on per-point basis. Specifically, spatial attention score is learnt from both shallow and deep features of content and style images. Then perpoint weighted statistics are calculated by regarding a style feature point as a distribution of attention-weighted output of all style feature points. Finally, the content feature is normalized so that they demonstrate the same local feature statistics as the calculated per-point weighted style feature statistics. Besides, a novel local feature loss is derived based on AdaAttN to enhance local visual quality. We also extend AdaAttN to be ready for video style transfer with slight modifications. Experiments demonstrate that our method achieves state-of-the-art arbitrary image/video style transfer. Codes and models are available on https://github.com/wzmsltw/AdaAttN.},
  archive   = {C_ICCV},
  author    = {Songhua Liu and Tianwei Lin and Dongliang He and Fu Li and Meiling Wang and Xin Li and Zhengxing Sun and Qian Li and Errui Ding},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00658},
  pages     = {6629-6638},
  title     = {AdaAttN: Revisit attention mechanism in arbitrary neural style transfer},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PixelPyramids: Exact inference models from lossless image
pyramids. <em>ICCV</em>, 6619–6628. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autoregressive models are a class of exact inference approaches with highly flexible functional forms, yielding state-of-the-art density estimates for natural images. Yet, the sequential ordering on the dimensions makes these models computationally expensive and limits their applicability to low-resolution imagery. In this work, we propose PixelPyramids, 1 a block-autoregressive approach employing a lossless pyramid decomposition with scale-specific representations to encode the joint distribution of image pixels. Crucially, it affords a sparser dependency structure compared to fully autoregressive approaches. Our PixelPyramids yield state-of-the-art results for density estimation on various image datasets, especially for high-resolution data. For CelebA-HQ 1024 × 1024, we observe that the density estimates (in terms of bits/dim) are improved to ~44\% of the baseline despite sampling speeds superior even to easily parallelizable flow-based models.},
  archive   = {C_ICCV},
  author    = {Shweta Mahajan and Stefan Roth},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00657},
  pages     = {6619-6628},
  title     = {PixelPyramids: Exact inference models from lossless image pyramids},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Domain generalization via gradient surgery. <em>ICCV</em>,
6610–6618. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In real-life applications, machine learning models often face scenarios where there is a change in data distribution between training and test domains. When the aim is to make predictions on distributions different from those seen at training, we incur in a domain generalization problem. Methods to address this issue learn a model using data from multiple source domains, and then apply this model to the unseen target domain. Our hypothesis is that when training with multiple domains, conflicting gradients within each mini-batch contain information specific to the individual domains which is irrelevant to the others, including the test domain. If left untouched, such disagreement may degrade generalization performance. In this work, we characterize the conflicting gradients emerging in domain shift scenarios and devise novel gradient agreement strategies based on gradient surgery to alleviate their effect. We validate our approach in image classification tasks with three multi-domain datasets, showing the value of the proposed agreement strategy in enhancing the generalization capability of deep learning models in domain shift scenarios.},
  archive   = {C_ICCV},
  author    = {Lucas Mansilla and Rodrigo Echeveste and Diego H. Milone and Enzo Ferrante},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00656},
  pages     = {6610-6618},
  title     = {Domain generalization via gradient surgery},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semantic perturbations with normalizing flows for improved
generalization. <em>ICCV</em>, 6599–6609. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Data augmentation is a widely adopted technique for avoiding overfitting when training deep neural networks. However, this approach requires domain-specific knowledge and is often limited to a fixed set of hard-coded transformations. Recently, several works proposed to use generative models for generating semantically meaningful perturbations to train a classifier. However, because accurate encoding and decoding are critical, these methods, which use architectures that approximate the latent-variable inference, remained limited to pilot studies on small datasets.Exploiting the exactly reversible encoder-decoder structure of normalizing flows, we perform on-manifold perturbations in the latent space to define fully unsupervised data augmentations. We demonstrate that such perturbations match the performance of advanced data augmentation techniques—reaching 96.6\% test accuracy for CIFAR10 using ResNet-18 and outperform existing methods, particularly in low data regimes—yielding 10–25\% relative improvement of test accuracy from classical training. We find that our latent adversarial perturbations adaptive to the classifier throughout its training are most effective, yielding the first test accuracy improvement results on real-world datasets—CIFAR-10/100—via latent-space perturbations.},
  archive   = {C_ICCV},
  author    = {Oğuz Kaan Yüksel and Sebastian U. Stich and Martin Jaggi and Tatjana Chavdarova},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00655},
  pages     = {6599-6609},
  title     = {Semantic perturbations with normalizing flows for improved generalization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust trust region for weakly supervised segmentation.
<em>ICCV</em>, 6588–6598. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Acquisition of training data for the standard semantic segmentation is expensive if requiring that each pixel is labeled. Yet, current methods significantly deteriorate in weakly supervised settings, e.g. where a fraction of pixels is labeled or when only image-level tags are available. It has been shown that regularized losses—originally developed for unsupervised low-level segmentation and representing geometric priors on pixel labels—can considerably improve the quality of weakly supervised training. However, many common priors require optimization stronger than gradient descent. Thus, such regularizers have limited applicability in deep learning. We propose a new robust trust region approach 1 for regularized losses improving the state-of-the-art results. Our approach can be seen as a higher-order generalization of the classic chain rule. It allows neural network optimization to use strong low-level solvers for the corresponding regularizers, including discrete ones.},
  archive   = {C_ICCV},
  author    = {Dmitrii Marin and Yuri Boykov},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00654},
  pages     = {6588-6598},
  title     = {Robust trust region for weakly supervised segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Paint transformer: Feed forward neural painting with stroke
prediction. <em>ICCV</em>, 6578–6587. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Neural painting refers to the procedure of producing a series of strokes for a given image and non-photo-realistically recreating it using neural networks. While reinforcement learning (RL) based agents can generate a stroke sequence step by step for this task, it is not easy to train a stable RL agent. On the other hand, stroke optimization methods search for a set of stroke parameters iteratively in a large search space; such low efficiency significantly limits their prevalence and practicality. Different from previous methods, in this paper, we formulate the task as a set prediction problem and propose a novel Transformer-based framework, dubbed Paint Transformer, to predict the parameters of a stroke set with a feed forward network. This way, our model can generate a set of strokes in parallel and obtain the final painting of size 512 × 512 in near real time. More importantly, since there is no dataset available for training the Paint Transformer, we devise a self-training pipeline such that it can be trained without any off-the-shelf dataset while still achieving excellent generalization capability. Experiments demonstrate that our method achieves better painting performance than previous ones with cheaper training and inference costs. Codes and models are available on https://github.com/wzmsltw/PaintTransformer.},
  archive   = {C_ICCV},
  author    = {Songhua Liu and Tianwei Lin and Dongliang He and Fu Li and Ruifeng Deng and Xin Li and Errui Ding and Hao Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00653},
  pages     = {6578-6587},
  title     = {Paint transformer: Feed forward neural painting with stroke prediction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Manifold matching via deep metric learning for generative
modeling. <em>ICCV</em>, 6567–6577. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a manifold matching approach to generative models which includes a distribution generator (or data generator) and a metric generator. In our framework, we view the real data set as some manifold embedded in a high-dimensional Euclidean space. The distribution generator aims at generating samples that follow some distribution condensed around the real data manifold. It is achieved by matching two sets of points using their geometric shape descriptors, such as centroid and p-diameter, with learned distance metric; the metric generator utilizes both real data and generated samples to learn a distance metric which is close to some intrinsic geodesic distance on the real data manifold. The produced distance metric is further used for manifold matching. The two networks learn simultaneously during the training process. We apply the approach on both unsupervised and supervised learning tasks: in unconditional image generation task, the proposed method obtains competitive results compared with existing generative models; in super-resolution task, we incorporate the framework in perception-based models and improve visual qualities by producing samples with more natural textures. Experiments and analysis demonstrate the feasibility and effectiveness of the proposed framework.},
  archive   = {C_ICCV},
  author    = {Mengyu Dai and Haibin Hang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00652},
  pages     = {6567-6577},
  title     = {Manifold matching via deep metric learning for generative modeling},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A lazy approach to long-horizon gradient-based
meta-learning. <em>ICCV</em>, 6557–6566. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Gradient-based meta-learning first trains task-specific models by an inner loop and then backpropagates meta-gradients through the loop to update the meta-model. To avoid high-order gradients, existing methods either take a small number of inner steps or approximate the meta-updates for the situations that the meta-model and task models lie in the same space. To enable long inner horizons for more general meta-learning problems, we instead propose an intuitive teacher-student strategy. The key idea is to employ a student network to adequately explore the search space of task-specific models, followed by a teacher’s &quot;leap&quot; toward the regions probed by the student. The teacher not only arrives at a high-quality model but also defines a lightweight computational graph for the meta-gradients. Our approach is generic; it performs well when applied to four meta-learning algorithms over three tasks: few-shot learning, long-tailed object recognition, and adversarial blackbox attack.},
  archive   = {C_ICCV},
  author    = {Muhammad Abdullah Jamal and Liqiang Wang and Boqing Gong},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00651},
  pages     = {6557-6566},
  title     = {A lazy approach to long-horizon gradient-based meta-learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-knowledge distillation with progressive refinement of
targets. <em>ICCV</em>, 6547–6556. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The generalization capability of deep neural networks has been substantially improved by applying a wide spectrum of regularization methods, e.g., restricting function space, injecting randomness during training, augmenting data, etc. In this work, we propose a simple yet effective regularization method named progressive self-knowledge distillation (PS-KD), which progressively distills a model’s own knowledge to soften hard targets (i.e., one-hot vectors) during training. Hence, it can be interpreted within a framework of knowledge distillation as a student becomes a teacher itself. Specifically, targets are adjusted adaptively by combining the ground-truth and past predictions from the model itself. We show that PS-KD provides an effect of hard example mining by rescaling gradients according to difficulty in classifying examples. The proposed method is applicable to any supervised learning tasks with hard targets and can be easily combined with existing regularization methods to further enhance the generalization performance. Furthermore, it is confirmed that PS-KD achieves not only better accuracy, but also provides high quality of confidence estimates in terms of calibration as well as ordinal ranking. Extensive experimental results on three different tasks, image classification, object detection, and machine translation, demonstrate that our method consistently improves the performance of the state-of-the-art baselines. The code is available at https://github.com/lgcnsai/PS-KD-Pytorch.},
  archive   = {C_ICCV},
  author    = {Kyungyul Kim and ByeongMoon Ji and Doyoung Yoon and Sangheum Hwang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00650},
  pages     = {6547-6556},
  title     = {Self-knowledge distillation with progressive refinement of targets},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bias loss for mobile neural networks. <em>ICCV</em>,
6536–6546. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Compact convolutional neural networks (CNNs) have witnessed exceptional improvements in performance in recent years. However, they still fail to provide the same predictive power as CNNs with a large number of parameters. The diverse and even abundant features captured by the layers is an important characteristic of these successful CNNs. However, differences in this characteristic between large CNNs and their compact counterparts have rarely been investigated. In compact CNNs, due to the limited number of parameters, abundant features are unlikely to be obtained, and feature diversity becomes an essential characteristic. Diverse features present in the activation maps derived from a data point during model inference may indicate the presence of a set of unique descriptors necessary to distinguish between objects of different classes. In contrast, data points with low feature diversity may not provide a sufficient amount of unique descriptors to make a valid prediction; we refer to them as random predictions. Random predictions can negatively impact the optimization process and harm the final performance. This paper proposes addressing the problem raised by random predictions by reshaping the standard cross-entropy to make it biased toward data points with a limited number of unique descriptive features. Our novel Bias Loss focuses the training on a set of valuable data points and prevents the vast number of samples with poor learning features from misleading the optimization process. Furthermore, to show the importance of diversity, we present a family of SkipblockNet models whose architectures are brought to boost the number of unique descriptors in the last layers. Experiments conducted on benchmark datasets demonstrate the superiority of the proposed loss function over the cross-entropy loss. Moreover, our SkipblockNet-M can achieve 1\% higher classification accuracy than MobileNetV3 Large with similar computational cost on the ImageNet ILSVRC-2012 classification dataset. The code is available on the link - https://github.com/lusinlu/biasloss_skipblocknet.},
  archive   = {C_ICCV},
  author    = {Lusine Abrahamyan and Valentin Ziatchin and Yiming Chen and Nikos Deligiannis},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00649},
  pages     = {6536-6546},
  title     = {Bias loss for mobile neural networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SPatchGAN: A statistical feature based discriminator for
unsupervised image-to-image translation. <em>ICCV</em>, 6526–6535. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For unsupervised image-to-image translation, we propose a discriminator architecture which focuses on the statistical features instead of individual patches. The network is stabilized by distribution matching of key statistical features at multiple scales. Unlike the existing methods which impose more and more constraints on the generator, our method facilitates the shape deformation and enhances the fine details with a greatly simplified framework. We show that the proposed method outperforms the existing state-of-the-art models in various challenging applications including selfie-to-anime, male-to-female and glasses removal.},
  archive   = {C_ICCV},
  author    = {Xuning Shao and Weidong Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00648},
  pages     = {6526-6535},
  title     = {SPatchGAN: A statistical feature based discriminator for unsupervised image-to-image translation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatio-temporal self-supervised representation learning for
3D point clouds. <em>ICCV</em>, 6515–6525. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To date, various 3D scene understanding tasks still lack practical and generalizable pre-trained models, primarily due to the intricate nature of 3D scene understanding tasks and their immense variations introduced by camera views, lighting, occlusions, etc. In this paper, we tackle this challenge by introducing a spatio-temporal representation learning (STRL) framework, capable of learning from unlabeled 3D point clouds in a self-supervised fashion. Inspired by how infants learn from visual data in the wild, we explore the rich spatio-temporal cues derived from the 3D data. Specifically, STRL takes two temporally-correlated frames from a 3D point cloud sequence as the input, transforms it with the spatial data augmentation, and learns the invariant representation self-supervisedly. To corroborate the efficacy of STRL, we conduct extensive experiments on three types (synthetic, indoor, and outdoor) of datasets. Experimental results demonstrate that, compared with supervised learning methods, the learned self-supervised representation facilitates various models to attain comparable or even better performances while capable of generalizing pre-trained models to downstream tasks, including 3D shape classification, 3D object detection, and 3D semantic segmentation. Moreover, the spatio-temporal contextual cues embedded in 3D point clouds significantly improve the learned representations.},
  archive   = {C_ICCV},
  author    = {Siyuan Huang and Yichen Xie and Song-Chun Zhu and Yixin Zhu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00647},
  pages     = {6515-6525},
  title     = {Spatio-temporal self-supervised representation learning for 3D point clouds},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning signed distance field for multi-view surface
reconstruction. <em>ICCV</em>, 6505–6514. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent works on implicit neural representations have shown promising results for multi-view surface reconstruction. However, most approaches are limited to relatively simple geometries and usually require clean object masks for reconstructing complex and concave objects. In this work, we introduce a novel neural surface reconstruction framework that leverages the knowledge of stereo matching and feature consistency to optimize the implicit surface representation. More specifically, we apply a signed distance field (SDF) and a surface light field to represent the scene geometry and appearance respectively. The SDF is directly supervised by geometry from stereo matching, and is refined by optimizing the multi-view feature consistency and the fidelity of rendered images. Our method is able to improve the robustness of geometry estimation and support reconstruction of complex scene topologies. Extensive experiments have been conducted on DTU, EPFL and Tanks and Temples datasets. Compared to previous state-of-the-art methods, our method achieves better mesh reconstruction in wide open scenes without masks as input.},
  archive   = {C_ICCV},
  author    = {Jingyang Zhang and Yao Yao and Long Quan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00646},
  pages     = {6505-6514},
  title     = {Learning signed distance field for multi-view surface reconstruction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vis2Mesh: Efficient mesh reconstruction from unstructured
point clouds of large scenes with learned virtual view visibility.
<em>ICCV</em>, 6494–6504. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel framework for mesh reconstruction from unstructured point clouds by taking advantage of the learned visibility of the 3D points in the virtual views and traditional graph-cut based mesh generation. Specifically, we first propose a three-step network that explicitly employs depth completion for visibility prediction. Then the visibility information of multiple views is aggregated to generate a 3D mesh model by solving an optimization problem considering visibility in which a novel adaptive visibility weighting in surface determination is also introduced to suppress line of sight with a large incident angle. Compared to other learning-based approaches, our pipeline only exercises the learning on a 2D binary classification task, i.e., points visible or not in a view, which is much more generalizable and practically more efficient and capable to deal with a large number of points. Experiments demonstrate that our method with favorable transferability and robustness, and achieve competing performances w.r.t. state-of-the-art learning-based approaches on small complex objects and outperforms on large indoor and outdoor scenes. Code is available at https://github.com/GDAOSU/vis2mesh.},
  archive   = {C_ICCV},
  author    = {Shuang Song and Zhaopeng Cui and Rongjun Qin},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00645},
  pages     = {6494-6504},
  title     = {Vis2Mesh: Efficient mesh reconstruction from unstructured point clouds of large scenes with learned virtual view visibility},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SA-ConvONet: Sign-agnostic optimization of convolutional
occupancy networks. <em>ICCV</em>, 6484–6493. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Surface reconstruction from point clouds is a fundamental problem in the computer vision and graphics community. Recent state-of-the-arts solve this problem by individually optimizing each local implicit field during inference. Without considering the geometric relationships between local fields, they typically require accurate normals to avoid the sign conflict problem in overlapped regions of local fields, which severely limits their applicability to raw scans where surface normals could be unavailable. Although SAL breaks this limitation via sign-agnostic learning, further works still need to explore how to extend this technique for local shape modeling. To this end, we propose to learn implicit surface reconstruction by sign-agnostic optimization of convolutional occupancy networks, to simultaneously achieve advanced scalability to large-scale scenes, generality to novel shapes, and applicability to raw scans in a unified framework. Concretely, we achieve this goal by a simple yet effective design, which further optimizes the pre-trained occupancy prediction networks with an unsigned cross-entropy loss during inference. The learning of occupancy fields is conditioned on convolutional features from an hourglass network architecture. Extensive experimental comparisons with previous state-of-the-arts on both object-level and scene-level datasets demonstrate the superior accuracy of our approach for surface reconstruction from un-orientated point clouds. The code is available at https://github.com/tangjiapeng/SA-ConvONet.},
  archive   = {C_ICCV},
  author    = {Jiapeng Tang and Jiabao Lei and Dan Xu and Feiying Ma and Kui Jia and Lei Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00644},
  pages     = {6484-6493},
  title     = {SA-ConvONet: Sign-agnostic optimization of convolutional occupancy networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). JEM++: Improved techniques for training JEM. <em>ICCV</em>,
6474–6483. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Joint Energy-based Model (JEM) [12] is a recently proposed hybrid model that retains strong discriminative power of modern CNN classifiers, while generating samples rivaling the quality of GAN-based approaches. In this paper, we propose a variety of new training procedures and architecture features to improve JEM’s accuracy, training stability, and speed altogether. 1) We propose a proximal SGLD to generate samples in the proximity of samples from previous step, which improves the stability. 2) We further treat the approximate maximum likelihood learning of EBM as a multi-step differential game, and extend the YOPO framework [47] to cut out redundant calculations during backpropagation, which accelerates the training substantially. 3) Rather than initializing SGLD chain from random noise, we introduce a new informative initialization that samples from a distribution estimated from training data. 4) This informative initialization allows us to enable batch normalization in JEM, which further releases the power of modern CNN architectures for hybrid modeling. 1},
  archive   = {C_ICCV},
  author    = {Xiulong Yang and Shihao Ji},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00643},
  pages     = {6474-6483},
  title     = {JEM++: Improved techniques for training JEM},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Collaborative optimization and aggregation for decentralized
domain generalization and adaptation. <em>ICCV</em>, 6464–6473. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Contemporary domain generalization (DG) and multisource unsupervised domain adaptation (UDA) methods mostly collect data from multiple domains together for joint optimization. However, this centralized training paradigm poses a threat to data privacy and is not applicable when data are non-shared across domains. In this work, we propose a new approach called Collaborative Optimization and Aggregation (COPA), which aims at optimizing a generalized target model for decentralized DG and UDA, where data from different domains are non-shared and private. Our base model consists of a domain-invariant feature extractor and an ensemble of domain-specific classifiers. In an iterative learning process, we optimize a local model for each domain, and then centrally aggregate local feature extractors and assemble domain-specific classifiers to construct a generalized global model, without sharing data from different domains. To improve generalization of feature extractors, we employ hybrid batch-instance normalization and collaboration of frozen classifiers. For better decentralized UDA, we further introduce a prediction agreement mechanism to overcome local disparities towards central model aggregation. Extensive experiments on five DG and UDA benchmark datasets show that COPA is capable of achieving comparable performance against the state-of-the-art DG and UDA methods without the need for centralized data collection in model training.},
  archive   = {C_ICCV},
  author    = {Guile Wu and Shaogang Gong},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00642},
  pages     = {6464-6473},
  title     = {Collaborative optimization and aggregation for decentralized domain generalization and adaptation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Generalized shuffled linear regression. <em>ICCV</em>,
6454–6463. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the shuffled linear regression problem where the correspondences between covariates and responses are unknown. While the existing formulation assumes an ideal underlying bijection in which all pieces of data should match, such an assumption barely holds in real-world applications due to either missing data or outliers. Therefore, in this work, we generalize the formulation of shuffled linear regression to a broader range of conditions where only part of the data should correspond. Moreover, we present a remarkably simple yet effective optimization algorithm with guaranteed global convergence. Distinct tasks validate the effectiveness of the proposed method. 1},
  archive   = {C_ICCV},
  author    = {Feiran Li and Kent Fujiwara and Fumio Okura and Yasuyuki Matsushita},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00641},
  pages     = {6454-6463},
  title     = {Generalized shuffled linear regression},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Progressive correspondence pruning by consensus learning.
<em>ICCV</em>, 6444–6453. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Correspondence pruning aims to correctly remove false matches (outliers) from an initial set of putative correspondences. The pruning process is challenging since putative matches are typically extremely unbalanced, largely dominated by outliers, and the random distribution of such outliers further complicates the learning process for learning-based methods. To address this issue, we propose to progressively prune the correspondences via a local-to-global consensus learning procedure. We introduce a &quot;pruning&quot; block that lets us identify reliable candidates among the initial matches according to consensus scores estimated using local-to-global dynamic graphs. We then achieve progressive pruning by stacking multiple pruning blocks sequentially. Our method outperforms state-of-the-arts on robust line fitting, camera pose estimation and retrieval-based image localization benchmarks by significant margins and shows promising generalization ability to different datasets and detector/descriptor combinations.},
  archive   = {C_ICCV},
  author    = {Chen Zhao and Yixiao Ge and Feng Zhu and Rui Zhao and Hongsheng Li and Mathieu Salzmann},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00640},
  pages     = {6444-6453},
  title     = {Progressive correspondence pruning by consensus learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Synchronization of group-labelled multi-graphs.
<em>ICCV</em>, 6433–6443. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Synchronization refers to the problem of inferring the unknown values attached to vertices of a graph where edges are labelled with the ratio of the incident vertices, and labels belong to a group. This paper addresses the synchronization problem on multi-graphs, that are graphs with more than one edge connecting the same pair of nodes. The problem naturally arises when multiple measures are available to model the relationship between two vertices. This happens when different sensors measure the same quantity, or when the original graph is partitioned into sub-graphs that are solved independently. In this case, the relationships among sub-graphs give rise to multi-edges and the problem can be traced back to a multi-graph synchronization. The baseline solution reduces multi-graphs to simple ones by averaging their multi-edges, however this approach falls short because: i) averaging is well defined only for some groups and ii) the resulting estimator is less precise and accurate, as we prove empirically. Specifically, we present MULTISYNC, a synchronization algorithm for multi-graphs that is based on a principled constrained eigenvalue optimization. MULTISYNC is a general solution that can cope with any linear group and we show to be profitably usable both on synthetic and real problems.},
  archive   = {C_ICCV},
  author    = {Andrea Porfiri Dal Cin and Luca Magri and Federica Arrigoni and Andrea Fusiello and Giacomo Boracchi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00639},
  pages     = {6433-6443},
  title     = {Synchronization of group-labelled multi-graphs},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning with noisy labels for robust point cloud
segmentation. <em>ICCV</em>, 6423–6432. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Point cloud segmentation is a fundamental task in 3D. Despite recent progress on point cloud segmentation with the power of deep networks, current deep learning methods based on the clean label assumptions may fail with noisy labels. Yet, object class labels are often mislabeled in real-world point cloud datasets. In this work, we take the lead in solving this issue by proposing a novel Point Noise-Adaptive Learning (PNAL) framework. Compared to existing noise-robust methods on image tasks, our PNAL is noise-rate blind, to cope with the spatially variant noise rate problem specific to point clouds . Specifically, we propose a novel point-wise confidence selection to obtain reliable labels based on the historical predictions of each point. A novel cluster-wise label correction is proposed with a voting strategy to generate the best possible label taking the neighbor point correlations into consideration. We conduct extensive experiments to demonstrate the effectiveness of PNAL on both synthetic and real-world noisy datasets. In particular, even with 60\% symmetric noisy labels, our proposed method produces much better results than its baseline counterpart without PNAL and is comparable to the ideal upper bound trained on a completely clean dataset. Moreover, we fully re-labeled the validation set of a popular but noisy real-world scene dataset ScanNetV2 to make it clean, for rigorous experiment and future research. Our code and data will be released.},
  archive   = {C_ICCV},
  author    = {Shuquan Ye and Dongdong Chen and Songfang Han and Jing Liao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00638},
  pages     = {6423-6432},
  title     = {Learning with noisy labels for robust point cloud segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bootstrap your own correspondences. <em>ICCV</em>,
6413–6422. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Geometric feature extraction is a crucial component of point cloud registration pipelines. Recent work has demonstrated how supervised learning can be leveraged to learn better and more compact 3D features. However, those approaches’ reliance on ground-truth annotation limits their scalability. We propose BYOC: a self-supervised approach that learns visual and geometric features from RGB-D video without relying on ground-truth pose or correspondence. Our key observation is that randomly-initialized CNNs readily provide us with good correspondences; allowing us to bootstrap the learning of both visual and geometric features. Our approach combines classic ideas from point cloud registration with more recent representation learning approaches. We evaluate our approach on indoor scene datasets and find that our method outperforms traditional and learned descriptors, while being competitive with current state-of-the-art supervised approaches.},
  archive   = {C_ICCV},
  author    = {Mohamed El Banani and Justin Johnson},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00637},
  pages     = {6413-6422},
  title     = {Bootstrap your own correspondences},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guided point contrastive learning for semi-supervised point
cloud semantic segmentation. <em>ICCV</em>, 6403–6412. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Rapid progress in 3D semantic segmentation is inseparable from the advances of deep network models, which highly rely on large-scale annotated data for training. To address the high cost and challenges of 3D point-level labeling, we present a method for semi-supervised point cloud semantic segmentation to adopt unlabeled point clouds in training to boost the model performance. Inspired by the recent contrastive loss in self-supervised tasks, we propose the guided point contrastive loss to enhance the feature representation and model generalization ability in semi-supervised setting. Semantic predictions on unlabeled point clouds serve as pseudo-label guidance in our loss to avoid negative pairs in the same category. Also, we design the confidence guidance to ensure high-quality feature learning. Besides, a category-balanced sampling strategy is proposed to collect positive and negative samples to mitigate the class imbalance problem. Extensive experiments on three datasets (ScanNet V2, S3DIS, and SemanticKITTI) show the effectiveness of our semi-supervised method to improve the prediction quality with unlabeled data.},
  archive   = {C_ICCV},
  author    = {Li Jiang and Shaoshuai Shi and Zhuotao Tian and Xin Lai and Shu Liu and Chi-Wing Fu and Jiaya Jia},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00636},
  pages     = {6403-6412},
  title     = {Guided point contrastive learning for semi-supervised point cloud semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Progressive seed generation auto-encoder for unsupervised
point cloud learning. <em>ICCV</em>, 6393–6402. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the development of 3D scanning technologies, 3D vision tasks have become a popular research area. Owing to the large amount of data acquired by sensors, unsupervised learning is essential for understanding and utilizing point clouds without an expensive annotation process. In this paper, we propose a novel framework and an effective auto-encoder architecture named &quot;PSG-Net&quot; for reconstruction-based learning of point clouds. Unlike existing studies that used fixed or random 2D points, our framework generates input-dependent point-wise features for the latent point set. PSG-Net uses the encoded input to produce point-wise features through the seed generation module and extracts richer features in multiple stages with gradually increasing resolution by applying the seed feature propagation module progressively. We prove the effectiveness of PSG-Net experimentally; PSG-Net shows state-of-the-art performances in point cloud reconstruction and unsupervised classification, and achieves comparable performance to counterpart methods in supervised completion.},
  archive   = {C_ICCV},
  author    = {Juyoung Yang and Pyunghwan Ahn and Doyeon Kim and Haeil Lee and Junmo Kim},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00635},
  pages     = {6393-6402},
  title     = {Progressive seed generation auto-encoder for unsupervised point cloud learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Geometry-aware self-training for unsupervised domain
adaptation on object point clouds. <em>ICCV</em>, 6383–6392. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The point cloud representation of an object can have a large geometric variation in view of inconsistent data acquisition procedure, which thus leads to domain discrepancy due to diverse and uncontrollable shape representation cross datasets. To improve discrimination on unseen distribution of point-based geometries in a practical and feasible perspective, this paper proposes a new method of geometry-aware self-training (GAST) for unsupervised domain adaptation of object point cloud classification. Specifically, this paper aims to learn a domain-shared representation of semantic categories, via two novel self-supervised geometric learning tasks as feature regularization. On one hand, the representation learning is empowered by a linear mixup of point cloud samples with their self-generated rotation labels, to capture a global topological configuration of local geometries. On the other hand, a diverse point distribution across datasets can be normalized with a novel curvature-aware distortion localization. Experiments on the PointDA-10 dataset show that our GAST method can significantly outperform the state-of-the-art methods. Source codes and pre-trained models are available at https://github.com/zou-longkun/GAST.},
  archive   = {C_ICCV},
  author    = {Longkun Zou and Hui Tang and Ke Chen and Kui Jia},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00634},
  pages     = {6383-6392},
  title     = {Geometry-aware self-training for unsupervised domain adaptation on object point clouds},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). WarpedGANSpace: Finding non-linear RBF paths in GAN latent
space. <em>ICCV</em>, 6373–6382. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work addresses the problem of discovering, in an unsupervised manner, interpretable paths in the latent space of pretrained GANs, so as to provide an intuitive and easy way of controlling the underlying generative factors. In doing so, it addresses some of the limitations of the state-of-the-art works, namely, a) that they discover directions that are independent of the latent code, i.e., paths that are linear, and b) that their evaluation relies either on visual inspection or on laborious human labeling. More specifically, we propose to learn non-linear warpings on the latent space, each one parametrized by a set of RBF-based latent space warping functions, and where each warping gives rise to a family of non-linear paths via the gradient of the function. Building on the work of [34], that discovers linear paths, we optimize the trainable parameters of the set of RBFs, so as that images that are generated by codes along different paths, are easily distinguishable by a discriminator network. This leads to easily distinguishable image transformations, such as pose and facial expressions in facial images. We show that linear paths can be derived as a special case of our method, and show experimentally that non-linear paths in the latent space lead to steeper, more disentangled and interpretable changes in the image space than in state-of-the art methods, both qualitatively and quantitatively. We make the code and the pretrained models publicly available at: https://github.com/chi0tzp/WarpedGANSpace.},
  archive   = {C_ICCV},
  author    = {Christos Tzelepis and Georgios Tzimiropoulos and Ioannis Patras},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00633},
  pages     = {6373-6382},
  title     = {WarpedGANSpace: Finding non-linear RBF paths in GAN latent space},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DRB-GAN: A dynamic ResBlock generative adversarial network
for artistic style transfer. <em>ICCV</em>, 6363–6372. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The paper proposes a Dynamic ResBlock Generative Adversarial Network (DRB-GAN) for artistic style transfer. The style code is modeled as the shared parameters for Dynamic ResBlocks connecting both the style encoding network and the style transfer network. In the style encoding network, a style class-aware attention mechanism is used to attend the style feature representation for generating the style codes. In the style transfer network, multiple Dynamic ResBlocks are designed to integrate the style code and the extracted CNN semantic feature and then feed into the spatial window Layer-Instance Normalization (SW-LIN) decoder, which enables high-quality synthetic images with artistic style transfer. Moreover, the style collection conditional discriminator is designed to equip our DRB-GAN model with abilities for both arbitrary style transfer and collection style transfer during the training stage. No matter for arbitrary style transfer or collection style transfer, extensive experiments strongly demonstrate that our proposed DRB-GAN outperforms state-of-the-art methods and exhibits its superior performance in terms of visual quality and efficiency. Our source code is available at https://github.com/xuwenju123/DRB-GAN.},
  archive   = {C_ICCV},
  author    = {Wenju Xu and Chengjiang Long and Ruisheng Wang and Guanghui Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00632},
  pages     = {6363-6372},
  title     = {DRB-GAN: A dynamic ResBlock generative adversarial network for artistic style transfer},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gradient normalization for generative adversarial networks.
<em>ICCV</em>, 6353–6362. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel normalization method called gradient normalization (GN) to tackle the training instability of Generative Adversarial Networks (GANs) caused by the sharp gradient space. Unlike existing work such as gradient penalty and spectral normalization, the proposed GN only imposes a hard 1-Lipschitz constraint on the discriminator function, which increases the capacity of the discriminator. Moreover, the proposed gradient normalization can be applied to different GAN architectures with little modification. Extensive experiments on four datasets show that GANs trained with gradient normalization outperform existing methods in terms of both Frechet Inception Distance and Inception Score.},
  archive   = {C_ICCV},
  author    = {Yi-Lun Wu and Hong-Han Shuai and Zhi-Rui Tam and Hong-Yu Chiu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00631},
  pages     = {6353-6362},
  title     = {Gradient normalization for generative adversarial networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Auto graph encoder-decoder for neural network pruning.
<em>ICCV</em>, 6342–6352. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Model compression aims to deploy deep neural networks (DNN) on mobile devices with limited computing and storage resources. However, most of the existing model compression methods rely on manually defined rules, which require domain expertise. DNNs are essentially computational graphs, which contain rich structural information. In this paper, we aim to find a suitable compression policy from DNNs’ structural information. We propose an automatic graph encoder-decoder model compression (AGMC) method combined with graph neural networks (GNN) and reinforcement learning (RL). We model the target DNN as a graph and use GNN to learn the DNN’s embeddings automatically. We compared our method with rule-based DNN embedding model compression methods to show the effectiveness of our method. Results show that our learning-based DNN embedding achieves better performance and a higher compression ratio with fewer search steps. We evaluated our method on over-parameterized and mobile-friendly DNNs and compared our method with handcrafted and learning-based model compression approaches. On over parameterized DNNs, such as ResNet-56, our method outperformed handcrafted and learning-based methods with 4.36\% and 2.56\% higher accuracy, respectively. Furthermore, on MobileNet-v2, we achieved a higher compression ratio than state-of-the-art methods with just 0.93\% accuracy loss.},
  archive   = {C_ICCV},
  author    = {Sixing Yu and Arya Mazaheri and Ali Jannesari},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00630},
  pages     = {6342-6352},
  title     = {Auto graph encoder-decoder for neural network pruning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GNeRF: GAN-based neural radiance field without posed camera.
<em>ICCV</em>, 6331–6341. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce GNeRF, a framework to marry Generative Adversarial Networks (GAN) with Neural Radiance Field (NeRF) reconstruction for the complex scenarios with unknown and even randomly initialized camera poses. Recent NeRF-based advances have gained popularity for remarkable realistic novel view synthesis. However, most of them heavily rely on accurate camera poses estimation, while few recent methods can only optimize the unknown camera poses in roughly forward-facing scenes with relatively short camera trajectories and require rough camera poses initialization. Differently, our GNeRF only utilizes randomly initialized poses for complex outside-in scenarios. We propose a novel two-phases end-to-end framework. The first phase takes the use of GANs into the new realm for optimizing coarse camera poses and radiance fields jointly, while the second phase refines them with additional photometric loss. We overcome local minima using a hybrid and iterative optimization scheme. Extensive experiments on a variety of synthetic and natural scenes demonstrate the effectiveness of GNeRF. More impressively, our approach outperforms the baselines favorably in those scenes with repeated patterns or even low textures that are regarded as extremely challenging before.},
  archive   = {C_ICCV},
  author    = {Quan Meng and Anpei Chen and Haimin Luo and Minye Wu and Hao Su and Lan Xu and Xuming He and Jingyi Yu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00629},
  pages     = {6331-6341},
  title     = {GNeRF: GAN-based neural radiance field without posed camera},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TMCOSS: Thresholded multi-criteria online subset selection
for data-efficient autonomous driving. <em>ICCV</em>, 6321–6330. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Training vision-based Autonomous driving models is a challenging problem with enormous practical implications. One of the main challenges is the requirement of storage and processing of vast volumes of (possibly redundant) driving video data. In this paper, we study the problem of data-efficient training of autonomous driving systems. We argue that in the context of an edge-device deployment, multi-criteria online video frame subset selection is an appropriate technique for developing such frameworks. We study existing convex optimization based solutions and show that they are unable to provide solution with high weightage to loss of selected video frames. We design a novel multi-criteria online subset selection algorithm, TMCOSS, which uses a thresholded concave function of selection variables. Extensive experiments using driving simulator CARLA show that we are able to drop 80\% of the frames, while succeeding to complete 100\% of the episodes. We also show that TMCOSS improves performance on the crucial affordance &quot;Relative Angle&quot; during turns, on inclusion of bucket-specific relative angle loss (BL), leading to selection of more frames in those parts. TMCOSS also achieves an 80\% reduction in number of training video frames, on real-world videos from the standard BDD and Cityscapes datasets, for the tasks of drivable area segmentation, and semantic segmentation.},
  archive   = {C_ICCV},
  author    = {Soumi Das and Harikrishna Patibandla and Suparna Bhattacharya and Kshounis Bera and Niloy Ganguly and Sourangshu Bhattacharya},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00628},
  pages     = {6321-6330},
  title     = {TMCOSS: Thresholded multi-criteria online subset selection for data-efficient autonomous driving},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Making higher order MOT scalable: An efficient approximate
solver for lifted disjoint paths. <em>ICCV</em>, 6310–6320. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an efficient approximate message passing solver for the lifted disjoint paths problem (LDP), a natural but NP-hard model for multiple object tracking (MOT). Our tracker scales to very large instances that come from long and crowded MOT sequences. Our approximate solver enables us to process the MOT15/16/17 benchmarks without sacrificing solution quality and allows for solving MOT20, which has been out of reach up to now for LDP solvers due to its size and complexity. On all these four standard MOT benchmarks we achieve performance comparable or better than current state-of-the-art methods including a tracker based on an optimal LDP solver.},
  archive   = {C_ICCV},
  author    = {Andrea Hornakova and Timo Kaiser and Paul Swoboda and Michal Rolinek and Bodo Rosenhahn and Roberto Henschel},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00627},
  pages     = {6310-6320},
  title     = {Making higher order MOT scalable: An efficient approximate solver for lifted disjoint paths},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast light-field disparity estimation with
multi-disparity-scale cost aggregation. <em>ICCV</em>, 6300–6309. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Light field images contain both angular and spatial information of captured light rays. The rich information of light fields enables straightforward disparity recovery capability but demands high computational cost as well. In this paper, we design a lightweight disparity estimation model with physical-based multi-disparity-scale cost volume aggregation for fast disparity estimation. By introducing a sub-network of edge guidance, we significantly improve the recovery of geometric details near edges and improve the overall performance. We test the proposed model extensively on both synthetic and real-captured datasets, which provide both densely and sparsely sampled light fields. Finally, we significantly reduce computation cost and GPU memory consumption, while achieving comparable performance with state-of-the-art disparity estimation methods for light fields. Our source code is available at https://github.com/zcong17huang/FastLFnet.},
  archive   = {C_ICCV},
  author    = {Zhicong Huang and Xuemei Hu and Zhou Xue and Weizhu Xu and Tao Yue},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00626},
  pages     = {6300-6309},
  title     = {Fast light-field disparity estimation with multi-disparity-scale cost aggregation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). UASNet: Uncertainty adaptive sampling network for deep
stereo matching. <em>ICCV</em>, 6291–6299. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent studies have shown that cascade cost volume can play a vital role in deep stereo matching to achieve high resolution depth map with efficient hardware usage. However, how to construct good cascade volume as well as effective sampling for them are still under in-depth study. Previous cascade-based methods usually perform uniform sampling in a predicted disparity range based on variance, which easily misses the ground truth disparity and decreases disparity map accuracy. In this paper, we propose an uncertainty adaptive sampling network (UASNet) featuring two modules: an uncertainty distribution-guided range prediction (URP) model and an uncertainty-based disparity sampler (UDS) module. The URP explores the more discriminative uncertainty distribution to handle the complex matching ambiguities and to improve disparity range prediction. The UDS adaptively adjusts sampling interval to localize disparity with improved accuracy. With the proposed modules, our UASNet learns to construct cascade cost volume and predict full-resolution disparity map directly. Extensive experiments show that the proposed method achieves the highest ground truth covering ratio compared with other cascade cost volume based stereo matching methods. Our method also achieves top performance on both SceneFlow dataset and KITTI benchmark.},
  archive   = {C_ICCV},
  author    = {Yamin Mao and Zhihua Liu and Weiming Li and Yuchao Dai and Qiang Wang and Yun-Tae Kim and Hong-Seok Lee},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00625},
  pages     = {6291-6299},
  title     = {UASNet: Uncertainty adaptive sampling network for deep stereo matching},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Learning to match features with seeded graph matching
network. <em>ICCV</em>, 6281–6290. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Matching local features across images is a fundamental problem in computer vision. Targeting towards high accuracy and efficiency, we propose Seeded Graph Matching Network, a graph neural network with sparse structure to reduce redundant connectivity and learn compact representation. The network consists of 1) Seeding Module, which initializes the matching by generating a small set of reliable matches as seeds. 2) Seeded Graph Neural Network, which utilizes seed matches to pass messages within/across images and predicts assignment costs. Three novel operations are proposed as basic elements for message passing: 1) Attentional Pooling, which aggregates keypoint features within the image to seed matches. 2) Seed Filtering, which enhances seed features and exchanges messages across images. 3) Attentional Unpooling, which propagates seed features back to original keypoints. Experiments show that our method reduces computational and memory complexity significantly compared with typical attention-based networks while competitive or higher performance is achieved.},
  archive   = {C_ICCV},
  author    = {Hongkai Chen and Zixin Luo and Jiahui Zhang and Lei Zhou and Xuyang Bai and Zeyu Hu and Chiew-Lan Tai and Long Quan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00624},
  pages     = {6281-6290},
  title     = {Learning to match features with seeded graph matching network},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distilling global and local logits with densely connected
relations. <em>ICCV</em>, 6270–6280. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In prevalent knowledge distillation, logits in most image recognition models are computed by global average pooling, then used to learn to encode the high-level and task-relevant knowledge. In this work, we solve the limitation of this global logit transfer in this distillation context. We point out that it prevents the transfer of informative spatial information, which provides localized knowledge as well as rich relational information across contexts of an input scene. To exploit the rich spatial information, we propose a simple yet effective logit distillation approach. We add a local spatial pooling layer branch to the penultimate layer, thereby our method extends the standard logit distillation and enables learning of both finely-localized knowledge and holistic representation. Our proposed method shows favorable accuracy improvement against the state-of-the-art methods on several image classification datasets. We show that our distilled students trained on the image classification task can be successfully leveraged for object detection and semantic segmentation tasks; this result demonstrates our method’s high transferability.},
  archive   = {C_ICCV},
  author    = {Youmin Kim and Jinbae Park and YounHo Jang and Muhammad Ali and Tae-Hyun Oh and Sung-Ho Bae},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00623},
  pages     = {6270-6280},
  title     = {Distilling global and local logits with densely connected relations},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FFT-OT: A fast algorithm for optimal transportation.
<em>ICCV</em>, 6260–6269. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An optimal transportation map finds the most economical way to transport one probability measure to the other. It has been applied in a broad range of applications in vision, deep learning and medical images. By Brenier theory, computing the optimal transport map is equivalent to solving a Monge-Ampère equation. Due to the highly non-linear nature, the computation of optimal transportation maps in large scale is very challenging.This work proposes a simple but powerful method, the FFT-OT algorithm, to tackle this difficulty based on three key ideas. First, solving Monge-Ampère equation is converted to a fixed point problem; Second, the obliqueness property of optimal transportation maps are reformulated as Neumann boundary conditions on rectangular domains; Third, FFT is applied in each iteration to solve a Poisson equation in order to improve the efficiency.Experiments on surfaces captured from 3D scanning and reconstructed from medical imaging are conducted, and compared with other existing methods. Our experimental results show that the proposed FFT-OT algorithm is simple, general and scalable with high efficiency and accuracy.},
  archive   = {C_ICCV},
  author    = {Na Lei and Xianfeng Gu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00622},
  pages     = {6260-6269},
  title     = {FFT-OT: A fast algorithm for optimal transportation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fusion moves for graph matching. <em>ICCV</em>, 6250–6259.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We contribute to approximate algorithms for the quadratic assignment problem also known as graph matching. Inspired by the success of the fusion moves technique developed for multilabel discrete Markov random fields, we investigate its applicability to graph matching. In particular, we show how fusion moves can be efficiently combined with the dedicated state-of-the-art dual methods that have recently shown superior results in computer vision and bioimaging applications. As our empirical evaluation on a wide variety of graph matching datasets suggests, fusion moves significantly improve performance of these methods in terms of speed and quality of the obtained solutions. Our method sets a new state-of-the-art with a notable margin with respect to its competitors.},
  archive   = {C_ICCV},
  author    = {Lisa Hutschenreiter and Stefan Haller and Lorenz Feineis and Carsten Rother and Dagmar Kainmüller and Bogdan Savchynskyy},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00621},
  pages     = {6250-6259},
  title     = {Fusion moves for graph matching},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Faster multi-object segmentation using parallel quadratic
pseudo-boolean optimization. <em>ICCV</em>, 6240–6249. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a parallel version of the Quadratic Pseudo-Boolean Optimization (QPBO) algorithm for solving binary optimization tasks, such as image segmentation. The original QPBO implementation by Kolmogorov and Rother relies on the Boykov-Kolmogorov (BK) maxflow/mincut algorithm and performs well for many image analysis tasks. However, the serial nature of their QPBO algorithm results in poor utilization of modern hardware. By redesigning the QPBO algorithm to work with parallel maxflow/mincut algorithms, we significantly reduce solve time of large optimization tasks. We compare our parallel QPBO implementation to other state-of-the-art solvers and benchmark them on two large segmentation tasks and a substantial set of small segmentation tasks. The results show that our parallel QPBO algorithm is over 20 times faster than the serial QPBO algorithm on the large tasks and over three times faster for the majority of the small tasks. Although we focus on image segmentation, our algorithm is generic and can be used for any QPBO problem. Our implementation and experimental results are available at DOI: 10.5281/zenodo.5201620},
  archive   = {C_ICCV},
  author    = {Niels Jeppesen and Patrick M. Jensen and Anders N. Christensen and Anders B. Dahl and Vedrana A. Dahl},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00620},
  pages     = {6240-6249},
  title     = {Faster multi-object segmentation using parallel quadratic pseudo-boolean optimization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to bundle-adjust: A graph network approach to
faster optimization of bundle adjustment for vehicular SLAM.
<em>ICCV</em>, 6230–6239. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bundle adjustment (BA) occupies a large portion of the execution time of SfM and visual SLAM. Local BA over the latest several keyframes plays a crucial role in visual SLAM. Its execution time should be sufficiently short for robust tracking; this is especially critical for embedded systems with a limited computational resource. This study proposes a learning-based bundle adjuster using a graph network. It works faster and can be used instead of conventional optimization-based BA. The graph network operates on a graph consisting of the nodes of keyframes and landmarks and the edges representing the landmarks’ visibility. The graph network receives the parameters’ initial values as inputs and predicts their updates to the optimal values. It internally uses an intermediate representation of inputs which we design inspired by the normal equation of the Levenberg-Marquardt method. It is trained using the sum of reprojection errors as a loss function. The experiments show that the proposed method outputs parameter estimates with slightly inferior accuracy in 1/60–1/10 of time compared with the conventional BA.},
  archive   = {C_ICCV},
  author    = {Tetsuya Tanaka and Yukihiro Sasagawa and Takayuki Okatani},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00619},
  pages     = {6230-6239},
  title     = {Learning to bundle-adjust: A graph network approach to faster optimization of bundle adjustment for vehicular SLAM},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DeepMultiCap: Performance capture of multiple characters
using sparse multiview cameras. <em>ICCV</em>, 6219–6229. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose DeepMultiCap, a novel method for multi-person performance capture using sparse multi-view cameras. Our method can capture time varying surface details without the need of using pre-scanned template models. To tackle with the serious occlusion challenge for close interacting scenes, we combine a recently proposed pixel-aligned implicit function with parametric model for robust reconstruction of the invisible surface areas. An effective attention-aware module is designed to obtain the fine-grained geometry details from multi-view images, where high-fidelity results can be generated. In addition to the spatial attention method, for video inputs, we further propose a novel temporal fusion method to alleviate the noise and temporal inconsistencies for moving character reconstruction. For quantitative evaluation, we contribute a high quality multi-person dataset, MultiHuman, which consists of 150 static scenes with different levels of occlusions and ground truth 3D human models. Experimental results demonstrate the state-of-the-art performance of our method and the well generalization to real multiview video data, which outperforms the prior works by a large margin.},
  archive   = {C_ICCV},
  author    = {Yang Zheng and Ruizhi Shao and Yuxiang Zhang and Tao Yu and Zerong Zheng and Qionghai Dai and Yebin Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00618},
  pages     = {6219-6229},
  title     = {DeepMultiCap: Performance capture of multiple characters using sparse multiview cameras},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). IMAP: Implicit mapping and positioning in real-time.
<em>ICCV</em>, 6209–6218. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We show for the first time that a multilayer perceptron (MLP) can serve as the only scene representation in a real-time SLAM system for a handheld RGB-D camera. Our network is trained in live operation without prior data, building a dense, scene-specific implicit 3D model of occupancy and colour which is also immediately used for tracking.Achieving real-time SLAM via continual training of a neural network against a live image stream requires significant innovation. Our iMAP algorithm uses a keyframe structure and multi-processing computation flow, with dynamic information-guided pixel sampling for speed, with tracking at 10 Hz and global map updating at 2 Hz. The advantages of an implicit MLP over standard dense SLAM techniques include efficient geometry representation with automatic detail control and smooth, plausible filling-in of unobserved regions such as the back surfaces of objects.},
  archive   = {C_ICCV},
  author    = {Edgar Sucar and Shikun Liu and Joseph Ortiz and Andrew J. Davison},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00617},
  pages     = {6209-6218},
  title     = {IMAP: Implicit mapping and positioning in real-time},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the limits of pseudo ground truth in visual camera
re-localisation. <em>ICCV</em>, 6198–6208. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Benchmark datasets that measure camera pose accuracy have driven progress in visual re-localisation research. To obtain poses for thousands of images, it is common to use a reference algorithm to generate pseudo ground truth. Popular choices include Structure-from-Motion (SfM) and Simultaneous-Localisation-and-Mapping (SLAM) using additional sensors like depth cameras if available. Re-localisation benchmarks thus measure how well each method replicates the results of the reference algorithm. This begs the question whether the choice of the reference algorithm favours a certain family of re-localisation methods. This paper analyzes two widely used re-localisation datasets and shows that evaluation outcomes indeed vary with the choice of the reference algorithm. We thus question common beliefs in the re-localisation literature, namely that learning-based scene coordinate regression outperforms classical feature-based methods, and that RGB-D-based methods outperform RGB-based methods. We argue that any claims on ranking re-localisation methods should take the type of the reference algorithm, and the similarity of the methods to the reference algorithm, into account.},
  archive   = {C_ICCV},
  author    = {Eric Brachmann and Martin Humenberger and Carsten Rother and Torsten Sattler},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00616},
  pages     = {6198-6208},
  title     = {On the limits of pseudo ground truth in visual camera re-localisation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). COTR: Correspondence transformer for matching across images.
<em>ICCV</em>, 6187–6197. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel framework for finding correspondences in images based on a deep neural network that, given two images and a query point in one of them, finds its correspondence in the other. By doing so, one has the option to query only the points of interest and retrieve sparse correspondences, or to query all points in an image and obtain dense mappings. Importantly, in order to capture both local and global priors, and to let our model relate between image regions using the most relevant among said priors, we realize our network using a transformer. At inference time, we apply our correspondence network by recursively zooming in around the estimates, yielding a multiscale pipeline able to provide highly-accurate correspondences. Our method significantly outperforms the state of the art on both sparse and dense correspondence problems on multiple datasets and tasks, ranging from wide-baseline stereo to optical flow, without any retraining for a specific dataset. We commit to releasing data, code, and all the tools necessary to train from scratch and ensure reproducibility.},
  archive   = {C_ICCV},
  author    = {Wei Jiang and Eduard Trulls and Jan Hosang and Andrea Tagliasacchi and Kwang Moo Yi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00615},
  pages     = {6187-6197},
  title     = {COTR: Correspondence transformer for matching across images},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Revisiting stereo depth estimation from a
sequence-to-sequence perspective with transformers. <em>ICCV</em>,
6177–6186. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Stereo depth estimation relies on optimal correspondence matching between pixels on epipolar lines in the left and right images to infer depth. In this work, we revisit the problem from a sequence-to-sequence correspondence perspective to replace cost volume construction with dense pixel matching using position information and attention. This approach, named STereo TRansformer (STTR), has several advantages: It 1) relaxes the limitation of a fixed disparity range, 2) identifies occluded regions and provides confidence estimates, and 3) imposes uniqueness constraints during the matching process. We report promising results on both synthetic and real-world datasets and demonstrate that STTR generalizes across different domains, even without fine-tuning.},
  archive   = {C_ICCV},
  author    = {Zhaoshuo Li and Xingtong Liu and Nathan Drenkow and Andy Ding and Francis X. Creighton and Russell H. Taylor and Mathias Unberath},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00614},
  pages     = {6177-6186},
  title     = {Revisiting stereo depth estimation from a sequence-to-sequence perspective with transformers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AA-RMVSNet: Adaptive aggregation recurrent multi-view stereo
network. <em>ICCV</em>, 6167–6176. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a novel recurrent multi-view stereo network based on long short-term memory (LSTM) with adaptive aggregation, namely AA-RMVSNet. We firstly introduce an intra-view aggregation module to adaptively extract image features by using context-aware convolution and multi-scale aggregation, which efficiently improves the performance on challenging regions, such as thin objects and large low-textured surfaces. To overcome the difficulty of varying occlusion in complex scenes, we propose an interview cost volume aggregation module for adaptive pixel-wise view aggregation, which is able to preserve better-matched pairs among all views. The two proposed adaptive aggregation modules are lightweight, effective and complementary regarding improving the accuracy and completeness of 3D reconstruction. Instead of conventional 3D CNNs, we utilize a hybrid network with recurrent structure for cost volume regularization, which allows high-resolution reconstruction and finer hypothetical plane sweep. The proposed network is trained end-to-end and achieves excellent performance on various datasets. It ranks 1 st among all submissions on Tanks and Temples benchmark and achieves competitive results on DTU dataset, which exhibits strong generalizability and robustness. Implementation of our method is available at https://github.com/QT-Zhu/AA-RMVSNet.},
  archive   = {C_ICCV},
  author    = {Zizhuang Wei and Qingtian Zhu and Chen Min and Yisong Chen and Guoping Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00613},
  pages     = {6167-6176},
  title     = {AA-RMVSNet: Adaptive aggregation recurrent multi-view stereo network},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Just a few points are all you need for multi-view stereo: A
novel semi-supervised learning method for multi-view stereo.
<em>ICCV</em>, 6158–6166. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While learning-based multi-view stereo (MVS) methods have recently shown successful performances in quality and efficiency, limited MVS data hampers generalization to unseen environments. A simple solution is to generate various large-scale MVS datasets, but generating dense ground truth for 3D structure requires a huge amount of time and resources. On the other hand, if the reliance on dense ground truth is relaxed, MVS systems will generalize more smoothly to new environments. To this end, we first introduce a novel semi-supervised multi-view stereo framework called a Sparse Ground truth-based MVS Network (SGT-MVSNet) that can reliably reconstruct the 3D structures even with a few ground truth 3D points. Our strategy is to divide the accurate and erroneous regions and individually conquer them based on our observation that a probability map can separate these regions. We propose a self-supervision loss called the 3D Point Consistency Loss to enhance the 3D reconstruction performance, which forces the 3D points back-projected from the corresponding pixels by the predicted depth values to meet at the same 3D co-ordinates. Finally, we propagate these improved depth pre-dictions toward edges and occlusions by the Coarse-to-fine Reliable Depth Propagation module. We generate the spare ground truth of the DTU dataset for evaluation and extensive experiments verify that our SGT-MVSNet outperforms the state-of-the-art MVS methods on the sparse ground truth setting. Moreover, our method shows comparable reconstruction results to the supervised MVS methods though we only used tens and hundreds of ground truth 3D points.},
  archive   = {C_ICCV},
  author    = {Taekyung Kim and Jaehoon Choi and Seokeon Choi and Dongki Jung and Changick Kim},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00612},
  pages     = {6158-6166},
  title     = {Just a few points are all you need for multi-view stereo: A novel semi-supervised learning method for multi-view stereo},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A confidence-based iterative solver of depths and surface
normals for deep multi-view stereo. <em>ICCV</em>, 6148–6157. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce a deep multi-view stereo (MVS) system that jointly predicts depths, surface normals and per-view confidence maps. The key to our approach is a novel solver that iteratively solves for per-view depth map and normal map by optimizing an energy potential based on the locally planar assumption. Specifically, the algorithm updates depth map by propagating from neigh-boring pixels with slanted planes, and updates normal map with local probabilistic plane fitting. Both two steps are monitored by a customized confidence map. This solver is not only effective as a post-processing tool for plane-based depth refinement and completion, but also differentiable such that it can be efficiently integrated into deep learning pipelines. Our multi-view stereo system employs multiple optimization steps of the solver over the initial prediction of depths and surface normals. The whole system can be trained end-to-end, decoupling the challenging problem of matching pixels within poorly textured regions from the cost-volume based neural network. Experimental results on ScanNet and RGB-D Scenes V2 demonstrate state-of-the-art performance of the proposed deep MVS system on multi-view depth estimation, with our proposed solver consistently improving the depth quality over both conventional and deep learning based MVS pipelines. Code is available at https://github.com/thuzhaowang/idn-solver.},
  archive   = {C_ICCV},
  author    = {Wang Zhao and Shaohui Liu and Yi Wei and Hengkai Guo and Yong-Jin Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00611},
  pages     = {6148-6157},
  title     = {A confidence-based iterative solver of depths and surface normals for deep multi-view stereo},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PatchMatch-RL: Deep MVS with pixelwise depth, normal, and
visibility. <em>ICCV</em>, 6138–6147. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent learning-based multi-view stereo (MVS) methods show excellent performance with dense cameras and small depth ranges. However, non-learning based approaches still outperform for scenes with large depth ranges and sparser wide-baseline views, in part due to their PatchMatch optimization over pixelwise estimates of depth, normals, and visibility. In this paper, we propose an end-to-end trainable PatchMatch-based MVS approach that combines advantages of trainable costs and regularizations with pixelwise estimates. To overcome the challenge of the non-differentiable PatchMatch optimization that involves iterative sampling and hard decisions, we use reinforcement learning to minimize expected photometric cost and maximize likelihood of ground truth depth and normals. We incorporate normal estimation by using dilated patch kernels and propose a recurrent cost regularization that applies beyond frontal plane-sweep algorithms to our pixelwise depth/normal estimates. We evaluate our method on widely used MVS benchmarks, ETH3D and Tanks and Temples (TnT). On ETH3D, our method outperforms other recent learning-based approaches and performs comparably on advanced TnT.},
  archive   = {C_ICCV},
  author    = {Jae Yong Lee and Joseph DeGol and Chuhang Zou and Derek Hoiem},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00610},
  pages     = {6138-6147},
  title     = {PatchMatch-RL: Deep MVS with pixelwise depth, normal, and visibility},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rational polynomial camera model warping for deep learning
based satellite multi-view stereo matching. <em>ICCV</em>, 6128–6137.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Satellite multi-view stereo (MVS) imagery is particularly suited for large-scale Earth surface reconstruction. Differing from the perspective camera model (pin-hole model) that is commonly used for close-range and aerial cameras, the cubic rational polynomial camera (RPC) model is the mainstream model for push-broom linear-array satellite cameras. However, the homography warping used in the prevailing learning based MVS methods is only applicable to pin-hole cameras. In order to apply the SOTA learning based MVS technology to the satellite MVS task for large-scale Earth surface reconstruction, RPC warping should be considered. In this work, we propose, for the first time, a rigorous RPC warping module. The rational polynomial coefficients are recorded as a tensor, and the RPC warping is formulated as a series of tensor transformations. Based on the RPC warping, we propose the deep learning based satellite MVS (SatMVS) framework for large-scale and wide depth range Earth surface reconstruction. We also introduce a large-scale satellite image dataset consisting of 519 5120×5120 images, which we call the TLC SatMVS dataset. The satellite images were acquired from a three-line camera (TLC) that catches triple-view images simultaneously, forming a valuable supplement to the existing open-source WorldView-3 datasets with single-scanline images. Experiments show that the proposed RPC warping module and the SatMVS framework can achieve a superior reconstruction accuracy compared to the pin-hole fitting method and conventional MVS methods. Code and data are available at https://github.com/WHU-GPCV/SatMVS.},
  archive   = {C_ICCV},
  author    = {Jian Gao and Jin Liu and Shunping Ji},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00609},
  pages     = {6128-6137},
  title     = {Rational polynomial camera model warping for deep learning based satellite multi-view stereo matching},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A robust loss for point cloud registration. <em>ICCV</em>,
6118–6127. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The performance of surface registration relies heavily on the metric used for the alignment error between the source and target shapes. Traditionally, such a metric is based on the point-to-point or point-to-plane distance from the points on the source surface to their closest points on the target surface, which is susceptible to failure due to instability of the closest-point correspondence. In this paper, we propose a novel metric based on the intersection points between the two shapes and a random straight line, which does not assume a specific correspondence. We verify the effectiveness of this metric by extensive experiments, including its direct optimization for a single registration problem as well as un-supervised learning for a set of registration problems. The results demonstrate that the algorithms utilizing our proposed metric outperforms the state-of-the-art optimization-based and unsupervised learning-based methods.},
  archive   = {C_ICCV},
  author    = {Zhi Deng and Yuxin Yao and Bailin Deng and Juyong Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00608},
  pages     = {6118-6127},
  title     = {A robust loss for point cloud registration},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sampling network guided cross-entropy method for
unsupervised point cloud registration. <em>ICCV</em>, 6108–6117. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, by modeling the point cloud registration task as a Markov decision process, we propose an end-to-end deep model embedded with the cross-entropy method (CEM) for unsupervised 3D registration. Our model consists of a sampling network module and a differentiable CEM module. In our sampling network module, given a pair of point clouds, the sampling network learns a prior sampling distribution over the transformation space. The learned sampling distribution can be used as a &quot;good&quot; initialization of the differentiable CEM module. In our differentiable CEM module, we first propose a maximum consensus criterion based alignment metric as the reward function for the point cloud registration task. Based on the reward function, for each state, we then construct a fused score function to evaluate the sampled transformations, where we weight the current and future rewards of the transformations. Particularly, the future rewards of the sampled transforms are obtained by performing the iterative closest point (ICP) algorithm on the transformed state. By selecting the top-k transformations with the highest scores, we iteratively update the sampling distribution. Furthermore, in order to make the CEM differentiable, we use the sparse-max function to replace the hard top-k selection. Finally, we formulate a Geman-McClure estimator based loss to train our end-to-end registration model. Extensive experimental results demonstrate the good registration performance of our method on benchmark datasets. Code is available at https://github.com/Jiang-HB/CEMNet.},
  archive   = {C_ICCV},
  author    = {Haobo Jiang and Yaqi Shen and Jin Xie and Jun Li and Jianjun Qian and Jian Yang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00607},
  pages     = {6108-6117},
  title     = {Sampling network guided cross-entropy method for unsupervised point cloud registration},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). AdaFit: Rethinking learning-based normal estimation on
point clouds. <em>ICCV</em>, 6098–6107. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a neural network for robust normal estimation on point clouds, named AdaFit, that can deal with point clouds with noise and density variations. Existing works use a network to learn point-wise weights for weighted least squares surface fitting to estimate the normals, which has difficulty in finding accurate normals in complex regions or containing noisy points. By analyzing the step of weighted least squares surface fitting, we find that it is hard to determine the polynomial order of the fitting surface and the fitting surface is sensitive to outliers. To address these problems, we propose a simple yet effective solution that adds an additional offset prediction to improve the quality of normal estimation. Furthermore, in order to take advantage of points from different neighborhood sizes, a novel Cascaded Scale Aggregation layer is proposed to help the network predict more accurate point-wise offsets and weights. Extensive experiments demonstrate that AdaFit achieves state-of-the-art performance on both the synthetic PCPNet dataset and the real-word SceneNN dataset. The code is publicly available at https://github.com/Runsong123/AdaFit.},
  archive   = {C_ICCV},
  author    = {Runsong Zhu and Yuan Liu and Zhen Dong and Yuan Wang and Tengping Jiang and Wenping Wang and Bisheng Yang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00606},
  pages     = {6098-6107},
  title     = {AdaFit: Rethinking learning-based normal estimation on point clouds},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). (Just) a spoonful of refinements helps the registration
error go down. <em>ICCV</em>, 6088–6097. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We tackle data-driven 3D point cloud registration. Given point correspondences, the standard Kabsch algorithm provides an optimal rotation estimate. This allows to train registration models in an end-to-end manner by differentiating the SVD operation. However, given the initial rotation estimate supplied by Kabsch, we show we can improve point correspondence learning during model training by extending the original optimization problem. In particular, we linearize the governing constraints of the rotation matrix and solve the resulting linear system of equations. We then iteratively produce new solutions by updating the initial estimate. Our experiments show that, by plugging our differentiable layer to existing learning-based registration methods, we improve the correspondence matching quality. This yields up to a 7\% decrease in rotation error for correspondence-based data-driven registration methods.},
  archive   = {C_ICCV},
  author    = {Sérgio Agostinho and Aljoša Ošep and Alessio Del Bue and Laura Leal-Taixé},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00605},
  pages     = {6088-6097},
  title     = {(Just) a spoonful of refinements helps the registration error go down},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pyramid point cloud transformer for large-scale place
recognition. <em>ICCV</em>, 6078–6087. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, deep learning based point cloud descriptors have achieved impressive results in the place recognition task. Nonetheless, due to the sparsity of point clouds, how to extract discriminative local features of point clouds to efficiently form a global descriptor is still a challenging problem. In this paper, we propose a pyramid point cloud transformer network (PPT-Net) to learn the discriminative global descriptors from point clouds for efficient retrieval. Specifically, we first develop a pyramid point transformer module that adaptively learns the spatial relationship of the different k-NN neighboring points of point clouds, where the grouped self-attention is proposed to extract discriminative local features of the point clouds. The grouped self-attention not only enhances long-term dependencies of the point clouds, but also reduces the computational cost. In order to obtain discriminative global descriptors, we construct a pyramid VLAD module to aggregate the multi-scale feature maps of point clouds into the global descriptors. By applying VLAD pooling on multi-scale feature maps, we utilize the context gating mechanism on the multiple global descriptors to adaptively weight the multi-scale global context information into the final global descriptor. Experimental results on the Oxford dataset and three in-house datasets show that our method achieves the state-of-the-art on the point cloud based place recognition task. Code is available at https://github.com/fpthink/PPT-Net.},
  archive   = {C_ICCV},
  author    = {Le Hui and Hang Yang and Mingmei Cheng and Jin Xie and Jian Yang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00604},
  pages     = {6078-6087},
  title     = {Pyramid point cloud transformer for large-scale place recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Differentiable surface rendering via non-differentiable
sampling. <em>ICCV</em>, 6068–6077. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a method for differentiable rendering of 3D surfaces that supports both explicit and implicit representations, provides derivatives at occlusion boundaries, and is fast and simple to implement. The method first samples the surface using non-differentiable rasterization, then applies differentiable, depth-aware point splatting to produce the final image. Our approach requires no differentiable meshing or rasterization steps, making it efficient for large 3D models and applicable to isosurfaces extracted from implicit surface definitions. We demonstrate the effectiveness of our method for implicit-, mesh-, and parametric-surface-based inverse rendering and neural-network training applications. In particular, we show for the first time efficient, differentiable rendering of an isosurface extracted from a neural radiance field (NeRF), and demonstrate surface-based, rather than volume-based, rendering of a NeRF.},
  archive   = {C_ICCV},
  author    = {Forrester Cole and Kyle Genova and Avneesh Sud and Daniel Vlasic and Zhoutong Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00603},
  pages     = {6068-6077},
  title     = {Differentiable surface rendering via non-differentiable sampling},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Digging into uncertainty in self-supervised multi-view
stereo. <em>ICCV</em>, 6058–6067. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-supervised Multi-view stereo (MVS) with a pretext task of image reconstruction has achieved significant progress recently. However, previous methods are built upon intuitions, lacking comprehensive explanations about the effectiveness of the pretext task in self-supervised MVS. To this end, we propose to estimate epistemic uncertainty in self-supervised MVS, accounting for what the model ignores. Specially, the limitations can be categorized into two types: ambiguious supervision in foreground and invalid supervision in background. To address these issues, we propose a novel Uncertainty reduction Multi-view Stereo (U-MVS) framework for self-supervised learning. To alleviate ambiguous supervision in foreground, we involve extra correspondence prior with a flow-depth consistency loss. The dense 2D correspondence of optical flows is used to regularize the 3D stereo correspondence in MVS. To handle the invalid supervision in background, we use Monte-Carlo Dropout to acquire the uncertainty map and further filter the unreliable supervision signals on invalid regions. Extensive experiments on DTU and Tank&amp;Temples benchmark show that our U-MVS framework 1 achieves the best performance among unsupervised MVS methods, with competitive performance with its supervised opponents.},
  archive   = {C_ICCV},
  author    = {Hongbin Xu and Zhipeng Zhou and Yali Wang and Wenxiong Kang and Baigui Sun and Hao Li and Yu Qiao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00602},
  pages     = {6058-6067},
  title     = {Digging into uncertainty in self-supervised multi-view stereo},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Minimal cases for computing the generalized relative pose
using affine correspondences. <em>ICCV</em>, 6048–6057. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose three novel solvers for estimating the relative pose of a multi-camera system from affine correspondences (ACs). A new constraint is derived interpreting the relationship of ACs and the generalized camera model. Using the constraint, we demonstrate efficient solvers for two types of motions assumed. Considering that the cameras undergo planar motion, we propose a minimal solution using a single AC and a solver with two ACs to overcome the degenerate case. Also, we propose a minimal solution using two ACs with known vertical direction, e.g., from an IMU. Since the proposed methods require significantly fewer correspondences than state-of-the-art algorithms, they can be efficiently used within RANSAC for outlier removal and initial motion estimation. The solvers are tested both on synthetic data and on real-world scenes from the KITTI odometry benchmark. It is shown that the accuracy of the estimated poses is superior to the state-of-the-art techniques.},
  archive   = {C_ICCV},
  author    = {Banglei Guan and Ji Zhao and Daniel Barath and Friedrich Fraundorfer},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00601},
  pages     = {6048-6057},
  title     = {Minimal cases for computing the generalized relative pose using affine correspondences},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cross-descriptor visual localization and mapping.
<em>ICCV</em>, 6038–6047. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual localization and mapping is the key technology underlying the majority of mixed reality and robotics systems. Most state-of-the-art approaches rely on local features to establish correspondences between images. In this paper, we present three novel scenarios for localization and mapping which require the continuous update of feature representations and the ability to match across different feature types. While localization and mapping is a fundamental computer vision problem, the traditional setup supposes the same local features are used throughout the evolution of a map. Thus, whenever the underlying features are changed, the whole process is repeated from scratch. However, this is typically impossible in practice, because raw images are often not stored and re-building the maps could lead to loss of the attached digital content. To overcome the limitations of current approaches, we present the first principled solution to cross-descriptor localization and mapping. Our data-driven approach is agnostic to the feature descriptor type, has low computational requirements, and scales linearly with the number of description algorithms. Extensive experiments demonstrate the effectiveness of our approach on state-of-the-art benchmarks for a variety of handcrafted and learned features.},
  archive   = {C_ICCV},
  author    = {Mihai Dusmanu and Ondrej Miksik and Johannes L. Schönberger and Marc Pollefeys},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00600},
  pages     = {6038-6047},
  title     = {Cross-descriptor visual localization and mapping},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stacked homography transformations for multi-view pedestrian
detection. <em>ICCV</em>, 6029–6037. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-view pedestrian detection aims to predict a bird’s eye view (BEV) occupancy map from multiple camera views. This task is confronted with two challenges: how to establish the 3D correspondences from views to the BEV map and how to assemble occupancy information across views. In this paper, we propose a novel Stacked HOmography Transformations (SHOT) approach, which is motivated by approximating projections in 3D world coordinates via a stack of homographies. We first construct a stack of transformations for projecting views to the ground plane at different height levels. Then we design a soft selection module so that the network learns to predict the likelihood of the stack of transformations. Moreover, we provide an in-depth theoretical analysis on constructing SHOT and how well SHOT approximates projections in 3D world coordinates. SHOT is empirically verified to be capable of estimating accurate correspondences from individual views to the BEV map, leading to new state-of-the-art performance on standard evaluation benchmarks.},
  archive   = {C_ICCV},
  author    = {Liangchen Song and Jialian Wu and Ming Yang and Qian Zhang and Yuan Li and Junsong Yuan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00599},
  pages     = {6029-6037},
  title     = {Stacked homography transformations for multi-view pedestrian detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DepthInSpace: Exploitation and fusion of multiple video
frames for structured-light depth estimation. <em>ICCV</em>, 6019–6028.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present DepthInSpace, a self-supervised deep-learning method for depth estimation using a structured-light camera. The design of this method is motivated by the commercial use case of embedded depth sensors in nowadays smartphones. We first propose to use estimated optical flow from ambient information of multiple video frames as a complementary guide for training a single-frame depth estimation network, helping to preserve edges and reduce over-smoothing issues. Utilizing optical flow, we also propose to fuse the data of multiple video frames to get a more accurate depth map. In particular, fused depth maps are more robust in occluded areas and incur less in flying pixels artifacts. We finally demonstrate that these more precise fused depth maps can be used as self-supervision for fine-tuning a single-frame depth estimation network to improve its performance. Our models’ effectiveness is evaluated and compared with state-of-the-art models on both synthetic and our newly introduced real datasets. The implementation code, training procedure, and both synthetic and captured real datasets are available at https://www.idiap.ch/paper/depthinspace.},
  archive   = {C_ICCV},
  author    = {Mohammad Mahdi Johari and Camilla Carta and François Fleuret},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00598},
  pages     = {6019-6028},
  title     = {DepthInSpace: Exploitation and fusion of multiple video frames for structured-light depth estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Matching in the dark: A dataset for matching image pairs of
low-light scenes. <em>ICCV</em>, 6009–6018. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper considers matching images of low-light scenes, aiming to widen the frontier of SfM and visual SLAM applications. Recent image sensors can record the brightness of scenes with more than eight-bit precision, available in their RAW-format image. We are interested in making full use of such high-precision information to match extremely low-light scene images that conventional methods cannot handle. For extreme low-light scenes, even if some of their brightness information exists in the RAW format images’ low bits, the standard raw image processing on cameras fails to utilize them properly. As was recently shown by Chen et al. [14], CNNs can learn to produce images with a natural appearance from such RAW-format images. To consider if and how well we can utilize such information stored in RAW-format images for image matching, we have created a new dataset named MID (matching in the dark). Using it, we experimentally evaluated combinations of eight image-enhancing methods and eleven image matching methods consisting of classical/neural local descriptors and classical/neural initial point-matching methods. The results show the advantage of using the RAW-format images and the strengths and weaknesses of the above component methods. They also imply there is room for further research.},
  archive   = {C_ICCV},
  author    = {Wenzheng Song and Masanori Suganuma and Xing Liu and Noriyuki Shimobayashi and Daisuke Maruta and Takayuki Okatani},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00597},
  pages     = {6009-6018},
  title     = {Matching in the dark: A dataset for matching image pairs of low-light scenes},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Transfusion: A novel SLAM method focused on transparent
objects. <em>ICCV</em>, 5999–6008. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently RGB-D sensors have become very popular in the area of Simultaneous Localisation and Mapping (SLAM). The RGB-D SLAM approach relies heavily on the accuracy of the input depth map. However, refraction and reflection of transparent objects will result in false depth input of RGB-D cameras, which makes the traditional RGB-D SLAM algorithm unable to work correctly in the presence of transparent objects. In this paper, we propose a novel SLAM approach called transfusion that allows transparent object existence and recovery in the video input. Our method is composed of two parts. Transparent Objects Cut Iterative Closest Points (TC-ICP)is first used to recover camera pose, detecting and removing transparent objects from input to reduce the trajectory errors. Then Transparent Objects Reconstruction (TO-Reconstruction) is used to reconstruct the transparent objects and opaque objects separately. The opaque objects are reconstructed with the traditional method, and the transparent objects are reconstructed with the visual hull-based method. To evaluate our algorithm, we construct a new RGB-D SLAM database containing 25 video sequences. Each sequence has at least one transparent object. Experiments show that our approach can work adequately in scenes contain transparent objects while the existing approach can not handle them. Our approach significantly improves the accuracy of the camera trajectory and the quality of environment reconstruction.},
  archive   = {C_ICCV},
  author    = {Yifan Zhu and Jiaxiong Qiu and Bo Ren},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00596},
  pages     = {5999-6008},
  title     = {Transfusion: A novel SLAM method focused on transparent objects},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SaccadeCam: Adaptive visual attention for monocular depth
sensing. <em>ICCV</em>, 5989–5998. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most monocular depth sensing methods use conventionally captured images that are created without considering scene content. In contrast, animal eyes have fast mechanical motions, called saccades, that control how the scene is imaged by the fovea, where resolution is highest. In this paper, we present the SaccadeCam framework for adaptively distributing resolution onto regions of interest in the scene. Our algorithm for adaptive resolution is a self-supervised network and we demonstrate results for end-to-end learning for monocular depth estimation. We also show preliminary results with a real SaccadeCam hardware prototype.},
  archive   = {C_ICCV},
  author    = {Brevin Tilmon and Sanjeev J. Koppal},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00595},
  pages     = {5989-5998},
  title     = {SaccadeCam: Adaptive visual attention for monocular depth sensing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ODAM: Object detection, association, and mapping using posed
RGB video. <em>ICCV</em>, 5978–5988. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Localizing objects and estimating their extent in 3D is an important step towards high-level 3D scene understanding, which has many applications in Augmented Reality and Robotics. We present ODAM, a system for 3D Object Detection, Association, and Mapping using posed RGB videos. The proposed system relies on a deep learning front-end to detect 3D objects from a given RGB frame and associate them to a global object-based map using a graph neural network (GNN). Based on these frame-to-model associations, our back-end optimizes object bounding volumes, represented as super-quadrics, under multi-view geometry constraints and the object scale prior. We validate the proposed system on ScanNet where we show a significant improvement over existing RGB-only methods.},
  archive   = {C_ICCV},
  author    = {Kejie Li and Daniel DeTone and Steven Chen and Minh Vo and Ian Reid and Hamid Rezatofighi and Chris Sweeney and Julian Straub and Richard Newcombe},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00594},
  pages     = {5978-5988},
  title     = {ODAM: Object detection, association, and mapping using posed RGB video},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pixel-perfect structure-from-motion with featuremetric
refinement. <em>ICCV</em>, 5967–5977. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Finding local features that are repeatable across multiple views is a cornerstone of sparse 3D reconstruction. The classical image matching paradigm detects keypoints per-image once and for all, which can yield poorly-localized features and propagate large errors to the final geometry. In this paper, we refine two key steps of structure-from-motion by a direct alignment of low-level image information from multiple views: we first adjust the initial keypoint locations prior to any geometric estimation, and subsequently refine points and camera poses as a post-processing. This refinement is robust to large detection noise and appearance changes, as it optimizes a featuremetric error based on dense features predicted by a neural network. This significantly improves the accuracy of camera poses and scene geometry for a wide range of keypoint detectors, challenging viewing conditions, and off-the-shelf deep features. Our system easily scales to large image collections, enabling pixel-perfect crowd-sourced localization at scale. Our code is publicly available at github.com/cvg/pixel-perfect-sfm as an add-on to the popular SfM software COLMAP.},
  archive   = {C_ICCV},
  author    = {Philipp Lindenberger and Paul-Edouard Sarlin and Viktor Larsson and Marc Pollefeys},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00593},
  pages     = {5967-5977},
  title     = {Pixel-perfect structure-from-motion with featuremetric refinement},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep permutation equivariant structure from motion.
<em>ICCV</em>, 5956–5966. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing deep methods produce highly accurate 3D reconstructions in stereo and multiview stereo settings, i.e., when cameras are both internally and externally calibrated. Nevertheless, the challenge of simultaneous recovery of camera poses and 3D scene structure in multiview settings with deep networks is still outstanding. Inspired by projective factorization for Structure from Motion (SFM) and by deep matrix completion techniques, we propose a neural network architecture that, given a set of point tracks in multiple images of a static scene, recovers both the camera parameters and a (sparse) scene structure by minimizing an unsupervised reprojection loss. Our network architecture is designed to respect the structure of the problem: the sought output is equivariant to permutations of both cameras and scene points. Notably, our method does not require initialization of camera parameters or 3D point locations. We test our architecture in two setups: (1) single scene reconstruction and (2) learning from multiple scenes. Our experiments, conducted on a variety of datasets in both internally calibrated and uncalibrated settings, indicate that our method accurately recovers pose and structure, on par with classical state of the art methods. Additionally, we show that a pre-trained network can be used to reconstruct novel scenes using inexpensive fine-tuning with no loss of accuracy.},
  archive   = {C_ICCV},
  author    = {Dror Moran and Hodaya Koslowsky and Yoni Kasten and Haggai Maron and Meirav Galun and Ronen Basri},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00592},
  pages     = {5956-5966},
  title     = {Deep permutation equivariant structure from motion},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). STR-GQN: Scene representation and rendering for unknown
cameras based on spatial transformation routing. <em>ICCV</em>,
5946–5955. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Geometry-aware modules are widely applied in recent deep learning architectures for scene representation and rendering. However, these modules require intrinsic camera information that might not be obtained accurately. In this paper, we propose a Spatial Transformation Routing (STR) mechanism to model the spatial properties without applying any geometric prior. The STR mechanism treats the spatial transformation as the message passing process, and the relation between the view poses and the routing weights is modeled by an end-to-end trainable neural network. Besides, an Occupancy Concept Mapping (OCM) framework is proposed to provide explainable rationals for scene-fusion processes. We conducted experiments on several datasets and show that the proposed STR mechanism improves the performance of the Generative Query Network (GQN). The visualization results reveal that the routing process can pass the observed information from one location of some view to the associated location in the other view, which demonstrates the advantage of the proposed model in terms of spatial cognition.},
  archive   = {C_ICCV},
  author    = {Wen-Cheng Chen and Min-Chun Hu and Chu-Song Chen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00591},
  pages     = {5946-5955},
  title     = {STR-GQN: Scene representation and rendering for unknown cameras based on spatial transformation routing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning efficient photometric feature transform for
multi-view stereo. <em>ICCV</em>, 5936–5945. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel framework to learn to convert the per-pixel photometric information at each view into spatially distinctive and view-invariant low-level features, which can be plugged into existing multi-view stereo pipeline for enhanced 3D reconstruction. Both the illumination conditions during acquisition and the subsequent per-pixel feature transform can be jointly optimized in a differentiable fashion. Our framework automatically adapts to and makes efficient use of the geometric information available in different forms of input data. High-quality 3D reconstructions of a variety of challenging objects are demonstrated on the data captured with an illumination multiplexing device, as well as a point light. Our results compare favorably with state-of-the-art techniques.},
  archive   = {C_ICCV},
  author    = {Kaizhang Kang and Cihui Xie and Ruisheng Zhu and Xiaohe Ma and Ping Tan and Hongzhi Wu and Kun Zhou},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00590},
  pages     = {5936-5945},
  title     = {Learning efficient photometric feature transform for multi-view stereo},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ELLIPSDF: Joint object pose and shape optimization with a
bi-level ellipsoid and signed distance function description.
<em>ICCV</em>, 5926–5935. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous systems need to understand the semantics and geometry of their surroundings in order to comprehend and safely execute object-level task specifications. This paper proposes an expressive yet compact model for joint object pose and shape optimization, and an associated optimization algorithm to infer an object-level map from multi-view RGB-D camera observations. The model is expressive because it captures the identities, positions, orientations, and shapes of objects in the environment. It is compact because it relies on a low-dimensional latent representation of implicit object shape, allowing onboard storage of large multi-category object maps. Different from other works that rely on a single object representation format, our approach has a bi-level object model that captures both the coarse level scale as well as the fine level shape details. Our approach is evaluated on the large-scale real-world ScanNet dataset and compared against state-of-the-art methods.},
  archive   = {C_ICCV},
  author    = {Mo Shan and Qiaojun Feng and You-Yi Jau and Nikolay Atanasov},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00589},
  pages     = {5926-5935},
  title     = {ELLIPSDF: Joint object pose and shape optimization with a bi-level ellipsoid and signed distance function description},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Calibrated and partially calibrated semi-generalized
homographies. <em>ICCV</em>, 5916–5925. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose the first minimal solutions for estimating the semi-generalized homography given a perspective and a generalized camera. The proposed solvers use five 2D-2D image point correspondences induced by a scene plane. One group of solvers assumes the perspective camera to be fully calibrated, while the other estimates the unknown focal length together with the absolute pose parameters. This setup is particularly important in structure-from-motion and visual localization pipelines, where a new camera is localized in each step with respect to a set of known cameras and 2D-3D correspondences might not be available. Thanks to a clever parametrization and the elimination ideal method, our solvers only need to solve a univariate polynomial of degree five or three, respectively a system of polynomial equations in two variables. All proposed solvers are stable and efficient as demonstrated by a number of synthetic and real-world experiments.},
  archive   = {C_ICCV},
  author    = {Snehal Bhayani and Torsten Sattler and Daniel Barath and Patrik Beliansky and Janne Heikkilä and Zuzana Kukelova},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00588},
  pages     = {5916-5925},
  title     = {Calibrated and partially calibrated semi-generalized homographies},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamical pose estimation. <em>ICCV</em>, 5906–5915. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of aligning two sets of 3D geometric primitives given known correspondences. Our first contribution is to show that this primitive alignment framework unifies five perception problems including point cloud registration, primitive (mesh) registration, category-level 3D registration, absolution pose estimation (APE), and category-level APE. Our second contribution is to propose DynAMical Pose estimation (DAMP), the first general and practical algorithm to solve primitive alignment problem by simulating rigid body dynamics arising from virtual springs and damping, where the springs span the shortest distances between corresponding primitives. We evaluate DAMP in simulated and real datasets across all five problems, and demonstrate (i) DAMP always converges to the globally optimal solution in the first three problems with 3D-3D correspondences; (ii) although DAMP sometimes converges to suboptimal solutions in the last two problems with 2D-3D correspondences, using a scheme for escaping local minima, DAMP always succeeds. Our third contribution is to demystify the surprising empirical performance of DAMP and formally prove a global convergence result in the case of point cloud registration by charactering local stability of the equilibrium points of the underlying dynamical system. 1},
  archive   = {C_ICCV},
  author    = {Heng Yang and Chris Doran and Jean-Jacques Slotine},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00587},
  pages     = {5906-5915},
  title     = {Dynamical pose estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Gaussian fusion: Accurate 3D reconstruction via
geometry-guided displacement interpolation. <em>ICCV</em>, 5896–5905.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reconstructing delicate geometric details with consumer RGB-D sensors is challenging due to sensor depth and poses uncertainties. To tackle this problem, we propose a unique geometry-guided fusion framework: 1) First, we characterize fusion correspondences with the geodesic curves derived from the mass transport problem, also known as the Monge-Kantorovich problem. Compared with the depth map back-projection methods, the geodesic curves reveal the geometric structures of the local surface. 2) Moving the points along the geodesic curves is the core of our fusion approach, guided by local geometric properties, i.e., Gaussian curvature and mean curvature. Compared with the state-of-the-art methods, our novel geometry-guided displacement interpolation fully utilizes the meaningful geometric features of the local surface. It makes the reconstruction accuracy and completeness improved. Finally, a significant number of experimental results on real object data verify the superior performance of the proposed method. Our technique achieves the most delicate geometric details on thin objects for which the original depth map back-projection fusion scheme suffers from severe artifacts (See Fig. 1).},
  archive   = {C_ICCV},
  author    = {Duo Chen and Zixin Tang and Zhenyu Xu and Yunan Zheng and Yiguang Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00586},
  pages     = {5896-5905},
  title     = {Gaussian fusion: Accurate 3D reconstruction via geometry-guided displacement interpolation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Radial distortion invariant factorization for structure from
motion. <em>ICCV</em>, 5886–5895. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Factorization methods are frequently used for structure from motion problems (SfM). In the presence of noise they are able to jointly estimate camera matrices and scene points in overdetermined settings, without the need for accurate initial solutions. While the early formulations were restricted to affine models, recent approaches have been show to work with pinhole cameras by minimizing object space errors.In this paper we propose a factorization approach using the so called radial camera, which is invariant to radial distortion and changes in focal length. Assuming a known principal point our approach can reconstruct the 3D scene in settings with unknown and varying radial distortion and focal length. We show on both real and synthetic data that our approach outperforms state-of-the-art factorization methods under these conditions. 1},
  archive   = {C_ICCV},
  author    = {José Pedro Iglesias and Carl Olsson},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00585},
  pages     = {5886-5895},
  title     = {Radial distortion invariant factorization for structure from motion},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PoGO-net: Pose graph optimization with graph neural
networks. <em>ICCV</em>, 5875–5885. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate camera pose estimation or global camera re-localization is a core component in Structure-from-Motion (SfM) and SLAM systems. Given pair-wise relative camera poses, pose-graph optimization (PGO) involves solving for an optimized set of globally-consistent absolute camera poses. In this work, we propose a novel PGO scheme fueled by graph neural networks (GNN), namely PoGO-Net, to conduct the absolute camera pose regression leveraging multiple rotation averaging (MRA). Specifically, PoGO-Net takes a noisy view-graph as the input, where the nodes and edges are designed to encode the geometric constraints and local graph consistency. Besides, we address the outlier edge removal by exploiting an implicit edge-dropping scheme where the noisy or corrupted edges are effectively filtered out with parameterized networks. Furthermore, we introduce a joint loss function embedding MRA formulation such that the robust inference is capable of achieving real-time performances even for large-scale scenes. Our proposed network is trained end-to-end on public benchmarks, outperforming state-of-the-art approaches in extensive experiments that demonstrate the efficiency and robustness of our proposed network.},
  archive   = {C_ICCV},
  author    = {Xinyi Li and Haibin Ling},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00584},
  pages     = {5875-5885},
  title     = {PoGO-net: Pose graph optimization with graph neural networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Putting NeRF on a diet: Semantically consistent few-shot
view synthesis. <em>ICCV</em>, 5865–5874. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present DietNeRF, a 3D neural scene representation estimated from a few images. Neural Radiance Fields (NeRF) learn a continuous volumetric representation of a scene through multi-view consistency, and can be rendered from novel viewpoints by ray casting. While NeRF has an impressive ability to reconstruct geometry and fine details given many images, up to 100 for challenging 360° scenes, it often finds a degenerate solution to its image reconstruction objective when only a few input views are available. To improve few-shot quality, we propose DietNeRF. We introduce an auxiliary semantic consistency loss that encourages realistic renderings at novel poses. DietNeRF is trained on individual scenes to (1) correctly render given input views from the same pose, and (2) match high-level semantic attributes across different, random poses. Our semantic loss allows us to supervise DietNeRF from arbitrary poses. We extract these semantics using a pre-trained visual encoder such as CLIP, a Vision Transformer trained on hundreds of millions of diverse single-view, 2D photographs mined from the web with natural language supervision. In experiments, DietNeRF improves the perceptual quality of few-shot view synthesis when learned from scratch, can render novel views with as few as one observed image when pre-trained on a multi-view dataset, and produces plausible completions of completely unobserved regions. Our project website is available at https://www.ajayj.com/dietnerf.},
  archive   = {C_ICCV},
  author    = {Ajay Jain and Matthew Tancik and Pieter Abbeel},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00583},
  pages     = {5865-5874},
  title     = {Putting NeRF on a diet: Semantically consistent few-shot view synthesis},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Baking neural radiance fields for real-time view synthesis.
<em>ICCV</em>, 5855–5864. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Neural volumetric representations such as Neural Radiance Fields (NeRF) have emerged as a compelling technique for learning to represent 3D scenes from images with the goal of rendering photorealistic images of the scene from unobserved viewpoints. However, NeRF’s computational requirements are prohibitive for real-time applications: rendering views from a trained NeRF requires querying a multilayer perceptron (MLP) hundreds of times per ray. We present a method to train a NeRF, then precompute and store (i.e. &quot;bake&quot;) it as a novel representation called a Sparse Neural Radiance Grid (SNeRG) that enables real-time rendering on commodity hardware. To achieve this, we introduce 1) a reformulation of NeRF’s architecture, and 2) a sparse voxel grid representation with learned feature vectors. The resulting scene representation retains NeRF’s ability to render fine geometric details and view-dependent appearance, is compact (averaging less than 90 MB per scene), and can be rendered in real-time (higher than 30 frames per second on a laptop GPU). Actual screen captures are shown in our video.},
  archive   = {C_ICCV},
  author    = {Peter Hedman and Pratul P. Srinivasan and Ben Mildenhall and Jonathan T. Barron and Paul Debevec},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00582},
  pages     = {5855-5864},
  title     = {Baking neural radiance fields for real-time view synthesis},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nerfies: Deformable neural radiance fields. <em>ICCV</em>,
5845–5854. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present the first method capable of photorealistically reconstructing deformable scenes using photos/videos captured casually from mobile phones. Our approach augments neural radiance fields (NeRF) by optimizing an additional continuous volumetric deformation field that warps each observed point into a canonical 5D NeRF. We observe that these NeRF-like deformation fields are prone to local minima, and propose a coarse-to-fine optimization method for coordinate-based models that allows for more robust optimization. By adapting principles from geometry processing and physical simulation to NeRF-like models, we propose an elastic regularization of the deformation field that further improves robustness. We show that our method can turn casually captured selfie photos/videos into deformable NeRF models that allow for photorealistic renderings of the subject from arbitrary viewpoints, which we dub &quot;nerfies.&quot; We evaluate our method by collecting time-synchronized data using a rig with two mobile phones, yielding train/validation images of the same pose at different viewpoints. We show that our method faithfully reconstructs non-rigidly deforming scenes and reproduces unseen views with high fidelity.},
  archive   = {C_ICCV},
  author    = {Keunhong Park and Utkarsh Sinha and Jonathan T. Barron and Sofien Bouaziz and Dan B Goldman and Steven M. Seitz and Ricardo Martin-Brualla},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00581},
  pages     = {5845-5854},
  title     = {Nerfies: Deformable neural radiance fields},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mip-NeRF: A multiscale representation for anti-aliasing
neural radiance fields. <em>ICCV</em>, 5835–5844. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call &quot;mip-NeRF&quot; (à la &quot;mipmap&quot;), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF’s ability to represent fine details, while also being 7\% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17\% on the dataset presented with NeRF and by 60\% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22× faster.},
  archive   = {C_ICCV},
  author    = {Jonathan T. Barron and Ben Mildenhall and Matthew Tancik and Peter Hedman and Ricardo Martin-Brualla and Pratul P. Srinivasan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00580},
  pages     = {5835-5844},
  title     = {Mip-NeRF: A multiscale representation for anti-aliasing neural radiance fields},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-calibrating neural radiance fields. <em>ICCV</em>,
5826–5834. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we propose a camera self-calibration algorithm for generic cameras with arbitrary non-linear distortions. We jointly learn the geometry of the scene and the accurate camera parameters without any calibration objects. Our camera model consists of a pinhole model, a fourth order radial distortion, and a generic noise model that can learn arbitrary non-linear camera distortions. While traditional self-calibration algorithms mostly rely on geometric constraints, we additionally incorporate photometric consistency. This requires learning the geometry of the scene, and we use Neural Radiance Fields (NeRF). We also propose a new geometric loss function, viz., projected ray distance loss, to incorporate geometric consistency for complex non-linear camera models. We validate our approach on standard real image datasets and demonstrate that our model can learn the camera intrinsics and extrinsics (pose) from scratch without COLMAP initialization. Also, we show that learning accurate camera models in a differentiable manner allows us to improve PSNR over baselines. Our module is an easy-to-use plugin that can be applied to NeRF variants to improve performance. The code and data are currently available at https://github.com/POSTECH-CVLab/SCNeRF},
  archive   = {C_ICCV},
  author    = {Yoonwoo Jeong and Seokjun Ahn and Christopher Choy and Animashree Anandkumar and Minsu Cho and Jaesik Park},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00579},
  pages     = {5826-5834},
  title     = {Self-calibrating neural radiance fields},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LSD-StructureNet: Modeling levels of structural detail in 3D
part hierarchies. <em>ICCV</em>, 5816–5825. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generative models for 3D shapes represented by hierarchies of parts can generate realistic and diverse sets of outputs. However, existing models suffer from the key practical limitation of modelling shapes holistically and thus cannot perform conditional sampling, i.e. they are not able to generate variants on individual parts of generated shapes without modifying the rest of the shape. This is limiting for applications such as 3D CAD design that involve adjusting created shapes at multiple levels of detail. To address this, we introduce LSD-StructureNet, an augmentation to the StructureNet architecture that enables re-generation of parts situated at arbitrary positions in the hierarchies of its outputs. We achieve this by learning individual, probabilistic conditional decoders for each hierarchy depth. We evaluate LSD-StructureNet on the PartNet dataset, the largest dataset of 3D shapes represented by hierarchies of parts. Our results show that contrarily to existing methods, LSD-StructureNet can perform conditional sampling without impacting inference speed or the realism and diversity of its outputs.},
  archive   = {C_ICCV},
  author    = {Dominic Roberts and Ara Danielyan and Hang Chu and Mani Golparvar-Fard and David Forsyth},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00578},
  pages     = {5816-5825},
  title     = {LSD-StructureNet: Modeling levels of structural detail in 3D part hierarchies},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3D shape generation and completion through point-voxel
diffusion. <em>ICCV</em>, 5806–5815. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel approach for probabilistic generative modeling of 3D shapes. Unlike most existing models that learn to deterministically translate a latent vector to a shape, our model, Point-Voxel Diffusion (PVD), is a unified, probabilistic formulation for unconditional shape generation and conditional, multi-modal shape completion. PVD marries denoising diffusion models with the hybrid, point-voxel representation of 3D shapes. It can be viewed as a series of denoising steps, reversing the diffusion process from observed point cloud data to Gaussian noise, and is trained by optimizing a variational lower bound to the (conditional) likelihood function. Experiments demonstrate that PVD is capable of synthesizing high-fidelity shapes, completing partial point clouds, and generating multiple completion results from single-view depth scans of real objects.},
  archive   = {C_ICCV},
  author    = {Linqi Zhou and Yilun Du and Jiajun Wu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00577},
  pages     = {5806-5815},
  title     = {3D shape generation and completion through point-voxel diffusion},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ARAPReg: An as-rigid-as possible regularization loss for
learning deformable shape generators. <em>ICCV</em>, 5795–5805. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces an unsupervised loss for training parametric deformation shape generators. The key idea is to enforce the preservation of local rigidity among the generated shapes. Our approach builds on an approximation of the as-rigid-as possible (or ARAP) deformation energy. We show how to develop the unsupervised loss via a spectral decomposition of the Hessian of the ARAP energy. Our loss nicely decouples pose and shape variations through a robust norm. The loss admits simple closed-form expressions. It is easy to train and can be plugged into any standard generation models, e.g., variational auto-encoder (VAE) and auto-decoder (AD). Experimental results show that our approach outperforms existing shape generation approaches considerably on public benchmark datasets of various shape categories such as human, animal and bone. Our code and data are available at https://github.com/GitBoSun/ARAPReg.},
  archive   = {C_ICCV},
  author    = {Qixing Huang and Xiangru Huang and Bo Sun and Zaiwei Zhang and Junfeng Jiang and Chandrajit Bajaj},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00576},
  pages     = {5795-5805},
  title     = {ARAPReg: An as-rigid-as possible regularization loss for learning deformable shape generators},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep hybrid self-prior for full 3D mesh generation.
<em>ICCV</em>, 5785–5794. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a deep learning pipeline that leverages network self-prior to recover a full 3D model consisting of both a triangular mesh and a texture map from the colored 3D point cloud. Different from previous methods either exploiting 2D self-prior for image editing or 3D self-prior for pure surface reconstruction, we propose to exploit a novel hybrid 2D-3D self-prior in deep neural networks to significantly improve the geometry quality and produce a high-resolution texture map, which is typically missing from the output of commodity-level 3D scanners. In particular, we first generate an initial mesh using a 3D convolutional neural network with 3D self-prior, and then encode both 3D information and color information in the 2D UV atlas, which is further refined by 2D convolutional neural networks with the self-prior. In this way, both 2D and 3D self-priors are utilized for the mesh and texture recovery. Experiments show that, without the need of any additional training data, our method recovers the 3D textured mesh model of high quality from sparse input, and outperforms the state-of-the-art methods in terms of both geometry and texture quality.},
  archive   = {C_ICCV},
  author    = {Xingkui Wei and Zhengqing Chen and Yanwei Fu and Zhaopeng Cui and Yinda Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00575},
  pages     = {5785-5794},
  title     = {Deep hybrid self-prior for full 3D mesh generation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GTT-net: Learned generalized trajectory triangulation.
<em>ICCV</em>, 5775–5784. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present GTT-Net, a supervised learning framework for the reconstruction of sparse dynamic 3D geometry. We build on a graph-theoretic formulation of the generalized trajectory triangulation problem, where non-concurrent multi-view imaging geometry is known but global image sequencing is not provided. GTT-Net learns pairwise affinities modeling the spatio-temporal relationships among our input observations and leverages them to determine 3D geometry estimates. Experiments reconstructing 3D motion-capture sequences show GTT-Net outperforms the state of the art in terms of accuracy and robustness. Within the context of articulated motion reconstruction, our proposed architecture is 1) able to learn and enforce semantic 3D motion priors for shared training and test domains, while being 2) able to generalize its performance across different training and test domains. Moreover, GTT-Net provides a computationally streamlined framework for trajectory triangulation with applications to multi-instance reconstruction and event segmentation.},
  archive   = {C_ICCV},
  author    = {Xiangyu Xu and Enrique Dunn},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00574},
  pages     = {5775-5784},
  title     = {GTT-net: Learned generalized trajectory triangulation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AD-NeRF: Audio driven neural radiance fields for talking
head synthesis. <em>ICCV</em>, 5764–5774. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generating high-fidelity talking head video by fitting with the input audio sequence is a challenging problem that receives considerable attentions recently. In this paper, we address this problem with the aid of neural scene representation networks. Our method is completely different from existing methods that rely on intermediate representations like 2D landmarks or 3D face models to bridge the gap between audio input and video output. Specifically, the feature of input audio signal is directly fed into a conditional implicit function to generate a dynamic neural radiance field, from which a high-fidelity talking-head video corresponding to the audio signal is synthesized using volume rendering. Another advantage of our framework is that not only the head (with hair) region is synthesized as previous methods did, but also the upper body is generated via two individual neural radiance fields. Experimental results demonstrate that our novel framework can (1) produce high-fidelity and natural results, and (2) support free adjustment of audio signals, viewing directions, and background images. Code is available at https://github.com/YudongGuo/AD-NeRF.},
  archive   = {C_ICCV},
  author    = {Yudong Guo and Keyu Chen and Sen Liang and Yong-Jin Liu and Hujun Bao and Juyong Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00573},
  pages     = {5764-5774},
  title     = {AD-NeRF: Audio driven neural radiance fields for talking head synthesis},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Editing conditional radiance fields. <em>ICCV</em>,
5753–5763. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A neural radiance field (NeRF) is a scene model supporting high-quality view synthesis, optimized per scene. In this paper, we explore enabling user editing of a category-level NeRF – also known as a conditional radiance field – trained on a shape category. Specifically, we introduce a method for propagating coarse 2D user scribbles to the 3D space, to modify the color or shape of a local region. First, we propose a conditional radiance field that incorporates new modular network components, including a shape branch that is shared across object instances. Observing multiple instances of the same category, our model learns underlying part semantics without any supervision, thereby allowing the propagation of coarse 2D user scribbles to the entire 3D region (e.g., chair seat). Next, we propose a hybrid network update strategy that targets specific network components, which balances efficiency and accuracy. During user interaction, we formulate an optimization problem that both satisfies the user’s constraints and preserves the original object structure. We demonstrate our editing approach on rendered views of three shape datasets and show that it outperforms prior neural editing approaches. Finally, we edit the appearance and shape of a single-view real photograph and show that the edit propagates to extrapolated novel views.},
  archive   = {C_ICCV},
  author    = {Steven Liu and Xiuming Zhang and Zhoutong Zhang and Richard Zhang and Jun-Yan Zhu and Bryan Russell},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00572},
  pages     = {5753-5763},
  title     = {Editing conditional radiance fields},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural articulated radiance field. <em>ICCV</em>, 5742–5752.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present Neural Articulated Radiance Field (NARF), a novel deformable 3D representation for articulated objects learned from images. While recent advances in 3D implicit representation have made it possible to learn models of complex objects, learning pose-controllable representations of articulated objects remains a challenge, as current methods require 3D shape supervision and are unable to render appearance. In formulating an implicit representation of 3D articulated objects, our method considers only the rigid transformation of the most relevant object part in solving for the radiance field at each 3D location. In this way, the proposed method represents pose-dependent changes without significantly increasing the computational complexity. NARF is fully differentiable and can be trained from images with pose annotations. Moreover, through the use of an autoencoder, it can learn appearance variations over multiple instances of an object class. Experiments show that the proposed method is efficient and can generalize well to novel poses. The code is available for research purposes at https://github.com/nogu-atsu/NARF.},
  archive   = {C_ICCV},
  author    = {Atsuhiro Noguchi and Xiao Sun and Stephen Lin and Tatsuya Harada},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00571},
  pages     = {5742-5752},
  title     = {Neural articulated radiance field},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PlenOctrees for real-time rendering of neural radiance
fields. <em>ICCV</em>, 5732–5741. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a method to render Neural Radiance Fields (NeRFs) in real time using PlenOctrees, an octree-based 3D representation which supports view-dependent effects. Our method can render 800×800 images at more than 150 FPS, which is over 3000 times faster than conventional NeRFs. We do so without sacrificing quality while preserving the ability of NeRFs to perform free-viewpoint rendering of scenes with arbitrary geometry and view-dependent effects. Real-time performance is achieved by pre-tabulating the NeRF into a PlenOctree. In order to preserve view-dependent effects such as specularities, we factorize the appearance via closed-form spherical basis functions. Specifically, we show that it is possible to train NeRFs to predict a spherical harmonic representation of radiance, removing the viewing direction as an input to the neural network. Furthermore, we show that PlenOctrees can be directly optimized to further minimize the reconstruction loss, which leads to equal or better quality compared to competing methods. Moreover, this octree optimization step can be used to reduce the training time, as we no longer need to wait for the NeRF training to converge fully. Our real-time neural rendering approach may potentially enable new applications such as 6-DOF industrial and product visualizations, as well as next generation AR/VR systems. PlenOctrees are amenable to in-browser rendering as well; please visit the project page for the interactive online demo, as well as video and code: https://alexyu.net/plenoctrees.},
  archive   = {C_ICCV},
  author    = {Alex Yu and Ruilong Li and Matthew Tancik and Hao Li and Ren Ng and Angjoo Kanazawa},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00570},
  pages     = {5732-5741},
  title     = {PlenOctrees for real-time rendering of neural radiance fields},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BARF: Bundle-adjusting neural radiance fields.
<em>ICCV</em>, 5721–5731. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Neural Radiance Fields (NeRF) [31] have recently gained a surge of interest within the computer vision community for its power to synthesize photorealistic novel views of real-world scenes. One limitation of NeRF, however, is its requirement of accurate camera poses to learn the scene representations. In this paper, we propose Bundle-Adjusting Neural Radiance Fields (BARF) for training NeRF from imperfect (or even unknown) camera poses — the joint problem of learning neural 3D representations and registering camera frames. We establish a theoretical connection to classical image alignment and show that coarse-to-fine registration is also applicable to NeRF. Furthermore, we show that naïvely applying positional encoding in NeRF has a negative impact on registration with a synthesis-based objective. Experiments on synthetic and real-world data show that BARF can effectively optimize the neural scene representations and resolve large camera pose misalignment at the same time. This enables view synthesis and localization of video sequences from unknown camera poses, opening up new avenues for visual localization systems (e.g. SLAM) and potential applications for dense 3D mapping and reconstruction.},
  archive   = {C_ICCV},
  author    = {Chen-Hsuan Lin and Wei-Chiu Ma and Antonio Torralba and Simon Lucey},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00569},
  pages     = {5721-5731},
  title     = {BARF: Bundle-adjusting neural radiance fields},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EPP-MVSNet: Epipolar-assembling based depth prediction for
multi-view stereo. <em>ICCV</em>, 5712–5720. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we proposed EPP-MVSNet, a novel deep learning network for 3D reconstruction from multi-view stereo (MVS). EPP-MVSNet can accurately aggregate features at high resolution to a limited cost volume with an optimal depth range, thus, leads to effective and efficient 3D construction. Distinct from existing works which measure feature cost at discrete positions which affects the 3D reconstruction accuracy, EPP-MVSNet introduces an epipolar-assembling-based kernel that operates on adaptive intervals along epipolar lines for making full use of the image resolution. Further, we introduce an entropy-based refining strategy where the cost volume describes the space geometry with the little redundancy. Moreover, we design a light-weighted network with Pseudo-3D convolutions integrated to achieve high accuracy and efficiency. We have conducted extensive experiments on challenging datasets Tanks &amp; Temples(TNT), ETH3D and DTU. As a result, we achieve promising results on all datasets and the highest F-Score on the online TNT intermediate benchmark. Code is available at https://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/cv/eppmvsnet.},
  archive   = {C_ICCV},
  author    = {Xinjun Ma and Yue Gong and Qirui Wang and Jingwei Huang and Lei Chen and Fan Yu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00568},
  pages     = {5712-5720},
  title     = {EPP-MVSNet: Epipolar-assembling based depth prediction for multi-view stereo},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-view 3D reconstruction with transformers.
<em>ICCV</em>, 5702–5711. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep CNN-based methods have so far achieved the state of the art results in multi-view 3D object reconstruction. Despite the considerable progress, the two core modules of these methods - view feature extraction and multi-view fusion, are usually investigated separately, and the relations among multiple input views are rarely explored. Inspired by the recent great success in Transformer models, we reformulate the multi-view 3D reconstruction as a sequence-to-sequence prediction problem and propose a framework named 3D Volume Transformer. Unlike previous CNN-based methods using a separate design, we unify the feature extraction and view fusion in a single Transformer network. A natural advantage of our design lies in the exploration of view-to-view relationships using self-attention among multiple unordered inputs. On ShapeNet - a large-scale 3D reconstruction benchmark, our method achieves a new state-of-the-art accuracy in multi-view reconstruction with fewer parameters (70\% less) than CNN-based methods. Experimental results also suggest the strong scaling capability of our method. Our code will be made publicly available.},
  archive   = {C_ICCV},
  author    = {Dan Wang and Xinrui Cui and Xun Chen and Zhengxia Zou and Tianyang Shi and Septimiu Salcudean and Z. Jane Wang and Rabab Ward},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00567},
  pages     = {5702-5711},
  title     = {Multi-view 3D reconstruction with transformers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic view synthesis from dynamic monocular video.
<em>ICCV</em>, 5692–5701. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an algorithm for generating novel views at arbitrary viewpoints and any input time step given a monocular video of a dynamic scene. Our work builds upon recent advances in neural implicit representation and uses continuous and differentiable functions for modeling the time-varying structure and the appearance of the scene. We jointly train a time-invariant static NeRF and a time-varying dynamic NeRF, and learn how to blend the results in an unsupervised manner. However, learning this implicit function from a single video is highly ill-posed (with infinitely many solutions that match the input video). To resolve the ambiguity, we introduce regularization losses to encourage a more physically plausible solution. We show extensive quantitative and qualitative results of dynamic view synthesis from casually captured videos.},
  archive   = {C_ICCV},
  author    = {Chen Gao and Ayush Saraf and Johannes Kopf and Jia-Bin Huang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00566},
  pages     = {5692-5701},
  title     = {Dynamic view synthesis from dynamic monocular video},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extreme structure from motion for indoor panoramas without
visual overlaps. <em>ICCV</em>, 5683–5691. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes an extreme Structure from Motion (SfM) algorithm for residential indoor panoramas that have little to no visual overlaps. Only a single panorama is present in a room for many cases, making the task infeasible for existing SfM algorithms. Our idea is to learn to evaluate the realism of room/door/window arrangements in the topdown semantic space. After using heuristics to enumerate possible arrangements based on door detections, we evaluate their realism scores, pick the most realistic arrangement, and return the corresponding camera poses. We evaluate the proposed approach on a dataset of 1029 panorama images with 286 houses. Our qualitative and quantitative evaluations show that an existing SfM approach completely fails for most of the houses. The proposed approach achieves the mean positional error of less than 1.0 meter for 47\% of the houses and even 78\% when considering the top five reconstructions. We will share the code and data in https://github.com/aminshabani/extreme-indoor-sfm.},
  archive   = {C_ICCV},
  author    = {Mohammad Amin Shabani and Weilian Song and Makoto Odamaki and Hirochika Fujiki and Yasutaka Furukawa},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00565},
  pages     = {5683-5691},
  title     = {Extreme structure from motion for indoor panoramas without visual overlaps},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pri3D: Can 3D priors help 2D representation learning?
<em>ICCV</em>, 5673–5682. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in 3D perception have shown impressive progress in understanding geometric structures of 3D shapes and even scenes. Inspired by these advances in geometric understanding, we aim to imbue image-based perception with representations learned under geometric constraints. We introduce an approach to learn view-invariant, geometry-aware representations for network pre-training, based on multi-view RGB-D data, that can then be effectively transferred to downstream 2D tasks. We propose to employ contrastive learning under both multi-view image constraints and image-geometry constraints to encode 3D priors into learned 2D representations. This results not only in improvement over 2D-only representation learning on the image-based tasks of semantic segmentation, instance segmentation and object detection on real-world indoor datasets, but moreover, provides significant improvement in the low data regime. We show significant improvement of 6.0\% on semantic segmentation on full data as well as 11.9\% on 20\% data against baselines on ScanNet. Our code is open sourced at https://github.com/Sekunde/Pri3D.},
  archive   = {C_ICCV},
  author    = {Ji Hou and Saining Xie and Benjamin Graham and Angela Dai and Matthias Nießner},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00564},
  pages     = {5673-5682},
  title     = {Pri3D: Can 3D priors help 2D representation learning?},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DeepPRO: Deep partial point cloud registration of objects.
<em>ICCV</em>, 5663–5672. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the problem of online and real-time registration of partial point clouds obtained from an unseen real-world rigid object without knowing its 3D model. The point cloud is partial as it is obtained by a depth sensor capturing only the visible part of the object from a certain viewpoint. It introduces two main challenges: 1) two partial point clouds do not fully overlap and 2) keypoints tend to be less reliable when the visible part of the object does not have salient local structures. To address these issues, we propose DeepPRO, a keypoint-free and an end-to-end trainable deep neural network. Its core idea is inspired by how humans align two point clouds: we can imagine how two point clouds will look like after the registration based on their shape. To realize the idea, DeepPRO has inputs of two partial point clouds and directly predicts the point-wise location of the aligned point cloud. By preserving the ordering of points during the prediction, we enjoy dense correspondences between input and predicted point clouds when inferring rigid transform parameters. We conduct extensive experiments on the real-world Linemod and synthetic ModelNet40 datasets. In addition, we collect and evaluate on the PRO1k dataset, a large-scale version of Linemod meant to test generalization to real-world scans. Results show that DeepPRO achieves the best accuracy against thirteen strong baseline methods, e.g., 2.2mm ADD on the Linemod dataset, while running 50 fps on mobile devices.},
  archive   = {C_ICCV},
  author    = {Donghoon Lee and Onur C. Hamsici and Steven Feng and Prachee Sharma and Thorsten Gernoth},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00563},
  pages     = {5663-5672},
  title     = {DeepPRO: Deep partial point cloud registration of objects},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3DeepCT: Learning volumetric scattering tomography of
clouds. <em>ICCV</em>, 5651–5662. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present 3DeepCT, a deep neural network for computed tomography, which performs 3D reconstruction of scattering volumes from multi-view images. The architecture is dictated by the stationary nature of atmospheric cloud fields. The task of volumetric scattering tomography aims at recovering a volume from its 2D projections. This problem has been approached by diverse inverse methods based on signal processing and physics models. However, such techniques are typically iterative, exhibiting a high computational load and a long convergence time. We show that 3DeepCT outperforms physics-based inverse scattering methods, in accuracy, as well as offering orders of magnitude improvement in computational run-time. We further introduce a hybrid model that combines 3DeepCT and physics-based analysis. The resultant hybrid technique enjoys fast inference time and improved recovery performance.},
  archive   = {C_ICCV},
  author    = {Yael Sde-Chen and Yoav Y. Schechner and Vadim Holodovsky and Eshkol Eytan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00562},
  pages     = {5651-5662},
  title     = {3DeepCT: Learning volumetric scattering tomography of clouds},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Learning icosahedral spherical probability map based on
bingham mixture model for vanishing point estimation. <em>ICCV</em>,
5641–5650. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing vanishing point (VP) estimation methods rely on pre-extracted image lines and/or prior knowledge of the number of VPs. However, in practice, this information may be insufficient or unavailable. To solve this problem, we propose a network that treats a perspective image as input and predicts a spherical probability map of VP. Based on this map, we can detect all the VPs. Our method is reliable thanks to four technical novelties. First, we leverage the icosahedral spherical representation to express our probability map. This representation provides uniform pixel distribution, and thus facilitates estimating arbitrary positions of VPs. Second, we design a loss function that enforces the antipodal symmetry and sparsity of our spherical probability map to prevent over-fitting. Third, we generate the ground truth probability map that reasonably expresses the locations and uncertainties of VPs. This map unnecessarily peaks at noisy annotated VPs, and also exhibits various anisotropic dispersions. Fourth, given a predicted probability map, we detect VPs by fitting a Bingham mixture model. This strategy can robustly handle close VPs and provide the confidence level of VP useful for practical applications. Experiments showed that our method achieves the best compromise between generality, accuracy, and efficiency, compared with state-of-the-art approaches.},
  archive   = {C_ICCV},
  author    = {Haoang Li and Kai Chen and Pyojin Kim and Kuk-Jin Yoon and Zhe Liu and Kyungdon Joo and Yun-Hui Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00561},
  pages     = {5641-5650},
  title     = {Learning icosahedral spherical probability map based on bingham mixture model for vanishing point estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive surface reconstruction with multiscale
convolutional kernels. <em>ICCV</em>, 5631–5640. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose generalized convolutional kernels for 3D reconstruction with ConvNets from point clouds. Our method uses multiscale convolutional kernels that can be applied to adaptive grids as generated with octrees. In addition to standard kernels in which each element has a distinct spatial location relative to the center, our elements have a distinct relative location as well as a relative scale level. Making our kernels span multiple resolutions allows us to apply ConvNets to adaptive grids for large problem sizes where the input data is sparse but the entire domain needs to be processed. Our ConvNet architecture can predict the signed and unsigned distance fields for large data sets with millions of input points and is faster and more accurate than classic energy minimization or recent learning approaches. We demonstrate this in a zero-shot setting where we only train on synthetic data and evaluate on the Tanks and Temples dataset of real-world large-scale 3D scenes.},
  archive   = {C_ICCV},
  author    = {Benjamin Ummenhofer and Vladlen Koltun},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00560},
  pages     = {5631-5640},
  title     = {Adaptive surface reconstruction with multiscale convolutional kernels},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Out-of-core surface reconstruction via global TGV
minimization. <em>ICCV</em>, 5621–5630. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an out-of-core variational approach for surface reconstruction from a set of aligned depth maps. Input depth maps are supposed to be reconstructed from regular photos or/and can be a representation of terrestrial LIDAR point clouds. Our approach is based on surface reconstruction via total generalized variation minimization (TGV) because of its strong visibility-based noise-filtering properties and GPU-friendliness. Our main contribution is an out-of-core OpenCL-accelerated adaptation of this numerical algorithm which can handle arbitrarily large real-world scenes with scale diversity.},
  archive   = {C_ICCV},
  author    = {Nikolai Poliarnyi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00559},
  pages     = {5621-5630},
  title     = {Out-of-core surface reconstruction via global TGV minimization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Scene synthesis via uncertainty-driven attribute
synchronization. <em>ICCV</em>, 5610–5620. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Developing deep neural networks to generate 3D scenes is a fundamental problem in neural synthesis with immediate applications in architectural CAD, computer graphics, as well as in generating virtual robot training environments. This task is challenging because 3D scenes exhibit diverse patterns, ranging from continuous ones, such as object sizes and the relative poses between pairs of shapes, to discrete patterns, such as occurrence and co-occurrence of objects with symmetrical relationships. This paper introduces a novel neural scene synthesis approach that can capture diverse feature patterns of 3D scenes. Our method combines the strength of both neural network-based and conventional scene synthesis approaches. We use the parametric prior distributions learned from training data, which provide uncertainties of object attributes and relative attributes, to regularize the outputs of feed-forward neural models. Moreover, instead of merely predicting a scene layout, our approach predicts an over-complete set of attributes. This methodology allows us to utilize the underlying consistency constraints among the predicted attributes to prune infeasible predictions. Experimental results show that our approach outperforms existing methods considerably. The generated 3D scenes interpolate the training data faithfully while preserving both continuous and discrete feature patterns.},
  archive   = {C_ICCV},
  author    = {Haitao Yang and Zaiwei Zhang and Siming Yan and Haibin Huang and Chongyang Ma and Yi Zheng and Chandrajit Bajaj and Qixing Huang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00558},
  pages     = {5610-5620},
  title     = {Scene synthesis via uncertainty-driven attribute synchronization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). H3D-net: Few-shot high-fidelity 3D head reconstruction.
<em>ICCV</em>, 5600–5609. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent learning approaches that implicitly represent surface geometry using coordinate-based neural representations have shown impressive results in the problem of multi-view 3D reconstruction. The effectiveness of these techniques is, however, subject to the availability of a large number (several tens) of input views of the scene, and computationally demanding optimizations. In this paper, we tackle these limitations for the specific problem of few-shot full 3D head reconstruction, by endowing coordinate-based representations with a probabilistic shape prior that enables faster convergence and better generalization when using few input images (down to three). First, we learn a shape model of 3D heads from thousands of incomplete raw scans using implicit representations. At test time, we jointly overfit two coordinate-based neural networks to the scene, one modelling the geometry and another estimating the surface radiance, using implicit differentiable rendering. We devise a two-stage optimization strategy in which the learned prior is used to initialize and constrain the geometry during an initial optimization phase. Then, the prior is unfrozen and fine-tuned to the scene. By doing this, we achieve high-fidelity head reconstructions, including hair and shoulders, and with a high level of detail that consistently outperforms both state-of-the-art 3D Morphable Models methods in the few-shot scenario, and nonparametric methods when large sets of views are available.},
  archive   = {C_ICCV},
  author    = {Eduard Ramon and Gil Triginer and Janna Escur and Albert Pumarola and Jaime Garcia and Xavier Giró-i-Nieto and Francesc Moreno-Noguer},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00557},
  pages     = {5600-5609},
  title     = {H3D-net: Few-shot high-fidelity 3D head reconstruction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NerfingMVS: Guided optimization of neural radiance fields
for indoor multi-view stereo. <em>ICCV</em>, 5590–5599. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we present a new multi-view depth estimation method that utilizes both conventional SfM reconstruction and learning-based priors over the recently proposed neural radiance fields (NeRF). Unlike existing neural network based optimization method that relies on estimated correspondences, our method directly optimizes over implicit volumes, eliminating the challenging step of matching pixels in indoor scenes. The key to our approach is to utilize the learning-based priors to guide the optimization process of NeRF. Our system firstly adapts a monocular depth network over the target scene by finetuning on its sparse SfM reconstruction. Then, we show that the shape-radiance ambiguity of NeRF still exists in indoor environments and propose to address the issue by employing the adapted depth priors to monitor the sampling process of volume rendering. Finally, a per-pixel confidence map acquired by error computation on the rendered image can be used to fur ther improve the depth quality. Experiments show that our proposed framework significantly outperforms state-of-the-art methods on indoor scenes, with surprising findings presented on the effectiveness of correspondence-based opti-mization and NeRF-based optimization over the adapted depth priors. In addition, we show that the guided opti-mization scheme does not sacrifice the original synthesis capability of neural radiance fields, improving the rendering quality on both seen and novel views. Code is available at https://github.com/weiyithu/NerfingMVS.},
  archive   = {C_ICCV},
  author    = {Yi Wei and Shaohui Liu and Yongming Rao and Wang Zhao and Jiwen Lu and Jie Zhou},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00556},
  pages     = {5590-5599},
  title     = {NerfingMVS: Guided optimization of neural radiance fields for indoor multi-view stereo},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PR-RRN: Pairwise-regularized residual-recursive networks for
non-rigid structure-from-motion. <em>ICCV</em>, 5580–5589. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose PR-RRN, a novel neural-network based method for Non-rigid Structure-from-Motion (NRSfM). PR-RRN consists of Residual-Recursive Networks (RRN) and two extra regularization losses. RRN is designed to effectively recover 3D shape and camera from 2D keypoints with novel residual-recursive structure. As NRSfM is a highly under-constrained problem, we propose two new pairwise regularization to further regularize the reconstruction. The Rigidity-based Pairwise Contrastive Loss regularizes the shape representation by encouraging higher similarity between the representations of high-rigidity pairs of frames than low-rigidity pairs. We propose minimum singular-value ratio to measure the pairwise rigidity. The Pairwise Consistency Loss enforces the reconstruction to be consistent when the estimated shapes and cameras are exchanged between pairs. Our approach achieves state-of-the-art performance on CMU MOCAP and PASCAL3D+ dataset.},
  archive   = {C_ICCV},
  author    = {Haitian Zeng and Yuchao Dai and Xin Yu and Xiaohan Wang and Yi Yang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00555},
  pages     = {5580-5589},
  title     = {PR-RRN: Pairwise-regularized residual-recursive networks for non-rigid structure-from-motion},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). UNISURF: Unifying neural implicit surfaces and radiance
fields for multi-view reconstruction. <em>ICCV</em>, 5569–5579. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Neural implicit 3D representations have emerged as a powerful paradigm for reconstructing surfaces from multi-view images and synthesizing novel views. Unfortunately, existing methods such as DVR or IDR require accurate per-pixel object masks as supervision. At the same time, neural radiance fields have revolutionized novel view synthesis. However, NeRF’s estimated volume density does not admit accurate surface reconstruction. Our key insight is that implicit surface models and radiance fields can be formulated in a unified way, enabling both surface and volume rendering using the same model. This unified perspective enables novel, more efficient sampling procedures and the ability to reconstruct accurate surfaces without input masks. We compare our method on the DTU, BlendedMVS, and a synthetic indoor dataset. Our experiments demonstrate that we outperform NeRF in terms of reconstruction quality while performing on par with IDR without requiring masks.},
  archive   = {C_ICCV},
  author    = {Michael Oechsle and Songyou Peng and Andreas Geiger},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00554},
  pages     = {5569-5579},
  title     = {UNISURF: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Minimal solutions for panoramic stitching given gravity
prior. <em>ICCV</em>, 5559–5568. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When capturing panoramas, people tend to align their cameras with the vertical axis, i.e., the direction of gravity. Moreover, modern devices, e.g. smartphones and tablets, are equipped with an IMU (Inertial Measurement Unit) that can measure the gravity vector accurately. Using this prior, the y-axes of the cameras can be aligned or assumed to be already aligned, reducing the relative orientation to 1-DOF (degree of freedom). Exploiting this assumption, we propose new minimal solutions to panoramic stitching of images taken by cameras with coinciding optical centers, i.e. undergoing pure rotation. We consider six practical camera configurations, from fully calibrated ones up to a camera with unknown fixed or varying focal length and with or without radial distortion. The solvers are tested both on synthetic scenes, on more than 500k real image pairs from the Sun360 dataset, and from scenes captured by us using two smartphones equipped with IMUs. The new solvers have similar or better accuracy than the state-of-the-art ones and outperform them in terms of processing time.},
  archive   = {C_ICCV},
  author    = {Yaqing Ding and Daniel Barath and Zuzana Kukelova},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00553},
  pages     = {5559-5568},
  title     = {Minimal solutions for panoramic stitching given gravity prior},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Orthographic-perspective epipolar geometry. <em>ICCV</em>,
5550–5558. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we consider the epipolar geometry between orthographic and perspective cameras. We generalize many of the classical results for the perspective essential matrix to this setting and derive novel minimal solvers, not only for the calibrated case, but also for partially calibrated and non-central camera setups. While orthographic cameras might seem exotic, they occur naturally in many applications. They can e.g. model 2D maps (such as floor plans), aerial/satellite photography and even approximate narrow field-of-view cameras (e.g. from telephoto lenses). In our experiments we highlight various applications of the developed theory and solvers, including Radar-Camera calibration and aligning Structure-from-Motion models to aerial or satellite images.},
  archive   = {C_ICCV},
  author    = {Viktor Larsson and Marc Pollefeys and Magnus Oskarsson},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00552},
  pages     = {5550-5558},
  title     = {Orthographic-perspective epipolar geometry},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Lightweight multi-person total motion capture using sparse
multi-view cameras. <em>ICCV</em>, 5540–5549. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-person total motion capture is extremely challenging when it comes to handle severe occlusions, different reconstruction granularities from body to face and hands, drastically changing observation scales and fast body movements. To overcome these challenges above, we contribute a lightweight total motion capture system for multi-person interactive scenarios using only sparse multi-view cameras. By contributing a novel hand and face bootstrapping algorithm, our method is capable of efficient localization and accurate association of the hands and faces even on severe occluded occasions. We leverage both pose regression and keypoints detection methods and further propose a unified two-stage parametric fitting method for achieving pixel-aligned accuracy. Moreover, for extremely self-occluded poses and close interactions, a novel feedback mechanism is proposed to propagate the pixel-aligned reconstructions into the next frame for more accurate association. Overall, we propose the first light-weight total capture system and achieves fast, robust and accurate multi-person total motion capture performance. The results and experiments show that our method achieves more accurate results than existing methods under sparse-view setups.},
  archive   = {C_ICCV},
  author    = {Yuxiang Zhang and Zhe Li and Liang An and Mengcheng Li and Tao Yu and Yebin Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00551},
  pages     = {5540-5549},
  title     = {Lightweight multi-person total motion capture using sparse multi-view cameras},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MBA-VO: Motion blur aware visual odometry. <em>ICCV</em>,
5530–5539. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion blur is one of the major challenges remaining for visual odometry methods. In low-light conditions where longer exposure times are necessary, motion blur can appear even for relatively slow camera motions. In this paper we present a novel hybrid visual odometry pipeline with direct approach that explicitly models and estimates the camera’s local trajectory within the exposure time. This allows us to actively compensate for any motion blur that occurs due to the camera motion. In addition, we also contribute a novel benchmarking dataset for motion blur aware visual odometry. In experiments we show that by directly modeling the image formation process, we are able to improve robustness of the visual odometry, while keeping comparable accuracy as that for images without motion blur. Both the code and the datasets can be found from https://github.com/ethliup/MBA-VO.},
  archive   = {C_ICCV},
  author    = {Peidong Liu and Xingxing Zuo and Viktor Larsson and Marc Pollefeys},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00550},
  pages     = {5530-5539},
  title     = {MBA-VO: Motion blur aware visual odometry},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Viewing graph solvability via cycle consistency.
<em>ICCV</em>, 5520–5529. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In structure-from-motion the viewing graph is a graph where vertices correspond to cameras and edges represent fundamental matrices. We provide a new formulation and an algorithm for establishing whether a viewing graph is solvable, i.e. it uniquely determines a set of projective cameras. Known theoretical conditions either do not fully characterize the solvability of all viewing graphs, or are exceedingly hard to compute for they involve solving a system of polynomial equations with a large number of unknowns. The main result of this paper is a method for reducing the number of unknowns by exploiting the cycle consistency. We advance the understanding of the solvability by (i) finishing the classification of all previously undecided minimal graphs up to 9 nodes, (ii) extending the practical solvability testing up to minimal graphs with up to 90 nodes, and (iii) definitely answering an open research question by showing that the finite solvability is not equivalent to the solvability. Finally, we present an experiment on real data showing that unsolvable graphs are appearing in practical situations.},
  archive   = {C_ICCV},
  author    = {Federica Arrigoni and Andrea Fusiello and Elisa Ricci and Tomas Pajdla},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00549},
  pages     = {5520-5529},
  title     = {Viewing graph solvability via cycle consistency},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Feature interactive representation for point cloud
registration. <em>ICCV</em>, 5510–5519. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Point cloud registration is the process of using the common structures in two point clouds to splice them together. To find out these common structures and make these structures match more accurately, interacting information of the source and target point clouds is essential. However, limited attention has been paid to explicitly model such feature interaction. To this end, we propose a Feature Interactive Representation learning Network (FIRE-Net), which can explore feature interaction among the source and target point clouds from different levels. Specifically, we first introduce a Combined Feature Encoder (CFE) based on feature interaction intra point cloud. The CFE extracts interactive features intra each point cloud and combines them to enhance the ability of the network to describe the local geometric structure. Then, we propose a feature interaction mechanism inter point clouds which includes a Local Interaction Unit (LIU) and a Global Interaction Unit (GIU). The former is used to interact information between point pairs across two point clouds, thus the point features in one point cloud and its similar point features in another point cloud can be aware of each other. The latter is applied to change the per-point features depending on the global cross information of two point clouds, thus one point cloud has the global perception of another. Extensive experiments on partially overlapping point cloud registration show that our method achieves state-of-the-art performance.},
  archive   = {C_ICCV},
  author    = {Bingli Wu and Jie Ma and Gaojie Chen and Pei An},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00548},
  pages     = {5510-5519},
  title     = {Feature interactive representation for point cloud registration},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 4D cloud scattering tomography. <em>ICCV</em>, 5500–5509.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We derive computed tomography (CT) of a time-varying volumetric scattering object, using a small number of moving cameras. We focus on passive tomography of dynamic clouds, as clouds have a major effect on the Earth’s climate. State of the art scattering CT assumes a static object. Existing 4D CT methods rely on a linear image formation model and often on significant priors. In this paper, the angular and temporal sampling rates needed for a proper recovery are discussed. Spatiotemporal CT is achieved using gradient-based optimization, which accounts for the correlation time of the dynamic object content. We demonstrate this in physics-based simulations and on experimental real-world data.},
  archive   = {C_ICCV},
  author    = {Roi Ronen and Yoav Y. Schechner and Eshkol Eytan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00547},
  pages     = {5500-5509},
  title     = {4D cloud scattering tomography},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Superpoint network for point cloud oversegmentation.
<em>ICCV</em>, 5490–5499. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Superpoints are formed by grouping similar points with local geometric structures, which can effectively reduce the number of primitives of point clouds for subsequent point cloud processing. Existing superpoint methods mainly focus on employing clustering or graph partition to generate superpoints with handcrafted or learned features. Nonetheless, these methods cannot learn superpoints of point clouds with an end-to-end network. In this paper, we develop a new deep iterative clustering network to directly generate superpoints from irregular 3D point clouds in an end-to-end manner. Specifically, in our clustering network, we first jointly learn a soft point-superpoint association map from the coordinate and feature spaces of point clouds, where each point is assigned to the superpoint with a learned weight. Furthermore, we then iteratively update the association map and superpoint centers so that we can more accurately group the points into the corresponding superpoints with locally similar geometric structures. Finally, by predicting the pseudo labels of the superpoint centers, we formulate a label consistency loss on the points and superpoint centers to train the network. Extensive experiments on various datasets indicate that our method not only achieves the state-of-the-art on superpoint generation but also improves the performance of point cloud semantic segmentation. Code is available at https://github.com/fpthink/SPNet.},
  archive   = {C_ICCV},
  author    = {Le Hui and Jia Yuan and Mingmei Cheng and Jin Xie and Xiaoya Zhang and Jian Yang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00546},
  pages     = {5490-5499},
  title     = {Superpoint network for point cloud oversegmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SnowflakeNet: Point cloud completion by snowflake point
deconvolution with skip-transformer. <em>ICCV</em>, 5479–5489. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Point cloud completion aims to predict a complete shape in high accuracy from its partial observation. However, previous methods usually suffered from discrete nature of point cloud and unstructured prediction of points in local regions, which makes it hard to reveal fine local geometric details on the complete shape. To resolve this issue, we propose SnowflakeNet with Snowflake Point Deconvolution (SPD) to generate the complete point clouds. The SnowflakeNet models the generation of complete point clouds as the snowflake-like growth of points in 3D space, where the child points are progressively generated by splitting their parent points after each SPD. Our insight of revealing detailed geometry is to introduce skip-transformer in SPD to learn point splitting patterns which can fit local regions the best. Skip-transformer leverages attention mechanism to summarize the splitting patterns used in the previous SPD layer to produce the splitting in the current SPD layer. The locally compact and structured point cloud generated by SPD is able to precisely capture the structure characteristic of 3D shape in local patches, which enables the network to predict highly detailed geometries, such as smooth regions, sharp edges and corners. Our experimental results outperform the state-of-the-art point cloud completion methods under widely used benchmarks. Code will be available at https://github.com/AllenXiangX/SnowflakeNet.},
  archive   = {C_ICCV},
  author    = {Peng Xiang and Xin Wen and Yu-Shen Liu and Yan-Pei Cao and Pengfei Wan and Wen Zheng and Zhizhong Han},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00545},
  pages     = {5479-5489},
  title     = {SnowflakeNet: Point cloud completion by snowflake point deconvolution with skip-transformer},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distinctiveness oriented positional equilibrium for point
cloud registration. <em>ICCV</em>, 5470–5478. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent state-of-the-art learning-based approaches to point cloud registration have largely been based on graph neural networks (GNN). However, these prominent GNN backbones suffer from the indistinguishable features problem associated with oversmoothing and structural ambiguity of the high-level features, a crucial bottleneck to point cloud registration that has evaded scrutiny in the recent relevant literature. To address this issue, we propose the Distinctiveness oriented Positional Equilibrium (DoPE) module, a novel positional embedding scheme that significantly improves the distinctiveness of the high-level features within both the source and target point clouds, resulting in superior point matching and hence registration accuracy. Specifically, we use the DoPE module in an iterative registration framework, whereby the two point clouds are gradually registered via rigid transformations that are computed from DoPE’s position-aware features. With every successive iteration, the DoPE module feeds increasingly consistent positional information to would-be corresponding pairs, which in turn enhances the resulting point-to-point correspondence predictions used to estimate the rigid transformation. Within only a few iterations, the network converges to a desired equilibrium, where the positional embeddings given to matching pairs become essentially identical. We validate the effectiveness of DoPE through comprehensive experiments on various registration benchmarks, registration task settings, and prominent backbones, yielding unprecedented performance improvement across all combinations.},
  archive   = {C_ICCV},
  author    = {Taewon Min and Chonghyuk Song and Eunseok Kim and Inwook Shim},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00544},
  pages     = {5470-5478},
  title     = {Distinctiveness oriented positional equilibrium for point cloud registration},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CanvasVAE: Learning to generate vector graphic documents.
<em>ICCV</em>, 5461–5469. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vector graphic documents present visual elements in a resolution free, compact format and are often seen in creative applications. In this work, we attempt to learn a generative model of vector graphic documents. We define vector graphic documents by a multi-modal set of attributes associated to a canvas and a sequence of visual elements such as shapes, images, or texts, and train variational auto-encoders to learn the representation of the documents. We collect a new dataset of design templates from an online service that features complete document structure including occluded elements. In experiments, we show that our model, named CanvasVAE, constitutes a strong baseline for generative modeling of vector graphic documents.},
  archive   = {C_ICCV},
  author    = {Kota Yamaguchi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00543},
  pages     = {5461-5469},
  title     = {CanvasVAE: Learning to generate vector graphic documents},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DeePSD: Automatic deep skinning and pose space deformation
for 3D garment animation. <em>ICCV</em>, 5451–5460. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel solution to the garment animation problem through deep learning. Our contribution allows animating any template outfit with arbitrary topology and geometric complexity. Recent works develop models for garment edition, resizing and animation at the same time by leveraging the support body model (encoding garments as body homotopies). This leads to complex engineering solutions that suffer from scalability, applicability and compatibility. By limiting our scope to garment animation only, we are able to propose a simple model that can animate any outfit, independently of its topology, vertex order or connectivity. Our proposed architecture maps outfits to animated 3D models into the standard format for 3D animation (blend weights and blend shapes matrices), automatically providing of compatibility with any graphics engine. We also propose a methodology to complement supervised learning with an unsupervised physically based learning that implicitly solves collisions and enhances cloth quality.},
  archive   = {C_ICCV},
  author    = {Hugo Bertiche and Meysam Madadi and Emilio Tylson and Sergio Escalera},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00542},
  pages     = {5451-5460},
  title     = {DeePSD: Automatic deep skinning and pose space deformation for 3D garment animation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ImGHUM: Implicit generative models of 3D human shape and
articulated pose. <em>ICCV</em>, 5441–5450. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present imGHUM, the first holistic generative model of 3D human shape and articulated pose, represented as a signed distance function. In contrast to prior work, we model the full human body implicitly as a function zero-level-set and without the use of an explicit template mesh. We propose a novel network architecture and a learning paradigm, which make it possible to learn a detailed implicit generative model of human pose, shape, and semantics, on par with state-of-the-art mesh-based models. Our model features desired detail for human models, such as articulated pose including hand motion and facial expressions, a broad spectrum of shape variations, and can be queried at arbitrary resolutions and spatial locations. Additionally, our model has attached spatial semantics making it straightforward to establish correspondences between different shape instances, thus enabling applications that are difficult to tackle using classical implicit representations. In extensive experiments, we demonstrate the model accuracy and its applicability to current research problems.},
  archive   = {C_ICCV},
  author    = {Thiemo Alldieck and Hongyi Xu and Cristian Sminchisescu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00541},
  pages     = {5441-5450},
  title     = {ImGHUM: Implicit generative models of 3D human shape and articulated pose},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rotation averaging in a split second: A primal-dual method
and a closed-form for cycle graphs. <em>ICCV</em>, 5432–5440. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A cornerstone of geometric reconstruction, rotation averaging seeks the set of absolute rotations that optimally explains a set of measured relative orientations between them. In spite of being an integral part of bundle adjustment and structure-from-motion, averaging rotations is both a non-convex and high-dimensional optimization problem. In this paper, we address it from a maximum likelihood estimation standpoint and make a twofold contribution. Firstly, we set forth a novel initialization-free primal-dual method which we show empirically to converge to the global optimum. Further, we derive what is to our knowledge, the first optimal closed-form solution for rotation averaging in cycle graphs and contextualize this result within spectral graph theory. Our proposed methods achieve a significant gain both in precision and performance.},
  archive   = {C_ICCV},
  author    = {Gabriel Moreira and Manuel Marques and João Paulo Costeira},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00540},
  pages     = {5432-5440},
  title     = {Rotation averaging in a split second: A primal-dual method and a closed-form for cycle graphs},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Structure-from-sherds: Incremental 3D reassembly of axially
symmetric pots from unordered and mixed fragment collections.
<em>ICCV</em>, 5423–5431. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Re-assembling multiple pots accurately from numerous 3D scanned fragments remains a challenging task to this date. Previous methods extract all potential matching pairs of pot sherds and considers them simultaneously to search for an optimal global pot configuration. In this work, we empirically show such global approach greatly suffers from false positive matches between sherds inflicted by indistinc-tive sharp fracture surfaces in pot fragments. To mitigate this problem, we take inspirations from the field of structure-from-motion (SfM), where many pipelines have matured in reconstructing a 3D scene from multiple images. Motivated by the success of the incremental approach in robust SfM, we present an efficient reassembly method for axially symmetric pots based on iterative registration of one sherd at a time. Our method goes beyond replicating incremental SfM and addresses indistinguishable false matches by embracing beam search to explore multitudes of registration possibilities. Additionally, we utilize multiple roots in each step to allow simultaneous reassembly of multiple pots. The proposed approach shows above 80\% reassembly accuracy on a dataset of real 80 fragments mixed from 5 pots, pushing the state-of-the-art and paving the way towards the goal of large-scale pot reassembly. Our code and preprocessed data is available at https://github.com/SeongJong-Yoo/structure-from-sherds.},
  archive   = {C_ICCV},
  author    = {Je Hyeong Hong and Seong Jong Yoo and Muhammad Arshad Zeeshan and Young Min Kim and Jinwook Kim},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00539},
  pages     = {5423-5431},
  title     = {Structure-from-sherds: Incremental 3D reassembly of axially symmetric pots from unordered and mixed fragment collections},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ZFlow: Gated appearance flow-based virtual try-on with 3D
priors. <em>ICCV</em>, 5413–5422. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Image-based virtual try-on involves synthesising perceptually convincing images of a model wearing a particular garment and has garnered significant research interest due to its immense practical applicability. Recent methods involve a two stage process: i) warping of the garment to align with the model ii) texture fusion of the warped garment and target model to generate the try-on output. Issues arise due to the non-rigid nature of garments and the lack of geometric information about the model or the garment. It often results in improper rendering of granular details. We propose ZFlow, an end-to-end framework, which seeks to alleviate these concerns regarding geometric and textural integrity (such as pose, depth-ordering, skin and neckline reproduction) through a combination of gated aggregation of hierarchical flow estimates termed Gated Appearance Flow, and dense structural priors at various stage of the network. ZFlow achieves state-of-the-art results as observed qualitatively, and on quantitative benchmarks of image quality (PSNR, SSIM, and FID). The paper presents extensive comparisons with other existing solutions including a detailed user study and ablation studies to gauge the effect of each of our contributions on multiple datasets.},
  archive   = {C_ICCV},
  author    = {Ayush Chopra and Rishabh Jain and Mayur Hemani and Balaji Krishnamurthy},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00538},
  pages     = {5413-5422},
  title     = {ZFlow: Gated appearance flow-based virtual try-on with 3D priors},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Explain me the painting: Multi-topic knowledgeable art
description generation. <em>ICCV</em>, 5402–5412. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Have you ever looked at a painting and wondered what is the story behind it? This work presents a framework to bring art closer to people by generating comprehensive descriptions of fine-art paintings. Generating informative descriptions for artworks, however, is extremely challenging, as it requires to 1) describe multiple aspects of the image such as its style, content, or composition, and 2) provide background and contextual knowledge about the artist, their influences, or the historical period. To address these challenges, we introduce a multi-topic and knowledgeable art description framework, which modules the generated sentences according to three artistic topics and, additionally, enhances each description with external knowledge. The framework is validated through an exhaustive analysis, both quantitative and qualitative, as well as a comparative human evaluation, demonstrating outstanding results in terms of both topic diversity and information veracity.},
  archive   = {C_ICCV},
  author    = {Zechen Bai and Yuta Nakashima and Noa Garcia},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00537},
  pages     = {5402-5412},
  title     = {Explain me the painting: Multi-topic knowledgeable art description generation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Towards real-world prohibited item detection: A large-scale
x-ray benchmark. <em>ICCV</em>, 5392–5401. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automatic security inspection using computer vision technology is a challenging task in real-world scenarios due to various factors, including intra-class variance, class imbalance, and occlusion. Most of the previous methods rarely solve the cases that the prohibited items are deliberately hidden in messy objects due to the lack of large-scale datasets, restricted their applications in real-world scenarios. Towards real-world prohibited item detection, we collect a large-scale dataset, named as PIDray, which covers various cases in real-world scenarios for prohibited item detection, especially for deliberately hidden items. With an intensive amount of effort, our dataset contains 12 categories of prohibited items in 47, 677 X-ray images with high-quality annotated segmentation masks and bounding boxes. To the best of our knowledge, it is the largest prohibited items detection dataset to date. Meanwhile, we design the selective dense attention network (SDANet) to construct a strong baseline, which consists of the dense attention module and the dependency refinement module. The dense attention module formed by the spatial and channel-wise dense attentions, is designed to learn the discriminative features to boost the performance. The dependency refinement module is used to exploit the dependencies of multi-scale features. Extensive experiments conducted on the collected PIDray dataset demonstrate that the proposed method performs favorably against the state-of-the-art methods, especially for detecting the deliberately hidden items.},
  archive   = {C_ICCV},
  author    = {Boying Wang and Libo Zhang and Longyin Wen and Xianglong Liu and Yanjun Wu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00536},
  pages     = {5392-5401},
  title     = {Towards real-world prohibited item detection: A large-scale X-ray benchmark},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BEV-net: Assessing social distancing compliance by joint
people localization and geometric reasoning. <em>ICCV</em>, 5381–5391.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Social distancing, an essential public health measure to limit the spread of contagious diseases, has gained significant attention since the outbreak of the COVID-19 pandemic. In this work, the problem of visual social distancing compliance assessment in busy public areas, with wide field-of-view cameras, is considered. A dataset of crowd scenes with people annotations under a bird’s eye view (BEV) and ground truth for metric distances is introduced, and several measures for the evaluation of social distance detection systems are proposed. A multi-branch network, BEV-Net, is proposed to localize individuals in world coordinates and identify high-risk regions where social distancing is violated. BEV-Net combines detection of head and feet locations, camera pose estimation, a differentiable homography module to map image into BEV coordinates, and geometric reasoning to produce a BEV map of the people locations in the scene. Experiments on complex crowded scenes demonstrate the power of the approach and show superior performance over baselines derived from methods in the literature. Applications of interest for public health decision makers are finally discussed. Datasets, code and pretrained models are publicly available at GitHub 1 .},
  archive   = {C_ICCV},
  author    = {Zhirui Dai and Yuepeng Jiang and Yi Li and Bo Liu and Antoni B. Chan and Nuno Vasconcelos},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00535},
  pages     = {5381-5391},
  title     = {BEV-net: Assessing social distancing compliance by joint people localization and geometric reasoning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). SmartShadow: Artistic shadow drawing tool for line
drawings. <em>ICCV</em>, 5371–5380. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {SmartShadow is a deep learning application for digital painting artists to draw shadows on line drawings, with three proposed tools. (1) Shadow brush: artists can draw scribbles to coarsely indicate the areas inside or outside their wanted shadows, and the application will generate the shadows in real-time. (2) Shadow boundary brush: this brush can precisely control the boundary of any specific shadow. (3) Global shadow generator: this tool can estimate the global shadow direction from input brush scribbles, and then consistently propagate local shadows to the entire image. These three tools can not only speed up the shadow drawing process (by 3.1× as experiments validate), but also allow for the flexibility to achieve various shadow effects and facilitate richer artistic creations. To this end, we train Convolutional Neural Networks (CNNs) with a collected large-scale dataset of both real and synthesized data, and especially, we collect 1670 shadow samples drawn by real artists. Both qualitative analysis and user study show that our approach can generate high-quality shadows that are practically usable in the daily works of digital painting artists. We present 30 additional results and 15 visual comparisons in the supplementary materiel.},
  archive   = {C_ICCV},
  author    = {Lvmin Zhang and Jinyue Jiang and Yi Ji and Chunping Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00534},
  pages     = {5371-5380},
  title     = {SmartShadow: Artistic shadow drawing tool for line drawings},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast and efficient DNN deployment via deep gaussian transfer
learning. <em>ICCV</em>, 5360–5370. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep neural networks (DNNs) have been widely used recently while their hardware deployment optimizations are very time-consuming and the historical deployment knowledge is not utilized efficiently. In this paper, to accelerate the optimization process and find better deployment configurations, we propose a novel transfer learning method based on deep Gaussian processes (DGPs). Firstly, a deep Gaussian process (DGP) model is built on the historical data to learn empirical knowledge. Secondly, to transfer knowledge to a new task, a tuning set is sampled for the new task under the guidance of the DGP model. Then DGP is tuned according to the tuning set via maximum-a-posteriori (MAP) estimation to accommodate for the new task and finally used to guide the deployments of the task. The experiments show that our method achieves the best inference latencies of convolutions while accelerating the optimization process significantly, compared with previous arts.},
  archive   = {C_ICCV},
  author    = {Qi Sun and Chen Bai and Tinghuan Chen and Hao Geng and Xinyun Zhang and Yang Bai and Bei Yu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00533},
  pages     = {5360-5370},
  title     = {Fast and efficient DNN deployment via deep gaussian transfer learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cluster-promoting quantization with bit-drop for minimizing
network quantization loss. <em>ICCV</em>, 5350–5359. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Network quantization, which aims to reduce the bitlengths of the network weights and activations, has emerged for their deployments to resource-limited devices. Although recent studies have successfully discretized a full-precision network, they still incur large quantization errors after training, thus giving rise to a significant performance gap between a full-precision network and its quantized counterpart. In this work, we propose a novel quantization method for neural networks, Cluster-Promoting Quantization (CPQ) that finds the optimal quantization grids while naturally encouraging the underlying full-precision weights to gather around those quantization grids cohesively during training. This property of CPQ is thanks to our two main ingredients that enable differentiable quantization: i) the use of the categorical distribution designed by a specific probabilistic parametrization in the forward pass and ii) our proposed multi-class straight-through estimator (STE) in the backward pass. Since our second component, multi-class STE, is intrinsically biased, we additionally propose a new bit-drop technique, DropBits, that revises the standard dropout regularization to randomly drop bits instead of neurons. As a natural extension of DropBits, we further introduce the way of learning heterogeneous quantization levels to find proper bit-length for each layer by imposing an additional regularization on DropBits. We experimentally validate our method on various benchmark datasets and network architectures, and also support a new hypothesis for quantization: learning heterogeneous quantization levels outperforms the case using the same but fixed quantization levels from scratch.},
  archive   = {C_ICCV},
  author    = {Jung Hyun Lee and Jihun Yun and Sung Ju Hwang and Eunho Yang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00532},
  pages     = {5350-5359},
  title     = {Cluster-promoting quantization with bit-drop for minimizing network quantization loss},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sub-bit neural networks: Learning to compress and accelerate
binary neural networks. <em>ICCV</em>, 5340–5349. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the low-bit quantization field, training Binarized Neural Networks (BNNs) is the extreme solution to ease the deployment of deep models on resource-constrained devices, having the lowest storage cost and significantly cheaper bit-wise operations compared to 32-bit floating-point counterparts. In this paper, we introduce Sub-bit Neural Networks (SNNs), a new type of binary quantization design tailored to compress and accelerate BNNs. SNNs are inspired by an empirical observation, showing that binary kernels learnt at convolutional layers of a BNN model are likely to be distributed over kernel subsets. As a result, unlike existing methods that binarize weights one by one, SNNs are trained with a kernel-aware optimization framework, which exploits binary quantization in the fine-grained convolutional kernel space. Specifically, our method includes a random sampling step generating layer-specific subsets of the kernel space, and a refinement step learning to adjust these subsets of binary kernels via optimization. Experiments on visual recognition benchmarks and the hardware deployment on FPGA validate the great potentials of SNNs. For instance, on ImageNet, SNNs of ResNet-18/ResNet-34 with 0.56-bit weights achieve 3.13/3.33× runtime speedup and 1.8× compression over conventional BNNs with moderate drops in recognition accuracy. Promising results are also obtained when applying SNNs to binarize both weights and activations. Our code is available at https://github.com/yikaiw/SNN.},
  archive   = {C_ICCV},
  author    = {Yikai Wang and Yi Yang and Fuchun Sun and Anbang Yao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00531},
  pages     = {5340-5349},
  title     = {Sub-bit neural networks: Learning to compress and accelerate binary neural networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards mixed-precision quantization of neural networks via
constrained optimization. <em>ICCV</em>, 5330–5339. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Quantization is a widely used technique to compress and accelerate deep neural networks. However, conventional quantization methods use the same bit-width for all (or most of) the layers, which often suffer significant accuracy degradation in the ultra-low precision regime and ignore the fact that emergent hardware accelerators begin to support mixed-precision computation. Consequently, we present a novel and principled framework to solve the mixed-precision quantization problem in this paper. Briefly speaking, we first formulate the mixed-precision quantization as a discrete constrained optimization problem. Then, to make the optimization tractable, we approximate the objective function with second-order Taylor expansion and propose an efficient approach to compute its Hessian matrix. Finally, based on the above simplification, we show that the original problem can be reformulated as a MultipleChoice Knapsack Problem (MCKP) and propose a greedy search algorithm to solve it efficiently. Compared with existing mixed-precision quantization works, our method is derived in a principled way and much more computationally efficient. Moreover, extensive experiments conducted on the ImageNet dataset and various kinds of network architectures also demonstrate its superiority over existing uniform and mixed-precision quantization approaches.},
  archive   = {C_ICCV},
  author    = {Weihan Chen and Peisong Wang and Jian Cheng},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00530},
  pages     = {5330-5339},
  title     = {Towards mixed-precision quantization of neural networks via constrained optimization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Once quantization-aware training: High performance extremely
low-bit architecture search. <em>ICCV</em>, 5320–5329. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Quantization Neural Networks (QNN) have attracted a lot of attention due to their high efficiency. To enhance the quantization accuracy, prior works mainly focus on designing advanced quantization algorithms but still fail to achieve satisfactory results under the extremely low-bit case. In this work, we take an architecture perspective to investigate the potential of high-performance QNN. Therefore, we propose to combine Network Architecture Search methods with quantization to enjoy the merits of the two sides. However, a naive combination inevitably faces unacceptable time consumption or unstable training problem. To alleviate these problems, we first propose the joint training of architecture and quantization with a shared step size to acquire a large number of quantized models. Then a bit-inheritance scheme is introduced to transfer the quantized models to the lower bit, which further reduces the time cost and meanwhile improves the quantization accuracy. Equipped with this overall framework, dubbed as Once Quantization-Aware Training (OQAT), our searched model family, OQATNets, achieves a new state-of-the-art compared with various architectures under different bit-widths. In particular, OQAT-2bit-M achieves 61.6\% ImageNet Top-1 accuracy, outperforming 2-bit counterpart MobileNetV3 by a large margin of 9\% with 10\% less computation cost. A series of quantization-friendly architectures are identified easily and extensive analysis can be made to summarize the interaction between quantization and neural architectures. Codes and models are released at https://github.com/LaVieEnRoseSMZ/OQA},
  archive   = {C_ICCV},
  author    = {Mingzhu Shen and Feng Liang and Ruihao Gong and Yuhang Li and Chuming Li and Chen Lin and Fengwei Yu and Junjie Yan and Wanli Ouyang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00529},
  pages     = {5320-5329},
  title     = {Once quantization-aware training: High performance extremely low-bit architecture search},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Dynamic dual gating neural networks. <em>ICCV</em>,
5310–5319. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In dynamic neural networks that adapt computations to different inputs, gating-based methods have demonstrated notable generality and applicability in trading-off the model complexity and accuracy. However, existing works only explore the redundancy from a single point of the network, limiting the performance. In this paper, we propose dual gating, a new dynamic computing method, to reduce the model complexity at run-time. For each convolutional block, dual gating identifies the informative features along two separate dimensions, spatial and channel. Specifically, the spatial gating module estimates which areas are essential, and the channel gating module predicts the salient channels that contribute more to the results. Then the computation of both unimportant regions and irrelevant channels can be skipped dynamically during inference. Extensive experiments on a variety of datasets demonstrate that our method can achieve higher accuracy under similar computing budgets compared with other dynamic execution methods. In particular, dynamic dual gating can provide 59.7\% saving in computing of ResNet50 with 76.41\% top-1 accuracy on ImageNet, which has advanced the state-of-the-art. Codes are available at https://github.com/lfr-0531/DGNet.},
  archive   = {C_ICCV},
  author    = {Fanrong Li and Gang Li and Xiangyu He and Jian Cheng},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00528},
  pages     = {5310-5319},
  title     = {Dynamic dual gating neural networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving generalization of batch whitening by convolutional
unit optimization. <em>ICCV</em>, 5301–5309. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Batch Whitening is a technique that accelerates and stabilizes training by transforming input features to have a zero mean (Centering) and a unit variance (Scaling), and by removing linear correlation between channels (Decorrelation). In commonly used structures, which are empirically optimized with Batch Normalization, the normalization layer appears between convolution and activation function. Following Batch Whitening studies have employed the same structure without further analysis; even Batch Whitening was analyzed on the premise that the input of a linear layer is whitened. To bridge the gap, we propose a new Convolutional Unit that in line with the theory, and our method generally improves the performance of Batch Whitening. Moreover, we show the inefficacy of the original Convolutional Unit by investigating rank and correlation of features. As our method is employable off-the-shelf whitening modules, we use Iterative Normalization (IterNorm), the state-of-the-art whitening module, and obtain significantly improved performance on five image classification datasets: CIFAR-10, CIFAR-100, CUB-200-2011, Stanford Dogs, and ImageNet. Notably, we verify that our method improves stability and performance of whitening when using large learning rate, group size, and iteration number. Code is available at https://github.com/YooshinCho/pytorch_ConvUnitOptimization.},
  archive   = {C_ICCV},
  author    = {Yooshin Cho and Hanbyel Cho and Youngsoo Kim and Junmo Kim},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00527},
  pages     = {5301-5309},
  title     = {Improving generalization of batch whitening by convolutional unit optimization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Channel-wise knowledge distillation for dense prediction.
<em>ICCV</em>, 5291–5300. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Knowledge distillation (KD) has been proven a simple and effective tool for training compact dense prediction models. Lightweight student networks are trained by extra supervision transferred from large teacher networks. Most previous KD variants for dense prediction tasks align the activation maps from the student and teacher network in the spatial domain, typically by normalizing the activation values on each spatial location and minimizing point-wise and/or pair-wise discrepancy. Different from the previous methods, here we propose to normalize the activation map of each channel to obtain a soft probability map. By simply minimizing the Kullback–Leibler (KL) divergence between the channel-wise probability map of the two networks, the distillation process pays more attention to the most salient regions of each channel, which are valuable for dense prediction tasks.We conduct experiments on a few dense prediction tasks, including semantic segmentation and object detection. Experiments demonstrate that our proposed method outperforms state-of-the-art distillation methods considerably, and can require less computational cost during training. In particular, we improve the RetinaNet detector (ResNet50 backbone) by 3.4\% in mAP on the COCO dataset, and PSPNet (ResNet18 backbone) by 5.81\% in mIoU on the Cityscapes dataset. Code is available at: https://git.io/Distiller},
  archive   = {C_ICCV},
  author    = {Changyong Shu and Yifan Liu and Jianfei Gao and Zheng Yan and Chunhua Shen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00526},
  pages     = {5291-5300},
  title     = {Channel-wise knowledge distillation for dense prediction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Meta-aggregator: Learning to aggregate for 1-bit graph
neural networks. <em>ICCV</em>, 5281–5290. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we study a novel meta aggregation scheme towards binarizing graph neural networks (GNNs). We begin by developing a vanilla 1-bit GNN framework that binarizes both the GNN parameters and the graph features. Despite the lightweight architecture, we observed that this vanilla framework suffered from insufficient discriminative power in distinguishing graph topologies, leading to a dramatic drop in performance. This discovery motivates us to devise meta aggregators to improve the expressive power of vanilla binarized GNNs, of which the aggregation schemes can be adaptively changed in a learnable manner based on the binarized features. Towards this end, we propose two dedicated forms of meta neighborhood aggregators, an exclusive meta aggregator termed as Greedy Gumbel Neighborhood Aggregator (GNA), and a diffused meta aggregator termed as Adaptable Hybrid Neighborhood Aggregator (ANA). GNA learns to exclusively pick one single optimal aggregator from a pool of candidates, while ANA learns a hybrid aggregation behavior to simultaneously retain the benefits of several individual aggregators. Furthermore, the proposed meta aggregators may readily serve as a generic plugin module into existing full-precision GNNs. Experiments across various domains demonstrate that the proposed method yields results superior to the state of the art.},
  archive   = {C_ICCV},
  author    = {Yongcheng Jing and Yiding Yang and Xinchao Wang and Mingli Song and Dacheng Tao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00525},
  pages     = {5281-5290},
  title     = {Meta-aggregator: Learning to aggregate for 1-bit graph neural networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Generalizable mixed-precision quantization via attribution
rank preservation. <em>ICCV</em>, 5271–5280. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a generalizable mixed-precision quantization (GMPQ) method for efficient inference. Conventional methods require the consistency of datasets for bitwidth search and model deployment to guarantee the policy optimality, leading to heavy search cost on challenging largescale datasets in realistic applications. On the contrary, our GMPQ searches the mixed-quantization policy that can be generalized to largescale datasets with only a small amount of data, so that the search cost is significantly reduced without performance degradation. Specifically, we observe that locating network attribution correctly is general ability for accurate visual analysis across different data distribution. Therefore, despite of pursuing higher model accuracy and complexity, we preserve attribution rank consistency between the quantized models and their full-precision counterparts via efficient capacity-aware attribution imitation for generalizable mixed-precision quantization strategy search. Extensive experiments show that our method obtains competitive accuracy-complexity trade-off compared with the state-of-the-art mixed-precision networks in significantly reduced search cost. The code is available at https://github.com/ZiweiWangTHU/GMPQ.git.},
  archive   = {C_ICCV},
  author    = {Ziwei Wang and Han Xiao and Jiwen Lu and Jie Zhou},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00524},
  pages     = {5271-5280},
  title     = {Generalizable mixed-precision quantization via attribution rank preservation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving neural network efficiency via post-training
quantization with adaptive floating-point. <em>ICCV</em>, 5261–5270. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Model quantization has emerged as a mandatory technique for efficient inference with advanced Deep Neural Networks (DNN) by representing model parameters with fewer bits. Nevertheless, prior model quantization either suffers from the inefficient data encoding method thus leading to noncompetitive model compression rate, or requires time-consuming quantization aware training process. In this work, we propose a novel Adaptive Floating-Point (AFP) as a variant of standard IEEE-754 floating-point format, with flexible configuration of exponent and mantissa segments. Leveraging the AFP for model quantization (i.e., encoding the parameter) could significantly enhance the model compression rate without accuracy degradation and model re-training. We also want to highlight that our proposed AFP could effectively eliminate the computationally intensive de-quantization step existing in the dynamic quantization technique adopted by the famous machine learning frameworks (e.g., pytorch, tensorRT, etc.). Moreover, we develop a framework to automatically optimize and choose the adequate AFP configuration for each layer, thus maximizing the compression efficacy. Our experiments indicate that AFP-encoded ResNet-50/MobileNet-v2 only has ∼0.04/0.6\% accuracy degradation w.r.t its full-precision counterpart. It outperforms the state-of-the-art works by 1.1\% in accuracy using the same bit-width while reducing the energy consumption by 11.2×, which is quite impressive for inference. Code is released at: https://github.com/MXHX7199/ICCV_2021_AFP},
  archive   = {C_ICCV},
  author    = {Fangxin Liu and Wenbo Zhao and Zhezhi He and Yanzhi Wang and Zongwu Wang and Changzhi Dai and Xiaoyao Liang and Li Jiang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00523},
  pages     = {5261-5270},
  title     = {Improving neural network efficiency via post-training quantization with adaptive floating-point},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distance-aware quantization. <em>ICCV</em>, 5251–5260. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the problem of network quantization, that is, reducing bit-widths of weights and/or activations to lighten network architectures. Quantization methods use a rounding function to map full-precision values to the nearest quantized ones, but this operation is not differentiable. There are mainly two approaches to training quantized networks with gradient-based optimizers. First, a straight-through estimator (STE) replaces the zero derivative of the rounding with that of an identity function, which causes a gradient mismatch problem. Second, soft quantizers approximate the rounding with continuous functions at training time, and exploit the rounding for quantization at test time. This alleviates the gradient mismatch, but causes a quantizer gap problem. We alleviate both problems in a unified framework. To this end, we introduce a novel quantizer, dubbed a distance-aware quantizer (DAQ), that mainly consists of a distance-aware soft rounding (DASR) and a temperature controller. To alleviate the gradient mismatch problem, DASR approximates the discrete rounding with the kernel soft argmax, which is based on our insight that the quantization can be formulated as a distance-based assignment problem between full-precision values and quantized ones. The controller adjusts the temperature parameter in DASR adaptively according to the input, addressing the quantizer gap problem. Experimental results on standard benchmarks show that DAQ outperforms the state of the art significantly for various bit-widths without bells and whistles.},
  archive   = {C_ICCV},
  author    = {Dohyung Kim and Junghyup Lee and Bumsub Ham},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00522},
  pages     = {5251-5260},
  title     = {Distance-aware quantization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving low-precision network quantization via bin
regularization. <em>ICCV</em>, 5241–5250. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Model quantization is an important mechanism for energy-efficient deployment of deep neural networks on resource-constrained devices by reducing the bit precision of weights and activations. However, it remains challenging to maintain high accuracy as bit precision decreases, especially for low-precision networks (e.g., 2-bit MobileNetV2). Existing methods have been explored to address this problem by minimizing the quantization error or mimicking the data distribution of full-precision networks. In this work, we propose a novel weight regularization algorithm for improving low-precision network quantization. Instead of constraining the overall data distribution, we separably optimize all elements in each quantization bin to be as close to the target quantized value as possible. Such bin regularization (BR) mechanism encourages the weight distribution of each quantization bin to be sharp and approximate to a Dirac delta distribution ideally. Experiments demonstrate that our method achieves consistent improvements over the state-of-the-art quantization-aware training methods for different low-precision networks. Particularly, our bin regularization improves LSQ for 2-bit MobileNetV2 and MobileNetV3-Small by 3.9\% and 4.9\% top-1 accuracy on ImageNet, respectively.},
  archive   = {C_ICCV},
  author    = {Tiantian Han and Dong Li and Ji Liu and Lu Tian and Yi Shan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00521},
  pages     = {5241-5250},
  title     = {Improving low-precision network quantization via bin regularization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RMSMP: A novel deep neural network quantization framework
with row-wise mixed schemes and multiple precisions. <em>ICCV</em>,
5231–5240. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work proposes a novel Deep Neural Network (DNN) quantization framework, namely RMSMP, with a Row-wise Mixed-Scheme and Multi-Precision approach. Specifically, this is the first effort to assign mixed quantization schemes and multiple precisions within layers – among rows of the DNN weight matrix, for simplified operations in hardware inference, while preserving accuracy. Furthermore, this paper makes a different observation from the prior work that the quantization error does not necessarily exhibit the layer-wise sensitivity, and actually can be mitigated as long as a certain portion of the weights in every layer are in higher precisions. This observation enables layer-wise uniformality in the hardware implementation towards guaranteed inference acceleration, while still enjoying row-wise flexibility of mixed schemes and multiple precisions to boost accuracy. The candidates of schemes and precisions are derived practically and effectively with a highly hardware-informative strategy to reduce the problem search space.With the offline determined ratio of different quantization schemes and precisions for all the layers, the RMSMP quantization algorithm uses Hessian and variance based method to effectively assign schemes and precisions for each row. The proposed RMSMP is tested for the image classification and natural language processing (BERT) applications, and achieves the best accuracy performance among state-of-the-arts under the same equivalent precisions. The RMSMP is implemented on FPGA devices, achieving 3.65× speedup in the end-to-end inference time for ResNet-18 on ImageNet, comparing with the 4-bit Fixed-point baseline.},
  archive   = {C_ICCV},
  author    = {Sung-En Chang and Yanyu Li and Mengshu Sun and Weiwen Jiang and Sijia Liu and Yanzhi Wang and Xue Lin},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00520},
  pages     = {5231-5240},
  title     = {RMSMP: A novel deep neural network quantization framework with row-wise mixed schemes and multiple precisions},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GDP: Stabilized neural network pruning via gates with
differentiable polarization. <em>ICCV</em>, 5219–5230. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Model compression techniques are recently gaining explosive attention for obtaining efficient AI models for various real time applications. Channel pruning is one important compression strategy, and widely used in slimming various DNNs. Previous gate-based or importance-based pruning methods aim to remove channels whose &quot;importance&quot; are smallest. However, it remains unclear what criteria the channel importance should be measured on, leading to various channel selection heuristics. Some other sampling-based pruning methods deploy sampling strategy to train sub-nets, which often causes the training instability and the compressed model’s degraded performance. In view of the research gaps, we present a new module named Gates with Differentiable Polarization (GDP), inspired by principled optimization ideas. GDP can be plugged before convolutional layers without bells and whistles, to control the on-and-off of each channel or whole layer block. During the training process, the polarization effect will drive a subset of gates to smoothly decrease to exact zero, while other gates gradually stay away from zero by a large margin. When training terminates, those zero-gated channels can be painlessly removed, while other non-zero gates can be absorbed into the succeeding convolution kernel, causing completely no interruption to training nor damage to the trained model. Experiments conducted over CIFAR-10 and ImageNet datasets show that the proposed GDP algorithm achieves the state-of-the-art performance on various benchmark DNNs at a broad range of pruning ratios. We also apply GDP to DeepLabV3Plus-ResNet50 on the challenging Pascal VOC segmentation task, whose test performance sees no drop (even slightly improved) with over 60\% FLOPs saving.},
  archive   = {C_ICCV},
  author    = {Yi Guo and Huan Yuan and Jianchao Tan and Zhangyang Wang and Sen Yang and Ji Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00519},
  pages     = {5219-5230},
  title     = {GDP: Stabilized neural network pruning via gates with differentiable polarization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards memory-efficient neural networks via multi-level in
situ generation. <em>ICCV</em>, 5209–5218. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep neural networks (DNN) have shown superior performance in a variety of tasks. As they rapidly evolve, their escalating computation and memory demands make it challenging to deploy them on resource-constrained edge devices. Though extensive efficient accelerator designs, from traditional electronics to emerging photonics, have been successfully demonstrated, they are still bottlenecked by expensive memory accesses due to tremendous gaps between the bandwidth/power/latency of electrical memory and computing cores. Previous solutions fail to fully-leverage the ultra-fast computational speed of emerging DNN accelerators to break through the critical memory bound. In this work, we propose a general and unified framework to trade expensive memory transactions with ultra-fast on-chip computations, directly translating to performance improvement. We are the first to jointly explore the intrinsic correlations and bit-level redundancy within DNN kernels and propose a multi-level in situ generation mechanism with mixed-precision bases to achieve on-the-fly recovery of high-resolution parameters with minimum hardware overhead. Extensive experiments demonstrate that our proposed joint method can boost the memory efficiency by 10-20× with comparable accuracy over four state-of-the-art designs, when benchmarked on ResNet-18/DenseNet-121/MobileNetV2/V3 with various tasks.},
  archive   = {C_ICCV},
  author    = {Jiaqi Gu and Hanqing Zhu and Chenghao Feng and Mingjie Liu and Zixuan Jiang and Ray T. Chen and David Z. Pan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00518},
  pages     = {5209-5218},
  title     = {Towards memory-efficient neural networks via multi-level in situ generation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FATNN: Fast and accurate ternary neural networks.
<em>ICCV</em>, 5199–5208. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ternary Neural Networks (TNNs) have received much attention due to being potentially orders of magnitude faster in inference, as well as more power efficient, than full-precision counterparts. However, 2 bits are required to encode the ternary representation with only 3 quantization levels leveraged. As a result, conventional TNNs have similar memory consumption and speed compared with the standard 2-bit models, but have worse representational capability. Moreover, there is still a significant gap in accuracy between TNNs and full-precision networks, hampering their deployment to real applications. To tackle these two challenges, in this work, we first show that, under some mild constraints, computational complexity of the ternary inner product can be reduced by 2×. Second, to mitigate the performance gap, we elaborately design an implementation-dependent ternary quantization algorithm. The proposed framework is termed Fast and Accurate Ternary Neural Networks (FATNN). Experiments on image classification demonstrate that our FATNN surpasses the state-of-the-arts by a significant margin in accuracy. More importantly, speedup evaluation compared with various precision is analyzed on several platforms, which serves as a strong benchmark for further research. Source code and models are available at: https://github.com/MonashAI/QTool},
  archive   = {C_ICCV},
  author    = {Peng Chen and Bohan Zhuang and Chunhua Shen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00517},
  pages     = {5199-5208},
  title     = {FATNN: Fast and accurate ternary neural networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HIRE-SNN: Harnessing the inherent robustness of
energy-efficient deep spiking neural networks by training with crafted
input noise. <em>ICCV</em>, 5189–5198. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Low-latency deep spiking neural networks (SNNs) have become a promising alternative to conventional artificial neural networks (ANNs) because of their potential for increased energy efficiency on event-driven neuromorphic hardware. Neural networks, including SNNs, however, are subject to various adversarial attacks and must be trained to remain resilient against such attacks for many applications. Nevertheless, due to prohibitively high training costs associated with SNNs, an analysis and optimization of deep SNNs under various adversarial attacks have been largely overlooked. In this paper, we first present a detailed analysis of the inherent robustness of low-latency SNNs against popular gradient-based attacks, namely fast gradient sign method (FGSM) and projected gradient descent (PGD). Motivated by this analysis, to harness the model’s robustness against these attacks we present an SNN training algorithm that uses crafted input noise and incurs no additional training time. To evaluate the merits of our algorithm, we conducted extensive experiments with variants of VGG and ResNet on both CIFAR-10 and CIFAR-100 dataset. Compared to standard trained direct-input SNNs, our trained models yield improved classification accuracy of up to 13.7\% and 10.1\% on FGSM and PGD attack generated images, respectively, with negligible loss in clean image accuracy. Our models also outperform inherently-robust SNNs trained on rate-coded inputs with improved or similar classification performance on attack-generated images while having up to 25× and ∼4.6× lower latency and computation energy, respectively. For reproducibility, we have open-sourced the code at github.com/ksouvik52/hiresnn2021.},
  archive   = {C_ICCV},
  author    = {Souvik Kundu and Massoud Pedram and Peter A. Beerel},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00516},
  pages     = {5189-5198},
  title     = {HIRE-SNN: Harnessing the inherent robustness of energy-efficient deep spiking neural networks by training with crafted input noise},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ReCU: Reviving the dead weights in binary neural networks.
<em>ICCV</em>, 5178–5188. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Binary neural networks (BNNs) have received increasing attention due to their superior reductions of computation and memory. Most existing works focus on either lessening the quantization error by minimizing the gap between the full-precision weights and their binarization or designing a gradient approximation to mitigate the gradient mismatch, while leaving the &quot;dead weights&quot; untouched. This leads to slow convergence when training BNNs. In this paper, for the first time, we explore the influence of &quot;dead weights&quot; which refer to a group of weights that are barely updated during the training of BNNs, and then introduce rectified clamp unit (ReCU) to revive the &quot;dead weights&quot; for updating. We prove that reviving the &quot;dead weights&quot; by ReCU can result in a smaller quantization error. Besides, we also take into account the information entropy of the weights, and then mathematically analyze why the weight standardization can benefit BNNs. We demonstrate the inherent contradiction between minimizing the quantization error and maximizing the information entropy, and then propose an adaptive exponential scheduler to identify the range of the &quot;dead weights&quot;. By considering the &quot;dead weights&quot;, our method offers not only faster BNN training, but also state-of-the-art performance on CIFAR-10 and ImageNet, compared with recent methods. Code can be available at https://github.com/z-hXu/ReCU.},
  archive   = {C_ICCV},
  author    = {Zihan Xu and Mingbao Lin and Jianzhuang Liu and Jie Chen and Ling Shao and Yue Gao and Yonghong Tian and Rongrong Ji},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00515},
  pages     = {5178-5188},
  title     = {ReCU: Reviving the dead weights in binary neural networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bit-mixer: Mixed-precision networks with runtime bit-width
selection. <em>ICCV</em>, 5168–5177. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mixed-precision networks allow for a variable bit-width quantization for every layer in the network. A major limitation of existing work is that the bit-width for each layer must be predefined during training time. This allows little flexibility if the characteristics of the device on which the network is deployed change during runtime. In this work, we propose Bit-Mixer, the very first method to train a meta-quantized network where during test time any layer can change its bit-width without affecting at all the overall network’s ability for highly accurate inference. To this end, we make 2 key contributions: (a) Transitional Batch-Norms, and (b) a 3-stage optimization process which is shown capable of training such a network. We show that our method can result in mixed precision networks that exhibit the desirable flexibility properties for on-device deployment without compromising accuracy. Code will be made available.},
  archive   = {C_ICCV},
  author    = {Adrian Bulat and Georgios Tzimiropoulos},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00514},
  pages     = {5168-5177},
  title     = {Bit-mixer: Mixed-precision networks with runtime bit-width selection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Unsupervised curriculum domain adaptation for no-reference
video quality assessment. <em>ICCV</em>, 5158–5167. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {During the last years, convolutional neural networks (C-NNs) have triumphed over video quality assessment (VQA) tasks. However, CNN-based approaches heavily rely on annotated data which are typically not available in VQA, leading to the difficulty of model generalization. Recent advances in domain adaptation technique makes it possible to adapt models trained on source data to unlabeled target data. However, due to the distortion diversity and content variation of the collected videos, the intrinsic subjectivity of VQA tasks hampers the adaptation performance. In this work, we propose a curriculum-style unsupervised domain adaptation to handle the cross-domain no-reference VQA problem. The proposed approach could be divided into two stages. In the first stage, we conduct an adaptation between source and target domains to predict the rating distribution for target samples, which can better reveal the subjective nature of VQA. From this adaptation, we split the data in target domain into confident and uncertain subdomains using the proposed uncertainty-based ranking function, through measuring their prediction confidences. In the second stage, by regarding samples in confident subdomain as the easy tasks in the curriculum, a fine-level adaptation is conducted between two subdomain-s to fine-tune the prediction model. Extensive experimental results on benchmark datasets highlight the superiority of the proposed method over the competing methods in both accuracy and speed. The source code is released at https://github.com/cpf0079/UCDA.},
  archive   = {C_ICCV},
  author    = {Pengfei Chen and Leida Li and Jinjian Wu and Weisheng Dong and Guangming Shi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00513},
  pages     = {5158-5167},
  title     = {Unsupervised curriculum domain adaptation for no-reference video quality assessment},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SACoD: Sensor algorithm co-design towards efficient
CNN-powered intelligent PhlatCam. <em>ICCV</em>, 5148–5157. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There has been a booming demand for integrating Convolutional Neural Networks (CNNs) powered functionalities into Internet-of-Thing (IoT) devices to enable ubiquitous intelligent &quot;IoT cameras&quot;. However, more extensive applications of such IoT systems are still limited by two challenges. First, some applications, especially medicine-and wearable-related ones, impose stringent requirements on the camera form factor. Second, powerful CNNs often require considerable storage and energy cost, whereas IoT devices often suffer from limited resources. PhlatCam, with its form factor potentially reduced by orders of magnitude, has emerged as a promising solution to the first aforementioned challenge, while the second one remains a bottleneck. Existing compression techniques, which can potentially tackle the second challenge, are far from realizing the full potential in storage and energy reduction, because they mostly focus on the CNN algorithm itself. To this end, this work proposes SACoD, a Sensor Algorithm Co-Design framework to develop more efficient CNN-powered PhlatCam. In particular, the mask coded in the Phlat-Cam sensor and the backend CNN model are jointly optimized in terms of both model parameters and architectures via differential neural architecture search. Extensive experiments including both simulation and physical measurement on manufactured masks show that the proposed SACoD framework achieves aggressive model compression and energy savings while maintaining or even boosting the task accuracy, when benchmarking over two state-of-the-art (SOTA) designs with six datasets across four different vision tasks including classification, segmentation, image translation, and face recognition. Our codes are available at: https://github.com/RICE-EIC/SACoD.},
  archive   = {C_ICCV},
  author    = {Yonggan Fu and Yang Zhang and Yue Wang and Zhihan Lu and Vivek Boominathan and Ashok Veeraraghavan and Yingyan Lin},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00512},
  pages     = {5148-5157},
  title     = {SACoD: Sensor algorithm co-design towards efficient CNN-powered intelligent PhlatCam},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BlockCopy: High-resolution video processing with
block-sparse feature propagation and online policies. <em>ICCV</em>,
5138–5147. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we propose BlockCopy, a scheme that accelerates pretrained frame-based CNNs to process video more efficiently, compared to standard frame-by-frame processing. To this end, a lightweight policy network determines important regions in an image, and operations are applied on selected regions only, using custom block-sparse convolutions. Features of non-selected regions are simply copied from the preceding frame, reducing the number of computations and latency. The execution policy is trained using reinforcement learning in an online fashion without requiring ground truth annotations. Our universal framework is demonstrated on dense prediction tasks such as pedestrian detection, instance segmentation and semantic segmentation, using both state of the art (Center and Scale Predictor, MGAN, SwiftNet) and standard baseline networks (Mask-RCNN, DeepLabV3+). BlockCopy achieves significant FLOPS savings and inference speedup with minimal impact on accuracy.},
  archive   = {C_ICCV},
  author    = {Thomas Verelst and Tinne Tuytelaars},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00511},
  pages     = {5138-5147},
  title     = {BlockCopy: High-resolution video processing with block-sparse feature propagation and online policies},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MUSIQ: Multi-scale image quality transformer. <em>ICCV</em>,
5128–5137. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Image quality assessment (IQA) is an important research topic for understanding and improving visual experience. The current state-of-the-art IQA methods are based on convolutional neural networks (CNNs). The performance of CNN-based models is often compromised by the fixed shape constraint in batch training. To accommodate this, the input images are usually resized and cropped to a fixed shape, causing image quality degradation. To address this, we design a multi-scale image quality Transformer (MUSIQ) to process native resolution images with varying sizes and aspect ratios. With a multi-scale image representation, our proposed method can capture image quality at different granularities. Furthermore, a novel hash-based 2D spatial embedding and a scale embedding is proposed to support the positional embedding in the multi-scale representation. Experimental results verify that our method can achieve state-of-the-art performance on multiple large scale IQA datasets such as PaQ-2-PiQ [41], SPAQ [11], and KonIQ-10k [16]. 1},
  archive   = {C_ICCV},
  author    = {Junjie Ke and Qifei Wang and Yilin Wang and Peyman Milanfar and Feng Yang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00510},
  pages     = {5128-5137},
  title     = {MUSIQ: Multi-scale image quality transformer},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spectral leakage and rethinking the kernel size in CNNs.
<em>ICCV</em>, 5118–5127. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Convolutional layers in CNNs implement linear filters which decompose the input into different frequency bands. However, most modern architectures neglect standard principles of filter design when optimizing their model choices regarding the size and shape of the convolutional kernel. In this work, we consider the well-known problem of spectral leakage caused by windowing artifacts in filtering operations in the context of CNNs. We show that the small size of CNN kernels make them susceptible to spectral leakage, which may induce performance-degrading artifacts. To address this issue, we propose the use of larger kernel sizes along with the Hamming window function to alleviate leakage in CNN architectures. We demonstrate improved classification accuracy on multiple benchmark datasets including Fashion-MNIST, CIFAR-10, CIFAR-100 and ImageNet with the simple use of a standard window function in convolutional layers. Finally, we show that CNNs employing the Hamming window display increased robustness against various adversarial attacks. Our code is available online 1 .},
  archive   = {C_ICCV},
  author    = {Nergis Tomen and Jan C. van Gemert},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00509},
  pages     = {5118-5127},
  title     = {Spectral leakage and rethinking the kernel size in CNNs},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Entropy maximization and meta classification for
out-of-distribution detection in semantic segmentation. <em>ICCV</em>,
5108–5117. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep neural networks (DNNs) for the semantic segmentation of images are usually trained to operate on a predefined closed set of object classes. This is in contrast to the &quot;open world&quot; setting where DNNs are envisioned to be deployed to. From a functional safety point of view, the ability to detect so-called &quot;out-of-distribution&quot; (OoD) samples, i.e., objects outside of a DNN’s semantic space, is crucial for many applications such as automated driving. A natural baseline approach to OoD detection is to threshold on the pixel-wise softmax entropy. We present a two-step procedure that significantly improves that approach. Firstly, we utilize samples from the COCO dataset as OoD proxy and introduce a second training objective to maximize the softmax entropy on these samples. Starting from pretrained semantic segmentation networks we re-train a number of DNNs on different in-distribution datasets and consistently observe improved OoD detection performance when evaluating on completely disjoint OoD datasets. Secondly, we perform a transparent post-processing step to discard false positive OoD samples by so-called &quot;meta classification.&quot; To this end, we apply linear models to a set of hand-crafted metrics derived from the DNN’s softmax probabilities. In our experiments we consistently observe a clear additional gain in OoD detection performance, cutting down the number of detection errors by 52\% when comparing the best baseline with our results. We achieve this improvement sacrificing only marginally in original segmentation performance. Therefore, our method contributes to safer DNNs with more reliable overall system performance.},
  archive   = {C_ICCV},
  author    = {Robin Chan and Matthias Rottmann and Hanno Gottschalk},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00508},
  pages     = {5108-5117},
  title     = {Entropy maximization and meta classification for out-of-distribution detection in semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pixel difference networks for efficient edge detection.
<em>ICCV</em>, 5097–5107. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, deep Convolutional Neural Networks (CNNs) can achieve human-level performance in edge detection with the rich and abstract edge representation capacities. However, the high performance of CNN based edge detection is achieved with a large pretrained CNN backbone, which is memory and energy consuming. In addition, it is surprising that the previous wisdom from the traditional edge detectors, such as Canny, Sobel, and LBP are rarely investigated in the rapid-developing deep learning era. To address these issues, we propose a simple, lightweight yet effective architecture named Pixel Difference Network (PiDiNet) for efficient edge detection. PiDiNet adopts novel pixel difference convolutions that integrate the traditional edge detection operators into the popular convolutional operations in modern CNNs for enhanced performance on the task, which enjoys the best of both worlds. Extensive experiments on BSDS500, NYUD, and Multicue are provided to demonstrate its effectiveness, and its high training and inference efficiency. Surprisingly, when training from scratch with only the BSDS500 and VOC datasets, PiDiNet can surpass the recorded result of human perception (0.807 vs. 0.803 in ODS F-measure) on the BSDS500 dataset with 100 FPS and less than 1M parameters. A faster version of PiDiNet with less than 0.1M parameters can still achieve comparable performance among state of the arts with 200 FPS. Results on the NYUD and Multicue datasets show similar observations. The codes are available at https://github.com/zhuoinoulu/pidinet.},
  archive   = {C_ICCV},
  author    = {Zhuo Su and Wenzhe Liu and Zitong Yu and Dewen Hu and Qing Liao and Qi Tian and Matti Pietikäinen and Li Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00507},
  pages     = {5097-5107},
  title     = {Pixel difference networks for efficient edge detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning multiple pixelwise tasks based on loss scale
balancing. <em>ICCV</em>, 5087–5096. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel loss weighting algorithm, called loss scale balancing (LSB), for multi-task learning (MTL) of pixelwise vision tasks. An MTL model is trained to estimate multiple pixelwise predictions using an overall loss, which is a linear combination of individual task losses. The proposed algorithm dynamically adjusts the linear weights to learn all tasks effectively. Instead of controlling the trend of each loss value directly, we balance the loss scale — the product of the loss value and its weight — periodically. In addition, by evaluating the difficulty of each task based on the previous loss record, the proposed algorithm focuses more on difficult tasks during training. Experimental results show that the proposed algorithm outperforms conventional weighting algorithms for MTL of various pixelwise tasks. Codes are available at https://github.com/jaehanlee-mcl/LSB-MTL.},
  archive   = {C_ICCV},
  author    = {Jae-Han Lee and Chul Lee and Chang-Su Kim},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00506},
  pages     = {5087-5096},
  title     = {Learning multiple pixelwise tasks based on loss scale balancing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NASOA: Towards faster task-oriented online fine-tuning with
a zoo of models. <em>ICCV</em>, 5077–5086. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fine-tuning from pre-trained ImageNet models has been a simple, effective, and popular approach for various computer vision tasks. The common practice of fine-tuning is to adopt a default hyperparameter setting with a fixed pre-trained model, while both of them are not optimized for specific tasks and time constraints. Moreover, in cloud computing or GPU clusters where the tasks arrive sequentially in a stream, faster online fine-tuning is a more desired and realistic strategy for saving money, energy consumption, and CO2 emission. In this paper, we propose a joint Neural Architecture Search and Online Adaption framework named NASOA towards a faster task-oriented fine-tuning upon the request of users. Specifically, NASOA first adopts an offline NAS to identify a group of training-efficient networks to form a pretrained model zoo. We propose a novel joint block and macro level search space to enable a flexible and effi-cient search. Then, by estimating fine-tuning performance via an adaptive model by accumulating experience from the past tasks, an online schedule generator is proposed to pick up the most suitable model and generate a personalized training regime with respect to each desired task in a one-shot fashion. The resulting model zoo 1 is more training efficient than SOTA models, e.g. 6x faster than RegNetY-16GF, and 1.7x faster than EfficientNetB3. Experiments on multiple datasets also show that NASOA achieves much better fine-tuning results, i.e. improving around 2.1\% accuracy than the best performance in RegNet series under various constraints and tasks; 40x faster compared to the BOHB.},
  archive   = {C_ICCV},
  author    = {Hang Xu and Ning Kang and Gengwei Zhang and Chuanlong Xie and Xiaodan Liang and Zhenguo Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00505},
  pages     = {5077-5086},
  title     = {NASOA: Towards faster task-oriented online fine-tuning with a zoo of models},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rethinking deep image prior for denoising. <em>ICCV</em>,
5067–5076. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep image prior (DIP) serves as a good inductive bias for diverse inverse problems. Among them, denoising is known to be particularly challenging for the DIP due to noise fitting with the requirement of an early stopping. To address the issue, we first analyze the DIP by the notion of effective degrees of freedom (DF) to monitor the optimization progress and propose a principled stopping criterion before fitting to noise without access of a paired ground truth image for Gaussian noise. We also propose the ‘stochastic temporal ensemble (STE)’ method for incorporating techniques to further improve DIP’s performance for denoising. We additionally extend our method to Poisson noise. Our empirical validations show that given a single noisy image, our method denoises the image while preserving rich textual details. Further, our approach outperforms prior arts in LPIPS by large margins with comparable PSNR and SSIM on seven different datasets.},
  archive   = {C_ICCV},
  author    = {Yeonsik Jo and Se Young Chun and Jonghyun Choi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00504},
  pages     = {5067-5076},
  title     = {Rethinking deep image prior for denoising},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BlockPlanner: City block generation with vectorized graph
representation. <em>ICCV</em>, 5057–5066. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {City modeling is the foundation for computational urban planning, navigation, and entertainment. In this work, we present the first generative model of city blocks named BlockPlanner, and showcase its ability to synthesize valid city blocks with varying land lots configurations. We propose a novel vectorized city block representation utilizing a ring topology and a two-tier graph to capture the global and local structures of a city block. Each land lot is abstracted into a vector representation covering both its 3D geometry and land use semantics. Such vectorized representation enables us to deploy a lightweight network to capture the underlying distribution of land lots configurations in a city block. To enforce intrinsic spatial constraints of a valid city block, a set of effective loss functions are imposed to shape rational results. We contribute a pilot city block dataset to demonstrate the effectiveness and efficiency of our representation and framework over the state-of-the-art. Notably, our BlockPlanner is also able to edit and manipulate city blocks, enabling several useful applications, e.g., topology refinement and footprint generation.},
  archive   = {C_ICCV},
  author    = {Linning Xu and Yuanbo Xiangli and Anyi Rao and Nanxuan Zhao and Bo Dai and Ziwei Liu and Dahua Lin},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00503},
  pages     = {5057-5066},
  title     = {BlockPlanner: City block generation with vectorized graph representation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive curriculum learning. <em>ICCV</em>, 5047–5056. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inspired by the human learning principle that learning easier concepts first and then gradually paying more attention to harder ones, curriculum learning uses the nonuniform sampling of mini-batches according to the order of examples’ difficulty. Just as a teacher adjusts the curriculum according to the learning progress of each student, a proper curriculum should be adapted to the current state of the model. Therefore, in contrast to recent works using a fixed curriculum, we devise a new curriculum learning method, Adaptive Curriculum Learning (Adaptive CL), adapting the difficulty of examples to the current state of the model. Specifically, we make use of the loss of the current model to adjust the difficulty score while retaining previous useful learned knowledge by KL divergence. Moreover, under a non-linear model and binary classification, we theoretically prove that the expected convergence rate of curriculum learning monotonically decreases with respect to the loss of a point regarding the optimal hypothesis, and monotonically increases with respect to the loss of a point regarding the current hypothesis. The analyses indicate that Adaptive CL could improve the convergence properties during the early stages of learning. Extensive experimental results demonstrate the superiority of the proposed approach over existing competitive curriculum learning methods.},
  archive   = {C_ICCV},
  author    = {Yajing Kong and Liu Liu and Jun Wang and Dacheng Tao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00502},
  pages     = {5047-5056},
  title     = {Adaptive curriculum learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Student customized knowledge distillation: Bridging the gap
between student and teacher. <em>ICCV</em>, 5037–5046. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Knowledge distillation (KD) transfers the dark knowledge from cumbersome networks (teacher) to lightweight (student) networks and expects the student to achieve more promising performance than training without the teacher’s knowledge. However, a counter-intuitive argument is that better teachers do not make better students due to the capacity mismatch. To this end, we present a novel adaptive knowledge distillation method to complement traditional approaches. The proposed method, named as Student Customized Knowledge Distillation (SCKD), examines the capacity mismatch between teacher and student from the perspective of gradient similarity. We formulate the knowledge distillation as a multi-task learning problem so that the teacher transfers knowledge to the student only if the student can benefit from learning such knowledge. We validate our methods on multiple datasets with various teacher-student configurations on image classification, object detection, and semantic segmentation.},
  archive   = {C_ICCV},
  author    = {Yichen Zhu and Yi Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00501},
  pages     = {5037-5046},
  title     = {Student customized knowledge distillation: Bridging the gap between student and teacher},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-born wiring for neural trees. <em>ICCV</em>, 5027–5036.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Neural trees aim at integrating deep neural networks and decision trees so as to bring the best of the two worlds, including representation learning from the former and faster inference from the latter. In this paper, we introduce a novel approach, termed as Self-born Wiring (SeBoW), to learn neural trees from a mother deep neural network. In contrast to prior neural-tree approaches that either adopt a pre-defined structure or grow hierarchical layers in a progressive manner, task-adaptive neural trees in SeBoW evolve from a deep neural network through a construction-by-destruction process, enabling a global-level parameter optimization that further yields favorable results. Specifically, given a designated network configuration like VGG, SeBoW disconnects all the layers and derives isolated filter groups, based on which a global-level wiring process is conducted to attach a subset of filter groups, eventually bearing a lightweight neural tree. Extensive experiments demonstrate that, with a lower computational cost, SeBoW outperforms all prior neural trees by a significant margin and even achieves results on par with predominant non-tree networks like ResNets. Moreover, SeBoW proves its scalability to large-scale datasets like ImageNet, which has been barely explored by prior tree networks.},
  archive   = {C_ICCV},
  author    = {Ying Chen and Feng Mao and Jie Song and Xinchao Wang and Huiqiong Wang and Mingli Song},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00500},
  pages     = {5027-5036},
  title     = {Self-born wiring for neural trees},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Polarimetric helmholtz stereopsis. <em>ICCV</em>, 5017–5026.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Helmholtz stereopsis (HS) exploits the reciprocity principle of light propagation (i.e., the Helmholtz reciprocity) for 3D reconstruction of surfaces with arbitrary reflectance. In this paper, we present the polarimetric Helmholtz stereopsis (polar-HS), which extends the classical HS by considering the polarization state of light in the reciprocal paths. With the additional phase information from polarization, polar-HS requires only one reciprocal image pair. We formulate new reciprocity and diffuse/specular polarimetric constraints to recover surface depths and normals using an optimization framework. Using a hardware prototype, we show that our approach produces high-quality 3D reconstruction for different types of surfaces, ranging from diffuse to highly specular.},
  archive   = {C_ICCV},
  author    = {Yuqi Ding and Yu Ji and Mingyuan Zhou and Sing Bing Kang and Jinwei Ye},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00499},
  pages     = {5017-5026},
  title     = {Polarimetric helmholtz stereopsis},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DC-ShadowNet: Single-image hard and soft shadow removal
using unsupervised domain-classifier guided network. <em>ICCV</em>,
5007–5016. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Shadow removal from a single image is generally still an open problem. Most existing learning-based methods use supervised learning and require a large number of paired images (shadow and corresponding non-shadow images) for training. A recent unsupervised method, Mask-ShadowGAN [13], addresses this limitation. However, it requires a binary mask to represent shadow regions, making it inapplicable to soft shadows. To address the problem, in this paper, we propose an unsupervised domain-classifier guided shadow removal network, DC-ShadowNet. Specifically, we propose to integrate a shadow/shadow-free domain classifier into a generator and its discriminator, enabling them to focus on shadow regions. To train our network, we introduce novel losses based on physics-based shadow-free chromaticity, shadow-robust perceptual features, and boundary smoothness. Moreover, we show that our unsupervised network can be used for test-time training that further improves the results. Our experiments show that all these novel components allow our method to handle soft shadows, and also to perform better on hard shadows both quantitatively and qualitatively than the existing state-of-the-art shadow removal methods.},
  archive   = {C_ICCV},
  author    = {Yeying Jin and Aashish Sharma and Robby T. Tan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00498},
  pages     = {5007-5016},
  title     = {DC-ShadowNet: Single-image hard and soft shadow removal using unsupervised domain-classifier guided network},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Location-aware single image reflection removal.
<em>ICCV</em>, 4997–5006. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a novel location-aware deep-learning-based single image reflection removal method. Our network has a reflection detection module to regress a probabilistic reflection confidence map, taking multi-scale Laplacian features as inputs. This probabilistic map tells if a region is reflection-dominated or transmission-dominated, and it is used as a cue for the network to control the feature flow when predicting the reflection and transmission layers. We design our network as a recurrent network to progressively refine reflection removal results at each iteration. The novelty is that we leverage Laplacian kernel parameters to emphasize the boundaries of strong reflections. It is beneficial to strong reflection detection and substantially improves the quality of reflection removal results. Extensive experiments verify the superior performance of the proposed method over state-of-the-art approaches. Our code and the pre-trained model can be found at https://github.com/zdlarr/Location-aware-SIRR.},
  archive   = {C_ICCV},
  author    = {Zheng Dong and Ke Xu and Yin Yang and Hujun Bao and Weiwei Xu and Rynson W.H. Lau},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00497},
  pages     = {4997-5006},
  title     = {Location-aware single image reflection removal},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to remove refractive distortions from underwater
images. <em>ICCV</em>, 4987–4996. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The fluctuation of the water surface causes refractive distortions that severely downgrade the image of an underwater scene. Here, we present the distortion-guided network (DG-Net) for restoring distortion-free underwater images. The key idea is to use a distortion map to guide network training. The distortion map models the pixel displacement caused by water refraction. We first use a physically constrained convolutional network to estimate the distortion map from the refracted image. We then use a generative adversarial network guided by the distortion map to restore the sharp distortion-free image. Since the distortion map indicates correspondences between the distorted image and the distortion-free one, it guides the network to make better predictions. We evaluate our network on several real and synthetic underwater image datasets and show that it out-performs the state-of-the-art algorithms, especially in presence of large distortions. We also show results of complex scenarios, including outdoor swimming pool images captured by drone and indoor aquarium images taken by cellphone camera.},
  archive   = {C_ICCV},
  author    = {Simron Thapa and Nianyi Li and Jinwei Ye},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00496},
  pages     = {4987-4996},
  title     = {Learning to remove refractive distortions from underwater images},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards flexible blind JPEG artifacts removal.
<em>ICCV</em>, 4977–4986. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Training a single deep blind model to handle different quality factors for JPEG image artifacts removal has been attracting considerable attention due to its convenience for practical usage. However, existing deep blind methods usually directly reconstruct the image without predicting the quality factor, thus lacking the flexibility to control the output as the non-blind methods. To remedy this problem, in this paper, we propose a flexible blind convolutional neural network, namely FBCNN, that can predict the adjustable quality factor to control the trade-off between artifacts removal and details preservation. Specifically, FBCNN decouples the quality factor from the JPEG image via a decoupler module and then embeds the predicted quality factor into the subsequent reconstructor module through a quality factor attention block for flexible control. Besides, We find existing methods are prone to fail on non-aligned double JPEG images even with only one pixel shift, and we thus propose a double JPEG degradation model to augment the training data. Extensive experiments on single JPEG images, more general double JPEG images and real-world JPEG images demonstrate that our proposed FBCNN achieves favorable performance against state-of-the-art methods in terms of both quantitative metrics and visual quality.},
  archive   = {C_ICCV},
  author    = {Jiaxi Jiang and Kai Zhang and Radu Timofte},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00495},
  pages     = {4977-4986},
  title     = {Towards flexible blind JPEG artifacts removal},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving de-raining generalization via neural
reorganization. <em>ICCV</em>, 4967–4976. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most existing image de-raining networks could only learn fixed mapping rules between paired rainy/clean images on single synthetic dataset and then stay static for lifetime. However, since single synthetic dataset merely provides a partial view for the distribution of rain streaks, deep models well trained on an individual synthetic dataset tend to overfit on this biased distribution. This leads to the inability of these methods to well generalize to complex and changeable real-world rainy scenes, thus limiting their practical applications. In this paper, we try for the first time to accumulate the de-raining knowledge from multiple synthetic datasets on a single network parameter set to improve the de-raining generalization of deep networks. To achieve this goal, we explore Neural Reorganization (NR) to allow the de-raining network to keep a subtle stability-plasticity trade-off rather than naive stabilization after training phase. Specifically, we design our NR algorithm by borrowing the synaptic consolidation mechanism in the biological brain and knowledge distillation. Equipped with our NR algorithm, the deep model can be trained on a list of synthetic rainy datasets by overcoming catastrophic forgetting, making it a general-version de-raining network. Extensive experimental validation shows that due to the successful accumulation of de-raining knowledge, our proposed method can not only process multiple synthetic datasets consistently, but also achieve state-of-the-art results when dealing with real-world rainy images.},
  archive   = {C_ICCV},
  author    = {Jie Xiao and Man Zhou and Xueyang Fu and Aiping Liu and Zheng-Jun Zha},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00494},
  pages     = {4967-4976},
  title     = {Improving de-raining generalization via neural reorganization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weakly-supervised video anomaly detection with robust
temporal feature magnitude learning. <em>ICCV</em>, 4955–4966. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Anomaly detection with weakly supervised video-level labels is typically formulated as a multiple instance learning (MIL) problem, in which we aim to identify snippets containing abnormal events, with each video represented as a bag of video snippets. Although current methods show effective detection performance, their recognition of the positive instances, i.e., rare abnormal snippets in the abnormal videos, is largely biased by the dominant negative instances, especially when the abnormal events are subtle anomalies that exhibit only small differences compared with normal events. This issue is exacerbated in many methods that ignore important video temporal dependencies. To address this issue, we introduce a novel and theoretically sound method, named Robust Temporal Feature Magnitude learning (RTFM), which trains a feature magnitude learning function to effectively recognise the positive instances, substantially improving the robustness of the MIL approach to the negative instances from abnormal videos. RTFM also adapts dilated convolutions and self-attention mechanisms to capture long- and short-range temporal dependencies to learn the feature magnitude more faithfully. Extensive experiments show that the RTFM-enabled MIL model (i) outperforms several state-of-the-art methods by a large margin on four benchmark data sets (ShanghaiTech, UCF-Crime, XD-Violence and UCSD-Peds) and (ii) achieves significantly improved subtle anomaly discriminability and sample efficiency.},
  archive   = {C_ICCV},
  author    = {Yu Tian and Guansong Pang and Yuanhong Chen and Rajvinder Singh and Johan W. Verjans and Gustavo Carneiro},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00493},
  pages     = {4955-4966},
  title     = {Weakly-supervised video anomaly detection with robust temporal feature magnitude learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive graph convolution for point cloud analysis.
<em>ICCV</em>, 4945–4954. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Convolution on 3D point clouds that generalized from 2D grid-like domains is widely researched yet far from perfect. The standard convolution characterises feature correspondences indistinguishably among 3D points, presenting an intrinsic limitation of poor distinctive feature learning. In this paper, we propose Adaptive Graph Convolution (AdaptConv) which generates adaptive kernels for points according to their dynamically learned features. Compared with using a fixed/isotropic kernel, AdaptConv improves the flexibility of point cloud convolutions, effectively and precisely capturing the diverse relations between points from different semantic parts. Unlike popular attentional weight schemes, the proposed AdaptConv implements the adaptiveness inside the convolution operation instead of simply assigning different weights to the neighboring points. Extensive qualitative and quantitative evaluations show that our method outperforms state-of-the-art point cloud classification and segmentation approaches on several benchmark datasets. Our code is available at https://github.com/hrzhou2/AdaptConv-master.},
  archive   = {C_ICCV},
  author    = {Haoran Zhou and Yidan Feng and Mingsheng Fang and Mingqiang Wei and Jing Qin and Tong Lu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00492},
  pages     = {4945-4954},
  title     = {Adaptive graph convolution for point cloud analysis},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The benefit of distraction: Denoising camera-based
physiological measurements using inverse attention. <em>ICCV</em>,
4935–4944. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Attention networks perform well on diverse computer vision tasks. The core idea is that the signal of interest is stronger in some pixels (&quot;foreground&quot;), and by selectively focusing computation on these pixels, networks can extract subtle information buried in noise and other sources of corruption. Our paper is based on one key observation: in many real-world applications, many sources of corruption, such as illumination and motion, are often shared between the &quot;foreground&quot; and the &quot;background&quot; pixels. Can we utilize this to our advantage? We propose the utility of inverse attention networks, which focus on extracting information about these shared sources of corruption. We show that this helps to effectively suppress shared covariates and amplify signal information, resulting in improved performance. We illustrate this on the task of camera-based physiological measurement where the signal of interest is weak and global illumination variations and motion act as significant shared sources of corruption. We perform experiments on three datasets and show that our approach of inverse attention produces state-of-the-art results, increasing the signal-to-noise ratio by up to 5.8 dB, reducing heart rate and breathing rate estimation errors by as much as 30\%, recovering subtle waveform dynamics, and generalizing from RGB to NIR videos without retraining.},
  archive   = {C_ICCV},
  author    = {Ewa M. Nowara and Daniel McDuff and Ashok Veeraraghavan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00491},
  pages     = {4935-4944},
  title     = {The benefit of distraction: Denoising camera-based physiological measurements using inverse attention},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A machine teaching framework for scalable recognition.
<em>ICCV</em>, 4925–4934. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the scalable recognition problem in the fine-grained expert domain where large-scale data collection is easy whereas annotation is difficult. Existing solutions are typically based on semi-supervised or self-supervised learning. We propose an alternative new framework, MEMORABLE, based on machine teaching and online crowd-sourcing platforms. A small amount of data is first labeled by experts and then used to teach online annotators for the classes of interest, who finally label the entire dataset. Preliminary studies show that the accuracy of classifiers trained on the final dataset is a function of the accuracy of the student annotators. A new machine teaching algorithm, CMaxGrad, is then proposed to enhance this accuracy by introducing explanations in a state-of-the-art machine teaching algorithm. For this, CMaxGrad leverages counterfactual explanations, which take into account student predictions, thereby proving feedback that is student-specific, explicitly addresses the causes of student confusion, and adapts to the level of competence of the student. Experiments show that both MEMORABLE and CMaxGrad outperform existing solutions to their respective problems.},
  archive   = {C_ICCV},
  author    = {Pei Wang and Nuno Vasconcelos},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00490},
  pages     = {4925-4934},
  title     = {A machine teaching framework for scalable recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). INAS: Integral NAS for device-aware salient object
detection. <em>ICCV</em>, 4914–4924. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing salient object detection (SOD) models usually focus on either backbone feature extractors or saliency heads, ignoring their relations. A powerful backbone could still achieve sub-optimal performance with a weak saliency head and vice versa. Moreover, the balance between model performance and inference latency poses a great challenge to model design, especially when considering different deployment scenarios. Considering all components in an integral neural architecture search (iNAS) space, we propose a flexible device-aware search scheme that only trains the SOD model once and quickly finds high-performance but low-latency models on multiple devices. An evolution search with latency-group sampling (LGS) is proposed to explore the entire latency area of our enlarged search space. Models searched by iNAS achieve similar performance with SOTA methods but reduce the 3.8×, 3.3×, 2.6×, 1.9× latency on Huawei Nova6 SE, Intel Core CPU, the Jetson Nano, and Nvidia Titan Xp. The code is released at https://mmcheng.net/inas/.},
  archive   = {C_ICCV},
  author    = {Yu-Chao Gu and Shang-Hua Gao and Xu-Sheng Cao and Peng Du and Shao-Ping Lu and Ming-Ming Cheng},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00489},
  pages     = {4914-4924},
  title     = {INAS: Integral NAS for device-aware salient object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Full-duplex strategy for video object segmentation.
<em>ICCV</em>, 4902–4913. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Appearance and motion are two important sources of information in video object segmentation (VOS). Previous methods mainly focus on using simplex solutions, lowering the upper bound of feature collaboration among and across these two cues. In this paper, we study a novel framework, termed the FSNet (Full-duplex Strategy Network), which designs a relational cross-attention module (RCAM) to achieve the bidirectional message propagation across embedding subspaces. Furthermore, the bidirectional purification module (BPM) is introduced to update the inconsistent features between the spatial-temporal embeddings, effectively improving the model robustness. By considering the mutual restraint within the full-duplex strategy, our FSNet performs the cross-modal feature-passing (i.e., transmission and receiving) simultaneously before the fusion and decoding stage, making it robust to various challenging scenarios (e.g., motion blur, occlusion) in VOS. Extensive experiments on five popular benchmarks (i.e., DAVIS 16 , FBMS, MCL, SegTrack-V2, and DAVSOD 19 ) show that our FSNet outperforms other state-of-the-arts for both the VOS and video salient object detection tasks.},
  archive   = {C_ICCV},
  author    = {Ge-Peng Ji and Keren Fu and Zhe Wu and Deng-Ping Fan and Jianbing Shen and Ling Shao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00488},
  pages     = {4902-4913},
  title     = {Full-duplex strategy for video object segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Collaborative unsupervised visual representation learning
from decentralized data. <em>ICCV</em>, 4892–4901. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unsupervised representation learning has achieved outstanding performances using centralized data available on the Internet. However, the increasing awareness of privacy protection limits sharing of decentralized unlabeled image data that grows explosively in multiple parties (e.g., mobile phones and cameras). As such, a natural problem is how to leverage these data to learn visual representations for downstream tasks while preserving data privacy. To address this problem, we propose a novel federated unsupervised learning framework, FedU. In this framework, each party trains models from unlabeled data independently using contrastive learning with an online network and a target network. Then, a central server aggregates trained models and updates clients’ models with the aggregated model. It preserves data privacy as each party only has access to its raw data. Decentralized data among multiple parties are normally non-independent and identically distributed (non-IID), leading to performance degradation. To tackle this challenge, we propose two simple but effective methods: 1) We design the communication protocol to upload only the encoders of online networks for server aggregation and update them with the aggregated encoder; 2) We introduce a new module to dynamically decide how to update predictors based on the divergence caused by non-IID. The predictor is the other component of the online network. Extensive experiments and ablations demonstrate the effectiveness and significance of FedU. It outperforms training with only one party by over 5\% and other methods by over 14\% in linear and semi-supervised evaluation on non-IID data.},
  archive   = {C_ICCV},
  author    = {Weiming Zhuang and Xin Gan and Yonggang Wen and Shuai Zhang and Shuai Yi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00487},
  pages     = {4892-4901},
  title     = {Collaborative unsupervised visual representation learning from decentralized data},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Video matting via consistency-regularized graph neural
networks. <em>ICCV</em>, 4882–4891. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning temporally consistent foreground opacity from videos, i.e., video matting, has drawn great attention due to the blossoming of video conferencing. Previous approaches are built on top of image matting models, which fail in maintaining the temporal coherence when being adapted to videos. They either utilize the optical flow to smooth frame-wise prediction, where the performance is dependent on the selected optical flow model; or naively combine feature maps from multiple frames, which does not model well the correspondence of pixels in adjacent frames. In this paper, we propose to enhance the temporal coherence by Consistency-Regularized Graph Neural Networks (CRGNN) with the aid of a synthesized video matting dataset. CRGNN utilizes Graph Neural Networks (GNN) to relate adjacent frames such that pixels or regions that are incorrectly predicted in one frame can be corrected by leveraging information from its neighboring frames. To generalize our model from synthesized videos to real-world videos, we propose a consistency regularization technique to enforce the consistency on the alpha and foreground when blending them with different backgrounds. To evaluate the efficacy of CRGNN, we further collect a real-world dataset with annotated alpha mattes. Compared with state-of-the-art methods that require hand-crafted trimaps or backgrounds for modeling training, CRGNN generates favorably results with the help of unlabeled real training dataset. The source code and datasets are available at https://github.com/TiantianWang/VideoMattingCRGNN.git.},
  archive   = {C_ICCV},
  author    = {Tiantian Wang and Sifei Liu and Yapeng Tian and Kai Li and Ming-Hsuan Yang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00486},
  pages     = {4882-4891},
  title     = {Video matting via consistency-regularized graph neural networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dense deep unfolding network with 3D-CNN prior for snapshot
compressive imaging. <em>ICCV</em>, 4872–4881. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Snapshot compressive imaging (SCI) aims to record three-dimensional signals via a two-dimensional camera. For the sake of building a fast and accurate SCI recovery algorithm, we incorporate the interpretability of model-based methods and the speed of learning-based ones and present a novel dense deep unfolding network (DUN) with 3D-CNN prior for SCI, where each phase is unrolled from an iteration of Half-Quadratic Splitting (HQS). To better exploit the spatial-temporal correlation among frames and address the problem of information loss between adjacent phases in existing DUNs, we propose to adopt the 3D-CNN prior in our proximal mapping module and develop a novel dense feature map (DFM) strategy, respectively. Besides, in order to promote network robustness, we further propose a dense feature map adaption (DFMA) module to allow inter-phase information to fuse adaptively. All the parameters are learned in an end-to-end fashion. Extensive experiments on simulation data and real data verify the superiority of our method. The source code is available at https://github.com/jianzhangcs/SCI3D.},
  archive   = {C_ICCV},
  author    = {Zhuoyuan Wt and Jian Zhangt and Chong Mou},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00485},
  pages     = {4872-4881},
  title     = {Dense deep unfolding network with 3D-CNN prior for snapshot compressive imaging},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). EvIntSR-net: Event guided multiple latent frames
reconstruction and super-resolution. <em>ICCV</em>, 4862–4871. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An event camera detects the scene radiance changes and sends a sequence of asynchronous event streams with high dynamic range, high temporal resolution, and low latency. However, the spatial resolution of event cameras is limited as a trade-off for these outstanding properties. To reconstruct high-resolution intensity images from event data, we propose EvIntSR-Net that converts Event data to multiple latent Intensity frames to achieve Super-Resolution on intensity images in this paper. EvIntSR-Net bridges the domain gap between event streams and intensity frames and learns to merge a sequence of latent intensity frames in a recurrent updating manner. Experimental results show that EvIntSR-Net can reconstruct SR intensity images with higher dynamic range and fewer blurry artifacts by fusing events with intensity frames for both simulated and real-world data. Furthermore, the proposed EvIntSR-Net is able to generate high-frame-rate videos with super-resolved frames.},
  archive   = {C_ICCV},
  author    = {Jin Han and Yixin Yang and Chu Zhou and Chao Xu and Boxin Shi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00484},
  pages     = {4862-4871},
  title     = {EvIntSR-net: Event guided multiple latent frames reconstruction and super-resolution},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Panoptic segmentation of satellite image time series with
convolutional temporal attention networks. <em>ICCV</em>, 4852–4861. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unprecedented access to multi-temporal satellite imagery has opened new perspectives for a variety of Earth observation tasks. Among them, pixel-precise panoptic segmentation of agricultural parcels has major economic and environmental implications. While researchers have explored this problem for single images, we argue that the complex temporal patterns of crop phenology are better addressed with temporal sequences of images. In this paper, we present the first end-to-end, single-stage method for panoptic segmentation of Satellite Image Time Series (SITS). This module can be combined with our novel image sequence encoding network which relies on temporal self-attention to extract rich and adaptive multi-scale spatiotemporal features. We also introduce PASTIS, the first open-access SITS dataset with panoptic annotations. We demonstrate the superiority of our encoder for semantic segmentation against multiple competing architectures, and set up the first state-of-the-art of panoptic segmentation of SITS. Our implementation and PASTIS are publicly available.},
  archive   = {C_ICCV},
  author    = {Vivien Sainte Fare Garnot and Loic Landrieu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00483},
  pages     = {4852-4861},
  title     = {Panoptic segmentation of satellite image time series with convolutional temporal attention networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attentive and contrastive learning for joint depth and
motion field estimation. <em>ICCV</em>, 4842–4851. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Estimating the motion of the camera together with the 3D structure of the scene from a monocular vision system is a complex task that often relies on the so-called scene rigidity assumption. When observing a dynamic environment, this assumption is violated which leads to an ambiguity between the ego-motion of the camera and the motion of the objects. To solve this problem, we present a self-supervised learning framework for 3D object motion field estimation from monocular videos. Our contributions are two-fold. First, we propose a two-stage projection pipeline to explicitly disentangle the camera ego-motion and the object motions with dynamics attention module, called DAM. Specifically, we design an integrated motion model that estimates the motion of the camera and object in the first and second warping stages, respectively, controlled by the attention module through a shared motion encoder. Second, we propose an object motion field estimation through contrastive sample consensus, called CSAC, taking advantage of weak semantic prior (bounding box from an object detector) and geometric constraints (each object respects the rigid body motion model). Experiments on KITTI, Cityscapes, and Waymo Open Dataset demonstrate the relevance of our approach and show that our method outperforms state-of-the-art algorithms for the tasks of self-supervised monocular depth estimation, object motion segmentation, monocular scene flow estimation, and visual odometry.},
  archive   = {C_ICCV},
  author    = {Seokju Lee and Francois Rameau and Fei Pan and In So Kweon},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00482},
  pages     = {4842-4851},
  title     = {Attentive and contrastive learning for joint depth and motion field estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). R-SLAM: Optimizing eye tracking from rolling shutter video
of the retina. <em>ICCV</em>, 4832–4841. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a method for optimization-based recovery of eye motion from rolling shutter video of the retina. Our approach formulates eye tracking as an optimization problem that jointly estimates the retina’s motion and appearance using convex optimization and a constrained version of gradient descent. By incorporating the rolling shutter imaging model into the formulation of our joint optimization, we achieve state-of-the-art accuracy both offline and in real-time. We apply our method to retina video captured with an adaptive optics scanning laser ophthalmoscope (AOSLO), demonstrating eye tracking at 1 kHz with accuracies below one arcminute—over an order of magnitude higher than conventional eye tracking systems.},
  archive   = {C_ICCV},
  author    = {Jay Shenoy and James Fong and Jeffrey Tan and Austin Roorda and Ren Ng},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00481},
  pages     = {4832-4841},
  title     = {R-SLAM: Optimizing eye tracking from rolling shutter video of the retina},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Out-of-boundary view synthesis towards full-frame video
stabilization. <em>ICCV</em>, 4822–4831. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Warping-based video stabilizers smooth camera trajectory by constraining each pixel&#39;s displacement and warp stabilized frames from unstable ones accordingly. However, since the view outside the boundary is not available during warping, the resulting holes around the boundary of the stabilized frame must be discarded (i.e., cropping) to maintain visual consistency, and thus does leads to a tradeoff between stability and cropping ratio. In this paper, we make a first attempt to address this issue by proposing a new Out-of-boundary View Synthesis (OVS) method. By the nature of spatial coherence between adjacent frames and within each frame, OVS extrapolates the out-of-boundary view by aligning adjacent frames to each reference one. Technically, it first calculates the optical flow and propagates it to the outer boundary region according to the affinity, and then warps pixels accordingly. OVS can be integrated into existing warping-based stabilizers as a plug-and-play module to significantly improve the cropping ratio of the stabilized results. In addition, stability is improved because the jitter amplification effect caused by cropping and resizing is reduced. Experimental results on the NUS benchmark show that OVS can improve the performance of five representative state-of-the-art methods in terms of objective metrics and subjective visual quality. 1},
  archive   = {C_ICCV},
  author    = {Yufei Xu and Jing Zhang and Dacheng Tao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00480},
  pages     = {4822-4831},
  title     = {Out-of-boundary view synthesis towards full-frame video stabilization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SSH: A self-supervised framework for image harmonization.
<em>ICCV</em>, 4812–4821. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Image harmonization aims to improve the quality of image compositing by matching the &quot;appearance&quot; (e.g., color tone, brightness and contrast) between foreground and background images. However, collecting large-scale annotated datasets for this task requires complex professional retouching. Instead, we propose a novel Self-Supervised Harmonization framework (SSH) that can be trained using just &quot;free&quot; natural images without being edited. We reformulate the image harmonization problem from a representation fusion perspective, which separately processes the foreground and background examples, to address the background occlusion issue. This framework design allows for a dual data augmentation method, where diverse [foreground, background, pseudo GT] triplets can be generated by cropping an image with perturbations using 3D color lookup tables (LUTs). In addition, we build a real-world harmonization dataset as carefully created by expert users, for evaluation and benchmarking purposes. Our results show that the proposed self-supervised method outperforms previous state-of-the-art methods in terms of reference metrics, visual quality, and subject user study. Code and dataset are available at https://github.com/VITA-Group/SSHarmonization.},
  archive   = {C_ICCV},
  author    = {Yifan Jiang and He Zhang and Jianming Zhang and Yilin Wang and Zhe Lin and Kalyan Sunkavalli and Simon Chen and Sohrab Amirghodsi and Sarah Kong and Zhangyang Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00479},
  pages     = {4812-4821},
  title     = {SSH: A self-supervised framework for image harmonization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Achieving on-mobile real-time super-resolution with neural
architecture and pruning search. <em>ICCV</em>, 4801–4811. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Though recent years have witnessed remarkable progress in single image super-resolution (SISR) tasks with the prosperous development of deep neural networks (DNNs), the deep learning methods are confronted with the computation and memory consumption issues in practice, especially for resource-limited platforms such as mobile devices. To overcome the challenge and facilitate the real-time deployment of SISR tasks on mobile, we combine neural architecture search with pruning search and propose an automatic search framework that derives sparse super-resolution (SR) models with high image quality while satisfying the real-time inference requirement. To decrease the search cost, we leverage the weight sharing strategy by introducing a supernet and decouple the search problem into three stages, including supernet construction, compiler-aware architecture and pruning search, and compiler-aware pruning ratio search. With the proposed framework, we are the first to achieve real-time SR inference (with only tens of milliseconds per frame) for implementing 720p resolution with competitive image quality (in terms of PSNR and SSIM) on mobile platforms (Samsung Galaxy S20).},
  archive   = {C_ICCV},
  author    = {Zheng Zhan and Yifan Gong and Pu Zhao and Geng Yuan and Wei Niu and Yushu Wu and Tianyun Zhang and Malith Jayaweera and David Kaeli and Bin Ren and Xue Lin and Yanzhi Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00478},
  pages     = {4801-4811},
  title     = {Achieving on-mobile real-time super-resolution with neural architecture and pruning search},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep blind video super-resolution. <em>ICCV</em>, 4791–4800.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing video super-resolution (SR) algorithms usually assume that the blur kernels in the degradation process are known and do not model the blur kernels in the restoration. However, this assumption does not hold for blind video SR and usually leads to over-smoothed super-resolved frames. In this paper, we propose an effective blind video SR algorithm based on deep convolutional neural networks (CNNs). Our algorithm first estimates blur kernels from low-resolution (LR) input videos. Then, with the estimated blur kernels, we develop an effective image deconvolution method based on the image formation model of blind video SR to generate intermediate latent frames so that sharp image contents can be restored well. To effectively explore the information from adjacent frames, we estimate the motion fields from LR input videos, extract features from LR videos by a feature extraction network, and warp the extracted features from LR inputs based on the motion fields. Moreover, we develop an effective sharp feature exploration method which first extracts sharp features from restored intermediate latent frames and then uses a transformation operation based on the extracted sharp features and warped features from LR inputs to generate better features for HR video restoration. We formulate the proposed algorithm into an end-to-end trainable framework and show that it performs favorably against state-of-the-art methods.},
  archive   = {C_ICCV},
  author    = {Jinshan Pan and Haoran Bai and Jiangxin Dong and Jiawei Zhang and Jinhui Tang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00477},
  pages     = {4791-4800},
  title     = {Deep blind video super-resolution},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning a single network for scale-arbitrary
super-resolution. <em>ICCV</em>, 4781–4790. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, the performance of single image super-resolution (SR) has been significantly improved with powerful networks. However, these networks are developed for image SR with specific integer scale factors (e.g., ×2/3/4), and cannot handle non-integer and asymmetric SR. In this paper, we propose to learn a scale-arbitrary image SR network from scale-specific networks. Specifically, we develop a plug-in module for existing SR networks to perform scale-arbitrary SR, which consists of multiple scale-aware feature adaption blocks and a scale-aware upsampling layer. Moreover, conditional convolution is used in our plug-in module to generate dynamic scale-aware filters, which enables our network to adapt to arbitrary scale factors. Our plug-in module can be easily adapted to existing networks to realize scale-arbitrary SR with a single model. These networks plugged with our module can produce promising results for non-integer and asymmetric SR while maintaining state-of-the-art performance for SR with integer scale factors. Besides, the additional computational and memory cost of our module is very small.},
  archive   = {C_ICCV},
  author    = {Longguang Wang and Yingqian Wang and Zaiping Lin and Jungang Yang and Wei An and Yulan Guo},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00476},
  pages     = {4781-4790},
  title     = {Learning a single network for scale-arbitrary super-resolution},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Designing a practical degradation model for deep blind image
super-resolution. <em>ICCV</em>, 4771–4780. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It is widely acknowledged that single image super-resolution (SISR) methods would not perform well if the assumed degradation model deviates from those in real images. Although several degradation models take additional factors into consideration, such as blur, they are still not effective enough to cover the diverse degradations of real images. To address this issue, this paper proposes to design a more complex but practical degradation model that consists of randomly shuffled blur, downsampling and noise degradations. Specifically, the blur is approximated by two convolutions with isotropic and anisotropic Gaussian kernels; the downsampling is randomly chosen from nearest, bilinear and bicubic interpolations; the noise is synthesized by adding Gaussian noise with different noise levels, adopting JPEG compression with different quality factors, and generating processed camera sensor noise via reverse-forward camera image signal processing (ISP) pipeline model and RAW image noise model. To verify the effectiveness of the new degradation model, we have trained a deep blind ES-RGAN super-resolver and then applied it to super-resolve both synthetic and real images with diverse degradations. The experimental results demonstrate that the new degradation model can help to significantly improve the practicability of deep super-resolvers, thus providing a powerful alternative solution for real SISR applications.},
  archive   = {C_ICCV},
  author    = {Kai Zhang and Jingyun Liang and Luc Van Gool and Radu Timofte},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00475},
  pages     = {4771-4780},
  title     = {Designing a practical degradation model for deep blind image super-resolution},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-world video super-resolution: A benchmark dataset and a
decomposition based learning scheme. <em>ICCV</em>, 4761–4770. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Video super-resolution (VSR) aims to improve the spatial resolution of low-resolution (LR) videos. Existing VSR methods are mostly trained and evaluated on synthetic datasets, where the LR videos are uniformly downsampled from their high-resolution (HR) counterparts by some simple operators (e.g., bicubic downsampling). Such simple synthetic degradation models, however, cannot well describe the complex degradation processes in real-world videos, and thus the trained VSR models become ineffective in real-world applications. As an attempt to bridge the gap, we build a real-world video super-resolution (RealVSR) dataset by capturing paired LR-HR video sequences using the multi-camera system of iPhone 11 Pro Max. Since the LR-HR video pairs are captured by two separate cameras, there are inevitably certain misalignment and luminance/color differences between them. To more robustly train the VSR model and recover more details from the LR inputs, we convert the LR-HR videos into YCbCr space and decompose the luminance channel into a Laplacian pyramid, and then apply different loss functions to different components. Experiments validate that VSR models trained on our RealVSR dataset demonstrate better visual quality than those trained on synthetic datasets under real-world settings. They also exhibit good generalization capability in cross-camera tests. The dataset and code can be found at https://github.com/IanYeung/RealVSR.},
  archive   = {C_ICCV},
  author    = {Xi Yang and Wangmeng Xiang and Hui Zeng and Lei Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00474},
  pages     = {4761-4770},
  title     = {Real-world video super-resolution: A benchmark dataset and a decomposition based learning scheme},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Morphable detector for object detection on demand.
<em>ICCV</em>, 4751–4760. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many emerging applications of intelligent robots need to explore and understand new environments, where it is desirable to detect objects of novel classes on the fly with minimum online efforts. This is an object detection on demand (ODOD) task. It is challenging, because it is impossible to annotate a large number of data on the fly, and the embedded systems are usually unable to perform back-propagation which is essential for training. Most existing few-shot detection methods are confronted here as they need extra training. We propose a novel morphable detector (MD), that simply &quot;morphs&quot; some of its changeable parameters online estimated from the few samples, so as to detect novel classes without any extra training. The MD has two sets of parameters, one for the feature embedding and the other for class representation (called &quot;prototypes&quot;). Each class is associated with a hidden prototype to be learned by integrating the visual and semantic embeddings. The learning of the MD is based on the alternate learning of the feature embedding and the prototypes in an EM-like approach which allows the recovery of an unknown prototype from a few samples of a novel class. Once an MD is learned, it is able to use a few samples of a novel class to directly compute its prototype to fulfill the online morphing process. We have shown the superiority of the MD in Pascal [12], COCO [27] and FSOD [13] datasets.},
  archive   = {C_ICCV},
  author    = {Xiangyun Zhao and Xu Zou and Ying Wu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00473},
  pages     = {4751-4760},
  title     = {Morphable detector for object detection on demand},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DivAug: Plug-in automated data augmentation with explicit
diversity maximization. <em>ICCV</em>, 4742–4750. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human-designed data augmentation strategies have been replaced by automatically learned augmentation policy in the past two years. Specifically, recent work has empirically shown that the superior performance of the automated data augmentation methods stems from increasing the diversity of augmented data [4], [5]. However, two factors regarding the diversity of augmented data are still missing: 1) the explicit definition (and thus measurement) of diversity and 2) the quantifiable relationship between diversity and its regularization effects. To bridge this gap, we propose a diversity measure called Variance Diversity and theoretically show that the regularization effect of data augmentation is promised by Variance Diversity. We validate in experiments that the relative gain from automated data augmentation in test accuracy is highly correlated to Variance Diversity. An unsupervised sampling-based framework, DivAug, is designed to directly maximize Variance Diversity and hence strengthen the regularization effect. Without requiring a separate search process, the performance gain from DivAug is comparable with the state-of-the-art method with better efficiency. Moreover, under the semi-supervised setting, our framework can further improve the performance of semi-supervised learning algorithms compared to RandAugment, making it highly applicable to real-world problems, where labeled data is scarce. The code is available at https://github.com/warai-0toko/DivAug.},
  archive   = {C_ICCV},
  author    = {Zirui Liu and Haifeng Jin and Ting-Hsiang Wang and Kaixiong Zhou and Xia Hu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00472},
  pages     = {4742-4750},
  title     = {DivAug: Plug-in automated data augmentation with explicit diversity maximization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unpaired learning for deep image deraining with rain
direction regularizer. <em>ICCV</em>, 4733–4741. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a simple yet effective unpaired learning based image rain removal method from an unpaired set of synthetic images and real rainy images by exploring the properties of rain maps. The proposed algorithm mainly consists of a semi-supervised learning part and a knowledge distillation part. The semi-supervised part estimates the rain map and reconstructs the derained image based on the well-established layer separation principle. To facilitate rain removal, we develop a rain direction regularizer to constrain the rain estimation network in the semi-supervised learning part. With the estimated rain maps from the semi-supervised learning part, we first synthesize a new paired set by adding to rain-free images based on the superimposition model. The real rainy images and the derained results constitute another paired set. Then we develop an effective knowledge distillation method to explore such two paired sets so that the deraining model in the semi-supervised learning part is distilled. We propose two new rainy datasets, named RainDirection and Real3000, to validate the effectiveness of the proposed method. Both quantitative and qualitative experimental results demonstrate that the proposed method achieves favorable results against state-of-the-art methods in benchmark datasets and real-world images.},
  archive   = {C_ICCV},
  author    = {Yang Liu and Ziyu Yue and Jinshan Pan and Zhixun Su},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00471},
  pages     = {4733-4741},
  title     = {Unpaired learning for deep image deraining with rain direction regularizer},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CANet: A context-aware network for shadow removal.
<em>ICCV</em>, 4723–4732. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel two-stage context-aware network named CANet for shadow removal, in which the contextual information from non-shadow regions is transferred to shadow regions at the embedded feature spaces. At Stage-I, we propose a contextual patch matching (CPM) module to generate a set of potential matching pairs of shadow and non-shadow patches. Combined with the potential contextual relationships between shadow and non-shadow regions, our well-designed contextual feature transfer (CFT) mechanism can transfer contextual information from non-shadow to shadow regions at different scales. With the reconstructed feature maps, we remove shadows at L and A/B channels separately. At Stage-II, we use an encoder-decoder to refine current results and generate the final shadow removal results. We evaluate our proposed CANet on two benchmark datasets and some real-world shadow images with complex scenes. Extensive experimental results strongly demonstrate the efficacy of our proposed CANet and exhibit superior performance to state-of-the-arts. Our source code is available at https://github.com/Zipei-Chen/CANet.},
  archive   = {C_ICCV},
  author    = {Zipei Chen and Chengjiang Long and Ling Zhang and Chunxia Xiao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00470},
  pages     = {4723-4732},
  title     = {CANet: A context-aware network for shadow removal},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HiNet: Deep image hiding by invertible network.
<em>ICCV</em>, 4713–4722. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Image hiding aims to hide a secret image into a cover image in an imperceptible way, and then recover the secret image perfectly at the receiver end. Capacity, invisibility and security are three primary challenges in image hiding task. This paper proposes a novel invertible neural network (INN) based framework, HiNet, to simultaneously overcome the three challenges in image hiding. For large capacity, we propose an inverse learning mechanism by simultaneously learning the image concealing and revealing processes. Our method is able to achieve the concealing of a full-size secret image into a cover image with the same size. For high invisibility, instead of pixel domain hiding, we propose to hide the secret information in wavelet domain. Furthermore, we propose a new low-frequency wavelet loss to constrain that secret information is hidden in high-frequency wavelet subbands, which significantly improves the hiding security. Experimental results show that our HiNet significantly outperforms other state-of-the-art image hiding methods, with more than 10 dB PSNR improvement in secret image recovery on ImageNet, COCO and DIV2K datasets. Codes are available at https://github.com/TomTomTommi/HiNet.},
  archive   = {C_ICCV},
  author    = {Junpeng Jing and Xin Deng and Mai Xu and Jianyi Wang and Zhenyu Guan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00469},
  pages     = {4713-4722},
  title     = {HiNet: Deep image hiding by invertible network},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual saliency transformer. <em>ICCV</em>, 4702–4712. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing state-of-the-art saliency detection methods heavily rely on CNN-based architectures. Alternatively, we rethink this task from a convolution-free sequence-to-sequence perspective and predict saliency by modeling long-range dependencies, which can not be achieved by convolution. Specifically, we develop a novel unified model based on a pure transformer, namely, Visual Saliency Transformer (VST), for both RGB and RGB-D salient object detection (SOD). It takes image patches as inputs and leverages the transformer to propagate global contexts among image patches. Unlike conventional architectures used in Vision Transformer (ViT), we leverage multi-level token fusion and propose a new token upsampling method under the transformer framework to get high-resolution detection results. We also develop a token-based multi-task decoder to simultaneously perform saliency and boundary detection by introducing task-related tokens and a novel patch-task-attention mechanism. Experimental results show that our model outperforms existing methods on both RGB and RGB-D SOD benchmark datasets. Most importantly, our whole framework not only provides a new perspective for the SOD field but also shows a new paradigm for transformer-based dense prediction models. Code is available at https://github.com/nnizhang/VST.},
  archive   = {C_ICCV},
  author    = {Nian Liu and Ni Zhang and Kaiyuan Wan and Ling Shao and Junwei Han},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00468},
  pages     = {4702-4712},
  title     = {Visual saliency transformer},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Light field saliency detection with dual local graph
learning and reciprocative guidance. <em>ICCV</em>, 4692–4701. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The application of light field data in salient object detection is becoming increasingly popular recently. The difficulty lies in how to effectively fuse the features within the focal stack and how to cooperate them with the feature of the all-focus image. Previous methods usually fuse focal stack features via convolution or ConvLSTM, which are both less effective and ill-posed. In this paper, we model the information fusion within focal stack via graph networks. They introduce powerful context propagation from neighbouring nodes and also avoid ill-posed implementations. On the one hand, we construct local graph connections thus avoiding prohibitive computational costs of traditional graph networks. On the other hand, instead of processing the two kinds of data separately, we build a novel dual graph model to guide the focal stack fusion process using all-focus patterns. To handle the second difficulty, previous methods usually implement one-shot fusion for focal stack and all-focus features, hence lacking a thorough exploration of their supplements. We introduce a reciprocative guidance scheme and enable mutual guidance between these two kinds of information at multiple steps. As such, both kinds of features can be enhanced iteratively, finally benefiting the saliency prediction. Extensive experimental results show that the proposed models are all beneficial and we achieve significantly better results than state-of-the-art methods.},
  archive   = {C_ICCV},
  author    = {Nian Liu and Wangbo Zhao and Dingwen Zhang and Junwei Han and Ling Shao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00467},
  pages     = {4692-4701},
  title     = {Light field saliency detection with dual local graph learning and reciprocative guidance},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mitigating intensity bias in shadow detection via feature
decomposition and reweighting. <em>ICCV</em>, 4682–4691. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although CNNs have achieved remarkable progress on the shadow detection task, they tend to make mistakes in dark non-shadow regions and relatively bright shadow regions. They are also susceptible to brightness change. These two phenomenons reveal that deep shadow detectors heavily depend on the intensity cue, which we refer to as intensity bias. In this paper, we propose a novel feature decomposition and reweighting scheme to mitigate this intensity bias, in which multi-level integrated features are decomposed into intensity-variant and intensity-invariant components through self-supervision. By reweighting these two types of features, our method can reallocate the attention to the corresponding latent semantics and achieves balanced exploitation of them. Extensive experiments on three popular datasets show that the proposed method outperforms state-of-the-art shadow detectors.},
  archive   = {C_ICCV},
  author    = {Lei Zhu and Ke Xu and Zhanghan Ke and Rynson W.H. Lau},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00466},
  pages     = {4682-4691},
  title     = {Mitigating intensity bias in shadow detection via feature decomposition and reweighting},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High-fidelity pluralistic image completion with
transformers. <em>ICCV</em>, 4672–4681. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Image completion has made tremendous progress with convolutional neural networks (CNNs), because of their powerful texture modeling capacity. However, due to some inherent properties (e.g., local inductive prior, spatial-invariant kernels), CNNs do not perform well in understanding global structures or naturally support pluralistic completion. Recently, transformers demonstrate their power in modeling the long-term relationship and generating diverse results, but their computation complexity is quadratic to input length, thus hampering the application in processing high-resolution images. This paper brings the best of both worlds to pluralistic image completion: appearance prior reconstruction with transformer and texture replenishment with CNN. The former transformer recovers pluralistic coherent structures together with some coarse textures, while the latter CNN enhances the local texture details of coarse priors guided by the high-resolution masked images. The proposed method vastly outperforms state-of-the-art methods in terms of three aspects: 1) large performance boost on image fidelity even compared to deterministic completion methods; 2) better diversity and higher fidelity for pluralistic completion; 3) exceptional generalization ability on large masks and generic dataset, like ImageNet. Code and pre-trained models have been publicly released at https://github.com/raywzy/ICT.},
  archive   = {C_ICCV},
  author    = {Ziyu Wan and Jingbo Zhang and Dongdong Chen and Jing Liao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00465},
  pages     = {4672-4681},
  title     = {High-fidelity pluralistic image completion with transformers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Specificity-preserving RGB-d saliency detection.
<em>ICCV</em>, 4661–4671. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {RGB-D saliency detection has attracted increasing attention, due to its effectiveness and the fact that depth cues can now be conveniently captured. Existing works often focus on learning a shared representation through various fusion strategies, with few methods explicitly considering how to preserve modality-specific characteristics. In this paper, taking a new perspective, we propose a specificity-preserving network (SP-Net) for RGB-D saliency detection, which benefits saliency detection performance by exploring both the shared information and modality-specific properties (e.g., specificity). Specifically, two modality-specific networks and a shared learning network are adopted to generate individual and shared saliency maps. A cross-enhanced integration module (CIM) is proposed to fuse cross-modal features in the shared learning network, which are then propagated to the next layer for integrating cross-level information. Besides, we propose a multi-modal feature aggregation (MFA) module to integrate the modality-specific features from each individual decoder into the shared decoder, which can provide rich complementary multi-modal information to boost the saliency detection performance. Further, a skip connection is used to combine hierarchical features between the encoder and decoder layers. Experiments on six benchmark datasets demonstrate that our SP-Net outperforms other state-of-the-art methods. Code is available at: https://github.com/taozh2017/SPNet.},
  archive   = {C_ICCV},
  author    = {Tao Zhou and Huazhu Fu and Geng Chen and Yi Zhou and Deng-Ping Fan and Ling Shao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00464},
  pages     = {4661-4671},
  title     = {Specificity-preserving RGB-D saliency detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DCT-SNN: Using DCT to distribute spatial information over
time for low-latency spiking neural networks. <em>ICCV</em>, 4651–4660.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Spiking Neural Networks (SNNs) offer a promising alternative to traditional deep learning, since they provide higher computational efficiency due to event-driven information processing. SNNs distribute the analog values of pixel intensities into binary spikes over time. However, the most widely used input coding schemes, such as Poisson based rate-coding, do not leverage the additional temporal learning capability of SNNs effectively. Moreover, these SNNs suffer from high inference latency which is a major bottleneck to their deployment. To overcome this, we propose a time-based encoding scheme that utilizes Discrete Cosine Transform (DCT) to reduce the number of timesteps required for inference (DCT-SNN). DCT decomposes an image into a weighted sum of sinusoidal basis images. At each time step, a single frequency base, taken in order and modulated by its corresponding DCT coefficient, is input to an accumulator that generates spikes upon crossing a threshold. We use the proposed scheme to train DCT-SNN, a low-latency deep SNN with leaky-integrate-and-fire neurons using surrogate gradient descent based backpropagation. We achieve top-1 accuracy of 89.94\%, 68.30\% and 52.43\% on CIFAR10, CIFAR-100 and TinyImageNet, respectively using VGG architectures. Notably, DCT-SNN performs inference with 2-14X reduced latency compared to other state-of-the-art SNNs, while achieving comparable accuracy to their standard deep learning counterparts. The dimension of the transform allows us to control the number of timesteps required for inference. Additionally, we can trade-off accuracy with latency in a principled manner by dropping the highest frequency components during inference. The code is publicly available. 1},
  archive   = {C_ICCV},
  author    = {Isha Garg and Sayeed Shafayet Chowdhury and Kaushik Roy},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00463},
  pages     = {4651-4660},
  title     = {DCT-SNN: Using DCT to distribute spatial information over time for low-latency spiking neural networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PnP-DETR: Towards efficient visual analysis with
transformers. <em>ICCV</em>, 4641–4650. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, DETR [3] pioneered the solution of vision tasks with transformers, it directly translates the image feature map into the object detection result. Though effective, translating the full feature map can be costly due to redundant computation on some area like the background. In this work, we encapsulate the idea of reducing spatial redundancy into a novel poll and pool (PnP) sampling module, with which we build an end-to-end PnP-DETR architecture that adaptively allocates its computation spatially to be more efficient. Concretely, the PnP module abstracts the image feature map into fine foreground object feature vectors and a small number of coarse background contextual feature vectors. The transformer models information interaction within the fine-coarse feature space and translates the features into the detection result. Moreover, the PnP-augmented model can instantly achieve various desired trade-offs between performance and computation with a single model by varying the sampled feature length, without requiring to train multiple models as existing methods. Thus it offers greater flexibility for deployment in diverse scenarios with varying computation constraint. We further validate the generalizability of the PnP module on panoptic segmentation and the recent transformer-based image recognition model ViT [7] and show consistent efficiency gain. We believe our method makes a step for efficient visual analysis with transformers, wherein spatial redundancy is commonly observed. Code and models will be available.},
  archive   = {C_ICCV},
  author    = {Tao Wang and Li Yuan and Yunpeng Chen and Jiashi Feng and Shuicheng Yan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00462},
  pages     = {4641-4650},
  title     = {PnP-DETR: Towards efficient visual analysis with transformers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cross-patch graph convolutional network for image denoising.
<em>ICCV</em>, 4631–4640. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, deep learning-based image denoising methods have achieved significant improvements over traditional methods. Due to the hardware limitation, most deep learning-based image denoising methods utilize cropped small patches to train a convolutional neural network to infer the clean images. However, the real noisy images in practical are mostly of high resolution rather than the cropped small patches and the vanilla training strategies ignore the cross-patch contextual dependency in the whole image. In this paper, we propose Cross-Patch Net (CPNet), which is the first deep-learning-based real image denoising method for HR (high resolution) input. Furthermore, we design a novel loss guided by the noise level map to obtain better performance. Compared with the vanilla patch-based training strategies, our approach effectively exploits the cross-patch contextual dependency. Besides, owing to the difficulty in capturing real noisy and noise-free image paired training data, we propose an effective method to generate realistic sRGB noisy images from their corresponding clean sRGB images for denoiser training. Denoising experiments on real-world sRGB images show the effectiveness of the proposed method. More importantly, our method achieves state-of-the-art performance on practical sRGB noisy image denoising.},
  archive   = {C_ICCV},
  author    = {Yao Li and Xueyang Fu and Zheng-Jun Zha},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00461},
  pages     = {4631-4640},
  title     = {Cross-patch graph convolutional network for image denoising},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rethinking coarse-to-fine approach in single image
deblurring. <em>ICCV</em>, 4621–4630. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Coarse-to-fine strategies have been extensively used for the architecture design of single image deblurring networks. Conventional methods typically stack sub-networks with multi-scale input images and gradually improve sharpness of images from the bottom sub-network to the top sub-network, yielding inevitably high computational costs. Toward a fast and accurate deblurring network design, we revisit the coarse-to-fine strategy and present a multi-input multi-output U-net (MIMO-UNet). The MIMO-UNet has three distinct features. First, the single encoder of the MIMO-UNet takes multi-scale input images to ease the difficulty of training. Second, the single decoder of the MIMO-UNet outputs multiple deblurred images with different scales to mimic multi-cascaded U-nets using a single U-shaped network. Last, asymmetric feature fusion is introduced to merge multi-scale features in an efficient manner. Extensive experiments on the GoPro and RealBlur datasets demonstrate that the proposed network outperforms the state-of-the-art methods in terms of both accuracy and computational complexity. Source code is available for research purposes at https://github.com/chosj95/MIMO-UNet.},
  archive   = {C_ICCV},
  author    = {Sung-Jin Cho and Seo-Won Ji and Jun-Pyo Hong and Seung-Won Jung and Sung-Jea Ko},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00460},
  pages     = {4621-4630},
  title     = {Rethinking coarse-to-fine approach in single image deblurring},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Overfitting the data: Compact neural video delivery via
content-aware feature modulation. <em>ICCV</em>, 4611–4620. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Internet video delivery has undergone a tremendous explosion of growth over the past few years. However, the quality of video delivery system greatly depends on the Internet bandwidth. Deep Neural Networks (DNNs) are utilized to improve the quality of video delivery recently. These methods divide a video into chunks, and stream LR video chunks and corresponding content-aware models to the client. The client runs the inference of models to super-resolve the LR chunks. Consequently, a large number of models are streamed in order to deliver a video. In this paper, we first carefully study the relation between models of different chunks, then we tactfully design a joint training framework along with the Content-aware Feature Modulation (CaFM) layer to compress these models for neural video delivery. With our method, each video chunk only requires less than 1\% of original parameters to be streamed, achieving even better SR performance. We conduct extensive experiments across various SR backbones, video time length, and scaling factors to demonstrate the advantages of our method. Besides, our method can be also viewed as a new approach of video coding. Our primary experiments achieve better video quality compared with the commercial H.264 and H.265 standard under the same storage cost, showing the great potential of the proposed method. Code is available at: https://github.com/Neural-video-delivery/ CaFM-Pytorch-ICCV2021},
  archive   = {C_ICCV},
  author    = {Jiaming Liu and Ming Lu and Kaixin Chen and Xiaoqi Li and Shizun Wang and Zhaoqing Wang and Enhua Wu and Yurong Chen and Chuang Zhang and Ming Wu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00459},
  pages     = {4611-4620},
  title     = {Overfitting the data: Compact neural video delivery via content-aware feature modulation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). RDI-net: Relational dynamic inference networks.
<em>ICCV</em>, 4601–4610. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dynamic inference networks, aimed at promoting computational efficiency, go along an adaptive executing path for a given sample. Prevalent methods typically assign a router for each convolutional block and sequentially make block-by-block executing decisions, without considering the relations during the dynamic inference. In this paper, we model the relations for dynamic inference from two aspects: the routers and the samples. We design a novel type of router called the relational router to model the relations among routers for a given sample. In principle, the current relational router aggregates the contextual features of preceding routers by graph convolution and propagates its router features to subsequent ones, making the executing decision for the current block in a long-range manner. Furthermore, we model the relation between samples by introducing a Sample Relation Module (SRM), encouraging correlated samples to go along correlated executing paths. As a whole, we call our method the Relational Dynamic Inference Network (RDI-Net). Extensive experiments on CIFAR-10/100 and ImageNet show that RDI-Net achieves state-of-the-art performance and computational cost reduction.},
  archive   = {C_ICCV},
  author    = {Huanyu Wang and Songyuan Li and Shihao Su and Zequn Qin and Xi Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00458},
  pages     = {4601-4610},
  title     = {RDI-net: Relational dynamic inference networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Low-rank tensor completion by approximating the tensor
average rank. <em>ICCV</em>, 4592–4600. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper focuses on the problem of low-rank tensor completion, the goal of which is to recover an underlying low-rank tensor from incomplete observations. Our method is motivated by the recently proposed t-product [8] based on any invertible linear transforms. First, we define the new tensor average rank under the invertible real linear trans-forms. We then propose a tensor completion model using a nonconvex surrogate to approximate the tensor average rank. This surrogate overcomes the discontinuity of the tensor average rank and alleviates the bias problem caused by the convex relaxation. Further, we develop an efficient algorithm to solve the proposed model and establish its convergence. Finally, experimental results on both synthetic and real data demonstrate the superiority of our method.},
  archive   = {C_ICCV},
  author    = {Zhanliang Wang and Junyu Dong and Xinguo Liu and Xueying Zeng},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00457},
  pages     = {4592-4600},
  title     = {Low-rank tensor completion by approximating the tensor average rank},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extensions of karger’s algorithm: Why they fail in theory
and how they are useful in practice. <em>ICCV</em>, 4582–4591. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The minimum graph cut and minimum s-t-cut problems are important primitives in the modeling of combinatorial problems in computer science, including in computer vision and machine learning. Some of the most efficient algorithms for finding global minimum cuts are randomized algorithms based on Karger’s groundbreaking contraction algorithm. Here, we study whether Karger’s algorithm can be successfully generalized to other cut problems. We first prove that a wide class of natural generalizations of Karger’s algorithm cannot efficiently solve the s-t-mincut or the normalized cut problem to optimality. However, we then present a simple new algorithm for seeded segmentation / graph-based semi-supervised learning that is closely based on Karger’s original algorithm, showing that for these problems, extensions of Karger’s algorithm can be useful. The new algorithm has linear asymptotic runtime and yields a potential that can be interpreted as the posterior probability of a sample belonging to a given seed / class. We clarify its relation to the random walker algorithm / harmonic energy minimization in terms of distributions over spanning forests. On classical problems from seeded image segmentation and graph-based semi-supervised learning on image data, the method performs at least as well as the random walker / harmonic energy minimization / Gaussian processes.},
  archive   = {C_ICCV},
  author    = {Erik Jenner and Enrique Fita Sanmartín and Fred A. Hamprecht},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00456},
  pages     = {4582-4591},
  title     = {Extensions of karger’s algorithm: Why they fail in theory and how they are useful in practice},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rethinking noise synthesis and modeling in raw denoising.
<em>ICCV</em>, 4573–4581. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The lack of large-scale real raw image denoising dataset gives rise to challenges on synthesizing realistic raw image noise for training denoising models. However, the real raw image noise is contributed by many noise sources and varies greatly among different sensors. Existing methods are unable to model all noise sources accurately, and building a noise model for each sensor is also laborious. In this paper, we introduce a new perspective to synthesize noise by directly sampling from the sensor’s real noise. It inherently generates accurate raw image noise for different camera sensors. Two efficient and generic techniques: pattern-aligned patch sampling and high-bit reconstruction help accurate synthesis of spatial-correlated noise and high-bit noise respectively. We conduct systematic experiments on SIDD and ELD datasets. The results show that (1) our method outperforms existing methods and demonstrates wide generalization on different sensors and lighting conditions. (2) Recent conclusions derived from DNN-based noise modeling methods are actually based on inaccurate noise parameters. The DNN-based methods still cannot outperform physics-based statistical methods. The code will be available at https://github.com/zhangyi-3/.},
  archive   = {C_ICCV},
  author    = {Yi Zhang and Hongwei Qin and Xiaogang Wang and Hongsheng Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00455},
  pages     = {4573-4581},
  title     = {Rethinking noise synthesis and modeling in raw denoising},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Score-based point cloud denoising. <em>ICCV</em>, 4563–4572.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Point clouds acquired from scanning devices are often perturbed by noise, which affects downstream tasks such as surface reconstruction and analysis. The distribution of a noisy point cloud can be viewed as the distribution of a set of noise-free samples p(x) convolved with some noise model n, leading to (p * n)(x) whose mode is the underlying clean surface. To denoise a noisy point cloud, we propose to increase the log-likelihood of each point from p * n via gradient ascent—iteratively updating each point’s position. Since p * n is unknown at test-time, and we only need the score (i.e., the gradient of the log-probability function) to perform gradient ascent, we propose a neural network architecture to estimate the score of p * n given only noisy point clouds as input. We derive objective functions for training the network and develop a denoising algorithm leveraging on the estimated scores. Experiments demonstrate that the proposed model outperforms state-of-the-art methods under a variety of noise models, and shows the potential to be applied in other tasks such as point cloud upsampling.},
  archive   = {C_ICCV},
  author    = {Shitong Luo and Wei Hu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00454},
  pages     = {4563-4572},
  title     = {Score-based point cloud denoising},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time video inference on edge devices via adaptive model
streaming. <em>ICCV</em>, 4552–4562. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-time video inference on edge devices like mobile phones and drones is challenging due to the high computation cost of Deep Neural Networks. We present Adaptive Model Streaming (AMS), a new approach to improving the performance of efficient lightweight models for video inference on edge devices. AMS uses a remote server to continually train and adapt a small model running on the edge device, boosting its performance on the live video using online knowledge distillation from a large, state-of-the-art model. We discuss the challenges of over-the-network model adaptation for video inference and present several techniques to reduce communication the cost of this approach: avoiding excessive overfitting, updating a small fraction of important model parameters, and adaptive sampling of training frames at edge devices. On the task of video semantic segmentation, our experimental results show 0.4–17.8 percent mean Intersection-over-Union improvement compared to a pretrained model across several video datasets. Our prototype can perform video segmentation at 30 frames-per-second with 40 milliseconds camera-to-label latency on a Samsung Galaxy S10+ mobile phone, using less than 300 Kbps uplink and downlink bandwidth on the device.},
  archive   = {C_ICCV},
  author    = {Mehrdad Khani and Pouya Hamadanian and Arash Nasr-Esfahany and Mohammad Alizadeh},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00453},
  pages     = {4552-4562},
  title     = {Real-time video inference on edge devices via adaptive model streaming},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Augmenting depth estimation with geospatial context.
<em>ICCV</em>, 4542–4551. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern cameras are equipped with a wide array of sensors that enable recording the geospatial context of an image. Taking advantage of this, we explore depth estimation under the assumption that the camera is geocalibrated, a problem we refer to as geo-enabled depth estimation. Our key insight is that if capture location is known, the corresponding overhead viewpoint offers a valuable resource for understanding the scale of the scene. We propose an end-to-end architecture for depth estimation that uses geospatial context to infer a synthetic ground-level depth map from a co-located overhead image, then fuses it inside of an encoder/decoder style segmentation network. To support evaluation of our methods, we extend a recently released dataset with overhead imagery and corresponding height maps. Results demonstrate that integrating geospatial context significantly reduces error compared to baselines, both at close ranges and when evaluating at much larger distances than existing benchmarks consider.},
  archive   = {C_ICCV},
  author    = {Scott Workman and Hunter Blanton},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00452},
  pages     = {4542-4551},
  title     = {Augmenting depth estimation with geospatial context},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust automatic monocular vehicle speed estimation for
traffic surveillance. <em>ICCV</em>, 4531–4541. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Even though CCTV cameras are widely deployed for traffic surveillance and have therefore the potential of becoming cheap automated sensors for traffic speed analysis, their large-scale usage toward this goal has not been reported yet. A key difficulty lies in fact in the camera calibration phase. Existing state-of-the-art methods perform the calibration using image processing or keypoint detection techniques that require high-quality video streams, yet typical CCTV footage is low-resolution and noisy. As a result, these methods largely fail in real-world conditions. In contrast, we propose two novel calibration techniques whose only inputs come from an off-the-shelf object detector. Both methods consider multiple detections jointly, leveraging the fact that cars have similar and well-known 3D shapes with normalized dimensions. The first one is based on minimizing an energy function corresponding to a 3D reprojection error, the second one instead learns from synthetic training data to predict the scene geometry directly. Noticing the lack of speed estimation benchmarks faithfully reflecting the actual quality of surveillance cameras, we introduce a novel dataset collected from public CCTV streams. Experimental results conducted on three diverse benchmarks demonstrate excellent speed estimation accuracy that could enable the wide use of CCTV cameras for traffic analysis, even in challenging conditions where state-of-the-art methods completely fail. Additional information can be found on our project web page: https://rebrand.ly/nle-cctv},
  archive   = {C_ICCV},
  author    = {Jerome Revaud and Martin Humenberger},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00451},
  pages     = {4531-4541},
  title     = {Robust automatic monocular vehicle speed estimation for traffic surveillance},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SUNet: Symmetric undistortion network for rolling shutter
correction. <em>ICCV</em>, 4521–4530. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The vast majority of modern consumer-grade cameras employ a rolling shutter mechanism, leading to image distortions if the camera moves during image acquisition. In this paper, we present a novel deep network to solve the generic rolling shutter correction problem with two consecutive frames. Our pipeline is symmetrically designed to predict the global shutter image corresponding to the intermediate time of these two frames, which is difficult for existing methods because it corresponds to a camera pose that differs most from the two frames. First, two time-symmetric dense undistortion flows are estimated by using well-established principles: pyramidal construction, warping, and cost volume processing. Then, both rolling shutter images are warped into a common global shutter one in the feature space, respectively. Finally, a symmetric consistency constraint is constructed in the image decoder to effectively aggregate the contextual cues of two rolling shutter images, thereby recovering the high-quality global shutter image. Extensive experiments with both synthetic and real data from public benchmarks demonstrate the superiority of our proposed approach over the state-of-the-art methods.},
  archive   = {C_ICCV},
  author    = {Bin Fan and Yuchao Dai and Mingyi He},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00450},
  pages     = {4521-4530},
  title     = {SUNet: Symmetric undistortion network for rolling shutter correction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Bringing events into video deblurring with non-consecutively
blurry frames. <em>ICCV</em>, 4511–4520. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, video deblurring has attracted considerable research attention, and several works suggest that events at high time rate can benefit deblurring. Existing video deblurring methods assume consecutively blurry frames, while neglecting the fact that sharp frames usually appear nearby blurry frame. In this paper, we develop a principled framework D 2 Nets for video deblurring to exploit non-consecutively blurry frames, and propose a flexible event fusion module (EFM) to bridge the gap between event-driven and video deblurring. In D 2 Nets, we propose to first detect nearest sharp frames (NSFs) using a bidirectional LST-M detector, and then perform deblurring guided by NSFs. Furthermore, the proposed EFM is flexible to be incorporated into D 2 Nets, in which events can be leveraged to notably boost the deblurring performance. EFM can also be easily incorporated into existing deblurring networks, making event-driven deblurring task benefit from state-of-the-art deblurring methods. On synthetic and real-world blurry datasets, our methods achieve better results than competing methods, and EFM not only benefits D 2 Nets but also significantly improves the competing deblurring networks.},
  archive   = {C_ICCV},
  author    = {Wei Shang and Dongwei Ren and Dongqing Zou and Jimmy S. Ren and Ping Luo and Wangmeng Zuo},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00449},
  pages     = {4511-4520},
  title     = {Bringing events into video deblurring with non-consecutively blurry frames},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient video compression via content-adaptive
super-resolution. <em>ICCV</em>, 4501–4510. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Video compression is a critical component of Internet video delivery. Recent work has shown that deep learning techniques can rival or outperform human-designed algorithms, but these methods are significantly less compute and power-efficient than existing codecs. This paper presents a new approach that augments existing codecs with a small, content-adaptive super-resolution model that significantly boosts video quality. Our method, SRVC, encodes video into two bitstreams: (i) a content stream, produced by compressing downsampled low-resolution video with the existing codec, (ii) a model stream, which encodes periodic updates to a lightweight super-resolution neural network customized for short segments of the video. SRVC decodes the video by passing the decompressed low-resolution video frames through the (time-varying) super-resolution model to reconstruct high-resolution video frames. Our results show that to achieve the same PSNR, SRVC requires 20\% of the bits-per-pixel of H.265 in slow mode, and 3\% of the bits-per-pixel of DVC, a recent deep learning-based video compression scheme. SRVC runs at 90 frames per second on an NVIDIA V100 GPU.},
  archive   = {C_ICCV},
  author    = {Mehrdad Khani and Vibhaalakshmi Sivaraman and Mohammad Alizadeh},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00448},
  pages     = {4501-4510},
  title     = {Efficient video compression via content-adaptive super-resolution},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ResRep: Lossless CNN pruning via decoupling remembering and
forgetting. <em>ICCV</em>, 4490–4500. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose ResRep, a novel method for lossless channel pruning (a.k.a. filter pruning), which slims down a CNN by reducing the width (number of output channels) of convolutional layers. Inspired by the neurobiology research about the independence of remembering and forgetting, we propose to re-parameterize a CNN into the remembering parts and forgetting parts, where the former learn to maintain the performance and the latter learn to prune. Via training with regular SGD on the former but a novel update rule with penalty gradients on the latter, we realize structured sparsity. Then we equivalently merge the remembering and forgetting parts into the original architecture with narrower layers. In this sense, ResRep can be viewed as a successful application of Structural Re-parameterization. Such a methodology distinguishes ResRep from the traditional learning-based pruning paradigm that applies a penalty on parameters to produce sparsity, which may suppress the parameters essential for the remembering. ResRep slims down a standard ResNet-50 with 76.15\% accuracy on ImageNet to a narrower one with only 45\% FLOPs and no accuracy drop, which is the first to achieve lossless pruning with such a high compression ratio. The code and models are at https://github.com/DingXiaoH/ResRep.},
  archive   = {C_ICCV},
  author    = {Xiaohan Ding and Tianxiang Hao and Jianchao Tan and Ji Liu and Jungong Han and Yuchen Guo and Guiguang Ding},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00447},
  pages     = {4490-4500},
  title     = {ResRep: Lossless CNN pruning via decoupling remembering and forgetting},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). A new journey from SDRTV to HDRTV. <em>ICCV</em>,
4480–4489. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Nowadays modern displays are capable to render video content with high dynamic range (HDR) and wide color gamut (WCG). However, most available resources are still in standard dynamic range (SDR). Therefore, there is an urgent demand to transform existing SDR-TV contents into their HDR-TV versions. In this paper, we conduct an analysis of SDRTV-to-HDRTV task by modeling the formation of SDRTV/HDRTV content. Base on the analysis, we propose a three-step solution pipeline including adaptive global color mapping, local enhancement and highlight generation. Moreover, the above analysis inspires us to present a lightweight network that utilizes global statistics as guidance to conduct image-adaptive color mapping. In addition, we construct a dataset using HDR videos in HDR10 standard, named HDRTV1K, and select five metrics to evaluate the results of SDRTV-to-HDRTV algorithms. Furthermore, our final results achieve state-of-the-art performance in quantitative comparisons and visual quality. The code and dataset are available at https://github.com/chxy95/HDRTVNet.},
  archive   = {C_ICCV},
  author    = {Xiangyu Chen and Zhengwen Zhang and Jimmy S. Ren and Lynhoo Tian and Yu Qiao and Chao Dong},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00446},
  pages     = {4480-4489},
  title     = {A new journey from SDRTV to HDRTV},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-conditioned probabilistic learning of video rescaling.
<em>ICCV</em>, 4470–4479. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bicubic downscaling is a prevalent technique used to reduce the video storage burden or to accelerate the downstream processing speed. However, the inverse upscaling step is non-trivial, and the downscaled video may also deteriorate the performance of downstream tasks. In this paper, we propose a self-conditioned probabilistic framework for video rescaling to learn the paired downscaling and upscaling procedures simultaneously. During the training, we decrease the entropy of the information lost in the downscaling by maximizing its probability conditioned on the strong spatial-temporal prior information within the downscaled video. After optimization, the downscaled video by our framework preserves more meaningful information, which is beneficial for both the upscaling step and the downstream tasks, e.g., video action recognition task. We further extend the framework to a lossy video compression system, in which a gradient estimator for non-differential industrial lossy codecs is proposed for the end-to-end training of the whole system. Extensive experimental results demonstrate the superiority of our approach on video rescaling, video compression, and efficient action recognition tasks.},
  archive   = {C_ICCV},
  author    = {Yuan Tian and Guo Lu and Xiongkuo Min and Zhaohui Che and Guangtao Zhai and Guodong Guo and Zhiyong Gao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00445},
  pages     = {4470-4479},
  title     = {Self-conditioned probabilistic learning of video rescaling},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Event stream super-resolution via spatiotemporal constraint
learning. <em>ICCV</em>, 4460–4469. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Event cameras are bio-inspired sensors that respond to brightness changes asynchronously and output in the form of event streams instead of frame-based images. They own outstanding advantages compared with traditional cameras: higher temporal resolution, higher dynamic range, and lower power consumption. However, the spatial resolution of existing event cameras is insufficient and challenging to be enhanced at the hardware level while maintaining the asynchronous philosophy of circuit design. Therefore, it is imperative to explore the algorithm of event stream super-resolution, which is a non-trivial task due to the sparsity and strong spatio-temporal correlation of the events from an event camera. In this paper, we propose an end-to-end framework based on spiking neural network for event stream super-resolution, which can generate high-resolution (HR) event stream from the input low-resolution (LR) event stream. A spatiotemporal constraint learning mechanism is proposed to learn the spatial and temporal distributions of the event stream simultaneously. We validate our method on four large-scale datasets and the results show that our method achieves state-of-the-art performance. The satisfying results on two downstream applications, i.e. object classification and image reconstruction, further demonstrate the usability of our method. To prove the application potential of our method, we deploy it on a mobile platform. The high-quality HR event stream generated by our real-time system demonstrates the effectiveness and efficiency of our method.},
  archive   = {C_ICCV},
  author    = {Siqi Li and Yutong Feng and Yipeng Li and Yu Jiang and Changqing Zou and Yue Gao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00444},
  pages     = {4460-4469},
  title     = {Event stream super-resolution via spatiotemporal constraint learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Super-resolving cross-domain face miniatures by peeking at
one-shot exemplar. <em>ICCV</em>, 4449–4459. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Conventional face super-resolution methods usually assume testing low-resolution (LR) images lie in the same domain as the training ones. Due to different lighting conditions and imaging hardware, domain gaps between training and testing images inevitably occur in many real-world scenarios. Neglecting those domain gaps would lead to inferior face super-resolution (FSR) performance. However, how to transfer a trained FSR model to a target domain efficiently and effectively has not been investigated. To tackle this problem, we develop a Domain-Aware Pyramid-based Face Super-Resolution network, named DAP-FSR network. Our DAP-FSR makes the first attempt to super-resolve LR faces from a target domain by exploiting only a pair of high-resolution (HR) and LR exemplar in the target domain. To be specific, our DAP-FSR firstly employs its encoder to extract the multi-scale latent representations of the input LR face. Considering only one target domain example is available, we propose to augment the target domain data by mixing the latent representations of the target domain face and source domain ones, and then feed the mixed representations to the decoder of our DAP-FSR. The decoder will generate new face images resembling the target domain image style. The generated HR faces in turn are used to optimize our decoder to reduce the domain gap. By iteratively updating the latent representations and our decoder, our DAP-FSR will be adapted to the target domain, thus achieving authentic and high-quality upsampled HR faces. Extensive experiments on three benchmarks validate the effectiveness and superior performance of our DAP-FSR compared to the state-of-the-art methods.},
  archive   = {C_ICCV},
  author    = {Peike Li and Xin Yu and Yi Yang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00443},
  pages     = {4449-4459},
  title     = {Super-resolving cross-domain face miniatures by peeking at one-shot exemplar},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Representative color transform for image enhancement.
<em>ICCV</em>, 4439–4448. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, the encoder-decoder and intensity transformation approaches lead to impressive progress in image enhancement. However, the encoder-decoder often loses details in input images during down-sampling and up-sampling processes. Also, the intensity transformation has a limited capacity to cover color transformation between low-quality and high-quality images. In this paper, we propose a novel approach, called representative color transform (RCT), to tackle these issues in existing methods. RCT determines different representative colors specialized in input images and estimates transformed colors for the representative colors. It then determines enhanced colors using these transformed colors based on the similarity between input and representative colors. Extensive experiments demonstrate that the proposed algorithm outperforms recent state-of-the-art algorithms on various image enhancement problems.},
  archive   = {C_ICCV},
  author    = {Hanul Kim and Su-Min Choi and Chang-Su Kim and Yeong Jun Koh},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00442},
  pages     = {4439-4448},
  title     = {Representative color transform for image enhancement},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ultra-high-definition image HDR reconstruction via
collaborative bilateral learning. <em>ICCV</em>, 4429–4438. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing single image high dynamic range (HDR) reconstruction methods attempt to expand the range of illuminance. They are not effective in generating plausible textures and colors in the reconstructed results, especially for high-density pixels in ultra-high-definition (UHD) images. To address these problems, we propose a new HDR reconstruction network for UHD images by collaboratively learning color and texture details. First, we propose a dual-path network to extract the content and chromatic features at a reduced resolution of the low dynamic range (LDR) input. These two types of features are used to fit bilateral-space affine models for real-time HDR reconstruction. To extract the main data structure of the LDR input, we propose to use 3D Tucker decomposition and reconstruction to prevent pseudo edges and noise amplification in the learned bilateral grid. As a result, the high-quality content and chromatic features can be reconstructed capitalized on guided bilateral upsampling. Finally, we fuse these two full-resolution feature maps into the HDR reconstructed results. Our proposed method can achieve real-time processing for UHD images (about 160 fps). Experimental results demonstrate that the proposed algorithm performs favorably against the state-of-the-art HDR reconstruction approaches on public benchmarks and real-world UHD images.},
  archive   = {C_ICCV},
  author    = {Zhuoran Zheng and Wenqi Ren and Xiaochun Cao and Tao Wang and Xiuyi Jia},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00441},
  pages     = {4429-4438},
  title     = {Ultra-high-definition image HDR reconstruction via collaborative bilateral learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive unfolding total variation network for low-light
image enhancement. <em>ICCV</em>, 4419–4428. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-world low-light images suffer from two main degradations, namely, inevitable noise and poor visibility. Since the noise exhibits different levels, its estimation has been implemented in recent works when enhancing low-light images from raw Bayer space. When it comes to sRGB color space, the noise estimation becomes more complicated due to the effect of the image processing pipeline. Nevertheless, most existing enhancing algorithms in sRGB space only focus on the low visibility problem or suppress the noise under a hypothetical noise level, leading them impractical due to the lack of robustness. To address this issue, we propose an adaptive unfolding total variation network (UTVNet), which approximates the noise level from the real sRGB low-light image by learning the balancing parameter in the model-based denoising method with total variation regularization. Meanwhile, we learn the noise level map by unrolling the corresponding minimization process for providing the inferences of smoothness and fidelity constraints. Guided by the noise level map, our UTVNet can recover finer details and is more capable to suppress noise in real captured low-light scenes. Extensive experiments on real-world low-light images clearly demonstrate the superior performance of UTVNet over state-of-the-art methods.},
  archive   = {C_ICCV},
  author    = {Chuanjun Zheng and Daming Shi and Wentian Shi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00440},
  pages     = {4419-4428},
  title     = {Adaptive unfolding total variation network for low-light image enhancement},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Omniscient video super-resolution. <em>ICCV</em>, 4409–4418.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most recent video super-resolution (SR) methods either adopt an iterative manner to deal with low-resolution (LR) frames from a temporally sliding window, or leverage the previously estimated SR output to help reconstruct the current frame recurrently. A few studies try to combine these two structures to form a hybrid framework but have failed to give full play to it. In this paper, we propose an omniscient framework to not only utilize the preceding SR output, but also leverage the SR outputs from the present and future. The omniscient framework is more generic because the iterative, recurrent and hybrid frameworks can be regarded as its special cases. The proposed omniscient framework enables a generator to behave better than its counterparts under other frameworks. Abundant experiments on public datasets show that our method is superior to the state-of-the-art methods in objective metrics, subjective visual effects and complexity.},
  archive   = {C_ICCV},
  author    = {Peng Yi and Zhongyuan Wang and Kui Jiang and Junjun Jiang and Tao Lu and Xin Tian and Jiayi Ma},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00439},
  pages     = {4409-4418},
  title     = {Omniscient video super-resolution},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Federated learning for non-IID data via unified feature
learning and optimization objective alignment. <em>ICCV</em>, 4400–4408.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Federated Learning (FL) aims to establish a shared model across decentralized clients under the privacy-preserving constraint. Despite certain success, it is still challenging for FL to deal with non-IID (non-independent and identical distribution) client data, which is a general scenario in real-world FL tasks. It has been demonstrated that the performance of FL will be reduced greatly under the non-IID scenario, since the discrepant data distributions will induce optimization inconsistency and feature divergence issues. Besides, naively minimizing an aggregate loss function in this scenario may have negative impacts on some clients and thus deteriorate their personal model performance. To address these issues, we propose a Unified Feature learning and Optimization objectives alignment method (FedUFO) for non-IID FL. In particular, an adversary module is proposed to reduce the divergence on feature representation among different clients, and two consensus losses are proposed to reduce the inconsistency on optimization objectives from two perspectives. Extensive experiments demonstrate that our FedUFO can outperform the state-of-the-art approaches, including the competitive one data-sharing method. Besides, FedUFO can enable more reasonable and balanced model performance among different clients.},
  archive   = {C_ICCV},
  author    = {Lin Zhang and Yong Luo and Yan Bai and Bo Du and Ling-Yu Duan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00438},
  pages     = {4400-4408},
  title     = {Federated learning for non-IID data via unified feature learning and optimization objective alignment},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MixMix: All you need for data-free compression are feature
and data mixing. <em>ICCV</em>, 4390–4399. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {User data confidentiality protection is becoming a rising challenge in the present deep learning research. Without access to data, conventional data-driven model compression faces a higher risk of performance degradation. Recently, some works propose to generate images from a specific pretrained model to serve as training data. However, the inversion process only utilizes biased feature statistics stored in one model and is from low-dimension to high-dimension. As a consequence, it inevitably encounters the difficulties of generalizability and inexact inversion, which leads to unsatisfactory performance. To address these problems, we propose MixMix based on two simple yet effective techniques: (1) Feature Mixing: utilizes various models to construct a universal feature space for generalized inversion; (2) Data Mixing: mixes the synthesized images and labels to generate exact label information. We prove the effectiveness of MixMix from both theoretical and empirical perspectives. Extensive experiments show that MixMix outperforms existing methods on the mainstream compression tasks, including quantization, knowledge distillation and pruning. Specifically, MixMix achieves up to 4\% and 20\% accuracy uplift on quantization and pruning, respectively, compared to existing data-free compression work.},
  archive   = {C_ICCV},
  author    = {Yuhang Li and Feng Zhu and Ruihao Gong and Mingzhu Shen and Xin Dong and Fengwei Yu and Shaoqing Lu and Shi Gu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00437},
  pages     = {4390-4399},
  title     = {MixMix: All you need for data-free compression are feature and data mixing},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Zero-shot day-night domain adaptation with a physics prior.
<em>ICCV</em>, 4379–4389. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We explore the zero-shot setting for day-night domain adaptation. The traditional domain adaptation setting is to train on one domain and adapt to the target domain by exploiting unlabeled data samples from the test set. As gathering relevant test data is expensive and sometimes even impossible, we remove any reliance on test data imagery and instead exploit a visual inductive prior derived from physics-based reflection models for domain adaptation. We cast a number of color invariant edge detectors as trainable layers in a convolutional neural network and evaluate their robustness to illumination changes. We show that the color invariant layer reduces the day-night distribution shift in feature map activations throughout the network. We demonstrate improved performance for zero-shot day to night domain adaptation on both synthetic as well as natural datasets in various tasks, including classification, segmentation and place recognition.},
  archive   = {C_ICCV},
  author    = {Attila Lengyel and Sourav Garg and Michael Milford and Jan C. van Gemert},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00436},
  pages     = {4379-4389},
  title     = {Zero-shot day-night domain adaptation with a physics prior},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-level curriculum for training a distortion-aware
barrel distortion rectification model. <em>ICCV</em>, 4369–4378. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Barrel distortion rectification aims at removing the radial distortion in a distorted image captured by a wide-angle lens. Previous deep learning methods mainly solve this problem by learning the implicit distortion parameters or the nonlinear rectified mapping function in a direct manner. However, this type of manner results in an indistinct learning process of rectification and thus limits the deep perception of distortion. In this paper, inspired by the curriculum learning, we analyze the barrel distortion rectification task in a progressive and meaningful manner. By considering the relationship among different construction levels in an image, we design a multi-level curriculum that disassembles the rectification task into three levels, structure recovery, semantics embedding, and texture rendering. With the guidance of the curriculum that corresponds to the construction of images, the proposed hierarchical architecture enables a progressive rectification and achieves more accurate results. Moreover, we present a novel distortion-aware pre-training strategy to facilitate the initial learning of neural networks, promoting the model to converge faster and better. Experimental results on the synthesized and real-world distorted image datasets show that the proposed approach significantly outperforms other learning methods, both qualitatively and quantitatively.},
  archive   = {C_ICCV},
  author    = {Kang Liao and Chunyu Lin and Lixin Liao and Yao Zhao and Weiyao Lin},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00435},
  pages     = {4369-4378},
  title     = {Multi-level curriculum for training a distortion-aware barrel distortion rectification model},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Equivariant imaging: Learning beyond the range space.
<em>ICCV</em>, 4359–4368. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In various imaging problems, we only have access to compressed measurements of the underlying signals, hindering most learning-based strategies which usually require pairs of signals and associated measurements for training Learning only from compressed measurements is impossible in general, as the compressed observations do not contain information outside the range of the forward sensing operator. We propose a new end-to-end self-supervised framework that overcomes this limitation by exploiting the equivariances present in natural signals. Our proposed learning strategy performs as well as fully supervised methods. Experiments demonstrate the potential of this frame- work on inverse problems including sparse-view X-ray computed tomography on real clinical data and image inpainting on natural images. Code has been made available at: https://github.com/edongdongchen/EI.},
  archive   = {C_ICCV},
  author    = {Dongdong Chen and Julián Tachella and Mike E. Davies},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00434},
  pages     = {4359-4368},
  title     = {Equivariant imaging: Learning beyond the range space},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning unsupervised metaformer for anomaly detection.
<em>ICCV</em>, 4349–4358. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Anomaly detection (AD) aims to address the task of classification or localization of image anomalies. This paper addresses two pivotal issues of reconstruction-based approaches to AD in images, namely, model adaptation and reconstruction gap. The former generalizes an AD model to tackling a broad range of object categories, while the latter provides useful clues for localizing abnormal regions. At the core of our method is an unsupervised universal model, termed as Metaformer, which leverages both meta-learned model parameters to achieve high model adaptation capability and instance-aware attention to emphasize the focal regions for localizing abnormal regions, i.e., to explore the reconstruction gap at those regions of interest. We justify the effectiveness of our method with SOTA results on the MVTec AD dataset of industrial images and highlight the adaptation flexibility of the universal Metaformer with multi-class and few-shot scenarios.},
  archive   = {C_ICCV},
  author    = {Jhih-Ciang Wu and Ding-Jie Chen and Chiou-Shann Fuh and Tyng-Luh Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00433},
  pages     = {4349-4358},
  title     = {Learning unsupervised metaformer for anomaly detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Deep structured instance graph for distilling object
detectors. <em>ICCV</em>, 4339–4348. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Effectively structuring deep knowledge plays a pivotal role in transfer from teacher to student, especially in semantic vision tasks. In this paper, we present a simple knowledge structure to exploit and encode information inside the detection system to facilitate detector knowledge distillation. Specifically, aiming at solving the feature imbalance problem while further excavating the missing relation inside semantic instances, we design a graph whose nodes correspond to instance proposal-level features and edges represent the relation between nodes. To further refine this graph, we design an adaptive background loss weight to reduce node noise and background samples mining to prune trivial edges. We transfer the entire graph as encoded knowledge representation from teacher to student, capturing local and global information simultaneously.We achieve new state-of-the-art results on the challenging COCO object detection task with diverse student-teacher pairs on both one- and two-stage detectors. We also experiment with instance segmentation to demonstrate robustness of our method. It is notable that distilled Faster R-CNN with ResNet18-FPN and ResNet50-FPN yields 38.68 and 41.82 Box AP respectively on the COCO benchmark, Faster R-CNN with ResNet101-FPN significantly achieves 43.38 AP, which outperforms ResNet152- FPN teacher about 0.7 AP. Code: https://github.com/dvlab-research/Dsig.},
  archive   = {C_ICCV},
  author    = {Yixin Chen and Pengguang Chen and Shu Liu and Liwei Wang and Jiaya Jia},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00432},
  pages     = {4339-4348},
  title     = {Deep structured instance graph for distilling object detectors},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning RAW-to-sRGB mappings with inaccurately aligned
supervision. <em>ICCV</em>, 4328–4338. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning RAW-to-sRGB mapping has drawn increasing attention in recent years, wherein an input raw image is trained to imitate the target sRGB image captured by another camera. However, the severe color inconsistency makes it very challenging to generate well-aligned training pairs of input raw and target sRGB images. While learning with inaccurately aligned supervision is prone to causing pixel shift and producing blurry results. In this paper, we circumvent such issue by presenting a joint learning model for image alignment and RAW-to-sRGB mapping. To diminish the effect of color inconsistency in image alignment, we introduce to use a global color mapping (GCM) module to generate an initial sRGB image given the input raw image, which can keep the spatial location of the pixels unchanged, and the target sRGB image is utilized to guide GCM for converting the color towards it. Then a pre-trained optical flow estimation network (e.g., PWC-Net) is deployed to warp the target sRGB image to align with the GCM output. To alleviate the effect of inaccurately aligned supervision, the warped target sRGB image is leveraged to learn RAW-to-sRGB mapping. When training is done, the GCM module and optical flow network can be detached, thereby bringing no extra computation cost for inference. Experiments show that our method performs favorably against state-of-the-arts on ZRR and SR-RAW datasets. With our joint learning model, a light-weight backbone can achieve better quantitative and qualitative performance on ZRR dataset. Codes are available at https://github.com/cszhilu1998/RAW-to-sRGB.},
  archive   = {C_ICCV},
  author    = {Zhilu Zhang and Haolin Wang and Ming Liu and Ruohao Wang and Jiawei Zhang and Wangmeng Zuo},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00431},
  pages     = {4328-4338},
  title     = {Learning RAW-to-sRGB mappings with inaccurately aligned supervision},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RGB-d saliency detection via cascaded mutual information
minimization. <em>ICCV</em>, 4318–4327. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing RGB-D saliency detection models do not explicitly encourage RGB and depth to achieve effective multi-modal learning. In this paper, we introduce a novel multistage cascaded learning framework via mutual information minimization to explicitly model the multi-modal information between RGB image and depth data. Specifically, we first map the feature of each mode to a lower dimensional feature vector, and adopt mutual information minimization as a regularizer to reduce the redundancy between appearance features from RGB and geometric features from depth. We then perform multi-stage cascaded learning to impose the mutual information minimization constraint at every stage of the network. Extensive experiments on benchmark RGB-D saliency datasets illustrate the effectiveness of our framework. Further, to prosper the development of this field, we contribute the largest (7× larger than NJU2K) COME15K dataset, which contains 15,625 image pairs with high quality polygon-/scribble-/object-/instance-/rank-level annotations. Based on these rich labels, we additionally construct four new benchmarks with strong baselines and observe some interesting phenomena, which can motivate future model design. Source code and dataset are available at https://github.com/JingZhang617/cascaded_rgbd_sod.},
  archive   = {C_ICCV},
  author    = {Jing Zhang and Deng-Ping Fan and Yuchao Dai and Xin Yu and Yiran Zhong and Nick Barnes and Ling Shao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00430},
  pages     = {4318-4327},
  title     = {RGB-D saliency detection via cascaded mutual information minimization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic attentive graph learning for image restoration.
<em>ICCV</em>, 4308–4317. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Non-local self-similarity in natural images has been verified to be an effective prior for image restoration. However, most existing deep non-local methods assign a fixed number of neighbors for each query item, neglecting the dynamics of non-local correlations. Moreover, the non-local correlations are usually based on pixels, prone to be biased due to image degradation. To rectify these weaknesses, in this paper, we propose a dynamic attentive graph learning model (DAGL) to explore the dynamic non-local property on patch level for image restoration. Specifically, we propose an improved graph model to perform patch-wise graph convolution with a dynamic and adaptive number of neighbors for each node. In this way, image content can adaptively balance over-smooth and over-sharp artifacts through the number of its connected neighbors, and the patch-wise non-local correlations can enhance the message passing process. Experimental results on various image restoration tasks: synthetic image denoising, real image denoising, image demosaicing, and compression artifact reduction show that our DAGL can produce state-of-the-art results with superior accuracy and visual quality. The source code is available at https://github.com/jianzhangcs/DAGL.},
  archive   = {C_ICCV},
  author    = {Chong Mou and Jian Zhang and Zhuoyuan Wu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00429},
  pages     = {4308-4317},
  title     = {Dynamic attentive graph learning for image restoration},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised real-world super-resolution: A domain
adaptation perspective. <em>ICCV</em>, 4298–4307. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most existing convolution neural network (CNN) based super-resolution (SR) methods generate their paired training dataset by artificially synthesizing low-resolution (LR) images from the high-resolution (HR) ones. However, this dataset preparation strategy harms the application of these CNNs in real-world scenarios due to the inherent domain gap between the training and testing data. A popular attempts towards the challenge is unpaired generative adversarial networks, which generate &quot;real&quot; LR counterparts from real HR images using image-to-image translation and then perform super-resolution from &quot;real&quot; LR→SR. Despite great progress, it is still difficult to synthesize perfect &quot;real&quot; LR images for super-resolution. In this paper, we firstly consider the real-world SR problem from the traditional domain adaptation perspective. We propose a novel unpaired SR training framework based on feature distribution alignment, with which we can obtain degradation-indistinguishable feature maps and then map them to HR images. In order to generate better SR images for target LR domain, we introduce several regularization losses to force the aligned feature to locate around the target domain. Our experiments indicate that our SR network obtains the state-of-the-art performance over both blind and unpaired SR methods on diverse datasets.},
  archive   = {C_ICCV},
  author    = {Wei Wang and Haochen Zhang and Zehuan Yuan and Changhu Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00428},
  pages     = {4298-4307},
  title     = {Unsupervised real-world super-resolution: A domain adaptation perspective},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning frequency-aware dynamic network for efficient
super-resolution. <em>ICCV</em>, 4288–4297. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep learning based methods, especially convolutional neural networks (CNNs) have been successfully applied in the field of single image super-resolution (SISR). To obtain better fidelity and visual quality, most of existing networks are of heavy design with massive computation. However, the computation resources of modern mobile devices are limited, which cannot easily support the expensive cost. To this end, this paper explores a novel frequency-aware dynamic network for dividing the input into multiple parts according to its coefficients in the discrete cosine transform (DCT) domain. In practice, the high-frequency part will be processed using expensive operations and the lower-frequency part is assigned with cheap operations to relieve the computation burden. Since pixels or image patches belong to low-frequency areas contain relatively few textural details, this dynamic network will not affect the quality of resulting super-resolution images. In addition, we embed predictors into the proposed dynamic network to end-to-end fine-tune the handcrafted frequency-aware masks. Extensive experiments conducted on benchmark SISR models and datasets show that the frequency-aware dynamic network can be employed for various SISR neural architectures to obtain the better tradeoff between visual quality and computational complexity. For instance, we can reduce the FLOPs of SR models by approximate 50\% while preserving state-of-the-art SISR performance.},
  archive   = {C_ICCV},
  author    = {Wenbin Xie and Dehua Song and Chang Xu and Chunjing Xu and Hui Zhang and Yunhe Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00427},
  pages     = {4288-4297},
  title     = {Learning frequency-aware dynamic network for efficient super-resolution},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pyramid architecture search for real-time image deblurring.
<em>ICCV</em>, 4278–4287. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-scale and multi-patch deep models have been shown effective in removing blurs of dynamic scenes. However, these methods still suffer from one major obstacle: manually designing a lightweight and high-efficiency network is challenging and time-consuming. To tackle this obstacle, we propose a novel deblurring method, dubbed PyNAS (pyramid neural architecture search network), towards automatically designing hyper-parameters including the scales, patches, and standard cell operators. The proposed PyNAS adopts gradient-based search strategies and innovatively searches the hierarchy patch and scale scheme not limited to cell searching. Specifically, we introduce a hierarchical search strategy tailored to the multi-scale and multi-patch deblurring task. The strategy follows the principle that the first distinguishes between the top-level (pyramid-scales and pyramid-patches) and bottom-level variables (cell operators) and then searches multi-scale variables using the top-to-bottom principle. During the search stage, PyNAS employs an early stopping strategy to avoid the collapse and computational issues. Furthermore, we use a path-level binarization mechanism for multi-scale cell searching to save the memory consumption. Our primary contribution is a real-time deblurring algorithm (around 58 fps) for 720p images while achieves state-of-the-art deblurring performance on the GoPro and Video Deblurring datasets.},
  archive   = {C_ICCV},
  author    = {Xiaobin Hu and Wenqi Ren and Kaicheng Yu and Kaihao Zhang and Xiaochun Cao and Wei Liu and Bjoern Menze},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00426},
  pages     = {4278-4287},
  title     = {Pyramid architecture search for real-time image deblurring},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic high-pass filtering and multi-spectral attention for
image super-resolution. <em>ICCV</em>, 4268–4277. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep convolutional neural networks (CNNs) have pushed forward the frontier of super-resolution (SR) research. However, current CNN models exhibit a major flaw: they are biased towards learning low-frequency signals. This bias becomes more problematic for the image SR task which targets reconstructing all fine details and image textures. To tackle this challenge, we propose to improve the learning of high-frequency features both locally and globally and introduce two novel architectural units to existing SR models. Specifically, we propose a dynamic high-pass filtering (HPF) module that locally applies adaptive filter weights for each spatial location and channel group to preserve high-frequency signals. We also propose a matrix multi-spectral channel attention (MMCA) module that predicts the attention map of features decomposed in the frequency domain. This module operates in a global context to adaptively recalibrate feature responses at different frequencies. Extensive qualitative and quantitative results demonstrate that our proposed modules achieve better accuracy and visual improvements against state-of-the-art methods on several benchmark datasets.},
  archive   = {C_ICCV},
  author    = {Salma Abdel Magid and Yulun Zhang and Donglai Wei and Won-Dong Jang and Zudi Lin and Yun Fu and Hanspeter Pfister},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00425},
  pages     = {4268-4277},
  title     = {Dynamic high-pass filtering and multi-spectral attention for image super-resolution},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Context reasoning attention network for image
super-resolution. <em>ICCV</em>, 4258–4267. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep convolutional neural networks (CNNs) are achieving great successes for image super-resolution (SR), where global context is crucial for accurate restoration. However, the basic convolutional layer in CNNs is designed to extract local patterns, lacking the ability to model global context. With global context information, lots of efforts have been devoted to augmenting SR networks, especially by global feature interaction methods. These works incorporate the global context into local feature representation. However, recent advances in neuroscience show that it is necessary for the neurons to dynamically modulate their functions according to context, which is neglected in most CNN based SR methods. Motivated by those observations and analyses, we propose context reasoning attention network (CRAN) to modulate the convolution kernel according to the global context adaptively. Specifically, we extract global context descriptors, which are further enhanced with semantic reasoning. Channel and spatial interactions are then introduced to generate context reasoning attention mask, which is applied to modify the convolution kernel adaptively. Such a modulated convolution layer is utilized as basic component to build the blocks and networks. Extensive experiments on benchmark datasets with multiple degradation models show that CRAN obtains superior results and favorable trade-off between performance and model complexity.},
  archive   = {C_ICCV},
  author    = {Yulun Zhang and Donglai Wei and Can Qin and Huan Wang and Hanspeter Pfister and Yun Fu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00424},
  pages     = {4258-4267},
  title     = {Context reasoning attention network for image super-resolution},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). End-to-end piece-wise unwarping of document images.
<em>ICCV</em>, 4248–4257. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Document unwarping attempts to undo physical deformations of the paper and recover a ’flatbed’ scanned document-image for downstream tasks such as OCR. Current state-of-the-art relies on global unwarping of the document which is not robust to local deformation changes. Moreover, a global unwarping often produces spurious warping artifacts in less warped regions to compensate for severe warps present in other parts of the document. In this paper, we propose the first end-to-end trainable piece-wise unwarping 1 method that predicts local deformation fields and stitches them together with global information to obtain an improved unwarping. The proposed piece-wise formulation results in 4\% improvement in terms of multi-scale structural similarity (MS-SSIM) and shows better performance in terms of OCR metrics, character error rate (CER) and word error rate (WER) compared to the state-of-the-art.},
  archive   = {C_ICCV},
  author    = {Sagnik Das and Kunwar Yashraj Singh and Jon Wu and Erhan Bas and Vijay Mahadevan and Rahul Bhotika and Dimitris Samaras},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00423},
  pages     = {4248-4257},
  title     = {End-to-end piece-wise unwarping of document images},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Event-intensity stereo: Estimating depth by the best of both
worlds. <em>ICCV</em>, 4238–4247. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Event cameras can report scene movements as an asynchronous stream of data called the events. Unlike traditional cameras, event cameras have very low latency (microseconds vs milliseconds) very high dynamic range (140 dB vs 60 dB), and low power consumption, as they report changes of a scene and not a complete frame. As they re- port per pixel feature-like events and not the whole intensity frame they are immune to motion blur. However, event cameras require movement between the scene and camera to fire events, i.e., they have no output when the scene is relatively static. Traditional cameras, however, report the whole frame of pixels at once in fixed intervals but have lower dynamic range and are prone to motion blur in case of rapid movements. We get the best from both worlds and use events and intensity images together in our complementary design and estimate dense disparity from this combination. The proposed end-to-end design combines events and images in a sequential manner and correlates them to estimate dense depth values. Our various experimental settings in real-world and simulated scenarios exploit the superiority of our method in predicting accurate depth values with fine details. We further extend our method to extreme cases of missing the left or right event or stereo pair and also investigate stereo depth estimation with inconsistent dynamic ranges or event thresholds on the left and right pairs.},
  archive   = {C_ICCV},
  author    = {S. Mohammad Mostafavi I and Kuk-Jin Yoon and Jonghyun Choi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00422},
  pages     = {4238-4247},
  title     = {Event-intensity stereo: Estimating depth by the best of both worlds},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ReconfigISP: Reconfigurable camera image processing
pipeline. <em>ICCV</em>, 4228–4237. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Image Signal Processor (ISP) is a crucial component in digital cameras that transforms sensor signals into images for us to perceive and understand. Existing ISP designs always adopt a fixed architecture, e.g., several sequential modules connected in a rigid order. Such a fixed ISP architecture may be suboptimal for real-world applications, where camera sensors, scenes and tasks are diverse. In this study, we propose a novel Reconfigurable ISP (ReconfigISP) whose architecture and parameters can be automatically tailored to specific data and tasks. In particular, we implement several ISP modules, and enable back-propagation for each module by training a differentiable proxy, hence allowing us to leverage the popular differentiable neural architecture search and effectively search for the optimal ISP architecture. A proxy tuning mechanism is adopted to maintain the accuracy of proxy networks in all cases. Extensive experiments conducted on image restoration and object detection, with different sensors, light conditions and efficiency constraints, validate the effectiveness of ReconfigISP. Only hundreds of parameters need tuning for every task 1 .},
  archive   = {C_ICCV},
  author    = {Ke Yu and Zexian Li and Yue Peng and Chen Change Loy and Jinwei Gu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00421},
  pages     = {4228-4237},
  title     = {ReconfigISP: Reconfigurable camera image processing pipeline},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Structure-preserving deraining with residue channel prior
guidance. <em>ICCV</em>, 4218–4227. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Single image deraining is important for many high-level computer vision tasks since the rain streaks can severely degrade the visibility of images, thereby affecting the recognition and analysis of the image. Recently, many CNN-based methods have been proposed for rain removal. Although these methods can remove part of the rain streaks, it is difficult for them to adapt to real-world scenarios and restore high-quality rain-free images with clear and accurate structures. To solve this problem, we propose a Structure-Preserving Deraining Network (SPDNet) with RCP guidance. SPDNet directly generates high-quality rain-free images with clear and accurate structures under the guidance of RCP but does not rely on any rain-generating assumptions. Specifically, we found that the RCP of images contains more accurate structural information than rainy images. Therefore, we introduced it to our deraining network to protect structure information of the rain-free image. Meanwhile, a Wavelet-based Multi-Level Module (WMLM) is proposed as the backbone for learning the background information of rainy images and an Interactive Fusion Module (IFM) is designed to make full use of RCP information. In addition, an iterative guidance strategy is proposed to gradually improve the accuracy of RCP, refining the result in a progressive path. Extensive experimental results on both synthetic and real-world datasets demonstrate that the proposed model achieves new state-of-the-art results. Code: https://github.com/Joyies/SPDNet},
  archive   = {C_ICCV},
  author    = {Qiaosi Yi and Juncheng Li and Qinyan Dai and Faming Fang and Guixu Zhang and Tieyong Zeng},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00420},
  pages     = {4218-4227},
  title     = {Structure-preserving deraining with residue channel prior guidance},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inverting a rolling shutter camera: Bring rolling shutter
images to high framerate global shutter video. <em>ICCV</em>, 4208–4217.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Rolling shutter (RS) images can be viewed as the result of the row-wise combination of global shutter (GS) images captured by a virtual moving GS camera over the period of camera readout time. The RS effect brings tremendous difficulties for the downstream applications. In this paper, we propose to invert the above RS imaging mechanism, i.e., recovering a high framerate GS video from consecutive RS images to achieve RS temporal super-resolution (RSSR). This extremely challenging problem, e.g., recovering 1440 GS images from two 720-height RS images, is far from being solved end-to-end. To address this challenge, we exploit the geometric constraint in the RS camera model, thus achieving geometry-aware inversion. Specifically, we make three contributions in resolving the above difficulties: (i) formulating the bidirectional RS undistortion flows under the constant velocity motion model, (ii) building the connection between the RS undistortion flow and optical flow via a scaling operation, and (iii) developing a mutual conversion scheme between varying RS undistortion flows that correspond to different scanlines. Building upon these formulations, we propose the first RS temporal super-resolution network in a cascaded structure to extract high framerate global shutter video. Our method explores the underlying spatio-temporal geometric relationships within a deep learning framework, where no extra supervision besides the middle-scanline ground truth GS image is needed. Essentially, our method can be very efficient for explicit propagation to generate GS images under any scanline. Experimental results on both synthetic and real data show that our method can produce high-quality GS image sequences with rich details, outperforming state-of-the-art methods.},
  archive   = {C_ICCV},
  author    = {Bin Fan and Yuchao Dai},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00419},
  pages     = {4208-4217},
  title     = {Inverting a rolling shutter camera: Bring rolling shutter images to high framerate global shutter video},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TransView: Inside, outside, and across the cropping view
boundaries. <em>ICCV</em>, 4198–4207. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We show that relation modeling between visual elements matters in cropping view recommendation. Cropping view recommendation addresses the problem of image recomposition conditioned on the composition quality and the ranking of views (cropped sub-regions). This task is challenging because the visual difference is subtle when a visual element is reserved or removed. Existing methods represent visual elements by extracting region-based convolutional features inside and outside the cropping view boundaries, without probing a fundamental question: why some visual elements are of interest or of discard? In this work, we observe that the relation between different visual elements significantly affects their relative positions to the desired cropping view, and such relation can be characterized by the attraction inside/outside the cropping view boundaries and the repulsion across the boundaries. By instantiating a transformer-based solution that represents visual elements as visual words and that models the dependencies between visual words, we report not only state-of-the-art performance on public benchmarks, but also interesting visualizations that depict the attraction and repulsion between visual elements, which may shed light on what makes for effective cropping view recommendation.},
  archive   = {C_ICCV},
  author    = {Zhiyu Pan and Zhiguo Cao and Kewei Wang and Hao Lu and Weicai Zhong},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00418},
  pages     = {4198-4207},
  title     = {TransView: Inside, outside, and across the cropping view boundaries},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploring visual engagement signals for representation
learning. <em>ICCV</em>, 4186–4197. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual engagement in social media platforms comprises interactions with photo posts including comments, shares, and likes. In this paper, we leverage such Visual Engagement clues as supervisory signals for representation learning. However, learning from engagement signals is non-trivial as it is not clear how to bridge the gap between low-level visual information and high-level social interactions. We present VisE, a weakly supervised learning approach, which maps social images to pseudo labels derived by clustered engagement signals. We then study how models trained in this way benefit subjective downstream computer vision tasks such as emotion recognition or political bias detection. Through extensive studies, we empirically demonstrate the effectiveness of VisE across a diverse set of classification tasks beyond the scope of conventional recognition 1 .},
  archive   = {C_ICCV},
  author    = {Menglin Jia and Zuxuan Wu and Austin Reiter and Claire Cardie and Serge Belongie and Ser-Nam Lim},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00417},
  pages     = {4186-4197},
  title     = {Exploring visual engagement signals for representation learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ALL snow removed: Single image desnowing algorithm using
hierarchical dual-tree complex wavelet representation and contradict
channel loss. <em>ICCV</em>, 4176–4185. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Snow is a highly complicated atmospheric phenomenon that usually contains snowflake, snow streak, and veiling effect (similar to the haze or the mist). In this literature, we propose a single image desnowing algorithm to address the diversity of snow particles in shape and size. First, to better represent the complex snow shape, we apply the dual-tree wavelet transform and propose a complex wavelet loss in the network. Second, we propose a hierarchical decomposition paradigm in our network for better under-standing the different sizes of snow particles. Last, we propose a novel feature called the contradict channel (CC) for the snow scenes. We find that the regions containing the snow particles tend to have higher intensity in the CC than that in the snow-free regions. We leverage this discriminative feature to construct the contradict channel loss for improving the performance of snow removal. Moreover, due to the limitation of existing snow datasets, to simulate the snow scenarios comprehensively, we propose a large-scale dataset called Comprehensive Snow Dataset (CSD). Experimental results show that the proposed method can favorably outperform existing methods in three synthetic datasets and real-world datasets. The code and dataset are released in https://github.com/weitingchen83/ICCV2021-Single-Image-Desnowing-HDCWNet.},
  archive   = {C_ICCV},
  author    = {Wei-Ting Chen and Hao-Yu Fang and Cheng-Lin Hsieh and Cheng-Che Tsai and I-Hsiang Chen and Jian-Jiun Ding and Sy-Yen Kuo},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00416},
  pages     = {4176-4185},
  title     = {ALL snow removed: Single image desnowing algorithm using hierarchical dual-tree complex wavelet representation and contradict channel loss},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PlaneTR: Structure-guided transformers for 3D plane
recovery. <em>ICCV</em>, 4166–4175. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a neural network built upon Transformers, namely PlaneTR, to simultaneously detect and reconstruct planes from a single image. Different from previous methods, PlaneTR jointly leverages the context information and the geometric structures in a sequence-to-sequence way to holistically detect plane instances in one forward pass. Specifically, we represent the geometric structures as line segments and conduct the network with three main components: (i) context and line segments encoders, (ii) a structure-guided plane decoder, (iii) a pixel-wise plane embedding decoder. Given an image and its detected line segments, PlaneTR generates the context and line segment sequences via two specially designed encoders and then feeds them into a Transformers-based decoder to directly predict a sequence of plane instances by simultaneously considering the context and global structure cues. Finally, the pixel-wise embeddings are computed to assign each pixel to one predicted plane instance which is nearest to it in embedding space. Comprehensive experiments demonstrate that PlaneTR achieves state-of-the-art performance on the ScanNet and NYUv2 datasets.},
  archive   = {C_ICCV},
  author    = {Bin Tan and Nan Xue and Song Bai and Tianfu Wu and Gui-Song Xia},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00415},
  pages     = {4166-4175},
  title     = {PlaneTR: Structure-guided transformers for 3D plane recovery},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Light source guided single-image flare removal from unpaired
data. <em>ICCV</em>, 4157–4165. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Causally-taken images often suffer from flare artifacts, due to the unintended reflections and scattering of light inside the camera. However, as flares may appear in a variety of shapes, positions, and colors, detecting and removing them entirely from an image is very challenging. Existing methods rely on predefined intensity and geometry priors of flares, and may fail to distinguish the difference between light sources and flare artifacts. We observe that the conditions of the light source in the image play an important role in the resulting flares. In this paper, we present a deep framework with light source aware guidance for single-image flare removal (SIFR). In particular, we first detect the light source regions and the flare regions separately, and then remove the flare artifacts based on the light source aware guidance. By learning the underlying relationships between the two types of regions, our approach can remove different kinds of flares from the image. In addition, instead of using paired training data which are difficult to collect, we propose the first unpaired flare removal dataset and new cycle-consistency constraints to obtain more diverse examples and avoid manual annotations. Extensive experiments demonstrate that our method outperforms the baselines qualitatively and quantitatively. We also show that our model can be applied to flare effect manipulation (e.g., adding or changing image flares).},
  archive   = {C_ICCV},
  author    = {Xiaotian Qiao and Gerhard P. Hancke and Rynson W. H. Lau},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00414},
  pages     = {4157-4165},
  title     = {Light source guided single-image flare removal from unpaired data},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Summarize and search: Learning consensus-aware dynamic
convolution for co-saliency detection. <em>ICCV</em>, 4147–4156. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans perform co-saliency detection by first summarizing the consensus knowledge in the whole group and then searching corresponding objects in each image. Previous methods usually lack robustness, scalability, or stability for the first process and simply fuse consensus features with image features for the second process. In this paper, we propose a novel consensus-aware dynamic convolution model to explicitly and effectively perform the &quot;summarize and search&quot; process. To summarize consensus image features, we first summarize robust features for every single image using an effective pooling method and then aggregate cross-image consensus cues via the self-attention mechanism. By doing this, our model meets the scalability and stability requirements. Next, we generate dynamic kernels from consensus features to encode the summarized consensus knowledge. Two kinds of kernels are generated in a supplementary way to summarize fine-grained image-specific consensus object cues and the coarse group-wise common knowledge, respectively. Then, we can effectively perform object searching by employing dynamic convolution at multiple scales. Besides, a novel and effective data synthesis method is also proposed to train our network. Experimental results on four benchmark datasets verify the effectiveness of our proposed method. Our code and saliency maps are available at https://github.com/nnizhang/CADC.},
  archive   = {C_ICCV},
  author    = {Ni Zhang and Junwei Han and Nian Liu and Ling Shao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00413},
  pages     = {4147-4156},
  title     = {Summarize and search: Learning consensus-aware dynamic convolution for co-saliency detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scene context-aware salient object detection. <em>ICCV</em>,
4136–4146. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Salient object detection identifies objects in an image that grab visual attention. Although contextual features are considered in recent literature, they often fail in real-world complex scenarios. We observe that this is mainly due to two issues: First, most existing datasets consist of simple foregrounds and backgrounds that hardly represent real-life scenarios. Second, current methods only learn contextual features of salient objects, which are insufficient to model high-level semantics for saliency reasoning in complex scenes. To address these problems, we first construct a new large-scale dataset with complex scenes in this paper. We then propose a context-aware learning approach to explicitly exploit the semantic scene contexts. Specifically, two modules are proposed to achieve the goal: 1) a Semantic Scene Context Refinement module to enhance contextual features learned from salient objects with scene context, and 2) a Contextual Instance Transformer to learn contextual relations between objects and scene context. To our knowledge, such high-level semantic contextual information of image scenes is under-explored for saliency detection in the literature. Extensive experiments demonstrate that the proposed approach outperforms state-of-the-art techniques in complex scenarios for saliency detection, and transfers well to other existing datasets. The code and dataset are available at https://github.com/SirisAvishek/Scene_Context_Aware_Saliency.},
  archive   = {C_ICCV},
  author    = {Avishek Siris and Jianbo Jiao and Gary K.L. Tam and Xianghua Xie and Rynson W.H. Lau},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00412},
  pages     = {4136-4146},
  title     = {Scene context-aware salient object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Uncertainty-guided transformer reasoning for camouflaged
object detection. <em>ICCV</em>, 4126–4135. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Spotting objects that are visually adapted to their surroundings is challenging for both humans and AI. Conventional generic / salient object detection techniques are suboptimal for this task because they tend to only discover easy and clear objects, while overlooking the difficult-to-detect ones with inherent uncertainties derived from indistinguishable textures. In this work, we contribute a novel approach using a probabilistic representational model in combination with transformers to explicitly reason under uncertainties, namely uncertainty-guided transformer reasoning (UGTR), for camouflaged object detection. The core idea is to first learn a conditional distribution over the backbone&#39;s output to obtain initial estimates and associated uncertainties, and then reason over these uncertain regions with attention mechanism to produce final predictions. Our approach combines the benefits of both Bayesian learning and Transformer-based reasoning, allowing the model to handle camouflaged object detection by leveraging both deterministic and probabilistic information. We empirically demonstrate that our proposed approach can achieve higher accuracy than existing state-of-the-art models on CHAMELEON, CAMO and COD10K datasets. Code is available at https://github.com/fanyang587/UGTR.},
  archive   = {C_ICCV},
  author    = {Fan Yang and Qiang Zhai and Xin Li and Rui Huang and Ao Luo and Hong Cheng and Deng-Ping Fan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00411},
  pages     = {4126-4135},
  title     = {Uncertainty-guided transformer reasoning for camouflaged object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MFNet: Multi-filter directive network for weakly supervised
salient object detection. <em>ICCV</em>, 4116–4125. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Weakly supervised salient object detection (WSOD) targets to train a CNNs-based saliency network using only low-cost annotations. Existing WSOD methods take various techniques to pursue single &quot;high-quality&quot; pseudo label from low-cost annotations and then develop their saliency networks. Though these methods have achieved good performance, the generated single label is inevitably affected by adopted refinement algorithms and shows prejudiced characteristics which further influence the saliency networks. In this work, we introduce a new multiple-pseudo-label framework to integrate more comprehensive and accurate saliency cues from multiple labels, avoiding the aforementioned problem. Specifically, we propose a multi-filter directive network (MFNet) including a saliency network as well as multiple directive filters. The directive filter (DF) is designed to extract and filter more accurate saliency cues from the noisy pseudo labels. The multiple accurate cues from multiple DFs are then simultaneously propagated to the saliency network with a multi-guidance loss. Extensive experiments on five datasets over four metrics demonstrate that our method outperforms all the existing con-generic methods. Moreover, it is also worth noting that our framework is flexible enough to apply to existing methods and improve their performance. The code and results of our method are available at https://github.com/OIPLab-DUT/MFNet.},
  archive   = {C_ICCV},
  author    = {Yongri Piao and Jian Wang and Miao Zhang and Huchuan Lu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00410},
  pages     = {4116-4125},
  title     = {MFNet: Multi-filter directive network for weakly supervised salient object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). StarEnhancer: Learning real-time and style-aware image
enhancement. <em>ICCV</em>, 4106–4115. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Image enhancement is a subjective process whose targets vary with user preferences. In this paper, we propose a deep learning-based image enhancement method covering multiple tonal styles using only a single model dubbed StarEnhancer. It can transform an image from one tonal style to another, even if that style is unseen. With a simple one-time setting, users can customize the model to make the enhanced images more in line with their aesthetics. To make the method more practical, we propose a well-designed enhancer that can process a 4K-resolution image over 200 FPS but surpasses the contemporaneous single style image enhancement methods in terms of PSNR, SSIM, and LPIPS. Finally, our proposed enhancement method has good inter-actability, which allows the user to fine-tune the enhanced image using intuitive options.},
  archive   = {C_ICCV},
  author    = {Yuda Song and Hui Qian and Xin Du},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00409},
  pages     = {4106-4115},
  title     = {StarEnhancer: Learning real-time and style-aware image enhancement},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Perceptual variousness motion deblurring with light global
context refinement. <em>ICCV</em>, 4096–4105. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep learning algorithms have made significant progress in dynamic scene deblurring. However, several challenges are still unsettled: 1) The degree and scale of blur in different regions of a blurred image can have a considerable variation in a large range. However, the traditional input pyramid or downscaling-upscaling, is designed to have limited and inflexible perceptual variousness to cope with large blur scale variation. 2) The nonlocal block is proved to be effective in the image enhancement tasks, but it requires high computation and memory cost. In this paper, we are the first to propose a light-weight globally-analyzing module into the image deblurring field, named Light Global Context Refinement (LGCR) module. With exponentially lower cost, it achieves even better performance than the nonlocal unit. Moreover, we propose the Perceptual Variousness Block (PVB) and PVB-piling strategy. By placing PVB repeatedly, the whole method possesses abundant reception field spectrum to be aware of the blur with various degrees and scales. Comprehensive experimental results from the different benchmarks and assessment metrics show that our method achieves excellent performance to set a new state-of-the-art in motion deblurring. 1},
  archive   = {C_ICCV},
  author    = {Jichun Li and Weimin Tan and Bo Yan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00408},
  pages     = {4096-4105},
  title     = {Perceptual variousness motion deblurring with light global context refinement},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). STAR: A structure-aware lightweight transformer for
real-time image enhancement. <em>ICCV</em>, 4086–4095. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Image and video enhancement such as color constancy, low light enhancement, and tone mapping on smartphones is challenging, because high-quality images should be achieved efficiently with a limited resource budget. Unlike prior works that either used very deep CNNs or large Trans-former models, we propose a structure-aware lightweight Transformer, termed STAR, for real-time image enhancement. STAR is formulated to capture long-range dependencies between image patches, which naturally and implicitly captures the structural relationships of different regions in an image. STAR is a general architecture that can be easily adapted to different image enhancement tasks. Extensive experiments show that STAR can effectively boost the quality and efficiency of many tasks such as illumination enhancement, auto white balance, and photo retouching, which are indispensable components for image processing on smartphones. For example, STAR reduces model complexity and improves image quality compared to the recent state-of-the-art [19] on the MIT-Adobe FiveK dataset [7] (i.e., 1.8dB PSNR improvements with 25\% parameters and 13\% float operations.)},
  archive   = {C_ICCV},
  author    = {Zhaoyang Zhang and Yitong Jiang and Jun Jiang and Xiaogang Wang and Ping Luo and Jinwei Gu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00407},
  pages     = {4086-4095},
  title     = {STAR: A structure-aware lightweight transformer for real-time image enhancement},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mutual affine network for spatially variant kernel
estimation in blind image super-resolution. <em>ICCV</em>, 4076–4085.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing blind image super-resolution (SR) methods mostly assume blur kernels are spatially invariant across the whole image. However, such an assumption is rarely applicable for real images whose blur kernels are usually spatially variant due to factors such as object motion and out-of-focus. Hence, existing blind SR methods would inevitably give rise to poor performance in real applications. To address this issue, this paper proposes a mutual affine network (MANet) for spatially variant kernel estimation. Specifically, MANet has two distinctive features. First, it has a moderate receptive field so as to keep the locality of degradation. Second, it involves a new mutual affine convolution (MAConv) layer that enhances feature expressiveness without increasing receptive field, model size and computation burden. This is made possible through exploiting channel interdependence, which applies each channel split with an affine transformation module whose input are the rest channel splits. Extensive experiments on synthetic and real images show that the proposed MANet not only performs favorably for both spatially variant and invariant kernel estimation, but also leads to state-of-the-art blind SR performance when combined with non-blind SR methods.},
  archive   = {C_ICCV},
  author    = {Jingyun Liang and Guolei Sun and Kai Zhang and Luc Van Gool and Radu Timofte},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00406},
  pages     = {4076-4085},
  title     = {Mutual affine network for spatially variant kernel estimation in blind image super-resolution},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning dual priors for JPEG compression artifacts removal.
<em>ICCV</em>, 4066–4075. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep learning (DL)-based methods have achieved great success in solving the ill-posed JPEG compression artifacts removal problem. However, as most DL architectures are designed to directly learn pixel-level mapping relationship-s, they largely ignore semantic-level information and lack sufficient interpretability. To address the above issues, in this work, we propose an interpretable deep network to learn both pixel-level regressive prior and semantic-level discriminative prior. Specifically, we design a variational model to formulate the image de-blocking problem and propose two prior terms for the image content and gradient, respectively. The content-relevant prior is formulated as a DL-based image-to-image regressor to perform as a de-blocker from the pixel-level. The gradient-relevant prior serves as a DL-based classifier to distinguish whether the image is compressed from the semantic-level. To effectively solve the variational model, we design an alternating minimization algorithm and unfold it into a deep network architecture. In this way, not only the interpretability of the deep network is increased, but also the dual priors can be well estimated from training samples. By integrating the two priors into a single framework, the image de-blocking problem can be well-constrained, leading to a better performance. Experiments on benchmarks and real-world use cases demonstrate the superiority of our method to the existing state-of-the-art approaches.},
  archive   = {C_ICCV},
  author    = {Xueyang Fu and Xi Wang and Aiping Liu and Junwei Han and Zheng-Jun Zha},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00405},
  pages     = {4066-4075},
  title     = {Learning dual priors for JPEG compression artifacts removal},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical conditional flow: A unified framework for image
super-resolution and image rescaling. <em>ICCV</em>, 4056–4065. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Normalizing flows have recently demonstrated promising results for low-level vision tasks. For image super-resolution (SR), it learns to predict diverse photo-realistic high-resolution (HR) images from the low-resolution (LR) image rather than learning a deterministic mapping. For image rescaling, it achieves high accuracy by jointly modelling the downscaling and upscaling processes. While existing approaches employ specialized techniques for these two tasks, we set out to unify them in a single formulation. In this paper, we propose the hierarchical conditional flow (HCFlow) as a unified framework for image SR and image rescaling. More specifically, HCFlow learns a bijective mapping between HR and LR image pairs by modelling the distribution of the LR image and the rest high-frequency component simultaneously. In particular, the high-frequency component is conditional on the LR image in a hierarchical manner. To further enhance the performance, other losses such as perceptual loss and GAN loss are combined with the commonly used negative log-likelihood loss in training. Extensive experiments on general image SR, face image SR and image rescaling have demonstrated that the proposed HCFlow achieves state-of-the-art performance in terms of both quantitative metrics and visual quality.},
  archive   = {C_ICCV},
  author    = {Jingyun Liang and Andreas Lugmayr and Kai Zhang and Martin Danelljan and Luc Van Gool and Radu Timofte},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00404},
  pages     = {4056-4065},
  title     = {Hierarchical conditional flow: A unified framework for image super-resolution and image rescaling},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CryoDRGN2: Ab initio neural reconstruction of 3D protein
structures from real cryo-EM images. <em>ICCV</em>, 4046–4055. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Protein structure determination from cryo-EM data requires reconstructing a 3D volume (or distribution of volumes) from many noisy and randomly oriented 2D projection images. While the standard homogeneous reconstruction task aims to recover a single static structure, recently-proposed neural and non-neural methods can reconstruct distributions of structures, thereby enabling the study of protein complexes that possess intrinsic structural or conformational heterogeneity. These heterogeneous reconstruction methods, however, require fixed image poses, which are typically estimated from an upstream homogeneous reconstruction and are not guaranteed to be accurate under highly heterogeneous conditions.In this work we describe cryoDRGN2, an ab initio reconstruction algorithm, which can jointly estimate image poses and learn a neural model of a distribution of 3D structures on real heterogeneous cryo-EM data. To achieve this, we adapt search algorithms from the traditional cryo-EM literature, and describe the optimizations and design choices required to make such a search procedure computationally tractable in the neural model setting. We show that cryoDRGN2 is robust to the high noise levels of real cryo-EM images, trains faster than earlier neural methods, and achieves state-of-the-art performance on real cryo-EM datasets.},
  archive   = {C_ICCV},
  author    = {Ellen D. Zhong and Adam Lerer and Joseph H. Davis and Bonnie Berger},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00403},
  pages     = {4046-4055},
  title     = {CryoDRGN2: Ab initio neural reconstruction of 3D protein structures from real cryo-EM images},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-supervised cryo-electron tomography volumetric image
restoration from single noisy volume with sparsity constraint.
<em>ICCV</em>, 4036–4045. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cryo-Electron Tomography (cryo-ET) is a powerful tool for 3D cellular visualization. Due to instrumental limitations, cryo-ET images and their volumetric reconstruction suffer from extremely low signal-to-noise ratio. In this paper, we propose a novel end-to-end self-supervised learning model, the Sparsity Constrained Network (SC-Net), to restore volumetric image from single noisy data in cryo-ET. The proposed method only requires a single noisy data as training input and no ground-truth is needed in the whole training procedure. A new target function is proposed to preserve both local smoothness and detailed structure. Additionally, a novel procedure for the simulation of electron tomographic photographing is designed to help the evaluation of methods. Experiments are done on three simulated data and four real-world data. The results show that our method could produce a strong enhancement for a single very noisy cryo-ET volumetric data, which is much better than the state-of-the-art Noise2Void, and with a competitive performance comparing with Noise2Noise. Code is available at https://github.com/icthrm/SC-Net.},
  archive   = {C_ICCV},
  author    = {Zhidong Yang and Fa Zhang and Renmin Han},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00402},
  pages     = {4036-4045},
  title     = {Self-supervised cryo-electron tomography volumetric image restoration from single noisy volume with sparsity constraint},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep survival analysis with longitudinal x-rays for
COVID-19. <em>ICCV</em>, 4026–4035. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Time-to-event analysis is an important statistical tool for allocating clinical resources such as ICU beds. However, classical techniques like the Cox model cannot directly incorporate images due to their high dimensionality. We propose a deep learning approach that naturally incorporates multiple, time-dependent imaging studies as well as non-imaging data into time-to-event analysis. Our techniques are bench-marked on a clinical dataset of 1,894 COVID-19 patients, and show that image sequences significantly improve predictions. For example, classical time-to-event methods produce a concordance error of around 30-40\% for predicting hospital admission, while our error is 25\% without images and 20\% with multiple X-rays included. Ablation studies suggest that our models are not learning spurious features such as scanner artifacts and that models which use multiple images tend to perform better than those which only use one. While our focus and evaluation is on COVID-19, the methods we develop are broadly applicable.},
  archive   = {C_ICCV},
  author    = {Michelle Shu and Richard Strong Bowen and Charles Herrmann and Gengmo Qi and Michele Santacatterina and Ramin Zabih},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00401},
  pages     = {4026-4035},
  title     = {Deep survival analysis with longitudinal X-rays for COVID-19},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mutual-complementing framework for nuclei detection and
segmentation in pathology image. <em>ICCV</em>, 4016–4025. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Detection and segmentation of nuclei are fundamental analysis operations in pathology images, the assessments derived from which serve as the gold standard for cancer diagnosis. Manual segmenting nuclei is expensive and time-consuming. What’s more, accurate segmentation detection of nuclei can be challenging due to the large appearance variation, conjoined and overlapping nuclei, and serious degeneration of histological structures. Supervised methods highly rely on massive annotated samples. The existing two unsupervised methods are prone to failure on degenerated samples. This paper proposes a Mutual-Complementing Framework (MCF) for nuclei detection and segmentation in pathology images. Two branches of MCF are trained in the mutual-complementing manner, where the detection branch complements the pseudo mask of the segmentation branch, while the progressive trained segmentation branch complements the missing nucleus templates through calculating the mask residual between the predicted mask and detected result. In the detection branch, two response map fusion strategies and gradient direction based postprocessing are devised to obtain the optimal detection response. Furthermore, the confidence loss combined with the synthetic samples and self-finetuning is adopted to train the segmentation network with only high confidence areas. Extensive experiments demonstrate that MCF achieves comparable performance with only a few nucleus patches as supervision. Especially, MCF possesses good robustness (only dropping by about 6\%) on degenerated samples, which are critical and common cases in clinical diagnosis.},
  archive   = {C_ICCV},
  author    = {Zunlei Feng and Zhonghua Wang and Xinchao Wang and Yining Mao and Thomas Li and Jie Lei and Yuexuan Wang and Mingli Song},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00400},
  pages     = {4016-4025},
  title     = {Mutual-complementing framework for nuclei detection and segmentation in pathology image},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CDNet: Centripetal direction network for nuclear instance
segmentation. <em>ICCV</em>, 4006–4015. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Nuclear instance segmentation is a challenging task due to a large number of touching and overlapping nuclei in pathological images. Existing methods cannot effectively recognize the accurate boundary owing to neglecting the relationship between pixels (e.g., direction information). In this paper, we propose a novel Centripetal Direction Net-work (CDNet) for nuclear instance segmentation. Specifically, we define centripetal direction feature as a class of adjacent directions pointing to the nuclear center to rep-resent the spatial relationship between pixels within the nucleus. These direction features are then used to construct a direction difference map to represent the similarity within instances and the differences between instances. Finally, we propose a direction-guided refinement module, which acts as a plug-and-play module to effectively integrate auxiliary tasks and aggregate the features of different branches. Experiments on MoNuSeg and CPM17 datasets show that CDNet is significantly better than the other methods and achieves the state-of-the-art performance. The code is available at https://github.com/honglianghe/CDNet.},
  archive   = {C_ICCV},
  author    = {Hongliang He and Zhongyi Huang and Yao Ding and Guoli Song and Lin Wang and Qian Ren and Pengxu Wei and Zhiqiang Gao and Jie Chen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00399},
  pages     = {4006-4015},
  title     = {CDNet: Centripetal direction network for nuclear instance segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multimodal co-attention transformer for survival prediction
in gigapixel whole slide images. <em>ICCV</em>, 3995–4005. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Survival outcome prediction is a challenging weakly-supervised and ordinal regression task in computational pathology that involves modeling complex interactions within the tumor microenvironment in gigapixel whole slide images (WSIs). Despite recent progress in formulating WSIs as bags for multiple instance learning (MIL), representation learning of entire WSIs remains an open and challenging problem, especially in overcoming: 1) the computational complexity of feature aggregation in large bags, and 2) the data heterogeneity gap in incorporating biological priors such as genomic measurements. In this work, we present a Multimodal Co-Attention Transformer (MCAT) framework that learns an interpretable, dense co-attention mapping between WSIs and genomic features formulated in an embedding space. Inspired by approaches in Visual Question Answering (VQA) that can attribute how word embed-dings attend to salient objects in an image when answering a question, MCAT learns how histology patches attend to genes when predicting patient survival. In addition to visualizing multimodal interactions, our co-attention trans-formation also reduces the space complexity of WSI bags, which enables the adaptation of Transformer layers as a general encoder backbone in MIL. We apply our proposed method on five different cancer datasets (4,730 WSIs, 67 million patches). Our experimental results demonstrate that the proposed method consistently achieves superior performance compared to the state-of-the-art methods.},
  archive   = {C_ICCV},
  author    = {Richard J. Chen and Ming Y. Lu and Wei-Hung Weng and Tiffany Y. Chen and Drew FK. Williamson and Trevor Manz and Maha Shady and Faisal Mahmood},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00398},
  pages     = {3995-4005},
  title     = {Multimodal co-attention transformer for survival prediction in gigapixel whole slide images},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-class cell detection using spatial context
representation. <em>ICCV</em>, 3985–3994. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In digital pathology, both detection and classification of cells are important for automatic diagnostic and prognostic tasks. Classifying cells into subtypes, such as tumor cells, lymphocytes or stromal cells is particularly challenging. Existing methods focus on morphological appearance of individual cells, whereas in practice pathologists often infer cell classes through their spatial context. In this paper, we propose a novel method for both detection and classification that explicitly incorporates spatial contextual in-formation. We use the spatial statistical function to describe local density in both a multi-class and a multi-scale manner. Through representation learning and deep clustering techniques, we learn advanced cell representation with both appearance and spatial context. On various benchmarks, our method achieves better performance than state-of-the-arts, especially on the classification task.},
  archive   = {C_ICCV},
  author    = {Shahira Abousamra and David Belinsky and John Van Arnam and Felicia Allard and Eric Yee and Rajarsi Gupta and Tahsin Kurc and Dimitris Samaras and Joel Saltz and Chao Chen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00397},
  pages     = {3985-3994},
  title     = {Multi-class cell detection using spatial context representation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The way to my heart is through contrastive learning: Remote
photoplethysmography from unlabelled video. <em>ICCV</em>, 3975–3984.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to reliably estimate physiological signals from video is a powerful tool in low-cost, pre-clinical health monitoring. In this work we propose a new approach to remote photoplethysmography (rPPG)–the measurement of blood volume changes from observations of a person’s face or skin. Similar to current state-of-the-art methods for rPPG, we apply neural networks to learn deep representations with invariance to nuisance image variation. In contrast to such methods, we employ a fully self-supervised training approach, which has no reliance on expensive ground truth physiological training data. Our proposed method uses contrastive learning with a weak prior over the frequency and temporal smoothness of the target signal of interest. We evaluate our approach on four rPPG datasets, showing that comparable or better results can be achieved compared to recent supervised deep learning methods but without using any annotation. In addition, we incorporate a learned saliency resampling module into both our unsupervised approach and supervised baseline. We show that by allowing the model to learn where to sample the input image, we can reduce the need for hand-engineered features while providing some interpretability into the model’s behavior and possible failure modes. We release code for our complete training and evaluation pipeline to encourage re-producible progress in this exciting new direction. 1},
  archive   = {C_ICCV},
  author    = {John Gideon and Simon Stent},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00396},
  pages     = {3975-3984},
  title     = {The way to my heart is through contrastive learning: Remote photoplethysmography from unlabelled video},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual-textual attentive semantic consistency for medical
report generation. <em>ICCV</em>, 3965–3974. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automatic report generation on medical radiographs have recently gained interest. However, identifying diseases as well as correctly predicting their corresponding sizes, locations and other medical description patterns, which is essential for generating high-quality reports, is challenging. Although previous methods focused on producing readable reports, how to accurately detect and describe findings that match with the query X-Ray has not been successfully addressed. In this paper, we propose a multi-modality semantic attention model to integrate visual features, predicted key finding embeddings, as well as clinical features, and progressively decode reports with visual-textual semantic consistency. First, multi-modality features are extracted and attended with the hidden states from the sentence de-coder, to encode enriched context vectors for better decoding a report. These modalities include regional visual features of scans, semantic word embeddings of the top-K findings predicted with high probabilities, and clinical features of indications. Second, the progressive report decoder consists of a sentence decoder and a word decoder, where we propose image-sentence matching and description accuracy losses to constrain the visual-textual semantic consistency. Extensive experiments on the public MIMIC-CXR and IU X-Ray datasets show that our model achieves consistent improvements over the state-of-the-art methods.},
  archive   = {C_ICCV},
  author    = {Yi Zhou and Lei Huang and Tao Zhou and Huazhu Fu and Ling Shao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00395},
  pages     = {3965-3974},
  title     = {Visual-textual attentive semantic consistency for medical report generation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RFNet: Region-aware fusion network for incomplete
multi-modal brain tumor segmentation. <em>ICCV</em>, 3955–3964. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most existing brain tumor segmentation methods usually exploit multi-modal magnetic resonance imaging (MRI) images to achieve high segmentation performance. However, the problem of missing certain modality images often happens in clinical practice, thus leading to severe segmentation performance degradation. In this work, we propose a Region-aware Fusion Network (RFNet) that is able to exploit different combinations of multi-modal data adaptively and effectively for tumor segmentation. Considering different modalities are sensitive to different brain tumor regions, we design a Region-aware Fusion Module (RFM) in RFNet to conduct modal feature fusion from available image modalities according to disparate regions. Benefiting from RFM, RFNet can adaptively segment tumor regions from an incomplete set of multi-modal images by effectively aggregating modal features. Furthermore, we also develop a segmentation-based regularizer to prevent RFNet from the insufficient and unbalanced training caused by the incomplete multi-modal data. Specifically, apart from obtaining segmentation results from fused modal features, we also segment each image modality individually from the corresponding encoded features. In this manner, each modal encoder is forced to learn discriminative features, thus improving the representation ability of the fused features. Remarkably, extensive experiments on BRATS2020, BRATS2018 and BRATS2015 datasets demonstrate that our RFNet outperforms the state-of-the-art significantly.},
  archive   = {C_ICCV},
  author    = {Yuhang Ding and Xin Yu and Yi Yang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00394},
  pages     = {3955-3964},
  title     = {RFNet: Region-aware fusion network for incomplete multi-modal brain tumor segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). T-AutoML: Automated machine learning for lesion segmentation
using transformers in 3D medical imaging. <em>ICCV</em>, 3942–3954. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Lesion segmentation in medical imaging has been an important topic in clinical research. Researchers have proposed various detection and segmentation algorithms to address this task. Recently, deep learning-based approaches have significantly improved the performance over conventional methods. However, most state-of-the-art deep learning methods require the manual design of multiple network components and training strategies. In this paper, we propose a new automated machine learning algorithm, T-AutoML, which not only searches for the best neural architecture, but also finds the best combination of hyper-parameters and data augmentation strategies simultaneously. The proposed method utilizes the modern transformer model, which is introduced to adapt to the dynamic length of the search space embedding and can significantly improve the ability of the search. We validate T-AutoML on several large-scale public lesion segmentation data-sets and achieve state-of-the-art performance.},
  archive   = {C_ICCV},
  author    = {Dong Yang and Andriy Myronenko and Xiaosong Wang and Ziyue Xu and Holger R. Roth and Daguang Xu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00393},
  pages     = {3942-3954},
  title     = {T-AutoML: Automated machine learning for lesion segmentation using transformers in 3D medical imaging},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semantic aware data augmentation for cell nuclei
microscopical images with artificial neural networks. <em>ICCV</em>,
3932–3941. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There exists many powerful architectures for object detection and semantic segmentation of both biomedical and natural images. However, a difficulty arises in the ability to create training datasets that are large and well-varied. The importance of this subject is nested in the amount of training data that artificial neural networks need to accurately identify and segment objects in images and the infeasibility of acquiring a sufficient dataset within the biomedical field. This paper introduces a new data augmentation method that generates artificial cell nuclei microscopical images along with their correct semantic segmentation labels. Data augmentation provides a step toward accessing higher generalization capabilities of artificial neural networks. An initial set of segmentation objects is used with Greedy AutoAugment to find the strongest performing augmentation policies. The found policies and the initial set of segmentation objects are then used in the creation of the final artificial images. When comparing the state-of-the-art data augmentation methods with the proposed method, the proposed method is shown to consistently outperform current solutions in the generation of nuclei microscopical images.},
  archive   = {C_ICCV},
  author    = {Alireza Naghizadeh and Hongye Xu and Mohab Mohamed and Dimitris N. Metaxas and Dongfang Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00392},
  pages     = {3932-3941},
  title     = {Semantic aware data augmentation for cell nuclei microscopical images with artificial neural networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GLoRIA: A multimodal global-local representation learning
framework for label-efficient medical image recognition. <em>ICCV</em>,
3922–3931. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, the growing utilization of medical imaging is placing an increasing burden on radiologists. Deep learning provides a promising solution for automatic medical image analysis and clinical decision support. However, large-scale manually labeled datasets required for training deep neural networks are difficult and expensive to obtain for medical images. The purpose of this work is to develop label-efficient multimodal medical imaging representations by leveraging radiology reports. We propose an attention-based framework for learning global and local representations by contrasting image sub-regions and words in the paired report. In addition, we propose methods to leverage the learned representations for various downstream medical image recognition tasks with limited labels. Our results demonstrate high-performance and label-efficiency for image-text retrieval, classification (finetuning and zerosshot settings), and segmentation on different datasets.},
  archive   = {C_ICCV},
  author    = {Shih-Cheng Huang and Liyue Shen and Matthew P. Lungren and Serena Yeung},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00391},
  pages     = {3922-3931},
  title     = {GLoRIA: A multimodal global-local representation learning framework for label-efficient medical image recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generative adversarial registration for improved conditional
deformable templates. <em>ICCV</em>, 3909–3921. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deformable templates are essential to large-scale medical image registration, segmentation, and population analysis. Current conventional and deep network-based methods for template construction use only regularized registration objectives and often yield templates with blurry and/or anatomically implausible appearance, confounding downstream biomedical interpretation. We reformulate deformable registration and conditional template estimation as an adversarial game wherein we encourage realism in the moved templates with a generative adversarial registration framework conditioned on flexible image covariates. The resulting templates exhibit significant gain in specificity to attributes such as age and disease, better fit underlying group-wise spatiotemporal trends, and achieve improved sharpness and centrality. These improvements enable more accurate population modeling with diverse covariates for standardized downstream analyses and easier anatomical delineation for structures of interest.},
  archive   = {C_ICCV},
  author    = {Neel Dey and Mengwei Ren and Adrian V. Dalca and Guido Gerig},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00390},
  pages     = {3909-3921},
  title     = {Generative adversarial registration for improved conditional deformable templates},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recurrent mask refinement for few-shot medical image
segmentation. <em>ICCV</em>, 3898–3908. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although having achieved great success in medical image segmentation, deep convolutional neural networks usually require a large dataset with manual annotations for training and are difficult to generalize to unseen classes. Few-shot learning has the potential to address these challenges by learning new classes from only a few labeled examples. In this work, we propose a new framework for few-shot medical image segmentation based on prototypical networks. Our innovation lies in the design of two key modules: 1) a context relation encoder (CRE) that uses correlation to capture local relation features between foreground and background regions; and 2) a recurrent mask refinement module that repeatedly uses the CRE and a prototypical network to recapture the change of context relationship and refine the segmentation mask iteratively. Experiments on two abdomen CT datasets and an abdomen MRI dataset show the proposed method obtains substantial improvement over the state-of-the-art methods by an average of 16.32\%, 8.45\% and 6.24\% in terms of DSC, respectively. Code is publicly available 1 .},
  archive   = {C_ICCV},
  author    = {Hao Tang and Xingwei Liu and Shanlin Sun and Xiangyi Yan and Xiaohui Xie},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00389},
  pages     = {3898-3908},
  title     = {Recurrent mask refinement for few-shot medical image segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Re-aging GAN: Toward personalized face age transformation.
<em>ICCV</em>, 3888–3897. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Face age transformation aims to synthesize past or future face images by reflecting the age factor on given faces. Ideally, this task should synthesize natural-looking faces across various age groups while maintaining identity. However, most of the existing work has focused on only one of these or is difficult to train while unnatural artifacts still appear. In this work, we propose Re-Aging GAN (RAGAN), a novel single framework considering all the critical factors in age transformation. Our framework achieves state-of-the-art personalized face age transformation by compelling the input identity to perform the self-guidance of the generation process. Specifically, RAGAN can learn the personalized age features by using high-order interactions between given identity and target age. Learned personalized age features are identity information that is recalibrated according to the target age. Hence, such features encompass identity and target age information that provides important clues on how an input identity should be at a certain age. Experimental result shows the lowest FID and KID scores and the highest age recognition accuracy compared to previous methods. The proposed method also demonstrates the visual superiority with fewer artifacts, identity preservation, and natural transformation across various age groups.},
  archive   = {C_ICCV},
  author    = {Farkhod Makhmudkhujaev and Sungeun Hong and In Kyu Park},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00388},
  pages     = {3888-3897},
  title     = {Re-aging GAN: Toward personalized face age transformation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards face encryption by generating adversarial identity
masks. <em>ICCV</em>, 3877–3887. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As billions of personal data being shared through social media and network, the data privacy and security have drawn an increasing attention. Several attempts have been made to alleviate the leakage of identity information from face photos, with the aid of, e.g., image obfuscation techniques. However, most of the present results are either perceptually unsatisfactory or ineffective against face recognition systems. Our goal in this paper is to develop a technique that can encrypt the personal photos such that they can protect users from unauthorized face recognition systems but remain visually identical to the original version for human beings. To achieve this, we propose a targeted identity-protection iterative method (TIP-IM) to generate adversarial identity masks which can be overlaid on facial images, such that the original identities can be concealed without sacrificing the visual quality. Extensive experiments demonstrate that TIP-IM provides 95\%+ protection success rate against various state-of-the-art face recognition models under practical test scenarios. Besides, we also show the practical and effective applicability of our method on a commercial API service.},
  archive   = {C_ICCV},
  author    = {Xiao Yang and Yinpeng Dong and Tianyu Pang and Hang Su and Jun Zhu and Yuefeng Chen and Hui Xue},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00387},
  pages     = {3877-3887},
  title     = {Towards face encryption by generating adversarial identity masks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Retrieve in style: Unsupervised facial feature transfer and
retrieval. <em>ICCV</em>, 3867–3876. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present Retrieve in Style (RIS), an unsupervised framework for facial feature transfer and retrieval on real images. Recent work shows capabilities of transferring local facial features by capitalizing on the disentanglement property of the StyleGAN latent space. RIS improves existing art on the following: 1) Introducing more effective feature disentanglement to allow for challenging transfers (i.e., hair, pose) that were not shown possible in SoTA methods. 2) Eliminating the need for per-image hyperparameter tuning, and for computing a catalog over a large batch of images. 3) Enabling fine-grained face retrieval using disentangled facial features (e.g., eyes). To our best knowledge, this is the first work to retrieve face images at this fine level. 4) Demonstrating robust, natural editing on real images. Our qualitative and quantitative analyses show RIS achieves both high-fidelity feature transfers and accurate fine-grained retrievals on real images. We also discuss the responsible applications of RIS. Our code is available at https://github.com/mchong6/RetrieveInStyle.},
  archive   = {C_ICCV},
  author    = {Min Jin Chong and Wen-Sheng Chu and Abhishek Kumar and David Forsyth},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00386},
  pages     = {3867-3876},
  title     = {Retrieve in style: Unsupervised facial feature transfer and retrieval},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Disentangled lifespan face synthesis. <em>ICCV</em>,
3857–3866. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A lifespan face synthesis (LFS) model aims to generate a set of photo-realistic face images of a person’s whole life, given only one snapshot as reference. The generated face image given a target age code is expected to be age-sensitive reflected by bio-plausible transformations of shape and texture, while being identity preserving. This is extremely challenging because the shape and texture characteristics of a face undergo separate and highly nonlinear transformations w.r.t. age. Most recent LFS models are based on generative adversarial networks (GANs) whereby age code conditional transformations are applied to a latent face representation. They benefit greatly from the recent advancements of GANs. However, without explicitly disentangling their latent representations into the texture, shape and identity factors, they are fundamentally limited in modeling the nonlinear age-related transformation on texture and shape whilst preserving identity. In this work, a novel LFS model is proposed to disentangle the key face characteristics including shape, texture and identity so that the unique shape and texture age transformations can be modeled effectively. This is achieved by extracting shape, texture and identity features separately from an encoder. Critically, two transformation modules, one conditional convolution based and the other channel attention based, are designed for modeling the nonlinear shape and texture feature transformations respectively. This is to accommodate their rather distinct aging processes and ensure that our synthesized images are both age-sensitive and identity preserving. Extensive experiments show that our LFS model is clearly superior to the state-of-the-art alternatives. Codes and demo are available on our project website: https://senhe.github.io/projects/iccv_2021_lifespan_face.},
  archive   = {C_ICCV},
  author    = {Sen He and Wentong Liao and Michael Ying Yang and Yi-Zhe Song and Bodo Rosenhahn and Tao Xiang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00385},
  pages     = {3857-3866},
  title     = {Disentangled lifespan face synthesis},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). FACIAL: Synthesizing dynamic talking face with implicit
attribute learning. <em>ICCV</em>, 3847–3856. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a talking face generation method that takes an audio signal as input and a short target video clip as reference, and synthesizes a photo-realistic video of the target face with natural lip motions, head poses, and eye blinks that are in-sync with the input audio signal. We note that the synthetic face attributes include not only explicit ones such as lip motions that have high correlations with speech, but also implicit ones such as head poses and eye blinks that have only weak correlation with the input audio. To model such complicated relationships among different face attributes with input audio, we propose a FACe Implicit Attribute Learning Generative Adversarial Network (FACIAL-GAN), which integrates the phonetics-aware, context-aware, and identity-aware information to synthesize the 3D face animation with realistic motions of lips, head poses, and eye blinks. Then, our Rendering-to-Video network takes the rendered face images and the attention map of eye blinks as input to generate the photorealistic output video frames. Experimental results and user studies show our method can generate realistic talking face videos with not only synchronized lip motions, but also natural head movements and eye blinks, with better qualities than the results of state-of-the-art methods.},
  archive   = {C_ICCV},
  author    = {Chenxu Zhang and Yifan Zhao and Yifei Huang and Ming Zeng and Saifeng Ni and Madhukar Budagavi and Xiaohu Guo},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00384},
  pages     = {3847-3856},
  title     = {FACIAL: Synthesizing dynamic talking face with implicit attribute learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). End-to-end robust joint unsupervised image alignment and
clustering. <em>ICCV</em>, 3834–3846. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Computing dense pixel-to-pixel image correspondences is a fundamental task of computer vision. Often, the objective is to align image pairs from the same semantic category for manipulation or segmentation purposes. Despite achieving superior performance, existing deep learning alignment methods cannot cluster images; consequently, clustering and pairing images needed to be a separate laborious and expensive step.Given a dataset with diverse semantic categories, we propose a multi-task model, Jim-Net, that can directly learn to cluster and align images without any pixel-level or image-level annotations. We design a pair-matching alignment unsupervised training algorithm that selectively matches and aligns image pairs from the clustering branch. Our unsupervised Jim-Net achieves comparable accuracy with state-of-the-art supervised methods on benchmark 2D image alignment dataset PF-PASCAL. Specifically, we apply Jim-Net to cryo-electron tomography, a revolutionary 3D microscopy imaging technique of native subcellular structures. After extensive evaluation on seven datasets, we demonstrate that Jim-Net enables systematic discovery and recovery of representative macromolecular structures in situ, which is essential for revealing molecular mechanisms underlying cellular functions. To our knowledge, Jim-Net is the first end-to-end model that can simultaneously align and cluster images, which significantly improves the performance as compared to performing each task alone.},
  archive   = {C_ICCV},
  author    = {Xiangrui Zeng and Gregory Howe and Min Xu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00383},
  pages     = {3834-3846},
  title     = {End-to-end robust joint unsupervised image alignment and clustering},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learn to cluster faces via pairwise classification.
<em>ICCV</em>, 3825–3833. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Face clustering plays an essential role in exploiting massive unlabeled face data. Recently, graph-based face clustering methods are getting popular for their satisfying performances. However, they usually suffer from excessive memory consumption especially on large-scale graphs, and rely on empirical thresholds to determine the connectivities between samples in inference, which restricts their applications in various real-world scenes. To address such problems, in this paper, we explore face clustering from the pairwise angle. Specifically, we formulate the face clustering task as a pairwise relationship classification task, avoiding the memory-consuming learning on large-scale graphs. The classifier can directly determine the relationship between samples and is enhanced by taking advantage of the contextual information. Moreover, to further facilitate the efficiency of our method, we propose a rank-weighted density to guide the selection of pairs sent to the classifier. Experimental results demonstrate that our method achieves state-of-the-art performances on several public clustering benchmarks at the fastest speed and shows a great advantage in comparison with graph-based clustering methods on memory consumption.},
  archive   = {C_ICCV},
  author    = {Junfu Liu and Di Qiu and Pengfei Yan and Xiaolin Wei},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00382},
  pages     = {3825-3833},
  title     = {Learn to cluster faces via pairwise classification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021d). Generalizing gaze estimation with outlier-guided
collaborative adaptation. <em>ICCV</em>, 3815–3824. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep neural networks have significantly improved appearance-based gaze estimation accuracy. However, it still suffers from unsatisfactory performance when generalizing the trained model to new domains, e.g., unseen environments or persons. In this paper, we propose a plug-and-play gaze adaptation framework (PnP-GA), which is an ensemble of networks that learn collaboratively with the guidance of outliers. Since our proposed framework does not require ground-truth labels in the target domain, the existing gaze estimation networks can be directly plugged into PnPGA and generalize the algorithms to new domains. We test PnP-GA on four gaze domain adaptation tasks, ETH-to-MPII, ETH-to-EyeDiap, Gaze360-to-MPII, and Gaze360to-EyeDiap. The experimental results demonstrate that the PnP-GA framework achieves considerable performance improvements of 36.9\%, 31.6\%, 19.4\%, and 11.8\% over the baseline system. The proposed framework also outperforms the state-of-the-art domain adaptation approaches on gaze domain adaptation tasks. Code has been released at https://github.com/DreamtaleCore/PnP-GA.},
  archive   = {C_ICCV},
  author    = {Yunfei Liu and Ruicong Liu and Haofei Wang and Feng Lu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00381},
  pages     = {3815-3824},
  title     = {Generalizing gaze estimation with outlier-guided collaborative adaptation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Topologically consistent multi-view face inference using
volumetric sampling. <em>ICCV</em>, 3804–3814. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {High-fidelity face digitization solutions often combine multi-view stereo (MVS) techniques for 3D reconstruction and a non-rigid registration step to establish dense correspondence across identities and expressions. A common problem is the need for manual clean-up after the MVS step, as 3D scans are typically affected by noise and outliers and contain hairy surface regions that need to be cleaned up by artists. Furthermore, mesh registration tends to fail for extreme facial expressions. Most learning-based methods use an underlying 3D morphable model (3DMM) to ensure robustness, but this limits the output accuracy for extreme facial expressions. In addition, the global bottleneck of regression architectures cannot produce meshes that tightly fit the ground truth surfaces. We propose ToFu, Topological consistent Face from multi-view, a geometry inference framework that can produce topologically consistent meshes across facial identities and expressions using a volumetric representation instead of an explicit underlying 3DMM. Our novel progressive mesh generation network embeds the topological structure of the face in a feature volume, sampled from geometry-aware local features. A coarse-to-fine architecture facilitates dense and accurate facial mesh predictions in a consistent mesh topology. ToFu further captures displacement maps for pore-level geometric details and facilitates high-quality rendering in the form of albedo and specular reflectance maps. These high-quality assets are readily usable by production studios for avatar creation, animation and physically-based skin rendering. We demonstrate state-of-the-art geometric and correspondence accuracy, while only taking 0.385 seconds to compute a mesh with 10K vertices, which is three orders of magnitude faster than traditional techniques. The code and the model are available for research purposes at https://tianyeli.github.io/tofu.},
  archive   = {C_ICCV},
  author    = {Tianye Li and Shichen Liu and Timo Bolkart and Jiayi Liu and Hao Li and Yajie Zhao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00380},
  pages     = {3804-3814},
  title     = {Topologically consistent multi-view face inference using volumetric sampling},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). DAM: Discrepancy alignment metric for face recognition.
<em>ICCV</em>, 3794–3803. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The field of face recognition (FR) has witnessed remarkable progress with the surge of deep learning. The effective loss functions play an important role for FR. In this paper, we observe that a majority of loss functions, including the widespread triplet loss and softmax-based cross-entropy loss, embed inter-class (negative) similarity s n and intra-class (positive) similarity s p into similarity pairs and optimize to reduce (s n − s p ) in the training process. However, in the verification process, existing metrics directly take the absolute similarity between two features as the confidence of belonging to the same identity, which inevitably causes a gap between the training and verification process. To bridge the gap, we propose a new metric called Discrepancy Alignment Metric (DAM) for verification, which introduces the Local Inter-class Discrepancy (LID) for each face image to normalize the absolute similarity score. To estimate the LID of each face image in the verification process, we propose two types of LID Estimation (LIDE) methods, which are reference-based and learning-based estimation methods, respectively. The proposed DAM is plug-and-play and can be easily applied to the most existing methods. Extensive experiments on multiple popular face recognition benchmark datasets demonstrate the effectiveness of our proposed method.},
  archive   = {C_ICCV},
  author    = {Jiaheng Liu and Yudong Wu and Yichao Wu and Chuming Li and Xiaolin Hu and Ding Liang and Mengyu Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00379},
  pages     = {3794-3803},
  title     = {DAM: Discrepancy alignment metric for face recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Physics-enhanced machine learning for virtual fluorescence
microscopy. <em>ICCV</em>, 3783–3793. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a new method of data-driven microscope design for virtual fluorescence microscopy. We use a deep neural network (DNN) to effectively design optical patterns for specimen illumination that substantially improve upon the ability to infer fluorescence image information from unstained microscope images. To achieve this design, we include an illumination model within the DNN’s first layers that is jointly optimized during network training. We validated our method on two different experimental setups, with different magnifications and sample types, to show a consistent improvement in performance as compared to conventional microscope imaging methods. Additionally, to understand the importance of learned illumination on the inference task, we varied the number of illumination patterns being optimized (and thus the number of unique images captured) and analyzed how the structure of the patterns changed as their number increased. This work demonstrates the power of programmable optical elements at enabling better machine learning algorithm performance and at providing physical insight into next generation of machine-controlled imaging systems.},
  archive   = {C_ICCV},
  author    = {Colin L. Cooke and Fanjie Kong and Amey Chaware and Kevin C. Zhou and Kanghyun Kim and Rong Xu and D. Michael Ando and Samuel J. Yang and Pavan Chandra Konda and Roarke Horstmeyer},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00378},
  pages     = {3783-3793},
  title     = {Physics-enhanced machine learning for virtual fluorescence microscopy},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DWKS: A local descriptor of deformations between meshes and
point clouds. <em>ICCV</em>, 3773–3782. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel pointwise descriptor, called DWKS, aimed at finding correspondences across two deformable shape collections. Unlike the majority of existing descriptors, rather than capturing local geometry, DWKS captures the deformation around a point within a collection in a multi-scale and informative manner. This, in turn, allows to compute inter-collection correspondences without using landmarks. To this end, we build upon the successful spectral WKS descriptors, but rather than using the Laplace-Beltrami operator, show that a similar construction can be performed on shape difference operators, that capture differences or distortion within a collection. By leveraging the collection information our descriptor facilitates difficult non-rigid shape matching tasks, even in the presence of strong partiality and significant deformations. We demonstrate the utility of our approach across a range of challenging matching problems on both meshes and point clouds. The code for this paper can be found at https://github.com/RobinMagnet/DWKS},
  archive   = {C_ICCV},
  author    = {Robin Magnet and Maks Ovsjanikov},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00377},
  pages     = {3773-3782},
  title     = {DWKS: A local descriptor of deformations between meshes and point clouds},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CrackFormer: Transformer network for fine-grained crack
detection. <em>ICCV</em>, 3763–3772. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cracks are irregular line structures that are of interest in many computer vision applications. Crack detection (e.g., from pavement images) is a challenging task due to intensity in-homogeneity, topology complexity, low contrast and noisy background. The overall crack detection accuracy can be significantly affected by the detection performance on fine-grained cracks. In this work, we propose a Crack Transformer network (CrackFormer) for fine-grained crack detection. The CrackFormer is composed of novel attention modules in a SegNet-like encoder-decoder architecture. Specifically, it consists of novel self-attention modules with 1x1 convolutional kernels for efficient contextual information extraction across feature-channels, and efficient positional embedding to capture large receptive field contextual information for long range interactions. It also introduces new scaling-attention modules to combine outputs from the corresponding encoder and decoder blocks to suppress non-semantic features and sharpen semantic ones. The CrackFormer is trained and evaluated on three classical crack datasets. The experimental results show that the CrackFormer achieves the Optimal Dataset Scale (ODS) values of 0.871, 0.877 and 0.881, respectively, on the three datasets and outperforms the state-of-the-art methods.},
  archive   = {C_ICCV},
  author    = {Huajun Liu and Xiangyu Miao and Christoph Mertz and Chengzhong Xu and Hui Kong},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00376},
  pages     = {3763-3772},
  title     = {CrackFormer: Transformer network for fine-grained crack detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CondLaneNet: A top-to-down lane detection framework based on
conditional convolution. <em>ICCV</em>, 3753–3762. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern deep-learning-based lane detection methods are successful in most scenarios but struggling for lane lines with complex topologies. In this work, we propose CondLaneNet, a novel top-to-down lane detection framework that detects the lane instances first and then dynamically predicts the line shape for each instance. Aiming to resolve lane instance-level discrimination problem, we introduce a conditional lane detection strategy based on conditional convolution and row-wise formulation. Further, we design the Recurrent Instance Module(RIM) to overcome the problem of detecting lane lines with complex topologies such as dense lines and fork lines. Benefit from the end-to-end pipeline which requires little post-process, our method has real-time efficiency. We extensively evaluate our method on three benchmarks of lane detection. Results show that our method achieves state-of-the-art performance on all three benchmark datasets. Moreover, our method has the coexistence of accuracy and efficiency, e.g. a 78.14 F1 score and 220 FPS on CULane. Our code is available at https://github.com/aliyun/conditional-lane-detection.},
  archive   = {C_ICCV},
  author    = {Lizhe Liu and Xiaohao Chen and Siyu Zhu and Ping Tan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00375},
  pages     = {3753-3762},
  title     = {CondLaneNet: A top-to-down lane detection framework based on conditional convolution},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-echo LiDAR for 3D object detection. <em>ICCV</em>,
3743–3752. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {LiDAR sensors can be used to obtain a wide range of measurement signals other than a simple 3D point cloud, and those signals can be leveraged to improve perception tasks like 3D object detection. A single laser pulse can be partially reflected by multiple objects along its path, resulting in multiple measurements called echoes. Multi-echo measurement can provide information about object contours and semi-transparent surfaces which can be used to better identify and locate objects. LiDAR can also measure surface reflectance (intensity of laser pulse return), as well as ambient light of the scene (sunlight reflected by objects). These signals are already available in commercial LiDAR devices but have not been used in most LiDAR-based detection models. We present a 3D object detection model which leverages the full spectrum of measurement signals provided by LiDAR. First, we propose a multi-signal fusion (MSF) module to combine (1) the reflectance and ambient features extracted with a 2D CNN, and (2) point cloud features extracted using a 3D graph neural network (GNN). Second, we propose a multi-echo aggregation (MEA) module to combine the information encoded in different sets of echo points. Compared with traditional single echo point cloud methods, our proposed Multi-Signal LiDAR Detector (MSLiD) extracts richer context information from a wider range of sensing measurements and achieves more accurate 3D object detection. Experiments show that by incorporating the multi-modality of LiDAR, our method outperforms the state-of-the-art by up to relatively 9.1\%.},
  archive   = {C_ICCV},
  author    = {Yunze Man and Xinshuo Weng and Prasanna Kumar Sivakumar and Matthew O’Toole and Kris Kitani},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00374},
  pages     = {3743-3752},
  title     = {Multi-echo LiDAR for 3D object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards efficient graph convolutional networks for point
cloud handling. <em>ICCV</em>, 3732–3742. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We aim at improving the computational efficiency of graph convolutional networks (GCNs) for learning on point clouds. The basic graph convolution that is composed of a K-nearest neighbor (KNN) search and a multilayer perceptron (MLP) is examined. By mathematically analyzing the operations there, two findings to improve the efficiency of GCNs are obtained. (1) The local geometric structure information of 3D representations propagates smoothly across the GCN that relies on KNN search to gather neighborhood features. This motivates the simplification of multiple KNN searches in GCNs. (2) Shuffling the order of graph feature gathering and an MLP leads to equivalent or similar composite operations. Based on those findings, we optimize the computational procedure in GCNs. A series of experiments show that the optimized networks have reduced computational complexity, decreased memory consumption, and accelerated inference speed while maintaining comparable accuracy for learning on point clouds.},
  archive   = {C_ICCV},
  author    = {Yawei Li and He Chen and Zhaopeng Cui and Radu Timofte and Marc Pollefeys and Gregory Chirikjian and Luc Van Gool},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00373},
  pages     = {3732-3742},
  title     = {Towards efficient graph convolutional networks for point cloud handling},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Looking here or there? Gaze following in 360-degree images.
<em>ICCV</em>, 3722–3731. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Gaze following, i.e., detecting the gaze target of a human subject, in 2D images has become an active topic in computer vision. However, it usually suffers from the out of frame issue due to the limited field-of-view (FoV) of 2D images. In this paper, we introduce a novel task, gaze following in 360-degree images which provide an omnidirectional FoV and can alleviate the out of frame issue. We collect the first dataset, &quot;GazeFollow360&quot; 1 , for this task, containing around 10,000 360-degree images with complex gaze behaviors under various scenes. Existing 2D gaze following methods suffer from performance degradation in 360degree images since they may use the assumption that a gaze target is in the 2D gaze sight line. However, this assumption is no longer true for long-distance gaze behaviors in 360-degree images, due to the distortion brought by sphere-to-plane projection. To address this challenge, we propose a 3D sight line guided dual-pathway framework, to detect the gaze target within a local region (here) and from a distant region (there), parallelly. Specifically, the local region is obtained as a 2D cone-shaped field along the 2D projection of the sight line starting at the human subject’s head position, and the distant region is obtained by searching along the sight line in 3D sphere space. Finally, the location of the gaze target is determined by fusing the estimations from both the local region and the distant region. Experimental results show that our method achieves significant improvements over previous 2D gaze following methods on our GazeFollow360 dataset.},
  archive   = {C_ICCV},
  author    = {Yunhao Li and Wei Shen and Zhongpai Gao and Yucheng Zhu and Guangtao Zhai and Guodong Guo},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00372},
  pages     = {3722-3731},
  title     = {Looking here or there? gaze following in 360-degree images},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Real-time vanishing point detector integrating
under-parameterized RANSAC and hough transform. <em>ICCV</em>,
3712–3721. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel approach that integrates underparameterized RANSAC (UPRANSAC) with Hough Transform to detect vanishing points (VPs) from un-calibrated monocular images. In our algorithm, the UPRANSAC chooses one hypothetical inlier in a sample set to find a portion of the VP’s degrees of freedom, which is followed by a highly reliable brute-force voting scheme (1-D Hough Transform) to find the VP’s remaining degrees of freedom along the extension line of the hypothetical inlier. Our approach is able to sequentially find a series of VPs by repeatedly removing inliers of any detected VPs from minimal sample sets until the stop criterion is reached. Compared to traditional RANSAC that selects 2 edges as a hypothetical inlier pair to fit a model of VP hypothesis and requires hitting a pair of inliners, the UPRANSAC has a higher likelihood to hit one inliner and is more reliable in VP detection. Meanwhile, the tremendously scaled-down voting space with the requirement of only 1 parameter for processing significantly increased the performance efficiency of Hough Transform in our scheme. Testing results with well-known benchmark datasets show that the detection accuracies of our approach were higher or on par with the SOTA while running in deeply real-time zone.},
  archive   = {C_ICCV},
  author    = {Jianping Wu and Liang Zhang and Ye Liu and Ke Chen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00371},
  pages     = {3712-3721},
  title     = {Real-time vanishing point detector integrating under-parameterized RANSAC and hough transform},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Free-form description guided 3D visual graph network for
object grounding in point cloud. <em>ICCV</em>, 3702–3711. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D object grounding aims to locate the most relevant target object in a raw point cloud scene based on a freeform language description. Understanding complex and diverse descriptions, and lifting them directly to a point cloud is a new and challenging topic due to the irregular and sparse nature of point clouds. There are three main challenges in 3D object grounding: to find the main focus in the complex and diverse description; to understand the point cloud scene; and to locate the target object. In this paper, we address all three challenges. Firstly, we propose a language scene graph module to capture the rich structure and long-distance phrase correlations. Secondly, we introduce a multi-level 3D proposal relation graph module to extract the object-object and object-scene co-occurrence relationships, and strengthen the visual features of the initial proposals. Lastly, we develop a description guided 3D visual graph module to encode global contexts of phrases and proposals by a nodes matching strategy. Extensive experiments on challenging benchmark datasets (ScanRefer [3] and Nr3D [42]) show that our algorithm outperforms existing state-of-the-art. Our code is available at https://github.com/PNXD/FFL-3DOG.},
  archive   = {C_ICCV},
  author    = {Mingtao Feng and Zhen Li and Qi Li and Liang Zhang and XiangDong Zhang and Guangming Zhu and Hui Zhang and Yaonan Wang and Ajmal Mian},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00370},
  pages     = {3702-3711},
  title     = {Free-form description guided 3D visual graph network for object grounding in point cloud},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VENet: Voting enhancement network for 3D object detection.
<em>ICCV</em>, 3692–3701. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hough voting, as has been demonstrated in VoteNet, is effective for 3D object detection, where voting is a key step. In this paper, we propose a novel VoteNet-based 3D detector with vote enhancement to improve the detection accuracy in cluttered indoor scenes. It addresses the limitations of current voting schemes, i.e., votes from neighboring objects and background have significant negative impacts. Before voting, we replace the classic MLP with the proposed Attentive MLP (AMLP) in the backbone network to get better feature description of seed points. During voting, we design a new vote attraction loss (VALoss) to enforce vote centers to locate closely and compactly to the corresponding object centers. After voting, we then devise a vote weighting module to integrate the foreground/background prediction into the vote aggregation process to enhance the capability of the original VoteNet to handle noise from background voting. The three proposed strategies all contribute to more effective voting and improved performance, resulting in a novel 3D object detector, termed VENet. Experiments show that our method outperforms state-of-the-art methods on benchmark datasets. Ablation studies demonstrate the effectiveness of the proposed components.},
  archive   = {C_ICCV},
  author    = {Qian Xie and Yu-Kun Lai and Jing Wu and Zhoutao Wang and Dening Lu and Mingqiang Wei and Jun Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00369},
  pages     = {3692-3701},
  title     = {VENet: Voting enhancement network for 3D object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cross-encoder for unsupervised gaze representation learning.
<em>ICCV</em>, 3682–3691. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In order to train 3D gaze estimators without too many annotations, we propose an unsupervised learning framework, Cross-Encoder, to leverage the unlabeled data to learn suitable representation for gaze estimation. To address the issue that the feature of gaze is always intertwined with the appearance of the eye, Cross-Encoder disentangles the features using a latent-code-swapping mechanism on eye-consistent image pairs and gaze-similar ones. Specifically, each image is encoded as a gaze feature and an eye feature. Cross-Encoder is trained to reconstruct each image in the eye-consistent pair according to its gaze feature and the other’s eye feature, but to reconstruct each image in the gaze-similar pair according to its eye feature and the other’s gaze feature. Experimental results show the validity of our work. First, using the Cross-Encoder-learned gaze representation, the gaze estimator trained with very few samples outperforms the ones using other unsupervised learning methods, under both within-dataset and cross-dataset protocol. Second, ResNet18 pretrained by Cross-Encoder is competitive with state-of-the-art gaze estimation methods. Third, ablation study shows that Cross-Encoder disentangles the gaze feature and eye feature.},
  archive   = {C_ICCV},
  author    = {Yunjia Sun and Jiabei Zeng and Shiguang Shan and Xilin Chen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00368},
  pages     = {3682-3691},
  title     = {Cross-encoder for unsupervised gaze representation learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Disentangled representation for age-invariant face
recognition: A mutual information minimization perspective.
<em>ICCV</em>, 3672–3681. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {General face recognition has seen remarkable progress in recent years. However, large age gap still remains a big challenge due to significant alterations in facial appearance and bone structure. Disentanglement plays a key role in partitioning face representations into identity-dependent and age-dependent components for age-invariant face recognition (AIFR). In this paper we propose a multi-task learning framework based on mutual information minimization (MT-MIM), which casts the disentangled representation learning as an objective of information constraints. The method trains a disentanglement network to minimize mutual information between the identity component and age component of the face image from the same person, and reduce the effect of age variations during the identification process. For quantitative measure of the degree of disentanglement, we verify that mutual information can represent as metric. The resulting identity-dependent representations are used for age-invariant face recognition. We evaluate MT-MIM on popular public-domain face aging datasets (FG-NET, MORPH Album 2, CACD and AgeDB) and obtained significant improvements over previous state-of-the-art methods. Specifically, our method exceeds the baseline models by over 0.4\% on MORPH Album 2, and over 0.7\% on CACD subsets, which are impressive improvements at the high accuracy levels of above 99\% and an average of 94\%.},
  archive   = {C_ICCV},
  author    = {Xuege Hou and Yali Li and Shengjin Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00367},
  pages     = {3672-3681},
  title     = {Disentangled representation for age-invariant face recognition: A mutual information minimization perspective},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fake it till you make it: Face analysis in the wild using
synthetic data alone. <em>ICCV</em>, 3661–3671. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We demonstrate that it is possible to perform face-related computer vision in the wild using synthetic data alone. The community has long enjoyed the benefits of synthesizing training data with graphics, but the domain gap between real and synthetic data has remained a problem, especially for human faces. Researchers have tried to bridge this gap with data mixing, domain adaptation, and domain-adversarial training, but we show that it is possible to synthesize data with minimal domain gap, so that models trained on synthetic data generalize to real in-the-wild datasets. We describe how to combine a procedurally-generated parametric 3D face model with a comprehensive library of hand-crafted assets to render training images with unprecedented realism and diversity. We train machine learning systems for face-related tasks such as landmark localization and face parsing, showing that synthetic data can both match real data in accuracy as well as open up new approaches where manual labeling would be impossible.},
  archive   = {C_ICCV},
  author    = {Erroll Wood and Tadas Baltrušaitis and Charlie Hewitt and Sebastian Dziadzio and Thomas J. Cashman and Jamie Shotton},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00366},
  pages     = {3661-3671},
  title     = {Fake it till you make it: Face analysis in the wild using synthetic data alone},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Teacher-student adversarial depth hallucination to improve
face recognition. <em>ICCV</em>, 3651–3660. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present the Teacher-Student Generative Adversarial Network (TS-GAN) to generate depth images from single RGB images in order to boost the performance of face recognition systems. For our method to generalize well across unseen datasets, we design two components in the architecture, a teacher and a student. The teacher, which itself consists of a generator and a discriminator, learns a latent mapping between input RGB and paired depth images in a supervised fashion. The student, which consists of two generators (one shared with the teacher) and a discriminator, learns from new RGB data with no available paired depth information, for improved generalization. The fully trained shared generator can then be used in runtime to hallucinate depth from RGB for downstream applications such as face recognition. We perform rigorous experiments to show the superiority of TS-GAN over other methods in generating synthetic depth images. Moreover, face recognition experiments demonstrate that our hallucinated depth along with the input RGB images boost performance across various architectures when compared to a single RGB modality by average values of +1.2\%, +2.6\%, and +2.6\% for IIIT-D, EURECOM, and LFW datasets respectively. We make our implementation public at: https://github.com/hardik-uppal/teacher-student-gan.git.},
  archive   = {C_ICCV},
  author    = {Hardik Uppal and Alireza Sepas-Moghaddam and Michael Greenspan and Ali Etemad},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00365},
  pages     = {3651-3660},
  title     = {Teacher-student adversarial depth hallucination to improve face recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Meta pairwise relationship distillation for unsupervised
person re-identification. <em>ICCV</em>, 3641–3650. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unsupervised person re-identification (Re-ID) remains challenging due to the lack of ground-truth labels. Existing methods often rely on estimated pseudo labels via iterative clustering and classification, and they are unfortunately highly susceptible to performance penalties incurred by the inaccurate estimated number of clusters. Alternatively, we propose the Meta Pairwise Relationship Distillation (MPRD) method to estimate the pseudo labels of sample pairs for unsupervised person Re-ID. Specifically, it consists of a Convolutional Neural Network (CNN) and Graph Convolutional Network (GCN), in which the GCN estimates the pseudo labels of sample pairs based on the current features extracted by CNN, and the CNN learns better features by involving high-fidelity positive and negative sample pairs imposed by GCN. To achieve this goal, a small amount of labeled samples are used to guide GCN training, which can distill meta knowledge to judge the difference in the neighborhood structure between positive and negative sample pairs. Extensive experiments on Market-1501, DukeMTMC-reID and MSMT17 datasets show that our method outperforms the state-of-the-art approaches.},
  archive   = {C_ICCV},
  author    = {Haoxuanye Ji and Le Wang and Sanping Zhou and Wei Tang and Nanning Zheng and Gang Hua},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00364},
  pages     = {3641-3650},
  title     = {Meta pairwise relationship distillation for unsupervised person re-identification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Conditional DETR for fast training convergence.
<em>ICCV</em>, 3631–3640. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The recently-developed DETR approach applies the transformer encoder and decoder architecture to object detection and achieves promising performance. In this paper, we handle the critical issue, slow training convergence, and present a conditional cross-attention mechanism for fast DETR training. Our approach is motivated by that the cross-attention in DETR relies highly on the content embeddings for localizing the four extremities and predicting the box, which increases the need for high-quality content embeddings and thus the training difficulty.Our approach, named conditional DETR, learns a conditional spatial query from the decoder embedding for decoder multi-head cross-attention. The benefit is that through the conditional spatial query, each cross-attention head is able to attend to a band containing a distinct region, e.g., one object extremity or a region inside the object box. This narrows down the spatial range for localizing the distinct regions for object classification and box regression, thus relaxing the dependence on the content embeddings and easing the training. Empirical results show that conditional DETR converges 6.7× faster for the backbones R50 and R101 and 10× faster for stronger backbones DC5-R50 and DC5-R101. Code is available at https://github.com/Atten4Vis/ConditionalDETR.},
  archive   = {C_ICCV},
  author    = {Depu Meng and Xiaokang Chen and Zejia Fan and Gang Zeng and Houqiang Li and Yuhui Yuan and Lei Sun and Jingdong Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00363},
  pages     = {3631-3640},
  title     = {Conditional DETR for fast training convergence},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mutual supervision for dense object detection.
<em>ICCV</em>, 3621–3630. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The classification and regression head are both indispensable components to build up a dense object detector, which are usually supervised by the same training samples and thus expected to have consistency with each other for detecting objects accurately in the detection pipeline. In this paper, we break the convention of the same training samples for these two heads in dense detectors and explore a novel supervisory paradigm, termed as Mutual Supervision (MuSu), to respectively and mutually assign training samples for the classification and regression head to ensure this consistency. MuSu defines training samples for the regression head mainly based on classification predicting scores and in turn, defines samples for the classification head based on localization scores from the regression head. Experimental results show that the convergence of detectors trained by this mutual supervision is guaranteed and the effectiveness of the proposed method is verified on the challenging MS COCO benchmark. We also find that tiling more anchors at the same location benefits detectors and leads to further improvements under this training scheme. We hope this work can inspire further researches on the interaction of the classification and regression task in detection and the supervision paradigm for detectors, especially separately for these two heads.},
  archive   = {C_ICCV},
  author    = {Ziteng Gao and Limin Wang and Gangshan Wu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00362},
  pages     = {3621-3630},
  title     = {Mutual supervision for dense object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reconcile prediction consistency for balanced object
detection. <em>ICCV</em>, 3611–3620. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Classification and regression are two pillars of object detectors. In most CNN-based detectors, these two pillars are optimized independently. Without direct interactions be-tween them, the classification loss and the regression loss can not be optimized synchronously toward the optimal direction in the training phase. This clearly leads to lots of inconsistent predictions with high classification score but low localization accuracy or low classification score but high localization accuracy in the inference phase, especially for the objects of irregular shape and occlusion, which severely hurts the detection performance of existing detectors after N-MS. To reconcile prediction consistency for balanced object detection, we propose a Harmonic loss to harmonize the optimization of classification branch and localization branch. The Harmonic loss enables these two branches to supervise and promote each other during training, thereby producing consistent predictions with high co-occurrence of top classification and localization in the inference phase. Furthermore, in order to prevent the localization loss from being dominated by outliers during training phase, a Harmonic IoU loss is proposed to harmonize the weight of the localization loss of different IoU-level samples. Comprehensive experiments on benchmarks PASCAL VOC and MS COCO demonstrate the generality and effectiveness of our model for facilitating existing object detectors to state-of-the-art accuracy.},
  archive   = {C_ICCV},
  author    = {Keyang Wang and Lei Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00361},
  pages     = {3611-3620},
  title     = {Reconcile prediction consistency for balanced object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast convergence of DETR with spatially modulated
co-attention. <em>ICCV</em>, 3601–3610. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The recently proposed Detection Transformer (DETR) model successfully applies Transformer to objects detection and achieves comparable performance with two-stage object detection frameworks, such as Faster-RCNN. However, DETR suffers from its slow convergence. Training DETR [4] from scratch needs 500 epochs to achieve a high accuracy. To accelerate its convergence, we propose a simple yet effective scheme for improving the DETR framework, namely Spatially Modulated Co-Attention (SMCA) mechanism. The core idea of SMCA is to conduct location-aware co-attention in DETR by constraining co-attention responses to be high near initially estimated bounding box locations. Our proposed SMCA increases DETR’s convergence speed by replacing the original co-attention mechanism in the decoder while keeping other operations in DETR unchanged. Furthermore, by integrating multi-head and scale-selection attention designs into SMCA, our fully-fledged SMCA can achieve better performance compared to DETR with a dilated convolution-based backbone (45.6 mAP at 108 epochs vs. 43.3 mAP at 500 epochs). We perform extensive ablation studies on COCO dataset to validate SMCA. Code is released at https://github.com/gaopengcuhk/SMCA-DETR.},
  archive   = {C_ICCV},
  author    = {Peng Gao and Minghang Zheng and Xiaogang Wang and Jifeng Dai and Hongsheng Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00360},
  pages     = {3601-3610},
  title     = {Fast convergence of DETR with spatially modulated co-attention},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rethinking transformer-based set prediction for object
detection. <em>ICCV</em>, 3591–3600. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {DETR is a recently proposed Transformer-based method which views object detection as a set prediction problem and achieves state-of-the-art performance but demands extra-long training time to converge. In this paper, we investigate the causes of the optimization difficulty in the training of DETR. Our examinations reveal several factors contributing to the slow convergence of DETR, primarily the issues with the Hungarian loss and the Transformer cross-attention mechanism. To overcome these issues we propose two solutions, namely, TSP-FCOS (Transformer-based Set Prediction with FCOS) and TSP-RCNN (Transformer-based Set Prediction with RCNN). Experimental results show that the proposed methods not only converge much faster than the original DETR, but also significantly outperform DETR and other baselines in terms of detection accuracy. Code is released at https://github.com/Edward-Sun/TSP-Detection.},
  archive   = {C_ICCV},
  author    = {Zhiqing Sun and Shengcao Cao and Yiming Yang and Kris Kitani},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00359},
  pages     = {3591-3600},
  title     = {Rethinking transformer-based set prediction for object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TransFER: Learning relation-aware facial expression
representations with transformers. <em>ICCV</em>, 3581–3590. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Facial expression recognition (FER) has received increasing interest in computer vision. We propose the Trans-FER model which can learn rich relation-aware local representations. It mainly consists of three components: Multi-Attention Dropping (MAD), ViT-FER, and Multi-head Self-Attention Dropping (MSAD). First, local patches play an important role in distinguishing various expressions, however, few existing works can locate discriminative and diverse local patches. This can cause serious problems when some patches are invisible due to pose variations or viewpoint changes. To address this issue, the MAD is proposed to randomly drop an attention map. Consequently, models are pushed to explore diverse local patches adaptively. Second, to build rich relations between different local patches, the Vision Transformers (ViT) are used in FER, called ViT-FER. Since the global scope is used to reinforce each local patch, a better representation is obtained to boost the FER performance. Thirdly, the multi-head self-attention allows ViT to jointly attend to features from different information subspaces at different positions. Given no explicit guidance, however, multiple self-attentions may extract similar relations. To address this, the MSAD is proposed to randomly drop one self-attention module. As a result, models are forced to learn rich relations among diverse local patches. Our proposed TransFER model outperforms the state-of-the-art methods on several FER benchmarks, showing its effectiveness and usefulness.},
  archive   = {C_ICCV},
  author    = {Fanglei Xue and Qiangchang Wang and Guodong Guo},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00358},
  pages     = {3581-3590},
  title     = {TransFER: Learning relation-aware facial expression representations with transformers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). G-DetKD: Towards general distillation framework for object
detectors via contrastive and semantic-guided feature imitation.
<em>ICCV</em>, 3571–3580. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we investigate the knowledge distillation (KD) strategy for object detection and propose an effective framework applicable to both homogeneous and heterogeneous student-teacher pairs. The conventional feature imitation paradigm introduces imitation masks to focus on informative foreground areas while excluding the background noises. However, we find that those methods fail to fully utilize the semantic information in all feature pyramid levels, which leads to inefficiency for knowledge distillation between FPN-based detectors. To this end, we propose a novel semantic-guided feature imitation technique, which automatically performs soft matching between feature pairs across all pyramid levels to provide the optimal guidance to the student. To push the envelop even further, we introduce contrastive distillation to effectively capture the information encoded in the relationship between different feature regions. Finally, we propose a generalized detection KD pipeline, which is capable of distilling both homogeneous and heterogeneous detector pairs. Our method consistently outperforms the existing detection KD techniques, and works when (1) components in the framework are used separately and in conjunction; (2) for both homogeneous and heterogenous student-teacher pairs and (3) on multiple detection benchmarks. With a powerful X101-FasterRCNN-Instaboost detector as the teacher, R50-FasterRCNN reaches 44.0\% AP, R50-RetinaNet reaches 43.3\% AP and R50-FCOS reaches 43.1\% AP on COCO dataset.},
  archive   = {C_ICCV},
  author    = {Lewei Yao and Renjie Pi and Hang Xu and Wei Zhang and Zhenguo Li and Tong Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00357},
  pages     = {3571-3580},
  title     = {G-DetKD: Towards general distillation framework for object detectors via contrastive and semantic-guided feature imitation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Disentangled high quality salient object detection.
<em>ICCV</em>, 3560–3570. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Aiming at discovering and locating most distinctive objects from visual scenes, salient object detection (SOD) plays an essential role in various computer vision systems. Coming to the era of high resolution, SOD methods are facing new challenges. The major limitation of previous methods is that they try to identify the salient regions and estimate the accurate objects boundaries simultaneously with a single regression task at low-resolution. This practice ignores the inherent difference between the two difficult problems, resulting in poor detection quality. In this paper, we propose a novel deep learning framework for high-resolution SOD task, which disentangles the task into a low-resolution saliency classification network (LRSCN) and a high-resolution refinement network (HRRN). As a pixel-wise classification task, LRSCN is designed to capture sufficient semantics at low-resolution to identify the definite salient, background and uncertain image regions. HRRN is a regression task, which aims at accurately refining the saliency value of pixels in the uncertain region to preserve a clear object boundary at high-resolution with limited GPU memory. It is worth noting that by introducing uncertainty into the training process, our HRRN can well address the high-resolution refinement task without using any high-resolution training data. Extensive experiments on high-resolution saliency datasets as well as some widely used saliency benchmarks show that the proposed method achieves superior performance compared to the state-of-the-art methods.},
  archive   = {C_ICCV},
  author    = {Lv Tang Bo Li and Yijie Zhong and Shouhong Ding and Mofei Song},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00356},
  pages     = {3560-3570},
  title     = {Disentangled high quality salient object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SimROD: A simple adaptation method for robust object
detection. <em>ICCV</em>, 3550–3559. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a Simple and effective unsupervised adaptation method for Robust Object Detection (SimROD). To overcome the challenging issues of domain shift and pseudo-label noise, our method integrates a novel domain-centric data augmentation, a gradual self-labeling adaptation procedure, and a teacher-guided fine-tuning mechanism. Using our method, target domain samples can be leveraged to adapt object detection models without changing the model architecture or generating synthetic data. When applied to image corruptions and high-level cross-domain adaptation benchmarks, our method outperforms prior baselines on multiple domain adaptation benchmarks. SimROD achieves new state-of-the-art on standard real-to-synthetic and cross-camera setup benchmarks. On the image corruption benchmark, models adapted with our method achieved a relative robustness improvement of 15-25\% AP50 on Pascal-C and 5-6\% AP on COCO-C and Cityscapes-C. On the cross-domain benchmark, our method outperformed the best baseline performance by up to 8\% and 4\% AP50 on Comic and Watercolor respectively. 1},
  archive   = {C_ICCV},
  author    = {Rindra Ramamonjison and Amin Banitalebi-Dehkordi and Xinyu Kang and Xiaolong Bai and Yong Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00355},
  pages     = {3550-3559},
  title     = {SimROD: A simple adaptation method for robust object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DualPoseNet: Category-level 6D object pose and size
estimation using dual pose network with refined learning of pose
consistency. <em>ICCV</em>, 3540–3549. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Category-level 6D object pose and size estimation is to predict full pose configurations of rotation, translation, and size for object instances observed in single, arbitrary views of cluttered scenes. In this paper, we propose a new method of Dual Pose Network with refined learning of pose consistency for this task, shortened as DualPoseNet. DualPoseNet stacks two parallel pose decoders on top of a shared pose encoder, where the implicit decoder predicts object poses with a working mechanism different from that of the explicit one; they thus impose complementary supervision on the training of pose encoder. We construct the encoder based on spherical convolutions, and design a module of Spherical Fusion wherein for a better embedding of pose-sensitive features from the appearance and shape observations. Given no testing CAD models, it is the novel introduction of the implicit decoder that enables the refined pose prediction during testing, by enforcing the predicted pose consistency between the two decoders using a self-adaptive loss term. Thorough experiments on benchmarks of both category- and instance-level object pose datasets confirm efficacy of our designs. DualPoseNet outperforms existing methods with a large margin in the regime of high precision. Our code is released publicly at https://github.com/Gorilla-Lab-SCUT/DualPoseNet.},
  archive   = {C_ICCV},
  author    = {Jiehong Lin and Zewei Wei and Zhihao Li and Songcen Xu and Kui Jia and Yuanqing Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00354},
  pages     = {3540-3549},
  title     = {DualPoseNet: Category-level 6D object pose and size estimation using dual pose network with refined learning of pose consistency},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual relationship detection using part-and-sum
transformers with composite queries. <em>ICCV</em>, 3530–3539. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Computer vision applications such as visual relationship detection and human object interaction can be formulated as a composite (structured) set detection problem in which both the parts (subject, object, and predicate) and the sum (triplet as a whole) are to be detected in a hierarchical fashion. In this paper, we present a new approach, denoted Part-and-Sum detection Transformer (PST), to perform end-to-end visual composite set detection. Different from existing Transformers in which queries are at a single level, we simultaneously model the joint part and sum hypotheses/interactions with composite queries and attention modules. We explicitly incorporate sum queries to enable better modeling of the part-and-sum relations that are absent in the standard Transformers. Our approach also uses novel tensor-based part queries and vector-based sum queries, and models their joint interaction. We report experiments on two vision tasks, visual relationship detection and human object interaction and demonstrate that PST achieves state of the art results among single-stage models, while nearly matching the results of custom designed two-stage models.},
  archive   = {C_ICCV},
  author    = {Qi Dong and Zhuowen Tu and Haofu Liao and Yuting Zhang and Vijay Mahadevan and Stefano Soatto},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00353},
  pages     = {3530-3539},
  title     = {Visual relationship detection using part-and-sum transformers with composite queries},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FMODetect: Robust detection of fast moving objects.
<em>ICCV</em>, 3521–3529. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose the first learning-based approach for fast moving objects detection. Such objects are highly blurred and move over large distances within one video frame. Fast moving objects are associated with a deblurring and matting problem, also called deblatting. We show that the separation of deblatting into consecutive matting and deblurring allows achieving real-time performance, i.e. an order of magnitude speed-up, and thus enabling new classes of application. The proposed method detects fast moving objects as a truncated distance function to the trajectory by learning from synthetic data. For the sharp appearance estimation and accurate trajectory estimation, we propose a matting and fitting network that estimates the blurred appearance without background, followed by an energy minimization based deblurring. The state-of-the-art methods are outperformed in terms of recall, precision, trajectory estimation, and sharp appearance reconstruction. Compared to other methods, such as deblatting, the inference is of several orders of magnitude faster and allows applications such as real-time fast moving object detection and retrieval in large video collections.},
  archive   = {C_ICCV},
  author    = {Denys Rozumnyi and Jiří Matas and Filip Šroubek and Marc Pollefeys and Martin R. Oswald},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00352},
  pages     = {3521-3529},
  title     = {FMODetect: Robust detection of fast moving objects},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards rotation invariance in object detection.
<em>ICCV</em>, 3510–3520. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Rotation augmentations generally improve a model’s in-variance/equivariance to rotation - except in object detection. In object detection the shape is not known, therefore rotation creates a label ambiguity. We show that the de-facto method for bounding box label rotation, the Largest Box Method, creates very large labels, leading to poor performance and in many cases worse performance than using no rotation at all. We propose a new method of rotation augmentation that can be implemented in a few lines of code. First, we create a differentiable approximation of label accuracy and show that axis-aligning the bounding box around an ellipse is optimal. We then introduce Rotation Uncertainty (RU) Loss, allowing the model to adapt to the uncertainty of the labels. On five different datasets (including COCO, PascalVOC, and Transparent Object Bin Picking), this approach improves the rotational invariance of both one-stage and two-stage architectures when measured with AP, AP50, and AP75.},
  archive   = {C_ICCV},
  author    = {Agastya Kalra and Guy Stoppi and Bradley Brown and Rishav Agarwal and Achuta Kadambi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00351},
  pages     = {3510-3520},
  title     = {Towards rotation invariance in object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Oriented r-CNN for object detection. <em>ICCV</em>,
3500–3509. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current state-of-the-art two-stage detectors generate oriented proposals through time-consuming schemes. This diminishes the detectors’ speed, thereby becoming the computational bottleneck in advanced oriented object detection systems. This work proposes an effective and simple oriented object detection framework, termed Oriented R-CNN, which is a general two-stage oriented detector with promising accuracy and efficiency. To be specific, in the first stage, we propose an oriented Region Proposal Network (oriented RPN) that directly generates high-quality oriented proposals in a nearly cost-free manner. The second stage is oriented R-CNN head for refining oriented Regions of Interest (oriented RoIs) and recognizing them. Without tricks, oriented R-CNN with ResNet50 achieves state-of-the-art detection accuracy on two commonly-used datasets for oriented object detection including DOTA (75.87\% mAP) and HRSC2016 (96.50\% mAP), while having a speed of 15.1 FPS with the image size of 1024×1024 on a single RTX 2080Ti. We hope our work could inspire rethinking the design of oriented detectors and serve as a baseline for oriented object detection. Code is available at https://github.com/jbwang1997/OBBDetection.},
  archive   = {C_ICCV},
  author    = {Xingxing Xie and Gong Cheng and Jiabao Wang and Xiwen Yao and Junwei Han},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00350},
  pages     = {3500-3509},
  title     = {Oriented R-CNN for object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TOOD: Task-aligned one-stage object detection.
<em>ICCV</em>, 3490–3499. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One-stage object detection is commonly implemented by optimizing two sub-tasks: object classification and localization, using heads with two parallel branches, which might lead to a certain level of spatial misalignment in predictions between the two tasks. In this work, we propose a Task-aligned One-stage Object Detection (TOOD) that explicitly aligns the two tasks in a learning-based manner. First, we design a novel Task-aligned Head (T-Head) which offers a better balance between learning task-interactive and task-specific features, as well as a greater flexibility to learn the alignment via a task-aligned predictor. Second, we propose Task Alignment Learning (TAL) to explicitly pull closer (or even unify) the optimal anchors for the two tasks during training via a designed sample assignment scheme and a task-aligned loss. Extensive experiments are conducted on MS-COCO, where TOOD achieves a 51.1 AP at single-model single-scale testing. This surpasses the recent one-stage detectors by a large margin, such as ATSS [30] (47.7 AP), GFL [14] (48.2 AP), and PAA [9] (49.0 AP), with fewer parameters and FLOPs. Qualitative results also demonstrate the effectiveness of TOOD for better aligning the tasks of object classification and localization. Code is available at https://github.com/fcjian/TOOD.},
  archive   = {C_ICCV},
  author    = {Chengjian Feng and Yujie Zhong and Yu Gao and Matthew R. Scott and Weilin Huang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00349},
  pages     = {3490-3499},
  title     = {TOOD: Task-aligned one-stage object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Preservational learning improves self-supervised medical
image models by reconstructing diverse contexts. <em>ICCV</em>,
3479–3489. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Preserving maximal information is one of principles of designing self-supervised learning methodologies. To reach this goal, contrastive learning adopts an implicit way which is contrasting image pairs. However, we believe it is not fully optimal to simply use the contrastive estimation for preservation. Moreover, it is necessary and complemental to introduce an explicit solution to preserve more information. From this perspective, we introduce Preservational Learning to reconstruct diverse image contexts in order to preserve more information in learned representations. Together with the contrastive loss, we present Preservational Contrastive Representation Learning (PCRL) for learning self-supervised medical representations. PCRL provides very competitive results under the pretraining-finetuning protocol, outperforming both self-supervised and supervised counterparts in 5 classification/segmentation tasks substantially. Codes are available at https://github.com/Luchixiang/PCRL.},
  archive   = {C_ICCV},
  author    = {Hong-Yu Zhou and Chixiang Lu and Sibei Yang and Xiaoguang Han and Yizhou Yu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00348},
  pages     = {3479-3489},
  title     = {Preservational learning improves self-supervised medical image models by reconstructing diverse contexts},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Collaborative and adversarial learning of focused and
dispersive representations for semi-supervised polyp segmentation.
<em>ICCV</em>, 3469–3478. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automatic polyp segmentation from colonoscopy images is an essential step in computer aided diagnosis for colorectal cancer. Most of polyp segmentation methods reported in recent years are based on fully supervised deep learning. However, annotation for polyp images by physicians during the diagnosis is time-consuming and costly. In this paper, we present a novel semi-supervised polyp segmentation via collaborative and adversarial learning of focused and dispersive representations learning model, where focused and dispersive extraction module are used to deal with the diversity of location and shape of polyps. In addition, confidence maps produced by a discriminator in an adversarial training framework shows the effectiveness of leveraging unlabeled data and improving the performance of segmentation network. Consistent regularization is further employed to optimize the segmentation networks to strengthen the representation of the outputs of focused and dispersive extraction module. We also propose an auxiliary adversarial learning method to better leverage unlabeled examples to further improve semantic segmentation accuracy. We conduct extensive experiments on two famous polyp datasets: Kvasir-SEG and CVC-Clinic DB. Experimental results demonstrate the effectiveness of the proposed model, consistently outperforming state-of-the-art semi-supervised segmentation models based on adversarial training and even some advanced fully supervised models.},
  archive   = {C_ICCV},
  author    = {Huisi Wu and Guilian Chen and Zhenkun Wen and Jing Qin},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00347},
  pages     = {3469-3478},
  title     = {Collaborative and adversarial learning of focused and dispersive representations for semi-supervised polyp segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Big self-supervised models advance medical image
classification. <em>ICCV</em>, 3458–3468. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-supervised pretraining followed by supervised fine-tuning has seen success in image recognition, especially when labeled examples are scarce, but has received limited attention in medical image analysis. This paper studies the effectiveness of self-supervised learning as a pre-training strategy for medical image classification. We conduct experiments on two distinct tasks: dermatology condition classification from digital camera images and multi-label chest X-ray classification, and demonstrate that self-supervised learning on ImageNet, followed by additional self-supervised learning on unlabeled domain-specific medical images significantly improves the accuracy of medical image classifiers. We introduce a novel Multi-Instance Contrastive Learning (MICLe) method that uses multiple images of the underlying pathology per patient case, when available, to construct more informative positive pairs for self-supervised learning. Combining our contributions, we achieve an improvement of 6.7\% in top-1 accuracy and an improvement of 1.1\% in mean AUC on dermatology and chest X-ray classification respectively, outperforming strong supervised baselines pretrained on ImageNet. In addition, we show that big self-supervised models are robust to distribution shift and can learn efficiently with a small number of labeled medical images.},
  archive   = {C_ICCV},
  author    = {Shekoofeh Azizi and Basil Mustafa and Fiona Ryan and Zachary Beaver and Jan Freyberg and Jonathan Deaton and Aaron Loh and Alan Karthikesalingam and Simon Kornblith and Ting Chen and Vivek Natarajan and Mohammad Norouzi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00346},
  pages     = {3458-3468},
  title     = {Big self-supervised models advance medical image classification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning hierarchical graph neural networks for image
clustering. <em>ICCV</em>, 3447–3457. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a hierarchical graph neural network (GNN) model that learns how to cluster a set of images into an unknown number of identities using a training set of images annotated with labels belonging to a disjoint set of identities. Our hierarchical GNN uses a novel approach to merge connected components predicted at each level of the hierarchy to form a new graph at the next level. Unlike fully unsupervised hierarchical clustering, the choice of grouping and complexity criteria stems naturally from supervision in the training set. The resulting method, Hi-LANDER, achieves an average of 49\% improvement in F-score and 7\% increase in Normalized Mutual Information (NMI) relative to current GNN-based clustering algorithms. Additionally, state-of-the-art GNN-based methods rely on separate models to predict linkage probabilities and node densities as intermediate steps of the clustering process. In contrast, our unified framework achieves a three-fold decrease in computational cost. Our training and inference code are released 1 .},
  archive   = {C_ICCV},
  author    = {Yifan Xing and Tong He and Tianjun Xiao and Yongxin Wang and Yuanjun Xiong and Wei Xia and David Wipf and Zheng Zhang and Stefano Soatto},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00345},
  pages     = {3447-3457},
  title     = {Learning hierarchical graph neural networks for image clustering},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FASA: Feature augmentation and sampling adaptation for
long-tailed instance segmentation. <em>ICCV</em>, 3437–3446. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent methods for long-tailed instance segmentation still struggle on rare object classes with few training data. We propose a simple yet effective method, Feature Augmentation and Sampling Adaptation (FASA), that addresses the data scarcity issue by augmenting the feature space especially for rare classes. Both the Feature Augmentation (FA) and feature sampling components are adaptive to the actual training status — FA is informed by the feature mean and variance of observed real samples from past iterations, and we sample the generated virtual features in a loss-adapted manner to avoid over-fitting. FASA does not require any elaborate loss design, and removes the need for inter-class transfer learning that often involves large cost and manually-defined head/tail class groups. We show FASA is a fast, generic method that can be easily plugged into standard or long-tailed segmentation frameworks, with consistent performance gains and little added cost. FASA is also applicable to other tasks like long-tailed classification with state-of-the-art performance. 1 2},
  archive   = {C_ICCV},
  author    = {Yuhang Zang and Chen Huang and Chen Change Loy},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00344},
  pages     = {3437-3446},
  title     = {FASA: Feature augmentation and sampling adaptation for long-tailed instance segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semi-supervised active learning with temporal output
discrepancy. <em>ICCV</em>, 3427–3436. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While deep learning succeeds in a wide range of tasks, it highly depends on the massive collection of annotated data which is expensive and time-consuming. To lower the cost of data annotation, active learning has been proposed to interactively query an oracle to annotate a small proportion of informative samples in an unlabeled dataset. Inspired by the fact that the samples with higher loss are usually more informative to the model than the samples with lower loss, in this paper we present a novel deep active learning approach that queries the oracle for data annotation when the unlabeled sample is believed to incorporate high loss. The core of our approach is a measurement Temporal Output Discrepancy (TOD) that estimates the sample loss by evaluating the discrepancy of outputs given by models at different optimization steps. Our theoretical investigation shows that TOD lower-bounds the accumulated sample loss thus it can be used to select informative unlabeled samples. On basis of TOD, we further develop an effective unlabeled data sampling strategy as well as an unsupervised learning criterion that enhances model performance by incorporating the unlabeled data. Due to the simplicity of TOD, our active learning approach is efficient, flexible, and task-agnostic. Extensive experimental results demonstrate that our approach achieves superior performances than the state-of-the-art active learning methods on image classification and semantic segmentation tasks.},
  archive   = {C_ICCV},
  author    = {Siyu Huang and Tianyang Wang and Haoyi Xiong and Jun Huan and Dejing Dou},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00343},
  pages     = {3427-3436},
  title     = {Semi-supervised active learning with temporal output discrepancy},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Training multi-object detector by estimating bounding box
distribution for input image. <em>ICCV</em>, 3417–3426. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In multi-object detection using neural networks, the fundamental problem is, &quot;How should the network learn a variable number of bounding boxes in different input images?&quot;. Previous methods train a multi-object detection network through a procedure that directly assigns the ground truth bounding boxes to the specific locations of the network’s output. However, this procedure makes the training of a multi-object detection network too heuristic and complicated. In this paper, we reformulate the multi-object detection task as a problem of density estimation of bounding boxes. Instead of assigning each ground truth to specific locations of network’s output, we train a network by estimating the probability density of bounding boxes in an input image using a mixture model. For this purpose, we propose a novel network for object detection called Mixture Density Object Detector (MDOD), and the corresponding objective function for the density-estimation-based training. We applied MDOD to MS COCO dataset. Our proposed method not only deals with multi-object detection problems in a new approach, but also improves detection performances through MDOD. The code is available: https://github.com/yoojy31/MDOD.},
  archive   = {C_ICCV},
  author    = {Jaeyoung Yoo and Hojun Lee and Inseop Chung and Geonseok Seo and Nojun Kwak},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00342},
  pages     = {3417-3426},
  title     = {Training multi-object detector by estimating bounding box distribution for input image},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Normalization matters in weakly supervised object
localization. <em>ICCV</em>, 3407–3416. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Weakly-supervised object localization (WSOL) enables finding an object using a dataset without any localization information. By simply training a classification model using only image-level annotations, the feature map of the model can be utilized as a score map for localization. In spite of many WSOL methods proposing novel strategies, there has not been any de facto standard about how to normalize the class activation map (CAM). Consequently, many WSOL methods have failed to fully exploit their own capacity because of the misuse of a normalization method. In this paper, we review many existing normalization methods and point out that they should be used according to the property of the given dataset. Additionally, we propose a new normalization method which substantially enhances the performance of any CAM-based WSOL methods. Using the proposed normalization method, we provide a comprehensive evaluation over three datasets (CUB, ImageNet and OpenImages) on three different architectures and observe significant performance gains over the conventional min-max normalization method in all the evaluated cases (See Fig. 1).},
  archive   = {C_ICCV},
  author    = {Jeesoo Kim and Junsuk Choe and Sangdoo Yun and Nojun Kwak},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00341},
  pages     = {3407-3416},
  title     = {Normalization matters in weakly supervised object localization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploring classification equilibrium in long-tailed object
detection. <em>ICCV</em>, 3397–3406. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The conventional detectors tend to make imbalanced classification and suffer performance drop, when the distribution of the training data is severely skewed. In this paper, we propose to use the mean classification score to indicate the classification accuracy for each category during training. Based on this indicator, we balance the classification via an Equilibrium Loss (EBL) and a Memory-augmented Feature Sampling (MFS) method. Specifically, EBL increases the intensity of the adjustment of the decision boundary for the weak classes by a designed score-guided loss margin between any two classes. On the other hand, MFS improves the frequency and accuracy of the adjustment of the decision boundary for the weak classes through over-sampling the instance features of those classes. Therefore, EBL and MFS work collaboratively for finding the classification equilibrium in long-tailed detection, and dramatically improve the performance of tail classes while maintaining or even improving the performance of head classes. We conduct experiments on LVIS using Mask R-CNN with various backbones including ResNet-50-FPN and ResNet-101-FPN to show the superiority of the proposed method. It improves the detection performance of tail classes by 15.6 AP, and outperforms the most recent long-tailed object detectors by more than 1 AP. Code is available at https://github.com/fcjian/LOCE.},
  archive   = {C_ICCV},
  author    = {Chengjian Feng and Yujie Zhong and Weilin Huang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00340},
  pages     = {3397-3406},
  title     = {Exploring classification equilibrium in long-tailed object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DiscoBox: Weakly supervised instance segmentation and
semantic correspondence from box supervision. <em>ICCV</em>, 3386–3396.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce DiscoBox, a novel framework that jointly learns instance segmentation and semantic correspondence using bounding box supervision. Specifically, we propose a self-ensembling framework where instance segmentation and semantic correspondence are jointly guided by a structured teacher in addition to the bounding box supervision. The teacher is a structured energy model incorporating a pairwise potential and a cross-image potential to model the pairwise pixel relationships both within and across the boxes. Minimizing the teacher energy simultaneously yields refined object masks and dense correspondences between intra-class objects, which are taken as pseudo-labels to supervise the task network and provide positive/negative correspondence pairs for dense contrastive learning. We show a symbiotic relationship where the two tasks mutually benefit from each other. Our best model achieves 37.9\% AP on COCO instance segmentation, surpassing prior weakly supervised methods and is competitive to supervised methods. We also obtain state of the art weakly supervised results on PASCAL VOC12 and PF-PASCAL with real-time inference.},
  archive   = {C_ICCV},
  author    = {Shiyi Lan and Zhiding Yu and Christopher Choy and Subhashree Radhakrishnan and Guilin Liu and Yuke Zhu and Larry S. Davis and Anima Anandkumar},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00339},
  pages     = {3386-3396},
  title     = {DiscoBox: Weakly supervised instance segmentation and semantic correspondence from box supervision},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ICON: Learning regular maps through inverse consistency.
<em>ICCV</em>, 3376–3385. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning maps between data samples is fundamental. Applications range from representation learning, image translation and generative modeling, to the estimation of spatial deformations. Such maps relate feature vectors, or map between feature spaces. Well-behaved maps should be regular, which can be imposed explicitly or may emanate from the data itself. We explore what induces regularity for spatial transformations, e.g., when computing image registrations. Classical optimization-based models compute maps between pairs of samples and rely on an appropriate regularizer for well-posedness. Recent deep learning approaches have attempted to avoid using such regularizers altogether by relying on the sample population instead. We explore if it is possible to obtain spatial regularity using an inverse consistency loss only and elucidate what explains map regularity in such a context. We find that deep networks combined with an inverse consistency loss and randomized off-grid interpolation yield well behaved, approximately diffeomorphic, spatial transformations. Despite the simplicity of this approach, our experiments present compelling evidence, on both synthetic and real data, that regular maps can be obtained without carefully tuned explicit regularizers, while achieving competitive registration performance.},
  archive   = {C_ICCV},
  author    = {Hastings Greer and Roland Kwitt and François-Xavier Vialard and Marc Niethammer},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00338},
  pages     = {3376-3385},
  title     = {ICON: Learning regular maps through inverse consistency},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Foreground activation maps for weakly supervised object
localization. <em>ICCV</em>, 3365–3375. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Weakly supervised object localization (WSOL) aims to localize objects with only image-level labels, which has better scalability and practicability than fully supervised methods in the actual deployment. However, with only image-level labels, learning object classification models tends to activate object parts and ignore the whole object, while expanding object parts into the whole object may deteriorate classification performance. To alleviate this problem, we propose foreground activation maps (FAM), whose aim is to optimize object localization and classification jointly via an object-aware attention module and a part-aware attention module in a unified model, where the two tasks can complement and enhance each other. To the best of our knowledge, this is the first work that can achieve remarkable performance for both tasks by optimizing them jointly via FAM for WSOL. Besides, the designed two modules can effectively highlight foreground objects for localization and discover discriminative parts for classification. Extensive experiments with four backbones on two standard benchmarks demonstrate that our FAM performs favorably against state-of-the-art WSOL methods.},
  archive   = {C_ICCV},
  author    = {Meng Meng and Tianzhu Zhang and Qi Tian and Yongdong Zhang and Feng Wu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00337},
  pages     = {3365-3375},
  title     = {Foreground activation maps for weakly supervised object localization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to better segment objects from unseen classes with
unlabeled videos. <em>ICCV</em>, 3355–3364. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to localize and segment objects from unseen classes would open the door to new applications, such as autonomous object learning in active vision. Nonetheless, improving the performance on unseen classes requires additional training data, while manually annotating the objects of the unseen classes can be labor-extensive and expensive. In this paper, we explore the use of unlabeled video sequences to automatically generate training data for objects of unseen classes. It is in principle possible to apply existing video segmentation methods to unlabeled videos and automatically obtain object masks, which can then be used as a training set even for classes with no manual labels available. However, our experiments show that these methods do not perform well enough for this purpose. We therefore introduce a Bayesian method that is specifically designed to automatically create such a training set: Our method starts from a set of object proposals and relies on (non-realistic) analysis-by-synthesis to select the correct ones by performing an efficient optimization over all the frames simultaneously. Through extensive experiments, we show that our method can generate a high-quality training set which significantly boosts the performance of segmenting objects of unseen classes. We thus believe that our method could open the door for open-world instance segmentation by exploiting abundant Internet videos.},
  archive   = {C_ICCV},
  author    = {Yuming Du and Yang Xiao and Vincent Lepetit},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00336},
  pages     = {3355-3364},
  title     = {Learning to better segment objects from unseen classes with unlabeled videos},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rethinking counting and localization in crowds: A purely
point-based framework. <em>ICCV</em>, 3345–3354. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Localizing individuals in crowds is more in accordance with the practical demands of subsequent high-level crowd analysis tasks than simply counting. However, existing localization based methods relying on intermediate representations (i.e., density maps or pseudo boxes) serving as learning targets are counter-intuitive and error-prone. In this paper, we propose a purely point-based framework for joint crowd counting and individual localization. For this framework, instead of merely reporting the absolute counting error at image level, we propose a new metric, called density Normalized Average Precision (nAP), to provide more comprehensive and more precise performance evaluation. Moreover, we design an intuitive solution under this framework, which is called Point to Point Network (P2PNet). P2PNet discards superfluous steps and directly predicts a set of point proposals to represent heads in an image, being consistent with the human annotation results. By thorough analysis, we reveal the key step towards implementing such a novel idea is to assign optimal learning targets for these proposals. Therefore, we propose to conduct this crucial association in an one-to-one matching manner using the Hungarian algorithm. The P2PNet not only significantly surpasses state-of-the-art methods on popular counting benchmarks, but also achieves promising localization accuracy. The codes will be available at: TencentYoutuResearch/CrowdCounting-P2PNet.},
  archive   = {C_ICCV},
  author    = {Qingyu Song and Changan Wang and Zhengkai Jiang and Yabiao Wang and Ying Tai and Chengjie Wang and Jilin Li and Feiyue Huang and Yang Wu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00335},
  pages     = {3345-3354},
  title     = {Rethinking counting and localization in crowds: A purely point-based framework},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-scale matching networks for semantic correspondence.
<em>ICCV</em>, 3334–3344. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep features have been proven powerful in building accurate dense semantic correspondences in various previous works. However, the multi-scale and pyramidal hierarchy of convolutional neural networks has not been well studied to learn discriminative pixel-level features for semantic correspondence. In this paper, we propose a multi-scale matching network that is sensitive to tiny semantic differences between neighboring pixels. We follow the coarse-to-fine matching strategy and build a top-down feature and matching enhancement scheme that is coupled with the multi-scale hierarchy of deep convolutional neural networks. During feature enhancement, intra-scale enhancement fuses same-resolution feature maps from multiple layers together via local self-attention and cross-scale enhancement hallucinates higher-resolution feature maps along the top-down pathway. Besides, we learn complementary matching details at different scales thus the overall matching score is refined by features of different semantic levels gradually. Our multi-scale matching network can be trained end-to-end easily with few additional learnable parameters. Experimental results demonstrate that the proposed method achieves state-of-the-art performance on three popular benchmarks with high computational efficiency. The code has been released at https://github.com/wintersun661/MMNet.},
  archive   = {C_ICCV},
  author    = {Dongyang Zhao and Ziyang Song and Zhenghao Ji and Gangming Zhao and Weifeng Ge and Yizhou Yu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00334},
  pages     = {3334-3344},
  title     = {Multi-scale matching networks for semantic correspondence},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Long-term temporally consistent unpaired video translation
from simulated surgical 3D data. <em>ICCV</em>, 3323–3333. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Research in unpaired video translation has mainly focused on short-term temporal consistency by conditioning on neighboring frames. However for transfer from simulated to photorealistic sequences, available information on the underlying geometry offers potential for achieving global consistency across views. We propose a novel approach which combines unpaired image translation with neural rendering to transfer simulated to photorealistic surgical abdominal scenes. By introducing global learnable textures and a lighting-invariant view-consistency loss, our method produces consistent translations of arbitrary views and thus enables long-term consistent video synthesis. We design and test our model to generate video sequences from minimally-invasive surgical abdominal scenes. Because labeled data is often limited in this domain, photorealistic data where ground truth information from the simulated domain is preserved is especially relevant. By extending existing image-based methods to view-consistent videos, we aim to impact the applicability of simulated training and evaluation environments for surgical applications. Code and data: http://opencas.dkfz.de/video-sim2real.},
  archive   = {C_ICCV},
  author    = {Dominik Rivoir and Micha Pfeiffer and Reuben Docea and Fiona Kolbinger and Carina Riediger and Jürgen Weitz and Stefanie Speidel},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00333},
  pages     = {3323-3333},
  title     = {Long-term temporally consistent unpaired video translation from simulated surgical 3D data},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Personalized and invertible face de-identification by
disentangled identity information manipulation. <em>ICCV</em>,
3314–3322. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The popularization of intelligent devices including smartphones and surveillance cameras results in more serious privacy issues. De-identification is regarded as an effective tool for visual privacy protection with the process of concealing or replacing identity information. Most of the existing de-identification methods suffer from some limitations since they mainly focus on the protection process and are usually non-reversible. In this paper, we propose a personalized and invertible de-identification method based on the deep generative model, where the main idea is introducing a user-specific password and an adjustable parameter to control the direction and degree of identity variation. Extensive experiments demonstrate the effectiveness and generalization of our proposed framework for both face de-identification and recovery.},
  archive   = {C_ICCV},
  author    = {Jingyi Cao and Bo Liu and Yunqian Wen and Rong Xie and Li Song},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00332},
  pages     = {3314-3322},
  title     = {Personalized and invertible face de-identification by disentangled identity information manipulation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GarmentNets: Category-level pose estimation for garments via
canonical space shape completion. <em>ICCV</em>, 3304–3313. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper tackles the task of category-level pose estimation for garments. With a near infinite degree of freedom, a garment’s full configuration (i.e., poses) is often described by the per-vertex 3D locations of its entire 3D surface. However, garments are also commonly subject to extreme cases of self-occlusion, especially when folded or crumpled, making it challenging to perceive their full 3D surface. To address these challenges, we propose GarmentNets, where the key idea is to formulate the deformable object pose estimation problem as a shape completion task in the canonical space. This canonical space is defined across garments instances within a category, therefore, specifies the shared category-level pose. By mapping the observed partial surface to the canonical space and completing it in this space, the output representation describes the garment’s full configuration using a complete 3D mesh with the per-vertex canonical coordinate label. To properly handle the thin 3D structure presented on garments, we proposed a novel 3D shape representation using the generalized winding number field. Experiments demonstrate that GarmentNets is able to generalize to unseen garment instances and achieve significantly better performance compared to alternative approaches. Code and data can be found in https://garmentnets.cs.columbia.edu.},
  archive   = {C_ICCV},
  author    = {Cheng Chi and Shuran Song},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00331},
  pages     = {3304-3313},
  title     = {GarmentNets: Category-level pose estimation for garments via canonical space shape completion},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). PICCOLO: Point cloud-centric omnidirectional localization.
<em>ICCV</em>, 3293–3303. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present PICCOLO, a simple and efficient algorithm for omnidirectional localization. Given a colored point cloud and a 360 ˝ panorama image of a scene, our objective is to recover the camera pose at which the panorama image is taken. Our pipeline works in an off-the-shelf manner with a single image given as a query and does not require any training of neural networks or collecting ground-truth poses of images. Instead, we match each point cloud color to the holistic view of the panorama image with gradient-descent optimization to find the camera pose. Our loss function, called sampling loss, is point cloud-centric, evaluated at the projected location of every point in the point cloud. In contrast, conventional photometric loss is image-centric, comparing colors at each pixel location. With a simple change in the compared entities, sampling loss effectively overcomes the severe visual distortion of omnidirectional images, and enjoys the global context of the 360 ˝ view to handle challenging scenarios for visual localization. PICCOLO outperforms existing omnidirectional localization algorithms in both accuracy and stability when evaluated in various environments.},
  archive   = {C_ICCV},
  author    = {Junho Kim and Changwoon Choi and Hojun Jang and Young Min Kim},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00330},
  pages     = {3293-3303},
  title     = {PICCOLO: Point cloud-centric omnidirectional localization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RePOSE: Fast 6D object pose refinement via deep texture
rendering. <em>ICCV</em>, 3283–3292. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present RePOSE, a fast iterative refinement method for 6D object pose estimation. Prior methods perform refinement by feeding zoomed-in input and rendered RGB images into a CNN and directly regressing an update of a refined pose. Their runtime is slow due to the computational cost of CNN, which is especially prominent in multiple-object pose refinement. To overcome this problem, RePOSE leverages image rendering for fast feature extraction using a 3D model with a learnable texture. We call this deep texture rendering, which uses a shallow multilayer perceptron to directly regress a view-invariant image representation of an object. Furthermore, we utilize differentiable Levenberg-Marquardt (LM) optimization to refine a pose fast and accurately by minimizing the distance between the input and rendered image representations without the need of zooming in. These image representations are trained such that differentiable LM optimization converges within few iterations. Consequently, RePOSE runs at 92 FPS and achieves state-of-the-art accuracy of 51.6\% on the Occlusion LineMOD dataset - a 4.1\% absolute improvement over the prior art, and comparable result on the YCB-Video dataset with a much faster runtime. The code is available at https://github.com/sh8/repose.},
  archive   = {C_ICCV},
  author    = {Shun Iwase and Xingyu Liu and Rawal Khirodkar and Rio Yokota and Kris M. Kitani},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00329},
  pages     = {3283-3292},
  title     = {RePOSE: Fast 6D object pose refinement via deep texture rendering},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploring geometry-aware contrast and clustering
harmonization for self-supervised 3D object detection. <em>ICCV</em>,
3273–3282. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current 3D object detection paradigms highly rely on extensive annotation efforts, which makes them not practical in many real-world industrial applications. Inspired by that a human driver can keep accumulating experiences from self-exploring the roads without any tutor’s guidance, we first step forwards to explore a simple yet effective self-supervised learning framework tailored for LiDAR-based 3D object detection. Although the self-supervised pipeline has achieved great success in 2D domain, the characteristic challenges (e.g., complex geometry structure and various 3D object views) encountered in the 3D domain hinder the direct adoption of existing techniques that often contrast the 2D augmented data or cluster single-view features. Here we present a novel self-supervised 3D Object detection framework that seamlessly integrates the geometry-aware contrast and clustering harmonization to lift the unsupervised 3D representation learning, named GCC-3D. First, GCC-3D introduces a Geometric-Aware Contrastive objective to learn spatial-sensitive local structure representation. This objective enforces the spatially close voxels to have high feature similarity. Second, a Pseudo-Instance Clustering harmonization mechanism is proposed to encourage that different views of pseudo-instances should have consistent similarities to clustering prototype centers. This module endows our model semantic discriminative capacity. Extensive experiments demonstrate our GCC-3D achieves significant performance improvement on data-efficient 3D object detection benchmarks (nuScenes and Waymo). Moreover, our GCC-3D framework can achieve state-of-the art performances on all popular 3D object detection benchmarks.},
  archive   = {C_ICCV},
  author    = {Hanxue Liang and Chenhan Jiang and Dapeng Feng and Xin Chen and Hang Xu and Xiaodan Liang and Wei Zhang and Zhenguo Li and Luc Van Gool},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00328},
  pages     = {3273-3282},
  title     = {Exploring geometry-aware contrast and clustering harmonization for self-supervised 3D object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RandomRooms: Unsupervised pre-training from synthetic shapes
and randomized layouts for 3D object detection. <em>ICCV</em>,
3263–3272. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D point cloud understanding has made great progress in recent years. However, one major bottleneck is the scarcity of annotated real datasets, especially compared to 2D object detection tasks, since a large amount of labor is involved in annotating the real scans of a scene. A promising solution to this problem is to make better use of the synthetic dataset, which consists of CAD object models, to boost the learning on real datasets. This can be achieved by the pre-training and fine-tuning procedure. However, recent work on 3D pre-training exhibits failure when transfer features learned on synthetic objects to other real-world applications. In this work, we put forward a new method called RandomRooms to accomplish this objective. In particular, we propose to generate random layouts of a scene by making use of the objects in the synthetic CAD dataset and learn the 3D scene representation by applying object-level contrastive learning on two random scenes generated from the same set of synthetic objects. The model pre-trained in this way can serve as a better initialization when later fine-tuning on the 3D object detection task. Empirically, we show consistent improvement in downstream 3D detection tasks on several base models, especially when less training data are used, which strongly demonstrates the effectiveness and generalization of our method. Benefiting from the rich semantic knowledge and diverse objects from synthetic data, our method establishes the new state-of-the-art on widely-used 3D detection benchmarks ScanNetV2 and SUN RGB-D. We expect our attempt to provide a new perspective for bridging object and scene-level 3D understanding.},
  archive   = {C_ICCV},
  author    = {Yongming Rao and Benlin Liu and Yi Wei and Jiwen Lu and Cho-Jui Hsieh and Jie Zhou},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00327},
  pages     = {3263-3272},
  title     = {RandomRooms: Unsupervised pre-training from synthetic shapes and randomized layouts for 3D object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-source domain adaptation for object detection.
<em>ICCV</em>, 3253–3262. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To reduce annotation labor associated with object detection, an increasing number of studies focus on transferring the learned knowledge from a labeled source domain to another unlabeled target domain. However, existing methods assume that the labeled data are sampled from a single source domain, which ignores a more generalized scenario, where labeled data are from multiple source domains. For the more challenging task, we propose a unified Faster R-CNN based framework, termed Divide-and-Merge Spindle Network (DMSN), which can simultaneously enhance domain invariance and preserve discriminative power. Specifically, the framework contains multiple source subnets and a pseudo target subnet. First, we propose a hierarchical feature alignment strategy to conduct strong and weak alignments for low- and high-level features, respectively, considering their different effects for object detection. Second, we develop a novel pseudo subnet learning algorithm to approximate optimal parameters of pseudo target subset by weighted combination of parameters in different source subnets. Finally, a consistency regularization for region proposal network is proposed to facilitate each subnet to learn more abstract invariances. Extensive experiments on different adaptation scenarios demonstrate the effectiveness of the proposed model.},
  archive   = {C_ICCV},
  author    = {Xingxu Yao and Sicheng Zhao and Pengfei Xu and Jufeng Yang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00326},
  pages     = {3253-3262},
  title     = {Multi-source domain adaptation for object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Query adaptive few-shot object detection with heterogeneous
graph convolutional networks. <em>ICCV</em>, 3243–3252. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Few-shot object detection (FSOD) aims to detect never-seen objects using few examples. This field sees recent improvement owing to the meta-learning techniques by learning how to match between the query image and few-shot class examples, such that the learned model can generalize to few-shot novel classes. However, currently, most of the meta-learning-based methods perform parwise matching between query image regions (usually proposals) and novel classes separately, therefore failing to take into account multiple relationships among them. In this paper, we propose a novel FSOD model using heterogeneous graph convolutional networks. Through efficient message passing among all the proposal and class nodes with three different types of edges, we could obtain context-aware proposal features and query-adaptive, multiclass-enhanced prototype representations for each class, which could help promote the pairwise matching and improve final FSOD accuracy. Extensive experimental results show that our proposed model, denoted as QA-FewDet, outperforms the current state-of-the-art approaches on the PASCAL VOC and MSCOCO FSOD benchmarks under different shots and evaluation metrics.},
  archive   = {C_ICCV},
  author    = {Guangxing Han and Yicheng He and Shiyuan Huang and Jiawei Ma and Shih-Fu Chang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00325},
  pages     = {3243-3252},
  title     = {Query adaptive few-shot object detection with heterogeneous graph convolutional networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Continual learning for image-based camera localization.
<em>ICCV</em>, 3232–3242. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For several emerging technologies such as augmented reality, autonomous driving and robotics, visual localization is a critical component. Directly regressing camera pose/3D scene coordinates from the input image using deep neural networks has shown great potential. However, such methods assume a stationary data distribution with all scenes simultaneously available during training. In this paper, we approach the problem of visual localization in a continual learning setup – whereby the model is trained on scenes in an incremental manner. Our results show that similar to the classification domain, non-stationary data induces catastrophic forgetting in deep networks for visual localization. To address this issue, a strong baseline based on storing and replaying images from a fixed buffer is proposed. Furthermore, we propose a new sampling method based on coverage score (Buff-CS) that adapts the existing sampling strategies in the buffering process to the problem of visual localization. Results demonstrate consistent improvements over standard buffering methods on two challenging datasets – 7Scenes, 12Scenes, and also 19Scenes by combining the former scenes 1 .},
  archive   = {C_ICCV},
  author    = {Shuzhe Wang and Zakaria Laskar and Iaroslav Melekhov and Xiaotian Li and Juho Kannala},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00324},
  pages     = {3232-3242},
  title     = {Continual learning for image-based camera localization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient large scale inlier voting for geometric vision
problems. <em>ICCV</em>, 3223–3231. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Outlier rejection and, equivalently, inlier set optimization is a key ingredient in numerous applications in computer vision such as filtering point-matches in camera pose estimation or plane and normal estimation in point clouds. Several approaches exist, yet at large scale we face a combinatorial explosion of possible solutions and state-of-the-art methods like RANSAC, Hough transform, or Branch&amp;Bound require a minimum inlier ratio or prior knowledge to remain practical. In fact, for problems such as camera posing in very large scenes these approaches become useless as they have exponential runtime growth.To approach the problem, we present an efficient and general algorithm for outlier rejection based on “intersecting” k-dimensional surfaces in R d . We provide a recipe for formulating a variety of geometric problems as finding a point in R d which maximizes the number of nearby surfaces (and thus inliers). The resulting algorithm has linear worst-case complexity with a better runtime dependency on the requested proximity of a query to its result than competing algorithms, while not requiring domain specific bounds. This is achieved by introducing a space decomposition scheme that bounds the number of computations by successively rounding and grouping surfaces. Our recipe and open-source code 1 enables anybody to derive such fast approaches to new problems across a wide range of domains. We demonstrate the approach on several camera posing problems with a large number of matches and low inlier ratio, achieving state-of-the-art results at significantly lower processing times.},
  archive   = {C_ICCV},
  author    = {Dror Aiger and Simon Lynen and Jan Hosang and Bernhard Zeisl},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00323},
  pages     = {3223-3231},
  title     = {Efficient large scale inlier voting for geometric vision problems},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Uniformity in heterogeneity: Diving deep into count interval
partition for crowd counting. <em>ICCV</em>, 3214–3222. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, the problem of inaccurate learning targets in crowd counting draws increasing attention. Inspired by a few pioneering work, we solve this problem by trying to predict the indices of pre-defined interval bins of counts instead of the count values themselves. However, an inappropriate interval setting might make the count error contributions from different intervals extremely imbalanced, leading to inferior counting performance. Therefore, we propose a novel count interval partition criterion called Uniform Error Partition (UEP), which always keeps the expected counting error contributions equal for all intervals to minimize the prediction risk. Then to mitigate the inevitably introduced discretization errors in the count quantization process, we propose another criterion called Mean Count Proxies (MCP). The MCP criterion selects the best count proxy for each interval to represent its count value during inference, making the overall expected discretization error of an image nearly negligible. As far as we are aware, this work is the first to delve into such a classification task and ends up with a promising solution for count interval partition. Following the above two theoretically demonstrated criterions, we propose a simple yet effective model termed Uniform Error Partition Network (UEPNet), which achieves state-of-the-art performance on several challenging datasets. The codes will be available at: TencentYoutuResearch/CrowdCounting-UEPNet.},
  archive   = {C_ICCV},
  author    = {Changan Wang and Qingyu Song and Boshen Zhang and Yabiao Wang and Ying Tai and Xuyi Hu and Chengjie Wang and Jilin Li and Jiayi Ma and Yang Wu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00322},
  pages     = {3214-3222},
  title     = {Uniformity in heterogeneity: Diving deep into count interval partition for crowd counting},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Are we missing confidence in pseudo-LiDAR methods for
monocular 3D object detection? <em>ICCV</em>, 3205–3213. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pseudo-LiDAR-based methods for monocular 3D object detection have received considerable attention in the community due to the performance gains exhibited on the KITTI3D benchmark, in particular on the commonly reported validation split. This generated a distorted impression about the superiority of Pseudo-LiDAR-based (PL-based) approaches over methods working with RGB images only. Our first contribution consists in rectifying this view by pointing out and showing experimentally that the validation results published by PL-based methods are substantially biased. The source of the bias resides in an overlap between the KITTI3D object detection validation set and the training/validation sets used to train depth predictors feeding PL-based methods. Surprisingly, the bias remains also after geographically removing the overlap. This leaves the test set as the only reliable set for comparison, where published PL-based methods do not excel. Our second contribution brings PL-based methods back up in the ranking with the design of a novel deep architecture which introduces a 3D confidence prediction module. We show that 3D confidence estimation techniques derived from RGB-only 3D detection approaches can be successfully integrated into our framework and, more importantly, that improved performance can be obtained with a newly designed 3D confidence measure, leading to state-of-the-art performance on the KITTI3D benchmark.},
  archive   = {C_ICCV},
  author    = {Andrea Simonelli and Samuel Rota Bulò and Lorenzo Porzi and Peter Kontschieder and Elisa Ricci},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00321},
  pages     = {3205-3213},
  title     = {Are we missing confidence in pseudo-LiDAR methods for monocular 3D object detection?},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploiting sample correlation for crowd counting with
multi-expert network. <em>ICCV</em>, 3195–3204. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Crowd counting is a difficult task because of the diversity of scenes. Most of the existing crowd counting methods adopt complex structures with massive backbones to enhance the generalization ability. Unfortunately, the performance of existing methods on large-scale data sets is not satisfactory. In order to handle various scenarios with less complex network, we explored how to efficiently use the multi-expert model for crowd counting tasks. We mainly focus on how to train more efficient expert networks and how to choose the most suitable expert. Specifically, we propose a task-driven similarity metric based on sample’s mutual enhancement, referred as co-fine-tune similarity, which can find a more efficient subset of data for training the expert network. Similar samples are considered as a cluster which is used to obtain parameters of an expert. Besides, to make better use of the proposed method, we design a simple network called FPN with Deconvolution Counting Network, which is a more suitable base model for the multi-expert counting network. Experimental results show that multiple experts FDC (MFDC) achieves the best performance on four public data sets, including the large scale NWPU-Crowd data set. Furthermore, the MFDC trained on an extensive dense crowd data set can generalize well on the other data sets without extra training or fine-tuning. 1},
  archive   = {C_ICCV},
  author    = {Xinyan Liu and Guorong Li and Zhenjun Han and Weigang Zhang and Yifan Yang and Qingming Huang and Nicu Sebe},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00320},
  pages     = {3195-3204},
  title     = {Exploiting sample correlation for crowd counting with multi-expert network},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards a universal model for cross-dataset crowd counting.
<em>ICCV</em>, 3185–3194. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes to handle the practical problem of learning a universal model for crowd counting across scenes and datasets. We dissect that the crux of this problem is the catastrophic sensitivity of crowd counters to scale shift, which is very common in the real world and caused by factors such as different scene layouts and image resolutions. Therefore it is difficult to train a universal model that can be applied to various scenes. To address this problem, we propose scale alignment as a prime module for establishing a novel crowd counting framework. We derive a closed-form solution to get the optimal image rescaling factors for alignment by minimizing the distances between their scale distributions. A novel neural network together with a loss function based on an efficient sliced Wasserstein distance is also proposed for scale distribution estimation. Benefiting from the proposed method, we have learned a universal model that generally works well on several datasets where can even outperform state-of-the-art models that are particularly fine-tuned for each dataset significantly. Experiments also demonstrate the much better generalizability of our model to unseen scenes.},
  archive   = {C_ICCV},
  author    = {Zhiheng Ma and Xiaopeng Hong and Xing Wei and Yunfeng Qiu and Yihong Gong},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00319},
  pages     = {3185-3194},
  title     = {Towards a universal model for cross-dataset crowd counting},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CrossDet: Crossline representation for object detection.
<em>ICCV</em>, 3175–3184. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Object detection aims to accurately locate and classify objects in an image, which requires precise object representations. Existing methods usually use rectangular anchor boxes or a set of points to represent objects. However, these methods either introduce background noise or miss the continuous appearance information inside the object, and thus cause incorrect detection results. In this paper, we propose a novel anchor-free object detection network, called Cross-Det, which uses a set of growing cross lines along horizontal and vertical axes as object representations. An object can be flexibly represented as cross lines in different combinations. It not only can effectively reduce the interference of noise, but also take into account the continuous object information, which is useful to enhance the discriminability of object features and find the object boundaries. Based on the learned cross lines, we propose a crossline extraction module to adaptively capture features of cross lines. Furthermore, we design a decoupled regression mechanism to regress the localization along the horizontal and vertical directions respectively, which helps to decrease the optimization difficulty because the optimization space is limited to a specific direction. Our method achieves consistently improvement on the PASCAL VOC and MS-COCO datasets. The experiment results demonstrate the effectiveness of our proposed method. Code can be available at: https://github.com/QiuHeqian/CrossDet.},
  archive   = {C_ICCV},
  author    = {Heqian Qiu and Hongliang Li and Qingbo Wu and Jianhua Cui and Zichen Song and Lanxiao Wang and Minjian Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00318},
  pages     = {3175-3184},
  title     = {CrossDet: Crossline representation for object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). You don’t only look once: Constructing spatial-temporal
memory for integrated 3D object detection and tracking. <em>ICCV</em>,
3165–3174. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans are able to continuously detect and track surrounding objects by constructing a spatial-temporal memory of the objects when looking around. In contrast, 3D object detectors in existing tracking-by-detection systems often search for objects in every new video frame from scratch, without fully leveraging memory from previous detection results. In this work, we propose a novel system for integrated 3D object detection and tracking, which uses a dynamic object occupancy map and previous object states as spatial-temporal memory to assist object detection in future frames. This memory, together with the ego-motion from back-end odometry, guides the detector to achieve more efficient object proposal generation and more accurate object state estimation. The experiments demonstrate the effectiveness of the proposed system and its performance on the ScanNet and KITTI datasets. Moreover, the proposed system produces stable bounding boxes and pose trajectories over time, while being able to handle occluded and truncated objects. Code is available at the project page: https://zju3dv.github.io/UDOLO.},
  archive   = {C_ICCV},
  author    = {Jiaming Sun and Yiming Xie and Siyu Zhang and Linghao Chen and Guofeng Zhang and Hujun Bao and Xiaowei Zhou},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00317},
  pages     = {3165-3174},
  title     = {You don’t only look once: Constructing spatial-temporal memory for integrated 3D object detection and tracking},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detecting invisible people. <em>ICCV</em>, 3154–3164. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Monocular object detection and tracking have improved drastically in recent years, but rely on a key assumption: that objects are visible to the camera. Many offline tracking approaches reason about occluded objects post-hoc, by linking together tracklets after the object re-appears, making use of reidentification (ReID). However, online tracking in embodied robotic agents (such as a self-driving vehicle) fundamentally requires object permanence, which is the ability to reason about occluded objects before they re-appear. In this work, we re-purpose tracking benchmarks and propose new metrics for the task of detecting invisible objects, focusing on the illustrative case of people. We demonstrate that current detection and tracking systems perform dramatically worse on this task. We introduce two key innovations to recover much of this performance drop. We treat occluded object detection in temporal sequences as a short-term forecasting challenge, bringing to bear tools from dynamic sequence prediction. Second, we build dynamic models that explicitly reason in 3D from monocular videos without calibration, using observations produced by monocular depth estimators. To our knowledge, ours is the first work to demonstrate the effectiveness of monocular depth estimation for the task of tracking and detecting occluded objects. Our approach strongly improves by 11.4\% over the baseline in ablations and by 5.0\% over the state-of-the-art in F1 score.},
  archive   = {C_ICCV},
  author    = {Tarasha Khurana and Achal Dave and Deva Ramanan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00316},
  pages     = {3154-3164},
  title     = {Detecting invisible people},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Voxel transformer for 3D object detection. <em>ICCV</em>,
3144–3153. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present Voxel Transformer (VoTr), a novel and effective voxel-based Transformer backbone for 3D object detection from point clouds. Conventional 3D convolutional backbones in voxel-based 3D detectors cannot efficiently capture large context information, which is crucial for object recognition and localization, owing to the limited receptive fields. In this paper, we resolve the problem by introducing a Transformer-based architecture that enables long-range relationships between voxels by self-attention. Given the fact that non-empty voxels are naturally sparse but numerous, directly applying standard Transformer on voxels is non-trivial. To this end, we propose the sparse voxel module and the submanifold voxel module, which can operate on the empty and non-empty voxel positions effectively. To further enlarge the attention range while maintaining comparable computational overhead to the convolutional counterparts, we propose two attention mechanisms for multi-head attention in those two modules: Local Attention and Dilated Attention, and we further propose Fast Voxel Query to accelerate the querying process in multi-head attention. VoTr contains a series of sparse and submanifold voxel modules, and can be applied in most voxel-based detectors. Our proposed VoTr shows consistent improvement over the convolutional baselines while maintaining computational efficiency on the KITTI dataset and the Waymo Open dataset.},
  archive   = {C_ICCV},
  author    = {Jiageng Mao and Yujing Xue and Minzhe Niu and Haoyue Bai and Jiashi Feng and Xiaodan Liang and Hang Xu and Chunjing Xu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00315},
  pages     = {3144-3153},
  title     = {Voxel transformer for 3D object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LIGA-stereo: Learning LiDAR geometry aware representations
for stereo-based 3D detector. <em>ICCV</em>, 3133–3143. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Stereo-based 3D detection aims at detecting 3D objects from stereo images, which provides a low-cost solution for 3D perception. However, its performance is still inferior compared with LiDAR-based detection algorithms. To detect and localize accurate 3D bounding boxes, LiDAR-based detectors encode high-level representations from Li-DAR point clouds, such as accurate object boundaries and surface normal directions. In contrast, high-level features learned by stereo-based detectors are easily affected by the erroneous depth estimation due to the limitation of stereo matching. To solve the problem, we propose LIGA-Stereo (LiDAR Geometry Aware Stereo Detector) to learn stereo-based 3D detectors under the guidance of high-level geometry-aware representations of LiDAR-based detection models. In addition, we found existing voxel-based stereo detectors failed to learn semantic features effectively from indirect 3D supervisions. We attach an auxiliary 2D detection head to provide direct 2D semantic supervisions. Experiment results show that the above two strategies improved the geometric and semantic representation capabilities. Compared with the state-of-the-art stereo detector, our method has improved the 3D detection performance of cars, pedestrians, cyclists by 10.44\%, 5.69\%, 5.97\% mAP respectively on the official KITTI benchmark. The gap between stereo-based and LiDAR-based 3D detectors is further narrowed. The code is available at https://xy-guo.github.io/liga/.},
  archive   = {C_ICCV},
  author    = {Xiaoyang Guo and Shaoshuai Shi and Xiaogang Wang and Hongsheng Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00314},
  pages     = {3133-3143},
  title     = {LIGA-stereo: Learning LiDAR geometry aware representations for stereo-based 3D detector},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Is pseudo-lidar needed for monocular 3D object detection?
<em>ICCV</em>, 3122–3132. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent progress in 3D object detection from single images leverages monocular depth estimation as a way to produce 3D pointclouds, turning cameras into pseudo-lidar sensors. These two-stage detectors improve with the accuracy of the intermediate depth estimation network, which can itself be improved without manual labels via large-scale self-supervised learning. However, they tend to suffer from overfitting more than end-to-end methods, are more complex, and the gap with similar lidar-based detectors remains significant. In this work, we propose an end-to-end, single stage, monocular 3D object detector, DD3D, that can benefit from depth pre-training like pseudo-lidar methods, but without their limitations. Our architecture is designed for effective information transfer between depth estimation and 3D detection, allowing us to scale with the amount of unlabeled pre-training data. Our method achieves state-of-the-art results on two challenging benchmarks, with 16.34\% and 9.28\% AP for Cars and Pedestrians (respectively) on the KITTI-3D benchmark, and 41.5\% mAP on NuScenes.},
  archive   = {C_ICCV},
  author    = {Dennis Park and Rareş Ambruş and Vitor Guizilini and Jie Li and Adrien Gaidon},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00313},
  pages     = {3122-3132},
  title     = {Is pseudo-lidar needed for monocular 3D object detection?},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). OMNet: Learning overlapping mask for partial-to-partial
point cloud registration. <em>ICCV</em>, 3112–3121. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Point cloud registration is a key task in many computational fields. Previous correspondence matching based methods require the inputs to have distinctive geometric structures to fit a 3D rigid transformation according to point-wise sparse feature matches. However, the accuracy of transformation heavily relies on the quality of extracted features, which are prone to errors with respect to partiality and noise. In addition, they can not utilize the geometric knowledge of all the overlapping regions. On the other hand, previous global feature based approaches can utilize the entire point cloud for the registration, however they ignore the negative effect of non-overlapping points when aggregating global features. In this paper, we present OM-Net, a global feature based iterative network for partial-to-partial point cloud registration. We learn overlapping masks to reject non-overlapping regions, which converts the partial-to-partial registration to the registration of the same shape. Moreover, the previously used data is sampled only once from the CAD models for each object, resulting in the same point clouds for the source and reference. We propose a more practical manner of data generation where a CAD model is sampled twice for the source and reference, avoiding the previously prevalent over-fitting issue. Experimental results show that our method achieves state-of-the-art performance compared to traditional and deep learning based methods. Code is available at https://github.com/megvii-research/OMNet.},
  archive   = {C_ICCV},
  author    = {Hao Xu and Shuaicheng Liu and Guangfu Wang and Guanghui Liu and Bing Zeng},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00312},
  pages     = {3112-3121},
  title     = {OMNet: Learning overlapping mask for partial-to-partial point cloud registration},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-instance pose networks: Rethinking top-down pose
estimation. <em>ICCV</em>, 3102–3111. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A key assumption of top-down human pose estimation approaches is their expectation of having a single person/instance present in the input bounding box. This often leads to failures in crowded scenes with occlusions. We propose a novel solution to overcome the limitations of this fundamental assumption. Our Multi-Instance Pose Network (MIPNet) allows for predicting multiple 2D pose instances within a given bounding box. We introduce a Multi-Instance Modulation Block (MIMB) that can adaptively modulate channel-wise feature responses for each instance and is parameter efficient. We demonstrate the efficacy of our approach by evaluating on COCO, CrowdPose, and OCHuman datasets. Specifically, we achieve 70.0 AP on CrowdPose and 42.5 AP on OCHuman test sets, a significant improvement of 2.4 AP and 6.5 AP over the prior art, respectively. When using ground truth bounding boxes for inference, MIP-Net achieves an improvement of 0.7 AP on COCO, 0.9 AP on CrowdPose, and 9.1 AP on OCHuman validation sets compared to HRNet. Interestingly, when fewer, high confidence bounding boxes are used, HRNet’s performance degrades (by 5 AP) on OCHuman, whereas MIPNet maintains a relatively stable performance (drop of 1 AP) for the same inputs.},
  archive   = {C_ICCV},
  author    = {Rawal Khirodkar and Visesh Chari and Amit Agrawal and Ambrish Tyagi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00311},
  pages     = {3102-3111},
  title     = {Multi-instance pose networks: Rethinking top-down pose estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Geometry uncertainty projection network for monocular 3D
object detection. <em>ICCV</em>, 3091–3101. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Geometry Projection is a powerful depth estimation method in monocular 3D object detection. It estimates depth dependent on heights, which introduces mathematical priors into the deep model. But projection process also introduces the error amplification problem, in which the error of the estimated height will be amplified and reflected greatly at the output depth. This property leads to uncontrollable depth inferences and also damages the training efficiency. In this paper, we propose a Geometry Uncertainty Projection Network (GUP Net) to tackle the error amplification problem at both inference and training stages. Specifically, a GUP module is proposed to obtains the geometry-guided uncertainty of the inferred depth, which not only provides high reliable confidence for each depth but also benefits depth learning. Furthermore, at the training stage, we propose a Hierarchical Task Learning strategy to reduce the instability caused by error amplification. This learning algorithm monitors the learning situation of each task by a proposed indicator and adaptively assigns the proper loss weights for different tasks according to their pre-tasks situation. Based on that, each task starts learning only when its pre-tasks are learned well, which can significantly improve the stability and efficiency of the training process. Extensive experiments demonstrate the effectiveness of the proposed method. The overall model can infer more reliable object depth than existing methods and outperforms the state-of-the-art image-based monocular 3D detectors by 3.74\% and 4.7\% AP 40 of the car and pedestrian categories on the KITTI benchmark. The code and model will be released at https://github.com/SuperMHP/GUPNet.},
  archive   = {C_ICCV},
  author    = {Yan Lu and Xinzhu Ma and Lei Yang and Tianzhu Zhang and Yating Liu and Qi Chu and Junjie Yan and Wanli Ouyang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00310},
  pages     = {3091-3101},
  title     = {Geometry uncertainty projection network for monocular 3D object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021d). MLVSNet: Multi-level voting siamese network for 3D visual
tracking. <em>ICCV</em>, 3081–3090. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Benefiting from the excellent performance of Siamese-based trackers, huge progress on 2D visual tracking has been achieved. However, 3D visual tracking is still under-explored. Inspired by the idea of Hough voting in 3D object detection, in this paper, we propose a Multi-level Voting Siamese Network (MLVSNet) for 3D visual tracking from outdoor point cloud sequences. To deal with sparsity in outdoor 3D point clouds, we propose to perform Hough voting on multi-level features to get more vote centers and retain more useful information, instead of voting only on the fi-nal level feature as in previous methods. We also design an efficient and lightweight Target-Guided Attention (TGA) module to transfer the target information and highlight the target points in the search area. Moreover, we propose a Vote-cluster Feature Enhancement (VFE) module to exploit the relationships between different vote clusters. Extensive experiments on the 3D tracking benchmark of KITTI dataset demonstrate that our MLVSNet outperforms state-of-the-art methods with significant margins. Code will be available at https://github.com/CodeWZT/MLVSNet.},
  archive   = {C_ICCV},
  author    = {Zhoutao Wang and Qian Xie and Yu-Kun Lai and Jing Wu and Kun Long and Jun Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00309},
  pages     = {3081-3090},
  title     = {MLVSNet: Multi-level voting siamese network for 3D visual tracking},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Causal attention for unbiased visual recognition.
<em>ICCV</em>, 3071–3080. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Attention module does not always help deep models learn causal features that are robust in any confounding context, e.g., a foreground object feature is invariant to different backgrounds. This is because the confounders trick the attention to capture spurious correlations that benefit the prediction when the training and testing data are IID (identical &amp; independent distribution); while harm the prediction when the data are OOD (out-of-distribution). The sole fundamental solution to learn causal attention is by causal intervention, which requires additional annotations of the confounders, e.g., a &quot;dog&quot; model is learned within &quot;grass+dog&quot; and &quot;road+dog&quot; respectively, so the &quot;grass&quot; and &quot;road&quot; contexts will no longer confound the &quot;dog&quot; recognition. However, such annotation is not only prohibitively expensive, but also inherently problematic, as the confounders are elusive in nature. In this paper, we propose a causal attention module (CaaM) that self-annotates the confounders in unsupervised fashion. In particular, multiple CaaMs can be stacked and integrated in conventional attention CNN and self-attention Vision Transformer. In OOD settings, deep models with CaaM outperform those without it significantly; even in IID settings, the attention localization is also improved by CaaM, showing a great potential in applications that require robust visual saliency. Codes are available at https://github.com/Wangt-CN/CaaM.},
  archive   = {C_ICCV},
  author    = {Tan Wang and Chang Zhou and Qianru Sun and Hanwang Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00308},
  pages     = {3071-3080},
  title     = {Causal attention for unbiased visual recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ADNet: Leveraging error-bias towards normal direction in
face alignment. <em>ICCV</em>, 3060–3070. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The recent progress of CNN has dramatically improved face alignment performance. However, few works have paid attention to the error-bias with respect to error distribution of facial landmarks. In this paper, we investigate the error-bias issue in face alignment, where the distributions of landmark errors tend to spread along the tangent line to landmark curves. This error-bias is not trivial since it is closely connected to the ambiguous landmark labeling task. Inspired by this observation, we seek a way to leverage the error-bias property for better convergence of CNN model. To this end, we propose anisotropic direction loss (ADL) and anisotropic attention module (AAM) for coordinate and heatmap regression, respectively. ADL imposes strong binding force in normal direction for each landmark point on facial boundaries. On the other hand, AAM is an attention module which can get anisotropic attention mask focusing on the region of point and its local edge connected by adjacent points, it has a stronger response in tangent than in normal, which means relaxed constraints in the tangent. These two methods work in a complementary manner to learn both facial structures and texture details. Finally, we integrate them into an optimized end-to-end training pipeline named ADNet. Our ADNet achieves state-of-the-art results on 300W, WFLW and COFW datasets, which demonstrates the effectiveness and robustness.},
  archive   = {C_ICCV},
  author    = {Yangyu Huang and Hao Yang and Chong Li and Jongyoo Kim and Fangyun Wei},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00307},
  pages     = {3060-3070},
  title     = {ADNet: Leveraging error-bias towards normal direction in face alignment},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CaT: Weakly supervised object detection with category
transfer. <em>ICCV</em>, 3050–3059. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A large gap exists between fully-supervised object detection and weakly-supervised object detection. To narrow this gap, some methods consider knowledge transfer from additional fully-supervised dataset. But these methods do not fully exploit discriminative category information in the fully-supervised dataset, thus causing low mAP. To solve this issue, we propose a novel category transfer framework for weakly supervised object detection. The intuition is to fully leverage both visually-discriminative and semantically-correlated category information in the fully-supervised dataset to enhance the object-classification ability of a weakly-supervised detector. To handle overlapping category transfer, we propose a double-supervision mean teacher to gather common category information and bridge the domain gap between two datasets. To handle non-overlapping category transfer, we propose a semantic graph convolutional network to promote the aggregation of semantic features between correlated categories. Experiments are conducted with Pascal VOC 2007 as the target weakly-supervised dataset and COCO as the source fully-supervised dataset. Our category transfer framework achieves 63.5\% mAP and 80.3\% CorLoc with 5 overlapping categories between two datasets, which outperforms the state-of-the-art methods. Codes are avaliable at https://github.com/MediaBrain-SJTU/CaT.},
  archive   = {C_ICCV},
  author    = {Tianyue Cao and Lianyu Du and Xiaoyun Zhang and Siheng Chen and Ya Zhang and Yan-Feng Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00306},
  pages     = {3050-3059},
  title     = {CaT: Weakly supervised object detection with category transfer},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). End-to-end semi-supervised object detection with soft
teacher. <em>ICCV</em>, 3040–3049. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an end-to-end semi-supervised object detection approach, in contrast to previous more complex multi-stage methods. The end-to-end training gradually improves pseudo label qualities during the curriculum, and the more and more accurate pseudo labels in turn benefit object detection training. We also propose two simple yet effective techniques within this framework: a soft teacher mechanism where the classification loss of each unlabeled bounding box is weighed by the classification score produced by the teacher network; a box jittering approach to select reliable pseudo boxes for the learning of box regression. On the COCO benchmark, the proposed approach outperforms previous methods by a large margin under various labeling ratios, i.e. 1\%, 5\% and 10\%. Moreover, our approach proves to perform also well when the amount of labeled data is relatively large. For example, it can improve a 40.9 mAP baseline detector trained using the full COCO training set by +3.6 mAP, reaching 44.5 mAP, by leveraging the 123K unlabeled images of COCO. On the state-of-the-art Swin Transformer based object detector (58.9 mAP on test-dev), it can still significantly improve the detection accuracy by +1.5 mAP, reaching 60.4 mAP, and improve the instance segmentation accuracy by +1.2 mAP, reaching 52.4 mAP. Further incorporating with the Object365 pre-trained model, the detection accuracy reaches 61.3 mAP and the instance segmentation accuracy reaches 53.0 mAP, pushing the new state-of-the-art. The code and models will be made publicly available at https://github.com/microsoft/SoftTeacher.},
  archive   = {C_ICCV},
  author    = {Mengde Xu and Zheng Zhang and Han Hu and Jianfeng Wang and Lijuan Wang and Fangyun Wei and Xiang Bai and Zicheng Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00305},
  pages     = {3040-3049},
  title     = {End-to-end semi-supervised object detection with soft teacher},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robust small-scale pedestrian detection with cued recall via
memory learning. <em>ICCV</em>, 3030–3039. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although the visual appearances of small-scale objects are not well observed, humans can recognize them by associating the visual cues of small objects from their memorized appearance. It is called cued recall. In this paper, motivated by the memory process of humans, we introduce a novel pedestrian detection framework that imitates cued recall in detecting small-scale pedestrians. We propose a large-scale embedding learning with the large-scale pedestrian recalling memory (LPR Memory). The purpose of the proposed large-scale embedding learning is to memorize and recall the large-scale pedestrian appearance via the LPR Memory. To this end, we employ the large-scale pedestrian exemplar set, so that, the LPR Memory can recall the information of the large-scale pedestrians from the small-scale pedestrians. Comprehensive quantitative and qualitative experimental results validate the effectiveness of the proposed framework with the LPR Memory.},
  archive   = {C_ICCV},
  author    = {Jung Uk Kim and Sungjune Park and Yong Man Ro},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00304},
  pages     = {3030-3039},
  title     = {Robust small-scale pedestrian detection with cued recall via memory learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Large-scale robust deep AUC maximization: A new surrogate
loss and empirical studies on medical image classification.
<em>ICCV</em>, 3020–3029. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep AUC Maximization (DAM) is a new paradigm for learning a deep neural network by maximizing the AUC score of the model on a dataset. Most previous works of AUC maximization focus on the perspective of optimization by designing efficient stochastic algorithms, and studies on generalization performance of large-scale DAM on difficult tasks are missing. In this work, we aim to make DAM more practical for interesting real-world applications (e.g., medical image classification). First, we propose a new margin-based min-max surrogate loss function for the AUC score (named as the AUC min-max-margin loss or simply AUC margin loss for short). It is more robust than the commonly used AUC square loss, while enjoying the same advantage in terms of large-scale stochastic optimization. Second, we conduct extensive empirical studies of our DAM method on four difficult medical image classification tasks, namely (i) classification of chest x-ray images for identifying many threatening diseases, (ii) classification of images of skin lesions for identifying melanoma, (iii) classification of mammogram for breast cancer screening, and (iv) classification of microscopic images for identifying tumor tissue. Our studies demonstrate that the proposed DAM method improves the performance of optimizing cross-entropy loss by a large margin, and also achieves better performance than optimizing the existing AUC square loss on these medical image classification tasks. Specifically, our DAM method has achieved the 1st place on Stanford CheXpert competition on Aug. 31, 2020. To the best of our knowledge, this is the first work that makes DAM succeed on large-scale medical image datasets. We also conduct extensive ablation studies to demonstrate the advantages of the new AUC margin loss over the AUC square loss on benchmark datasets. The proposed method is implemented in our open-sourced library LibAUC (www.libauc.org) whose github address is https://github.com/Optimization-AI/LibAUC.},
  archive   = {C_ICCV},
  author    = {Zhuoning Yuan and Yan Yan and Milan Sonka and Tianbao Yang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00303},
  pages     = {3020-3029},
  title     = {Large-scale robust deep AUC maximization: A new surrogate loss and empirical studies on medical image classification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DecentLaM: Decentralized momentum SGD for large-batch deep
training. <em>ICCV</em>, 3009–3019. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The scale of deep learning nowadays calls for efficient distributed training algorithms. Decentralized momentum SGD (DmSGD), in which each node averages only with its neighbors, is more communication efficient than vanilla Parallel momentum SGD that incurs global average across all computing nodes. On the other hand, the large-batch training has been demonstrated critical to achieve runtime speedup. This motivates us to investigate how DmSGD performs in the large-batch scenario.In this work, we find the momentum term can amplify the inconsistency bias in DmSGD. Such bias becomes more evident as batch-size grows large and hence results in severe performance degradation. We next propose DecentLaM, a novel decentralized large-batch momentum SGD to remove the momentum-incurred bias. The convergence rate for both strongly convex and non-convex scenarios is established. Our theoretical results justify the superiority of DecentLaM to DmSGD especially in the large-batch scenario. Experimental results on a a variety of computer vision tasks and models show that DecentLaM promises both efficient and high-quality training.},
  archive   = {C_ICCV},
  author    = {Kun Yuan and Yiming Chen and Xinmeng Huang and Yingya Zhang and Pan Pan and Yinghui Xu and Wotao Yin},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00302},
  pages     = {3009-3019},
  title     = {DecentLaM: Decentralized momentum SGD for large-batch deep training},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Switchable k-class hyperplanes for noise-robust
representation learning. <em>ICCV</em>, 2999–3008. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Optimizing the K-class hyperplanes in the latent space has become the standard paradigm for efficient representation learning. However, it’s almost impossible to find an optimal K-class hyperplane to accurately describe the latent space of massive noisy data. For this potential problem, we constructively propose a new method, named Switchable K-class Hyperplanes (SKH), to sufficiently describe the latent space by the mixture of K-class hyperplanes. It can directly replace the conventional single K-class hyperplane optimization as the new paradigm for noise-robust representation learning. When collaborated with the popular ArcFace on million-level data representation learning, we found that the switchable manner in SKH can effectively eliminate the gradient conflict generated by real-world label noise on a single K-class hyperplane. Moreover, combined with the margin-based loss functions (e.g. ArcFace), we propose a simple Posterior Data Clean strategy to reduce the model optimization deviation on clean dataset caused by the reduction of valid categories in each K-class hyperplane. Extensive experiments demonstrate that the proposed SKH easily achieves new state-of-the-art on IJB-B and IJB-C by encouraging noise-robust representation learning. Our code will be available at https://github.com/liubx07/SKH.git.},
  archive   = {C_ICCV},
  author    = {Boxiao Liu and Guanglu Song and Manyuan Zhang and Haihang You and Yu Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00301},
  pages     = {2999-3008},
  title     = {Switchable K-class hyperplanes for noise-robust representation learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rank &amp; sort loss for object detection and instance
segmentation. <em>ICCV</em>, 2989–2998. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose Rank &amp; Sort (RS) Loss, a ranking-based loss function to train deep object detection and instance segmentation methods (i.e. visual detectors). RS Loss supervises the classifier, a sub-network of these methods, to rank each positive above all negatives as well as to sort positives among themselves with respect to (wrt.) their localisation qualities (e.g. Intersection-over-Union - IoU). To tackle the non-differentiable nature of ranking and sorting, we reformulate the incorporation of error-driven update with back-propagation as Identity Update, which enables us to model our novel sorting error among positives. With RS Loss, we significantly simplify training: (i) Thanks to our sorting objective, the positives are prioritized by the classifier without an additional auxiliary head (e.g. for centerness, IoU, mask-IoU), (ii) due to its ranking-based nature, RS Loss is robust to class imbalance, and thus, no sampling heuristic is required, and (iii) we address the multi-task nature of visual detectors using tuning-free task-balancing coefficients. Using RS Loss, we train seven diverse visual detectors only by tuning the learning rate, and show that it consistently outperforms baselines: e.g. our RS Loss improves (i) Faster R-CNN by ∼ 3 box AP and aLRP Loss (ranking-based baseline) by ∼ 2 box AP on COCO dataset, (ii) Mask R-CNN with repeat factor sampling (RFS) by 3.5 mask AP (∼ 7 AP for rare classes) on LVIS dataset; and also outperforms all counterparts. Code is available at: https://github.com/kemaloksuz/RankSortLoss.},
  archive   = {C_ICCV},
  author    = {Kemal Oksuz and Baris Can Cam and Emre Akbas and Sinan Kalkan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00300},
  pages     = {2989-2998},
  title     = {Rank &amp; sort loss for object detection and instance segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-scale vision longformer: A new vision transformer for
high-resolution image encoding. <em>ICCV</em>, 2978–2988. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a new Vision Transformer (ViT) architecture Multi-Scale Vision Longformer, which significantly enhances the ViT of [12] for encoding high-resolution images using two techniques. The first is the multi-scale model structure, which provides image encodings at multiple scales with manageable computational cost. The second is the attention mechanism of Vision Long-former, which is a variant of Longformer [3], originally developed for natural language processing, and achieves a linear complexity w.r.t. the number of input tokens. A comprehensive empirical study shows that the new ViT significantly outperforms several strong baselines, including the existing ViT models and their ResNet counterparts, and the Pyramid Vision Transformer from a concurrent work [47], on a range of vision tasks, including image classification, object detection, and segmentation. The models and source code are released at https://github.com/microsoft/vision-longformer.},
  archive   = {C_ICCV},
  author    = {Pengchuan Zhang and Xiyang Dai and Jianwei Yang and Bin Xiao and Lu Yuan and Lei Zhang and Jianfeng Gao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00299},
  pages     = {2978-2988},
  title     = {Multi-scale vision longformer: A new vision transformer for high-resolution image encoding},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic DETR: End-to-end object detection with dynamic
attention. <em>ICCV</em>, 2968–2977. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a novel Dynamic DETR (Detection with Transformers) approach by introducing dynamic attentions into both the encoder and decoder stages of DETR to break its two limitations on small feature resolution and slow training convergence. To address the first limitation, which is due to the quadratic computational complexity of the self-attention module in Transformer encoders, we propose a dynamic encoder to approximate the Transformer encoder’s attention mechanism using a convolution-based dynamic encoder with various attention types. Such an encoder can dynamically adjust attentions based on multiple factors such as scale importance, spatial importance, and representation (i.e., feature dimension) importance. To mitigate the second limitation of learning difficulty, we introduce a dynamic decoder by replacing the cross-attention module with a ROI-based dynamic attention in the Transformer decoder. Such a decoder effectively assists Transformers to focus on region of interests from a coarse-to-fine manner and dramatically lowers the learning difficulty, leading to a much faster convergence with fewer training epochs. We conduct a series of experiments to demonstrate our advantages. Our Dynamic DETR significantly reduces the training epochs (by 14×), yet results in a much better performance (by 3.6 on mAP). Meanwhile, in the standard 1× setup with ResNet-50 backbone, we archive a new state-of-the-art performance that further proves the learning effectiveness of the proposed approach.},
  archive   = {C_ICCV},
  author    = {Xiyang Dai and Yinpeng Chen and Jianwei Yang and Pengchuan Zhang and Lu Yuan and Lei Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00298},
  pages     = {2968-2977},
  title     = {Dynamic DETR: End-to-end object detection with dynamic attention},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). WB-DETR: Transformer-based detector without backbone.
<em>ICCV</em>, 2959–2967. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Transformer-based detector is a new paradigm in object detection, which aims to achieve pretty-well performance while eliminates the priori knowledge driven components, e.g., anchors, proposals and the NMS. DETR, the state-of-the-art model among them, is composed of three sub-modules, i.e., a CNN-based backbone and paired transformer encoder-decoder. The CNN is applied to extract local features and the transformer is used to capture global contexts. This pipeline, however, is not concise enough. In this paper, we propose WB-DETR (DETR-based detector Without Backbone) to prove that the reliance on CNN features extraction for a transformer-based detector is not necessary. Unlike the original DETR, WB-DETR is composed of only an encoder and a decoder without CNN backbone. For an input image, WB-DETR serializes it directly to encode the local features into each individual token. To make up the deficiency of transformer in modeling local information, we design an LIE-T2T (local information enhancement tokens to token) module to enhance the internal information of tokens after unfolding. Experimental results demonstrate that WB-DETR, the first pure-transformer detector without CNN to our knowledge, yields on par accuracy and faster inference speed with only half number of parameters compared with DETR baseline.},
  archive   = {C_ICCV},
  author    = {Fanfan Liu and Haoran Wei and Wenzhe Zhao and Guozhen Li and Jingquan Peng and Zihao Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00297},
  pages     = {2959-2967},
  title     = {WB-DETR: Transformer-based detector without backbone},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). ELSD: Efficient line segment detector and descriptor.
<em>ICCV</em>, 2949–2958. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present the novel Efficient Line Segment Detector and Descriptor (ELSD) to simultaneously detect line segments and extract their descriptors in an image. Unlike the traditional pipelines that conduct detection and description separately, ELSD utilizes a shared feature extractor for both detection and description, to provide the essential line features to the higher-level tasks like SLAM and image matching in real time. First, we design a one-stage compact model, and propose to use the mid-point, angle and length as the minimal representation of line segment, which also guarantees the center-symmetry. The non-centerness suppression is proposed to filter out the fragmented line segments caused by lines’ intersections. The fine offset prediction is designed to refine the mid-point localization. Second, the line descriptor branch is integrated with the detector branch, and the two branches are jointly trained in an end-to-end manner. In the experiments, the proposed ELSD achieves the state-of-the-art performance on the Wireframe dataset and YorkUrban dataset, in both accuracy and efficiency. The line description ability of ELSD also outperforms the previous works on the line matching task.},
  archive   = {C_ICCV},
  author    = {Haotian Zhang and Yicheng Luo and Fangbo Qin and Yijia He and Xiao Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00296},
  pages     = {2949-2958},
  title     = {ELSD: Efficient line segment detector and descriptor},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Body-face joint detection via embedding and head hook.
<em>ICCV</em>, 2939–2948. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Detecting pedestrians and their associated faces jointly is a challenging task. On one hand, body or face could be absent because of occlusion or non-frontal human pose. On the other hand, the association becomes difficult or even miss-leading in crowded scenes due to the lack of strong correlational evidence. This paper proposes Body-Face Joint (BFJ) detector, a novel framework for detecting bodies and their faces with accurate correspondance. We follow the classical multi-class detector design by detecting body and face in parallel but with two key contributions. First, we propose an Embedding Matching Loss (EML) to learn an associative embedding for matching body and face of the same person. Second, we introduce a novel concept, &quot;head hook&quot;, to bridge the gap of matching body and faces spatially. With the new semantical and geometrical sources of information, BFJ greatly reduces the difficulty of detecting body and face in pairs. Since the problem is unexplored yet, we design a new metric named log-average miss matching rate (mMR −2 ) to evaluate the association performance and extend the CrowdHuman and CityPersons benchmarks by annotating each face box. Experiments show that our BFJ detector can maintain state-of-the-art performance in pedestrian detection on both one-stage and two-stage structures while greatly outperform various body-face association strategies. Code will be available at: https://github.com/AibeeDetect/BFJDet.},
  archive   = {C_ICCV},
  author    = {Junfeng Wan and Jiangfan Deng and Xiaosong Qiu and Feng Zhou},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00295},
  pages     = {2939-2948},
  title     = {Body-face joint detection via embedding and head hook},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Group-free 3D object detection via transformers.
<em>ICCV</em>, 2929–2938. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, directly detecting 3D objects from 3D point clouds has received increasing attention. To extract object representation from an irregular point cloud, existing methods usually take a point grouping step to assign the points to an object candidate so that a PointNet-like network could be used to derive object features from the grouped points. However, the inaccurate point assignments caused by the hand-crafted grouping scheme decrease the performance of 3D object detection.In this paper, we present a simple yet effective method for directly detecting 3D objects from the 3D point cloud. Instead of grouping local points to each object candidate, our method computes the feature of an object from all the points in the point cloud with the help of an attention mechanism in the Transformers [42], where the contribution of each point is automatically learned in the network training. With an improved attention stacking scheme, our method fuses object features in different stages and generates more accurate object detection results. With few bells and whistles, the proposed method achieves state-of-the-art 3D object detection performance on two widely used benchmarks, Scan-Net V2 and SUN RGB-D. The code and models are publicly available at https://github.com/zeliu98/Group-Free-3D},
  archive   = {C_ICCV},
  author    = {Ze Liu and Zheng Zhang and Yue Cao and Han Hu and Xin Tong},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00294},
  pages     = {2929-2938},
  title     = {Group-free 3D object detection via transformers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gated3D: Monocular 3D object detection from temporal
illumination cues. <em>ICCV</em>, 2918–2928. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Today’s state-of-the-art methods for 3D object detection are based on lidar, stereo, or monocular cameras. Lidar-based methods achieve the best accuracy, but have a large footprint, high cost, and mechanically-limited angular sampling rates, resulting in low spatial resolution at long ranges. Recent approaches using low-cost monocular or stereo cameras promise to overcome these limitations but struggle in low-light or low-contrast regions as they rely on passive CMOS sensors. We propose a novel 3D object detection modality that exploits temporal illumination cues from a low-cost monocular gated imager. We introduce a novel deep detection architecture, Gated3D, that is tailored to temporal illumination cues in gated images. This modality allows us to exploit mature 2D object feature extractors that guide the 3D predictions through a frustum segment estimation. We assess the proposed method experimentally on a 3D detection dataset that includes gated images captured over 10,000 km of driving data. We validate that our method outperforms state-of-the-art monocular and stereo methods, opening up a new sensor modality as an avenue to replace lidar in autonomous driving. https://light.princeton.edu/gated3d},
  archive   = {C_ICCV},
  author    = {Frank Julca-Aguilar and Jason Taylor and Mario Bijelic and Fahim Mannan and Ethan Tseng and Felix Heide},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00293},
  pages     = {2918-2928},
  title     = {Gated3D: Monocular 3D object detection from temporal illumination cues},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). 3DVG-transformer: Relation modeling for visual grounding on
point clouds. <em>ICCV</em>, 2908–2917. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual grounding on 3D point clouds is an emerging vision and language task that benefits various applications in understanding the 3D visual world. By formulating this task as a grounding-by-detection problem, lots of recent works focus on how to exploit more powerful detectors and comprehensive language features, but (1) how to model complex relations for generating context-aware object proposals and (2) how to leverage proposal relations to distinguish the true target object from similar proposals are not fully studied yet. Inspired by the well-known transformer architecture, we propose a relation-aware visual grounding method on 3D point clouds, named as 3DVG-Transformer, to fully utilize the contextual clues for relation-enhanced proposal generation and cross-modal proposal disambiguation, which are enabled by a newly designed coordinate-guided contextual aggregation (CCA) module in the object proposal generation stage, and a multiplex attention (MA) module in the cross-modal feature fusion stage. We validate that our 3DVG-Transformer outperforms the state-of-the-art methods by a large margin, on two point cloud-based visual grounding datasets, ScanRefer and Nr3D/Sr3D from ReferIt3D, especially for complex scenarios containing multiple objects of the same category.},
  archive   = {C_ICCV},
  author    = {Lichen Zhao and Daigang Cai and Lu Sheng and Dong Xu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00292},
  pages     = {2908-2917},
  title     = {3DVG-transformer: Relation modeling for visual grounding on point clouds},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RangeDet: In defense of range view for LiDAR-based 3D object
detection. <em>ICCV</em>, 2898–2907. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose an anchor-free single-stage LiDAR-based 3D object detector – RangeDet. The most notable difference with previous works is that our method is purely based on the range view representation. Compared with the commonly used voxelized or Bird’s Eye View (BEV) representations, the range view representation is more compact and without quantization error. Although there are works adopting it for semantic segmentation, its performance in object detection is largely behind voxelized or BEV counterparts. We first analyze the existing range-view-based methods and find two issues overlooked by previous works: 1) the scale variation between nearby and far away objects; 2) the inconsistency between the 2D range image coordinates used in feature extraction and the 3D Cartesian coordinates used in output. Then we deliberately design three components to address these issues in our RangeDet. We test our RangeDet in the large-scale Waymo Open Dataset (WOD). Our best model achieves 72.9/75.9/65.8 3D AP on vehicle/pedestrian/cyclist. These results outperform other range-view-based methods by a large margin, and are overall comparable with the state-of-the-art multi-view-based methods. Codes will be released at https://github.com/TuSimple/RangeDet.},
  archive   = {C_ICCV},
  author    = {Lue Fan and Xuan Xiong and Feng Wang and Naiyan Wang and Zhaoxiang Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00291},
  pages     = {2898-2907},
  title     = {RangeDet: In defense of range view for LiDAR-based 3D object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An end-to-end transformer model for 3D object detection.
<em>ICCV</em>, 2886–2897. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose 3DETR, an end-to-end Transformer based object detection model for 3D point clouds. Compared to existing detection methods that employ a number of 3D-specific inductive biases, 3DETR requires minimal modifications to the vanilla Transformer block. Specifically, we find that a standard Transformer with non-parametric queries and Fourier positional embeddings is competitive with specialized architectures that employ libraries of 3D-specific operators with hand-tuned hyperparameters. Nevertheless, 3DETR is conceptually simple and easy to implement, enabling further improvements by incorporating 3D domain knowledge. Through extensive experiments, we show 3DETR outperforms the well-established and highly optimized VoteNet baselines on the challenging ScanNetV2 dataset by 9.5\%. Furthermore, we show 3DETR is applicable to 3D tasks beyond detection, and can serve as a building block for future research.},
  archive   = {C_ICCV},
  author    = {Ishan Misra and Rohit Girdhar and Armand Joulin},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00290},
  pages     = {2886-2897},
  title     = {An end-to-end transformer model for 3D object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semi-supervised active learning for semi-supervised models:
Exploit adversarial examples with graph-based virtual labels.
<em>ICCV</em>, 2876–2885. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The performance of computer vision models significantly improves with more labeled data. However, the acquisition of labeled data is limited by the high cost. To mitigate the reliance on large labeled datasets, active learning (AL) and semi-supervised learning (SSL) are frequently adopted. Although current mainstream methods begin to combine SSL and AL (SSL-AL) to excavate the diverse expressions of unlabeled samples, these methods’ fully supervised task models are still trained only with labeled data. Besides, these method’s SSL-AL frameworks suffer from mismatch problems. Here, we propose a graph-based SSL-AL framework to unleash the SSL task models’ power and make an effective SSL-AL interaction. In the framework, SSL leverages graph-based label propagation to deliver virtual labels to unlabeled samples, rendering AL samples’ structural distribution and boosting AL. AL finds samples near the clusters’ boundary to help SSL perform better label propagation by exploiting adversarial examples. The information exchange in the closed-loop realizes mutual enhancement of SSL and AL. Experimental results show that our method outperforms the state-of-the-art methods against classification and segmentation benchmarks.},
  archive   = {C_ICCV},
  author    = {Jiannan Guo and Haochen Shi and Yangyang Kang and Kun Kuang and Siliang Tang and Zhuoren Jiang and Changlong Sun and Fei Wu and Yueting Zhuang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00289},
  pages     = {2876-2885},
  title     = {Semi-supervised active learning for semi-supervised models: Exploit adversarial examples with graph-based virtual labels},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TS-CAM: Token semantic coupled attention map for weakly
supervised object localization. <em>ICCV</em>, 2866–2875. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Weakly supervised object localization (WSOL) is a challenging problem when given image category labels but requires to learn object localization models. Optimizing a convolutional neural network (CNN) for classification tends to activate local discriminative regions while ignoring complete object extent, causing the partial activation issue. In this paper, we argue that partial activation is caused by the intrinsic characteristics of CNN, where the convolution operations produce local receptive fields and experience difficulty to capture long-range feature dependency among pixels. We introduce the token semantic coupled attention map (TS-CAM) to take full advantage of the self-attention mechanism in visual transformer for long-range dependency extraction. TS-CAM first splits an image into a sequence of patch tokens for spatial embedding, which produce attention maps of long-range visual dependency to avoid partial activation. TS-CAM then re-allocates category-related semantics for patch tokens, enabling each of them to be aware of object categories. TS-CAM finally couples the patch tokens with the semantic-agnostic attention map to achieve semantic-aware localization. Experiments on the ILSVRC/CUB-200-2011 datasets show that TS-CAM outperforms its CNN-CAM counterparts by 7.1\%/27.1\% for WSOL, achieving state-of-the-art performance. Code is available at https://github.com/vasgaowei/TS-CAM},
  archive   = {C_ICCV},
  author    = {Wei Gao and Fang Wan and Xingjia Pan and Zhiliang Peng and Qi Tian and Zhenjun Han and Bolei Zhou and Qixiang Ye},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00288},
  pages     = {2866-2875},
  title     = {TS-CAM: Token semantic coupled attention map for weakly supervised object localization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Boosting weakly supervised object detection via learning
bounding box adjusters. <em>ICCV</em>, 2856–2865. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Weakly-supervised object detection (WSOD) has emerged as an inspiring recent topic to avoid expensive instance-level object annotations. However, the bounding boxes of most existing WSOD methods are mainly determined by precomputed proposals, thereby being limited in precise object localization. In this paper, we defend the problem setting for improving localization performance by leveraging the bounding box regression knowledge from a well-annotated auxiliary dataset. First, we use the well-annotated auxiliary dataset to explore a series of learnable bounding box adjusters (LBBAs) in a multi-stage training manner, which is class-agnostic. Then, only LBBAs and a weakly-annotated dataset with non-overlapped classes are used for training LBBA-boosted WSOD. As such, our LBBAs are practically more convenient and economical to implement while avoiding the leakage of the auxiliary well-annotated dataset. In particular, we formulate learning bounding box adjusters as a bi-level optimization problem and suggest an EM-like multi-stage training algorithm. Then, a multi-stage scheme is further presented for LBBA-boosted WSOD. Additionally, a masking strategy is adopted to improve proposal classification. Experimental results verify the effectiveness of our method. Our method performs favorably against state-of-the-art WSOD methods and knowledge transfer model with similar problem setting. Code is publicly available at https://github.com/DongSky/lbba_boosted_wsod.},
  archive   = {C_ICCV},
  author    = {Bowen Dong and Zitong Huang and Yuelin Guo and Qilong Wang and Zhenxing Niu and Wangmeng Zuo},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00287},
  pages     = {2856-2865},
  title     = {Boosting weakly supervised object detection via learning bounding box adjusters},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PreDet: Large-scale weakly supervised pre-training for
detection. <em>ICCV</em>, 2845–2855. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {State-of-the-art object detection approaches typically rely on pre-trained classification models to achieve better performance and faster convergence. We hypothesize that classification pre-training strives to achieve translation invariance, and consequently ignores the localization aspect of the problem. We propose a new large-scale pre-training strategy for detection, where noisy class labels are available for all images, but not bounding-boxes. In this setting, we augment standard classification pre-training with a new detection-specific pretext task. Motivated by the noise-contrastive learning based self-supervised approaches, we design a task that forces bounding boxes with high-overlap to have similar representations in different views of an image, compared to non-overlapping boxes. We redesign Faster R-CNN modules to perform this task efficiently. Our experimental results show significant improvements over existing weakly-supervised and self-supervised pre-training approaches in both detection accuracy as well as fine-tuning speed.},
  archive   = {C_ICCV},
  author    = {Vignesh Ramanathan and Rui Wang and Dhruv Mahajan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00286},
  pages     = {2845-2855},
  title     = {PreDet: Large-scale weakly supervised pre-training for detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Human detection and segmentation via multi-view consensus.
<em>ICCV</em>, 2835–2844. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-supervised detection and segmentation of foreground objects aims for accuracy without annotated training data. However, existing approaches predominantly rely on restrictive assumptions on appearance and motion.For scenes with dynamic activities and camera motion, we propose a multi-camera framework in which geometric constraints are embedded in the form of multi-view consistency during training via coarse 3D localization in a voxel grid and fine-grained offset regression. In this manner, we learn a joint distribution of proposals over multiple views. At inference time, our method operates on single RGB images. We outperform state-of-the-art techniques both on images that visually depart from those of standard benchmarks and on those of the classical Human3.6M dataset.},
  archive   = {C_ICCV},
  author    = {Isinsu Katircioglu and Helge Rhodin and Jörg Spörri and Mathieu Salzmann and Pascal Fua},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00285},
  pages     = {2835-2844},
  title     = {Human detection and segmentation via multi-view consensus},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Self-supervised image prior learning with GMM from a single
noisy image. <em>ICCV</em>, 2825–2834. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The lack of clean images undermines the practicability of supervised image prior learning methods, of which the training schemes require a large number of clean images. To free image prior learning from the image collection burden, a novel Self-Supervised learning method for Gaussian Mixture Model (SS-GMM) is proposed in this paper. It can simultaneously achieve the noise level estimation and the image prior learning directly from only a single noisy image. This work is derived from our study on eigenvalues of the GMM’s covariance matrix. Through statistical experiments and theoretical analysis, we conclude that (1) covariance eigenvalues for clean images hold the sparsity; and that (2) those for noisy images contain sufficient information for noise estimation. The first conclusion inspires us to impose a sparsity constraint on covariance eigenvalues during the learning process to suppress the influence of noise. The second conclusion leads to a self-contained noise estimation module of high accuracy in our proposed method. This module serves to estimate the noise level and automatically determine the specific level of the sparsity constraint. Our final derived method requires only minor modifications to the standard expectation-maximization algorithm. This makes it easy to implement. Very interestingly, the GMM learned via our proposed self-supervised learning method can even achieve better image denoising performance than its supervised counterpart, i.e., the EPLL. Also, it is on par with the state-of-the-art self-supervised deep learning method, i.e., the Self2Self. Code is available at https://github.com/HUST-Tan/SS-GMM.},
  archive   = {C_ICCV},
  author    = {Haosen Liu and Xuan Liu and Jiangbo Lu and Shan Tan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00284},
  pages     = {2825-2834},
  title     = {Self-supervised image prior learning with GMM from a single noisy image},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weakly supervised 3D semantic segmentation using cross-image
consensus and inter-voxel affinity relations. <em>ICCV</em>, 2814–2824.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel weakly supervised approach for 3D semantic segmentation on volumetric images. Unlike most existing methods that require voxel-wise densely labeled training data, our weakly-supervised CIVA-Net is the first model that only needs image-level class labels as guidance to learn accurate volumetric segmentation. Our model learns from cross-image co-occurrence for integral region generation, and explores inter-voxel affinity relations to predict segmentation with accurate boundaries. We empirically validate our model on both simulated and real cryo-ET datasets. Our experiments show that CIVA-Net achieves comparable performance to the state-of-the-art models trained with stronger supervision.},
  archive   = {C_ICCV},
  author    = {Xiaoyu Zhu and Jeffrey Chen and Xiangrui Zeng and Junwei Liang and Chengqi Li and Sinuo Liu and Sima Behpour and Min Xu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00283},
  pages     = {2814-2824},
  title     = {Weakly supervised 3D semantic segmentation using cross-image consensus and inter-voxel affinity relations},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Prior to segment: Foreground cues for weakly annotated
classes in partially supervised instance segmentation. <em>ICCV</em>,
2804–2813. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Instance segmentation methods require large datasets with expensive and thus limited instance-level mask labels. Partially supervised instance segmentation aims to improve mask prediction with limited mask labels by utilizing the more abundant weak box labels. In this work, we show that a class agnostic mask head, commonly used in partially supervised instance segmentation, has difficulties learning a general concept of foreground for the weakly annotated classes using box supervision only. To resolve this problem, we introduce an object mask prior (OMP) that provides the mask head with the general concept of foreground implicitly learned by the box classification head under the supervision of all classes. This helps the class agnostic mask head to focus on the primary object in a region of interest (RoI) and improves generalization to the weakly annotated classes. We test our approach on the COCO dataset using different splits of strongly and weakly supervised classes. Our approach significantly improves over the Mask R-CNN baseline and obtains competitive performance with the state-of-the-art, while offering a much simpler architecture. 1},
  archive   = {C_ICCV},
  author    = {David Biertimpel and Sindi Shkodrani and Anil S. Baslamisli and Nóra Baka},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00282},
  pages     = {2804-2813},
  title     = {Prior to segment: Foreground cues for weakly annotated classes in partially supervised instance segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparse-shot learning with exclusive cross-entropy for
extremely many localisations. <em>ICCV</em>, 2793–2803. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Object localisation, in the context of regular images, often depicts objects like people or cars. In these images, there is typically a relatively small number of objects per class, which usually is manageable to annotate. However, outside the setting of regular images, we are often confronted with a different situation. In computational pathology, digitised tissue sections are extremely large images, whose dimensions quickly exceed 250’000 × 250’000 pixels, where relevant objects, such as tumour cells or lymphocytes can quickly number in the millions. Annotating them all is practically impossible and annotating sparsely a few, out of many more, is the only possibility. Unfortunately, learning from sparse annotations, or sparse-shot learning, clashes with standard supervised learning because what is not annotated is treated as a negative. However, assigning negative labels to what are true positives leads to confusion in the gradients and biased learning. To this end, we present exclusive cross-entropy, which slows down the biased learning by examining the second-order loss derivatives in order to drop the loss terms corresponding to likely biased terms. Experiments on nine datasets and two different localisation tasks, detection with YOLLO and segmentation with Unet, show that we obtain considerable improvements compared to cross-entropy or focal loss, while often reaching the best possible performance for the model with only 10-40\% of annotations.},
  archive   = {C_ICCV},
  author    = {Andreas Panteli and Jonas Teuwen and Hugo Horlings and Efstratios Gavves},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00281},
  pages     = {2793-2803},
  title     = {Sparse-shot learning with exclusive cross-entropy for extremely many localisations},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Contrastive attention maps for self-supervised
co-localization. <em>ICCV</em>, 2783–2792. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The goal of unsupervised co-localization is to locate the object in a scene under the assumptions that 1) the dataset consists of only one superclass, e.g., birds, and 2) there are no human-annotated labels in the dataset. The most recent method achieves impressive co-localization performance by employing self-supervised representation learning approaches such as predicting rotation. In this paper, we introduce a new contrastive objective directly on the attention maps to enhance co-localization performance. Our contrastive loss function exploits rich information of location, which induces the model to activate the extent of the object effectively. In addition, we propose a pixel-wise attention pooling that selectively aggregates the feature map regarding their magnitudes across channels. Our methods are simple and shown effective by extensive qualitative and quantitative evaluation, achieving state-of-the-art co-localization performances by large margins on four datasets: CUB-200-2011, Stanford Cars, FGVC-Aircraft, and Stanford Dogs. Our code will be publicly available online for the research community.},
  archive   = {C_ICCV},
  author    = {Minsong Ki and Youngjung Uh and Junsuk Choe and Hyeran Byun},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00280},
  pages     = {2783-2792},
  title     = {Contrastive attention maps for self-supervised co-localization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). PR-GCN: A deep graph convolutional network with point
refinement for 6D pose estimation. <em>ICCV</em>, 2773–2782. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {RGB-D based 6D pose estimation has recently achieved remarkable progress, but still suffers from two major limitations: (1) ineffective representation of depth data and (2) insufficient integration of different modalities. This paper proposes a novel deep learning approach, namely Graph Convolutional Network with Point Refinement (PR-GCN), to simultaneously address the issues above in a unified way. It first introduces the Point Refinement Network (PRN) to polish 3D point clouds, recovering missing parts with noise removed. Subsequently, the Multi-Modal Fusion Graph Convolutional Network (MMF-GCN) is presented to strengthen RGB-D combination, which captures geometry-aware inter-modality correlation through local information propagation in the graph convolutional network. Extensive experiments are conducted on three widely used benchmarks, and state-of-the-art performance is reached. Besides, it is also shown that the proposed PRN and MMF-GCN modules are well generalized to other frameworks.},
  archive   = {C_ICCV},
  author    = {Guangyuan Zhou and Huiqun Wang and Jiaxin Chen and Di Huang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00279},
  pages     = {2773-2782},
  title     = {PR-GCN: A deep graph convolutional network with point refinement for 6D pose estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Instance segmentation in 3D scenes using semantic superpoint
tree networks. <em>ICCV</em>, 2763–2772. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Instance segmentation in 3D scenes is fundamental in many applications of scene understanding. It is yet challenging due to the compound factors of data irregularity and uncertainty in the numbers of instances. State-of-the-art methods largely rely on a general pipeline that first learns point-wise features discriminative at semantic and instance levels, followed by a separate step of point grouping for proposing object instances. While promising, they have the shortcomings that (1) the second step is not supervised by the main objective of instance segmentation, and (2) their point-wise feature learning and grouping are less effective to deal with data irregularities, possibly resulting in fragmented segmentations. To address these issues, we propose in this work an end-to-end solution of Semantic Superpoint Tree Network (SSTNet) for proposing object instances from scene points. Key in SST-Net is an intermediate, semantic superpoint tree (SST), which is constructed based on the learned semantic features of superpoints, and which will be traversed and split at intermediate tree nodes for proposals of object instances. We also design in SSTNet a refinement module, termed CliqueNet, to prune superpoints that may be wrongly grouped into instance proposals. Experiments on the benchmarks of ScanNet and S3DIS show the efficacy of our proposed method. At the time of submission, SSTNet ranks top on the ScanNet (V2) leaderboard, with 2\% higher of mAP than the second best method. The source code in PyTorch is available at https://github.com/Gorilla-Lab-SCUT/SSTNet.},
  archive   = {C_ICCV},
  author    = {Zhihao Liang and Zhihao Li and Songcen Xu and Mingkui Tan and Kui Jia},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00278},
  pages     = {2763-2772},
  title     = {Instance segmentation in 3D scenes using semantic superpoint tree networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SGPA: Structure-guided prior adaptation for category-level
6D object pose estimation. <em>ICCV</em>, 2753–2762. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Category-level 6D object pose estimation aims to predict the position and orientation for unseen objects, which plays a pillar role in many scenarios such as robotics and augmented reality. The significant intra-class variation is the bottleneck challenge in this task yet remains unsolved so far. In this paper, we take advantage of category prior to overcome this problem by innovating a structure-guided prior adaptation scheme to accurately estimate 6D pose for individual objects. Different from existing prior based methods, given one object and its corresponding category prior, we propose to leverage their structure similarity to dynamically adapt the prior to the observed object. The prior adaptation intrinsically associates the adopted prior with different objects, from which we can accurately reconstruct the 3D canonical model of the specific object for pose estimation. To further enhance the structure characteristic of objects, we extract low-rank structure points from the dense object point cloud, therefore more efficiently incorporating sparse structural information during prior adaptation. Extensive experiments on CAMERA25 and REAL275 benchmarks demonstrate significant performance improvement. Project homepage: https://www.cse.cuhk.edu.hk/˜kaichen/projects/sgpa/sgpa.html.},
  archive   = {C_ICCV},
  author    = {Kai Chen and Qi Dou},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00277},
  pages     = {2753-2762},
  title     = {SGPA: Structure-guided prior adaptation for category-level 6D object pose estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GraphFPN: Graph feature pyramid network for object
detection. <em>ICCV</em>, 2743–2752. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Feature pyramids have been proven powerful in image understanding tasks that require multi-scale features. State-of-the-art methods for multi-scale feature learning focus on performing feature interactions across space and scales using neural networks with a fixed topology. In this paper, we propose graph feature pyramid networks that are capable of adapting their topological structures to varying intrinsic image structures, and supporting simultaneous feature interactions across all scales. We first define an image specific superpixel hierarchy for each input image to represent its intrinsic image structures. The graph feature pyramid network inherits its structure from this superpixel hierarchy. Contextual and hierarchical layers are designed to achieve feature interactions within the same scale and across different scales. To make these layers more powerful, we introduce two types of local channel attention for graph neural networks by generalizing global channel attention for convolutional neural networks. The proposed graph feature pyramid network can enhance the multiscale features from a convolutional feature pyramid network.We evaluate our graph feature pyramid network in the object detection task by integrating it into the Faster R-CNN algorithm. The modified algorithm outperforms not only previous state-of-the-art feature pyramid based methods with a clear margin but also other popular detection methods on both MS-COCO 2017 validation and test datasets.},
  archive   = {C_ICCV},
  author    = {Gangming Zhao and Weifeng Ge and Yizhou Yu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00276},
  pages     = {2743-2752},
  title     = {GraphFPN: Graph feature pyramid network for object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HPNet: Deep primitive segmentation using hybrid
representations. <em>ICCV</em>, 2733–2742. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces HPNet, a novel deep-learning approach for segmenting a 3D shape represented as a point cloud into primitive patches. The key to deep primitive segmentation is learning a feature representation that can separate points of different primitives. Unlike utilizing a single feature representation, HPNet leverages hybrid representations that combine one learned semantic descriptor, two spectral descriptors derived from predicted geometric parameters, as well as an adjacency matrix that encodes sharp edges. Moreover, instead of merely concatenating the descriptors, HPNet optimally combines hybrid representations by learning combination weights. This weighting module builds on the entropy of input features. The output primitive segmentation is obtained from a meanshift clustering module. Experimental results on benchmark datasets ANSI and ABCParts show that HPNet leads to significant performance gains from baseline approaches.},
  archive   = {C_ICCV},
  author    = {Siming Yan and Zhenpei Yang and Chongyang Ma and Haibin Huang and Etienne Vouga and Qixing Huang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00275},
  pages     = {2733-2742},
  title     = {HPNet: Deep primitive segmentation using hybrid representations},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving 3D object detection with channel-wise transformer.
<em>ICCV</em>, 2723–2732. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Though 3D object detection from point clouds has achieved rapid progress in recent years, the lack of flexible and high-performance proposal refinement remains a great hurdle for existing state-of-the-art two-stage detectors. Previous works on refining 3D proposals have relied on human-designed components such as keypoints sampling, set abstraction and multi-scale feature fusion to produce powerful 3D object representations. Such methods, however, have limited ability to capture rich contextual dependencies among points. In this paper, we leverage the high-quality region proposal network and a Channel-wise Transformer architecture to constitute our two-stage 3D object detection framework (CT3D) with minimal hand-crafted design. The proposed CT3D simultaneously performs proposal-aware embedding and channel-wise context aggregation for the point features within each proposal. Specifically, CT3D uses proposal’s keypoints for spatial contextual modelling and learns attention propagation in the encoding module, mapping the proposal to point embeddings. Next, a new channel-wise decoding module enriches the query-key interaction via channel-wise re-weighting to effectively merge multi-level contexts, which contributes to more accurate object predictions. Extensive experiments demonstrate that our CT3D method has superior performance and excellent scalability. Remarkably, CT3D achieves the AP of 81.77\% in the moderate car category on the KITTI test 3D detection benchmark, outperforms state-of-the-art 3D detectors.},
  archive   = {C_ICCV},
  author    = {Hualian Shenga and Sijia Cai and Yuan Liu and Bing Deng and Jianqiang Huang and Xian-Sheng Hua and Min-Jian Zhao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00274},
  pages     = {2723-2732},
  title     = {Improving 3D object detection with channel-wise transformer},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning multi-scene absolute pose regression with
transformers. <em>ICCV</em>, 2713–2722. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Absolute camera pose regressors estimate the position and orientation of a camera from the captured image alone. Typically, a convolutional backbone with a multi-layer perceptron head is trained using images and pose labels to embed a single reference scene at a time. Recently, this scheme was extended for learning multiple scenes by replacing the MLP head with a set of fully connected layers. In this work, we propose to learn multi-scene absolute camera pose regression with Transformers, where encoders are used to aggregate activation maps with self-attention and decoders transform latent features and scenes encoding into candidate pose predictions. This mechanism allows our model to focus on general features that are informative for localization while embedding multiple scenes in parallel. We evaluate our method on commonly benchmarked indoor and outdoor datasets and show that it surpasses both multi-scene and state-of-the-art single-scene absolute pose regressors. We make our code publicly available from https://github.com/yolish/multi-scene-pose-transformer.},
  archive   = {C_ICCV},
  author    = {Yoli Shavit and Ron Ferens and Yosi Keller},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00273},
  pages     = {2713-2722},
  title     = {Learning multi-scene absolute pose regression with transformers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pyramid r-CNN: Towards better performance and adaptability
for 3D object detection. <em>ICCV</em>, 2703–2712. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a flexible and high-performance framework, named Pyramid R-CNN, for two-stage 3D object detection from point clouds. Current approaches generally rely on the points or voxels of interest for RoI feature extraction on the second stage, but cannot effectively handle the sparsity and non-uniform distribution of those points, and this may result in failures in detecting objects that are far away. To resolve the problems, we propose a novel second-stage module, named pyramid RoI head, to adaptively learn the features from the sparse points of interest. The pyramid RoI head consists of three key components. Firstly, we propose the RoI-grid Pyramid, which mitigates the sparsity problem by extensively collecting points of interest for each RoI in a pyramid manner. Secondly, we propose RoI-grid Attention, a new operation that can encode richer information from sparse points by incorporating conventional attention-based and graph-based point operators into a unified formulation. Thirdly, we propose the Density-Aware Radius Prediction (DARP) module, which can adapt to different point density levels by dynamically adjusting the focusing range of RoIs. Combining the three components, our pyramid RoI head is robust to the sparse and imbalanced circumstances, and can be applied upon various 3D backbones to consistently boost the detection performance. Extensive experiments show that Pyramid R-CNN outperforms the state-of-the-art 3D detection models by a large margin on both the KITTI dataset and the Waymo Open dataset.},
  archive   = {C_ICCV},
  author    = {Jiageng Mao and Minzhe Niu and Haoyue Bai and Xiaodan Liang and Hang Xu and Chunjing Xu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00272},
  pages     = {2703-2712},
  title     = {Pyramid R-CNN: Towards better performance and adaptability for 3D object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The devil is in the task: Exploiting reciprocal
appearance-localization features for monocular 3D object detection.
<em>ICCV</em>, 2693–2702. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Low-cost monocular 3D object detection plays a fundamental role in autonomous driving, whereas its accuracy is still far from satisfactory. In this paper, we dig into the 3D object detection task and reformulate it as the sub-tasks of object localization and appearance perception, which benefits to a deep excavation of reciprocal information underlying the entire task. We introduce a Dynamic Feature Reflecting Network, named DFR-Net, which contains two novel standalone modules: (i) the Appearance-Localization Feature Reflecting module (ALFR) that first separates task-specific features and then self-mutually reflects the reciprocal features; (ii) the Dynamic Intra-Trading module (DIT) that adaptively realigns the training processes of various sub-tasks via a self-learning manner. Extensive experiments on the challenging KITTI dataset demonstrate the effectiveness and generalization of DFR-Net. We rank 1 st among all the monocular 3D object detectors in the KITTI test set (till March 16th, 2021). The proposed method is also easy to be plug-and-play in many cutting-edge 3D detection frameworks at negligible cost to boost performance. The code will be made publicly available.},
  archive   = {C_ICCV},
  author    = {Zhikang Zou and Xiaoqing Ye and Liang Du and Xianhui Cheng and Xiao Tan and Li Zhang and Jianfeng Feng and Xiangyang Xue and Errui Ding},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00271},
  pages     = {2693-2702},
  title     = {The devil is in the task: Exploiting reciprocal appearance-localization features for monocular 3D object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Dual bipartite graph learning: A general approach for
domain adaptive object detection. <em>ICCV</em>, 2683–2692. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Domain Adaptive Object Detection (DAOD) relieves the reliance on large-scale annotated data by transferring the knowledge learned from a labeled source domain to a new unlabeled target domain. Recent DAOD approaches resort to local feature alignment in virtue of domain adversarial training in conjunction with the ad-hoc detection pipelines to achieve feature adaptation. However, these methods are limited to adapt the specific types of object detectors and do not explore the cross-domain topological relations. In this paper, we first formulate DAOD as an open-set domain adaptation problem in which foregrounds (pixel or region) can be seen as the “known class”, while backgrounds (pixel or region) are referred to as the “unknown class”. To this end, we present a new and general perspective for DAOD named Dual Bipartite Graph Learning (DBGL), which captures the cross-domain interactions on both pixel-level and semantic-level via increasing the distinction between foregrounds and backgrounds and modeling the cross-domain dependencies among different semantic categories. Experiments reveal that the proposed DBGL in conjunction with one-stage and two-stage detectors exceeds the state-of-the-art performance on standard DAOD benchmarks.},
  archive   = {C_ICCV},
  author    = {Chaoqi Chen and Jiongcheng Li and Zebiao Zheng and Yue Huang and Xinghao Ding and Yizhou Yu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00270},
  pages     = {2683-2692},
  title     = {Dual bipartite graph learning: A general approach for domain adaptive object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Time-multiplexed coded aperture imaging: Learned coded
aperture and pixel exposures for compressive imaging systems.
<em>ICCV</em>, 2672–2682. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Compressive imaging using coded apertures (CA) is a powerful technique that can be used to recover depth, light fields, hyperspectral images and other quantities from a single snapshot. The performance of compressive imaging systems based on CAs mostly depends on two factors: the properties of the mask&#39;s attenuation pattern, that we refer to as &quot;codification&quot;, and the computational techniques used to recover the quantity of interest from the coded snapshot. In this work, we introduce the idea of using time-varying CAs synchronized with spatially varying pixel shutters. We divide the exposure of a sensor into sub-exposures at the beginning of which the CA mask changes and at which the sensor&#39;s pixels are simultaneously and individually switched &quot;on&quot; or &quot;off&quot;. This is a practically appealing codification as it does not introduce additional optical components other than the already present CA but uses a change in the pixel shutter that can be easily realized electronically. We show that our proposed time-multiplexed coded aperture (TMCA) can be optimized end to end and induces better coded snapshots enabling superior reconstructions in two different applications: compressive light field imaging and hyperspectral imaging. We demonstrate both in simulation and with real captures (taken with prototypes we built) that this codification outperforms the state-of-the-art compressive imaging systems by a large margin in those applications.},
  archive   = {C_ICCV},
  author    = {Edwin Vargas and Julien N.P. Martel and Gordon Wetzstein and Henry Arguello},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00269},
  pages     = {2672-2682},
  title     = {Time-multiplexed coded aperture imaging: Learned coded aperture and pixel exposures for compressive imaging systems},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hybrid frequency-spatial domain model for sparse image
reconstruction in scanning transmission electron microscopy.
<em>ICCV</em>, 2662–2671. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Scanning transmission electron microscopy (STEM) is a powerful technique in high-resolution atomic imaging of materials. Decreasing scanning time and reducing electron beam exposure with an acceptable signal-to-noise ratio are two popular research aspects when applying STEM to beam-sensitive materials. Specifically, partially sampling with fixed electron doses is one of the most important solutions, and then the lost information is restored by computational methods. Following successful applications of deep learning in image in-painting, we have developed an encoder-decoder network to reconstruct STEM images in extremely sparse sampling cases. In our model, we combine both local pixel information from convolution operators and global texture features, by applying specific filter operations on the frequency domain to acquire initial reconstruction and global structure prior. Our method can effectively restore texture structures and be robust in different sampling ratios with Poisson noise. A comprehensive study demonstrates that our method gains about 50\% performance enhancement in comparison with the state-of-art methods. Code is available at https://github.com/icthrm/Sparse-Sampling-Reconstruction.},
  archive   = {C_ICCV},
  author    = {Bintao He and Fa Zhang and Huanshui Zhang and Renmin Han},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00268},
  pages     = {2662-2671},
  title     = {A hybrid frequency-spatial domain model for sparse image reconstruction in scanning transmission electron microscopy},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multispectral illumination estimation using deep unrolling
network. <em>ICCV</em>, 2652–2661. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper examines the problem of illumination spectra estimation in multispectral images. We cast the problem into a constrained matrix factorization problem and present a method for both single-global and multiple illumination estimation in which a deep unrolling network is constructed from the alternating direction method of multipliers(ADMM) optimization for solving the matrix factorization problem. To alleviate the lack of multispectral training data, we build a large multispectral reflectance image dataset for generating synthesized data and use them for training and evaluating our model. The results of simulations and real experiments demonstrate that the proposed method is able to outperform state-of-the-art spectral illumination estimation methods, and that it generalizes well to a wide variety of scenes and spectra.},
  archive   = {C_ICCV},
  author    = {Yuqi Li and Qiang Fu and Wolfgang Heidrich},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00267},
  pages     = {2652-2661},
  title     = {Multispectral illumination estimation using deep unrolling network},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Incorporating learnable membrane time constant to enhance
learning of spiking neural networks. <em>ICCV</em>, 2641–2651. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Spiking Neural Networks (SNNs) have attracted enormous research interest due to temporal information processing capability, low power consumption, and high biological plausibility. However, the formulation of efficient and high-performance learning algorithms for SNNs is still challenging. Most existing learning methods learn weights only, and require manual tuning of the membrane-related parameters that determine the dynamics of a single spiking neuron. These parameters are typically chosen to be the same for all neurons, which limits the diversity of neurons and thus the expressiveness of the resulting SNNs. In this paper, we take inspiration from the observation that membrane-related parameters are different across brain regions, and propose a training algorithm that is capable of learning not only the synaptic weights but also the membrane time constants of SNNs. We show that incorporating learnable membrane time constants can make the network less sensitive to initial values and can speed up learning. In addition, we reevaluate the pooling methods in SNNs and find that max-pooling will not lead to significant information loss and have the advantage of low computation cost and binary compatibility. We evaluate the proposed method for image classification tasks on both traditional static MNIST, Fashion-MNIST, CIFAR-10 datasets, and neuromorphic N-MNIST, CIFAR10-DVS, DVS128 Gesture datasets. The experiment results show that the proposed method outperforms the state-of-the-art accuracy on nearly all datasets, using fewer time-steps. Our codes are available at https://github.com/fangwei123456/Parametric-Leaky-Integrate-and-Fire-Spiking-Neuron.},
  archive   = {C_ICCV},
  author    = {Wei Fang and Zhaofei Yu and Yanqi Chen and Timothée Masquelier and Tiejun Huang and Yonghong Tian},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00266},
  pages     = {2641-2651},
  title     = {Incorporating learnable membrane time constant to enhance learning of spiking neural networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Single-shot hyperspectral-depth imaging with learned
diffractive optics. <em>ICCV</em>, 2631–2640. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Imaging depth and spectrum have been extensively studied in isolation from each other for decades. Recently, hyperspectral-depth (HS-D) imaging emerges to capture both information simultaneously by combining two different imaging systems; one for depth, the other for spectrum. While being accurate, this combinational approach induces increased form factor, cost, capture time, and alignment/registration problems. In this work, departing from the combinational principle, we propose a compact single-shot monocular HS-D imaging method. Our method uses a diffractive optical element (DOE), the point spread function of which changes with respect to both depth and spectrum. This enables us to reconstruct spectrum and depth from a single captured image. To this end, we develop a differentiable simulator and a neural-network-based reconstruction method that are jointly optimized via automatic differentiation. To facilitate learning the DOE, we present a first HS-D dataset by building a benchtop HS-D imager that acquires high-quality ground truth. We evaluate our method with synthetic and real experiments by building an experimental prototype and achieve state-of-the-art HS-D imaging results.},
  archive   = {C_ICCV},
  author    = {Seung-Hwan Baek and Hayato Ikoma and Daniel S. Jeon and Yuqi Li and Wolfgang Heidrich and Gordon Wetzstein and Min H. Kim},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00265},
  pages     = {2631-2640},
  title     = {Single-shot hyperspectral-depth imaging with learned diffractive optics},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Single image defocus deblurring using kernel-sharing
parallel atrous convolutions. <em>ICCV</em>, 2622–2630. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a novel deep learning approach for single image defocus deblurring based on inverse kernels. In a defocused image, the blur shapes are similar among pixels although the blur sizes can spatially vary. To utilize the property with inverse kernels, we exploit the observation that when only the size of a defocus blur changes while keeping the shape, the shape of the corresponding inverse kernel remains the same and only the scale changes. Based on the observation, we propose a kernel-sharing parallel atrous convolutional (KPAC) block specifically designed by incorporating the property of inverse kernels for single image defocus deblurring. To effectively simulate the invariant shapes of inverse kernels with different scales, KPAC shares the same convolutional weights among multiple atrous convolution layers. To efficiently simulate the varying scales of inverse kernels, KPAC consists of only a few atrous convolution layers with different dilations and learns per-pixel scale attentions to aggregate the outputs of the layers. KPAC also utilizes the shape attention to combine the outputs of multiple convolution filters in each atrous convolution layer, to deal with defocus blur with a slightly varying shape. We demonstrate that our approach achieves state-of-the-art performance with a much smaller number of parameters than previous methods.},
  archive   = {C_ICCV},
  author    = {Hyeongseok Son and Junyong Lee and Sunghyun Cho and Seungyong Lee},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00264},
  pages     = {2622-2630},
  title     = {Single image defocus deblurring using kernel-sharing parallel atrous convolutions},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extreme-quality computational imaging via degradation
framework. <em>ICCV</em>, 2612–2621. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To meet the space limitation of optical elements, free-form surfaces or high-order aspherical lenses are adopted in mobile cameras to compress volume. However, the application of free-form surfaces also introduces the problem of image quality mutation. Existing model-based deconvolution methods are inefficient in dealing with the degradation that shows a wide range of spatial variants over regions. And the deep learning techniques in low-level and physics-based vision suffer from a lack of accurate data. To address this issue, we develop a degradation framework to estimate the spatially variant point spread functions (PSFs) of mobile cameras. When input extreme-quality digital images, the proposed framework generates degraded images sharing a common domain with real-world photographs. Supplied with the synthetic image pairs, we design a Field-Of-View shared kernel prediction network (FOV-KPN) to perform spatial-adaptive reconstruction on real degraded photos. Extensive experiments demonstrate that the proposed approach achieves extreme-quality computational imaging and outperforms the state-of-the-art methods. Furthermore, we illustrate that our technique can be integrated into existing postprocessing systems, resulting in significantly improved visual quality.},
  archive   = {C_ICCV},
  author    = {Shiqi Chen and Huajun Feng and Keming Gao and Zhihai Xu and Yueting Chen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00263},
  pages     = {2612-2621},
  title     = {Extreme-quality computational imaging via degradation framework},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-supervised neural networks for spectral snapshot
compressive imaging. <em>ICCV</em>, 2602–2611. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider using untrained neural networks to solve the reconstruction problem of snapshot compressive imaging (SCI), which uses a two-dimensional (2D) detector to capture a high-dimensional (usually 3D) data-cube in a compressed manner. Various SCI systems have been built in recent years to capture data such as high-speed videos, hyperspectral images, and the state-of-the-art reconstruction is obtained by the deep neural networks. However, most of these networks are trained in an end-to-end manner by a large amount of corpus with sometimes simulated ground truth, measurement pairs. In this paper, inspired by the untrained neural networks such as deep image priors (DIP) and deep decoders, we develop a framework by integrating DIP into the plug-and-play regime, leading to a self-supervised network for spectral SCI reconstruction. Extensive synthetic and real data results show that the proposed algorithm without training is capable of achieving competitive results to the training based networks. Furthermore, by integrating the proposed method with a pre-trained deep denoising prior, we have achieved state-of-the-art results. Our code is available at https://github.com/mengziyi64/CASSI-Self-Supervised.},
  archive   = {C_ICCV},
  author    = {Ziyi Meng and Zhenming Yu and Kun Xu and Xin Yuan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00262},
  pages     = {2602-2611},
  title     = {Self-supervised neural networks for spectral snapshot compressive imaging},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Universal and flexible optical aberration correction using
deep-prior based deconvolution. <em>ICCV</em>, 2593–2601. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {High quality imaging usually requires bulky and expensive lenses to compensate geometric and chromatic aberrations. This poses high constraints on the optical hash or low cost applications. Although one can utilize algorithmic reconstruction to remove the artifacts of low-end lenses, the degeneration from optical aberrations is spatially varying and the computation has to trade off efficiency for performance. For example, we need to conduct patch-wise optimization or train a large set of local deep neural networks to achieve high reconstruction performance across the whole image. In this paper, we propose a PSF aware deep network, which takes the aberrant image and PSF map as input and produces the latent high quality version via incorporating deep priors, thus leading to a universal and flexible optical aberration correction method. Specifically, we pre-train a base model from a set of diverse lenses and then adapt it to a given lens by quickly refining the parameters, which largely alleviates the time and memory consumption of model learning. The approach is of high efficiency in both training and testing stages. Extensive results verify the promising applications of our proposed approach for compact low-end cameras. The code is available at https://github.com/leehsiu/UABC},
  archive   = {C_ICCV},
  author    = {Xiu Li and Jinli Suo and Weihang Zhang and Xin Yuan and Qionghai Dai},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00261},
  pages     = {2593-2601},
  title     = {Universal and flexible optical aberration correction using deep-prior based deconvolution},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A simple framework for 3D lensless imaging with programmable
masks. <em>ICCV</em>, 2583–2592. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Lensless cameras provide a framework to build thin imaging systems by replacing the lens in a conventional camera with an amplitude or phase mask near the sensor. Existing methods for lensless imaging can recover the depth and intensity of the scene, but they require solving computationally-expensive inverse problems. Furthermore, existing methods struggle to recover dense scenes with large depth variations. In this paper, we propose a lensless imaging system that captures a small number of measurements using different patterns on a programmable mask. In this context, we make three contributions. First, we present a fast recovery algorithm to recover textures on a fixed number of depth planes in the scene. Second, we consider the mask design problem, for programmable lensless cameras, and provide a design template for optimizing the mask patterns with the goal of improving depth estimation. Third, we use a refinement network as a post-processing step to identify and remove artifacts in the reconstruction. These modifications are evaluated extensively with experimental results on a lensless camera prototype to showcase the performance benefits of the optimized masks and recovery algorithms over the state of the art.},
  archive   = {C_ICCV},
  author    = {Yucheng Zheng and Yi Hua and Aswin C. Sankaranarayanan and M. Salman Asif},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00260},
  pages     = {2583-2592},
  title     = {A simple framework for 3D lensless imaging with programmable masks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Objects as cameras: Estimating high-frequency illumination
from shadows. <em>ICCV</em>, 2573–2582. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We recover high-frequency information encoded in the shadows cast by an object to estimate a hemispherical photograph from the viewpoint of the object, effectively turning objects into cameras. Estimating environment maps is useful for advanced image editing tasks such as relighting, object insertion or removal, and material parameter estimation. Because the problem is ill-posed, recent works in illumination recovery have tackled the problem of low- frequency lighting for object insertion, rely upon specular surface materials, or make use of data-driven methods that are susceptible to hallucination without physically plausible constraints. We incorporate an optimization scheme to update scene parameters that could enable practical capture of real-world scenes. Furthermore, we develop a methodology for evaluating expected recovery performance for different types and shapes of objects.},
  archive   = {C_ICCV},
  author    = {Tristan Swedish and Connor Henley and Ramesh Raskar},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00259},
  pages     = {2573-2582},
  title     = {Objects as cameras: Estimating high-frequency illumination from shadows},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Motion deblurring with real events. <em>ICCV</em>,
2563–2572. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose an end-to-end learning framework for event-based motion deblurring in a self-supervised manner, where real-world events are exploited to alleviate the performance degradation caused by data inconsistency. To achieve this end, optical flows are predicted from events, with which the blurry consistency and photometric consistency are exploited to enable self-supervision on the deblurring network with real-world data. Furthermore, a piecewise linear motion model is proposed to take into account motion non-linearities and thus leads to an accurate model for the physical formation of motion blurs in the real-world scenario. Extensive evaluation on both synthetic and real motion blur datasets demonstrates that the proposed algorithm bridges the gap between simulated and real-world motion blurs and shows remarkable performance for eventbased motion deblurring in real-world scenarios.},
  archive   = {C_ICCV},
  author    = {Fang Xu and Lei Yu and Bishan Wang and Wen Yang and Gui-Song Xia and Xu Jia and Zhendong Qiao and Jianzhuang Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00258},
  pages     = {2563-2572},
  title     = {Motion deblurring with real events},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning privacy-preserving optics for human pose
estimation. <em>ICCV</em>, 2553–2562. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The widespread use of always-connected digital cameras in our everyday life has led to increasing concerns about the users’ privacy and security. How to develop privacy- preserving computer vision systems? In particular, we want to prevent the camera from obtaining detailed visual data that may contain private information. However, we also want the camera to capture useful information to perform computer vision tasks. Inspired by the trend of jointly designing optics and algorithms, we tackle the problem of privacy-preserving human pose estimation by optimizing an optical encoder (hardware-level protection) with a software decoder (convolutional neural network) in an end-to- end framework. We introduce a visual privacy protection layer in our optical encoder that, parametrized appropriately, enables the optimization of the camera lens’s point spread function (PSF). We validate our approach with extensive simulations and a prototype camera. We show that our privacy-preserving deep optics approach successfully degrades or inhibits private attributes while maintaining important features to perform human pose estimation.},
  archive   = {C_ICCV},
  author    = {Carlos Hinojosa and Juan Carlos Niebles and Henry Arguello},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00257},
  pages     = {2553-2562},
  title     = {Learning privacy-preserving optics for human pose estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Event-based video reconstruction using transformer.
<em>ICCV</em>, 2543–2552. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Event cameras, which output events by detecting spatio- temporal brightness changes, bring a novel paradigm to image sensors with high dynamic range and low latency. Previous works have achieved impressive performances on event-based video reconstruction by introducing convolutional neural networks (CNNs). However, intrinsic locality of convolutional operations is not capable of modeling long-range dependency, which is crucial to many vision tasks. In this paper, we present a hybrid CNN- Transformer network for event-based video reconstruction (ET-Net), which merits the fine local information from CNN and global contexts from Transformer In addition, we further propose a Token Pyramid Aggregation strategy to implement multi-scale token integration for relating internal and intersected semantic concepts in the token-space. Experimental results demonstrate that our proposed method achieves superior performance over state-of-the-art methods on multiple real-world event datasets. The code is available at https://github.com/WarranWeng/ET-Net.},
  archive   = {C_ICCV},
  author    = {Wenming Weng and Yueyi Zhang and Zhiwei Xiong},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00256},
  pages     = {2543-2552},
  title     = {Event-based video reconstruction using transformer},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multitask AET with orthogonal tangent regularity for dark
object detection. <em>ICCV</em>, 2533–2542. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dark environment becomes a challenge for computer vision algorithms owing to insufficient photons and undesirable noise. To enhance object detection in a dark environment, we propose a novel multitask auto encoding transformation (MAET) model which is able to explore the intrinsic pattern behind illumination translation. In a self-supervision manner, the MAET learns the intrinsic visual structure by encoding and decoding the realistic illumination-degrading transformation considering the physical noise model and image signal processing (ISP). Based on this representation, we achieve the object detection task by decoding the bounding box coordinates and classes. To avoid the over-entanglement of two tasks, our MAET disentangles the object and degrading features by imposing an orthogonal tangent regularity. This forms a parametric manifold along which multitask predictions can be geometrically formulated by maximizing the orthogonality between the tangents along the outputs of respective tasks. Our framework can be implemented based on the mainstream object detection architecture and directly trained end-to-end using normal target detection datasets, such as VOC and COCO. We have achieved the state-of-the-art performance using synthetic and real-world datasets. Codes will be released at https://github.com/cuiziteng/MAET.},
  archive   = {C_ICCV},
  author    = {Ziteng Cui and Guo-Jun Qi and Lin Gu and Shaodi You and Zenghui Zhang and Tatsuya Harada},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00255},
  pages     = {2533-2542},
  title     = {Multitask AET with orthogonal tangent regularity for dark object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). COMISR: Compression-informed video super-resolution.
<em>ICCV</em>, 2523–2532. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most video super-resolution methods focus on restoring high-resolution video frames from low-resolution videos without taking into account compression. However, most videos on the web or mobile devices are compressed, and the compression can be severe when the bandwidth is limited. In this paper, we propose a new compression-informed video super-resolution model to restore high-resolution content without introducing artifacts caused by compression. The proposed model consists of three modules for video super-resolution: bi-directional recurrent warping, detail-preserving flow estimation, and Laplacian enhancement. All these three modules are used to deal with compression properties such as the location of the intra-frames in the input and smoothness in the output frames. For thorough performance evaluation, we conducted extensive experiments on standard datasets with a wide range of compression rates, covering many real video use cases. We showed that our method not only recovers high-resolution content on uncompressed frames from the widely-used benchmark datasets, but also achieves state-of-the-art performance in super-resolving compressed videos based on numerous quantitative metrics. We also evaluated the proposed method by simulating streaming from YouTube to demonstrate its effectiveness and robustness. The source codes and trained models are available at https://github.com/google-research/googleresearch/tree/master/comisr.},
  archive   = {C_ICCV},
  author    = {Yinxiao Li and Pengchong Jin and Feng Yang and Ce Liu and Ming-Hsuan Yang and Peyman Milanfar},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00254},
  pages     = {2523-2532},
  title     = {COMISR: Compression-informed video super-resolution},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Super resolve dynamic scene from continuous spike streams.
<em>ICCV</em>, 2513–2522. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, a novel retina-inspired camera, namely spike camera, has shown great potential for recording high-speed dynamic scenes. Unlike conventional digital cameras that compact the visual information within an exposure interval into a single snapshot, the spike camera continuously outputs binary spike streams to record the dynamic scenes, yielding a very high temporal resolution. Most of the existing reconstruction methods for spike camera focus on reconstructing images with the same resolution as spike camera. However, as a trade-off of high temporal resolution, the spatial resolution of spike camera is limited, resulting in inferior details of the reconstruction. To address this issue, we develop a spike camera super-resolution framework, aiming to super resolve high-resolution intensity images from the low-resolution binary spike streams. Due to the relative motion between the camera and the objects to capture, the spikes fired by the same sensor pixel no longer describes the same points in the external scene. In this paper, we exploit the relative motion and derive the relationship between light intensity and each spike, so as to recover the external scene with both high temporal and high spatial resolution. Experimental results demonstrate that the proposed method can reconstruct pleasant high-resolution images from low- resolution spike streams.},
  archive   = {C_ICCV},
  author    = {Jing Zhao and Jiyu Xie and Ruiqin Xiong and Jian Zhang and Zhaofei Yu and Tiejun Huang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00253},
  pages     = {2513-2522},
  title     = {Super resolve dynamic scene from continuous spike streams},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised non-rigid image distortion removal via grid
deformation. <em>ICCV</em>, 2502–2512. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many computer vision problems face difficulties when imaging through turbulent refractive media (e.g., air and water) due to the refraction and scattering of light. These effects cause geometric distortion that requires either handcrafted physical priors or supervised learning methods to remove. In this paper, we present a novel unsupervised network to recover the latent distortion-free image. The key idea is to model non-rigid distortions as deformable grids. Our network consists of a grid deformer that estimates the distortion field and an image generator that outputs the distortion-free image. By leveraging the positional encoding operator, we can simplify the network structure while maintaining fine spatial details in the recovered images. Our method doesn&#39;t need to be trained on labeled data and has good transferability across various turbulent image datasets with different types of distortions. Extensive experiments on both simulated and real-captured turbulent images demonstrate that our method can remove both air and water distortions without much customization.},
  archive   = {C_ICCV},
  author    = {Nianyi Li and Simron Thapa and Cameron Whyte and Albert Reed and Suren Jayasuriya and Jinwei Ye},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00252},
  pages     = {2502-2512},
  title     = {Unsupervised non-rigid image distortion removal via grid deformation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Photon-starved scene inference using single photon cameras.
<em>ICCV</em>, 2492–2501. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Scene understanding under low-light conditions is a challenging problem. This is due to the small number of photons captured by the camera and the resulting low signal-to-noise ratio (SNR). Single-photon cameras (SPCs) are an emerging sensing modality that are capable of capturing images with high sensitivity. Despite having minimal read-noise, images captured by SPCs in photon-starved conditions still suffer from strong shot noise, preventing reliable scene inference. We propose photon scale-space – a collection of high-SNR images spanning a wide range of photons-per-pixel (PPP) levels (but same scene content) as guides to train inference model on low photon flux images. We develop training techniques that push images with different illumination levels closer to each other in feature representation space. The key idea is that having a spectrum of different brightness levels during training enables effective guidance, and increases robustness to shot noise even in extreme noise cases. Based on the proposed approach, we demonstrate, via simulations and real experiments with a SPAD camera, high-performance on various inference tasks such as image classification and monocular depth estimation under ultra low-light, down to &lt; 1 PPP. Project Page: https://wisionlab.cs.wisc.edu/project/photon-net},
  archive   = {C_ICCV},
  author    = {Bhavya Goyal and Mohit Gupta},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00251},
  pages     = {2492-2501},
  title     = {Photon-starved scene inference using single photon cameras},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). HDR video reconstruction: A coarse-to-fine network and a
real-world benchmark dataset. <em>ICCV</em>, 2482–2491. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {High dynamic range (HDR) video reconstruction from sequences captured with alternating exposures is a very challenging problem. Existing methods often align low dynamic range (LDR) input sequence in the image space using optical flow, and then merge the aligned images to produce HDR output. However, accurate alignment and fusion in the image space are difficult due to the missing details in the over-exposed regions and noise in the under-exposed regions, resulting in unpleasing ghosting artifacts. To enable more accurate alignment and HDR fusion, we introduce a coarse-to-fine deep learning framework for HDR video reconstruction. Firstly, we perform coarse alignment and pixel blending in the image space to estimate the coarse HDR video. Secondly, we conduct more sophisticated alignment and temporal fusion in the feature space of the coarse HDR video to produce better reconstruction. Considering the fact that there is no publicly available dataset for quantitative and comprehensive evaluation of HDR video reconstruction methods, we collect such a benchmark dataset, which contains 97 sequences of static scenes and 184 testing pairs of dynamic scenes. Extensive experiments show that our method outperforms previous state-of-the-art methods. Our code and dataset can be found at https://guanyingc.github.io/DeepHDRVideo.},
  archive   = {C_ICCV},
  author    = {Guanying Chen and Chaofeng Chen and Shi Guo and Zhetong Liang and Kwan-Yee K. Wong and Lei Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00250},
  pages     = {2482-2491},
  title     = {HDR video reconstruction: A coarse-to-fine network and a real-world benchmark dataset},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SeLFVi: Self-supervised light-field video reconstruction
from stereo video. <em>ICCV</em>, 2471–2481. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Light-field imaging is appealing to the mobile devices market because of its capability for intuitive post-capture processing. Acquiring light field (LF) data with high angular, spatial and temporal resolution poses significant challenges, especially with space constraints preventing bulky optics. At the same time, stereo video capture, now available on many consumer devices, can be interpreted as a sparse LF-capture. We explore the application of small baseline stereo videos for reconstructing high fidelity LF videos.We propose a self-supervised learning-based algorithm for LF video reconstruction from stereo video. The self- supervised LF video reconstruction is guided via the geometric information from the individual stereo pairs and the temporal information from the video sequence. LF estimation is further regularized by a low-rank constraint based on layered LF displays. The proposed self-supervised algorithm facilitates advantages such as post-training finetuning on test sequences and variable angular view interpolation and extrapolation. Quantitatively the reconstructed LF videos show higher fidelity than previously proposed unsupervised approaches. We demonstrate our results via LF videos generated from publicly available stereo videos acquired from commercially available stereoscopic cameras. Finally, we demonstrate that our reconstructed LF videos allow applications such as post-capture focus control and region-of-interest (RoI) based focus tracking for videos.},
  archive   = {C_ICCV},
  author    = {Prasan Shedligeri and Florian Schiffers and Sushobhan Ghosh and Oliver Cossairt and Kaushik Mitra},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00249},
  pages     = {2471-2481},
  title     = {SeLFVi: Self-supervised light-field video reconstruction from stereo video},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distillation-guided image inpainting. <em>ICCV</em>,
2461–2470. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Image inpainting methods have shown significant improvements by using deep neural networks recently. However, many of these techniques often create distorted structures or blurry inconsistent textures. The problem is rooted in the encoder layers’ ineffectiveness in building a complete and faithful embedding of the missing regions from scratch. Existing solutions like course-to-fine, progressive refinement, structural guidance, etc. suffer from huge computational overheads owing to multiple generator networks, limited ability of handcrafted features, and sub-optimal utilization of the information present in the ground truth. We propose a distillation-based approach for inpainting, where we provide direct feature level supervision while training. We deploy cross and self-distillation techniques and design a dedicated completion-block in encoder to produce more accurate encoding of the holes. Next, we demonstrate how an inpainting network’s attention module can improve by leveraging a distillation-based attention transfer technique and further enhance coherence by using a pixeladaptive global-local feature fusion. We conduct extensive evaluations on multiple datasets to validate our method. Along with achieving significant improvements over previous SOTA methods, the proposed approach’s effectiveness is also demonstrated through its ability to improve existing inpainting works.},
  archive   = {C_ICCV},
  author    = {Maitreya Suin and Kuldeep Purohit and A. N. Rajagopalan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00248},
  pages     = {2461-2470},
  title     = {Distillation-guided image inpainting},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Real-time image enhancer via learnable spatial-aware 3D
lookup tables. <em>ICCV</em>, 2451–2460. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, deep learning-based image enhancement algorithms achieved state-of-the-art (SOTA) performance on several publicly available datasets. However, most existing methods fail to meet practical requirements either for visual perception or for computation efficiency, especially for high-resolution images. In this paper, we propose a novel real-time image enhancer via learnable spatial-aware 3dimentional lookup tables(3D LUTs), which well considers global scenario and local spatial information. Specifically, we introduce a light weight two-head weight predictor that has two outputs. One is a 1D weight vector used for image-level scenario adaptation, the other is a 3D weight map aimed for pixel-wise category fusion. We learn the spatial-aware 3D LUTs and fuse them according to the aforementioned weights in an end-to-end manner. The fused LUT is then used to transform the source image into the target tone in an efficient way. Extensive results show that our model outperforms SOTA image enhancement methods on public datasets both subjectively and objectively, and that our model only takes about 4ms to process a 4K resolution image on one NVIDIA V100 GPU.},
  archive   = {C_ICCV},
  author    = {Tao Wang and Yong Li and Jingyang Peng and Yipeng Ma and Xian Wang and Fenglong Song and Youliang Yan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00247},
  pages     = {2451-2460},
  title     = {Real-time image enhancer via learnable spatial-aware 3D lookup tables},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep reparametrization of multi-frame super-resolution and
denoising. <em>ICCV</em>, 2440–2450. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a deep reparametrization of the maximum a posteriori formulation commonly employed in multi-frame image restoration tasks. Our approach is derived by introducing a learned error metric and a latent representation of the target image, which transforms the MAP objective to a deep feature space. The deep reparametrization allows us to directly model the image formation process in the latent space, and to integrate learned image priors into the prediction. Our approach thereby leverages the advantages of deep learning, while also benefiting from the principled multi-frame fusion provided by the classical MAP formulation. We validate our approach through comprehensive experiments on burst denoising and burst super-resolution datasets. Our approach sets a new state-of-the-art for both tasks, demonstrating the generality and effectiveness of the proposed formulation.},
  archive   = {C_ICCV},
  author    = {Goutam Bhat and Martin Danelljan and Fisher Yu and Luc Van Gool and Radu Timofte},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00246},
  pages     = {2440-2450},
  title     = {Deep reparametrization of multi-frame super-resolution and denoising},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning dynamic interpolation for extremely sparse light
fields with wide baselines. <em>ICCV</em>, 2430–2439. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we tackle the problem of dense light field (LF) reconstruction from sparsely-sampled ones with wide baselines and propose a learnable model, namely dynamic interpolation, to replace the commonly-used geometry warping operation. Specifically, with the estimated geometric relation between input views, we first construct a lightweight neural network to dynamically learn weights for interpolating neighbouring pixels from input views to synthesize each pixel of novel views independently. In contrast to the fixed and content-independent weights employed in the geometry warping operation, the learned interpolation weights implicitly incorporate the correspondences between the source and novel views and adapt to different image content information. Then, we recover the spatial correlation between the independently synthesized pixels of each novel view by referring to that of input views using a geometry-based spatial refinement module. We also constrain the angular correlation between the novel views through a disparity-oriented LF structure loss. Experimental results on LF datasets with wide baselines show that the reconstructed LFs achieve much higher PSNR/SSIM and preserve the LF parallax structure better than state-of-the-art methods. The source code is publicly available at https://github.com/MantangGuo/DI4SLF.},
  archive   = {C_ICCV},
  author    = {Mantang Guo and Jing Jin and Hui Liu and Junhui Hou},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00245},
  pages     = {2430-2439},
  title     = {Learning dynamic interpolation for extremely sparse light fields with wide baselines},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Virtual light transport matrices for non-line-of-sight
imaging. <em>ICCV</em>, 2420–2429. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The light transport matrix (LTM) is an instrumental tool in line-of-sight (LOS) imaging, describing how light interacts with the scene and enabling applications such as relighting or separation of illumination components. We introduce a framework to estimate the LTM of non-line-of-sight (NLOS) scenarios, coupling recent virtual forward light propagation models for NLOS imaging with the LOS light transport equation. We design computational projector-camera setups, and use these virtual imaging systems to estimate the transport matrix of hidden scenes. We introduce the specific illumination functions to compute the different elements of the matrix, overcoming the challenging wide-aperture conditions of NLOS setups. Our NLOS light transport matrix allows us to (re)illuminate specific locations of a hidden scene, and separate direct, first-order indirect, and higher-order indirect illumination of complex cluttered hidden scenes, similar to existing LOS techniques.},
  archive   = {C_ICCV},
  author    = {Julio Marco and Adrian Jarabo and Ji Hyun Nam and Xiaochun Liu and Miguel Ángel Cosculluela and Andreas Velten and Diego Gutierrez},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00244},
  pages     = {2420-2429},
  title     = {Virtual light transport matrices for non-line-of-sight imaging},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A dark flash normal camera. <em>ICCV</em>, 2410–2419. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Casual photography is often performed in uncontrolled lighting that can result in low quality images and degrade the performance of downstream processing. We consider the problem of estimating surface normal and reflectance maps of scenes depicting people despite these conditions by supplementing the available visible illumination with a single near infrared (NIR) light source and camera, a socalled &quot;dark flash image&quot;. Our method takes as input a single color image captured under arbitrary visible lighting and a single dark flash image captured under controlled front-lit NIR lighting at the same viewpoint, and computes a normal map, a diffuse albedo map, and a specular intensity map of the scene. Since ground truth normal and reflectance maps of faces are difficult to capture, we propose a novel training technique that combines information from two readily available and complementary sources: a stereo depth signal and photometric shading cues. We evaluate our method over a range of subjects and lighting conditions and describe two applications: optimizing stereo geometry and filling the shadows in an image.},
  archive   = {C_ICCV},
  author    = {Zhihao Xia and Jason Lawrence and Supreeth Achar},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00243},
  pages     = {2410-2419},
  title     = {A dark flash normal camera},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A light stage on every desk. <em>ICCV</em>, 2400–2409. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Every time you sit in front of a TV or monitor, your face is actively illuminated by time-varying patterns of light. This paper proposes to use this time-varying illumination for synthetic relighting of your face with any new illumination condition. In doing so, we take inspiration from the light stage work of Debevec et al. [4], who first demonstrated the ability to relight people captured in a controlled lighting environment. Whereas existing light stages require expensive, room-scale spherical capture gantries and exist in only a few labs in the world, we demonstrate how to acquire useful data from a normal TV or desktop monitor. Instead of subjecting the user to uncomfortable rapidly flashing light patterns, we operate on images of the user watching a YouTube video or other standard content. We train a deep network on images plus monitor patterns of a given user and learn to predict images of that user under any target illumination (monitor pattern). Experimental evaluation shows that our method produces realistic relighting results. Video results are available at grail.cs.washington.edu/projects/Light_Stage_on_Every_Desk/.},
  archive   = {C_ICCV},
  author    = {Soumyadip Sengupta and Brian Curless and Ira Kemelmacher-Shlizerman and Steve Seitz},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00242},
  pages     = {2400-2409},
  title     = {A light stage on every desk},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Large scale multi-illuminant (LSMI) dataset for developing
white balance algorithm under mixed illumination. <em>ICCV</em>,
2390–2399. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a Large Scale Multi-Illuminant (LSMI) Dataset that contains 7,486 images, captured with three different cameras on more than 2,700 scenes with two or three illuminants. For each image in the dataset, the new dataset provides not only the pixel-wise ground truth illumination but also the chromaticity of each illuminant in the scene and the mixture ratio of illuminants per pixel. Images in our dataset are mostly captured with illuminants existing in the scene, and the ground truth illumination is computed by taking the difference between the images with different illumination combination. Therefore, our dataset captures natural composition in the real-world setting with wide field-of-view, providing more extensive dataset compared to existing datasets for multi-illumination white balance. As conventional single illuminant white balance algorithms cannot be directly applied, we also apply per-pixel DNN-based white balance algorithm and show its effectiveness against using patch-wise white balancing. We validate the benefits of our dataset through extensive analysis including a user-study, and expect the dataset to make meaningful contribution for future work in white balancing.},
  archive   = {C_ICCV},
  author    = {Dongyoung Kim and Jinwoo Kim and Seonghyeon Nam and Dongwoo Lee and Yeonkyung Lee and Nahyup Kang and Hyong-Euk Lee and ByungIn Yoo and Jae-Joon Han and Seon Joo Kim},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00241},
  pages     = {2390-2399},
  title     = {Large scale multi-illuminant (LSMI) dataset for developing white balance algorithm under mixed illumination},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NeuSpike-net: High speed video reconstruction via
bio-inspired neuromorphic cameras. <em>ICCV</em>, 2380–2389. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Neuromorphic vision sensor is a new bio-inspired imaging paradigm that emerged in recent years, which continuously sensing luminance intensity and firing asynchronous spikes (events) with high temporal resolution. Typically, there are two types of neuromorphic vision sensors, namely dynamic vision sensor (DVS) and spike camera. From the perspective of bio-inspired sampling, DVS only perceives movement by imitating the retinal periphery, while the spike camera was developed to perceive fine textures by simulating the fovea. It is meaningful to explore how to combine two types of neuromorphic cameras to reconstruct high quality image like human vision. In this paper, we propose a NeuSpike-Net to learn both the high dynamic range and high motion sensitivity of DVS and the full texture sampling of spike camera to achieve high-speed and high dynamic image reconstruction. We propose a novel representation to effectively extract the temporal information of spike and event data. By introducing the feature fusion module, the two types of neuromorphic data achieve complementary to each other. The experimental results on the simulated and real datasets demonstrate that the proposed approach is effective to reconstruct high-speed and high dynamic range images via the combination of spike and event data.},
  archive   = {C_ICCV},
  author    = {Lin Zhu and Jianing Li and Xiao Wang and Tiejun Huang and Yonghong Tian},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00240},
  pages     = {2380-2389},
  title     = {NeuSpike-net: High speed video reconstruction via bio-inspired neuromorphic cameras},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). V-DESIRR: Very fast deep embedded single image reflection
removal. <em>ICCV</em>, 2370–2379. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real world images often gets corrupted due to unwanted reflections and their removal is highly desirable. A major share of such images originate from smart phone cameras capable of very high resolution captures. Most of the existing methods either focus on restoration quality by compromising on processing speed and memory requirements or, focus on removing reflections at very low resolutions, there by limiting their practical deploy-ability. We propose a light weight deep learning model for reflection removal using a novel scale space architecture. Our method processes the corrupted image in two stages, a Low Scale Sub-network (LSSNet) to process the lowest scale and a Progressive Inference (PI) stage to process all the higher scales. In order to reduce the computational complexity, the sub-networks in PI stage are designed to be much shallower than LSSNet. Moreover, we employ weight sharing between various scales within the PI stage to limit the model size. This also allows our method to generalize to very high resolutions without explicit retraining. Our method is superior both qualitatively and quantitatively compared to the state of the art methods and at the same time 20× faster with 50× less number of parameters compared to the most recent state-of-the-art algorithm RAGNet. We implemented our method on an android smart phone, where a high resolution 12 MP image is restored in under 5 seconds.},
  archive   = {C_ICCV},
  author    = {B H Pawan Prasad and K S Green Rosh and R B Lokesh and Kaushik Mitra and Sanjoy Chowdhury},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00239},
  pages     = {2370-2379},
  title     = {V-DESIRR: Very fast deep embedded single image reflection removal},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Variable-rate deep image compression through
spatially-adaptive feature transform. <em>ICCV</em>, 2360–2369. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a versatile deep image compression network based on Spatial Feature Transform (SFT) [45], which takes a source image and a corresponding quality map as inputs and produce a compressed image with variable rates. Our model covers a wide range of compression rates using a single model, which is controlled by arbitrary pixel-wise quality maps. In addition, the proposed framework allows us to perform task-aware image compressions for various tasks, e.g., classification, by efficiently estimating optimized quality maps specific to target tasks for our encoding network. This is even possible with a pretrained network without learning separate models for individual tasks. Our algorithm achieves outstanding rate-distortion trade-off compared to the approaches based on multiple models that are optimized separately for several different target rates. At the same level of compression, the proposed approach successfully improves performance on image classification and text region quality preservation via task-aware quality map estimation without additional model training. The code is available at the project website 1 .},
  archive   = {C_ICCV},
  author    = {Myungseo Song and Jinyoung Choi and Bohyung Han},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00238},
  pages     = {2360-2369},
  title     = {Variable-rate deep image compression through spatially-adaptive feature transform},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Lucas-kanade reloaded: End-to-end super-resolution from raw
image bursts. <em>ICCV</em>, 2350–2359. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This presentation addresses the problem of reconstructing a high-resolution image from multiple lower-resolution snapshots captured from slightly different viewpoints in space and time. Key challenges for solving this super-resolution problem include (i) aligning the input pictures with sub-pixel accuracy, (ii) handling raw (noisy) images for maximal faithfulness to native camera data, and (iii) designing/learning an image prior (regularizer) well suited to the task. We address these three challenges with a hybrid algorithm building on the insight from [45] that aliasing is an ally in this setting, with parameters that can be learned end to end, while retaining the interpretability of classical approaches to inverse problems. The effectiveness of our approach is demonstrated on synthetic and real image bursts, setting a new state of the art on several benchmarks and delivering excellent qualitative results on real raw bursts captured by smartphones and prosumer cameras. Our code is available at https://github.com/bruno-31/lkburst.git.},
  archive   = {C_ICCV},
  author    = {Bruno Lecouat and Jean Ponce and Julien Mairal},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00237},
  pages     = {2350-2359},
  title     = {Lucas-kanade reloaded: End-to-end super-resolution from raw image bursts},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fourier space losses for efficient perceptual image
super-resolution. <em>ICCV</em>, 2340–2349. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many super-resolution (SR) models are optimized for high performance only and therefore lack efficiency due to large model complexity. As large models are often not practical in real-world applications, we investigate and propose novel loss functions, to enable SR with high perceptual quality from much more efficient models. The representative power for a given low-complexity generator network can only be fully leveraged by strong guidance towards the optimal set of parameters. We show that it is possible to improve the performance of a recently introduced efficient generator architecture solely with the application of our proposed loss functions. In particular, we use a Fourier space supervision loss for improved restoration of missing high-frequency (HF) content from the ground truth image and design a discriminator architecture working directly in the Fourier domain to better match the target HF distribution. We show that our losses’ direct emphasis on the frequencies in Fourier-space significantly boosts the perceptual image quality, while at the same time retaining high restoration quality in comparison to previously proposed loss functions for this task. The performance is further improved by utilizing a combination of spatial and frequency domain losses, as both representations provide complementary information during training. On top of that, the trained generator achieves comparable results with and is 2.4× and 48× faster than state-of-the-art perceptual SR methods RankSRGAN and SRFlow respectively.},
  archive   = {C_ICCV},
  author    = {Dario Fuoli and Luc Van Gool and Radu Timofte},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00236},
  pages     = {2340-2349},
  title     = {Fourier space losses for efficient perceptual image super-resolution},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). C2N: Practical generative noise modeling for real-world
denoising. <em>ICCV</em>, 2330–2339. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning-based image denoising methods have been bounded to situations where well-aligned noisy and clean images are given, or samples are synthesized from predetermined noise models, e.g., Gaussian. While recent generative noise modeling methods aim to simulate the unknown distribution of real-world noise, several limitations still exist. In a practical scenario, a noise generator should learn to simulate the general and complex noise distribution without using paired noisy and clean images. However, since existing methods are constructed on the unrealistic assumption of real-world noise, they tend to generate implausible patterns and cannot express complicated noise maps. Therefore, we introduce a Clean-to-Noisy image generation framework, namely C2N, to imitate complex real-world noise without using any paired examples. We construct the noise generator in C2N accordingly with each component of real-world noise characteristics to express a wide range of noise accurately. Combined with our C2N, conventional denoising CNNs can be trained to outperform existing unsupervised methods on challenging real-world benchmarks by a large margin.},
  archive   = {C_ICCV},
  author    = {Geonwoon Jang and Wooseok Lee and Sanghyun Son and Kyoungmu Lee},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00235},
  pages     = {2330-2339},
  title     = {C2N: Practical generative noise modeling for real-world denoising},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inference of black hole fluid-dynamics from sparse
interferometric measurements. <em>ICCV</em>, 2320–2329. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We develop an approach to recover the underlying properties of fluid-dynamical processes from sparse measurements. We are motivated by the task of imaging the stochastically evolving environment surrounding black holes, and demonstrate how flow parameters can be estimated from sparse interferometric measurements used in radio astronomical imaging. To model the stochastic flow we use spatio-temporal Gaussian Random Fields (GRFs). The high dimensionality of the underlying source video makes direct representation via a GRF’s full covariance matrix intractable. In contrast, stochastic partial differential equations are able to capture correlations at multiple scales by specifying only local interaction coefficients. Our approach estimates the coefficients of a space-time diffusion equation that dictates the stationary statistics of the dynamical process. We analyze our approach on realistic simulations of black hole evolution and demonstrate its advantage over state-of-the-art dynamic black hole imaging techniques.},
  archive   = {C_ICCV},
  author    = {Aviad Levis and Daeyoung Lee and Joel A. Tropp and Charles F. Gammie and Katherine L. Bouman},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00234},
  pages     = {2320-2329},
  title     = {Inference of black hole fluid-dynamics from sparse interferometric measurements},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). What you can learn by staring at a blank wall.
<em>ICCV</em>, 2310–2319. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a passive non-line-of-sight method that infers the number of people or activity of a person from the observation of a blank wall in an unknown room. Our technique analyzes complex imperceptible changes in indirect illumination in a video of the wall to reveal a signal that is correlated with motion in the hidden part of a scene. We use this signal to classify between zero, one, or two moving people, or the activity of a person in the hidden scene. We train two convolutional neural networks using data collected from 20 different scenes, and achieve an accuracy of ≈ 94\% for both tasks in unseen test environments and real-time online settings. Unlike other passive non-line-of-sight methods, the technique does not rely on known occluders or controllable light sources, and generalizes to unknown rooms with no recalibration. We analyze the generalization and robustness of our method with both real and synthetic data, and study the effect of the scene parameters on the signal quality. 1},
  archive   = {C_ICCV},
  author    = {Prafull Sharma and Miika Aittala and Yoav Y. Schechner and Antonio Torralba and Gregory W. Wornell and William T. Freeman and Frédo Durand},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00233},
  pages     = {2310-2319},
  title     = {What you can learn by staring at a blank wall},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Anonymizing egocentric videos. <em>ICCV</em>, 2300–2309. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In egocentric videos, the face of a wearer capturing the video is never captured. This gives a false sense of security that the wearer’s privacy is preserved while sharing such videos. However, egocentric cameras are typically harnessed to wearer’s head, and hence, also capture wearer’s gait. Recent works have shown that wearer gait signatures can be extracted from egocentric videos, which can be used to determine if two egocentric videos have the same wearer. In a more damaging scenario, one can even recognize a wearer using hand gestures from egocentric videos, or identify a wearer in third person videos such as from a surveillance camera. We believe, this could be a death knell in sharing of egocentric videos, and fatal for egocentric vision research. In this work, we suggest a novel technique to anonymize egocentric videos, which create carefully crafted, but small, and imperceptible optical flow perturbations in an egocentric video’s frames. Importantly, these perturbations do not affect object detection or action/activity recognition from egocentric videos but are strong enough to dis-balance the gait recovery process. In our experiments on benchmark EPIC-Kitchens dataset, the proposed perturbation degrades the wearer recognition performance of [42], from 66.3\% to 13.4\%, while preserving the activity recognition performance of [10] from 89.6\% to 87.4\%. To test our anonymization with more wearer recognition techniques, we also developed a stronger, and more generalizable wearer recognition method based on camera egomotion cues. The approach achieves state-ofthe-art (SOTA) performance of 59.67\% on EPIC-Kitchens, compared to 55.06\% by [42]. However, the accuracy of our recognition technique also drops to 12\% using the proposed anonymizing perturbations.},
  archive   = {C_ICCV},
  author    = {Daksh Thapar and Aditya Nigam and Chetan Arora},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00232},
  pages     = {2300-2309},
  title     = {Anonymizing egocentric videos},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatially-adaptive image restoration using distortion-guided
networks. <em>ICCV</em>, 2289–2299. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a general learning-based solution for restoring images suffering from spatially-varying degradations. Prior approaches are typically degradation-specific and employ the same processing across different images and different pixels within. However, we hypothesize that such spatially rigid processing is suboptimal for simultaneously restoring the degraded pixels as well as reconstructing the clean regions of the image. To overcome this limitation, we propose SPAIR, a network design that harnesses distortion-localization information and dynamically adjusts computation to difficult regions in the image. SPAIR comprises of two components, (1) a localization network that identifies degraded pixels, and (2) a restoration network that exploits knowledge from the localization network in filter and feature domain to selectively and adaptively restore degraded pixels. Our key idea is to exploit the non-uniformity of heavy degradations in spatial-domain and suitably embed this knowledge within distortion-guided modules performing sparse normalization, feature extraction and attention. Our architecture is agnostic to physical formation model and generalizes across several types of spatially-varying degradations. We demonstrate the efficacy of SPAIR individually on four restoration tasks- removal of rain-streaks, raindrops, shadows and motion blur. Extensive qualitative and quantitative comparisons with prior art on 11 benchmark datasets demonstrate that our degradation-agnostic network design offers significant performance gains over state-of-the-art degradation-specific architectures. Code available at https://github.com/humananalysis/spatially-adaptive-image-restoration.},
  archive   = {C_ICCV},
  author    = {Kuldeep Purohit and Maitreya Suin and A. N. Rajagopalan and Vishnu Naresh Boddeti},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00231},
  pages     = {2289-2299},
  title     = {Spatially-adaptive image restoration using distortion-guided networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hybrid neural fusion for full-frame video stabilization.
<em>ICCV</em>, 2279–2288. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing video stabilization methods often generate visible distortion or require aggressive cropping of frame boundaries, resulting in smaller field of views. In this work, we present a frame synthesis algorithm to achieve full-frame video stabilization. We first estimate dense warp fields from neighboring frames and then synthesize the stabilized frame by fusing the warped contents. Our core technical novelty lies in the learning-based hybrid-space fusion that alleviates artifacts caused by optical flow inaccuracy and fast-moving objects. We validate the effectiveness of our method on the NUS, selfie, and DeepStab video datasets. Extensive experiment results demonstrate the merits of our approach over prior video stabilization methods.},
  archive   = {C_ICCV},
  author    = {Yu-Lun Liu and Wei-Sheng Lai and Ming-Hsuan Yang and Yung-Yu Chuang and Jia-Bin Huang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00230},
  pages     = {2279-2288},
  title     = {Hybrid neural fusion for full-frame video stabilization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to reduce defocus blur by realistically modeling
dual-pixel data. <em>ICCV</em>, 2269–2278. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent work has shown impressive results on data-driven defocus deblurring using the two-image views available on modern dual-pixel (DP) sensors. One significant challenge in this line of research is access to DP data. Despite many cameras having DP sensors, only a limited number provide access to the low-level DP sensor images. In addition, capturing training data for defocus deblurring involves a time-consuming and tedious setup requiring the camera’s aperture to be adjusted. Some cameras with DP sensors (e.g., smartphones) do not have adjustable apertures, further limiting the ability to produce the necessary training data. We address the data capture bottleneck by proposing a procedure to generate realistic DP data synthetically. Our synthesis approach mimics the optical image formation found on DP sensors and can be applied to virtual scenes rendered with standard computer software. Leveraging these realistic synthetic DP images, we introduce a recurrent convolutional network (RCN) architecture that improves deblurring results and is suitable for use with single-frame and multi-frame data (e.g., video) captured by DP sensors. Finally, we show that our synthetic DP data is useful for training DNN models targeting video deblurring applications where access to DP data remains challenging.},
  archive   = {C_ICCV},
  author    = {Abdullah Abuolaim and Mauricio Delbracio and Damien Kelly and Michael S. Brown and Peyman Milanfar},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00229},
  pages     = {2269-2278},
  title     = {Learning to reduce defocus blur by realistically modeling dual-pixel data},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Semantic-embedded unsupervised spectral reconstruction from
single RGB images in the wild. <em>ICCV</em>, 2259–2268. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper investigates the problem of reconstructing hyperspectral (HS) images from single RGB images captured by commercial cameras, without using paired HS and RGB images during training. To tackle this challenge, we propose a new lightweight and end-to-end learning-based framework. Specifically, on the basis of the intrinsic imaging degradation model of RGB images from HS images, we progressively spread the differences between input RGB images and re-projected RGB images from recovered HS images via effective unsupervised camera spectral response function estimation. To enable the learning without paired ground-truth HS images as supervision, we adopt the adversarial learning manner and boost it with a simple yet effective ℒ 1 gradient clipping scheme. Besides, we embed the semantic information of input RGB images to locally regularize the unsupervised learning, which is expected to promote pixels with identical semantics to have consistent spectral signatures. In addition to conducting quantitative experiments over two widely-used datasets for HS image reconstruction from synthetic RGB images, we also evaluate our method by applying recovered HS images from real RGB images to HS-based visual tracking. Extensive results show that our method significantly outperforms state-of-the-art unsupervised methods and even exceeds the latest supervised method under some settings. The source code is public available at https://github.com/zbzhzhy/Unsupervised-Spectral-Reconstruction.},
  archive   = {C_ICCV},
  author    = {Zhiyu Zhu and Hui Liu and Junhui Hou and Huanqiang Zeng and Qingfu Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00228},
  pages     = {2259-2268},
  title     = {Semantic-embedded unsupervised spectral reconstruction from single RGB images in the wild},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High quality disparity remapping with two-stage warping.
<em>ICCV</em>, 2249–2258. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A high quality disparity remapping method that preserves 2D shapes and 3D structures, and adjusts disparities of important objects in stereo image pairs is proposed. It is formulated as a constrained optimization problem, whose solution is challenging, since we need to meet multiple requirements of disparity remapping simultaneously. The one-stage optimization process either degrades the quality of important objects or introduces serious distortions in background regions. To address this challenge, we propose a two-stage warping process to solve it. In the first stage, we develop a warping model that finds the optimal warping grids for important objects to fulfill multiple requirements of disparity remapping. In the second stage, we derive another warping model to refine warping results in less important regions by eliminating serious distortions in shape, disparity and 3D structure. The superior performance of the proposed method is demonstrated by experimental results.},
  archive   = {C_ICCV},
  author    = {Bing Li and Chia-Wen Lin and Cheng Zheng and Shan Liu and Junsong Yuan and Bernard Ghanem and C.-C. Jay Kuo},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00227},
  pages     = {2249-2258},
  title     = {High quality disparity remapping with two-stage warping},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic CT reconstruction from limited views with implicit
neural representations and parametric motion fields. <em>ICCV</em>,
2238–2248. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reconstructing dynamic, time-varying scenes with computed tomography (4D-CT) is a challenging and ill-posed problem common to industrial and medical settings. Existing 4D-CT reconstructions are designed for sparse sampling schemes that require fast CT scanners to capture multiple, rapid revolutions around the scene in order to generate high quality results. However, if the scene is moving too fast, then the sampling occurs along a limited view and is difficult to reconstruct due to spatiotemporal ambiguities. In this work, we design a reconstruction pipeline using implicit neural representations coupled with a novel parametric motion field warping to perform limited view 4D-CT reconstruction of rapidly deforming scenes. Importantly, we utilize a differentiable analysis-bysynthesis approach to compare with captured x-ray sinogram data in a self-supervised fashion. Thus, our resulting optimization method requires no training data to reconstruct the scene. We demonstrate that our proposed system robustly reconstructs scenes containing deformable and periodic motion and validate against state-of-the-art baselines. Further, we demonstrate an ability to reconstruct continuous spatiotemporal representations of our scenes and upsample them to arbitrary volumes and frame rates post-optimization. This research opens a new avenue for implicit neural representations in computed tomography reconstruction in general. Code is available at https://github.com/awreed/DynamicCTReconstruction.},
  archive   = {C_ICCV},
  author    = {Albert W. Reed and Hyojin Kim and Rushil Anirudh and K. Aditya Mohan and Kyle Champley and Jingu Kang and Suren Jayasuriya},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00226},
  pages     = {2238-2248},
  title     = {Dynamic CT reconstruction from limited views with implicit neural representations and parametric motion fields},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hyperspectral image denoising with realistic data.
<em>ICCV</em>, 2228–2237. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The hyperspectral image (HSI) denoising has been widely utilized to improve HSI qualities. Recently, learning-based HSI denoising methods have shown their effectiveness, but most of them are based on synthetic dataset and lack the generalization capability on real testing HSI. Moreover, there is still no public paired real HSI denoising dataset to learn HSI denoising network and quantitatively evaluate HSI methods. In this paper, we mainly focus on how to produce realistic dataset for learning and evaluating HSI denoising network. On the one hand, we collect a paired real HSI denoising dataset, which consists of short-exposure noisy HSIs and the corresponding long-exposure clean HSIs. On the other hand, we propose an accurate HSI noise model which matches the distribution of real data well and can be employed to synthesize realistic dataset. On the basis of the noise model, we present an approach to calibrate the noise parameters of the given hyperspectral camera. The extensive experimental results show that a network learned with only synthetic data generated by our noise model performs as well as it is learned with paired real data. Our code and data are available at: https://github.com/ColinTaoZhang/HSIDwRD.},
  archive   = {C_ICCV},
  author    = {Tao Zhang and Ying Fu and Cheng Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00225},
  pages     = {2228-2237},
  title     = {Hyperspectral image denoising with realistic data},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). How to train neural networks for flare removal.
<em>ICCV</em>, 2219–2227. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When a camera is pointed at a strong light source, the resulting photograph may contain lens flare artifacts. Flares appear in a wide variety of patterns (halos, streaks, color bleeding, haze, etc.) and this diversity in appearance makes flare removal challenging. Existing analytical solutions make strong assumptions about the artifact’s geometry or brightness, and therefore only work well on a small subset of flares. Machine learning techniques have shown success in removing other types of artifacts, like reflections, but have not been widely applied to flare removal due to the lack of training data. To solve this problem, we explicitly model the optical causes of flare either empirically or using wave optics, and generate semi-synthetic pairs of flare-corrupted and clean images. This enables us to train neural networks to remove lens flare for the first time. Experiments show our data synthesis approach is critical for accurate flare removal, and that models trained with our technique generalize well to real lens flares across different scenes, lighting conditions, and cameras.},
  archive   = {C_ICCV},
  author    = {Yicheng Wu and Qiurui He and Tianfan Xue and Rahul Garg and Jiawen Chen and Ashok Veeraraghavan and Jonathan T. Barron},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00224},
  pages     = {2219-2227},
  title     = {How to train neural networks for flare removal},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Defocus map estimation and deblurring from a single
dual-pixel image. <em>ICCV</em>, 2208–2218. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a method that takes as input a single dual-pixel image, and simultaneously estimates the image’s defocus map—the amount of defocus blur at each pixel—and recovers an all-in-focus image. Our method is inspired from recent works that leverage the dual-pixel sensors available in many consumer cameras to assist with autofocus, and use them for recovery of defocus maps or all-in-focus images. These prior works have solved the two recovery problems independently of each other, and often require large labeled datasets for supervised training. By contrast, we show that it is beneficial to treat these two closely-connected problems simultaneously. To this end, we set up an optimization problem that, by carefully modeling the optics of dual-pixel images, jointly solves both problems. We use data captured with a consumer smartphone camera to demonstrate that, after a one-time calibration step, our approach improves upon prior works for both defocus map estimation and blur removal, despite being entirely unsupervised.},
  archive   = {C_ICCV},
  author    = {Shumian Xin and Neal Wadhwa and Tianfan Xue and Jonathan T. Barron and Pratul P. Srinivasan and Jiawen Chen and Ioannis Gkioulekas and Rahul Garg},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00223},
  pages     = {2208-2218},
  title     = {Defocus map estimation and deblurring from a single dual-pixel image},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adversarial attack on deep cross-modal hamming retrieval.
<em>ICCV</em>, 2198–2207. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, Cross-Modal Hamming space Retrieval (CMHR) regains ever-increasing attention, mainly benefiting from the excellent representation capability of deep neural networks. On the other hand, the vulnerability of deep networks exposes a deep cross-modal retrieval system to various safety risks (e.g., adversarial attack). However, attacking deep cross-modal Hamming retrieval remains underexplored. In this paper, we propose an effective Adversarial Attack on Deep Cross-Modal Hamming Retrieval, dubbed AACH, which fools a target deep CMHR model in a black-box setting. Specifically, given a target model, we first construct its substitute model to exploit cross-modal correlations within hamming space, with which we create adversarial examples by limitedly querying from a target model. Furthermore, to enhance the efficiency of adversarial attacks, we design a triplet construction module to exploit cross-modal positive and negative instances. In this way, perturbations can be learned to fool the target model through pulling perturbed examples far away from the positive instances whereas pushing them close to the negative ones. Extensive experiments on three widely used cross-modal (image and text) retrieval benchmarks demonstrate the superiority of the proposed AACH. We find that AACH can successfully attack a given target deep CMHR model with fewer interactions, and that its performance is on par with previous state-of-the-art attacks.},
  archive   = {C_ICCV},
  author    = {Chao Li and Shangqian Gao and Cheng Deng and Wei Liu and Heng Huang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00222},
  pages     = {2198-2207},
  title     = {Adversarial attack on deep cross-modal hamming retrieval},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). COOKIE: Contrastive cross-modal knowledge sharing
pre-training for vision-language representation. <em>ICCV</em>,
2188–2197. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There has been a recent surge of interest in cross-modal pre-training. However, existed approaches pre-train a one-stream model to learn joint vision-language representation, which suffers from calculation explosion when conducting cross-modal retrieval. In this work, we propose the Contrastive Cross-Modal Knowledge Sharing Pretraining (COOKIE) method to learn universal text-image representations. There are two key designs in it, one is the weight-sharing transformer on top of the visual and textual encoders to align text and image semantically, the other is three kinds of contrastive learning designed for sharing knowledge between different modalities. Cross-modal knowledge sharing greatly promotes the learning of unimodal representation. Experiments on multi-modal matching tasks including cross-modal retrieval, text matching, and image retrieval show the effectiveness and efficiency of our pre-training framework. Our COOKIE finetuned on cross-modal datasets MSCOCO, Flickr30K, and MSRVTT achieves new state-of-the-art results while using only 3/1000 inference time comparing to one-stream models. There are also 5.7\% and 3.9\% improvements in the task of image retrieval and text matching. Source code will be available at https://github.com/kywen1119/COOKIE.},
  archive   = {C_ICCV},
  author    = {Keyu Wen and Jin Xia and Yuanyuan Huang and Linyang Li and Jiayan Xu and Jie Shao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00221},
  pages     = {2188-2197},
  title     = {COOKIE: Contrastive cross-modal knowledge sharing pre-training for vision-language representation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Auto-parsing network for image captioning and visual
question answering. <em>ICCV</em>, 2177–2187. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose an Auto-Parsing Network (APN) to discover and exploit the input data’s hidden tree structures for improving the effectiveness of the Transformer-based vision-language systems. Specifically, we impose a Probabilistic Graphical Model (PGM) parameterized by the attention operations on each self-attention layer to incorporate sparse assumption. We use this PGM to softly segment an input sequence into a few clusters where each cluster can be treated as the parent of the inside entities. By stacking these PGM constrained self-attention layers, the clusters in a lower layer compose into a new sequence, and the PGM in a higher layer will further segment this sequence. Iteratively, a sparse tree can be implicitly parsed, and this tree’s hierarchical knowledge is incorporated into the transformed embeddings, which can be used for solving the target vision-language tasks. Specifically, we showcase that our APN can strengthen Transformer based networks in two major vision-language tasks: Captioning and Visual Question Answering. Also, a PGM probability-based parsing algorithm is developed by which we can discover what the hidden structure of input is during the inference.},
  archive   = {C_ICCV},
  author    = {Xu Yang and Chongyang Gao and Hanwang Zhang and Jianfei Cai},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00220},
  pages     = {2177-2187},
  title     = {Auto-parsing network for image captioning and visual question answering},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Partial off-policy learning: Balance accuracy and diversity
for human-oriented image captioning. <em>ICCV</em>, 2167–2176. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human-oriented image captioning with both high diversity and accuracy is a challenging task in vision+language modeling. The reinforcement learning (RL) based frameworks promote the accuracy of image captioning, yet seriously hurt the diversity. In contrast, other methods based on variational auto-encoder (VAE) or generative adversarial network (GAN) can produce diverse yet less accurate captions. In this work, we devote our attention to promote the diversity of RL-based image captioning. To be specific, we devise a partial off-policy learning scheme to balance accuracy and diversity. First, we keep the model exposed to varied candidate captions by sampling from the initial state before RL launched. Second, a novel criterion named max-CIDEr is proposed to serve as the reward for promoting diversity. We combine the above-mentioned offpolicy strategy with the on-policy one to moderate the exploration effect, further balancing the diversity and accuracy for human-like image captioning. Experiments show that our method locates the closest to human performance in the diversity-accuracy space, and achieves the highest Pearson correlation as 0.337 with human performance.},
  archive   = {C_ICCV},
  author    = {Jiahe Shi and Yali Li and Shengjin Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00219},
  pages     = {2167-2176},
  title     = {Partial off-policy learning: Balance accuracy and diversity for human-oriented image captioning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hierarchical graph attention network for few-shot
visual-semantic learning. <em>ICCV</em>, 2157–2166. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep learning has made tremendous success in computer vision, natural language processing and even visual-semantic learning, which requires a huge amount of labeled training data. Nevertheless, the goal of human-level intelligence is to enable a model to quickly obtain an in-depth understanding given a small number of samples, especially with heterogeneity in the multi-modal scenarios such as visual question answering and image captioning. In this paper, we study the few-shot visual-semantic learning and present the Hierarchical Graph ATtention network (HGAT). This two-stage network models the intra- and inter-modal relationships with limited image-text samples. The main contributions of HGAT can be summarized as follows: 1) it sheds light on tackling few-shot multi-modal learning problems, which focuses primarily, but not exclusively on visual and semantic modalities, through better exploitation of the intra-relationship of each modality and an attention-based co-learning framework between modalities using a hierarchical graph-based architecture; 2) it achieves superior performance on both visual question answering and image captioning in the few-shot setting; 3) it can be easily extended to the semi-supervised setting where image-text samples are partially unlabeled. We show via extensive experiments that HGAT delivers state-of-the-art performance on three widely-used benchmarks of two visual-semantic learning tasks.},
  archive   = {C_ICCV},
  author    = {Chengxiang Yin and Kun Wu and Zhengping Che and Bo Jiang and Zhiyuan Xu and Jian Tang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00218},
  pages     = {2157-2166},
  title     = {Hierarchical graph attention network for few-shot visual-semantic learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LocTex: Learning data-efficient visual representations from
localized textual supervision. <em>ICCV</em>, 2147–2156. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Computer vision tasks such as object detection and semantic/instance segmentation rely on the painstaking annotation of large training datasets. In this paper, we propose LocTex that takes advantage of the low-cost localized textual annotations (i.e., captions and synchronized mouse-over gestures) to reduce the annotation effort. We introduce a contrastive pre-training framework between images and captions, and propose to supervise the cross-modal attention map with rendered mouse traces to provide coarse localization signals. Our learned visual features capture rich semantics (from free-form captions) and accurate localization (from mouse traces), which are very effective when transferred to various downstream vision tasks. Compared with ImageNet supervised pre-training, LocTex can reduce the size of the pre-training dataset by 10× or the target dataset by 2× while achieving comparable or even improved performance on COCO instance segmentation. When provided with the same amount of annotations, LocTex achieves around 4\% higher accuracy than the previous state-of-the-art &quot;vision+language&quot; pre-training approach on the task of PASCAL VOC image classification.},
  archive   = {C_ICCV},
  author    = {Zhijian Liu and Simon Stent and Jie Li and John Gideon and Song Han},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00217},
  pages     = {2147-2156},
  title     = {LocTex: Learning data-efficient visual representations from localized textual supervision},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Patch craft: Video denoising by deep modeling and patch
matching. <em>ICCV</em>, 2137–2146. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The non-local self-similarity property of natural images has been exploited extensively for solving various image processing problems. When it comes to video sequences, harnessing this force is even more beneficial due to the temporal redundancy. In the context of image and video denoising, many classically-oriented algorithms employ self-similarity, splitting the data into overlapping patches, gathering groups of similar ones and processing these together somehow. With the emergence of convolutional neural networks (CNN), the patch-based framework has been abandoned. Most CNN denoisers operate on the whole image, leveraging non-local relations only implicitly by using a large receptive field. This work proposes a novel approach for leveraging self-similarity in the context of video denoising, while still relying on a regular convolutional architecture. We introduce a concept of patch-craft frames – artificial frames that are similar to the real ones, built by tiling matched patches. Our algorithm augments video sequences with patch-craft frames and feeds them to a CNN. We demonstrate the substantial boost in denoising performance obtained with the proposed approach.},
  archive   = {C_ICCV},
  author    = {Gregory Vaksman and Michael Elad and Peyman Milanfar},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00216},
  pages     = {2137-2146},
  title     = {Patch craft: Video denoising by deep modeling and patch matching},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). N-ImageNet: Towards robust, fine-grained object recognition
with event cameras. <em>ICCV</em>, 2126–2136. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce N-ImageNet, a large-scale dataset targeted for robust, fine-grained object recognition with event cameras. The dataset is collected using programmable hardware in which an event camera consistently moves around a monitor displaying images from ImageNet. N-ImageNet serves as a challenging benchmark for event-based object recognition, due to its large number of classes and samples. We empirically show that pretraining on N-ImageNet improves the performance of event-based classifiers and helps them learn with few labeled data. In addition, we present several variants of N-ImageNet to test the robustness of event-based classifiers under diverse camera trajectories and severe lighting conditions, and propose a novel event representation to alleviate the performance degradation. To the best of our knowledge, we are the first to quantitatively investigate the consequences caused by various environmental conditions on event-based object recognition algorithms. N-ImageNet and its variants are expected to guide practical implementations for deploying event-based object recognition algorithms in the real world.},
  archive   = {C_ICCV},
  author    = {Junho Kim and Jaehyeok Bae and Gangin Park and Dongsu Zhang and Young Min Kim},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00215},
  pages     = {2126-2136},
  title     = {N-ImageNet: Towards robust, fine-grained object recognition with event cameras},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dual transfer learning for event-based end-task prediction
via pluggable event to image translation. <em>ICCV</em>, 2115–2125. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Event cameras are novel sensors that perceive the perpixel intensity changes and output asynchronous event streams with high dynamic range and less motion blur. It has been shown that events alone can be used for end-task learning, e.g., semantic segmentation, based on encoder-decoder-like networks. However, as events are sparse and mostly reflect edge information, it is difficult to recover original details merely relying on the decoder. Moreover, most methods resort to the pixel-wise loss alone for supervision, which might be insufficient to fully exploit the visual details from sparse events, thus leading to less optimal performance. In this paper, we propose a simple yet flexible two-stream framework named Dual Transfer Learning (DTL) to effectively enhance the performance on the end-tasks without adding extra inference cost. The proposed approach consists of three parts: event to end-task learning (EEL) branch, event to image translation (EIT) branch, and transfer learning (TL) module that simultaneously explores the feature-level affinity information and pixel-level knowledge from the EIT branch to improve the EEL branch. This simple yet novel method leads to strong representation learning from events and is evidenced by the significant performance boost on the end-tasks such as semantic segmentation and depth estimation.},
  archive   = {C_ICCV},
  author    = {Lin Wang and Yujeong Chae and Kuk-Jin Yoon},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00214},
  pages     = {2115-2125},
  title     = {Dual transfer learning for event-based end-task prediction via pluggable event to image translation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image retrieval on real-life images with pre-trained
vision-and-language models. <em>ICCV</em>, 2105–2114. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We extend the task of composed image retrieval, where an input query consists of an image and short textual description of how to modify the image. Existing methods have only been applied to non-complex images within narrow domains, such as fashion products, thereby limiting the scope of study on in-depth visual reasoning in rich image and language contexts. To address this issue, we collect the Compose Image Retrieval on Real-life images (CIRR) dataset, which consists of over 36,000 pairs of crowd-sourced, open-domain images with human-generated modifying text. To extend current methods to the open-domain, we propose CIRPLANT, a transformer based model that leverages rich pre-trained vision-and-language (V&amp;L) knowledge for modifying visual features conditioned on natural language. Retrieval is then done by nearest neighbor lookup on the modified features. We demonstrate that with a relatively simple architecture, CIRPLANT outperforms existing methods on open-domain images, while matching state-of-the-art accuracy on the existing narrow datasets, such as fashion. Together with the release of CIRR, we believe this work will inspire further research on composed image retrieval. Our dataset, code and pre-trained models are available at https://cuberick-orion.github.io/CIRR/.},
  archive   = {C_ICCV},
  author    = {Zheyuan Liu and Cristian Rodriguez-Opazo and Damien Teney and Stephen Gould},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00213},
  pages     = {2105-2114},
  title     = {Image retrieval on real-life images with pre-trained vision-and-language models},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Language-guided global image editing via cross-modal cyclic
mechanism. <em>ICCV</em>, 2095–2104. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Editing an image automatically via a linguistic request can significantly save laborious manual work and is friendly to photography novice. In this paper, we focus on the task of language-guided global image editing. Existing works suffer from imbalanced and insufficient data distribution of real-world datasets and thus fail to understand language requests well. To handle this issue, we propose to create a cycle with our image generator by creating a novel model called Editing Description Network (EDNet) which predicts an editing embedding given a pair of images. Given the cycle, we propose several free augmentation strategies to help our model understand various editing requests given the imbalanced dataset. In addition, two other novel ideas are proposed: an Image-Request Attention (IRA) module which allows our method to edit an image spatial-adaptively when the image requires different editing degree at different regions, as well as a new evaluation metric for this task which is more semantic and reasonable than conventional pixel losses (e.g. L1). Extensive experiments on two benchmark datasets demonstrate the effectiveness of our method over existing approaches.},
  archive   = {C_ICCV},
  author    = {Wentao Jiang and Ning Xu and Jiayun Wang and Chen Gao and Jing Shi and Zhe Lin and Si Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00212},
  pages     = {2095-2104},
  title     = {Language-guided global image editing via cross-modal cyclic mechanism},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Motion-focused contrastive learning of video
representations. <em>ICCV</em>, 2085–2094. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion, as the most distinct phenomenon in a video to involve the changes over time, has been unique and critical to the development of video representation learning. In this paper, we ask the question: how important is the motion particularly for self-supervised video representation learning. To this end, we compose a duet of exploiting the motion for data augmentation and feature learning in the regime of contrastive learning. Specifically, we present a Motion-focused Contrastive Learning (MCL) method that regards such duet as the foundation. On one hand, MCL capitalizes on optical flow of each frame in a video to temporally and spatially sample the tubelets (i.e., sequences of associated frame patches across time) as data augmentations. On the other hand, MCL further aligns gradient maps of the convolutional layers to optical flow maps from spatial, temporal and spatio-temporal perspectives, in order to ground motion information in feature learning. Extensive experiments conducted on R(2+1)D backbone demonstrate the effectiveness of our MCL. On UCF101, the linear classifier trained on the representations learnt by MCL achieves 81.91\% top-1 accuracy, outperforming ImageNet supervised pre-training by 6.78\%. On Kinetics-400, MCL achieves 66.62\% top-1 accuracy under the linear protocol.},
  archive   = {C_ICCV},
  author    = {Rui Li and Yiheng Zhang and Zhaofan Qiu and Ting Yao and Dong Liu and Tao Mei},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00211},
  pages     = {2085-2094},
  title     = {Motion-focused contrastive learning of video representations},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Viewpoint-agnostic change captioning with cycle consistency.
<em>ICCV</em>, 2075–2084. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Change captioning is the task of identifying the change and describing it with a concise caption. Despite recent advancements, filtering out insignificant changes still remains as a challenge. Namely, images from different camera perspectives can cause issues; a mere change in viewpoint should be disregarded while still capturing the actual changes. In order to tackle this problem, we present a new Viewpoint-Agnostic change captioning network with Cycle Consistency (VACC) that requires only one image each for the before and after scene, without depending on any other information. We achieve this by devising a new difference encoder module which can encode viewpoint information and model the difference more effectively. In addition, we propose a cycle consistency module that can potentially improve the performance of any change captioning networks in general by matching the composite feature of the generated caption and before image with the after image feature. We evaluate the performance of our proposed model across three datasets for change captioning, including a novel dataset we introduce here that contains images with changes under extreme viewpoint shifts. Through our experiments, we show the excellence of our method with respect to the CIDEr, BLEU-4, METEOR and SPICE scores. Moreover, we demonstrate that attaching our proposed cycle consistency module yields a performance boost for existing change captioning networks, even with varying image encoding mechanisms.},
  archive   = {C_ICCV},
  author    = {Hoeseong Kim and Jongseok Kim and Hyungseok Lee and Hyunsung Park and Gunhee Kim},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00210},
  pages     = {2075-2084},
  title     = {Viewpoint-agnostic change captioning with cycle consistency},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). StyleCLIP: Text-driven manipulation of StyleGAN imagery.
<em>ICCV</em>, 2065–2074. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inspired by the ability of StyleGAN to generate highly realistic images in a variety of domains, much recent work has focused on understanding how to use the latent spaces of StyleGAN to manipulate generated and real images. However, discovering semantically meaningful latent manipulations typically involves painstaking human examination of the many degrees of freedom, or an annotated collection of images for each desired manipulation. In this work, we explore leveraging the power of recently introduced Contrastive Language-Image Pre-training (CLIP) models in order to develop a text-based interface for StyleGAN image manipulation that does not require such manual effort. We first introduce an optimization scheme that utilizes a CLIP-based loss to modify an input latent vector in response to a user-provided text prompt. Next, we describe a latent mapper that infers a text-guided latent manipulation step for a given input image, allowing faster and more stable text-based manipulation. Finally, we present a method for mapping text prompts to input-agnostic directions in StyleGAN’s style space, enabling interactive text-driven image manipulation. Extensive results and comparisons demonstrate the effectiveness of our approaches.},
  archive   = {C_ICCV},
  author    = {Or Patashnik and Zongze Wu and Eli Shechtman and Daniel Cohen-Or and Dani Lischinski},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00209},
  pages     = {2065-2074},
  title     = {StyleCLIP: Text-driven manipulation of StyleGAN imagery},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TRAR: Routing the attention spans in transformer for visual
question answering. <em>ICCV</em>, 2054–2064. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Due to the superior ability of global dependency modeling, Transformer and its variants have become the primary choice of many vision-and-language tasks. However, in tasks like Visual Question Answering (VQA) and Referring Expression Comprehension (REC), the multimodal prediction often requires visual information from macro- to micro-views. Therefore, how to dynamically schedule the global and local dependency modeling in Transformer has become an emerging issue. In this paper, we propose an example-dependent routing scheme called TRAnsformer Routing (TRAR) to address this issue 1 . Specifically, in TRAR, each visual Transformer layer is equipped with a routing module with different attention spans. The model can dynamically select the corresponding attentions based on the output of the previous inference step, so as to formulate the optimal routing path for each example. Notably, with careful designs, TRAR can reduce the additional computation and memory overhead to almost negligible. To validate TRAR, we conduct extensive experiments on five benchmark datasets of VQA and REC, and achieve superior performance gains than the standard Transformers and a bunch of state-of-the-art methods.},
  archive   = {C_ICCV},
  author    = {Yiyi Zhou and Tianhe Ren and Chaoyang Zhu and Xiaoshuai Sun and Jianzhuang Liu and Xinghao Ding and Mingliang Xu and Rongrong Ji},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00208},
  pages     = {2054-2064},
  title     = {TRAR: Routing the attention spans in transformer for visual question answering},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the hidden treasure of dialog in video question
answering. <em>ICCV</em>, 2044–2053. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {High-level understanding of stories in video such as movies and TV shows from raw data is extremely challenging. Modern video question answering (VideoQA) systems often use additional human-made sources like plot synopses, scripts, video descriptions or knowledge bases. In this work, we present a new approach to understand the whole story without such external sources. The secret lies in the dialog: unlike any prior work, we treat dialog as a noisy source to be converted into text description via dialog summarization, much like recent methods treat video. The input of each modality is encoded by transformers independently, and a simple fusion method combines all modalities, using soft temporal attention for localization over long inputs. Our model outperforms the state of the art on the KnowIT VQA dataset by a large margin, without using question-specific human annotation or human-made plot summaries. It even outperforms human evaluators who have never watched any whole episode before. Code is available at https://engindeniz.github.io/dialogsummary-videoqa},
  archive   = {C_ICCV},
  author    = {Deniz Engin and François Schnitzler and Ngoc Q. K. Duong and Yannis Avrithis},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00207},
  pages     = {2044-2053},
  title     = {On the hidden treasure of dialog in video question answering},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AESOP: Abstract encoding of stories, objects, and pictures.
<em>ICCV</em>, 2032–2043. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual storytelling and story comprehension are uniquely human skills that play a central role in how we learn about and experience the world. Despite remarkable progress in recent years in synthesis of visual and textual content in isolation and learning effective joint visual-linguistic representations, existing systems still operate only at a superficial, factual level. With the goal of developing systems that are able to comprehend rich human-generated narratives, and co-create new stories, we introduce AESOP: a new dataset that captures the creative process associated with visual storytelling. Visual panels are composed of clip-art objects with specific attributes enabling a broad range of creative expression. Using AESOP, we propose foundational storytelling tasks that are generative variants of story cloze tests, to better measure the creative and causal reasoning ability required for visual storytelling. We further develop a generalized story completion framework that models stories as the co-evolution of visual and textual concepts. We benchmark the proposed approach with human baselines and evaluate using comprehensive qualitative and quantitative metrics. Our results highlight key insights related to the dataset, modelling and evaluation of visual storytelling for future research in this promising field of study.},
  archive   = {C_ICCV},
  author    = {Hareesh Ravi and Kushal Kafle and Scott Cohen and Jonathan Brandt and Mubbasir Kapadia},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00206},
  pages     = {2032-2043},
  title     = {AESOP: Abstract encoding of stories, objects, and pictures},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adversarial VQA: A new benchmark for evaluating the
robustness of VQA models. <em>ICCV</em>, 2022–2031. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Benefiting from large-scale pre-training, we have witnessed significant performance boost on the popular Visual Question Answering (VQA) task. Despite rapid progress, it remains unclear whether these state-of-the-art (SOTA) models are robust when encountering examples in the wild. To study this, we introduce Adversarial VQA, a new large-scale VQA benchmark, collected iteratively via an adversarial human-and-model-in-the-loop procedure. Through this new benchmark, we discover several interesting findings. (i) Surprisingly, we find that during dataset collection, non-expert annotators can easily attack SOTA VQA models successfully. (ii) Both large-scale pre-trained models and adversarial training methods achieve far worse performance on the new benchmark than over standard VQA v2 dataset, revealing the fragility of these models while demonstrating the effectiveness of our adversarial dataset. (iii) When used for data augmentation, our dataset can effectively boost model performance on other robust VQA benchmarks. We hope our Adversarial VQA dataset can shed new light on robustness study in the community and serve as a valuable benchmark for future work.},
  archive   = {C_ICCV},
  author    = {Linjie Li and Jie Lei and Zhe Gan and Jingjing Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00205},
  pages     = {2022-2031},
  title     = {Adversarial VQA: A new benchmark for evaluating the robustness of VQA models},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pano-AVQA: Grounded audio-visual question answering on 360°
videos. <em>ICCV</em>, 2011–2021. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {360° videos convey holistic views for the surroundings of a scene. It provides audio-visual cues beyond predetermined normal field of views and displays distinctive spatial relations on a sphere. However, previous benchmark tasks for panoramic videos are still limited to evaluate the semantic understanding of audio-visual relationships or spherical spatial property in surroundings. We propose a novel benchmark named Pano-AVQA as a large-scale grounded audio-visual question answering dataset on panoramic videos. Using 5.4K 360° video clips harvested online, we collect two types of novel question-answer pairs with bounding-box grounding: spherical spatial relation QAs and audio-visual relation QAs. We train several transformer-based models from Pano-AVQA, where the results suggest that our proposed spherical spatial embeddings and multimodal training objectives fairly contribute to a better semantic understanding of the panoramic surroundings on the dataset.},
  archive   = {C_ICCV},
  author    = {Heeseung Yun and Youngjae Yu and Wonsuk Yang and Kangil Lee and Gunhee Kim},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00204},
  pages     = {2011-2021},
  title     = {Pano-AVQA: Grounded audio-visual question answering on 360° videos},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Explainable video entailment with grounded visual evidence.
<em>ICCV</em>, 2001–2010. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Video entailment aims at determining if a hypothesis textual statement is entailed or contradicted by a premise video. The main challenge of video entailment is that it requires fine-grained reasoning to understand the complex and long story-based videos. To this end, we propose to incorporate visual grounding to the entailment by explicitly linking the entities described in the statement to the evidence in the video. If the entities are grounded in the video, we enhance the entailment judgment by focusing on the frames where the entities occur. Besides, in the entailment dataset, the entailed/contradictory (also named as real/fake) statements are formed in pairs with subtle discrepancy, which allows an add-on explanation module to predict which words or phrases make the statement contradictory to the video and regularize the training of the entailment judgment. Experimental results demonstrate that our approach outperforms the state-of-the-art methods.},
  archive   = {C_ICCV},
  author    = {Junwen Chen and Yu Kong Golisano},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00203},
  pages     = {2001-2010},
  title     = {Explainable video entailment with grounded visual evidence},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Let’s see clearly: Contaminant artifact removal for moving
cameras. <em>ICCV</em>, 1991–2000. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Contaminants such as dust, dirt and moisture adhering to the camera lens can greatly affect the quality and clarity of the resulting image or video. In this paper, we propose a video restoration method to automatically remove these contaminants and produce a clean video. Our approach first seeks to detect attention maps that indicate the regions that need to be restored. In order to leverage the corresponding clean pixels from adjacent frames, we propose a flow completion module to hallucinate the flow of the background scene to the attention regions degraded by the contaminants. Guided by the attention maps and completed flows, we propose a recurrent technique to restore the input frame by fetching clean pixels from adjacent frames. Finally, a multi-frame processing stage is used to further process the entire video sequence in order to enforce temporal consistency. The entire network is trained on a synthetic dataset that approximates the physical lighting properties of contaminant artifacts. This new dataset and our novel framework lead to our method that is able to address different contaminants and outperforms competitive restoration approaches both qualitatively and quantitatively.},
  archive   = {C_ICCV},
  author    = {Xiaoyu Li and Bo Zhang and Jing Liao and Pedro V. Sander},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00202},
  pages     = {1991-2000},
  title     = {Let’s see clearly: Contaminant artifact removal for moving cameras},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Dual-camera super-resolution with aligned attention
modules. <em>ICCV</em>, 1981–1990. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel approach to reference-based super-resolution (RefSR) with the focus on dual-camera super-resolution (DCSR), which utilizes reference images for high-quality and high-fidelity results. Our proposed method generalizes the standard patch-based feature matching with spatial alignment operations. We further explore the dual-camera super-resolution that is one promising application of RefSR, and build a dataset that consists of 146 image pairs from the main and telephoto cameras in a smartphone. To bridge the domain gaps between real-world images and the training images, we propose a self-supervised domain adaptation strategy for real-world images. Extensive experiments on our dataset and a public benchmark demonstrate clear improvement achieved by our method over state of the art in both quantitative evaluation and visual comparisons. Our code and data are available at https://tengfei-wang.github.io/Dual-Camera-SR/index.html.},
  archive   = {C_ICCV},
  author    = {Tengfei Wang and Jiaxin Xie and Wenxiu Sun and Qiong Yan and Qifeng Chen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00201},
  pages     = {1981-1990},
  title     = {Dual-camera super-resolution with aligned attention modules},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). IICNet: A generic framework for reversible image conversion.
<em>ICCV</em>, 1971–1980. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reversible image conversion (RIC) aims to build a reversible transformation between specific visual content (e.g., short videos) and an embedding image, where the original content can be restored from the embedding when necessary. This work develops Invertible Image Conversion Net (IIC-Net) as a generic solution to various RIC tasks due to its strong capacity and task-independent design. Unlike previous encoder-decoder based methods, IICNet maintains a highly invertible structure based on invertible neural networks (INNs) to better preserve the information during conversion. We use a relation module and a channel squeeze layer to improve the INN nonlinearity to extract cross-image relations and the network flexibility, respectively. Experimental results demonstrate that IICNet outperforms the specifically-designed methods on existing RIC tasks and can generalize well to various newly-explored tasks. With our generic IICNet, we no longer need to hand-engineer task-specific embedding networks for rapidly occurring visual content. Our source codes are available at: https://github.com/felixcheng97/IICNet.},
  archive   = {C_ICCV},
  author    = {Ka Leong Cheng and Yueqi Xie and Qifeng Chen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00200},
  pages     = {1971-1980},
  title     = {IICNet: A generic framework for reversible image conversion},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cross-camera convolutional color constancy. <em>ICCV</em>,
1961–1970. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present &quot;Cross-Camera Convolutional Color Constancy&quot; (C5), a learning-based method, trained on images from multiple cameras, that accurately estimates a scene’s illuminant color from raw images captured by a new camera previously unseen during training. C5 is a hypernetwork-like extension of the convolutional color constancy (CCC) approach: C5 learns to generate the weights of a CCC model that is then evaluated on the input image, with the CCC weights dynamically adapted to different input content. Unlike prior cross-camera color constancy models, which are usually designed to be agnostic to the spectral properties of test-set images from unobserved cameras, C5 approaches this problem through the lens of transductive inference: additional unlabeled images are provided as input to the model at test time, which allows the model to calibrate itself to the spectral properties of the test-set camera during inference. C5 achieves state-of-the-art accuracy for cross-camera color constancy on several datasets, is fast to evaluate (~7 and ~90 ms per image on a GPU or CPU, respectively), and requires little memory (~2 MB), and thus is a practical solution to the problem of calibration-free automatic white balance for mobile photography.},
  archive   = {C_ICCV},
  author    = {Mahmoud Afifi and Jonathan T. Barron and Chloe LeGendre and Yun-Ta Tsai and Francois Bleibel},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00199},
  pages     = {1961-1970},
  title     = {Cross-camera convolutional color constancy},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Describing and localizing multiple changes with
transformers. <em>ICCV</em>, 1951–1960. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Change captioning tasks aim to detect changes in image pairs observed before and after a scene change and generate a natural language description of the changes. Existing change captioning studies have mainly focused on a single change. However, detecting and describing multiple changed parts in image pairs is essential for enhancing adaptability to complex scenarios. We solve the above issues from three aspects: (i) We propose a simulation-based multi-change captioning dataset; (ii) We benchmark existing state-of-the-art methods of single change captioning on multi-change captioning; (iii) We further propose Multi-Change Captioning transformers (MCCFormers) that identify change regions by densely correlating different regions in image pairs and dynamically determines the related change regions with words in sentences. The proposed method obtained the highest scores on four conventional change captioning evaluation metrics for multi-change captioning. Additionally, our proposed method can separate attention maps for each change and performs well with respect to change localization. Moreover, the proposed framework outperformed the previous state-of-the-art methods on an existing change captioning benchmark, CLEVR-Change, by a large margin (+6.1 on BLEU-4 and +9.7 on CIDEr scores), indicating its general ability in change captioning tasks. The code and dataset are available at the project page 1 .},
  archive   = {C_ICCV},
  author    = {Yue Qiu and Shintaro Yamamoto and Kodai Nakashima and Ryota Suzuki and Kenji Iwata and Hirokatsu Kataoka and Yutaka Satoh},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00198},
  pages     = {1951-1960},
  title     = {Describing and localizing multiple changes with transformers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). IntraTomo: Self-supervised learning-based tomography via
sinogram synthesis and prediction. <em>ICCV</em>, 1940–1950. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose IntraTomo, a powerful framework that combines the benefits of learning-based and model-based approaches for solving highly ill-posed inverse problems in the Computed Tomography (CT) context. IntraTomo is composed of two core modules: a novel sinogram prediction module, and a geometry refinement module, which are applied iteratively. In the first module, the unknown density field is represented as a continuous and differentiable function, parameterized by a deep neural network. This network is learned, in a self-supervised fashion, from the incomplete or/and degraded input sinogram. After getting estimated through the sinogram prediction module, the density field is consistently refined in the second module using local and non-local geometrical priors. With these two core modules, we show that IntraTomo significantly outperforms existing approaches on several ill-posed inverse problems, such as limited angle tomography with a range of 45 degrees, sparse view tomographic reconstruction with as few as eight views, or super-resolution tomography with eight times increased resolution. The experiments on simulated and real data show that our approach can achieve results of unprecedented quality.},
  archive   = {C_ICCV},
  author    = {Guangming Zang and Ramzi Idoughi and Rui Li and Peter Wonka and Wolfgang Heidrich},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00197},
  pages     = {1940-1950},
  title     = {IntraTomo: Self-supervised learning-based tomography via sinogram synthesis and prediction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). T-net: Effective permutation-equivariant network for
two-view correspondence learning. <em>ICCV</em>, 1930–1939. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We develop a conceptually simple, flexible, and effective framework (named T-Net) for two-view correspondence learning. Given a set of putative correspondences, we reject outliers and regress the relative pose encoded by the essential matrix, by an end-to-end framework, which is consisted of two novel structures: &quot;−&quot; structure and &quot;|&quot; structure. &quot; − &quot; structure adopts an iterative strategy to learn correspondence features. &quot;|&quot; structure integrates all the features of the iterations and outputs the correspondence weight. In addition, we introduce Permutation-Equivariant Context Squeeze-and-Excitation module, an adapted version of SE module, to process sparse correspondences in a permutation-equivariant way and capture both global and channel-wise contextual information. Extensive experiments on outdoor and indoor scenes show that the proposed T-Net achieves state-of-the-art performance. On outdoor scenes (YFCC100M dataset), T-Net achieves an mAP of 52.28\%, a 34.22\% precision increase from the best-published result (38.95\%). On indoor scenes (SUN3D dataset), T-Net (19.71\%) obtains a 21.82\% precision increase from the best-published result (16.18\%). Source code: https://github.com/x-gb/T-Net.},
  archive   = {C_ICCV},
  author    = {Zhen Zhong and Guobao Xiao and Linxin Zheng and Yan Lu and Jiayi Ma},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00196},
  pages     = {1930-1939},
  title     = {T-net: Effective permutation-equivariant network for two-view correspondence learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Spatial-temporal consistency network for low-latency
trajectory forecasting. <em>ICCV</em>, 1920–1929. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trajectory forecasting is a crucial step for autonomous vehicles and mobile robots in order to navigate and interact safely. In order to handle the spatial interactions between objects, graph-based approaches have been proposed. These methods, however, model motion on a frame-to-frame basis and do not provide a strong temporal model. To overcome this limitation, we propose a compact model called Spatial-Temporal Consistency Network (STC-Net). In STC-Net, dilated temporal convolutions are introduced to model long-range dependencies along each trajectory for better temporal modeling while graph convolutions are employed to model the spatial interaction among different trajectories. Furthermore, we propose a feature-wise convolution to generate the predicted trajectories in one pass and refine the forecast trajectories together with the reconstructed observed trajectories. We demonstrate that STC-Net generates spatially and temporally consistent trajectories and outperforms other graph-based methods. Since STC-Net requires only 0.7k parameters and forecasts the future with a latency of only 1.3ms, it advances the state-of-the-art and satisfies the requirements for realistic applications.},
  archive   = {C_ICCV},
  author    = {Shijie Li and Yanying Zhou and Jinhui Yi and Juergen Gall},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00195},
  pages     = {1920-1929},
  title     = {Spatial-temporal consistency network for low-latency trajectory forecasting},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Localize to binauralize: Audio spatialization from visual
sound source localization. <em>ICCV</em>, 1910–1919. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Videos with binaural audios provide immersive viewing experience by enabling 3D sound sensation. Recent works attempt to generate binaural audio in a multimodal learning framework using large quantities of videos with accompanying binaural audio. In contrast, we attempt a more challenging problem – synthesizing binaural audios for a video with monaural audio in a weakly semi-supervised setting. Our key idea is that any down-stream task that can be solved only using binaural audios can be used to provide proxy supervision for binaural audio generation, thereby reducing the reliance on explicit supervision. In this work, as a proxy-task for weak supervision, we use Sound Source Localization with only audio. We design a two-stage architecture called Localize-to-Binauralize Network (L2BNet). The first stage of L2BNet is a Stereo Generation (SG) network employed to generate two-stream audio from monaural audio using visual frame information as guidance. In the second stage, an Audio Localization (AL) network is designed to use the synthesized two-stream audio to localize sound sources in visual frames. The entire network is trained end-to-end so that the AL network provides necessary supervision for the SG network. We experimentally show that our weakly-supervised framework generates two-stream audio containing binaural cues. Through user study, we further validate that our proposed approach generates binaural-quality audio using as little as 10\% of explicit binaural supervision data for the SG network.},
  archive   = {C_ICCV},
  author    = {Kranthi Kumar Rachavarapu and Aakanksha Aakanksha and Vignesh Sundaresha and Rajagopalan A. N},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00194},
  pages     = {1910-1919},
  title     = {Localize to binauralize: Audio spatialization from visual sound source localization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mixed SIGNals: Sign language production via a mixture of
motion primitives. <em>ICCV</em>, 1899–1909. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It is common practice to represent spoken languages at their phonetic level. However, for sign languages, this implies breaking motion into its constituent motion primitives. Avatar based Sign Language Production (SLP) has traditionally done just this, building up animation from sequences of hand motions, shapes and facial expressions. However, more recent deep learning based solutions to SLP have tackled the problem using a single network that estimates the full skeletal structure.We propose splitting the SLP task into two distinct jointly-trained sub-tasks. The first translation sub-task translates from spoken language to a latent sign language representation, with gloss supervision. Subsequently, the animation sub-task aims to produce expressive sign language sequences that closely resemble the learnt spatio-temporal representation. Using a progressive transformer for the translation sub-task, we propose a novel Mixture of Motion Primitives (MOMP) architecture for sign language animation. A set of distinct motion primitives are learnt during training, that can be temporally combined at inference to animate continuous sign language sequences.We evaluate on the challenging RWTH-PHOENIX-Weather-2014T (PHOENIX14T) dataset, presenting extensive ablation studies and showing that MOMP outperforms baselines in user evaluations. We achieve state-of-the-art back translation performance with an 11\% improvement over competing results. Importantly, and for the first time, we showcase stronger performance for a full translation pipeline going from spoken language to sign, than from gloss to sign.},
  archive   = {C_ICCV},
  author    = {Ben Saunders and Necati Cihan Camgoz and Richard Bowden},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00193},
  pages     = {1899-1909},
  title     = {Mixed SIGNals: Sign language production via a mixture of motion primitives},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weakly supervised relative spatial reasoning for visual
question answering. <em>ICCV</em>, 1888–1898. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vision-and-language (V&amp;L) reasoning necessitates perception of visual concepts such as objects and actions, understanding semantics and language grounding, and reasoning about the interplay between the two modalities. One crucial aspect of visual reasoning is spatial understanding, which involves understanding relative locations of objects, i.e. implicitly learning the geometry of the scene. In this work, we evaluate the faithfulness of V&amp;L models to such geometric understanding, by formulating the prediction of pair-wise relative locations of objects as a classification as well as a regression task. Our findings suggest that state-of-the-art transformer-based V&amp;L models lack sufficient abilities to excel at this task. Motivated by this, we design two objectives as proxies for 3D spatial reasoning (SR) – object centroid estimation, and relative position estimation, and train V&amp;L with weak supervision from off-the-shelf depth estimators. This leads to considerable improvements in accuracy for the &quot;GQA&quot; visual question answering challenge (in fully supervised, few-shot, and O.O.D settings) as well as improvements in relative spatial reasoning. Code and data will be released here.},
  archive   = {C_ICCV},
  author    = {Pratyay Banerjee and Tejas Gokhale and Yezhou Yang and Chitta Baral},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00192},
  pages     = {1888-1898},
  title     = {Weakly supervised relative spatial reasoning for visual question answering},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unified questioner transformer for descriptive question
generation in goal-oriented visual dialogue. <em>ICCV</em>, 1878–1887.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Building an interactive artificial intelligence that can ask questions about the real world is one of the biggest challenges for vision and language problems. In particular, goal-oriented visual dialogue, where the aim of the agent is to seek information by asking questions during a turn-taking dialogue, has been gaining scholarly attention recently. While several existing models based on the GuessWhat?! dataset [10] have been proposed, the Questioner typically asks simple category-based questions or absolute spatial questions. This might be problematic for complex scenes where the objects share attributes, or in cases where descriptive questions are required to distinguish objects. In this paper, we propose a novel Questioner architecture, called Unified Questioner Transformer (UniQer), for descriptive question generation with referring expressions. In addition, we build a goal-oriented visual dialogue task called CLEVR Ask. It synthesizes complex scenes that require the Questioner to generate descriptive questions. We train our model with two variants of CLEVR Ask datasets. The results of the quantitative and qualitative evaluations show that UniQer outperforms the baseline.},
  archive   = {C_ICCV},
  author    = {Shoya Matsumori and Kosuke Shingyouchi and Yuki Abe and Yosuke Fukuchi and Komei Sugiura and Michita Imai},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00191},
  pages     = {1878-1887},
  title     = {Unified questioner transformer for descriptive question generation in goal-oriented visual dialogue},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Factorizing perception and policy for interactive
instruction following. <em>ICCV</em>, 1868–1877. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Performing simple household tasks based on language directives is very natural to humans, yet it remains an open challenge for AI agents. The ‘interactive instruction following’ task attempts to make progress towards building agents that jointly navigate, interact, and reason in the environment at every step. To address the multifaceted problem, we propose a model that factorizes the task into interactive perception and action policy streams with enhanced components and name it as MOCA, a Modular Object-Centric Approach. We empirically validate that MOCA outperforms prior arts by significant margins on the ALFRED benchmark with improved generalization.},
  archive   = {C_ICCV},
  author    = {Kunal Pratap Singh and Suvaansh Bhambri and Byeonghwi Kim and Roozbeh Mottaghi and Jonghyun Choi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00190},
  pages     = {1868-1877},
  title     = {Factorizing perception and policy for interactive instruction following},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interpretable visual reasoning via induced symbolic space.
<em>ICCV</em>, 1858–1867. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of concept induction in visual reasoning, i.e., identifying concepts and their hierarchical relationships from question-answer pairs associated with images; and achieve an interpretable model via working on the induced symbolic concept space. To this end, we first design a new framework named object-centric compositional attention model (OCCAM) to perform the visual reasoning task with object-level visual features. Then, we come up with a method to induce concepts of objects and relations using clues from the attention patterns between objects’ visual features and question words. Finally, we achieve a higher level of interpretability by imposing OCCAM on the objects represented in the induced symbolic concept space. Experiments on the CLEVR and GQA datasets demonstrate: 1) our OCCAM achieves a new state of the art without human-annotated functional programs; 2) our induced concepts are both accurate and sufficient as OCCAM achieves an on-par performance on objects represented either in visual features or in the induced symbolic concept space.},
  archive   = {C_ICCV},
  author    = {Zhonghao Wang and Kai Wang and Mo Yu and Jinjun Xiong and Wen-Mei Hwu and Mark Hasegawa-Johnson and Humphrey Shi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00189},
  pages     = {1858-1867},
  title     = {Interpretable visual reasoning via induced symbolic space},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Adaptive hierarchical graph reasoning with semantic
coherence for video-and-language inference. <em>ICCV</em>, 1847–1857.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Video-and-Language Inference is a recently proposed task for joint video-and-language understanding. This new task requires a model to draw inference on whether a natural language statement entails or contradicts a given video clip. In this paper, we study how to address three critical challenges for this task: judging the global correctness of the statement involved multiple semantic meanings, joint reasoning over video and subtitles, and modeling long-range relationships and complex social interactions. First, we propose an adaptive hierarchical graph network that achieves in-depth understanding of the video over complex interactions. Specifically, it performs joint reasoning over video and subtitles in three hierarchies, where the graph structure is adaptively adjusted according to the semantic structures of the statement. Secondly, we introduce semantic coherence learning to explicitly encourage the semantic coherence of the adaptive hierarchical graph network from three hierarchies. The semantic coherence learning can further improve the alignment between vision and linguistics, and the coherence across a sequence of video segments. Experimental results show that our method significantly outperforms the baseline by a large margin.},
  archive   = {C_ICCV},
  author    = {Juncheng Li and Siliang Tang and Linchao Zhu and Haochen Shi and Xuanwen Huang and Fei Wu and Yi Yang and Yueting Zhuang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00188},
  pages     = {1847-1857},
  title     = {Adaptive hierarchical graph reasoning with semantic coherence for video-and-language inference},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SAT: 2D semantics assisted training for 3D visual grounding.
<em>ICCV</em>, 1836–1846. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D visual grounding aims at grounding a natural language description about a 3D scene, usually represented in the form of 3D point clouds, to the targeted object region. Point clouds are sparse, noisy, and contain limited semantic information compared with 2D images. These inherent limitations make the 3D visual grounding problem more challenging. In this study, we propose 2D Semantics Assisted Training (SAT) that utilizes 2D image semantics in the training stage to ease point-cloud-language joint representation learning and assist 3D visual grounding. The main idea is to learn auxiliary alignments between rich, clean 2D object representations and the corresponding objects or mentioned entities in 3D scenes. SAT takes 2D object semantics, i.e., object label, image feature, and 2D geometric feature, as the extra input in training but does not require such inputs during inference. By effectively utilizing 2D semantics in training, our approach boosts the accuracy on the Nr3D dataset from 37.7\% to 49.2\%, which significantly surpasses the non-SAT baseline with the identical network architecture and inference input. Our approach outperforms the state of the art by large margins on multiple 3D visual grounding datasets, i.e., +10.4\% absolute accuracy on Nr3D, +9.9\% on Sr3D, and +5.6\% on ScanRef.},
  archive   = {C_ICCV},
  author    = {Zhengyuan Yang and Songyang Zhang and Liwei Wang and Jiebo Luo},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00187},
  pages     = {1836-1846},
  title     = {SAT: 2D semantics assisted training for 3D visual grounding},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Weakly supervised human-object interaction detection in
video via contrastive spatiotemporal regions. <em>ICCV</em>, 1825–1835.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce the task of weakly supervised learning for detecting human and object interactions in videos. Our task poses unique challenges as a system does not know what types of human-object interactions are present in a video or the actual spatiotemporal location of the human and the object. To address these challenges, we introduce a contrastive weakly supervised training loss that aims to jointly associate spatiotemporal regions in a video with an action and object vocabulary and encourage temporal continuity of the visual appearance of moving objects as a form of self-supervision. To train our model, we introduce a dataset comprising over 6.5k videos with human-object interaction annotations that have been semi-automatically curated from sentence captions associated with the videos. We demonstrate improved performance over weakly supervised baselines adapted to our task on our video dataset.},
  archive   = {C_ICCV},
  author    = {Shuang Li and Yilun Du and Antonio Torralba and Josef Sivic and Bryan Russell},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00186},
  pages     = {1825-1835},
  title     = {Weakly supervised human-object interaction detection in video via contrastive spatiotemporal regions},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ask&amp;confirm: Active detail enriching for cross-modal
retrieval with partial query. <em>ICCV</em>, 1815–1824. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Text-based image retrieval has seen considerable progress in recent years. However, the performance of existing methods suffers in real life since the user is likely to provide an incomplete description of an image, which often leads to results filled with false positives that fit the incomplete description. In this work, we introduce the partial-query problem and extensively analyze its influence on text-based image retrieval. Previous interactive methods tackle the problem by passively receiving users’ feedback to supplement the incomplete query iteratively, which is time-consuming and requires heavy user effort. Instead, we propose a novel retrieval framework that conducts the interactive process in an Ask-and-Confirm fashion, where AI actively searches for discriminative details missing in the current query, and users only need to confirm AI’s proposal. Specifically, we propose an object-based interaction to make the interactive retrieval more user-friendly and present a reinforcement-learning-based policy to search for discriminative objects. Furthermore, since fully-supervised training is often infeasible due to the difficulty of obtaining human-machine dialog data, we present a weakly-supervised training strategy that needs no human-annotated dialogs other than a text-image dataset. Experiments show that our framework significantly improves the performance of text-based image retrieval. Code is available at https://github.com/CuthbertCai/Ask-Confirm.},
  archive   = {C_ICCV},
  author    = {Guanyu Cai and Jun Zhang and Xinyang Jiang and Yifei Gong and Lianghua He and Fufu Yu and Pai Peng and Xiaowei Guo and Feiyue Huang and Xing Sun},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00185},
  pages     = {1815-1824},
  title     = {Ask&amp;Confirm: Active detail enriching for cross-modal retrieval with partial query},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to generate scene graph from natural language
supervision. <em>ICCV</em>, 1803–1814. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning from image-text data has demonstrated recent success for many recognition tasks, yet is currently limited to visual features or individual visual concepts such as objects. In this paper, we propose one of the first methods that learn from image-sentence pairs to extract a graphical representation of localized objects and their relationships within an image, known as scene graph. To bridge the gap between images and texts, we leverage an off-the-shelf object detector to identify and localize object instances, match labels of detected regions to concepts parsed from captions, and thus create &quot;pseudo&quot; labels for learning scene graph. Further, we design a Transformer-based model to predict these &quot;pseudo&quot; labels via a masked token prediction task. Learning from only image-sentence pairs, our model achieves 30\% relative gain over a latest method trained with human-annotated unlocalized scene graphs. Our model also shows strong results for weakly and fully supervised scene graph generation. In addition, we explore an open-vocabulary setting for detecting scene graphs, and present the first result for open-set scene graph generation.},
  archive   = {C_ICCV},
  author    = {Yiwu Zhong and Jing Shi and Jianwei Yang and Chenliang Xu and Yin Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00184},
  pages     = {1803-1814},
  title     = {Learning to generate scene graph from natural language supervision},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Wasserstein coupled graph learning for cross-modal
retrieval. <em>ICCV</em>, 1793–1802. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graphs play an important role in cross-modal image-text understanding as they characterize the intrinsic structure which is robust and crucial for the measurement of crossmodal similarity. In this work, we propose a Wasserstein Coupled Graph Learning (WCGL) method to deal with the cross-modal retrieval task. First, graphs are constructed according to two input cross-modal samples separately, and passed through the corresponding graph encoders to extract robust features. Then, a Wasserstein coupled dictionary, containing multiple pairs of counterpart graph keys with each key corresponding to one modality, is constructed for further feature learning. Based on this dictionary, the input graphs can be transformed into the dictionary space to facilitate the similarity measurement through a Wasserstein Graph Embedding (WGE) process. The WGE could capture the graph correlation between the input and each corresponding key through optimal transport, and hence well characterize the inter-graph structural relationship. To further achieve discriminant graph learning, we specifically define a Wasserstein discriminant loss on the coupled graph keys to make the intra-class (counterpart) keys more compact and inter-class (non-counterpart) keys more dispersed, which further promotes the final cross-modal retrieval task. Experimental results demonstrate the effectiveness and state-of-the-art performance.},
  archive   = {C_ICCV},
  author    = {Yun Wang and Tong Zhang and Xueya Zhang and Zhen Cui and Yuge Huang and Pengcheng Shen and Shaoxin Li and Jian Yang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00183},
  pages     = {1793-1802},
  title     = {Wasserstein coupled graph learning for cross-modal retrieval},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detector-free weakly supervised grounding by separation.
<em>ICCV</em>, 1781–1792. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Nowadays, there is an abundance of data involving images and surrounding free-form text weakly corresponding to those images. Weakly Supervised phrase-Grounding (WSG) deals with the task of using this data to learn to localize (or to ground) arbitrary text phrases in images without any additional annotations. However, most recent SotA methods for WSG assume an existence of a pre-trained object detector, relying on it to produce the ROIs for localization. In this work, we focus on the task of Detector-Free WSG (DF-WSG) to solve WSG without relying on a pre-trained detector. The key idea behind our proposed Grounding by Separation (GbS) method is synthesizing ‘text to image-regions’ associations by random alpha-blending of arbitrary image pairs and using the corresponding texts of the pair as conditions to recover the alpha map from the blended image via a segmentation network. At test time, this allows using the query phrase as a condition for a non-blended query image, thus interpreting the test image as a composition of a region corresponding to the phrase and the complement region. Our GbS shows an 8.5\% accuracy improvement over previous DF-WSG SotA, for a range of benchmarks including Flickr30K, Visual Genome, and ReferIt, as well as a complementary improvement (above 7\%) over the detector-based approaches for WSG.},
  archive   = {C_ICCV},
  author    = {Assaf Arbelle and Sivan Doveh and Amit Alfassy and Joseph Shtok and Guy Lev and Eli Schwartz and Hilde Kuehne and Hila Barak Levi and Prasanna Sattigeri and Rameswar Panda and Chun-Fu Chen and Alex Bronstein and Kate Saenko and Shimon Ullman and Raja Giryes and Rogerio Feris and Leonid Karlinsky},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00182},
  pages     = {1781-1792},
  title     = {Detector-free weakly supervised grounding by separation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). InstanceRefer: Cooperative holistic understanding for visual
grounding on point clouds through instance multi-level contextual
referring. <em>ICCV</em>, 1771–1780. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Compared with the visual grounding on 2D images, the natural-language-guided 3D object localization on point clouds is more challenging. In this paper, we propose a new model, named InstanceRefer 1 , to achieve a superior 3D visual grounding through the grounding-by-matching strategy. In practice, our model first predicts the target category from the language descriptions using a simple language classification model. Then, based on the category, our model sifts out a small number of instance candidates (usually less than 20) from the panoptic segmentation on point clouds. Thus, the non-trivial 3D visual grounding task has been effectively re-formulated as a simplified instance-matching problem, considering that instance-level candidates are more rational than the redundant 3D object proposals. Subsequently, for each candidate, we perform the multi-level contextual inference, i.e., referring from instance attribute perception, instance-to-instance relation perception, and instance-to-background global localization perception, respectively. Eventually, the most relevant candidate is selected and localized by ranking confidence scores, which are obtained by the cooperative holistic visual-language feature matching. Experiments confirm that our method outperforms previous state-of-the-arts on ScanRefer online benchmark and Nr3D/Sr3D datasets.},
  archive   = {C_ICCV},
  author    = {Zhihao Yuan and Xu Yan and Yinghong Liao and Ruimao Zhang and Sheng Wang and Zhen Li and Shuguang Cui},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00181},
  pages     = {1771-1780},
  title     = {InstanceRefer: Cooperative holistic understanding for visual grounding on point clouds through instance multi-level contextual referring},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MDETR - modulated detection for end-to-end multi-modal
understanding. <em>ICCV</em>, 1760–1770. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-modal reasoning systems rely on a pre-trained object detector to extract regions of interest from the image. However, this crucial module is typically used as a black box, trained independently of the downstream task and on a fixed vocabulary of objects and attributes. This makes it challenging for such systems to capture the long tail of visual concepts expressed in free form text. In this paper we propose MDETR, an end-to-end modulated detector that detects objects in an image conditioned on a raw text query, like a caption or a question. We use a transformer-based architecture to reason jointly over text and image by fusing the two modalities at an early stage of the model. We pre-train the network on 1.3M text-image pairs, mined from pre-existing multi-modal datasets having explicit alignment between phrases in text and objects in the image. We then fine-tune on several downstream tasks such as phrase grounding, referring expression comprehension and segmentation, achieving state-of-the-art results on popular benchmarks. We also investigate the utility of our model as an object detector on a given label set when fine-tuned in a few-shot setting. We show that our pre-training approach provides a way to handle the long tail of object categories which have very few labelled instances. Our approach can be easily extended for visual question answering, achieving competitive performance on GQA and CLEVR. The code and models are available at https://github.com/ashkamath/mdetr.},
  archive   = {C_ICCV},
  author    = {Aishwarya Kamath and Mannat Singh and Yann LeCun and Gabriel Synnaeve and Ishan Misra and Nicolas Carion},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00180},
  pages     = {1760-1770},
  title     = {MDETR - modulated detection for end-to-end multi-modal understanding},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TransVG: End-to-end visual grounding with transformers.
<em>ICCV</em>, 1749–1759. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a neat yet effective transformer-based framework for visual grounding, namely TransVG, to address the task of grounding a language query to the corresponding region onto an image. The state-of-the-art methods, including two-stage or one-stage ones, rely on a complex module with manually-designed mechanisms to perform the query reasoning and multi-modal fusion. However, the involvement of certain mechanisms in fusion module design, such as query decomposition and image scene graph, makes the models easily overfit to datasets with specific scenarios, and limits the plenitudinous interaction between the visual-linguistic context. To avoid this caveat, we propose to establish the multi-modal correspondence by leveraging transformers, and empirically show that the complex fusion modules (e.g., modular attention network, dynamic graph, and multi-modal tree) can be replaced by a simple stack of transformer encoder layers with higher performance. Moreover, we re-formulate the visual grounding as a direct coordinates regression problem and avoid making predictions out of a set of candidates (i.e., region proposals or anchor boxes). Extensive experiments are conducted on five widely used datasets, and a series of state-of-the-art records are set by our TransVG. We build the benchmark of transformer-based visual grounding framework and make the code available at https://github.com/djiajunustc/TransVG.},
  archive   = {C_ICCV},
  author    = {Jiajun Deng and Zhengyuan Yang and Tianlang Chen and Wengang Zhou and Houqiang Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00179},
  pages     = {1749-1759},
  title     = {TransVG: End-to-end visual grounding with transformers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unsupervised deep video denoising. <em>ICCV</em>, 1739–1748.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep convolutional neural networks (CNNs) for video denoising are typically trained with supervision, assuming the availability of clean videos. However, in many applications, such as microscopy, noiseless videos are not available. To address this, we propose an Unsupervised Deep Video Denoiser (UDVD 1 ), a CNN architecture designed to be trained exclusively with noisy data. The performance of UDVD is comparable to the supervised state-of-the-art, even when trained only on a single short noisy video. We demonstrate the promise of our approach in real-world imaging applications by denoising raw video, fluorescence-microscopy and electron-microscopy data. In contrast to many current approaches to video denoising, UDVD does not require explicit motion compensation. This is advantageous because motion compensation is computationally expensive, and can be unreliable when the input data are noisy. A gradient-based analysis reveals that UDVD automatically adapts to local motion in the input noisy videos. Thus, the network learns to perform implicit motion compensation, even though it is only trained for denoising.},
  archive   = {C_ICCV},
  author    = {Dev Yashpal Sheth and Sreyas Mohan and Joshua L. Vincent and Ramon Manzorro and Peter A. Crozier and Mitesh M. Khapra and Eero P. Simoncelli and Carlos Fernandez-Granda},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00178},
  pages     = {1739-1748},
  title     = {Unsupervised deep video denoising},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep 3D mask volume for view synthesis of dynamic scenes.
<em>ICCV</em>, 1729–1738. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Image view synthesis has seen great success in reconstructing photorealistic visuals, thanks to deep learning and various novel representations. The next key step in immersive virtual experiences is view synthesis of dynamic scenes. However, several challenges exist due to the lack of high-quality training datasets, and the additional time dimension for videos of dynamic scenes. To address this issue, we introduce a multi-view video dataset, captured with a custom 10-camera rig in 120FPS. The dataset contains 96 high-quality scenes showing various visual effects and human interactions in outdoor scenes. We develop a new algorithm, Deep 3D Mask Volume, which enables temporally-stable view extrapolation from binocular videos of dynamic scenes, captured by static cameras. Our algorithm addresses the temporal inconsistency of disocclusions by identifying the error-prone areas with a 3D mask volume, and replaces them with static background observed throughout the video. Our method enables manipulation in 3D space as opposed to simple 2D masks, We demonstrate better temporal stability than frame-by-frame static view synthesis methods, or those that use 2D masks. The resulting view synthesis videos show minimal flickering artifacts and allow for larger translational movements.},
  archive   = {C_ICCV},
  author    = {Kai-En Lin and Lei Xiao and Feng Liu and Guowei Yang and Ravi Ramamoorthi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00177},
  pages     = {1729-1738},
  title     = {Deep 3D mask volume for view synthesis of dynamic scenes},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Video instance segmentation with a propose-reduce paradigm.
<em>ICCV</em>, 1719–1728. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Video instance segmentation (VIS) aims to segment and associate all instances of predefined classes for each frame in videos. Prior methods usually obtain segmentation for a frame or clip first, and merge the incomplete results by tracking or matching. These methods may cause error accumulation in the merging step. Contrarily, we propose a new paradigm – Propose-Reduce, to generate complete sequences for input videos by a single step. We further build a sequence propagation head on the existing image-level instance segmentation network for long-term propagation. To ensure robustness and high recall of our proposed framework, multiple sequences are proposed where redundant sequences of the same instance are reduced. We achieve state-of-the-art performance on two representative benchmark datasets – we obtain 47.6\% in terms of AP on YouTube-VIS validation set and 70.4\% for J&amp;F on DAVIS-UVOS validation set.},
  archive   = {C_ICCV},
  author    = {Huaijia Lin and Ruizheng Wu and Shu Liu and Jiangbo Lu and Jiaya Jia},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00176},
  pages     = {1719-1728},
  title     = {Video instance segmentation with a propose-reduce paradigm},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Frozen in time: A joint video and image encoder for
end-to-end retrieval. <em>ICCV</em>, 1708–1718. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Our objective in this work is video-text retrieval – in particular a joint embedding that enables efficient text-to-video retrieval. The challenges in this area include the design of the visual architecture and the nature of the training data, in that the available large scale video-text training datasets, such as HowTo100M, are noisy and hence competitive performance is achieved only at scale through large amounts of compute.We address both these challenges in this paper. We propose an end-to-end trainable model that is designed to take advantage of both large-scale image and video captioning datasets. Our model is an adaptation and extension of the recent ViT and Timesformer architectures, and consists of attention in both space and time. The model is flexible and can be trained on both image and video text datasets, either independently or in conjunction. It is trained with a curriculum learning schedule that begins by treating images as ‘frozen’ snapshots of video, and then gradually learns to attend to increasing temporal context when trained on video datasets. We also provide a new video-text pretraining dataset WebVid-2M, comprised of over two million videos with weak captions scraped from the internet. Despite training on datasets that are an order of magnitude smaller, we show that this approach yields state-of-the-art results on standard downstream video-retrieval benchmarks including MSR-VTT, MSVD, DiDeMo and LSMDC.},
  archive   = {C_ICCV},
  author    = {Max Bain and Arsha Nagrani and Gül Varol and Andrew Zisserman},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00175},
  pages     = {1708-1718},
  title     = {Frozen in time: A joint video and image encoder for end-to-end retrieval},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiple pairwise ranking networks for personalized video
summarization. <em>ICCV</em>, 1698–1707. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we investigate video summarization in the supervised setting. Since video summarization is subjective to the preference of the end-user, the design of a unique model is limited. In this work, we propose a model that provides personalized video summaries by conditioning the summarization process with predefined categorical user labels referred to as preferences. The underlying method is based on multiple pairwise rankers (called Multi-ranker), where the rankers are trained jointly to provide local summaries as well as a global summarization of a given video. In order to demonstrate the relevance and applications of our method in contrast with a classical global summarizer, we conduct experiments on multiple benchmark datasets, notably through a user study and comparisons with the state-of-art methods in the global video summarization task.},
  archive   = {C_ICCV},
  author    = {Yassir Saquil and Da Chen and Yuan He and Chuan Li and Yong-Liang Yang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00174},
  pages     = {1698-1707},
  title     = {Multiple pairwise ranking networks for personalized video summarization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Video question answering using language-guided deep
compressed-domain video feature. <em>ICCV</em>, 1688–1697. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Video Question Answering (Video QA) aims to give an answer to the question through semantic reasoning between visual and linguistic information. Recently, handling large amounts of multi-modal video and language information of a video is considered important in the industry. However, the current video QA models use deep features, suffered from significant computational complexity and insufficient representation capability both in training and testing. Existing features are extracted using pre-trained networks after all the frames are decoded, which is not always suitable for video QA tasks. In this paper, we develop a novel deep neural network to provide video QA features obtained from coded video bit-stream to reduce the complexity. The proposed network includes several dedicated deep modules to both the video QA and the video compression system, which is the first attempt at the video QA task. The proposed network is predominantly model-agnostic. It is integrated into the state-of-the-art networks for improved performance without any computationally expensive motion-related deep models. The experimental results demonstrate that the proposed network outperforms the previous studies at lower complexity. https://github.com/Nayoung-Kim-ICP/VQAC},
  archive   = {C_ICCV},
  author    = {Nayoung Kim and Seong Jong Ha and Je-Won Kang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00173},
  pages     = {1688-1697},
  title     = {Video question answering using language-guided deep compressed-domain video feature},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). HAIR: Hierarchical visual-semantic relational reasoning for
video question answering. <em>ICCV</em>, 1678–1687. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Relational reasoning is at the heart of video question answering. However, existing approaches suffer from several common limitations: (1) they only focus on either object-level or frame-level relational reasoning, and fail to integrate the both; and (2) they neglect to leverage semantic knowledge for relational reasoning. In this work, we propose a Hierarchical VisuAl-Semantic RelatIonal Reasoning (HAIR) framework to address these limitations. Specifically, we present a novel graph memory mechanism to perform relational reasoning, and further develop two types of graph memory: a) visual graph memory that leverages visual information of video for relational reasoning; b) semantic graph memory that is specifically designed to explicitly leverage semantic knowledge contained in the classes and attributes of video objects, and perform relational reasoning in the semantic space. Taking advantage of both graph memory mechanisms, we build a hierarchical framework to enable visual-semantic relational reasoning from object level to frame level. Experiments on four challenging benchmark datasets show that the proposed framework leads to state-of-the-art performance, with fewer parameters and faster inference speed. Besides, our approach also shows superior performance on other video+language task.},
  archive   = {C_ICCV},
  author    = {Fei Liu and Jing Liu and Weining Wang and Hanqing Lu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00172},
  pages     = {1678-1687},
  title     = {HAIR: Hierarchical visual-semantic relational reasoning for video question answering},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Just ask: Learning to answer questions from millions of
narrated videos. <em>ICCV</em>, 1666–1677. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent methods for visual question answering rely on large-scale annotated datasets. Manual annotation of questions and answers for videos, however, is tedious, expensive and prevents scalability. In this work, we propose to avoid manual annotation and generate a large-scale training dataset for video question answering making use of automatic cross-modal supervision. We leverage a question generation transformer trained on text data and use it to generate question-answer pairs from transcribed video narrations. Given narrated videos, we then automatically generate the HowToVQA69M dataset with 69M video-question-answer triplets. To handle the open vocabulary of diverse answers in this dataset, we propose a training procedure based on a contrastive loss between a video-question multi-modal transformer and an answer transformer. We introduce the zero-shot VideoQA task and show excellent results, in particular for rare answers. Furthermore, we demonstrate our method to significantly outperform the state of the art on MSRVTT-QA, MSVD-QA, ActivityNet-QA and How2QA. Finally, for a detailed evaluation we introduce iVQA, a new VideoQA dataset with reduced language biases and high-quality redundant manual annotations.},
  archive   = {C_ICCV},
  author    = {Antoine Yang and Antoine Miech and Josef Sivic and Ivan Laptev and Cordelia Schmid},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00171},
  pages     = {1666-1677},
  title     = {Just ask: Learning to answer questions from millions of narrated videos},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Env-QA: A video question answering benchmark for
comprehensive understanding of dynamic environments. <em>ICCV</em>,
1655–1665. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual understanding goes well beyond the study of images or videos on the web. To achieve complex tasks in volatile situations, the human can deeply understand the environment, quickly perceive events happening around, and continuously track objects’ state changes, which are still challenging for current AI systems. To equip AI system with the ability to understand dynamic ENVironments, we build a video Question Answering dataset named Env-QA. Env-QA contains 23K egocentric videos, where each video is composed of a series of events about exploring and interacting in the environment. It also provides 85K questions to evaluate the ability of understanding the composition, layout, and state changes of the environment presented by the events in videos. Moreover, we propose a video QA model, Temporal Segmentation and Event Attention network (TSEA), which introduces event-level video representation and corresponding attention mechanisms to better extract environment information and answer questions. Comprehensive experiments demonstrate the effectiveness of our framework and show the formidable challenges of Env-QA in terms of long-term state tracking, multi-event temporal reasoning and event counting, etc.},
  archive   = {C_ICCV},
  author    = {Difei Gao and Ruiping Wang and Ziyi Bai and Xilin Chen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00170},
  pages     = {1655-1665},
  title     = {Env-QA: A video question answering benchmark for comprehensive understanding of dynamic environments},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). VLGrammar: Grounded grammar induction of vision and
language. <em>ICCV</em>, 1645–1654. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cognitive grammar suggests that the acquisition of language grammar is grounded within visual structures. While grammar is an essential representation of natural language, it also exists ubiquitously in vision to represent the hierarchical part-whole structure. In this work, we study grounded grammar induction of vision and language in a joint learning framework. Specifically, we present VLGrammar, a method that uses compound probabilistic context-free grammars (compound PCFGs) to induce the language grammar and the image grammar simultaneously. We propose a novel contrastive learning framework to guide the joint learning of both modules. To provide a benchmark for the grounded grammar induction task, we collect a large-scale dataset, PARTIT, which contains human-written sentences that describe part-level semantics for 3D objects. Experiments on the PARTIT dataset show that VLGrammar outperforms all baselines in image grammar induction and language grammar induction. The learned VLGrammar naturally benefits related downstream tasks. Specifically, it improves the image unsupervised clustering accuracy by 30\%, and performs well in image retrieval and text retrieval. Notably, the induced grammar shows superior generalizability by easily generalizing to unseen categories. Code and pre-trained models are released at https://github.com/evelinehong/VLGrammar.},
  archive   = {C_ICCV},
  author    = {Yining Hong and Qing Li and Song-Chun Zhu and Siyuan Huang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00169},
  pages     = {1645-1654},
  title     = {VLGrammar: Grounded grammar induction of vision and language},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The road to know-where: An object-and-room informed
sequential BERT for indoor vision-language navigation. <em>ICCV</em>,
1635–1644. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vision-and-Language Navigation (VLN) requires an agent to find a path to a remote location on the basis of natural-language instructions and a set of photo-realistic panoramas. Most existing methods take the words in the instructions and the discrete views of each panorama as the minimal unit of encoding. However, this requires a model to match different nouns (e.g., TV, table) against the same input view feature. In this work, we propose an object-informed sequential BERT to encode visual perceptions and linguistic instructions at the same fine-grained level, namely objects and words. Our sequential BERT also enables the visual-textual clues to be interpreted in light of the temporal context, which is crucial to multi-round VLN tasks. Additionally, we enable the model to identify the relative direction (e.g., left/right/front/back) of each navigable location and the room type (e.g., bedroom, kitchen) of its current and final navigation goal, as such information is widely mentioned in instructions implying the desired next and final locations. We thus enable the model to know-where the objects lie in the images, and to know-where they stand in the scene. Extensive experiments demonstrate the effectiveness compared against several state-of-the-art methods on three indoor VLN tasks: REVERIE, NDH, and R2R. Project repository: https://github.com/YuankaiQi/ORIST},
  archive   = {C_ICCV},
  author    = {Yuankai Qi and Zizheng Pan and Yicong Hong and Ming-Hsuan Yang and Anton van den Hengel and Qi Wu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00168},
  pages     = {1635-1644},
  title     = {The road to know-where: An object-and-room informed sequential BERT for indoor vision-language navigation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vision-language navigation with random environmental mixup.
<em>ICCV</em>, 1624–1634. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vision-language Navigation (VLN) tasks require an agent to navigate step-by-step while perceiving the visual observations and comprehending a natural language instruction. Large data bias, which is caused by the disparity ratio between the small data scale and large navigation space, makes the VLN task challenging. Previous works have proposed various data augmentation methods to reduce data bias. However, these works do not explicitly reduce the data bias across different house scenes. Therefore, the agent would overfit to the seen scenes and achieve poor navigation performance in the unseen scenes. To tackle this problem, we propose the Random Environmental Mixup (REM) method, which generates cross-connected house scenes as augmented data via mixuping environment. Specifically, we first select key viewpoints according to the room connection graph for each scene. Then, we cross-connect the key views of different scenes to construct augmented scenes. Finally, we generate augmented instruction-path pairs in the cross-connected scenes. The experimental results on benchmark datasets demonstrate that our augmentation data via REM help the agent reduce its performance gap between the seen and unseen environment and improve the overall performance, making our model the best existing approach on the standard VLN benchmark.},
  archive   = {C_ICCV},
  author    = {Chong Liu and Fengda Zhu and Xiaojun Chang and Xiaodan Liang and Zongyuan Ge and Yi-Dong Shen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00167},
  pages     = {1624-1634},
  title     = {Vision-language navigation with random environmental mixup},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Airbert: In-domain pretraining for vision-and-language
navigation. <em>ICCV</em>, 1614–1623. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vision-and-language navigation (VLN) aims to enable embodied agents to navigate in realistic environments using natural language instructions. Given the scarcity of domain-specific training data and the high diversity of image and language inputs, the generalization of VLN agents to unseen environments remains challenging. Recent methods explore pretraining to improve generalization, however, the use of generic image-caption datasets or existing small-scale VLN environments is suboptimal and results in limited improvements. In this work, we introduce BnB 1 , a large-scale and diverse in-domain VLN dataset. We first collect image-caption (IC) pairs from hundreds of thousands of listings from online rental marketplaces. Using IC pairs we next propose automatic strategies to generate millions of VLN path-instruction (PI) pairs. We further propose a shuffling loss that improves the learning of temporal order inside PI pairs. We use BnB to pretrain our Airbert 2 model that can be adapted to discriminative and generative settings and show that it outperforms state of the art for Room-to-Room (R2R) navigation and Remote Referring Expression (REVERIE) benchmarks. Moreover, our in-domain pretraining significantly increases performance on a challenging few-shot VLN evaluation, where we train the model only on VLN instructions from a few houses.},
  archive   = {C_ICCV},
  author    = {Pierre-Louis Guhur and Makarand Tapaswi and Shizhe Chen and Ivan Laptev and Cordelia Schmid},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00166},
  pages     = {1614-1623},
  title     = {Airbert: In-domain pretraining for vision-and-language navigation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LapsCore: Language-guided person search via color reasoning.
<em>ICCV</em>, 1604–1613. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The key point of language-guided person search is to construct the cross-modal association between visual and textual input. Existing methods focus on designing multimodal attention mechanisms and novel cross-modal loss functions to learn such association implicitly. We propose a representation learning method for language-guided person search based on color reasoning (LapsCore). It can explicitly build a fine-grained cross-modal association bidirectionally. Specifically, a pair of dual sub-tasks, image colorization and text completion, is designed. In the former task, rich text information is learned to colorize gray images, and the latter one requests the model to understand the image and complete color word vacancies in the captions. The two sub-tasks enable models to learn correct alignments between text phrases and image regions, so that rich multimodal representations can be learned. Extensive experiments on multiple datasets demonstrate the effectiveness and superiority of the proposed method.},
  archive   = {C_ICCV},
  author    = {Yushuang Wu and Zizheng Yan and Xiaoguang Han and Guanbin Li and Changqing Zou and Shuguang Cui},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00165},
  pages     = {1604-1613},
  title     = {LapsCore: Language-guided person search via color reasoning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Linguistically routing capsule network for
out-of-distribution visual question answering. <em>ICCV</em>, 1594–1603.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generalization on out-of-distribution (OOD) test data is an essential but underexplored topic in visual question answering. Current state-of-the-art VQA models often exploit the biased correlation between data and labels, which results in a large performance drop when the test and training data have different distributions. Inspired by the fact that humans can recognize novel concepts by composing existed concepts and capsule network’s ability of representing part-whole hierarchies, we propose to use capsules to represent parts and introduce &quot;Linguistically Routing&quot; to merge parts with human-prior hierarchies. Specifically, we first fuse visual features with a single question word as atomic parts. Then we introduce the &quot;Linguistically Routing&quot; to reweight the capsule connections between two layers such that: 1) the lower layer capsules can transfer their outputs to the most compatible higher capsules, and 2) two capsules can be merged if their corresponding words are merged in the question parse tree. The routing process maximizes the above unary and binary potentials across multiple layers and finally carves a tree structure inside the capsule network. We evaluate our proposed routing method on the CLEVR compositional generation test, the VQA-CP2 dataset and the VQAv2 dataset. The experimental results show that our proposed method can improve current VQA models on OOD split without losing performance on the in-domain test data.},
  archive   = {C_ICCV},
  author    = {Qingxing Cao and Wentao Wan and Keze Wang and Xiaodan Liang and Liang Lin},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00164},
  pages     = {1594-1603},
  title     = {Linguistically routing capsule network for out-of-distribution visual question answering},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Contrast and classify: Training robust VQA models.
<em>ICCV</em>, 1584–1593. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent Visual Question Answering (VQA) models have shown impressive performance on the VQA benchmark but remain sensitive to small linguistic variations in input questions. Existing approaches address this by augmenting the dataset with question paraphrases from visual question generation models or adversarial perturbations. These approaches use the combined data to learn an answer classifier by minimizing the standard cross-entropy loss. To more effectively leverage augmented data, we build on the recent success in contrastive learning. We propose a novel training paradigm (ConClaT) that optimizes both cross-entropy and contrastive losses. The contrastive loss encourages representations to be robust to linguistic variations in questions while the cross-entropy loss preserves the discriminative power of representations for answer prediction.We find that optimizing both losses – either alternately or jointly – is key to effective training. On the VQA-Rephrasings [44] benchmark, which measures the VQA model’s answer consistency across human paraphrases of a question, ConClaT improves Consensus Score by 1.63\% over an improved baseline. In addition, on the standard VQA 2.0 benchmark, we improve the VQA accuracy by 0.78\% overall. We also show that ConClaT is agnostic to the type of data-augmentation strategy used.},
  archive   = {C_ICCV},
  author    = {Yash Kant and Abhinav Moudgil and Dhruv Batra and Devi Parikh and Harsh Agrawal},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00163},
  pages     = {1584-1593},
  title     = {Contrast and classify: Training robust VQA models},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-motivated communication agent for real-world
vision-dialog navigation. <em>ICCV</em>, 1574–1583. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vision-Dialog Navigation (VDN) requires an agent to ask questions and navigate following the human responses to find target objects. Conventional approaches are only allowed to ask questions at predefined locations, which are built upon expensive dialogue annotations, and inconvenience the real-word human-robot communication and cooperation. In this paper, we propose a Self-Motivated Communication Agent (SCoA) that learns whether and what to communicate with human adaptively to acquire instructive information for realizing dialogue annotation-free navigation and enhancing the transferability in real-world unseen environment. Specifically, we introduce a whether-to-ask (WeTA) policy, together with uncertainty of which action to choose, to indicate whether the agent should ask a question. Then, a what-to-ask (WaTA) policy is proposed, in which, along with the oracle’s answers, the agent learns to score question candidates so as to pick up the most informative one for navigation, and meanwhile mimic oracle’s answering. Thus, the agent can navigate in a self-Q&amp;A manner even in real-world environment where the human assistance is often unavailable. Through joint optimization of communication and navigation in a unified imitation learning and reinforcement learning framework, SCoA asks a question if necessary and obtains a hint for guiding the agent to move towards the target with less communication cost. Experiments on seen and unseen environments demonstrate that SCoA shows not only superior performance over existing baselines without dialog annotations, but also competing results compared with rich dialog annotations based counterparts.},
  archive   = {C_ICCV},
  author    = {Yi Zhu and Yue Weng and Fengda Zhu and Xiaodan Liang and Qixiang Ye and Yutong Lu and Jianbin Jiao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00162},
  pages     = {1574-1583},
  title     = {Self-motivated communication agent for real-world vision-dialog navigation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Greedy gradient ensemble for robust visual question
answering. <em>ICCV</em>, 1564–1573. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Language bias is a critical issue in Visual Question Answering (VQA), where models often exploit dataset biases for the final decision without considering the image information. As a result, they suffer from performance drop on out-of-distribution data and inadequate visual explanation. Based on experimental analysis for existing robust VQA methods, we stress the language bias in VQA that comes from two aspects, i.e., distribution bias and shortcut bias. We further propose a new de-bias framework, Greedy Gradient Ensemble (GGE), which combines multiple biased models for unbiased base model learning. With the greedy strategy, GGE forces the biased models to over-fit the biased data distribution in priority, thus makes the base model pay more attention to examples that are hard to solve by biased models. The experiments demonstrate that our method makes better use of visual information and achieves state-of-the-art performance on diagnosing dataset VQACP without using extra annotations.},
  archive   = {C_ICCV},
  author    = {Xinzhe Han and Shuhui Wang and Chi Su and Qingming Huang and Qi Tian},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00161},
  pages     = {1564-1573},
  title     = {Greedy gradient ensemble for robust visual question answering},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Beyond question-based biases: Assessing multimodal shortcut
learning in visual question answering. <em>ICCV</em>, 1554–1563. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce an evaluation methodology for visual question answering (VQA) to better diagnose cases of shortcut learning. These cases happen when a model exploits spurious statistical regularities to produce correct answers but does not actually deploy the desired behavior. There is a need to identify possible shortcuts in a dataset and assess their use before deploying a model in the real world. The research community in VQA has focused exclusively on question-based shortcuts, where a model might, for example, answer &quot;What is the color of the sky&quot; with &quot;blue&quot; by relying mostly on the question-conditional training prior and give little weight to visual evidence. We go a step further and consider multimodal shortcuts that involve both questions and images. We first identify potential shortcuts in the popular VQA v2 training set by mining trivial predictive rules such as co-occurrences of words and visual elements. We then introduce VQA-CounterExamples (VQACE), an evaluation protocol based on our subset of CounterExamples i.e. image-question-answer triplets where our rules lead to incorrect answers. We use this new evaluation in a large-scale study of existing approaches for VQA. We demonstrate that even state-of-the-art models perform poorly and that existing techniques to reduce biases are largely ineffective in this context. Our findings suggest that past work on question-based biases in VQA has only addressed one facet of a complex issue. The code for our method is available at https://github.com/cdancette/detect-shortcuts},
  archive   = {C_ICCV},
  author    = {Corentin Dancette and Rémi Cadène and Damien Teney and Matthieu Cord},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00160},
  pages     = {1554-1563},
  title     = {Beyond question-based biases: Assessing multimodal shortcut learning in visual question answering},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). Learning motion-appearance co-attention for zero-shot video
object segmentation. <em>ICCV</em>, 1544–1553. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {How to make the appearance and motion information interact effectively to accommodate complex scenarios is a fundamental issue in flow-based zero-shot video object segmentation. In this paper, we propose an Attentive Multi-Modality Collaboration Network (AMC-Net) to utilize appearance and motion information uniformly. Specifically, AMC-Net fuses robust information from multi-modality features and promotes their collaboration in two stages. First, we propose a Multi-Modality Co-Attention Gate (MCG) on the bilateral encoder branches, in which a gate function is used to formulate co-attention scores for balancing the contributions of multi-modality features and suppressing the redundant and misleading information. Then, we propose a Motion Correction Module (MCM) with a visualmotion attention mechanism, which is constructed to emphasize the features of foreground objects by incorporating the spatio-temporal correspondence between appearance and motion cues. Extensive experiments on three public challenging benchmark datasets verify that our proposed network performs favorably against existing state-of-the-art methods via training with fewer data. The code is released at https://github.com/isyangshu/AMC-Net.},
  archive   = {C_ICCV},
  author    = {Shu Yang and Lu Zhang and Jinqing Qi and Huchuan Lu and Shuo Wang and Xiaoxing Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00159},
  pages     = {1544-1553},
  title     = {Learning motion-appearance co-attention for zero-shot video object segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Dynamic context-sensitive filtering network for video
salient object detection. <em>ICCV</em>, 1533–1543. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to capture inter-frame dynamics has been critical to the development of video salient object detection (VSOD). While many works have achieved great success in this field, a deeper insight into its dynamic nature should be developed. In this work, we aim to answer the following questions: How can a model adjust itself to dynamic variations as well as perceive fine differences in the real-world environment; How are the temporal dynamics well introduced into spatial information over time? To this end, we propose a dynamic context-sensitive filtering network (DCFNet) equipped with a dynamic context-sensitive filtering module (DCFM) and an effective bidirectional dynamic fusion strategy. The proposed DCFM sheds new light on dynamic filter generation by extracting location-related affinities between consecutive frames. Our bidirectional dynamic fusion strategy encourages the interaction of spatial and temporal information in a dynamic manner. Experimental results demonstrate that our proposed method can achieve state-of-the-art performance on most VSOD datasets while ensuring a real-time speed of 28 fps. The source code is publicly available at https://github.com/OIPLab-DUT/DCFNet.},
  archive   = {C_ICCV},
  author    = {Miao Zhang and Jie Liu and Yifei Wang and Yongri Piao and Shunyu Yao and Wei Ji and Jingjing Li and Huchuan Lu and Zhongxuan Luo},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00158},
  pages     = {1533-1543},
  title     = {Dynamic context-sensitive filtering network for video salient object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Motion guided region message passing for video captioning.
<em>ICCV</em>, 1523–1532. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Video captioning is an important vision task and has been intensively studied in the computer vision community. Existing methods that utilize the fine-grained spatial information have achieved significant improvements, however, they either rely on costly external object detectors or do not sufficiently model the spatial/temporal relations. In this paper, we aim at designing a spatial information extraction and aggregation method for video captioning without the need of external object detectors. For this purpose, we propose a Recurrent Region Attention module to better extract diverse spatial features, and by employing Motion-Guided Cross-frame Message Passing, our model is aware of the temporal structure and able to establish high-order relations among the diverse regions across frames. They jointly encourage information communication and produce compact and powerful video representations. Furthermore, an Adjusted Temporal Graph Decoder is proposed to flexibly update video features and model high-order temporal relations during decoding. Experimental results on three benchmark datasets: MSVD, MSR-VTT, and VATEX demonstrate that our proposed method can outperform state-of-the-art methods.},
  archive   = {C_ICCV},
  author    = {Shaoxiang Chen and Yu-Gang Jiang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00157},
  pages     = {1523-1532},
  title     = {Motion guided region message passing for video captioning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). STVGBert: A visual-linguistic transformer based framework
for spatio-temporal video grounding. <em>ICCV</em>, 1513–1522. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Spatio-temporal video grounding (STVG) aims to localize a spatio-temporal tube of a target object in an untrimmed video based on a query sentence. In this work, we propose a one-stage visual-linguistic transformer based framework called STVGBert for the STVG task, which can simultaneously localize the target object in both spatial and temporal domains. Specifically, without resorting to pregenerated object proposals, our STVGBert directly takes a video and a query sentence as the input, and then produces the cross-modal features by using the newly introduced cross-modal feature learning module ST-ViLBert. Based on the cross-modal features, our method then generates bounding boxes and predicts the starting and ending frames to produce the predicted object tube. To the best of our knowledge, our STVGBert is the first one-stage method, which can handle the STVG task without relying on any pre-trained object detectors. Comprehensive experiments demonstrate our newly proposed framework outperforms the state-of-the-art multi-stage methods on two benchmark datasets Vid-STG and HC-STVG.},
  archive   = {C_ICCV},
  author    = {Rui Su and Qian Yu and Dong Xu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00156},
  pages     = {1513-1522},
  title     = {STVGBert: A visual-linguistic transformer based framework for spatio-temporal video grounding},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast video moment retrieval. <em>ICCV</em>, 1503–1512. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper targets at fast video moment retrieval (fast VMR), aiming to localize the target moment efficiently and accurately as queried by a given natural language sentence. We argue that most existing VMR approaches can be divided into three modules namely video encoder, text encoder, and cross-modal interaction module, where the last module is the test-time computational bottleneck. To tackle this issue, we replace the cross-modal interaction module with a cross-modal common space, in which moment-query alignment is learned and efficient moment search can be performed. For the sake of robustness in the learned space, we propose a fine-grained semantic distillation framework to transfer knowledge from additional semantic structures. Specifically, we build a semantic role tree that decomposes a query sentence into different phrases (subtrees). A hierarchical semantic-guided attention module is designed to perform message propagation across the whole tree and yield discriminative features. Finally, the important and discriminative semantics are transferred to the common space by a matching-score distillation process. Extensive experimental results on three popular VMR benchmarks demonstrate that our proposed method enjoys the merits of high speed and significant performance.},
  archive   = {C_ICCV},
  author    = {Junyu Gao and Changsheng Xu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00155},
  pages     = {1503-1512},
  title     = {Fast video moment retrieval},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MGSampler: An explainable sampling strategy for video action
recognition. <em>ICCV</em>, 1493–1502. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Frame sampling is a fundamental problem in video action recognition due to the essential redundancy in time and limited computation resources. The existing sampling strategy often employs a fixed frame selection and lacks the flexibility to deal with complex variations in videos. In this paper, we present a simple, sparse, and explainable frame sampler, termed as Motion-Guided Sampler (MGSampler). Our basic motivation is that motion is an important and universal signal that can drive us to adaptively select frames from videos. Accordingly, we propose two important properties in our MGSampler design: motion sensitive and motion uniform. First, we present two different motion representations to enable us to efficiently distinguish the motion-salient frames from the background. Then, we devise a motion-uniform sampling strategy based on the cumulative motion distribution to ensure the sampled frames evenly cover all the important segments with high motion salience. Our MGSampler yields a new principled and holistic sampling scheme, that could be incorporated into any existing video architecture. Experiments on five benchmarks demonstrate the effectiveness of our MGSampler over the previous fixed sampling strategies, and its generalization power across different backbones, video models, and datasets. The code is available at https://github.com/MCG-NJU/MGSampler.},
  archive   = {C_ICCV},
  author    = {Yuan Zhi and Zhan Tong and Limin Wang and Gangshan Wu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00154},
  pages     = {1493-1502},
  title     = {MGSampler: An explainable sampling strategy for video action recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vi2CLR: Video and image for visual contrastive learning of
representation. <em>ICCV</em>, 1482–1492. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce a novel self-supervised visual representation learning method which understands both images and videos in a joint learning fashion. The proposed neural network architecture and objectives are designed to obtain two different Convolutional Neural Networks for solving visual recognition tasks in the domain of videos and images. Our method called Video/Image for Visual Contrastive Learning of Representation(Vi 2 CLR) uses unlabeled videos to exploit dynamic and static visual cues for self-supervised and instances similarity/dissimilarity learning. Vi 2 CLR optimization pipeline consists of visual clustering part and representation learning based on groups of similar positive instances within a cluster and negative ones from other clusters and learning visual clusters and their distances. We show how a joint self-supervised visual clustering and instance similarity learning with 2D (image) and 3D (video) CovNet encoders yields such robust and near to supervised learning performance.We extensively evaluate the method on downstream tasks like large scale action recognition, image and object classification on datasets like Kinetics, ImageNet, Pascal VOC’07 and UCF101 and achieve outstanding results compared to state-of-the-art self-supervised methods.},
  archive   = {C_ICCV},
  author    = {Ali Diba and Vivek Sharma and Reza Safdari and Dariush Lotfi and M. Saquib Sarfraz and Rainer Stiefelhagen and Luc Van Gool},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00153},
  pages     = {1482-1492},
  title     = {Vi2CLR: Video and image for visual contrastive learning of representation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dense interaction learning for video-based person
re-identification. <em>ICCV</em>, 1470–1481. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Video-based person re-identification (re-ID) aims at matching the same person across video clips. Efficiently exploiting multi-scale fine-grained features while building the structural interaction among them is pivotal for its success. In this paper, we propose a hybrid framework, Dense Interaction Learning (DenseIL), that takes the principal advantages of both CNN-based and Attention-based architectures to tackle video-based person re-ID difficulties. DenseIL contains a CNN encoder and a Dense Interaction (DI) decoder. The CNN encoder is responsible for efficiently extracting discriminative spatial features while the DI decoder is designed to densely model spatial-temporal inherent interaction across frames. Different from previous works, we additionally let the DI decoder densely attends to intermediate fine-grained CNN features and that naturally yields multi-grained spatial-temporal representation for each video clip. Moreover, we introduce Spatio-TEmporal Positional Embedding (STEP-Emb) into the DI decoder to investigate the positional relation among the spatial-temporal inputs. Our experiments consistently and significantly outperform all the state-of-the-art methods on multiple standard video-based person re-ID datasets.},
  archive   = {C_ICCV},
  author    = {Tianyu He and Xin Jin and Xu Shen and Jianqiang Huang and Zhibo Chen and Xian-Sheng Hua},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00152},
  pages     = {1470-1481},
  title     = {Dense interaction learning for video-based person re-identification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning temporal dynamics from cycles in narrated video.
<em>ICCV</em>, 1460–1469. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning to model how the world changes as time elapses has proven a challenging problem for the computer vision community. We introduce a self-supervised approach to this problem that solves a multi-modal temporal cycle consistency objective jointly in vision and language. This objective requires a model to learn modality-agnostic functions to predict the future and past that undo each other when composed. We hypothesize that a model trained on this objective will discover long-term temporal dynamics in video. We verify this hypothesis by using the resultant visual representations and predictive models as-is to solve a variety of downstream tasks. Our method outperforms state-of-the-art self-supervised video prediction methods on future action anticipation, temporal image ordering, and arrow-of-time classification tasks, without training on target datasets or their labels.},
  archive   = {C_ICCV},
  author    = {Dave Epstein and Jiajun Wu and Cordelia Schmid and Chen Sun},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00151},
  pages     = {1460-1469},
  title     = {Learning temporal dynamics from cycles in narrated video},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Zero-shot natural language video localization.
<em>ICCV</em>, 1450–1459. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Understanding videos to localize moments with natural language often requires large expensive annotated video regions paired with language queries. To eliminate the annotation costs, we make a first attempt to train a natural language video localization model in zero-shot manner. Inspired by unsupervised image captioning setup, we merely require random text corpora, unlabeled video collections, and an off-the-shelf object detector to train a model. With the unpaired data, we propose to generate pseudo-supervision of candidate temporal regions and corresponding query sentences, and develop a simple NLVL model to train with the pseudo-supervision. Our empirical validations show that the proposed pseudo-supervised method outperforms several baseline approaches and a number of methods using stronger supervision on Charades-STA and ActivityNet-Captions.},
  archive   = {C_ICCV},
  author    = {Jinwoo Nam and Daechul Ahn and Dongyeop Kang and Seong Jong Ha and Jonghyun Choi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00150},
  pages     = {1450-1459},
  title     = {Zero-shot natural language video localization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph constrained data representation learning for human
motion segmentation. <em>ICCV</em>, 1440–1449. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, transfer subspace learning based approaches have shown to be a valid alternative to unsupervised subspace clustering and temporal data clustering for human motion segmentation (HMS). These approaches leverage prior knowledge from a source domain to improve clustering performance on a target domain, and currently they represent the state of the art in HMS. Bucking this trend, in this paper, we propose a novel unsupervised model that learns a representation of the data and digs clustering information from the data itself. Our model is reminiscent of temporal subspace clustering, but presents two critical differences. First, we learn an auxiliary data matrix that can deviate from the initial data, hence confers more degrees of freedom to the coding matrix. Second, we introduce a regularization term for this auxiliary data matrix that preserves the local geometrical structure present in the high-dimensional space. The proposed model is efficiently optimized by using an original Alternating Direction Method of Multipliers (ADMM) formulation allowing to learn jointly the auxiliary data representation, a nonnegative dictionary and a coding matrix. Experimental results on four benchmark datasets for HMS demonstrate that our approach achieves significantly better clustering performance then state-of-the-art methods, including both unsupervised and more recent semi-supervised transfer learning approaches 1 .},
  archive   = {C_ICCV},
  author    = {Mariella Dimiccoli and Lluís Garrido and Guillem Rodriguez-Corominas and Herwig Wendt},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00149},
  pages     = {1440-1449},
  title     = {Graph constrained data representation learning for human motion segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CrossCLR: Cross-modal contrastive learning for multi-modal
video representations. <em>ICCV</em>, 1430–1439. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Contrastive learning allows us to flexibly define powerful losses by contrasting positive pairs from sets of negative samples. Recently, the principle has also been used to learn cross-modal embeddings for video and text, yet without exploiting its full potential. In particular, previous losses do not take the intra-modality similarities into account, which leads to inefficient embeddings, as the same content is mapped to multiple points in the embedding space. With CrossCLR, we present a contrastive loss that fixes this issue. Moreover, we define sets of highly related samples in terms of their input embeddings and exclude them from the negative samples to avoid issues with false negatives. We show that these principles consistently improve the quality of the learned embeddings. The joint embeddings learned with CrossCLR extend the state of the art in video-text retrieval on Youcook2 and LSMDC datasets and in video captioning on Youcook2 dataset by a large margin. We also demonstrate the generality of the concept by learning improved joint embeddings for other pairs of modalities.},
  archive   = {C_ICCV},
  author    = {Mohammadreza Zolfaghari and Yi Zhu and Peter Gehler and Thomas Brox},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00148},
  pages     = {1430-1439},
  title     = {CrossCLR: Cross-modal contrastive learning for multi-modal video representations},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). UniT: Multimodal multitask learning with a unified
transformer. <em>ICCV</em>, 1419–1429. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose UniT, a Unified Transformer model to simultaneously learn the most prominent tasks across different domains, ranging from object detection to natural language understanding and multimodal reasoning. Based on the transformer encoder-decoder architecture, our UniT model encodes each input modality with an encoder and makes predictions on each task with a shared decoder over the encoded input representations, followed by task-specific output heads. The entire model is jointly trained end-to-end with losses from each task. Compared to previous efforts on multi-task learning with transformers, we share the same model parameters across all tasks instead of separately fine-tuning task-specific models and handle a much higher variety of tasks across different domains. In our experiments, we learn 7 tasks jointly over 8 datasets, achieving strong performance on each task with significantly fewer parameters. Our code is available in MMF at https://mmf.sh.},
  archive   = {C_ICCV},
  author    = {Ronghang Hu and Amanpreet Singh},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00147},
  pages     = {1419-1429},
  title     = {UniT: Multimodal multitask learning with a unified transformer},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Compressing visual-linguistic model via knowledge
distillation. <em>ICCV</em>, 1408–1418. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite exciting progress in pre-training for visual-linguistic (VL) representations, very few aspire to a small VL model. In this paper, we study knowledge distillation (KD) to effectively compress a transformer based large VL model into a small VL model. The major challenge arises from the inconsistent regional visual tokens extracted from different detectors of Teacher and Student, resulting in the misalignment of hidden representations and attention distributions. To address the problem, we retrain and adapt the Teacher by using the same region proposals from Student’s detector while the features are from Teacher’s own object detector. With aligned network inputs, the adapted Teacher is capable of transferring the knowledge through the intermediate representations. Specifically, we use the mean square error loss to mimic the attention distribution inside the transformer block, and present a token-wise noise contrastive loss to align the hidden state by contrasting with negative representations stored in a sample queue. To this end, we show that our proposed distillation significantly improves the performance of small VL models on image captioning and visual question answering tasks. It reaches 120.8 in CIDEr score on COCO captioning, an improvement of 5.1 over its non-distilled counterpart; and an accuracy of 69.8 on VQA 2.0, a 0.8 gain from the baseline. Our extensive experiments and ablations confirm the effectiveness of VL distillation in both pre-training and fine-tuning stages.},
  archive   = {C_ICCV},
  author    = {Zhiyuan Fang and Jianfeng Wang and Xiaowei Hu and Lijuan Wang and Yezhou Yang and Zicheng Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00146},
  pages     = {1408-1418},
  title     = {Compressing visual-linguistic model via knowledge distillation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Unshuffling data for improved generalization in visual
question answering. <em>ICCV</em>, 1397–1407. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generalization beyond the training distribution is a core challenge in machine learning. The common practice of mixing and shuffling examples when training neural networks may not be optimal in this regard. We show that partitioning the data into well-chosen, non-i.i.d. subsets treated as multiple training environments can guide the learning of models with better out-of-distribution generalization. We describe a training procedure to capture the patterns that are stable across environments while discarding spurious ones. The method makes a step beyond correlation-based learning: the choice of the partitioning allows injecting information about the task that cannot be otherwise recovered from the joint distribution of the training data.We demonstrate multiple use cases with the task of visual question answering, which is notorious for dataset biases. We obtain significant improvements on VQA-CP, using environments built from prior knowledge, existing meta data, or unsupervised clustering. We also get improvements on GQA using annotations of &quot;equivalent questions&quot;, and on multi-dataset training (VQA v2 / Visual Genome) by treating them as distinct environments.},
  archive   = {C_ICCV},
  author    = {Damien Teney and Ehsan Abbasnejad and Anton van den Hengel},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00145},
  pages     = {1397-1407},
  title     = {Unshuffling data for improved generalization in visual question answering},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). In defense of scene graphs for image captioning.
<em>ICCV</em>, 1387–1396. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The mainstream image captioning models rely on Convolutional Neural Network (CNN) image features to generate captions via recurrent models. Recently, image scene graphs have been used to augment captioning models so as to leverage their structural semantics, such as object entities, relationships and attributes. Several studies have noted that the naive use of scene graphs from a black-box scene graph generator harms image captioning performance and that scene graph-based captioning models have to incur the overhead of explicit use of image features to generate decent captions. Addressing these challenges, we propose SG2Caps, a framework that utilizes only the scene graph labels for competitive image captioning performance. The basic idea is to close the semantic gap between the two scene graphs - one derived from the input image and the other from its caption. In order to achieve this, we leverage the spatial location of objects and the Human-Object-Interaction (HOI) labels as an additional HOI graph. SG2Caps outperforms existing scene graph-only captioning models by a large margin, indicating scene graphs as a promising representation for image captioning. Direct utilization of scene graph labels avoids expensive graph convolutions over high-dimensional CNN features resulting in 49\% fewer trainable parameters. Our code is available at: https://github.com/Kien085/SG2Caps},
  archive   = {C_ICCV},
  author    = {Kien Nguyen and Subarna Tripathi and Bang Du and Tanaya Guha and Truong Q. Nguyen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00144},
  pages     = {1387-1396},
  title     = {In defense of scene graphs for image captioning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Synthesis of compositional animations from textual
descriptions. <em>ICCV</em>, 1376–1386. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {&quot;How can we animate 3D-characters from a movie script or move robots by simply telling them what we would like them to do?&quot; &quot;How unstructured and complex can we make a sentence and still generate plausible movements from it?&quot; These are questions that need to be answered in the long-run, as the field is still in its infancy. Inspired by these problems, we present a new technique for generating compositional actions, which handles complex input sentences. Our output is a 3D pose sequence depicting the actions in the input sentence. We propose a hierarchical two-stream sequential model to explore a finer joint-level mapping between natural language sentences and 3D pose sequences corresponding to the given motion. We learn two manifold representations of the motion, one each for the upper body and the lower body movements. Our model can generate plausible pose sequences for short sentences describing single actions as well as long complex sentences describing multiple sequential and compositional actions. We evaluate our proposed model on the publicly available KIT Motion-Language Dataset containing 3D pose data with human-annotated sentences. Experimental results show that our model advances the state-of-the-art on text-based motion synthesis in objective evaluations by a margin of 50\%. Qualitative evaluations based on a user study indicate that our synthesized motions are perceived to be the closest to the ground-truth motion captures for both short and compositional sentences.},
  archive   = {C_ICCV},
  author    = {Anindita Ghosh and Noshaba Cheema and Cennet Oguz and Christian Theobalt and Philipp Slusallek},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00143},
  pages     = {1376-1386},
  title     = {Synthesis of compositional animations from textual descriptions},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021c). YouRefIt: Embodied reference understanding with language
and gesture. <em>ICCV</em>, 1365–1375. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the machine’s understanding of embodied reference: One agent uses both language and gesture to refer to an object to another agent in a shared physical environment. Of note, this new visual task requires understanding multimodal cues with perspective-taking to identify which object is being referred to. To tackle this problem, we introduce YouRefIt, a new crowd-sourced dataset of embodied reference collected in various physical scenes; the dataset contains 4,195 unique reference clips in 432 indoor scenes. To the best of our knowledge, this is the first embodied reference dataset that allows us to study referring expressions in daily physical scenes to understand referential behavior, human communication, and human-robot interaction. We further devise two benchmarks for image-based and video-based embodied reference understanding. Comprehensive baselines and extensive experiments provide the very first result of machine perception on how the referring expressions and gestures affect the embodied reference understanding. Our results provide essential evidence that gestural cues are as critical as language cues in understanding the embodied reference.},
  archive   = {C_ICCV},
  author    = {Yixin Chen and Qing Li and Deqian Kong and Yik Lun Kei and Song-Chun Zhu and Tao Gao and Yixin Zhu and Siyuan Huang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00142},
  pages     = {1365-1375},
  title     = {YouRefIt: Embodied reference understanding with language and gesture},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Who’s waldo? Linking people across text and images.
<em>ICCV</em>, 1354–1364. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a task and benchmark dataset for person-centric visual grounding, the problem of linking between people named in a caption and people pictured in an image. In contrast to prior work in visual grounding, which is predominantly object-based, our new task masks out the names of people in captions in order to encourage methods trained on such image–caption pairs to focus on contextual cues, such as the rich interactions between multiple people, rather than learning associations between names and appearances. To facilitate this task, we introduce a new dataset, Who’s Waldo, mined automatically from image–caption data on Wikimedia Commons. We propose a Transformer-based method that outperforms several strong baselines on this task, and release our data to the research community to spur work on contextual models that consider both vision and language. Code and data are available at: https://whoswaldo.github.io},
  archive   = {C_ICCV},
  author    = {Claire Yuqing Cui and Apoorv Khandelwal and Yoav Artzi and Noah Snavely and Hadar Averbuch-Elor},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00141},
  pages     = {1354-1364},
  title     = {Who’s waldo? linking people across text and images},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Panoptic narrative grounding. <em>ICCV</em>, 1344–1353. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes Panoptic Narrative Grounding, a spatially fine and general formulation of the natural language visual grounding problem. We establish an experimental framework for the study of this new task, including new ground truth and metrics, and we propose a strong baseline method to serve as stepping stone for future work. We exploit the intrinsic semantic richness in an image by including panoptic categories, and we approach visual grounding at a fine-grained level by using segmentations. In terms of ground truth, we propose an algorithm to automatically transfer Localized Narratives annotations to specific regions in the panoptic segmentations of the MS COCO dataset. To guarantee the quality of our annotations, we take advantage of the semantic structure contained in WordNet to exclusively incorporate noun phrases that are grounded to a meaningfully related panoptic segmentation region. The proposed baseline achieves a performance of 55.4 absolute Average Recall points. This result is a suitable foundation to push the envelope further in the development of methods for Panoptic Narrative Grounding.},
  archive   = {C_ICCV},
  author    = {Cristina González and Nicolás Ayobi and Isabela Hernández and José Hernández and Jordi Pont-Tuset and Pablo Arbeláez},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00140},
  pages     = {1344-1353},
  title     = {Panoptic narrative grounding},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LFI-CAM: Learning feature importance for better visual
explanation. <em>ICCV</em>, 1335–1343. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Class Activation Mapping (CAM) is a powerful technique used to understand the decision making of Convolutional Neural Network (CNN) in computer vision. Recently, there have been attempts not only to generate better visual explanations, but also to improve classification performance using visual explanations. However, previous works still have their own drawbacks. In this paper, we propose a novel architecture, LFI-CAM *** (Learning Feature Importance Class Activation Mapping), which is trainable for image classification and visual explanation in an end-to-end manner. LFI-CAM generates attention map for visual explanation during forward propagation, and simultaneously uses attention map to improve classification performance through the attention mechanism. Feature Importance Network (FIN) focuses on learning the feature importance instead of directly learning the attention map to obtain a more reliable and consistent attention map. We confirmed that LFI-CAM is optimized not only by learning the feature importance but also by enhancing the backbone feature representation to focus more on important features of the input image. Experiments show that LFI-CAM outperforms baseline models’ accuracy on classification tasks as well as significantly improves on previous works in terms of attention map quality and stability over different hyper-parameters.},
  archive   = {C_ICCV},
  author    = {Kwang Hee Lee and Chaewon Park and Junghyun Oh and Nojun Kwak},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00139},
  pages     = {1335-1343},
  title     = {LFI-CAM: Learning feature importance for better visual explanation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Finding representative interpretations on convolutional
neural networks. <em>ICCV</em>, 1325–1334. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Interpreting the decision logic behind effective deep convolutional neural networks (CNN) on images complements the success of deep learning models. However, the existing methods can only interpret some specific decision logic on individual or a small number of images. To facilitate human understandability and generalization ability, it is important to develop representative interpretations that interpret common decision logics of a CNN on a large group of similar images, which reveal the common semantics data contributes to many closely related predictions. In this paper, we develop a novel unsupervised approach to produce a highly representative interpretation for a large number of similar images. We formulate the problem of finding representative interpretations as a co-clustering problem, and convert it into a submodular cost submodular cover problem based on a sample of the linear decision boundaries of a CNN. We also present a visualization and similarity ranking method. Our extensive experiments demonstrate the excellent performance of our method.},
  archive   = {C_ICCV},
  author    = {Peter Cho-Ho Lam and Lingyang Chu and Maxim Torgonskiy and Jian Pei and Yong Zhang and Lanjun Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00138},
  pages     = {1325-1334},
  title     = {Finding representative interpretations on convolutional neural networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards better explanations of class activation mapping.
<em>ICCV</em>, 1316–1324. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Increasing demands for understanding the internal behavior of convolutional neural networks (CNNs) have led to remarkable improvements in explanation methods. Particularly, several class activation mapping (CAM) based methods, which generate visual explanation maps by a linear combination of activation maps from CNNs, have been proposed. However, the majority of the methods lack a clear theoretical basis on how they assign the coefficients of the linear combination. In this paper, we revisit the intrinsic linearity of CAM with respect to the activation maps; we construct an explanation model of CNN as a linear function of binary variables that denote the existence of the corresponding activation maps. With this approach, the explanation model can be determined by additive feature attribution methods in an analytic manner. We then demonstrate the adequacy of SHAP values, which is a unique solution for the explanation model with a set of desirable properties, as the coefficients of CAM. Since the exact SHAP values are unattainable, we introduce an efficient approximation method, LIFT-CAM, based on DeepLIFT. Our proposed LIFT-CAM can estimate the SHAP values of the activation maps with high speed and accuracy. Furthermore, it greatly outperforms other previous CAM-based methods in both qualitative and quantitative aspects.},
  archive   = {C_ICCV},
  author    = {Hyungsik Jung and Youngrock Oh},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00137},
  pages     = {1316-1324},
  title     = {Towards better explanations of class activation mapping},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Towards learning spatially discriminative feature
representations. <em>ICCV</em>, 1306–1315. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The backbone of traditional CNN classifier is generally considered as a feature extractor, followed by a linear layer which performs the classification. We propose a novel loss function, termed as CAM-loss, to constrain the embedded feature maps with the class activation maps (CAMs) which indicate the spatially discriminative regions of an image for particular categories. CAM-loss drives the backbone to express the features of target category and suppress the features of non-target categories or background, so as to obtain more discriminative feature representations. It can be simply applied in any CNN architecture with neglectable additional parameters and calculations. Experimental results show that CAM-loss is applicable to a variety of network structures and can be combined with mainstream regularization methods to improve the performance of image classification. The strong generalization ability of CAMloss is validated in the transfer learning and few shot learning tasks. Based on CAM-loss, we also propose a novel CAAM-CAM matching knowledge distillation method. This method directly uses the CAM generated by the teacher network to supervise the CAAM generated by the student network, which effectively improves the accuracy and convergence rate of the student network.},
  archive   = {C_ICCV},
  author    = {Chaofei Wang and Jiayu Xiao and Yizeng Han and Qisen Yang and Shiji Song and Gao Huang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00136},
  pages     = {1306-1315},
  title     = {Towards learning spatially discriminative feature representations},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Shape-biased domain generalization via shock graph
embeddings. <em>ICCV</em>, 1295–1305. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There is an emerging sense that the vulnerability of Image Convolutional Neural Networks (CNN), i.e., sensitivity to image corruptions, perturbations, and adversarial attacks, is connected with Texture Bias. This relative lack of Shape Bias is also responsible for poor performance in Domain Generalization (DG). The inclusion of a role of shape alleviates these vulnerabilities and some approaches have achieved this by training on negative images, images endowed with edge maps, or images with conflicting shape and texture information. This paper advocates an explicit and complete representation of shape using a classical computer vision approach, namely, representing the shape content of an image with the shock graph of its contour map. The resulting graph and its descriptor is a complete representation of contour content and is classified using recent Graph Neural Network (GNN) methods. The experimental results on three domain shift datasets, Colored MNIST, PACS, and VLCS demonstrate that even without using appearance the shape-based approach exceeds classical Image CNN based methods in domain generalization.},
  archive   = {C_ICCV},
  author    = {Maruthi Narayanan and Vickram Rajendran and Benjamin Kimia},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00135},
  pages     = {1295-1305},
  title     = {Shape-biased domain generalization via shock graph embeddings},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive boundary proposal network for arbitrary shape text
detection. <em>ICCV</em>, 1285–1294. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Arbitrary shape text detection is a challenging task due to the high complexity and variety of scene texts. In this work, we propose a novel adaptive boundary proposal network for arbitrary shape text detection, which can learn to directly produce accurate boundary for arbitrary shape text without any post-processing. Our method mainly consists of a boundary proposal model and an innovative adaptive boundary deformation model. The boundary proposal model constructed by multi-layer dilated convolutions is adopted to produce prior information (including classification map, distance field, and direction field) and coarse boundary proposals. The adaptive boundary deformation model is an encoder-decoder network, in which the encoder mainly consists of a Graph Convolutional Network (GCN) and a Recurrent Neural Network (RNN). It aims to perform boundary deformation in an iterative way for obtaining text instance shape guided by prior information from the boundary proposal model. In this way, our method can directly and efficiently generate accurate text boundaries without complex post-processing. Extensive experiments on publicly available datasets demonstrate the state-of-the-art performance of our method. Code is available at the website: https://github.com/GXYM/TextBPN.},
  archive   = {C_ICCV},
  author    = {Shi-Xue Zhang and Xiaobin Zhu and Chun Yang and Hongfa Wang and Xu-Cheng Yin},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00134},
  pages     = {1285-1294},
  title     = {Adaptive boundary proposal network for arbitrary shape text detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TGRNet: A table graph reconstruction network for table
structure recognition. <em>ICCV</em>, 1275–1284. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A table arranging data in rows and columns is a very effective data structure, which has been widely used in business and scientific research. Considering large-scale tabular data in online and offline documents, automatic table recognition has attracted increasing attention from the document analysis community. Though human can easily understand the structure of tables, it remains a challenge for machines to understand that, especially due to a variety of different table layouts and styles. Existing methods usually model a table as either the markup sequence or the adjacency matrix between different table cells, failing to address the importance of the logical location of table cells, e.g., a cell is located in the first row and the second column of the table. In this paper, we reformulate the problem of table structure recognition as the table graph reconstruction, and propose an end-to-end trainable table graph reconstruction network (TGRNet) for table structure recognition. Specifically, the proposed method has two main branches, a cell detection branch and a cell logical location branch, to jointly predict the spatial location and the logical location of different cells. Experimental results on three popular table recognition datasets and a new dataset with table graph annotations (TableGraph-350K) demonstrate the effectiveness of the proposed TGRNet for table structure recognition. Code and annotations will be made publicly available at https://github.com/xuewenyuan/TGRNet.},
  archive   = {C_ICCV},
  author    = {Wenyuan Xue and Baosheng Yu and Wen Wang and Dacheng Tao and Qingyong Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00133},
  pages     = {1275-1284},
  title     = {TGRNet: A table graph reconstruction network for table structure recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to discover reflection symmetry via polar matching
convolution. <em>ICCV</em>, 1265–1274. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The task of reflection symmetry detection remains challenging due to significant variations and ambiguities of symmetry patterns in the wild. Furthermore, since the local regions are required to match in reflection for detecting a symmetry pattern, it is hard for standard convolutional networks, which are not equivariant to rotation and reflection, to learn the task. To address the issue, we introduce a new convolutional technique, dubbed the polar matching convolution, which leverages a polar feature pooling, a self-similarity encoding, and a systematic kernel design for axes of different angles. The proposed high-dimensional kernel convolution network effectively learns to discover symmetry patterns from real-world images, overcoming the limitations of standard convolution. In addition, we present a new dataset and introduce a self-supervised learning strategy by augmenting the dataset with synthesizing images. Experiments demonstrate that our method outperforms state-of-the-art methods in terms of accuracy and robustness.},
  archive   = {C_ICCV},
  author    = {Ahyun Seo and Woohyeon Shim and Minsu Cho},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00132},
  pages     = {1265-1274},
  title     = {Learning to discover reflection symmetry via polar matching convolution},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Embed me if you can: A geometric perceptron. <em>ICCV</em>,
1256–1264. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Solving geometric tasks involving point clouds by using machine learning is a challenging problem. Standard feed-forward neural networks combine linear or, if the bias parameter is included, affine layers and activation functions. Their geometric modeling is limited, which motivated the prior work introducing the multilayer hypersphere perceptron (MLHP). Its constituent part, i.e., the hypersphere neuron, is obtained by applying a conformal embedding of Euclidean space. By virtue of Clifford algebra, it can be implemented as the Cartesian dot product of inputs and weights. If the embedding is applied in a manner consistent with the dimensionality of the input space geometry, the decision surfaces of the model units become combinations of hyperspheres and make the decision-making process geometrically interpretable for humans. Our extension of the MLHP model, the multilayer geometric perceptron (MLGP), and its respective layer units, i.e., geometric neurons, are consistent with the 3D geometry and provide a geometric handle of the learned coefficients. In particular, the geometric neuron activations are isometric in 3D, which is necessary for rotation and translation equivariance. When classifying the 3D Tetris shapes, we quantitatively show that our model requires no activation function in the hidden layers other than the embedding to outperform the vanilla multilayer perceptron. In the presence of noise in the data, our model is also superior to the MLHP.},
  archive   = {C_ICCV},
  author    = {Pavlo Melnyk and Michael Felsberg and Mårten Wadenbäck},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00131},
  pages     = {1256-1264},
  title     = {Embed me if you can: A geometric perceptron},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Hypergraph neural networks for hypergraph matching.
<em>ICCV</em>, 1246–1255. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hypergraph matching is a useful tool to find feature correspondence by considering higher-order structural information. Recently, the employment of deep learning has made great progress in the matching of graphs, suggesting its potential for hypergraphs. Hence, in this paper, we present the first, to our best knowledge, unified hypergraph neural network (HNN) solution for hypergraph matching. Specifically, given two hypergraphs to be matched, we first construct an association hypergraph over them and convert the hypergraph matching problem into a node classification problem on the association hypergraph. Then, we design a novel hypergraph neural network to effectively solve the node classification problem. Being end-to-end trainable, our proposed method, named HNN-HM, jointly learns all its components with improved optimization. For evaluation, HNN-HM is tested on various benchmarks and shows a clear advantage over state-of-the-arts.},
  archive   = {C_ICCV},
  author    = {Xiaowei Liao and Yong Xu and Haibin Ling},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00130},
  pages     = {1246-1255},
  title     = {Hypergraph neural networks for hypergraph matching},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Broaden your views for self-supervised video learning.
<em>ICCV</em>, 1235–1245. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most successful self-supervised learning methods are trained to align the representations of two independent views from the data. State-of-the-art methods in video are inspired by image techniques, where these two views are similarly extracted by cropping and augmenting the resulting crop. However, these methods miss a crucial element in the video domain: time. We introduce BraVe, a self-supervised learning framework for video. In BraVe, one of the views has access to a narrow temporal window of the video while the other view has a broad access to the video content. Our models learn to generalise from the narrow view to the general content of the video. Furthermore, BraVe processes the views with different backbones, enabling the use of alternative augmentations or modalities into the broad view such as optical flow, randomly convolved RGB frames, audio or their combinations. We demonstrate that BraVe achieves state-of-the-art results in self-supervised representation learning on standard video and audio classification benchmarks including UCF101, HMDB51, Kinetics, ESC-50 and AudioSet.},
  archive   = {C_ICCV},
  author    = {Adrià Recasens and Pauline Luc and Jean-Baptiste Alayrac and Luyu Wang and Florian Strub and Corentin Tallec and Mateusz Malinowski and Viorica Pătrăaucean and Florent Altché and Michal Valko and Jean-Bastien Grill and Aäron van den Oord and Andrew Zisserman},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00129},
  pages     = {1235-1245},
  title     = {Broaden your views for self-supervised video learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). E-ViL: A dataset and benchmark for natural language
explanations in vision-language tasks. <em>ICCV</em>, 1224–1234. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, there has been an increasing number of efforts to introduce models capable of generating natural language explanations (NLEs) for their predictions on vision-language (VL) tasks. Such models are appealing, because they can provide human-friendly and comprehensive explanations. However, there is a lack of comparison between existing methods, which is due to a lack of re-usable evaluation frameworks and a scarcity of datasets. In this work, we introduce e-ViL and e-SNLI-VE. e-ViL is a benchmark for explainable vision-language tasks that establishes a unified evaluation framework and provides the first comprehensive comparison of existing approaches that generate NLEs for VL tasks. It spans four models and three datasets and both automatic metrics and human evaluation are used to assess model-generated explanations. e-SNLI-VE is currently the largest existing VL dataset with NLEs (over 430k instances). We also propose a new model that combines UNITER [15], which learns joint embeddings of images and text, and GPT-2 [38], a pre-trained language model that is well-suited for text generation. It surpasses the previous state of the art by a large margin across all datasets. Code and data are available here: https://github.com/maximek3/e-ViL.},
  archive   = {C_ICCV},
  author    = {Maxime Kayser and Oana-Maria Camburu and Leonard Salewski and Cornelius Emde and Virginie Do and Zeynep Akata and Thomas Lukasiewicz},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00128},
  pages     = {1224-1234},
  title     = {E-ViL: A dataset and benchmark for natural language explanations in vision-language tasks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Explanations for occluded images. <em>ICCV</em>, 1214–1223.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing algorithms for explaining the output of image classifiers perform poorly on inputs where the object of interest is partially occluded. We present a novel, black-box algorithm for computing explanations that uses a principled approach based on causal theory. We have implemented the method in the DEEPCOVER tool. We obtain explanations that are much more accurate than those generated by the existing explanation tools on images with occlusions and observe a level of performance comparable to the state of the art when explaining images without occlusions.},
  archive   = {C_ICCV},
  author    = {Hana Chockler and Daniel Kroening and Youcheng Sun},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00127},
  pages     = {1214-1223},
  title     = {Explanations for occluded images},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Explaining local, global, and higher-order interactions in
deep learning. <em>ICCV</em>, 1204–1213. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a simple yet highly generalizable method for explaining interacting parts within a neural network’s reasoning process. First, we design an algorithm based on cross derivatives for computing statistical interaction effects between individual features, which is generalized to both 2-way and higher-order (3-way or more) interactions. We present results side by side with a weight-based attribution technique, corroborating that cross derivatives are a superior metric for both 2-way and higher-order interaction detection. Moreover, we extend the use of cross derivatives as an explanatory device in neural networks to the computer vision setting by expanding Grad-CAM, a popular gradient-based explanatory tool for CNNs, to the higher order. While Grad-CAM can only explain the importance of individual objects in images, our method, which we call Taylor-CAM, can explain a neural network’s relational reasoning across multiple objects. We show the success of our explanations both qualitatively and quantitatively, including with a user study. We will release all code as a tool package to facilitate explainable deep learning.},
  archive   = {C_ICCV},
  author    = {Samuel Lerman and Charles Venuto and Henry Kautz and Chenliang Xu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00126},
  pages     = {1204-1213},
  title     = {Explaining local, global, and higher-order interactions in deep learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Better aggregation in test-time augmentation. <em>ICCV</em>,
1194–1203. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Test-time augmentation—the aggregation of predictions across transformed versions of a test input—is a common practice in image classification. Traditionally, predictions are combined using a simple average. In this paper, we present 1) experimental analyses that shed light on cases in which the simple average is suboptimal and 2) a method to address these shortcomings. A key finding is that even when test-time augmentation produces a net improvement in accuracy, it can change many correct predictions into incorrect predictions. We delve into when and why test-time augmentation changes a prediction from being correct to incorrect and vice versa. Building on these insights, we present a learning-based method for aggregating test-time augmentations. Experiments across a diverse set of models, datasets, and augmentations show that our method delivers consistent improvements over existing approaches.},
  archive   = {C_ICCV},
  author    = {Divya Shanmugam and Davis Blalock and Guha Balakrishnan and John Guttag},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00125},
  pages     = {1194-1203},
  title     = {Better aggregation in test-time augmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual scene graphs for audio source separation.
<em>ICCV</em>, 1184–1193. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {State-of-the-art approaches for visually-guided audio source separation typically assume sources that have characteristic sounds, such as musical instruments. These approaches often ignore the visual context of these sound sources or avoid modeling object interactions that may be useful to better characterize the sources, especially when the same object class may produce varied sounds from distinct interactions. To address this challenging problem, we propose Audio Visual Scene Graph Segmenter (AVSGS), a novel deep learning model that embeds the visual structure of the scene as a graph and segments this graph into subgraphs, each subgraph being associated with a unique sound obtained by co-segmenting the audio spectrogram. At its core, AVSGS uses a recursive neural network that emits mutually-orthogonal sub-graph embeddings of the visual graph using multi-head attention. These embeddings are used for conditioning an audio encoder-decoder towards source separation. Our pipeline is trained end-to-end via a self-supervised task consisting of separating audio sources using the visual graph from artificially mixed sounds.In this paper, we also introduce an “in the wild” video dataset for sound source separation that contains multiple non-musical sources, which we call Audio Separation in the Wild (ASIW). This dataset is adapted from the AudioCaps dataset, and provides a challenging, natural, and daily-life setting for source separation. Thorough experiments on the proposed ASIW and the standard MUSIC datasets demonstrate state-of-the-art sound separation performance of our method against recent prior approaches.},
  archive   = {C_ICCV},
  author    = {Moitreya Chatterjee and Jonathan Le Roux and Narendra Ahuja and Anoop Cherian},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00124},
  pages     = {1184-1193},
  title     = {Visual scene graphs for audio source separation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). How to design a three-stage architecture for audio-visual
active speaker detection in the wild. <em>ICCV</em>, 1173–1183. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Successful active speaker detection requires a three-stage pipeline: (i) audio-visual encoding for all speakers in the clip, (ii) inter-speaker relation modeling between a reference speaker and the background speakers within each frame, and (iii) temporal modeling for the reference speaker. Each stage of this pipeline plays an important role for the final performance of the created architecture. Based on a series of controlled experiments, this work presents several practical guidelines for audio-visual active speaker detection. Correspondingly, we present a new architecture called ASDNet, which achieves a new state-of-the-art on the AVA-ActiveSpeaker dataset with a mAP of 93.5\% outperforming the second best with a large margin of 4.7\%. Our code and pretrained models are publicly available 1 .},
  archive   = {C_ICCV},
  author    = {Okan Köpüklü and Maja Taseska and Gerhard Rigoll},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00123},
  pages     = {1173-1183},
  title     = {How to design a three-stage architecture for audio-visual active speaker detection in the wild},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Audio-visual floorplan reconstruction. <em>ICCV</em>,
1163–1172. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Given only a few glimpses of an environment, how much can we infer about its entire floorplan? Existing methods can map only what is visible or immediately apparent from context, and thus require substantial movements through a space to fully map it. We explore how both audio and visual sensing together can provide rapid floorplan reconstruction from limited viewpoints. Audio not only helps sense geometry outside the camera’s field of view, but it also reveals the existence of distant freespace (e.g., a dog barking in another room) and suggests the presence of rooms not visible to the camera (e.g., a dishwasher humming in what must be the kitchen to the left). We introduce AV-Map, a novel multi-modal encoder-decoder framework that reasons jointly about audio and vision to reconstruct a floorplan from a short input video sequence. We train our model to predict both the interior structure of the environment and the associated rooms’ semantic labels. Our results on 85 large real-world environments show the impact: with just a few glimpses spanning 26\% of an area, we can estimate the whole area with 66\% accuracy—substantially better than the state of the art approach for extrapolating visual maps.},
  archive   = {C_ICCV},
  author    = {Senthil Purushwalkam and Sebastià Vicenc Amengual Garí and Vamsi Krishna Ithapu and Carl Schissler and Philip Robinson and Abhinav Gupta and Kristen Grauman},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00122},
  pages     = {1163-1172},
  title     = {Audio-visual floorplan reconstruction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MeshTalk: 3D face animation from speech using cross-modality
disentanglement. <em>ICCV</em>, 1153–1162. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a generic method for generating full facial 3D animation from speech. Existing approaches to audio-driven facial animation exhibit uncanny or static upper face animation, fail to produce accurate and plausible co-articulation or rely on person-specific models that limit their scalability. To improve upon existing models, we propose a generic audio-driven facial animation approach that achieves highly realistic motion synthesis results for the entire face. At the core of our approach is a categorical latent space for facial animation that disentangles audio-correlated and audio-uncorrelated information based on a novel cross-modality loss. Our approach ensures highly accurate lip motion, while also synthesizing plausible animation of the parts of the face that are uncorrelated to the audio signal, such as eye blinks and eye brow motion. We demonstrate that our approach outperforms several baselines and obtains state-of-the-art quality both qualitatively and quantitatively. A perceptual user study demonstrates that our approach is deemed more realistic than the current state-of-the-art in over 75\% of cases. We recommend watching the supplemental video before reading the paper: https://github.com/facebookresearch/meshtalk},
  archive   = {C_ICCV},
  author    = {Alexander Richard and Michael Zollhöfer and Yandong Wen and Fernando de la Torre and Yaser Sheikh},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00121},
  pages     = {1153-1162},
  title     = {MeshTalk: 3D face animation from speech using cross-modality disentanglement},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). IDARTS: Interactive differentiable architecture search.
<em>ICCV</em>, 1143–1152. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Differentiable Architecture Search (DARTS) improves the efficiency of architecture search by learning the architecture and network parameters end-to-end. However, the intrinsic relationship between the architecture’s parameters is neglected, leading to a sub-optimal optimization process. The reason lies in the fact that the gradient descent method used in DARTS ignores the coupling relationship of the parameters and therefore degrades the optimization. In this paper, we address this issue by formulating DARTS as a bi-linear optimization problem and introducing an Interactive Differentiable Architecture Search (IDARTS). We first develop a backtracking backpropagation process, which can decouple the relationships of different kinds of parameters and train them in the same framework. The backtracking method coordinates the training of different parameters that fully explore their interaction and optimize training. We present experiments on the CIFAR10 and ImageNet datasets that demonstrate the efficacy of the IDARTS approach by achieving a top-1 accuracy of 76.52\% on ImageNet without additional search cost vs. 75.8\% with the state-of-the-art PC-DARTS.},
  archive   = {C_ICCV},
  author    = {Song Xue and Runqi Wang and Baochang Zhang and Tian Wang and Guodong Guo and David Doermann},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00120},
  pages     = {1143-1152},
  title     = {IDARTS: Interactive differentiable architecture search},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CODEs: Chamfer out-of-distribution examples against
overconfidence issue. <em>ICCV</em>, 1133–1142. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Overconfident predictions on out-of-distribution (OOD) samples is a thorny issue for deep neural networks. The key to resolve the OOD overconfidence issue inherently is to build a subset of OOD samples and then suppress predictions on them. This paper proposes the Chamfer OOD examples (CODEs), whose distribution is close to that of in-distribution samples, and thus could be utilized to alleviate the OOD overconfidence issue effectively by suppressing predictions on them. To obtain CODEs, we first generate seed OOD examples via slicing&amp;splicing operations on in-distribution samples from different categories, and then feed them to the Chamfer generative adversarial network for distribution transformation, without accessing to any extra data. Training with suppressing predictions on CODEs is validated to alleviate the OOD overconfidence issue largely without hurting classification accuracy, and outperform the state-of-the-art methods. Besides, we demonstrate CODEs are useful for improving OOD detection and classification.},
  archive   = {C_ICCV},
  author    = {Keke Tang and Dingruibo Miao and Weilong Peng and Jianpeng Wu and Yawen Shi and Zhaoquan Gu and Zhihong Tian and Wenping Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00119},
  pages     = {1133-1142},
  title     = {CODEs: Chamfer out-of-distribution examples against overconfidence issue},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Transforms based tensor robust PCA: Corrupted low-rank
tensors recovery via convex optimization. <em>ICCV</em>, 1125–1132. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work studies the Tensor Robust Principal Component Analysis (TRPCA) problem, which aims to exactly recover the low-rank and sparse components from their sum. Our model is motivated by the recently proposed linear transforms based tensor-tensor product and tensor SVD. We define a new transforms depended tensor rank and the corresponding tensor nuclear norm. Then we solve the TR-PCA problem by convex optimization whose objective is a weighted combination of the new tensor nuclear norm and ℓ 1 -norm. In theory, we prove that under some incoherence conditions, the convex program exactly recovers the underlying low-rank and sparse components with high probability. Our new TRPCA is much more general since it allows to use any invertible linear transforms. Thus, we have more choices in practice for different tasks and different type of data. Numerical experiments verify our results and the application on image recovery demonstrates the superiority of our method.},
  archive   = {C_ICCV},
  author    = {Canyi Lu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00118},
  pages     = {1125-1132},
  title     = {Transforms based tensor robust PCA: Corrupted low-rank tensors recovery via convex optimization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predicting with confidence on unseen distributions.
<em>ICCV</em>, 1114–1124. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent work has shown that the accuracy of machine learning models can vary substantially when evaluated on a distribution that even slightly differs from that of the training data. As a result, predicting model performance on previously unseen distributions without access to labeled data is an important challenge with implications for increasing the reliability of machine learning models. In the context of distribution shift, distance measures are often used to adapt models and improve their performance on new domains, however accuracy estimation is seldom explored in these investigations. Our investigation determines that common distributional distances such as Frechet distance or Maximum Mean Discrepancy, fail to induce reliable estimates of performance under distribution shift. On the other hand, we find that our proposed difference of confidences (DoC) approach yields successful estimates of a classifier’s performance over a variety of shifts and model architectures. Despite its simplicity, we observe that DoC outperforms other methods across synthetic, natural, and adversarial distribution shifts, reducing error by (&gt; 46\%) on several realistic and challenging datasets such as ImageNet-Vid-Robust and ImageNet-Rendition.},
  archive   = {C_ICCV},
  author    = {Devin Guillory and Vaishaal Shankar and Sayna Ebrahimi and Trevor Darrell and Ludwig Schmidt},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00117},
  pages     = {1114-1124},
  title     = {Predicting with confidence on unseen distributions},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Striking a balance between stability and plasticity for
class-incremental learning. <em>ICCV</em>, 1104–1113. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Class-incremental learning (CIL) aims at continuously updating a trained model with new classes (plasticity) without forgetting previously learned old ones (stability). Contemporary studies resort to storing representative exemplars for rehearsal or preventing consolidated model parameters from drifting, but the former requires an additional space for storing exemplars at every incremental phase while the latter usually shows poor model generalization. In this paper, we focus on resolving the stability-plasticity dilemma in class-incremental learning where no exemplars from old classes are stored. To make a trade-off between learning new information and maintaining old knowledge, we reformulate a simple yet effective baseline method based on a cosine classifier framework and reciprocal adaptive weights. With the reformulated baseline, we present two new approaches to CIL by learning class-independent knowledge and multi-perspective knowledge, respectively. The former exploits class-independent knowledge to bridge learning new and old classes, while the latter learns knowledge from different perspectives to facilitate CIL. Extensive experiments on several widely used CIL benchmark datasets show the superiority of our approaches over the state-of-the-art methods.},
  archive   = {C_ICCV},
  author    = {Guile Wu and Shaogang Gong and Pan Li Queen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00116},
  pages     = {1104-1113},
  title     = {Striking a balance between stability and plasticity for class-incremental learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Why approximate matrix square root outperforms accurate SVD
in global covariance pooling? <em>ICCV</em>, 1095–1103. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Global Covariance Pooling (GCP) aims at exploiting the second-order statistics of the convolutional feature. Its effectiveness has been demonstrated in boosting the classification performance of Convolutional Neural Networks (CNNs). Singular Value Decomposition (SVD) is used in GCP to compute the matrix square root. However, the approximate matrix square root calculated using Newton-Schulz iteration [14] outperforms the accurate one computed via SVD [15]. We empirically analyze the reason behind the performance gap from the perspectives of data precision and gradient smoothness. Various remedies for computing smooth SVD gradients are investigated. Based on our observation and analyses, a hybrid training protocol is proposed for SVD-based GCP meta-layers such that competitive performances can be achieved against Newton-Schulz iteration. Moreover, we propose a new GCP meta-layer that uses SVD in the forward pass, and Padé approximants in the backward propagation to compute the gradients. The proposed meta-layer has been integrated into different CNN models and achieves state-of-the-art performances on both large-scale and fine-grained datasets.},
  archive   = {C_ICCV},
  author    = {Yue Song and Nicu Sebe and Wei Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00115},
  pages     = {1095-1103},
  title     = {Why approximate matrix square root outperforms accurate SVD in global covariance pooling?},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The right to talk: An audio-visual transformer approach.
<em>ICCV</em>, 1085–1094. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Turn-taking has played an essential role in structuring the regulation of a conversation. The task of identifying the main speaker (who is properly taking his/her turn of speaking) and the interrupters (who are interrupting or reacting to the main speaker’s utterances) remains a challenging task. Although some prior methods have partially addressed this task, there still remain some limitations. Firstly, a direct association of Audio and Visual features may limit the correlations to be extracted due to different modalities. Secondly, the relationship across temporal segments helping to maintain the consistency of localization, separation and conversation contexts is not effectively exploited. Finally, the interactions between speakers that usually contain the tracking and anticipatory decisions about transition to a new speaker is usually ignored. Therefore, this work introduces a new Audio-Visual Transformer approach to the problem of localization and highlighting the main speaker in both audio and visual channels of a multi-speaker conversation video in the wild. The proposed method exploits different types of correlations presented in both visual and audio signals. The temporal audio-visual relationships across spatial-temporal space are anticipated and optimized via the self-attention mechanism in a Transformer structure. Moreover, a newly collected dataset is introduced for the main speaker detection. To the best of our knowledge, it is one of the first studies that is able to automatically localize and highlight the main speaker in both visual and audio channels in multi-speaker conversation videos.},
  archive   = {C_ICCV},
  author    = {Thanh-Dat Truong and Chi Nhan Duong and The De Vu and Hoang Anh Pham and Bhiksha Raj and Ngan Le and Khoa Luu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00114},
  pages     = {1085-1094},
  title     = {The right to talk: An audio-visual transformer approach},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interpreting attributions and interactions of adversarial
attacks. <em>ICCV</em>, 1075–1084. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper aims to explain adversarial attacks in terms of how adversarial perturbations contribute to the attacking task. We estimate attributions of different image regions to the decrease of the attacking cost based on the Shapley value. We define and quantify interactions among adversarial perturbation pixels, and decompose the entire perturbation map into relatively independent perturbation components. The decomposition of the perturbation map shows that adversarially-trained DNNs have more perturbation components in the foreground than normally-trained DNNs. Moreover, compared to the normally-trained DNN, the adversarially-trained DNN have more components which mainly decrease the score of the true category. Above analyses provide new insights into the understanding of adversarial attacks.},
  archive   = {C_ICCV},
  author    = {Xin Wang and Shuyun Lin and Hao Zhang and Yufei Zhu and Quanshi Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00113},
  pages     = {1075-1084},
  title     = {Interpreting attributions and interactions of adversarial attacks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Handwriting transformers. <em>ICCV</em>, 1066–1074. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel transformer-based styled handwritten text image generation approach, HWT, that strives to learn both style-content entanglement as well as global and local style patterns. The proposed HWT captures the long and short range relationships within the style examples through a self-attention mechanism, thereby encoding both global and local style patterns. Further, the proposed transformer-based HWT comprises an encoder-decoder attention that enables style-content entanglement by gathering the style features of each query character. To the best of our knowledge, we are the first to introduce a transformer-based network for styled handwritten text generation.Our proposed HWT generates realistic styled handwritten text images and outperforms the state-of-the-art demonstrated through extensive qualitative, quantitative and human-based evaluations. The proposed HWT can handle arbitrary length of text and any desired writing style in a few-shot setting. Further, our HWT generalizes well to the challenging scenario where both words and writing style are unseen during training, generating realistic styled handwritten text images. Code is available at: https://github.com/ankanbhunia/Handwriting-Transformers},
  archive   = {C_ICCV},
  author    = {Ankan Kumar Bhunia and Salman Khan and Hisham Cholakkal and Rao Muhammad Anwer and Fahad Shahbaz Khan and Mubarak Shah},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00112},
  pages     = {1066-1074},
  title     = {Handwriting transformers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). De-rendering stylized texts. <em>ICCV</em>, 1056–1065. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Editing raster text is a promising but challenging task. We propose to apply text vectorization for the task of raster text editing in display media, such as posters, web pages, or advertisements. In our approach, instead of applying image transformation or generation in the raster domain, we learn a text vectorization model to parse all the rendering parameters including text, location, size, font, style, effects, and hidden background, then utilize those parameters for reconstruction and any editing task. Our text vectorization takes advantage of differentiable text rendering to accurately reproduce the input raster text in a resolution-free parametric format. We show in the experiments that our approach can successfully parse text, styling, and background information in the unified model, and produces artifact-free text editing compared to a raster baseline.},
  archive   = {C_ICCV},
  author    = {Wataru Shimoda and Daichi Haraguchi and Seiichi Uchida and Kota Yamaguchi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00111},
  pages     = {1056-1065},
  title     = {De-rendering stylized texts},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). From culture to clothing: Discovering the world events
behind a century of fashion images. <em>ICCV</em>, 1046–1055. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fashion is intertwined with external cultural factors, but identifying these links remains a manual process limited to only the most salient phenomena. We propose a data-driven approach to identify specific cultural factors affecting the clothes people wear. Using large-scale datasets of news articles and vintage photos spanning a century, we present a multi-modal statistical model to detect influence relationships between happenings in the world and people’s choice of clothing. Furthermore, on two image datasets we apply our model to improve the concrete vision tasks of visual style forecasting and photo timestamping. Our work is a first step towards a computational, scalable, and easily refreshable approach to link culture to clothing.},
  archive   = {C_ICCV},
  author    = {Wei-Lin Hsiao and Kristen Grauman},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00110},
  pages     = {1046-1055},
  title     = {From culture to clothing: Discovering the world events behind a century of fashion images},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Beyond trivial counterfactual explanations with diverse
valuable explanations. <em>ICCV</em>, 1036–1045. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Explainability for machine learning models has gained considerable attention within the research community given the importance of deploying more reliable machine-learning systems. In computer vision applications, generative counterfactual methods indicate how to perturb a model’s input to change its prediction, providing details about the model’s decision-making. Current methods tend to generate trivial counterfactuals about a model’s decisions, as they often suggest to exaggerate or remove the presence of the attribute being classified. For the machine learning practitioner, these types of counterfactuals offer little value, since they provide no new information about undesired model or data biases. In this work, we identify the problem of trivial counterfactual generation and we propose DiVE to alleviate it. DiVE learns a perturbation in a disentangled latent space that is constrained using a diversity-enforcing loss to uncover multiple valuable explanations about the model’s prediction. Further, we introduce a mechanism to prevent the model from producing trivial explanations. Experiments on CelebA and Synbols demonstrate that our model improves the success rate of producing high-quality valuable explanations when compared to previous state-of-the-art methods. Code is available at https://github.com/ElementAI/beyond-trivial-explanations.},
  archive   = {C_ICCV},
  author    = {Pau Rodríguez and Massimo Caccia and Alexandre Lacoste and Lee Zamparo and Issam Laradji and Laurent Charlin and David Vazquez},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00109},
  pages     = {1036-1045},
  title     = {Beyond trivial counterfactual explanations with diverse valuable explanations},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). SCOUTER: Slot attention-based classifier for explainable
image recognition. <em>ICCV</em>, 1026–1035. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Explainable artificial intelligence has been gaining attention in the past few years. However, most existing methods are based on gradients or intermediate features, which are not directly involved in the decision-making process of the classifier. In this paper, we propose a slot attention-based classifier called SCOUTER for transparent yet accurate classification. Two major differences from other attention-based methods include: (a) SCOUTER’s explanation is involved in the final confidence for each category, offering more intuitive interpretation, and (b) all the categories have their corresponding positive or negative explanation, which tells &quot;why the image is of a certain category&quot; or &quot;why the image is not of a certain category.&quot; We design a new loss tailored for SCOUTER that controls the model’s behavior to switch between positive and negative explanations, as well as the size of explanatory regions. Experimental results show that SCOUTER can give better visual explanations in terms of various metrics while keeping good accuracy on small and medium-sized datasets. Code is available 1 .},
  archive   = {C_ICCV},
  author    = {Liangzhi Li and Bowen Wang and Manisha Verma and Yuta Nakashima and Ryo Kawasaki and Hajime Nagahara},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00108},
  pages     = {1026-1035},
  title     = {SCOUTER: Slot attention-based classifier for explainable image recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning canonical 3D object representation for fine-grained
recognition. <em>ICCV</em>, 1015–1025. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel framework for fine-grained object recognition that learns to recover object variation in 3D space from a single image, trained on an image collection without using any ground-truth 3D annotation. We accomplish this by representing an object as a composition of 3D shape and its appearance, while eliminating the effect of camera viewpoint, in a canonical configuration. Unlike conventional methods modeling spatial variation in 2D images only, our method is capable of reconfiguring the appearance feature in a canonical 3D space, thus enabling the subsequent object classifier to be invariant under 3D geometric variation. Our representation also allows us to go beyond existing methods, by incorporating 3D shape variation as an additional cue for object recognition. To learn the model without ground-truth 3D annotation, we deploy a differentiable renderer in an analysis-by-synthesis frame- work. By incorporating 3D shape and appearance jointly in a deep representation, our method learns the discriminative representation of the object and achieves competitive performance on fine-grained image recognition and vehicle re-identification. We also demonstrate that the performance of 3D shape reconstruction is improved by learning fine-grained shape deformation in a boosting manner.},
  archive   = {C_ICCV},
  author    = {Sunghun Joung and Seungryong Kim and Minsu Kim and Ig-Jae Kim and Kwanghoon Sohn},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00107},
  pages     = {1015-1025},
  title     = {Learning canonical 3D object representation for fine-grained recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Counterfactual attention learning for fine-grained visual
categorization and re-identification. <em>ICCV</em>, 1005–1014. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Attention mechanism has demonstrated great potential in fine-grained visual recognition tasks. In this paper, we present a counterfactual attention learning method to learn more effective attention based on causal inference. Unlike most existing methods that learn visual attention based on conventional likelihood, we propose to learn the attention with counterfactual causality, which provides a tool to measure the attention quality and a powerful supervisory signal to guide the learning process. Specifically, we analyze the effect of the learned visual attention on network prediction through counterfactual intervention and maximize the effect to encourage the network to learn more useful attention for fine-grained image recognition. Empirically, we evaluate our method on a wide range of fine-grained recognition tasks where attention plays a crucial role, including fine-grained image categorization, person re-identification, and vehicle re-identification. The consistent improvement on all benchmarks demonstrates the effectiveness of our method. 1},
  archive   = {C_ICCV},
  author    = {Yongming Rao and Guangyi Chen and Jiwen Lu and Jie Zhou},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00106},
  pages     = {1005-1014},
  title     = {Counterfactual attention learning for fine-grained visual categorization and re-identification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Effectively leveraging attributes for visual similarity.
<em>ICCV</em>, 995–1004. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Measuring similarity between two images often requires performing complex reasoning along different axes (e.g., color, texture, or shape). Insights into what might be important for measuring similarity can be provided by annotated attributes. Prior work tends to view these annotations as complete, resulting in them using a simplistic approach of predicting attributes on single images, which are, in turn, used to measure similarity. However, it is impractical for a dataset to fully annotate every attribute that may be important. Thus, only representing images based on these incomplete annotations may miss out on key information. To address this issue, we propose the Pairwise Attribute-informed similarity Network (PAN), which breaks similarity learning into capturing similarity conditions and relevance scores from a joint representation of two images. This enables our model to identify that two images contain the same attribute, but can have it deemed irrelevant (e.g., due to fine-grained differences between them) and ignored for measuring similarity between the two images. Notably, while prior methods of using attribute annotations are often unable to outperform prior art, PAN obtains a 4-9\% improvement on compatibility prediction between clothing items on Polyvore Outfits, a 5\% gain on few shot classification of images using Caltech-UCSD Birds (CUB), and over 1\% boost to Recall@1 on In-Shop Clothes Retrieval. Implementation available at https://github.com/samarth4149/PAN},
  archive   = {C_ICCV},
  author    = {Samarth Mishra and Zhongping Zhang and Yuan Shen and Ranjitha Kumar and Venkatesh Saligrama and Bryan A. Plummer},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00105},
  pages     = {995-1004},
  title     = {Effectively leveraging attributes for visual similarity},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). LayoutTransformer: Layout generation and completion with
self-attention. <em>ICCV</em>, 984–994. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the problem of scene layout generation for diverse domains such as images, mobile applications, documents, and 3D objects. Most complex scenes, natural or human-designed, can be expressed as a meaningful arrangement of simpler compositional graphical primitives. Generating a new layout or extending an existing layout re- quires understanding the relationships between these primitives. To do this, we propose LayoutTransformer, a novel framework that leverages self-attention to learn contextual relationships between layout elements and generate novel layouts in a given domain. Our framework allows us to generate a new layout either from an empty set or from an initial seed set of primitives, and can easily scale to support an arbitrary of primitives per layout. Furthermore, our analyses show that the model is able to automatically capture the semantic properties of the primitives. We propose simple improvements in both representation of layout primitives, as well as training methods to demonstrate competitive performance in very diverse data domains such as object bounding boxes in natural images (COCO bounding box), documents (PubLayNet), mobile applications (RICO dataset) as well as 3D shapes (Part-Net). Code and other materials will be made available at https://kampta.github.io/layout.},
  archive   = {C_ICCV},
  author    = {Kamal Gupta and Justin Lazarow and Alessandro Achille and Larry Davis and Vijay Mahadevan and Abhinav Shrivastava},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00104},
  pages     = {984-994},
  title     = {LayoutTransformer: Layout generation and completion with self-attention},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DocFormer: End-to-end transformer for document
understanding. <em>ICCV</em>, 973–983. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present DocFormer - a multi-modal transformer based architecture for the task of Visual Document Understanding (VDU). VDU is a challenging problem which aims to understand documents in their varied formats (forms, receipts etc.) and layouts. In addition, DocFormer is pre-trained in an unsupervised fashion using carefully designed tasks which encourage multi-modal interaction. DocFormer uses text, vision and spatial features and combines them using a novel multi-modal self-attention layer. DocFormer also shares learned spatial embeddings across modalities which makes it easy for the model to correlate text to visual tokens and vice versa. DocFormer is evaluated on 4 different datasets each with strong baselines. DocFormer achieves state-of-the-art results on all of them, sometimes beating models 4x its size (in no. of parameters).},
  archive   = {C_ICCV},
  author    = {Srikar Appalaraju and Bhavan Jasani and Bhargava Urala Kota and Yusheng Xie and R. Manmatha},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00103},
  pages     = {973-983},
  title     = {DocFormer: End-to-end transformer for document understanding},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Text is text, no matter what: Unifying text recognition
using knowledge distillation. <em>ICCV</em>, 963–972. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Text recognition remains a fundamental and extensively researched topic in computer vision, largely owing to its wide array of commercial applications. The challenging nature of the very problem however dictated a fragmentation of research efforts: Scene Text Recognition (STR) that deals with text in everyday scenes, and Handwriting Text Recognition (HTR) that tackles hand-written text. In this paper, for the first time, we argue for their unification – we aim for a single model that can compete favourably with two separate state-of-the-art STR and HTR models. We first show that cross-utilisation of STR and HTR models trigger significant performance drops due to differences in their inherent challenges. We then tackle their union by introducing a knowledge distillation (KD) based framework. This however is non-trivial, largely due to the variable-length and sequential nature of text sequences, which renders off-the-shelf KD techniques that mostly work with global fixed length data, inadequate. For that, we propose four distillation losses, all of which are specifically designed to cope with the aforementioned unique characteristics of text recognition. Empirical evidence suggests that our proposed unified model performs at par with individual models, even surpassing them in certain cases. Ablative studies demonstrate that naive baselines such as a two-stage framework, multi-task and domain adaption/generalisation alternatives do not work that well, further authenticating our design.},
  archive   = {C_ICCV},
  author    = {Ayan Kumar Bhunia and Aneeshan Sain and Pinaki Nath Chowdhury and Yi-Zhe Song},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00102},
  pages     = {963-972},
  title     = {Text is text, no matter what: Unifying text recognition using knowledge distillation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Detecting persuasive atypicality by modeling contextual
compatibility. <em>ICCV</em>, 952–962. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a new approach to detect atypicality in persuasive imagery. Unlike atypicality which has been studied in prior work, persuasive atypicality has a particular purpose to convey meaning, and relies on understanding the common-sense spatial relations of objects. We propose a self-supervised attention-based technique which captures contextual compatibility, and models spatial relations in a precise manner. We further experiment with capturing common sense through the semantics of co-occurring object classes. We verify our approach on a dataset of atypicality in visual advertisements, as well as a second dataset capturing atypicality that has no persuasive intent.},
  archive   = {C_ICCV},
  author    = {Meiqi Guo and Rebecca Hwa and Adriana Kovashka},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00101},
  pages     = {952-962},
  title     = {Detecting persuasive atypicality by modeling contextual compatibility},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatial and semantic consistency regularizations for
pedestrian attribute recognition. <em>ICCV</em>, 942–951. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While recent studies on pedestrian attribute recognition have shown remarkable progress in leveraging complicated networks and attention mechanisms, most of them neglect the inter-image relations and an important prior: spatial consistency and semantic consistency of attributes under surveillance scenarios. The spatial locations of the same attribute should be consistent between different pedestrian images, e.g., the &quot;hat&quot; attribute and the &quot;boots&quot; attribute are always located at the top and bottom of the picture respectively. In addition, the inherent semantic feature of the &quot;hat&quot; attribute should be consistent, whether it is a baseball cap, beret, or helmet. To fully exploit inter-image relations and aggregate human prior in the model learning process, we construct a Spatial and Semantic Consistency (SSC) framework that consists of two complementary regularizations to achieve spatial and semantic consistency for each attribute. Specifically, we first propose a spatial consistency regularization to focus on reliable and stable attribute-related regions. Based on the precise attribute locations, we further propose a semantic consistency regularization to extract intrinsic and discriminative semantic features. We conduct extensive experiments on popular benchmarks including PA100K, RAP, and PETA. Results show that the proposed method performs favorably against state- of-the-art methods without increasing parameters.},
  archive   = {C_ICCV},
  author    = {Jian Jia and Xiaotang Chen and Kaiqi Huang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00100},
  pages     = {942-951},
  title     = {Spatial and semantic consistency regularizations for pedestrian attribute recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SketchLattice: Latticed representation for sketch
manipulation. <em>ICCV</em>, 933–941. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The key challenge in designing a sketch representation lies with handling the abstract and iconic nature of sketches. Existing work predominantly utilizes either, (i) a pixelative format that treats sketches as natural images employing off-the-shelf CNN-based networks, or (ii) an elaborately designed vector format that leverages the structural information of drawing orders using sequential RNN-based methods. While the pixelative format lacks intuitive exploitation of structural cues, sketches in vector format are absent in most cases limiting their practical usage. Hence, in this paper, we propose a lattice structured sketch representation that not only removes the bottleneck of requiring vector data but also preserves the structural cues that vector data provides. Essentially, sketch lattice is a set of points sampled from the pixelative format of the sketch using a lattice graph. We show that our lattice structure is particularly amenable to structural changes that largely benefits sketch abstraction modeling for generation tasks. Our lattice representation could be effectively encoded using a graph model, that uses significantly fewer model parameters (13.5 times lesser) than existing state-of-the-art. Extensive experiments demonstrate the effectiveness of sketch lattice for sketch manipulation, including sketch healing and image-to-sketch synthesis.},
  archive   = {C_ICCV},
  author    = {Yonggang Qi and Guoyao Su and Pinaki Nath Chowdhury and Mingkang Li and Yi-Zhe Song},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00099},
  pages     = {933-941},
  title     = {SketchLattice: Latticed representation for sketch manipulation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parsing table structures in the wild. <em>ICCV</em>,
924–932. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper tackles the problem of table structure parsing (TSP) from images in the wild. In contrast to existing studies that mainly focus on parsing well-aligned tabular images with simple layouts from scanned PDF documents, we aim to establish a practical table structure parsing system for real-world scenarios where tabular input images are taken or scanned with severe deformation, bending or occlusions. For designing such a system, we propose an approach named Cycle-CenterNet on the top of CenterNet with a novel cycle-pairing module to simultaneously detect and group tabular cells into structured tables. In the cycle-pairing module, a new pairing loss function is proposed for the network training. Alongside with our Cycle-CenterNet, we also present a large-scale dataset, named Wired Table in the Wild (WTW), which includes well-annotated structure parsing of multiple style tables in several scenes like photo, scanning files, web pages, etc.. In experiments, we demonstrate that our Cycle-CenterNet consistently achieves the best accuracy of table structure parsing on the new WTW dataset by 24.6\% absolute improvement evaluated by the TEDS metric. A more comprehensive experimental analysis also validates the advantages of our proposed methods for the TSP task.},
  archive   = {C_ICCV},
  author    = {Rujiao Long and Wen Wang and Nan Xue and Feiyu Gao and Zhibo Yang and Yongpan Wang and Gui-Song Xia},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00098},
  pages     = {924-932},
  title     = {Parsing table structures in the wild},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Graph-based asynchronous event processing for rapid object
recognition. <em>ICCV</em>, 914–923. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Different from traditional video cameras, event cam- eras capture asynchronous events stream in which each event encodes pixel location, trigger time, and the polarity of the brightness changes. In this paper, we introduce a novel graph-based framework for event cameras, namely SlideGCN. Unlike some recent graph-based methods that use groups of events as input, our approach can efficiently process data event-by-event, unlock the low latency nature of events data while still maintaining the graph’s structure internally. For fast graph construction, we develop a radius search algorithm, which better exploits the partial regular structure of event cloud against k-d tree based generic methods. Experiments show that our method reduces the computational complexity up to 100 times with respect to current graph-based methods while keeping state-of-the-art performance on object recognition. Moreover, we verify the superiority of event-wise processing with our method. When the state becomes stable, we can give a prediction with high confidence, thus making an early recognition.},
  archive   = {C_ICCV},
  author    = {Yijin Li and Han Zhou and Bangbang Yang and Ye Zhang and Zhaopeng Cui and Hujun Bao and Guofeng Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00097},
  pages     = {914-923},
  title     = {Graph-based asynchronous event processing for rapid object recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). End-to-end trainable trident person search network using
adaptive gradient propagation. <em>ICCV</em>, 905–913. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Person search suffers from the conflicting objectives of commonness and uniqueness between the person detection and re-identification tasks that make the end-to-end training of person search networks difficult. In this paper, we propose a trident network for person search that performs detection, re-identification, and part classification together. We also devise a novel end-to-end training method using adaptive gradient weighting that controls the flow of backpropagated gradients through the re-identification and part classification networks according to the quality of the person detection. The proposed method not only prevents the over-fitting but encourages to exploit fine-grained features by incorporating the part classification branch into the person search framework. Experimental results on the CUHK-SYSU and PRW datasets demonstrate that the proposed method achieves the best performance among the state-of-the-art end-to-end person search methods.},
  archive   = {C_ICCV},
  author    = {Byeong-Ju Han and Kuhyeun Ko and Jae-Young Sim},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00096},
  pages     = {905-913},
  title     = {End-to-end trainable trident person search network using adaptive gradient propagation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Walk in the cloud: Learning curves for point clouds shape
analysis. <em>ICCV</em>, 895–904. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Discrete point cloud objects lack sufficient shape descriptors of 3D geometries. In this paper, we present a novel method for aggregating hypothetical curves in point clouds. Sequences of connected points (curves) are initially grouped by taking guided walks in the point clouds, and then subsequently aggregated back to augment their pointwise features. We provide an effective implementation of the proposed aggregation strategy including a novel curve grouping operator followed by a curve aggregation operator. Our method was benchmarked on several point cloud analysis tasks where we achieved the state-of-the-art classification accuracy of 94.2\% on the ModelNet40 classification task, instance IoU of 86.8\% on the ShapeNetPart segmentation task and cosine error of 0.11 on the ModelNet40 normal estimation task. Our project page with source code is available at: https://curvenet.github.io/.},
  archive   = {C_ICCV},
  author    = {Tiange Xiang and Chaoyi Zhang and Yang Song and Jianhui Yu and Weidong Cai},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00095},
  pages     = {895-904},
  title     = {Walk in the cloud: Learning curves for point clouds shape analysis},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generating attribution maps with disentangled masked
backpropagation. <em>ICCV</em>, 885–894. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Attribution map visualization has arisen as one of the most effective techniques to understand the underlying inference process of Convolutional Neural Networks. In this task, the goal is to compute an score for each image pixel related to its contribution to the network output. In this paper, we introduce Disentangled Masked Backpropagation (DMBP), a novel gradient-based method that leverages on the piecewise linear nature of ReLU networks to decompose the model function into different linear mappings. This decomposition aims to disentangle the attribution maps into positive, negative and nuisance factors by learning a set of variables masking the contribution of each filter during back-propagation. A thorough evaluation over standard architectures (ResNet50 and VGG16) and benchmark datasets (PASCAL VOC and ImageNet) demonstrates that DMBP generates more visually interpretable attribution maps than previous approaches. Additionally, we quantitatively show that the maps produced by our method are more consistent with the true contribution of each pixel to the final network output.},
  archive   = {C_ICCV},
  author    = {Adria Ruiz and Antonio Agudo and Francesc Moreno-Noguer},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00094},
  pages     = {885-894},
  title     = {Generating attribution maps with disentangled masked backpropagation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interpretable image recognition by constructing transparent
embedding space. <em>ICCV</em>, 875–884. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans usually explain their reasoning (e.g. classification) by dissecting the image and pointing out the evidence from these parts to the concepts in their minds. Inspired by this cognitive process, several part-level interpretable neural network architectures have been proposed to explain the predictions. However, they suffer from the complex data structure and confusing the effect of the individual part to output category. In this work, an interpretable image recognition deep network is designed by introducing a plug-in transparent embedding space (TesNet) to bridge the high-level input patches (e.g. CNN feature maps) and the out- put categories. This plug-in embedding space is spanned by transparent basis concepts which are constructed on the Grassmann manifold. These basis concepts are enforced to be category-aware and within-category concepts are orthogonal to each other, which makes sure the embedding space is disentangled. Meanwhile, each basis concept can be traced back to the particular image patches, thus they are transparent and friendly to explain the reasoning process. By comparing with state-of-the-art interpretable methods, TesNet is much more beneficial to classification tasks, esp. providing better interpretability on predictions and improve the final accuracy. The code is available at https://github.com/JackeyWang96/TesNet.},
  archive   = {C_ICCV},
  author    = {Jiaqi Wang and Huafeng Liu and Xinyue Wang and Liping Jing},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00093},
  pages     = {875-884},
  title     = {Interpretable image recognition by constructing transparent embedding space},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attentional pyramid pooling of salient visual residuals for
place recognition. <em>ICCV</em>, 865–874. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The core of visual place recognition (VPR) lies in how to identify task-relevant visual cues and embed them into dis- criminative representations. Focusing on these two points, we propose a novel encoding strategy named Attentional Pyramid Pooling of Salient Visual Residuals (APPSVR). It incorporates three types of attention modules to model the saliency of local features in individual, spatial and cluster dimensions respectively. (1) To inhibit task-irrelevant local features, a semantic-reinforced local weighting scheme is employed for local feature refinement; (2) To leverage the spatial context, an attentional pyramid structure is constructed to adaptively encode regional features according to their relative spatial saliency; (3) To distinguish the different importance of visual clusters to the task, a parametric normalization is proposed to adjust their contribution to image descriptor generation. Experiments demonstrate APPSVR outperforms the existing techniques and achieves a new state-of-the-art performance on VPR benchmark datasets. The visualization shows the saliency map learned in a weakly supervised manner is largely consistent with human cognition.},
  archive   = {C_ICCV},
  author    = {Guohao Peng and Jun Zhang and Heshan Li and Danwei Wang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00092},
  pages     = {865-874},
  title     = {Attentional pyramid pooling of salient visual residuals for place recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Grafit: Learning fine-grained image representations with
coarse labels. <em>ICCV</em>, 854–864. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper tackles the problem of learning a finer representation than the one provided by training labels. This enables fine-grained category retrieval of images in a collection annotated with coarse labels only.Our network is learned with a nearest-neighbor classifier objective, and an instance loss inspired by self-supervised learning. By jointly leveraging the coarse labels and the underlying fine-grained latent space, it significantly improves the accuracy of category-level retrieval methods.Our strategy outperforms all competing methods for retrieving or classifying images at a finer granularity than that available at train time. It also improves the accuracy for transfer learning tasks to fine-grained datasets.},
  archive   = {C_ICCV},
  author    = {Hugo Touvron and Alexandre Sablayrolles and Matthijs Douze and Matthieu Cord and Hervé Jégou},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00091},
  pages     = {854-864},
  title     = {Grafit: Learning fine-grained image representations with coarse labels},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FaPN: Feature-aligned pyramid network for dense image
prediction. <em>ICCV</em>, 844–853. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advancements in deep neural networks have made remarkable leap-forwards in dense image prediction. However, the issue of feature alignment remains as neglected by most existing approaches for simplicity. Direct pixel addition between upsampled and local features leads to feature maps with misaligned contexts that, in turn, translate to mis-classifications in prediction, especially on object boundaries. In this paper, we propose a feature alignment module that learns transformation offsets of pixels to contextually align upsampled higher-level features; and another feature selection module to emphasize the lower-level features with rich spatial details. We then integrate these two modules in a top-down pyramidal architecture and present the Feature-aligned Pyramid Network (FaPN). Extensive experimental evaluations on four dense prediction tasks and four datasets have demonstrated the efficacy of FaPN, yielding an overall improvement of 1.2 - 2.6 points in AP / mIoU over FPN when paired with Faster / Mask R-CNN. In particular, our FaPN achieves the state-of-the-art of 56.7\% mIoU on ADE20K when integrated within Mask-Former. The code is available from https://github.com/EMI-Group/FaPN.},
  archive   = {C_ICCV},
  author    = {Shihua Huang and Zhichao Lu and Ran Cheng and Cheng He},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00090},
  pages     = {844-853},
  title     = {FaPN: Feature-aligned pyramid network for dense image prediction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multimodal knowledge expansion. <em>ICCV</em>, 834–843. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The popularity of multimodal sensors and the accessibility of the Internet have brought us a massive amount of unlabeled multimodal data. Since existing datasets and well-trained models are primarily unimodal, the modality gap between a unimodal network and unlabeled multi-modal data poses an interesting problem: how to transfer a pre-trained unimodal network to perform the same task with extra unlabeled multimodal data? In this work, we propose multimodal knowledge expansion (MKE), a knowledge distillation-based framework to effectively utilize multimodal data without requiring labels. Opposite to traditional knowledge distillation, where the student is designed to be lightweight and inferior to the teacher, we observe that a multimodal student model consistently rectifies pseudo labels and generalizes better than its teacher. Extensive experiments on four tasks and different modalities verify this finding. Furthermore, we connect the mechanism of MKE to semi-supervised learning and offer both empirical and theoretical explanations to understand the expansion capability of a multimodal student. 1},
  archive   = {C_ICCV},
  author    = {Zihui Xue and Sucheng Ren and Zhengqi Gao and Hang Zhao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00089},
  pages     = {834-843},
  title     = {Multimodal knowledge expansion},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SS-IL: Separated softmax for incremental learning.
<em>ICCV</em>, 824–833. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider class incremental learning (CIL) problem, in which a learning agent continuously learns new classes from incrementally arriving training data batches and aims to predict well on all the classes learned so far. The main challenge of the problem is the catastrophic forgetting, and for the exemplar-memory based CIL methods, it is generally known that the forgetting is commonly caused by the classification score bias that is injected due to the data imbalance between the new classes and the old classes (in the exemplar-memory). While several methods have been proposed to correct such score bias by some additional post-processing, e.g., score re-scaling or balanced fine-tuning, no systematic analysis on the root cause of such bias has been done. To that end, we analyze that computing the softmax probabilities by combining the output scores for all old and new classes could be the main cause of the bias. Then, we propose a new method, dubbed as Separated Softmax for Incremental Learning (SS-IL), that consists of separated softmax (SS) output layer combined with task-wise knowledge distillation (TKD) to resolve such bias. Throughout our extensive experimental results on several large-scale CIL benchmark datasets, we show our SS-IL achieves strong state-of-the-art accuracy through attaining much more balanced prediction scores across old and new classes, without any additional post-processing.},
  archive   = {C_ICCV},
  author    = {Hongjoon Ahn and Jihwan Kwak and Subin Lim and Hyeonsu Bang and Hyojun Kim and Taesup Moon},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00088},
  pages     = {824-833},
  title     = {SS-IL: Separated softmax for incremental learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to diversify for single domain generalization.
<em>ICCV</em>, 814–823. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Domain generalization (DG) aims to generalize a model trained on multiple source (i.e., training) domains to a distributionally different target (i.e., test) domain. In contrast to the conventional DG that strictly requires the availability of multiple source domains, this paper considers a more realistic yet challenging scenario, namely Single Domain Generalization (Single-DG), where only one source domain is available for training. In this scenario, the limited diversity may jeopardize the model generalization on unseen target domains. To tackle this problem, we propose a style-complement module to enhance the generalization power of the model by synthesizing images from diverse distributions that are complementary to the source ones. More specifically, we adopt a tractable upper bound of mutual information (MI) between the generated and source samples and perform a two-step optimization iteratively: (1) by minimizing the MI upper bound approximation for each sample pair, the generated images are forced to be diversified from the source samples; (2) subsequently, we maximize the MI between the samples from the same semantic category, which assists the network to learn discriminative features from diverse-styled images. Extensive experiments on three benchmark datasets demonstrate the superiority of our approach, which surpasses the state-of-the-art single-DG methods by up to 25.14\%. The code will be publicly available at https://github.com/BUserName/Learning_to_diversify},
  archive   = {C_ICCV},
  author    = {Zijian Wang and Yadan Luo and Ruihong Qiu and Zi Huang and Mahsa Baktashmotlagh},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00087},
  pages     = {814-823},
  title     = {Learning to diversify for single domain generalization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MixMo: Mixing multiple inputs for multiple outputs via deep
subnetworks. <em>ICCV</em>, 803–813. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent strategies achieved ensembling &quot;for free&quot; by fitting concurrently diverse subnetworks inside a single base network. The main idea during training is that each sub-network learns to classify only one of the multiple inputs simultaneously provided. However, the question of how to best mix these multiple inputs has not been studied so farIn this paper, we introduce MixMo, a new generalized framework for learning multi-input multi-output deep subnetworks. Our key motivation is to replace the suboptimal summing operation hidden in previous approaches by a more appropriate mixing mechanism. For that purpose, we draw inspiration from successful mixed sample data augmentations. We show that binary mixing in features - particularly with rectangular patches from CutMix - enhances results by making subnetworks stronger and more diverse.We improve state of the art for image classification on CIFAR-100 and Tiny ImageNet datasets. Our easy to implement models notably outperform data augmented deep ensembles, without the inference and memory overheads. As we operate in features and simply better leverage the expressiveness of large networks, we open a new line of research complementary to previous works.},
  archive   = {C_ICCV},
  author    = {Alexandre Ramé and Rémy Sun and Matthieu Cord},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00086},
  pages     = {803-813},
  title     = {MixMo: Mixing multiple inputs for multiple outputs via deep subnetworks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). OpenGAN: Open-set recognition via open data generation.
<em>ICCV</em>, 793–802. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-world machine learning systems need to analyze novel testing data that differs from the training data. In K-way classification, this is crisply formulated as open-set recognition, core to which is the ability to discriminate open-set data outside the K closed-set classes. Two conceptually elegant ideas for open-set discrimination are: 1) discriminatively learning an open-vs-closed binary discriminator by exploiting some outlier data as the open-set, and 2) unsupervised learning the closed-set data distribution with a GAN and using its discriminator as the open-set likelihood function. However, the former generalizes poorly to diverse open test data due to overfitting to the training outliers, which unlikely exhaustively span the open-world. The latter does not work well, presumably due to the instable training of GANs. Motivated by the above, we propose OpenGAN, which addresses the limitation of each approach by combining them with several technical insights. First, we show that a carefully selected GAN-discriminator on some real outlier data already achieves the state-of-the-art. Second, we augment the available set of real open training examples with adversarially synthesized &quot;fake&quot; data. Third and most importantly, we build the discriminator over the features computed by the closed-world K-way networks. Extensive experiments show that Open-GAN significantly outperforms prior open-set methods.},
  archive   = {C_ICCV},
  author    = {Shu Kong and Deva Ramanan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00085},
  pages     = {793-802},
  title     = {OpenGAN: Open-set recognition via open data generation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Neural video portrait relighting in real-time via
consistency modeling. <em>ICCV</em>, 782–792. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Video portraits relighting is critical in user-facing human photography, especially for immersive VR/AR experience. Recent advances still fail to recover consistent relit result under dynamic illuminations from monocular RGB stream, suffering from the lack of video consistency supervision. In this paper, we propose a neural approach for real-time, high-quality and coherent video portrait relighting, which jointly models the semantic, temporal and lighting consistency using a new dynamic OLAT dataset. We propose a hybrid structure and lighting disentanglement in an encoder-decoder architecture, which combines a multi-task and adversarial training strategy for semantic-aware consistency modeling. We adopt a temporal modeling scheme via flow-based supervision to encode the conjugated temporal consistency in a cross manner. We also propose a lighting sampling strategy to model the illumination consistency and mutation for natural portrait light manipulation in real-world. Extensive experiments demonstrate the effectiveness of our approach for consistent video portrait light-editing and relighting, even using mobile computing.},
  archive   = {C_ICCV},
  author    = {Longwen Zhang and Qixuan Zhang and Minye Wu and Jingyi Yu and Lan Xu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00084},
  pages     = {782-792},
  title     = {Neural video portrait relighting in real-time via consistency modeling},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Global pooling, more than meets the eye: Position
information is encoded channel-wise in CNNs. <em>ICCV</em>, 773–781. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we challenge the common assumption that collapsing the spatial dimensions of a 3D (spatial-channel) tensor in a convolutional neural network (CNN) into a vector via global pooling removes all spatial information. Specifically, we demonstrate that positional information is encoded based on the ordering of the channel dimensions, while semantic information is largely not. Following this demonstration, we show the real world impact of these findings by applying them to two applications. First, we propose a simple yet effective data augmentation strategy and loss function which improves the translation invariance of a CNN’s output. Second, we propose a method to efficiently determine which channels in the latent representation are responsible for (i) encoding overall position information or (ii) region-specific positions. We first show that semantic segmentation has a significant reliance on the overall position channels to make predictions. We then show for the first time that it is possible to perform a ‘region-specific’ attack, and degrade a network’s performance in a particular part of the input. We believe our findings and demonstrated applications will benefit research areas concerned with understanding the characteristics of CNNs. Code is available at: https://github.com/islamamirul/PermuteNet.},
  archive   = {C_ICCV},
  author    = {Md Amirul Islam and Matthew Kowal and Sen Jia and Konstantinos G. Derpanis and Neil D. B. Bruce},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00083},
  pages     = {773-781},
  title     = {Global pooling, more than meets the eye: Position information is encoded channel-wise in CNNs},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). FcaNet: Frequency channel attention networks. <em>ICCV</em>,
763–772. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Attention mechanism, especially channel attention, has gained great success in the computer vision field. Many works focus on how to design efficient channel attention mechanisms while ignoring a fundamental problem, i.e., channel attention mechanism uses scalar to represent channel, which is difficult due to massive information loss. In this work, we start from a different view and regard the channel representation problem as a compression process using frequency analysis. Based on the frequency analysis, we mathematically prove that the conventional global average pooling is a special case of the feature decomposition in the frequency domain. With the proof, we naturally generalize the compression of the channel attention mechanism in the frequency domain and propose our method with multi-spectral channel attention, termed as FcaNet. FcaNet is simple but effective. We can change a few lines of code in the calculation to implement our method within existing channel attention methods. Moreover, the proposed method achieves state-of-the-art results compared with other channel attention methods on image classification, object detection, and instance segmentation tasks. Our method could consistently outperform the baseline SENet, with the same number of parameters and the same computational cost. Our code and models are publicly available at https://github.com/cfzd/FcaNet.},
  archive   = {C_ICCV},
  author    = {Zequn Qin and Pengyi Zhang and Fei Wu and Xi Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00082},
  pages     = {763-772},
  title     = {FcaNet: Frequency channel attention networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TrivialAugment: Tuning-free yet state-of-the-art data
augmentation. <em>ICCV</em>, 754–762. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automatic augmentation methods have recently become a crucial pillar for strong model performance in vision tasks. While existing automatic augmentation methods need to trade off simplicity, cost and performance, we present a most simple baseline, TrivialAugment, that outperforms previous methods for almost free. TrivialAugment is parameter-free and only applies a single augmentation to each image. Thus, TrivialAugment&#39;s effectiveness is very unexpected to us and we performed very thorough experiments to study its performance. First, we compare TrivialAugment to previous state-of-the-art methods in a variety of image classification scenarios. Then, we perform multiple ablation studies with different augmentation spaces, augmentation methods and setups to understand the crucial requirements for its performance. Additionally, we provide a simple interface to facilitate the widespread adoption of automatic augmentation methods, as well as our full code base for reproducibility 1 . Since our work reveals a stagnation in many parts of automatic augmentation research, we end with a short proposal of best practices for sustained future progress in automatic augmentation methods.},
  archive   = {C_ICCV},
  author    = {Samuel G. Müller and Frank Hutter},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00081},
  pages     = {754-762},
  title     = {TrivialAugment: Tuning-free yet state-of-the-art data augmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Recursively conditional gaussian for ordinal unsupervised
domain adaptation. <em>ICCV</em>, 744–753. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The unsupervised domain adaptation (UDA) has been widely adopted to alleviate the data scalability issue, while the existing works usually focus on classifying independently discrete labels. However, in many tasks (e.g., medical diagnosis), the labels are discrete and successively distributed. The UDA for ordinal classification requires inducing non-trivial ordinal distribution prior to the latent space. Target for this, the partially ordered set (poset) is defined for constraining the latent vector Instead of the typically i.i.d. Gaussian latent prior, in this work, a recursively conditional Gaussian (RCG) set is adapted for ordered constraint modeling, which admits a tractable joint distribution prior Furthermore, we are able to control the density of content vector that violates the poset constraints by a simple &quot;three-sigma rule&quot;. We explicitly disentangle the cross-domain images into a shared ordinal prior induced ordinal content space and two separate source/target ordinal-unrelated spaces, and the self-training is worked on the shared space exclusively for ordinal-aware domain alignment. Extensive experiments on UDA medical diagnoses and facial age estimation demonstrate its effectiveness.},
  archive   = {C_ICCV},
  author    = {Xiaofeng Liu and Site Li and Yubin Ge and Pengyi Ye and Jane You and Jun Lu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00080},
  pages     = {744-753},
  title     = {Recursively conditional gaussian for ordinal unsupervised domain adaptation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Contrastive multimodal fusion with TupleInfoNCE.
<em>ICCV</em>, 734–743. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a method for representation learning of multimodal data using contrastive losses. A traditional approach is to contrast different modalities to learn the information shared among them. However, that approach could fail to learn the complementary synergies between modalities that might be useful for downstream tasks. Another approach is to concatenate all the modalities into a tuple and then contrast positive and negative tuple correspondences. However, that approach could consider only the stronger modalities while ignoring the weaker ones. To address these issues, we propose a novel contrastive learning objective, TupleInfoNCE. It contrasts tuples based not only on positive and negative correspondences, but also by composing new negative tuples using modalities describing different scenes. Training with these additional negatives encourages the learning model to examine the correspondences among modalities in the same tuple, ensuring that weak modalities are not ignored. We provide a theoretical justification based on mutual-information for why this approach works, and we propose a sample optimization algorithm to generate positive and negative samples to maximize training efficacy. We find that TupleInfoNCE significantly outperforms previous state of the arts on three different downstream tasks.},
  archive   = {C_ICCV},
  author    = {Yunze Liu and Qingnan Fan and Shanghang Zhang and Hao Dong and Thomas Funkhouser and Li Yi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00079},
  pages     = {734-743},
  title     = {Contrastive multimodal fusion with TupleInfoNCE},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Statistically consistent saliency estimation. <em>ICCV</em>,
725–733. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The growing use of deep learning for a wide range of data problems has highlighted the need to understand and diagnose these models appropriately, making deep learning interpretation techniques an essential tool for data analysts. The numerous model interpretation methods proposed in recent years are generally based on heuristics, with little or no theoretical guarantees. Here, we present a statistical framework for saliency estimation for black-box computer vision models. Our proposed model-agnostic estimation procedure, which is statistically consistent and capable of passing sanity checks, has polynomial-time computational efficiency since it only requires solving a linear program. An upper bound is established on the number of model evaluations needed to recover regions of importance with high probability through our theoretical analysis. Furthermore, a new perturbation scheme is presented for the estimation of local gradients that is more efficient than commonly used random perturbation schemes. The validity and excellence of our new method are demonstrated experimentally via sensitivity analyses.},
  archive   = {C_ICCV},
  author    = {Shunyan Luo and Emre Barut and Fang Jin},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00078},
  pages     = {725-733},
  title     = {Statistically consistent saliency estimation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Influence-balanced loss for imbalanced visual
classification. <em>ICCV</em>, 715–724. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a balancing training method to address problems in imbalanced data learning. To this end, we derive a new loss used in the balancing training phase that alleviates the influence of samples that cause an overfitted decision boundary. The proposed loss efficiently improves the performance of any type of imbalance learning methods. In experiments on multiple benchmark data sets, we demonstrate the validity of our method and reveal that the proposed loss outperforms the state-of-the-art cost-sensitive loss methods. Furthermore, since our loss is not restricted to a specific task, model, or training method, it can be easily used in combination with other recent resampling, meta-learning, and cost-sensitive learning methods for class-imbalance problems. Our code is made available at https://github.com/pseulki/IB-Loss.},
  archive   = {C_ICCV},
  author    = {Seulki Park and Jongin Lim and Younghan Jeon and Jin Young Choi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00077},
  pages     = {715-724},
  title     = {Influence-balanced loss for imbalanced visual classification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning fast sample re-weighting without reward data.
<em>ICCV</em>, 705–714. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Training sample re-weighting is an effective approach for tackling data biases such as imbalanced and corrupted labels. Recent methods develop learning-based algorithms to learn sample re-weighting strategies jointly with model training based on the frameworks of reinforcement learning and meta learning. However, depending on additional unbiased reward data is limiting their general applicability. Furthermore, existing learning-based sample re-weighting methods require nested optimizations of models and weighting parameters, which requires expensive second-order computation. This paper addresses these two problems and presents a novel learning-based fast sample re-weighting (FSR) method that does not require additional reward data. The method is based on two key ideas: learning from history to build proxy reward data and feature sharing to reduce the optimization cost. Our experiments show the proposed method achieves competitive results compared to state of the arts on label noise robustness and long-tailed recognition, and does so while achieving significantly improved training efficiency. The source code is publicly available at https://github.com/google-research/google-research/tree/master/ieg.},
  archive   = {C_ICCV},
  author    = {Zizhao Zhang and Tomas Pfister},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00076},
  pages     = {705-714},
  title     = {Learning fast sample re-weighting without reward data},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parametric contrastive learning. <em>ICCV</em>, 695–704. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose Parametric Contrastive Learning (PaCo) to tackle long-tailed recognition. Based on theoretical analysis, we observe supervised contrastive loss tends to bias on high-frequency classes and thus increases the difficulty of imbalanced learning. We introduce a set of parametric class-wise learnable centers to rebalance from an optimization perspective. Further, we analyze our PaCo loss under a balanced setting. Our analysis demonstrates that PaCo can adaptively enhance the intensity of pushing samples of the same class close as more samples are pulled together with their corresponding centers and benefit hard example learning. Experiments on long-tailed CIFAR, ImageNet, Places, and iNaturalist 2018 manifest the new state-of-the-art for long-tailed recognition. On full ImageNet, models trained with PaCo loss surpass supervised contrastive learning across various ResNet backbones, e.g., our ResNet-200 achieves 81.8\% top-1 accuracy. Our code is available at https://github.com/dvlab-research/Parametric-Contrastive-Learning.},
  archive   = {C_ICCV},
  author    = {Jiequan Cui and Zhisheng Zhong and Shu Liu and Bei Yu and Jiaya Jia},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00075},
  pages     = {695-704},
  title     = {Parametric contrastive learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ground-truth or DAER: Selective re-query of secondary
information. <em>ICCV</em>, 683–694. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many vision tasks use secondary information at inference time—a seed—to assist a computer vision model in solving a problem. For example, an initial bounding box is needed to initialize visual object tracking. To date, all such work makes the assumption that the seed is a good one. However, in practice, from crowdsourcing to noisy automated seeds, this is often not the case. We hence propose the problem of seed rejection—determining whether to reject a seed based on the expected performance degradation when it is provided in place of a gold-standard seed. We provide a formal definition to this problem, and focus on two meaningful subgoals: understanding causes of error and understanding the model’s response to noisy seeds conditioned on the primary input. With these goals in mind, we propose a novel training method and evaluation metrics for the seed rejection problem. We then use seeded versions of the viewpoint estimation and fine-grained classification tasks to evaluate these contributions. In these experiments, we show our method can reduce the number of seeds that need to be reviewed for a target performance by over 23\% compared to strong baselines.},
  archive   = {C_ICCV},
  author    = {Stephan J. Lemmer and Jason J. Corso},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00074},
  pages     = {683-694},
  title     = {Ground-truth or DAER: Selective re-query of secondary information},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Explaining in style: Training a GAN to explain a classifier
in StyleSpace. <em>ICCV</em>, 673–682. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Image classification models can depend on multiple different semantic attributes of the image. An explanation of the decision of the classifier needs to both discover and visualize these properties. Here we present StylEx, a method for doing this, by training a generative model to specifically explain multiple attributes that underlie classifier decisions. A natural source for such attributes is the StyleSpace of StyleGAN, which is known to generate semantically meaningful dimensions in the image. However, because standard GAN training is not dependent on the classifier, it may not represent those attributes which are important for the classifier decision, and the dimensions of StyleSpace may represent irrelevant at-tributes. To overcome this, we propose a training procedure for a StyleGAN, which incorporates the classifier model, in order to learn a classifier-specific StyleSpace. Explanatory attributes are then selected from this space. These can be used to visualize the effect of changing multiple attributes per image, thus providing image-specific explanations. We apply StylEx to multiple domains, including animals, leaves, faces and retinal images. For these, we show how an image can be modified in different ways to change its classifier output. Our results show that the method finds attributes that align well with semantic ones, generate meaningful image-specific explanations, and are human-interpretable as measured in user-studies. 1},
  archive   = {C_ICCV},
  author    = {Oran Lang and Yossi Gandelsman and Michal Yarom and Yoav Wald and Gal Elidan and Avinatan Hassidim and William T. Freeman and Phillip Isola and Amir Globerson and Michal Irani and Inbar Mosseri},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00073},
  pages     = {673-682},
  title     = {Explaining in style: Training a GAN to explain a classifier in StyleSpace},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploiting explanations for model inversion attacks.
<em>ICCV</em>, 662–672. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The successful deployment of artificial intelligence (AI) in many domains from healthcare to hiring requires their responsible use, particularly in model explanations and privacy. Explainable artificial intelligence (XAI) provides more information to help users to understand model decisions, yet this additional knowledge exposes additional risks for privacy attacks. Hence, providing explanation harms privacy. We study this risk for image-based model inversion attacks and identified several attack architectures with increasing performance to reconstruct private image data from model explanations. We have developed several multi-modal transposed CNN architectures that achieve significantly higher inversion performance than using the target model prediction only. These XAI-aware inversion models were designed to exploit the spatial knowledge in image explanations. To understand which explanations have higher privacy risk, we analyzed how various explanation types and factors influence inversion performance. In spite of some models not providing explanations, we further demonstrate increased inversion performance even for non-explainable target models by exploiting explanations of surrogate models through attention transfer. This method first inverts an explanation from the target prediction, then reconstructs the target image. These threats highlight the urgent and significant privacy risks of explanations and calls attention for new privacy preservation techniques that balance the dual-requirement for AI explainability and privacy.},
  archive   = {C_ICCV},
  author    = {Xuejun Zhao and Wencan Zhang and Xiaokui Xiao and Brian Lim},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00072},
  pages     = {662-672},
  title     = {Exploiting explanations for model inversion attacks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Architecture disentanglement for deep neural networks.
<em>ICCV</em>, 652–661. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Understanding the inner workings of deep neural networks (DNNs) is essential to provide trustworthy artificial intelligence techniques for practical applications. Existing studies typically involve linking semantic concepts to units or layers of DNNs, but fail to explain the inference process. In this paper, we introduce neural architecture disentanglement (NAD) to fill the gap. Specifically, NAD learns to disentangle a pre-trained DNN into sub-architectures according to independent tasks, forming information flows that describe the inference processes. We investigate whether, where, and how the disentanglement occurs through experiments conducted with handcrafted and automatically-searched network architectures, on both object-based and scene-based datasets. Based on the experimental results, we present three new findings that provide fresh insights into the inner logic of DNNs. First, DNNs can be divided into sub-architectures for independent tasks. Second, deeper layers do not always correspond to higher semantics. Third, the connection type in a DNN affects how the information flows across layers, leading to different disentanglement behaviors. With NAD, we further explain why DNNs sometimes give wrong predictions. Experimental results show that misclassified images have a high probability of being assigned to task sub-architectures similar to the correct ones. Our code is available at https://github.com/hujiecpp/NAD.},
  archive   = {C_ICCV},
  author    = {Jie Hu and Liujuan Cao and Tong Tong and Qixiang Ye and Shengchuan Zhang and Ke Li and Feiyue Huang and Ling Shao and Rongrong Ji},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00071},
  pages     = {652-661},
  title     = {Architecture disentanglement for deep neural networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adversarial attacks are reversible with natural supervision.
<em>ICCV</em>, 641–651. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We find that images contain intrinsic structure that enables the reversal of many adversarial attacks. Attack vectors cause not only image classifiers to fail, but also collaterally disrupt incidental structure in the image. We demonstrate that modifying the attacked image to restore the natural structure will reverse many types of attacks, providing a defense. Experiments demonstrate significantly improved robustness for several state-of-the-art models across the CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets. Our results show that our defense is still effective even if the attacker is aware of the defense mechanism. Since our defense is deployed during inference instead of training, it is compatible with pre-trained networks as well as most other defenses. Our results suggest deep networks are vulnerable to adversarial examples partly because their representations do not enforce the natural structure of images.},
  archive   = {C_ICCV},
  author    = {Chengzhi Mao and Mia Chiquier and Hao Wang and Junfeng Yang and Carl Vondrick},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00070},
  pages     = {641-651},
  title     = {Adversarial attacks are reversible with natural supervision},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Shallow bayesian meta learning for real-world few-shot
recognition. <em>ICCV</em>, 631–640. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many state-of-the-art few-shot learners focus on developing effective training procedures for feature representations, before using simple (e.g., nearest centroid) classifiers. We take an approach that is agnostic to the features used, and focus exclusively on meta-learning the final classifier layer. Specifically, we introduce MetaQDA, a Bayesian meta-learning generalisation of the classic quadratic discriminant analysis. This approach has several benefits of interest to practitioners: meta-learning is fast and memory efficient, without the need to fine-tune features. It is agnostic to the off-the-shelf features chosen, and thus will continue to benefit from future advances in feature representations. Empirically, it leads to excellent performance in cross-domain few-shot learning, class-incremental few-shot learning, and crucially for real-world applications, the Bayesian formulation leads to state-of-the-art uncertainty calibration in predictions.},
  archive   = {C_ICCV},
  author    = {Xueting Zhang and Debin Meng and Henry Gouk and Timothy Hospedales},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00069},
  pages     = {631-640},
  title     = {Shallow bayesian meta learning for real-world few-shot recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Semantic diversity learning for zero-shot multi-label
classification. <em>ICCV</em>, 620–630. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Training a neural network model for recognizing multiple labels associated with an image, including identifying unseen labels, is challenging, especially for images that portray numerous semantically diverse labels. As challenging as this task is, it is an essential task to tackle since it represents many real-world cases, such as image retrieval of natural images. We argue that using a single embedding vector to represent an image, as commonly practiced, is not sufficient to rank both relevant seen and unseen labels accurately. This study introduces an end-to-end model training for multi-label zero-shot learning that supports the semantic diversity of the images and labels. We propose to use an embedding matrix having principal embedding vectors trained using a tailored loss function. In addition, during training, we suggest up-weighting in the loss function image samples presenting higher semantic diversity to encourage the diversity of the embedding matrix. Extensive experiments show that our proposed method improves the zero-shot model’s quality in tag-based image retrieval achieving SoTA results on several common datasets (NUS-Wide, COCO, Open Images).},
  archive   = {C_ICCV},
  author    = {Avi Ben-Cohen and Nadav Zamir and Emanuel Ben Baruch and Itamar Friedman and Lihi Zelnik-Manor},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00068},
  pages     = {620-630},
  title     = {Semantic diversity learning for zero-shot multi-label classification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Self supervision to distillation for long-tailed visual
recognition. <em>ICCV</em>, 610–619. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep learning has achieved remarkable progress for visual recognition on large-scale balanced datasets but still performs poorly on real-world long-tailed data. Previous methods often adopt class re-balanced training strategies to effectively alleviate the imbalance issue, but might be a risk of over-fitting tail classes. The recent decoupling method overcomes over-fitting issues by using a multi-stage training scheme, yet, it is still incapable of capturing tail class information in the feature learning stage. In this paper, we show that soft label can serve as a powerful solution to incorporate label correlation into a multi-stage training scheme for long-tailed recognition. The intrinsic relation between classes embodied by soft labels turns out to be helpful for long-tailed recognition by transferring knowledge from head to tail classes.Specifically, we propose a conceptually simple yet particularly effective multi-stage training scheme, termed as Self Supervised to Distillation (SSD). This scheme is composed of two parts. First, we introduce a self-distillation framework for long-tailed recognition, which can mine the label relation automatically. Second, we present a new distillation label generation module guided by self-supervision. The distilled labels integrate information from both label and data domains that can model long-tailed distribution effectively. We conduct extensive experiments and our method achieves the state-of-the-art results on three long-tailed recognition benchmarks: ImageNet-LT, CIFAR100-LT and iNaturalist 2018. Our SSD outperforms the strong LWS baseline by from 2.7\% to 4.5\% on various datasets.},
  archive   = {C_ICCV},
  author    = {Tianhao Li and Limin Wang and Gangshan Wu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00067},
  pages     = {610-619},
  title     = {Self supervision to distillation for long-tailed visual recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stochastic partial swap: Enhanced model generalization and
interpretability for fine-grained recognition. <em>ICCV</em>, 600–609.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning mid-level representation for fine-grained recognition is easily dominated by a limited number of highly discriminative patterns, degrading its robustness and generalization capability. To this end, we propose a novel Stochastic Partial Swap (SPS) 1 scheme to address this issue. Our method performs element-wise swapping for partial features between samples to inject noise during training. It equips a regularization effect similar to Dropout, which promotes more neurons to represent the concepts. Furthermore, it also exhibits other advantages: 1) suppressing over-activation to some part patterns to improve feature representativeness, and 2) enriching pattern combination and simulating noisy cases to enhance classifier generalization. We verify the effectiveness of our approach through comprehensive experiments across four network backbones and three fine-grained datasets. Moreover, we demonstrate its ability to complement high-level representations, allowing a simple model to achieve performance comparable to the top-performing technologies in fine-grained recognition, indoor scene recognition, and material recognition while improving model interpretability.},
  archive   = {C_ICCV},
  author    = {Shaoli Huang and Xinchao Wang and Dacheng Tao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00066},
  pages     = {600-609},
  title     = {Stochastic partial swap: Enhanced model generalization and interpretability for fine-grained recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Joint representation learning and novel category discovery
on single- and multi-modal data. <em>ICCV</em>, 590–599. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies the problem of novel category discovery on single- and multi-modal data with labels from different but relevant categories. We present a generic, end-to-end framework to jointly learn a reliable representation and assign clusters to unlabelled data. To avoid over-fitting the learnt embedding to labelled data, we take inspiration from self-supervised representation learning by noise-contrastive estimation and extend it to jointly handle labelled and unlabelled data. In particular, we propose using category discrimination on labelled data and cross-modal discrimination on multi-modal data to augment instance discrimination used in conventional contrastive learning approaches. We further employ Winner-Take-All (WTA) hashing algorithm on the shared representation space to generate pairwise pseudo labels for unlabelled data to better predict cluster assignments. We thoroughly evaluate our framework on large-scale multi-modal video benchmarks Kinetics-400 and VGG-Sound, and image benchmarks CIFAR10, CIFAR100 and ImageNet, obtaining state-of-the-art results.},
  archive   = {C_ICCV},
  author    = {Xuhui Jia and Kai Han and Yukun Zhu and Bradley Green},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00065},
  pages     = {590-599},
  title     = {Joint representation learning and novel category discovery on single- and multi-modal data},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Visual transformers: Where do transformers really belong in
vision models? <em>ICCV</em>, 579–589. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A recent trend in computer vision is to replace convolutions with transformers. However, the performance gain of transformers is attained at a steep cost, requiring GPU years and hundreds of millions of samples for training. This excessive resource usage compensates for a misuse of transformers: Transformers densely model relationships between its inputs - ideal for late stages of a neural network, when concepts are sparse and spatially-distant, but extremely inefficient for early stages of a network, when patterns are redundant and localized. To address these issues, we leverage the respective strengths of both operations, building convolution-transformer hybrids. Critically, in sharp contrast to pixel-space transformers, our Visual Transformer (VT) operates in a semantic token space, judiciously attending to different image parts based on context. Our VTs significantly outperforms baselines: On ImageNet, our VT-ResNets outperform convolution-only ResNet by 4.6 to 7 points and transformer-only ViT-B by 2.6 points with 2.5× fewer FLOPs, 2.1× fewer parameters. For semantic segmentation on LIP and COCO-stuff, VT-based feature pyramid networks (FPN) achieve 0.35 points higher mIoU while reducing the FPN module’s FLOPs by 6.5x.},
  archive   = {C_ICCV},
  author    = {Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Zhicheng Yan and Masayoshi Tomizuka and Joseph Gonzalez and Kurt Keutzer and Peter Vajda},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00064},
  pages     = {579-589},
  title     = {Visual transformers: Where do transformers really belong in vision models?},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Visformer: The vision-friendly transformer. <em>ICCV</em>,
569–578. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The past year has witnessed the rapid development of applying the Transformer module to vision problems. While some researchers have demonstrated that Transformer-based models enjoy a favorable ability of fitting data, there are still growing number of evidences showing that these models suffer over-fitting especially when the training data is limited. This paper offers an empirical study by performing step-by-step operations to gradually transit a Transformer-based model to a convolution-based model. The results we obtain during the transition process deliver useful messages for improving visual recognition. Based on these observations, we propose a new architecture named Visformer, which is abbreviated from the ‘Vision-friendly Transformer’. With the same computational complexity, Visformer outperforms both the Transformer-based and convolution-based models in terms of ImageNet classification accuracy, and the advantage becomes more significant when the model complexity is lower or the training set is smaller. The code is available at https://github.com/danczs/Visformer.},
  archive   = {C_ICCV},
  author    = {Zhengsu Chen and Lingxi Xie and Jianwei Niu and Xuefeng Liu and Longhui Wei and Qi Tian},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00063},
  pages     = {569-578},
  title     = {Visformer: The vision-friendly transformer},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Incorporating convolution designs into visual transformers.
<em>ICCV</em>, 559–568. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motivated by the success of Transformers in natural language processing (NLP) tasks, there emerge some attempts (e.g., ViT and DeiT) to apply Transformers to the vision domain. However, pure Transformer architectures often require a large amount of training data or extra supervision to obtain comparable performance with convolutional neural networks (CNNs). To overcome these limitations, we analyze the potential drawbacks when directly borrowing Transformer architectures from NLP. Then we propose a new Convolution-enhanced image Transformer (CeiT) which combines the advantages of CNNs in extracting low-level features, strengthening locality, and the advantages of Transformers in establishing long-range dependencies. Three modifications are made to the original Transformer: 1) instead of the straightforward tokenization from raw input images, we design an Image-to-Tokens (I2T) module that extracts patches from generated low-level features; 2) the feed-froward network in each encoder block is replaced with a Locally-enhanced Feed-Forward (LeFF) layer that promotes the correlation among neighboring tokens in the spatial dimension; 3) a Layer-wise Class token Attention (LCA) is attached at the top of the Transformer that utilizes the multi-level representations.Experimental results on ImageNet and seven down-stream tasks show the effectiveness and generalization ability of CeiT compared with previous Transformers and state- of-the-art CNNs, without requiring a large amount of training data and extra CNN teachers. Besides, CeiT models demonstrate better convergence with 3× fewer training iterations, which can reduce the training cost significantly 1 .},
  archive   = {C_ICCV},
  author    = {Kun Yuan and Shaopeng Guo and Ziwei Liu and Aojun Zhou and Fengwei Yu and Wei Wu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00062},
  pages     = {559-568},
  title     = {Incorporating convolution designs into visual transformers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Pyramid vision transformer: A versatile backbone for dense
prediction without convolutions. <em>ICCV</em>, 548–558. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although convolutional neural networks (CNNs) have achieved great success in computer vision, this work investigates a simpler, convolution-free backbone network use-fid for many dense prediction tasks. Unlike the recently-proposed Vision Transformer (ViT) that was designed for image classification specifically, we introduce the Pyramid Vision Transformer (PVT), which overcomes the difficulties of porting Transformer to various dense prediction tasks. PVT has several merits compared to current state of the arts. (1) Different from ViT that typically yields low-resolution outputs and incurs high computational and memory costs, PVT not only can be trained on dense partitions of an image to achieve high output resolution, which is important for dense prediction, but also uses a progressive shrinking pyramid to reduce the computations of large feature maps. (2) PVT inherits the advantages of both CNN and Transformer, making it a unified backbone for various vision tasks without convolutions, where it can be used as a direct replacement for CNN backbones. (3) We validate PVT through extensive experiments, showing that it boosts the performance of many downstream tasks, including object detection, instance and semantic segmentation. For example, with a comparable number of parameters, PVT+RetinaNet achieves 40.4 AP on the COCO dataset, surpassing ResNet50+RetinNet (36.3 AP) by 4.1 absolute AP (see Figure 2). We hope that PVT could, serre as an alternative and useful backbone for pixel-level predictions and facilitate future research.},
  archive   = {C_ICCV},
  author    = {Wenhai Wang and Enze Xie and Xiang Li and Deng-Ping Fan and Kaitao Song and Ding Liang and Tong Lu and Ping Luo and Ling Shao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00061},
  pages     = {548-558},
  title     = {Pyramid vision transformer: A versatile backbone for dense prediction without convolutions},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tokens-to-token ViT: Training vision transformers from
scratch on ImageNet. <em>ICCV</em>, 538–547. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Transformers, which are popular for language modeling, have been explored for solving vision tasks recently, e.g., the Vision Transformer (ViT) for image classification. The ViT model splits each image into a sequence of tokens with fixed length and then applies multiple Transformer layers to model their global relation for classification. However, ViT achieves inferior performance to CNNs when trained from scratch on a midsize dataset like ImageNet. We find it is because: 1) the simple tokenization of input images fails to model the important local structure such as edges and lines among neighboring pixels, leading to low training sample efficiency; 2) the redundant attention backbone design of ViT leads to limited feature richness for fixed computation budgets and limited training samples. To overcome such limitations, we propose a new Tokens-To-Token Vision Transformer (T2T-VTT), which incorporates 1) a layer-wise Tokens-to-Token (T2T) transformation to progressively structurize the image to tokens by recursively aggregating neighboring Tokens into one Token (Tokens-to-Token), such that local structure represented by surrounding tokens can be modeled and tokens length can be reduced; 2) an efficient backbone with a deep-narrow structure for vision transformer motivated by CNN architecture design after empirical study. Notably, T2T-ViT reduces the parameter count and MACs of vanilla ViT by half, while achieving more than 3.0\% improvement when trained from scratch on ImageNet. It also outperforms ResNets and achieves comparable performance with MobileNets by directly training on ImageNet. For example, T2T-ViT with comparable size to ResNet50 (21.5M parameters) can achieve 83.3\% top1 accuracy in image resolution 384x384 on ImageNet. 1},
  archive   = {C_ICCV},
  author    = {Li Yuan and Yunpeng Chen and Tao Wang and Weihao Yu and Yujun Shi and Zihang Jiang and Francis E. H. Tay and Jiashi Feng and Shuicheng Yan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00060},
  pages     = {538-547},
  title     = {Tokens-to-token ViT: Training vision transformers from scratch on ImageNet},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Point cloud augmentation with weighted local
transformations. <em>ICCV</em>, 528–537. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite the extensive usage of point clouds in 3D vision, relatively limited data are available for training deep neural networks. Although data augmentation is a standard approach to compensate for the scarcity of data, it has been less explored in the point cloud literature. In this paper, we propose a simple and effective augmentation method called PointWOLF for point cloud augmentation. The proposed method produces smoothly varying non-rigid deformations by locally weighted transformations centered at multiple anchor points. The smooth deformations allow diverse and realistic augmentations. Furthermore, in order to minimize the manual efforts to search the optimal hyperparameters for augmentation, we present AugTune, which generates augmented samples of desired difficulties producing targeted confidence scores. Our experiments show our framework consistently improves the performance for both shape classification and part segmentation tasks. Particularly, with PointNet++, PointWOLF achieves the state-of-the-art 89.7 accuracy on shape classification with the real-world ScanObjectNN dataset. The code is available at https://github.com/mlvlab/PointWOLF.},
  archive   = {C_ICCV},
  author    = {Sihyeon Kim and Sanghyeok Lee and Dasol Hwang and Jaewon Lee and Seong Jae Hwang and Hyunwoo J. Kim},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00059},
  pages     = {528-537},
  title     = {Point cloud augmentation with weighted local transformations},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Continual learning on noisy data streams via self-purified
replay. <em>ICCV</em>, 517–527. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Continually learning in the real world must overcome many challenges, among which noisy labels are a common and inevitable issue. In this work, we present a replay-based continual learning framework that simultaneously addresses both catastrophic forgetting and noisy labels for the first time. Our solution is based on two observations; (i) forgetting can be mitigated even with noisy labels via self-supervised learning, and (ii) the purity of the replay buffer is crucial. Building on this regard, we propose two key components of our method: (i) a self-supervised replay technique named Self-Replay which can circumvent erroneous training signals arising from noisy labeled data, and (ii) the Self-Centered filter that maintains a purified replay buffer via centrality-based stochastic graph ensembles. The empirical results on MNIST, CIFAR-10, CIFAR-100, and WebVision with real-world noise demonstrate that our framework can maintain a highly pure replay buffer amidst noisy streamed data while greatly outperforming the combinations of the state-of-the-art continual learning and noisy label learning methods.},
  archive   = {C_ICCV},
  author    = {Chris Dongjoo Kim and Jinseo Jeong and Sangwoo Moon and Gunhee Kim},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00058},
  pages     = {517-527},
  title     = {Continual learning on noisy data streams via self-purified replay},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Aggregation with feature detection. <em>ICCV</em>, 507–516.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Aggregating features from different depths of a network is widely adopted to improve the network capability. Lots of modern architectures are equipped with skip connections, which actually makes the feature aggregation happen in all these networks. Since different features tell different semantic meanings, there are inconsistencies and incompatibilities to be solved. However, existing works naively blend deep features via element-wise summation or concatenation with a convolution behind. Better feature aggregation method beyond summation or concatenation is rarely explored. In this paper, given two layers of features to be aggregated together, we first detect and identify where and what needs to be updated in one layer, then replace the feature at the identified location with the information of the other layer This process, which we call DEtect-rePLAce (DEPLA), enables us to avoid inconsistent patterns while keeping useful information in the merged outputs. Experimental results demonstrate our method largely boosts multiple baselines e.g. ResNet, FishNet and FPN on three major vision tasks including ImageNet classification, MS COCO object detection and instance segmentation.},
  archive   = {C_ICCV},
  author    = {Shuyang Sun and Xiaoyu Yue and Xiaojuan Qi and Wanli Ouyang and Victor Prisacariu and Philip Torr},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00057},
  pages     = {507-516},
  title     = {Aggregation with feature detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning meta-class memory for few-shot semantic
segmentation. <em>ICCV</em>, 497–506. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Currently, the state-of-the-art methods treat few-shot semantic segmentation task as a conditional foreground-background segmentation problem, assuming each class is independent. In this paper, we introduce the concept of meta-class, which is the meta information (e.g. certain middle-level features) shareable among all classes. To explicitly learn meta-class representations in few-shot segmentation task, we propose a novel Meta-class Memory based few-shot segmentation method (MM-Net), where we introduce a set of learnable memory embeddings to memorize the meta-class information during the base class training and transfer to novel classes during the inference stage. Moreover, for the k-shot scenario, we propose a novel image quality measurement module to select images from the set of support images. A high-quality class prototype could be obtained with the weighted sum of support image features based on the quality measure. Experiments on both PASCAL-5 i and COCO datasets show that our proposed method is able to achieve state-of-the-art results in both 1-shot and 5-shot settings. Particularly, our proposed MM-Net achieves 37.5\% mIoU on the COCO dataset in 1-shot setting, which is 5.1\% higher than the previous state-of-the-art.},
  archive   = {C_ICCV},
  author    = {Zhonghua Wu and Xiangxi Shi and Guosheng Lin and Jianfei Cai},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00056},
  pages     = {497-506},
  title     = {Learning meta-class memory for few-shot semantic segmentation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to resize images for computer vision tasks.
<em>ICCV</em>, 487–496. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For all the ways convolutional neural nets have revolutionized computer vision in recent years, one important aspect has received surprisingly little attention: the effect of image size on the accuracy of tasks being trained for. Typically, to be efficient, the input images are resized to a relatively small spatial resolution (e.g. 224 × 224), and both training and inference are carried out at this resolution. The actual mechanism for this re-scaling has been an afterthought: Namely, off-the-shelf image re sizers such as bilinear and bicubic are commonly used in most machine learning software frameworks. But do these re sizers limit the on-task performance of the trained networks? The answer is yes. Indeed, we show that the typical linear re sizer can be replaced with learned resizers that can substantially improve performance. Importantly, while the classical re-sizers typically result in better perceptual quality of the downscaled images, our proposed learned resizers do not necessarily give better visual quality, but instead improve task performance.Our learned image resizer is jointly trained with a base-line vision model. This learned CNN-based resizer creates machine friendly visual manipulations that lead to a consistent improvement of the end task metric over the baseline model. Specifically, here we focus on the classification task with the ImageNet dataset [26], and experiment with four different models to learn resizers adapted to each model. Moreover, we show that the proposed resizer can also be useful for fine-tuning the classification baselines for other vision tasks. To this end, we experiment with three different baselines to develop image quality assessment (IQA) models on the AVA dataset [24].},
  archive   = {C_ICCV},
  author    = {Hossein Talebi and Peyman Milanfar},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00055},
  pages     = {487-496},
  title     = {Learning to resize images for computer vision tasks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploration and estimation for model compression.
<em>ICCV</em>, 477–486. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep neural networks achieve great success in many visual recognition tasks. However, the model deployment is usually subject to some computational resources. Model pruning under computational budget has attracted growing attention. In this paper, we focus on the discrimination-aware compression of Convolutional Neural Networks (CNNs). In prior arts, directly searching the optimal sub-network is an integer programming problem, which is non-smooth, non-convex, and NP-hard. Meanwhile, the heuristic pruning criterion lacks clear interpretability and doesn’t generalize well in applications. To address this problem, we formulate sub-networks as samples from a multivariate Bernoulli distribution and resort to the approximation of continuous problem. We propose a new flexible search scheme via alternating exploration and estimation. In the exploration step, we employ stochastic gradient Hamiltonian Monte Carlo with budget-awareness to generate sub-networks, which allows large search space with efficient computation. In the estimation step, we deduce the sub-network sampler to a near-optimal point, to promote the generation of high-quality sub-networks. Unifying the exploration and estimation, our approach avoids early falling into local minimum via a fast gradient-based search in a larger space. Extensive experiments on CIFAR-10 and ImageNet show that our method achieves state-of-the-art performances on pruning several popular CNNs.},
  archive   = {C_ICCV},
  author    = {Yanfu Zhang and Shangqian Gao and Heng Huang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00054},
  pages     = {477-486},
  title     = {Exploration and estimation for model compression},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Group-wise inhibition based feature regularization for
robust classification. <em>ICCV</em>, 468–476. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The convolutional neural network (CNN) is vulnerable to degraded images with even very small variations (e.g. corrupted and adversarial samples). One of the possible reasons is that CNN pays more attention to the most discriminative regions, but ignores the auxiliary features when learning, leading to the lack of feature diversity for final judgment. In our method, we propose to dynamically suppress significant activation values of CNN by group-wise inhibition, but not fixedly or randomly handle them when training. The feature maps with different activation distribution are then processed separately to take the feature independence into account. CNN is finally guided to learn richer discriminative features hierarchically for robust classification according to the proposed regularization. Our method is comprehensively evaluated under multiple settings, including classification against corruptions, adversarial attacks and low data regime. Extensive experimental results show that the proposed method can achieve significant improvements in terms of both robustness and generalization performances, when compared with the state-of-the-art methods. Code is available at https://github.com/LinusWu/TENET_Training.},
  archive   = {C_ICCV},
  author    = {Haozhe Liu and Haoqian Wu and Weicheng Xie and Feng Liu and Linlin Shen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00053},
  pages     = {468-476},
  title     = {Group-wise inhibition based feature regularization for robust classification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MicroNet: Improving image recognition with extremely low
FLOPs. <em>ICCV</em>, 458–467. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper aims at addressing the problem of substantial performance degradation at extremely low computational cost (e.g. 5M FLOPs on ImageNet classification). We found that two factors, sparse connectivity and dynamic activation function, are effective to improve the accuracy. The former avoids the significant reduction of network width, while the latter mitigates the detriment of reduction in network depth. Technically, we propose micro-factorized convolution, which factorizes a convolution matrix into low rank matrices, to integrate sparse connectivity into convolution. We also present a new dynamic activation function, named Dynamic Shift Max, to improve the non-linearity via maxing out multiple dynamic fusions between an input feature map and its circular channel shift. Building upon these two new operators, we arrive at a family of networks, named MicroNet, that achieves significant performance gains over the state of the art in the low FLOP regime. For instance, under the constraint of 12M FLOPs, MicroNet achieves 59.4\% top-1 accuracy on ImageNet classification, outperforming MobileNetV3 by 9.6\%. Source code is at https://github.com/liyunsheng13/micronet.},
  archive   = {C_ICCV},
  author    = {Yunsheng Li and Yinpeng Chen and Xiyang Dai and Dongdong Chen and Mengchen Liu and Lu Yuan and Zicheng Liu and Lei Zhang and Nuno Vasconcelos},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00052},
  pages     = {458-467},
  title     = {MicroNet: Improving image recognition with extremely low FLOPs},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Amplitude-phase recombination: Rethinking robustness of
convolutional neural networks in frequency domain. <em>ICCV</em>,
448–457. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, the generalization behavior of Convolutional Neural Networks (CNN) is gradually transparent through explanation techniques with the frequency components decomposition. However, the importance of the phase spectrum of the image for a robust vision system is still ignored. In this paper, we notice that the CNN tends to converge at the local optimum which is closely related to the high-frequency components of the training images, while the amplitude spectrum is easily disturbed such as noises or common corruptions. In contrast, more empirical studies found that humans rely on more phase components to achieve robust recognition. This observation leads to more explanations of the CNN’s generalization behaviors in both robustness to common perturbations and out-of-distribution detection, and motivates a new perspective on data augmentation designed by re-combing the phase spectrum of the current image and the amplitude spectrum of the distracter image. That is, the generated samples force the CNN to pay more attention to the structured information from phase components and keep robust to the variation of the amplitude. Experiments on several image datasets indicate that the proposed method achieves state-of-the-art performances on multiple generalizations and calibration tasks, including adaptability for common corruptions and surface variations, out-of-distribution detection, and adversarial attack. The code is released on github/iCGY96/APR.},
  archive   = {C_ICCV},
  author    = {Guangyao Chen and Peixi Peng and Li Ma and Jia Li and Lin Du and Yonghong Tian},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00051},
  pages     = {448-457},
  title     = {Amplitude-phase recombination: Rethinking robustness of convolutional neural networks in frequency domain},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An asynchronous kalman filter for hybrid event cameras.
<em>ICCV</em>, 438–447. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Event cameras are ideally suited to capture HDR visual information without blur but perform poorly on static or slowly changing scenes. Conversely, conventional image sensors measure absolute intensity of slowly changing scenes effectively but do poorly on high dynamic range or quickly changing scenes. In this paper, we present an event-based video reconstruction pipeline for High Dynamic Range (HDR) scenarios. The proposed algorithm includes a frame augmentation pre-processing step that deblurs and temporally interpolates frame data using events. The augmented frame and event data are then fused using a novel asynchronous Kalman filter under a unifying uncertainty model for both sensors. Our experimental results are evaluated on both publicly available datasets with challenging lighting conditions and fast motions and our new dataset with HDR reference. The proposed algorithm outperforms state-of-the-art methods in both absolute intensity error (48\% reduction) and image similarity indexes (average 11\% improvement).},
  archive   = {C_ICCV},
  author    = {Ziwei Wang and Yonhon Ng and Cedric Scheerlinck and Robert Mahony},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00050},
  pages     = {438-447},
  title     = {An asynchronous kalman filter for hybrid event cameras},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Virtual multi-modality self-supervised foreground matting
for human-object interaction. <em>ICCV</em>, 428–437. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most existing human matting algorithms tried to separate pure human-only foreground from the background. In this paper, we propose a Virtual Multi-modality Foreground Matting (VMFM) method to learn human-object interactive foreground (human and objects interacted with him or her) from a raw RGB image. The VMFM method requires no additional inputs, e.g. trimap or known background. We reformulate foreground matting as a self-supervised multi-modality problem: factor each input image into estimated depth map, segmentation mask, and interaction heatmap using three auto-encoders. In order to fully utilize the characteristics of each modality, we first train a dual encoder-to-decoder network to estimate the same alpha matte. Then we introduce a self-supervised method: Complementary Learning(CL) to predict deviation probability map and exchange reliable gradients across modalities without label. We conducted extensive experiments to analyze the effectiveness of each modality and the significance of different components in complementary learning. We demonstrate that our model outperforms the state-of-the-art methods.},
  archive   = {C_ICCV},
  author    = {Bo Xu and Han Huang and Cheng Lu and Ziwen Li and Yandong Guo},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00049},
  pages     = {428-437},
  title     = {Virtual multi-modality self-supervised foreground matting for human-object interaction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towers of babel: Combining images, language, and 3D geometry
for learning multimodal vision. <em>ICCV</em>, 418–427. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The abundance and richness of Internet photos of landmarks and cities has led to significant progress in 3D vision over the past two decades, including automated 3D reconstructions of the world’s landmarks from tourist photos. However, a major source of information available for these 3D-augmented collections—namely language, e.g., from image captions—has been virtually untapped. In this work, we present WikiScenes, a new, large-scale dataset of landmark photo collections that contains descriptive text in the form of captions and hierarchical category names. WikiScenes forms a new testbed for multimodal reasoning involving images, text, and 3D geometry. We demonstrate the utility of WikiScenes for learning semantic concepts over images and 3D models. Our weakly-supervised framework connects images, 3D structure, and semantics—utilizing the strong constraints provided by 3D geometry—to associate semantic concepts to image pixels and 3D points. 1},
  archive   = {C_ICCV},
  author    = {Xiaoshi Wu and Hadar Averbuch-Elor and Jin Sun and Noah Snavely},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00048},
  pages     = {418-427},
  title     = {Towers of babel: Combining images, language, and 3D geometry for learning multimodal vision},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MosaicOS: A simple and effective use of object-centric
images for long-tailed object detection. <em>ICCV</em>, 407–417. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many objects do not appear frequently enough in complex scenes (e.g., certain handbags in living rooms) for training an accurate object detector, but are often found frequently by themselves (e.g., in product images). Yet, these object-centric images are not effectively leveraged for improving object detection in scene-centric images. In this paper, we propose Mosaic of Object-centric images as Scene-centric images (MosaicOS), a simple and novel framework that is surprisingly effective at tackling the challenges of long-tailed object detection. Keys to our approach are three-fold: (i) pseudo scene-centric image construction from object-centric images for mitigating domain differences, (ii) high-quality bounding box imputation using the object-centric images’ class labels, and (iii) a multi-stage training procedure. On LVIS object detection (and instance segmentation), MosaicOS leads to a massive 60\% (and 23\%) relative improvement in average precision for rare object categories. We also show that our framework can be compatibly used with other existing approaches to achieve even further gains. Our pre-trained models are publicly available at https://github.com/czhang0528/MosaicOS/.},
  archive   = {C_ICCV},
  author    = {Cheng Zhang and Tai-Yu Pan and Yandong Li and Hexiang Hu and Dong Xuan and Soravit Changpinyo and Boqing Gong and Wei-Lun Chao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00047},
  pages     = {407-417},
  title     = {MosaicOS: A simple and effective use of object-centric images for long-tailed object detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning canonical view representation for 3D shape
recognition with arbitrary views. <em>ICCV</em>, 397–406. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we focus on recognizing 3D shapes from arbitrary views, i.e., arbitrary numbers and positions of viewpoints. It is a challenging and realistic setting for view-based 3D shape recognition. We propose a canonical view representation to tackle this challenge. We first transform the original features of arbitrary views to a fixed number of view features, dubbed canonical view representation, by aligning the arbitrary view features to a set of learnable reference view features using optimal transport. In this way, each 3D shape with arbitrary views is represented by a fixed number of canonical view features, which are further aggregated to generate a rich and robust 3D shape representation for shape recognition. We also propose a canonical view feature separation constraint to enforce that the view features in canonical view representation can be embedded into scattered points in a Euclidean space. Experiments on the ModelNet40, ScanObjectNN, and RGBD datasets show that our method achieves competitive results under the fixed viewpoint settings, and significantly outperforms the applicable methods under the arbitrary view setting.},
  archive   = {C_ICCV},
  author    = {Xin Wei and Yifei Gong and Fudong Wang and Xing Sun and Jian Sun},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00046},
  pages     = {397-406},
  title     = {Learning canonical view representation for 3D shape recognition with arbitrary views},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Generic attention-model explainability for interpreting
bi-modal and encoder-decoder transformers. <em>ICCV</em>, 387–396. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Transformers are increasingly dominating multi-modal reasoning tasks, such as visual question answering, achieving state-of-the-art results thanks to their ability to contextualize information using the self-attention and co-attention mechanisms. These attention modules also play a role in other computer vision tasks including object detection and image segmentation. Unlike Transformers that only use self-attention, Transformers with co-attention require to consider multiple attention maps in parallel in order to highlight the information that is relevant to the prediction in the model’s input. In this work, we propose the first method to explain prediction by any Transformer-based architecture, including bi-modal Transformers and Transformers with co-attentions. We provide generic solutions and apply these to the three most commonly used of these architectures: (i) pure self-attention, (ii) self-attention combined with co-attention, and (iii) encoder-decoder attention. We show that our method is superior to all existing methods which are adapted from single modality explainability. Our code is available at: https://github.com/hila-chefer/Transformer-MM-Explainability.},
  archive   = {C_ICCV},
  author    = {Hila Chefer and Shir Gur and Lior Wolf},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00045},
  pages     = {387-396},
  title     = {Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Vision transformer with progressive sampling. <em>ICCV</em>,
377–386. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Transformers with powerful global relation modeling abilities have been introduced to fundamental computer vision tasks recently. As a typical example, the Vision Trans-former (ViT) directly applies a pure transformer architecture on image classification, by simply splitting images into tokens with a fixed length, and employing transformers to learn relations between these tokens. However, such naive tokenization could destruct object structures, assign grids to uninterested regions such as background, and introduce interference signals. To mitigate the above issues, in this paper, we propose an iterative and progressive sampling strategy to locate discriminative regions. At each iteration, embeddings of the current sampling step are fed into a transformer encoder layer, and a group of sampling off-sets is predicted to update the sampling locations for the next step. The progressive sampling is differentiable. When combined with the Vision Transformer, the obtained PS-ViT network can adaptively learn where to look. The proposed PS-ViT is both effective and efficient. When trained from scratch on ImageNet, PS-ViT performs 3.8\% higher than the vanilla ViT in terms of top-1 accuracy with about 4× fewer parameters and 10× fewer FLOPs. Code is available at https://github.com/yuexy/PS-ViT.},
  archive   = {C_ICCV},
  author    = {Xiaoyu Yue and Shuyang Sun and Zhanghui Kuang and Meng Wei and Philip Torr and Wayne Zhang and Dahua Lin},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00044},
  pages     = {377-386},
  title     = {Vision transformer with progressive sampling},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scalable vision transformers with hierarchical pooling.
<em>ICCV</em>, 367–376. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The recently proposed Visual image Transformers (ViT) with pure attention have achieved promising performance on image recognition tasks, such as image classification. However, the routine of the current ViT model is to maintain a full-length patch sequence during inference, which is redundant and lacks hierarchical representation. To this end, we propose a Hierarchical Visual Transformer (HVT) which progressively pools visual tokens to shrink the sequence length and hence reduces the computational cost, analogous to the feature maps downsampling in Convolutional Neural Networks (CNNs). It brings a great benefit that we can increase the model capacity by scaling dimensions of depth/width/resolution/patch size without introducing extra computational complexity due to the reduced sequence length. Moreover, we empirically find that the average pooled visual tokens contain more discriminative information than the single class token. To demonstrate the improved scalability of our HVT, we conduct extensive experiments on the image classification task. With comparable FLOPs, our HVT outperforms the competitive baselines on ImageNet and CIFAR-100 datasets. Code is available at https://github.com/MonashAI/HVT.},
  archive   = {C_ICCV},
  author    = {Zizheng Pan and Bohan Zhuang and Jing Liu and Haoyu He and Jianfei Cai},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00043},
  pages     = {367-376},
  title     = {Scalable vision transformers with hierarchical pooling},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Conformer: Local features coupling global representations
for visual recognition. <em>ICCV</em>, 357–366. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Within Convolutional Neural Network (CNN), the convolution operations are good at extracting local features but experience difficulty to capture global representations. Within visual transformer, the cascaded self-attention modules can capture long-distance feature dependencies but unfortunately deteriorate local feature details. In this paper, we propose a hybrid network structure, termed Conformer, to take advantage of convolutional operations and self-attention mechanisms for enhanced representation learning. Conformer roots in the Feature Coupling Unit (FCU), which fuses local features and global representations under different resolutions in an interactive fashion. Conformer adopts a concurrent structure so that local features and global representations are retained to the maximum extent. Experiments show that Conformer, under the comparable parameter complexity, outperforms the visual transformer (DeiT-B) by 2.3\% on ImageNet. On MSCOCO, it outperforms ResNet-101 by 3.7\% and 3.6\% mAPs for object detection and instance segmentation, respectively, demonstrating the great potential to be a general backbone network. Code is available at github.com/pengzhiliang/Conformer.},
  archive   = {C_ICCV},
  author    = {Zhiliang Peng and Wei Huang and Shanzhi Gu and Lingxi Xie and Yaowei Wang and Jianbin Jiao and Qixiang Ye},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00042},
  pages     = {357-366},
  title     = {Conformer: Local features coupling global representations for visual recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CrossViT: Cross-attention multi-scale vision transformer for
image classification. <em>ICCV</em>, 347–356. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The recently developed vision transformer (ViT) has achieved promising results on image classification compared to convolutional neural networks. Inspired by this, in this paper, we study how to learn multi-scale feature representations in transformer models for image classification. To this end, we propose a dual-branch transformer to com-bine image patches (i.e., tokens in a transformer) of different sizes to produce stronger image features. Our approach processes small-patch and large-patch tokens with two separate branches of different computational complexity and these tokens are then fused purely by attention multiple times to complement each other. Furthermore, to reduce computation, we develop a simple yet effective token fusion module based on cross attention, which uses a single token for each branch as a query to exchange information with other branches. Our proposed cross-attention only requires linear time for both computational and memory complexity instead of quadratic time otherwise. Extensive experiments demonstrate that our approach performs better than or on par with several concurrent works on vision transformer, in addition to efficient CNN models. For example, on the ImageNet1K dataset, with some architectural changes, our approach outperforms the recent DeiT by a large margin of 2\% with a small to moderate increase in FLOPs and model parameters. Our source codes and models are available at https://github.com/IBM/CrossViT.},
  archive   = {C_ICCV},
  author    = {Chun-Fu Richard Chen and Quanfu Fan and Rameswar Panda},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00041},
  pages     = {347-356},
  title     = {CrossViT: Cross-attention multi-scale vision transformer for image classification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Zen-NAS: A zero-shot NAS for high-performance image
recognition. <em>ICCV</em>, 337–346. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accuracy predictor is a key component in Neural Architecture Search (NAS) for ranking architectures. Building a high-quality accuracy predictor usually costs enormous computation. To address this issue, instead of using an accuracy predictor, we propose a novel zero-shot index dubbed Zen-Score to rank the architectures. The Zen-Score represents the network expressivity and positively correlates with the model accuracy. The calculation of Zen-Score only takes a few forward inferences through a randomly initialized network, without training network parameters. Built upon the Zen-Score, we further propose a new NAS algorithm, termed as Zen-NAS, by maximizing the Zen-Score of the target network under given inference budgets. Within less than half GPU day, Zen-NAS is able to directly search high performance architectures in a data-free style. Comparing with previous NAS methods, the proposed Zen-NAS is magnitude times faster on multiple server-side and mobile-side GPU platforms with state-of-the-art accuracy on ImageNet. Searching and training code as well as pre-trained models are available from https://github.com/idstcv/ZenNAS.},
  archive   = {C_ICCV},
  author    = {Ming Lin and Pichao Wang and Zhenhong Sun and Hesen Chen and Xiuyu Sun and Qi Qian and Hao Li and Rong Jin},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00040},
  pages     = {337-346},
  title     = {Zen-NAS: A zero-shot NAS for high-performance image recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AutoSpace: Neural architecture search with less human
interference. <em>ICCV</em>, 327–336. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current neural architecture search (NAS) algorithms still require expert knowledge and effort to design a search space for network construction. In this paper, we consider automating the search space design to minimize human interference, which however faces two challenges: the ex-plosive complexity of the exploration space and the expensive computation cost to evaluate the quality of different search spaces. To solve them, we propose a novel differentiable evolutionary framework named AutoSpace, which evolves the search space to an optimal one with following novel techniques: a differentiable fitness scoring function to efficiently evaluate the performance of cells and a reference architecture to speedup the evolution procedure and avoid falling into sub-optimal solutions. The frame-work is generic and compatible with additional computational constraints, making it feasible to learn specialized search spaces that fit different computational bud-gets. With the learned search space, the performance of recent NAS algorithms can be improved significantly compared with using previously manually designed spaces. Remarkably, the models generated from the new search space achieve 77.8\% top-1 accuracy on ImageNet under the mobile setting (MAdds 500M), outperforming previous SOTA EfficientNet-B0 by≤0.7\%. https://github.com/zhoudaquan/AutoSpace.git.},
  archive   = {C_ICCV},
  author    = {Daquan Zhou and Xiaojie Jin and Xiaochen Lian and Linjie Yang and Yujing Xue and Qibin Hou and Jiashi Feng},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00039},
  pages     = {327-336},
  title     = {AutoSpace: Neural architecture search with less human interference},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Differentiable dynamic wirings for neural networks.
<em>ICCV</em>, 317–326. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A standard practice of deploying deep neural networks is to apply the same architecture to all the input instances. However, a fixed architecture may not be suitable for different data with high diversity. To boost the model capacity, existing methods usually employ larger convolutional kernels or deeper network layers, which incurs prohibitive computational costs. In this paper, we address this issue by proposing Differentiable Dynamic Wirings (DDW), which learns the instance-aware connectivity that creates different wiring patterns for different instances. 1) Specifically, the network is initialized as a complete directed acyclic graph, where the nodes represent convolutional blocks and the edges represent the connection paths. 2) We generate edge weights by a learnable module, Router, and select the edges whose weights are larger than a threshold, to adjust the connectivity of the neural network structure. 3) Instead of using the same path of the network, DDW aggregates features dynamically in each node, which allows the network to have more representation power.To facilitate effective training, we further represent the network connectivity of each sample as an adjacency matrix. The matrix is updated to aggregate features in the forward pass, cached in the memory, and used for gradient computing in the backward pass. We validate the effectiveness of our approach with several mainstream architectures, including MobileNetV2, ResNet, ResNeXt, and RegNet. Extensive experiments are performed on ImageNet classification and COCO object detection, which demonstrates the effectiveness and generalization ability of our approach.},
  archive   = {C_ICCV},
  author    = {Kun Yuan and Quanquan Li and Shaopeng Guo and Dapeng Chen and Aojun Zhou and Fengwei Yu and Ziwei Liu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00038},
  pages     = {317-326},
  title     = {Differentiable dynamic wirings for neural networks},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). BN-NAS: Neural architecture search with batch normalization.
<em>ICCV</em>, 307–316. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present BN-NAS, neural architecture search with Batch Normalization (BN-NAS), to accelerate neural architecture search (NAS). BN-NAS can significantly reduce the time required by model training and evaluation in NAS. Specifically, for fast evaluation, we propose a BN-based indicator for predicting subnet performance at a very early training stage. The BN-based indicator further facilitates us to improve the training efficiency by only training the BN parameters during the supernet training. This is based on our observation that training the whole supernet is not necessary while training only BN parameters accelerates network convergence for network architecture search. Extensive experiments show that our method can significantly shorten the time of training supernet by more than 10 times and shorten the time of evaluating subnets by more than 600,000 times without losing accuracy. The source codes are available at https://github.com/bychen515/BNNAS.},
  archive   = {C_ICCV},
  author    = {Boyu Chen and Peixia Li and Baopu Li and Chen Lin and Chuming Li and Ming Sun and Junjie Yan and Wanli Ouyang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00037},
  pages     = {307-316},
  title     = {BN-NAS: Neural architecture search with batch normalization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-modality associative bridging through memory: Speech
sound recollected from face video. <em>ICCV</em>, 296–306. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce a novel audio-visual multi-modal bridging framework that can utilize both audio and visual information, even with uni-modal inputs. We exploit a memory network that stores source (i.e., visual) and target (i.e., audio) modal representations, where source modal representation is what we are given, and target modal representations are what we want to obtain from the memory network. We then construct an associative bridge between source and target memories that considers the inter-relationship between the two memories. By learning the interrelationship through the associative bridge, the proposed bridging framework is able to obtain the target modal representations inside the memory network, even with the source modal input only, and it provides rich information for its downstream tasks. We apply the proposed framework to two tasks: lip reading and speech reconstruction from silent video. Through the proposed associative bridge and modality-specific memories, each task knowledge is enriched with the recalled audio context, achieving state-of-the-art performance. We also verify that the associative bridge properly relates the source and target memories.},
  archive   = {C_ICCV},
  author    = {Minsu Kim and Joanna Hong and Se Jin Park and Yong Man Ro},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00036},
  pages     = {296-306},
  title     = {Multi-modality associative bridging through memory: Speech sound recollected from face video},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image2Reverb: Cross-modal reverb impulse response synthesis.
<em>ICCV</em>, 286–295. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Measuring the acoustic characteristics of a space is often done by capturing its impulse response (IR), a representation of how a full-range stimulus sound excites it. This work generates an IR from a single image, which can then be applied to other signals using convolution, simulating the reverberant characteristics of the space shown in the image. Recording these IRs is both time-intensive and expensive, and often infeasible for inaccessible locations. We use an end-to-end neural network architecture to generate plausible audio impulse responses from single images of acoustic environments. We evaluate our method both by comparisons to ground truth data and by human expert evaluation. We demonstrate our approach by generating plausible impulse responses from diverse settings and formats including well known places, musical halls, rooms in paintings, images from animations and computer games, synthetic environments generated from text, panoramic images, and video conference backgrounds.},
  archive   = {C_ICCV},
  author    = {Nikhil Singh and Jeff Mentch and Jerry Ng and Matthew Beveridge and Iddo Drori},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00035},
  pages     = {286-295},
  title     = {Image2Reverb: Cross-modal reverb impulse response synthesis},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Move2Hear: Active audio-visual source separation.
<em>ICCV</em>, 275–285. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce the active audio-visual source separation problem, where an agent must move intelligently in order to better isolate the sounds coming from an object of interest in its environment. The agent hears multiple audio sources simultaneously (e.g., a person speaking down the hall in a noisy household) and it must use its eyes and ears to automatically separate out the sounds originating from a target object within a limited time budget. Towards this goal, we introduce a reinforcement learning approach that trains movement policies controlling the agent’s camera and microphone placement over time, guided by the improvement in predicted audio separation quality. We demonstrate our approach in scenarios motivated by both augmented reality (system is already co-located with the target object) and mobile robotics (agent begins arbitrarily far from the target object). Using state-of-the-art realistic audio-visual simulations in 3D environments, we demonstrate our model’s ability to find minimal movement sequences with maximal payoff for audio source separation. Project: http://vision.cs.utexas.edu/projects/move2hear.},
  archive   = {C_ICCV},
  author    = {Sagnik Majumder and Ziad Al-Halah and Kristen Grauman},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00034},
  pages     = {275-285},
  title     = {Move2Hear: Active audio-visual source separation},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MAAS: Multi-modal assignation for active speaker detection.
<em>ICCV</em>, 265–274. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Active speaker detection requires a mindful integration of multi-modal cues. Current methods focus on modeling and fusing short-term audiovisual features for individual speakers, often at frame level. We present a novel approach to active speaker detection that directly addresses the multi-modal nature of the problem and provides a straightforward strategy, where independent visual features (speakers) in the scene are assigned to a previously detected speech event. Our experiments show that a small graph data structure built from local information can approximate an instantaneous audio-visual assignment problem. More-over, the temporal extension of this initial graph achieves a new state-of-the-art performance on the AVA-ActiveSpeaker dataset with a mAP of 88.8\%.},
  archive   = {C_ICCV},
  author    = {Juan León Alcázar and Fabian Caba Heilbron and Ali K. Thabet and Bernard Ghanem},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00033},
  pages     = {265-274},
  title     = {MAAS: Multi-modal assignation for active speaker detection},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). When pigs fly: Contextual reasoning in synthetic and natural
scenes. <em>ICCV</em>, 255–264. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Context is of fundamental importance to both human and machine vision; e.g., an object in the air is more likely to be an airplane than a pig. The rich notion of context incorporates several aspects including physics rules, statistical co-occurrences, and relative object sizes, among others. While previous work has focused on crowd-sourced out-of-context photographs from the web to study scene context, controlling the nature and extent of contextual violations has been a daunting task. Here we introduce a diverse, synthetic Out-of-Context Dataset (OCD) with fine-grained control over scene context. By leveraging a 3D simulation engine, we systematically control the gravity, object co-occurrences and relative sizes across 36 object categories in a virtual household environment. We conducted a series of experiments to gain insights into the impact of contextual cues on both human and machine vision using OCD. We conducted psychophysics experiments to establish a human benchmark for out-of-context recognition, and then compared it with state-of-the-art computer vision models to quantify the gap between the two. We propose a context-aware recognition transformer model, fusing object and contextual information via multi-head attention. Our model captures useful information for contextual reasoning, enabling human-level performance and better robustness in out-of-context conditions compared to baseline models across OCD and other out-of-context datasets. All source code and data are publicly available at https://github.com/kreimanlab/WhenPigsFlyContext},
  archive   = {C_ICCV},
  author    = {Philipp Bomatter and Mengmi Zhang and Dimitar Karev and Spandan Madan and Claire Tseng and Gabriel Kreiman},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00032},
  pages     = {255-264},
  title     = {When pigs fly: Contextual reasoning in synthetic and natural scenes},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Neural photofit: Gaze-based mental image reconstruction.
<em>ICCV</em>, 245–254. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel method that leverages human fixations to visually decode the image a person has in mind into a photofit (facial composite). Our method combines three neural networks: An encoder, a scoring network, and a decoder. The encoder extracts image features and predicts a neural activation map for each face looked at by a human observer. A neural scoring network compares the human and neural attention and predicts a relevance score for each extracted image feature. Finally, image features are aggregated into a single feature vector as a linear combination of all features weighted by relevance which a decoder de-codes into the final photofit. We train the neural scoring network on a novel dataset containing gaze data of 19 participants looking at collages of synthetic faces. We show that our method significantly outperforms a mean baseline predictor and report on a human study that shows that we can decode photofits that are visually plausible and close to the observer’s mental image.},
  archive   = {C_ICCV},
  author    = {Florian Strohm and Ekta Sood and Sven Mayer and Philipp Müller and Mihai Bâce and Andreas Bulling},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00031},
  pages     = {245-254},
  title     = {Neural photofit: Gaze-based mental image reconstruction},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distilling virtual examples for long-tailed recognition.
<em>ICCV</em>, 235–244. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We tackle the long-tailed visual recognition problem from the knowledge distillation perspective by proposing a Distill the Virtual Examples (DiVE) method. Specifically, by treating the predictions of a teacher model as virtual examples, we prove that distilling from these virtual examples is equivalent to label distribution learning under certain constraints. We show that when the virtual example distribution becomes flatter than the original input distribution, the under-represented tail classes will receive significant improvements, which is crucial in long-tailed recognition. The proposed DiVE method can explicitly tune the virtual example distribution to become flat. Extensive experiments on three benchmark datasets, including the large-scale iNaturalist ones, justify that the proposed DiVE method can significantly outperform state-of-the-art methods. Further-more, additional analyses and experiments verify the virtual example interpretation, and demonstrate the effectiveness of tailored designs in DiVE for long-tailed problems.},
  archive   = {C_ICCV},
  author    = {Yin-Yin He and Jianxin Wu and Xiu-Shen Wei},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00030},
  pages     = {235-244},
  title     = {Distilling virtual examples for long-tailed recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Syncretic modality collaborative learning for visible
infrared person re-identification. <em>ICCV</em>, 225–234. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visible infrared person re-identification (VI-REID) aims to match pedestrian images between the daytime visible and nighttime infrared camera views. The large cross-modality discrepancies have become the bottleneck which limits the performance of VI-REID. Existing methods mainly focus on capturing cross-modality sharable representations by learning an identity classifier. However, the heterogeneous pedestrian images taken by different spectrum cameras differ significantly in image styles, resulting in inferior discriminability of feature representations. To alleviate the above problem, this paper explores the correlation between two modalities and proposes a novel syncretic modality collaborative learning (SMCL) model to bridge the cross-modality gap. A new modality that incorporates features of heterogeneous images is constructed automatically to steer the generation of modality-invariant representations. Challenge enhanced homogeneity learning (CEHL) and auxiliary distributional similarity learning (ADSL) are integrated to project heterogeneous features on a unified space and enlarge the inter-class disparity, thus strengthening the discriminative power. Extensive experiments on two cross-modality benchmarks demonstrate the effectiveness and superiority of the proposed method. Especially, on SYSU-MM01 dataset, our SMCL model achieves 67.39\% rank-1 accuracy and 61.78\% mAP, surpassing the cutting-edge works by a large margin.},
  archive   = {C_ICCV},
  author    = {Ziyu Wei and Xi Yang and Nannan Wang and Xinbo Gao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00029},
  pages     = {225-234},
  title     = {Syncretic modality collaborative learning for visible infrared person re-identification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attack-guided perceptual data generation for real-world
re-identification. <em>ICCV</em>, 215–224. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In unconstrained real-world surveillance scenarios, person re-identification (Re-ID) models usually suffer from different low-level perceptual variations, e.g., cross-resolution and insufficient lighting. Due to the limited variation range of training data, existing models are difficult to generalize to scenes with unknown perceptual interference types. To address the above problem, in this paper, we propose two disjoint data-generation ways to complement existing training samples to improve the robustness of Re-ID mod-els. Firstly, considering the sparsity and imbalance of samples in the perceptual space, a dense resampling method from the estimated perceptual distribution is performed. Secondly, to dig more representative generated samples for identity representation learning, we introduce a graph-based white-box attacker to guide the data generation process with intra-batch ranking and discriminate attention. In addition, two synthetic-to-real feature constraints are introduced into the Re-ID training to prevent the generated data from bringing domain bias. Our method is effective, easy-to-implement, and independent of the specific network architecture. Applying our approach to a ResNet-50 base-line can already achieve competitive results, surpassing state-of-the-art methods by +1.2\% at Rank-1 on the MLR-CUHK03 dataset.},
  archive   = {C_ICCV},
  author    = {Yukun Huang and Xueyang Fu and Zheng-Jun Zha},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00028},
  pages     = {215-224},
  title     = {Attack-guided perceptual data generation for real-world re-identification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Heterogeneous relational complement for vehicle
re-identification. <em>ICCV</em>, 205–214. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The crucial problem in vehicle re-identification is to find the same vehicle identity when reviewing this object from cross-view cameras, which sets a higher demand for learning viewpoint-invariant representations. In this paper, we propose to solve this problem from two aspects: constructing robust feature representations and proposing camera-sensitive evaluations. We first propose a novel Heterogeneous Relational Complement Network (HRCN) by incorporating region-specific features and cross-level features as complements for the original high-level output. Considering the distributional differences and semantic misalignment, we propose graph-based relation modules to embed these heterogeneous features into one unified high-dimensional space. On the other hand, considering the deficiencies of cross-camera evaluations in existing measures (i.e., CMC and AP), we then propose a Cross-camera Generalization Measure (CGM) to improve the evaluations by introducing position-sensitivity and cross-camera generalization penalties. We further construct a new benchmark of existing models with our proposed CGM and experimental results reveal that our proposed HRCN model achieves new state-of-the-art in VeRi-776, VehicleID, and VERI-Wild.},
  archive   = {C_ICCV},
  author    = {Jiajian Zhao and Yifan Zhao and Jia Li and Ke Yan and Yonghong Tian},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00027},
  pages     = {205-214},
  title     = {Heterogeneous relational complement for vehicle re-identification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-supervised geometric features discovery via
interpretable attention for vehicle re-identification and beyond.
<em>ICCV</em>, 194–204. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To learn distinguishable patterns, most of recent works in vehicle re-identification (ReID) struggled to redevelop official benchmarks to provide various supervisions, which requires prohibitive human labors. In this paper, we seek to achieve the similar goal but do not involve more human efforts. To this end, we introduce a novel framework, which successfully encodes both geometric local features and global representations to distinguish vehicle instances, optimized only by the supervision from official ID labels. Specifically, given our insight that objects in ReID share similar geometric characteristics, we propose to borrow self-supervised representation learning to facilitate geometric features discovery. To condense these features, we introduce an interpretable attention module, with the core of local maxima aggregation instead of fully automatic learning, whose mechanism is completely understandable and whose response map is physically reasonable. To the best of our knowledge, we are the first that perform self-supervised learning to discover geometric features. We conduct comprehensive experiments on three most popular datasets for vehicle ReID, i.e., VeRi-776, CityFlow-ReID, and VehicleID. We report our state-of-the-art (SOTA) performances and promising visualization results. We also show the excel-lent scalability of our approach on other ReID related tasks, i.e., person ReID and multi-target multi-camera (MTMC) vehicle tracking.},
  archive   = {C_ICCV},
  author    = {Ming Li and Xinming Huang and Ziming Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00026},
  pages     = {194-204},
  title     = {Self-supervised geometric features discovery via interpretable attention for vehicle re-identification and beyond},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Residual attention: A simple but effective method for
multi-label recognition. <em>ICCV</em>, 184–193. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-label image recognition is a challenging computer vision task of practical use. Progresses in this area, how-ever, are often characterized by complicated methods, heavy computations, and lack of intuitive explanations. To effectively capture different spatial regions occupied by objects from different categories, we propose an embarrassingly simple module, named class-specific residual attention (CSRA). CSRA generates class-specific features for every category by proposing a simple spatial attention score, and then combines it with the class-agnostic average pooling feature. CSRA achieves state-of-the-art results on multi-label recognition, and at the same time is much simpler than them. Furthermore, with only 4 lines of code, CSRA also leads to consistent improvement across many diverse pretrained models and datasets without any extra training. CSRA is both easy to implement and light in computations, which also enjoys intuitive explanations and visualizations.},
  archive   = {C_ICCV},
  author    = {Ke Zhu and Jianxin Wu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00025},
  pages     = {184-193},
  title     = {Residual attention: A simple but effective method for multi-label recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dance with self-attention: A new look of conditional random
fields on anomaly detection in videos. <em>ICCV</em>, 173–183. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a novel weakly supervised approach for anomaly detection, which begins with a relation-aware feature extractor to capture the multi-scale convolutional neural network (CNN) features from a video. Afterwards, self-attention is integrated with conditional random fields (CRFs), the core of the network, to make use of the ability of self-attention in capturing the short-range correlations of the features and the ability of CRFs in learning the inter-dependencies of these features. Such a framework can learn not only the spatio-temporal interactions among the actors which are important for detecting complex movements, but also their short- and long-term dependencies across frames. Also, to deal with both local and non-local relationships of the features, a new variant of self-attention is developed by taking into consideration a set of cliques with different temporal localities. Moreover, a contrastive multi-instance learning scheme is considered to broaden the gap between the normal and abnormal instances, resulting in more accurate abnormal discrimination. Simulations reveal that the new method provides superior performance to the state-of-the-art works on the widespread UCF-Crime and Shang-haiTech datasets.},
  archive   = {C_ICCV},
  author    = {Didik Purwanto and Yie-Tarng Chen and Wen-Hsien Fang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00024},
  pages     = {173-183},
  title     = {Dance with self-attention: A new look of conditional random fields on anomaly detection in videos},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Transformer-based dual relation graph for multi-label image
recognition. <em>ICCV</em>, 163–172. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The simultaneous recognition of multiple objects in one image remains a challenging task, spanning multiple events in the recognition field such as various object scales, inconsistent appearances, and confused inter-class relationships. Recent research efforts mainly resort to the statistic label co-occurrences and linguistic word embedding to enhance the unclear semantics. Different from these researches, in this paper, we propose a novel Transformer-based Dual Relation learning framework, constructing complementary relationships by exploring two aspects of correlation, i.e., structural relation graph and semantic relation graph. The structural relation graph aims to capture long-range correlations from object context, by developing a cross-scale transformer-based architecture. The semantic graph dynamically models the semantic meanings of image objects with explicit semantic-aware constraints. In addition, we also incorporate the learnt structural relationship into the semantic graph, constructing a joint relation graph for robust representations. With the collaborative learning of these two effective relation graphs, our approach achieves new state-of-the-art on two popular multi-label recognition benchmarks, i.e. MS-COCO and VOC 2007 dataset.},
  archive   = {C_ICCV},
  author    = {Jiawei Zhao and Ke Yan and Yifan Zhao and Xiaowei Guo and Feiyue Huang and Jia Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00023},
  pages     = {163-172},
  title     = {Transformer-based dual relation graph for multi-label image recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatio-temporal representation factorization for video-based
person re-identification. <em>ICCV</em>, 152–162. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite much recent progress in video-based person re-identification (re-ID), the current state-of-the-art still suffers from common real-world challenges such as appearance similarity among various people, occlusions, and frame misalignment. To alleviate these problems, we propose Spatio-Temporal Representation Factorization (STRF), a flexible new computational unit that can be used in conjunction with most existing 3D convolutional neural network architectures for re-ID. The key innovations of STRF over prior work include explicit pathways for learning discriminative temporal and spatial features, with each component further factorized to capture complementary person-specific appearance and motion information. Specifically, temporal factorization comprises two branches, one each for static features (e.g., the color of clothes) that do not change much over time, and dynamic features (e.g., walking patterns) that change over time. Further, spatial factorization also comprises two branches to learn both global (coarse segments) as well as local (finer segments) appearance features, with the local features particularly useful in cases of occlusion or spatial misalignment. These two factorization operations taken together result in a modular architecture for our parameter-wise light STRF unit that can be plugged in between any two 3D convolutional layers, resulting in an end-to-end learning framework. We empirically show that STRF improves performance of various existing baseline architectures while demonstrating new state-of-the-art results using standard person re-ID evaluation protocols on three benchmarks.},
  archive   = {C_ICCV},
  author    = {Abhishek Aich and Meng Zheng and Srikrishna Karanam and Terrence Chen and Amit K. Roy-Chowdhury and Ziyan Wu},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00022},
  pages     = {152-162},
  title     = {Spatio-temporal representation factorization for video-based person re-identification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Z-score normalization, hubness, and few-shot learning.
<em>ICCV</em>, 142–151. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The goal of few-shot learning (FSL) is to recognize a set of novel classes with only few labeled samples by exploiting a large set of abundant base class samples. Adopting a meta-learning framework, most recent FSL methods meta-learn a deep feature embedding network, and during inference classify novel class samples using nearest neighbor in the learned high-dimensional embedding space. This means that these methods are prone to the hubness problem, that is, a certain class prototype becomes the nearest neighbor of many test instances regardless which classes they belong to. However, this problem is largely ignored in existing FSL studies. In this work, for the first time we show that many FSL methods indeed suffer from the hubness problem. To mitigate its negative effects, we further propose to employ z-score feature normalization, a simple yet effective trans-formation, during meta-training. A theoretical analysis is provided on why it helps. Extensive experiments are then conducted to show that with z-score normalization, the performance of many recent FSL methods can be boosted, resulting in new state-of-the-art on three benchmarks.},
  archive   = {C_ICCV},
  author    = {Nanyi Fei and Yizhao Gao and Zhiwu Lu and Tao Xiang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00021},
  pages     = {142-151},
  title     = {Z-score normalization, hubness, and few-shot learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online refinement of low-level feature based activation map
for weakly supervised object localization. <em>ICCV</em>, 132–141. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a two-stage learning framework for weakly supervised object localization (WSOL). While most previous efforts rely on high-level feature based CAMs (Class Activation Maps), this paper proposes to localize objects using the low-level feature based activation maps. In the first stage, an activation map generator produces activation maps based on the low-level feature maps in the classifier, such that rich contextual object information is included in an online manner. In the second stage, we employ an evaluator to evaluate the activation maps predicted by the activation map generator. Based on this, we further propose a weighted entropy loss, an attentive erasing, and an area loss to drive the activation map generator to substantially reduce the uncertainty of activations between object and background, and explore less discriminative regions. Based on the low-level object information preserved in the first stage, the second stage model gradually generates a well-separated, complete, and compact activation map of object in the image, which can be easily thresholded for accurate localization. Extensive experiments on CUB-200-2011 and ImageNet-1K datasets show that our framework surpasses previous methods by a large margin, which sets a new state-of-the-art for WSOL. Code will be available soon.},
  archive   = {C_ICCV},
  author    = {Jinheng Xie and Cheng Luo and Xiangping Zhu and Ziqi Jin and Weizeng Lu and Linlin Shen},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00020},
  pages     = {132-141},
  title     = {Online refinement of low-level feature based activation map for weakly supervised object localization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). FREE: Feature refinement for generalized zero-shot
learning. <em>ICCV</em>, 122–131. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generalized zero-shot learning (GZSL) has achieved significant progress, with many efforts dedicated to over-coming the problems of visual-semantic domain gap and seen-unseen bias. However, most existing methods directly use feature extraction models trained on ImageNet alone, ignoring the cross-dataset bias between ImageNet and GZSL benchmarks. Such a bias inevitably results in poor-quality visual features for GZSL tasks, which potentially limits the recognition performance on both seen and unseen classes. In this paper, we propose a simple yet effective GZSL method, termed feature refinement for generalized zero-shot learning (FREE), to tackle the above problem. FREE employs a feature refinement (FR) module that in-corporates semantic→visual mapping into a unified generative model to refine the visual features of seen and unseen class samples. Furthermore, we propose a self-adaptive margin center loss (SAMC-loss) that cooperates with a semantic cycle-consistency loss to guide FR to learn class- and semantically-relevant representations, and concatenate the features in FR to extract the fully refined features. Extensive experiments on five benchmark datasets demonstrate the significant performance gain of FREE over its baseline and current state-of-the-art methods. The code is available at https://github.com/shiming-chen/FREE.},
  archive   = {C_ICCV},
  author    = {Shiming Chen and Wenjie Wang and Beihao Xia and Qinmu Peng and Xinge You and Feng Zheng and Ling Shao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00019},
  pages     = {122-131},
  title     = {FREE: Feature refinement for generalized zero-shot learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ACE: Ally complementary experts for solving long-tailed
recognition in one-shot. <em>ICCV</em>, 112–121. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One-stage long-tailed recognition methods improve the overall performance in a &quot;seesaw&quot; manner, i.e., either sacrifice the head’s accuracy for better tail classification or elevate the head’s accuracy even higher but ignore the tail. Existing algorithms bypass such trade-off by a multi-stage training process: pre-training on imbalanced set and fine-tuning on balanced set. Though achieving promising performance, not only are they sensitive to the generalizability of the pre-trained model, but also not easily integrated into other computer vision tasks like detection and segmentation, where pre-training of classifiers solely is not applicable. In this paper, we propose a one-stage long-tailed recognition scheme, ally complementary experts (ACE), where the expert is the most knowledgeable specialist in a sub-set that dominates its training, and is complementary to other experts in the less-seen categories without being disturbed by what it has never seen. We design a distribution-adaptive optimizer to adjust the learning pace of each expert to avoid over-fitting. Without special bells and whistles, the vanilla ACE outperforms the current one-stage SOTA method by 3 ~ 10\% on CIFAR10-LT, CIFAR100-LT, ImageNet-LT and iNaturalist datasets. It is also shown to be the first one to break the &quot;seesaw&quot; trade-off by improving the accuracy of the majority and minority categories simultaneously in only one stage. Code and trained models are at https://github.com/jrcai/ACE.},
  archive   = {C_ICCV},
  author    = {Jiarui Cai and Yizhou Wang and Jenq-Neng Hwang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00018},
  pages     = {112-121},
  title     = {ACE: Ally complementary experts for solving long-tailed recognition in one-shot},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Conditional variational capsule network for open set
recognition. <em>ICCV</em>, 103–111. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In open set recognition, a classifier has to detect unknown classes that are not known at training time. In order to recognize new categories, the classifier has to project the input samples of known classes in very compact and separated regions of the features space for discriminating samples of unknown classes. Recently proposed Capsule Networks have shown to outperform alternatives in many fields, particularly in image recognition, however they have not been fully applied yet to open-set recognition. In capsule networks, scalar neurons are replaced by capsule vectors or matrices, whose entries represent different proper-ties of objects. In our proposal, during training, capsules features of the same known class are encouraged to match a pre-defined gaussian, one for each class. To this end, we use the variational autoencoder framework, with a set of gaussian priors as the approximation for the posterior distribution. In this way, we are able to control the compactness of the features of the same class around the center of the gaussians, thus controlling the ability of the classifier in detecting samples from unknown classes. We conducted several experiments and ablation of our model, obtaining state of the art results on different datasets in the open set recognition and unknown detection tasks.},
  archive   = {C_ICCV},
  author    = {Yunrui Guo and Guglielmo Camporese and Wenjing Yang and Alessandro Sperduti and Lamberto Ballan},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00017},
  pages     = {103-111},
  title     = {Conditional variational capsule network for open set recognition},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Procrustean training for imbalanced deep learning.
<em>ICCV</em>, 92–102. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Neural networks trained with class-imbalanced data are known to perform poorly on minor classes of scarce training data. Several recent works attribute this to over-fitting to minor classes. In this paper, we provide a novel explanation of this issue. We found that a neural network tends to first under-fit the minor classes by classifying most of their data into the major classes in early training epochs. To correct these wrong predictions, the neural network then must focus on pushing features of minor class data across the decision boundaries between major and minor classes, leading to much larger gradients for features of minor classes. We argue that such an under-fitting phase over-emphasizes the competition between major and minor classes, hinders the neural network from learning the discriminative knowledge that can be generalized to test data, and eventually results in over-fitting. To address this issue, we propose a novel learning strategy to equalize the training progress across classes. We mix features of the major class data with those of other data in a mini-batch, intentionally weakening their features to prevent a neural network from fitting them first. We show that this strategy can largely balance the training accuracy and feature gradients across classes, effectively mitigating the under-fitting then over-fitting problem for minor class data. On several benchmark datasets, our approach achieves the state-of-the-art accuracy, especially for the challenging step-imbalanced cases.},
  archive   = {C_ICCV},
  author    = {Han-Jia Ye and De-Chuan Zhan and Wei-Lun Chao},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00016},
  pages     = {92-102},
  title     = {Procrustean training for imbalanced deep learning},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Asymmetric loss for multi-label classification.
<em>ICCV</em>, 82–91. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In a typical multi-label setting, a picture contains on average few positive labels, and many negative ones. This positive-negative imbalance dominates the optimization process, and can lead to under-emphasizing gradients from positive labels during training, resulting in poor accuracy. In this paper, we introduce a novel asymmetric loss (&quot;ASL&quot;), which operates differently on positive and negative samples. The loss enables to dynamically down-weights and hard-thresholds easy negative samples, while also discarding possibly mislabeled samples. We demonstrate how ASL can balance the probabilities of different samples, and how this balancing is translated to better mAP scores. With ASL, we reach state-of-the-art results on multiple popular multi-label datasets: MS-COCO, Pascal-VOC, NUS-WIDE and Open Images. We also demonstrate ASL applicability for other tasks, such as single-label classification and object detection. ASL is effective, easy to implement, and does not increase the training time or complexity. Implementation is available at: https://github.com/Alibaba-MIIL/ASL.},
  archive   = {C_ICCV},
  author    = {Tal Ridnik and Emanuel Ben-Baruch and Nadav Zamir and Asaf Noy and Itamar Friedman and Matan Protter and Lihi Zelnik-Manor},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00015},
  pages     = {82-91},
  title     = {Asymmetric loss for multi-label classification},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning with noisy labels via sparse regularization.
<em>ICCV</em>, 72–81. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning with noisy labels is an important and challenging task for training accurate deep neural networks. Some commonly-used loss functions, such as Cross Entropy (CE), suffer from severe overfitting to noisy labels. Robust loss functions that satisfy the symmetric condition were tailored to remedy this problem, which however encounter the underfitting effect. In this paper, we theoretically prove that any loss can be made robust to noisy labels by restricting the network output to the set of permutations over a fixed vector. When the fixed vector is one-hot, we only need to constrain the output to be one-hot, which however produces zero gradients almost everywhere and thus makes gradient-based optimization difficult. In this work, we introduce the sparse regularization strategy to approximate the one-hot constraint, which is composed of network output sharpening operation that enforces the output distribution of a net-work to be sharp and the ℓ p -norm (p ≤ 1) regularization that promotes the network output to be sparse. This simple approach guarantees the robustness of arbitrary loss functions while not hindering the fitting ability. Experimental results demonstrate that our method can significantly improve the performance of commonly-used loss functions in the presence of noisy labels and class imbalance, and out-perform the state-of-the-art methods. The code is available at https://github.com/hitcszx/lnl_sr.},
  archive   = {C_ICCV},
  author    = {Xiong Zhou and Xianming Liu and Chenyang Wang and Deming Zhai and Junjun Jiang and Xiangyang Ji},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00014},
  pages     = {72-81},
  title     = {Learning with noisy labels via sparse regularization},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). NGC: A unified framework for learning with open-world noisy
data. <em>ICCV</em>, 62–71. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The existence of noisy data is prevalent in both the training and testing phases of machine learning systems, which inevitably leads to the degradation of model performance. There have been plenty of works concentrated on learning with indistribution (IND) noisy labels in the last decade, i.e., some training samples are assigned incorrect labels that do not correspond to their true classes. Nonetheless, in real application scenarios, it is necessary to consider the influence of out-of-distribution (OOD) samples, i.e., samples that do not belong to any known classes, which has not been sufficiently explored yet. To remedy this, we study a new problem setup, namely Learning with Open-world Noisy Data (LOND). The goal of LOND is to simultaneously learn a classifier and an OOD detector from datasets with mixed IND and OOD noise. In this paper, we propose a new graph-based framework, namely Noisy Graph Cleaning (NGC), which collects clean samples by leveraging geometric structure of data and model predictive confidence. Without any additional training effort, NGC can detect and reject the OOD samples based on the learned class prototypes directly in testing phase. We conduct experiments on multiple benchmarks with different types of noise and the results demonstrate the superior performance of our method against state of the arts.},
  archive   = {C_ICCV},
  author    = {Zhi-Fan Wu and Tong Wei and Jianwen Jiang and Chaojie Mao and Mingqian Tang and Yu-Feng Li},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00013},
  pages     = {62-71},
  title     = {NGC: A unified framework for learning with open-world noisy data},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CrossNorm and SelfNorm for generalization under distribution
shifts. <em>ICCV</em>, 52–61. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traditional normalization techniques (e.g., Batch Normalization and Instance Normalization) generally and simplistically assume that training and test data follow the same distribution. As distribution shifts are inevitable in real-world applications, well-trained models with previous normalization methods can perform badly in new environments. Can we develop new normalization methods to improve generalization robustness under distribution shifts? In this paper, we answer the question by proposing Cross-Norm and SelfNorm. CrossNorm exchanges channel-wise mean and variance between feature maps to enlarge training distribution, while SelfNorm uses attention to recalibrate the statistics to bridge gaps between training and test distributions. CrossNorm and SelfNorm can complement each other, though exploring different directions in statistics usage. Extensive experiments on different fields (vision and language), tasks (classification and segmentation), settings (supervised and semi-supervised), and distribution shift types (synthetic and natural) show the effectiveness. Code is available at https://github.com/amazon-research/crossnorm-selfnorm},
  archive   = {C_ICCV},
  author    = {Zhiqiang Tang and Yunhe Gao and Yi Zhu and Zhi Zhang and Mu Li and Dimitris Metaxas},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00012},
  pages     = {52-61},
  title     = {CrossNorm and SelfNorm for generalization under distribution shifts},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). DTMNet: A discrete tchebichef moments-based deep neural
network for multi-focus image fusion. <em>ICCV</em>, 43–51. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Compared with traditional methods, the deep learning-based multi-focus image fusion methods can effectively improve the performance of image fusion tasks. However, the existing deep learning-based methods encounter a common issue of a large number of parameters, which leads to the deep learning models with high time complexity and low fusion efficiency. To address this issue, we propose a novel discrete Tchebichef moment-based Deep neural network, termed as DTMNet, for multi-focus image fusion. The proposed DTMNet is an end-to-end deep neural network with only one convolutional layer and three fully connected layers. The convolutional layer is fixed with DTM co-efficients (DTMConv) to extract high/low-frequency information without learning parameters effectively. The three fully connected layers have learnable parameters for feature classification. Therefore, the proposed DTMNet for multi-focus image fusion has a small number of parameters (0.01M paras vs. 4.93M paras of regular CNN) and high computational efficiency (0.32s vs. 79.09s by regular CNN to fuse an image). In addition, a large-scale multi-focus image dataset is synthesized for training and verifying the deep learning model. Experimental results on three public datasets demonstrate that the proposed method is competitive with or even outperforms the state-of-the-art multi-focus image fusion methods in terms of subjective visual perception and objective evaluation metrics.},
  archive   = {C_ICCV},
  author    = {Bin Xiao and Haifeng Wu and Xiuli Bi},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00011},
  pages     = {43-51},
  title     = {DTMNet: A discrete tchebichef moments-based deep neural network for multi-focus image fusion},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Going deeper with image transformers. <em>ICCV</em>, 32–42.
(<a href="https://doi.org/10.1109/ICCV48922.2021.00010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Transformers have been recently adapted for large scale image classification, achieving high scores shaking up the long supremacy of convolutional neural networks. However the optimization of vision transformers has been little studied so far. In this work, we build and optimize deeper transformer networks for image classification. In particular, we investigate the interplay of architecture and optimization of such dedicated transformers. We make two architecture changes that significantly improve the accuracy of deep transformers. This leads us to produce models whose performance does not saturate early with more depth, for in-stance we obtain 86.5\% top-1 accuracy on Imagenet when training with no external data, we thus attain the current sate of the art with less floating-point operations and parameters. Our best model establishes the new state of the art on Imagenet with Reassessed labels and Imagenet-V2 / match frequency, in the setting with no additional training data. We share our code and models 1 .},
  archive   = {C_ICCV},
  author    = {Hugo Touvron and Matthieu Cord and Alexandre Sablayrolles and Gabriel Synnaeve and Hervé Jégou},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00010},
  pages     = {32-42},
  title     = {Going deeper with image transformers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CvT: Introducing convolutions to vision transformers.
<em>ICCV</em>, 22–31. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present in this paper a new architecture, named Convolutional vision Transformer (CvT), that improves Vision Transformer (ViT) in performance and efficiency by introducing convolutions into ViT to yield the best of both de-signs. This is accomplished through two primary modifications: a hierarchy of Transformers containing a new convolutional token embedding, and a convolutional Transformer block leveraging a convolutional projection. These changes introduce desirable properties of convolutional neural networks (CNNs) to the ViT architecture (i.e. shift, scale, and distortion invariance) while maintaining the merits of Transformers (i.e. dynamic attention, global context, and better generalization). We validate CvT by conducting extensive experiments, showing that this approach achieves state-of-the-art performance over other Vision Transformers and ResNets on ImageNet-1k, with fewer parameters and lower FLOPs. In addition, performance gains are maintained when pretrained on larger datasets (e.g. ImageNet-22k) and fine-tuned to downstream tasks. Pretrained on ImageNet-22k, our CvT-W24 obtains a top-1 accuracy of 87.7\% on the ImageNet-1k val set. Finally, our results show that the positional encoding, a crucial component in existing Vision Transformers, can be safely re-moved in our model, simplifying the design for higher resolution vision tasks. Code will be released at https://github.com/microsoft/CvT.},
  archive   = {C_ICCV},
  author    = {Haiping Wu and Bin Xiao and Noel Codella and Mengchen Liu and Xiyang Dai and Lu Yuan and Lei Zhang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00009},
  pages     = {22-31},
  title     = {CvT: Introducing convolutions to vision transformers},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). GLiT: Neural architecture search for global and local image
transformer. <em>ICCV</em>, 12–21. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce the first Neural Architecture Search (NAS) method to find a better transformer architecture for image recognition. Recently, transformers without CNN-based backbones are found to achieve impressive performance for image recognition. However, the transformer is designed for NLP tasks and thus could be sub-optimal when directly used for image recognition. In order to improve the visual representation ability for transformers, we propose a new search space and searching algorithm. Specifically, we introduce a locality module that models the local correlations in images explicitly with fewer computational cost. With the locality module, our search space is defined to let the search algorithm freely trade off between global and local information as well as optimizing the low-level design choice in each module. To tackle the problem caused by huge search space, a hierarchical neural architecture search method is proposed to search the optimal vision transformer from two levels separately with the evolutionary algorithm. Extensive experiments on the ImageNet dataset demonstrate that our method can find more discriminative and efficient trans-former variants than the ResNet family (e.g., ResNet101) and the baseline ViT for image classification. The source codes are available at https://github.com/bychen515/GLiT.},
  archive   = {C_ICCV},
  author    = {Boyu Chen and Peixia Li and Chuming Li and Baopu Li and Lei Bai and Chen Lin and Ming Sun and Junjie Yan and Wanli Ouyang},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00008},
  pages     = {12-21},
  title     = {GLiT: Neural architecture search for global and local image transformer},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MVTN: Multi-view transformation network for 3D shape
recognition. <em>ICCV</em>, 1–11. (<a
href="https://doi.org/10.1109/ICCV48922.2021.00007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-view projection methods have demonstrated their ability to reach state-of-the-art performance on 3D shape recognition. Those methods learn different ways to aggregate information from multiple views. However, the camera view-points for those views tend to be heuristically set and fixed for all shapes. To circumvent the lack of dynamism of current multi-view methods, we propose to learn those view-points. In particular, we introduce the Multi-View Transformation Network (MVTN) that regresses optimal view-points for 3D shape recognition, building upon advances in differentiable rendering. As a result, MVTN can be trained end-to-end along with any multi-view network for 3D shape classification. We integrate MVTN in a novel adaptive multi-view pipeline that can render either 3D meshes or point clouds. MVTN exhibits clear performance gains in the tasks of 3D shape classification and 3D shape retrieval with-out the need for extra training supervision. In these tasks, MVTN achieves state-of-the-art performance on ModelNet40, ShapeNet Core55, and the most recent and realistic ScanObjectNN dataset (up to 6\% improvement). Interestingly, we also show that MVTN can provide network robustness against rotation and occlusion in the 3D domain. The code is available at https://github.com/ajhamdi/MVTN.},
  archive   = {C_ICCV},
  author    = {Abdullah Hamdi and Silvio Giancola and Bernard Ghanem},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision},
  doi       = {10.1109/ICCV48922.2021.00007},
  pages     = {1-11},
  title     = {MVTN: Multi-view transformation network for 3D shape recognition},
  year      = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
