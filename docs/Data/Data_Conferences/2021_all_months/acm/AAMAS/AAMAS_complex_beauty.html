<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>AAMAS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="aamas---304">AAMAS - 304</h2>
<ul>
<li><details>
<summary>
(2021). Mechanism design in facility location games. <em>AAMAS</em>,
1850–1852. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Algorithmic game theory is a research field integrating game theory and algorithm design. The major target is to design good algorithms in strategic environments. In this thesis, I intend to study a well-established problem in algorithmic game theory called facility location games. In the most classic setting, the government plans to build a facility on a street where some strategic agents with private information live. Agents want to be as close as possible to the facility. The objective of the government is to collect the agents&#39; information and use a mechanism to decide where to build the facility so that agents will not gain by reporting false information and certain objective values are approximately optimized.In recent years, many extensions to the original facility location games are proposed and studied by the researchers. This thesis studies three different kinds of facility location games, and designs truthful mechanisms with good performances, which will offer new insight on extending the classic facility location games.},
  archive   = {C_AAMAS},
  author    = {Zhang, Mengqi},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1850–1852},
  title     = {Mechanism design in facility location games},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464262},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Design and analysis of networks under strategic behavior.
<em>AAMAS</em>, 1848–1849. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Networks are enablers, allowing the diffusion of valuable information. But just as a network is a conduit for valuable information, so it is for misinformation. One major challenge in a networked environment is limiting the spread of misinformation. Consider a large online social network, such as Twitter. Unethical users spread anti-social posts, which negatively affect other users and damage community dynamics[6], fraudsters send spam and phishing emails that threaten people&#39;s financial security[7], accounts occupied by malicious parties spread toxic information (e.g., hate speech, fake news), stirring up controversy and manipulating political views among social network users[1], fake reviews posted by bots mislead consumers&#39; decision making[15], etc. An intuitive idea to limit the spread of misinformation is removing malicious nodes from networks, for example, terminate accounts on Twitter that spread spam. Importantly, a principled method to decide which nodes to remove from a network has wide applications; in the case of infectious disease, the inoculation of a group of people is essentially &quot;removing&quot; them from the contagion network[2,5,8,12,16-19]. A critical observation is that the loss associated with a decision whether to remove a node depends both on the node&#39;s likelihood of being malicious and its local network structure. Consequently, the typical approach in which we simply classify nodes as malicious or benign using a threshold on the associated maliciousness probability[10] is inadequate, as it fails to account for network consequences of such decisions. Rather, the problem is fundamentally about choosing which subset of nodes to remove, as decisions about removing individual nodes are no longer independent. We developed a model that provides decisions about which nodes to remove[20]. The model considers both the likelihood of nodes being malicious and their local network structures. Several algorithmic insights are derived from studying the model, including hardness results, as well as approximation algorithms. Our ongoing effort focuses on making the model scalable to large-scale networks.Another challenge in a networked environment arises when taking individuals&#39; strategic behavior into account. When facing strategic individuals, game theory is a powerful tool to model their interaction. Many game-theoretic models have been proposed to model strategic behavior on networks, e.g., graphical games[13], networked public goods game[3,4,11], etc. Among these models, the research on equilibrium outcomes has attracted much attention. In particular, equilibrium outcomes are not always socially preferable. When the equilibrium outcomes are not socially preferable, a principal may be interested in changing the parameters of the game so as to induce equilibrium outcomes that are better aligned with the social interest. Changing the payment structure is one way to promote preferable equilibria, as in traditional mechanism design, or the structure of information available to the players[9], another parameter subject to change is the network structure itself. A prominent challenge in a networked environment is to induce desirable equilibrium outcomes through modifying network structures. We initiated an algorithmic study of network structure modifications in networked public goods games with binary actions, with the goal of inducing equilibrium outcomes with desirable properties[14,21]. Such desirable properties are application dependent, for example, in the case of crime prevention, it would be desirable to encourage as many individuals in a community to invest in safety service as possible. From a wider perspective, our study is categorized into network design. One interpretation of network design is through the lens of optimization, that is, a principal has an objective in mind and she optimizes over the underlying network to achieve the objective. Many interesting questions arise from the angle of optimization, for example, what is the objective, how should we define the feasible region of the modifications, is the design choice robust, etc. These questions consist of our ongoing and future research plan.},
  archive   = {C_AAMAS},
  author    = {Yu, Sixie},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1848–1849},
  title     = {Design and analysis of networks under strategic behavior},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464261},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Inducing rules about distributed robotic systems for fault
detection and diagnosis. <em>AAMAS</em>, 1845–1847. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an extended abstract for the PhD topic Inducing Rules about Distributed Robotic Systems for Fault Detection and Diagnosis. The research focuses on developing novel methods for fault detection and diagnosis using explainable machine learning. The main field of application is distributed robotic systems. With current developments in distributed robot technology, the problem of detecting and diagnosing faults becomes more complex.},
  archive   = {C_AAMAS},
  author    = {Youssef, Youssef Mahmoud},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1845–1847},
  title     = {Inducing rules about distributed robotic systems for fault detection and diagnosis},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464260},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computing using samples: Theoretical guarantees with the
direct learning approach. <em>AAMAS</em>, 1842–1844. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Machine learning algorithms in the field of economics and game theory usually involve computing an intermediate valuation function from data samples and using this approximate function to compute desired solution concepts. This approach has several problems ranging from a high sample complexity to a lack of provable guarantees about the final solution. In order to avoid these problems, we explore a new method to learn solution concepts from data: instead of learning an intermediate valuation function, we learn the solution concept directly from the samples. This approach provides an alternative way to approximately learn solution concepts using fewer samples. In addition to this, from our study of using this approach to learn market equilibria, we find that, in a lot of settings, it is easier to prove efficiency and fairness guarantees about the learned solutions.},
  archive   = {C_AAMAS},
  author    = {Viswanathan, Vignesh},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1842–1844},
  title     = {Computing using samples: Theoretical guarantees with the direct learning approach},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464259},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A concurrent language for negotiation and debate with
argumentation. <em>AAMAS</em>, 1840–1841. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper summarises the main results obtained within my Ph.D. thesis where I studied argumentation from the point of view of dynamics, focusing, in particular, in the ability to manage the evolution of information. I considered different aspects of argumentation and devised theoretical and practical tools useful for developing argumentation-based applications in the context of multi-agent systems, where complex interactions between agents need to be modelled and handled.},
  archive   = {C_AAMAS},
  author    = {Taticchi, Carlo},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1840–1841},
  title     = {A concurrent language for negotiation and debate with argumentation},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464258},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improving sample-based reinforcement learning through
complex non-parametric distributions. <em>AAMAS</em>, 1837–1839. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sampling-based approaches in Reinforcement Learning (RL) typically involve learning or maintaining distributions. While many elegant algorithms were proposed in literature, most methods involve prior assumptions of the underlying distributions (eg. being Natural Exponential Family), or the number of modality for either simplicity or tractability reasons. A method to effectively apply complex or non-parametric distributions, for example, distributions approximated using neural network, is still lacking. One example is the limitation of using of reparameterized Gaussian policy, rather than any arbitrary non-parametric policy in Soft Actor-Critic (SAC) amenable to the necessary entropy estimation. The thesis would be focusing on proposing and evaluating methods to enable better approximation of complex distributions, and methods to estimate measurements of non-parametric distributions. The motivation is to allow better connections and applications of many deep learning and information theory techniques to the sampling-based approaches in RL, by alleviating limitations and difficulties of complex non-parametric distributions.},
  archive   = {C_AAMAS},
  author    = {Tang, Shi Yuan},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1837–1839},
  title     = {Improving sample-based reinforcement learning through complex non-parametric distributions},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464257},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptable and verifiable BDI reasoning. <em>AAMAS</em>,
1835–1836. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Long-term autonomy requires autonomous systems to adapt as their capabilities no longer perform as expected. To achieve this, a system must first be capable of detecting such changes. In this extended abstract, a specification for BDI autonomous agents capable of adapting to changes in a dynamic environment is discussed, and the required research is outlined. Specifically, an agent-maintained self-model is described alongside the accompanying theories of durative actions and learning new action descriptions in BDI systems.},
  archive   = {C_AAMAS},
  author    = {Stringer, Peter},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1835–1836},
  title     = {Adaptable and verifiable BDI reasoning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464256},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploiting hidden convexities for real-time and reliable
optimization algorithms for challenging motion planning and control
applications. <em>AAMAS</em>, 1832–1834. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion Planning and Control algorithms are often formulated as optimization problems as desired robot behaviors can be intuitively encoded in the form of cost and constraint functions. A fundamental challenge in robotics is to make optimization based motion planning reliable and real-time. This thesis aims to achieve this for motion planning and control problems encountered in a wide class of applications ranging from autonomous driving and object transportation to manipulation. In particular, we aim to develop optimization algorithms that identifies and leverages the underlying useful albeit limited convex structures in the problem through techniques like Alternating Direction Method of Multipliers and Bergman Iteration. During the first 1.5 years of the thesis, we have already obtained encouraging results that validated our core hypothesis and led to optimizer that surpasses the state of the art in many challenging problems.},
  archive   = {C_AAMAS},
  author    = {Rastgar, Fatemeh},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1832–1834},
  title     = {Exploiting hidden convexities for real-time and reliable optimization algorithms for challenging motion planning and control applications},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464255},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simulating realistic pedestrian behaviors in the context of
autonomous vehicles in shared spaces. <em>AAMAS</em>, 1829–1831. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this PhD thesis, we investigate how pedestrians will behave in a shared space with an autonomous vehicle, by modeling and simulating realistic pedestrians&#39; behavior. Our approach integrates agent-based social simulation with standard robotic tools. We proposed an agent-based model through extensions of the Social Force Model. We implemented this model as an open source tool to simulate the interactions between pedestrians and an autonomous vehicle in shared spaces. Simulation results, when compared with available data, show that combining an agent-based model and robotics is promising for handling real-world scenarios. The proposed model and simulator are used: 1) to simulate various shared space scenarios with heterogeneous and realistic pedestrians&#39; behaviors; 2) to provide a simulated environment to test autonomous vehicle navigation strategies; 3) to reproduce real-world scenes and predict pedestrians trajectories around the autonomous car in real time.},
  archive   = {C_AAMAS},
  author    = {Pr\&#39;{e}dhumeau, Manon},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1829–1831},
  title     = {Simulating realistic pedestrian behaviors in the context of autonomous vehicles in shared spaces},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464254},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modelling trust in human-AI interaction. <em>AAMAS</em>,
1826–1828. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trust is an important element of any interaction, but especially when we are interacting with a piece of technology which does not think like we do. Therefore, AI systems need to understand how humans trust them, and what to do to promote appropriate trust. The aim of this research is to study trust through both a formal and social lens. We will be working on formal models of trust, but with a focus on the social nature of trust in order to represent how humans trust AI. We will then employ methods from human-computer interaction research to study if these models work in practice, and what would eventually be necessary for systems to elicit appropriate levels of trust from their users. The context of this research will be AI agents which interact with their users to offer personal support.},
  archive   = {C_AAMAS},
  author    = {Mehrotra, Siddharth},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1826–1828},
  title     = {Modelling trust in human-AI interaction},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464253},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Non-manipulability in set-valued and probabilistic social
choice theory. <em>AAMAS</em>, 1823–1825. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A fundamental requirement in social choice theory is non-manipulability, i.e., voters should not be able to benefit by voting dishonestly. Unfortunately, a seminal result by Gibbard [1973] and Satterthwaite [1975] states that only extremely unattractive voting rules can be strategyproof if it is required to choose a single winner deterministically. Two common approaches for circumventing this impossibility are to allow for sets of winners and to allow for randomization. It is for both approaches possible to define various strategyproofness notions based on different assumptions on how voters compare sets of alternatives or lotteries on alternatives, and consequently, both positive and negative results can be obtained. The goal of this PhD project is to analyze for both models the boundary between possibility and impossibility results for various strategyproofness notions.},
  archive   = {C_AAMAS},
  author    = {Lederer, Patrick},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1823–1825},
  title     = {Non-manipulability in set-valued and probabilistic social choice theory},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464252},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Intention-aware human-robot collaborative design.
<em>AAMAS</em>, 1820–1822. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots pose unique potential partners for human designers when thought of as physical and social embodiments of computational agents. In this work, we propose the efficacy of robotic collaborative design agents and observe challenges and potential directions in an exploratory study of human collaboration with a robot on a design task. Based on these observations, we outline future studies of human-robot design collaboration and focus on the particular challenge of inferring a human partner&#39;s design intentions in order to better navigate collaborative design with a robot.},
  archive   = {C_AAMAS},
  author    = {Law, Matthew V.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1820–1822},
  title     = {Intention-aware human-robot collaborative design},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464251},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Balancing fairness, efficiency and strategy-proofness in
voting and facility location problems. <em>AAMAS</em>, 1818–1819. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the field of computational social choice, researchers seek mechanisms that fulfil the notion of strategy-proofness, so that it is optimal for agents to simply report their truthful preferences. This notion can be very difficult to achieve, and mechanisms that satisfy this strict constraint often sacrifice ideal properties such as fairness and efficiency. For example, a voting rule satisfying strategy-proofness must be dictatorial, which may be considered unfair and wasteful to the voters, as it only takes into account one voter&#39;s preference. Strategy-proof facility location mechanisms are also known to be sub-optimal in terms of fairness and efficiency. Focussing on these two areas, we question if strategy-proofness is too strict of a constraint, and whether mechanisms satisfying weaker variations of the property are more fair and efficient.},
  archive   = {C_AAMAS},
  author    = {Lam, Alexander},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1818–1819},
  title     = {Balancing fairness, efficiency and strategy-proofness in voting and facility location problems},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464250},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Leveraging social interactions in human-agent
decision-making. <em>AAMAS</em>, 1816–1817. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Through social interactions, humans and machines can express their intents, acknowledge each other&#39;s, and coordinate with one another to arrive at a joint decision. These interactions can also help achieve goals beyond just task performance. They can help build trust between interactants, which is crucial for effective collaborations. My work aims to i) develop frameworks for social interaction in human-agent joint decision-making and ii) implement artificial agents that improve joint decisions while considering the social and interpersonal implications of their actions. In this extended abstract, I describe past and current work and propose future directions for my dissertation.},
  archive   = {C_AAMAS},
  author    = {Jeong, JiHyun},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1816–1817},
  title     = {Leveraging social interactions in human-agent decision-making},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464249},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Software engineering methods for responsible artificial
intelligence. <em>AAMAS</em>, 1814–1815. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In order to ensure responsible Artificial intelligence (AI) applications engineering, we need to make sure that the development of AI systems is mindful of the consequences for individuals and societies. By anticipating the consequences of the design choices, reflecting upon the problem being solved by engaging all stakeholders and taking appropriate actions to ensure openness and the system&#39;s social, legal, and ethical acceptability. This research aims to develop an engineering process model by which ethical considerations can be addressed throughout the AI systems&#39; software development life-cycle. The design methodological framework engineered in this PhD research will support aligning system goals with key ethical values by providing explicit values analysis and interpretation mechanisms, formal representation of ethical values, mechanisms for stakeholders participation in handling ethical deliberation, and providing support for governance and compliance mechanisms.},
  archive   = {C_AAMAS},
  author    = {Islam, Zahoor Ul},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1814–1815},
  title     = {Software engineering methods for responsible artificial intelligence},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464248},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Impact of recommender systems on the dynamics of users’
choices. <em>AAMAS</em>, 1811–1813. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The major focus of recommender systems (RSs) research has so far been on improving the precision and quality of the recommendations. However, it is also important to understand whether the recommended items are actually chosen and how they influence users&#39; choice making. Few studies have attempted to analyse the impact of RSs on users&#39; choice making. In this PhD research, we aim at better understanding the impact of RSs on the evolution of the choices made by a collection of users. We propose simulation procedures where users are simulated to make choices over a period of time when they are exposed to alternative RSs. We measure several properties of the users&#39; choices distribution, which capture the RS effect. Our goal is to understand the evolution of the choices of a collection of users as time goes; next choices are influenced by previous choices used by the RS to generate recommendations. Additionally, we propose online experiments to study the effect of RSs on real users&#39; choices. We propose to design web-based platforms where alternative RSs recommend items to the users and study RSs impact by analysing the evolution of the choices.},
  archive   = {C_AAMAS},
  author    = {Hazrati, Naieme},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1811–1813},
  title     = {Impact of recommender systems on the dynamics of users&#39; choices},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464247},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reward-sharing relational networks in multi-agent
reinforcement learning as a framework for emergent behavior.
<em>AAMAS</em>, 1808–1810. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most prior works on MARL seek to implement intra-agent complex interactions by explicitly communicating agent actions. However, there have only been a few efforts that examine emergence as arising from complex &#39;social&#39; interactions or relations based on individual objectives and reward functions. This study is about integrating a user-defined relational network into the MARL setup and evaluating the effects of agent-agent relations on the generation of emergent behaviors. Specifically, we propose a framework that uses the notion of Reward-Sharing Relational Networks (RSRN) to determine the relationship between agents where edge weights determine how much one agent is invested in the success of (or &#39;cares about&#39;) another. The preliminary results indicate that reward-sharing relational networks can effectively influence the learned behaviors towards the imposed relational network.},
  archive   = {C_AAMAS},
  author    = {Haeri, Hossein},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1808–1810},
  title     = {Reward-sharing relational networks in multi-agent reinforcement learning as a framework for emergent behavior},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464246},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning realistic and safe pedestrian behavior by
imitation. <em>AAMAS</em>, 1805–1807. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Navigating amongst pedestrians is a very complex task for an autonomous agent. Not only must the agent understand traffic rules and navigate safely, but it must also act in a way that obeys social norms and does not interfere with other pedestrians. Here, we focus on obtaining a model that portrays pedestrian behavior from real-life demonstrations using imitation learning. We create a reinforcement learning environment that allows an agent to learn to navigate using the obtained behavior model. The work is still in progress, but we illustrate how we generate demonstrations of pedestrian behavior from video captured by smart glasses and we incorporate them into a reinforcement learning environment.},
  archive   = {C_AAMAS},
  author    = {Cruz, Jos\&#39;{e} Aleixo},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1805–1807},
  title     = {Learning realistic and safe pedestrian behavior by imitation},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464245},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-agent ranked delegations in voting. <em>AAMAS</em>,
1802–1804. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We generalise liquid democracy, a voting model where an agent can either vote on an issue or delegate to another agent who votes on their behalf. As delegations are transitive, delegates can choose to vote directly or to delegate their votes further. Transitivity can cause delegation cycles, making it unclear how to determine these votes. Our generalisation allows for ranked delegations in order to break delegation cycles. Agents can also give more expressive delegations than in liquid democracy, being able to state how their vote should be determined from the votes of others. For example, an agent may want their vote to correspond to the majority opinion of a group of trusted experts. We focus on how to gain a collective decision in this setting; we propose six unravelling procedures that find a standard voting profile, from the complex ballots, that can be aggregated. We study the properties of these unravelling procedures, including the complexity of finding an outcome, as well as axiomatic properties of the procedures. Following this, we lay out some future research directions.},
  archive   = {C_AAMAS},
  author    = {Colley, Rachael},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1802–1804},
  title     = {Multi-agent ranked delegations in voting},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464244},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Computing desirable outcomes in specific multi-agent
scenarios. <em>AAMAS</em>, 1799–1801. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Coalition formation and Schelling segregation are important scenarios in algorithmic game theory. While the former considers the strategic behavior of agents gathering in coalitions, the latter is a setting in which agents of two groups seek to surround themselves with like-minded agents. In each case, the quality of outcomes can be measured in form of axioms of optimality and stability. The thesis investigates how to compute such desirable outcomes efficiently and how to deal with computational intractability by means of approximation algorithms, randomization, or domain restrictions.},
  archive   = {C_AAMAS},
  author    = {Bullinger, Martin},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1799–1801},
  title     = {Computing desirable outcomes in specific multi-agent scenarios},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464243},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Understanding the role of inequality in creating and
sustaining the alcohol harm paradox using agent-based modelling.
<em>AAMAS</em>, 1797–1798. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This PhD applies a different perspective and novel methods to attempt to understand a complex health phenomenon, the Alcohol Harm Paradox (AHP). This is the consistent finding that poorer people experience greater risk of alcohol harm despite drinking the same or less as richer people. Thus far a variable-centric approach focused on testing the relationships between health behaviours and harm has dominated research on the AHP. This thesis shifts to a mechanism-based approach rooted in the theories of health inequality to understand how the determinants of health inequality; from the actions and interactions of individuals to the impact of society, contribute to the AHP. Simulation methods, specifically agent-based models, are implemented to test the explanatory value of health inequality theory in creating and sustaining the AHP.},
  archive   = {C_AAMAS},
  author    = {Boyd, Jennifer},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1797–1798},
  title     = {Understanding the role of inequality in creating and sustaining the alcohol harm paradox using agent-based modelling},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464242},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploring the relationship between social choice and machine
learning. <em>AAMAS</em>, 1794–1796. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {My thesis will study the intersection of social choice and machine learning, with a focus on recent or under-explored social choice paradigms, such as liquid democracy, and how social choice and ML can benefit each other. My initial results show the idea of using ML and social choice to understand the other holds promise. An early project of mine uses deep learning to enhance social choice by creating a neural network that acts as a voting rule, able to be trained to select a winner satisfying customizable sets of axioms. More recently, I have explored the idea of using liquid democracy as a framework for ensembles for classification problems. I am particularly interested in improving the real-world applicability of existing social choice methods and understanding how they can more beneficially impact the world. Going forward, my primary tools in these goals are simulation and provable axiomatic or performance guarantees.},
  archive   = {C_AAMAS},
  author    = {Armstrong, Ben},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1794–1796},
  title     = {Exploring the relationship between social choice and machine learning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464241},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Graph-based self-adaptive conversational agent.
<em>AAMAS</em>, 1791–1793. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Conversational agents have been widely adopted in dialogue systems for various business purposes. Many existing conversational agents are rule-based and require significant human intervention to adapt the knowledge and conversational flow. In this paper, we propose a graph-based adaptive conversational agent model which is capable of learning knowledge from human beings and adapting the knowledge-base according to human-agent interactions. Studies to evaluate the proposed model are conducted and presented, which compare the responses from the proposed adaptive agent model and a conventional agent.},
  archive   = {C_AAMAS},
  author    = {Zhang, Lan and Li, Weihua and Bai, Quan and Lai, Edmund},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1791–1793},
  title     = {Graph-based self-adaptive conversational agent},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464239},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ATPT: Automate typhoon contingency plan generation from
text. <em>AAMAS</em>, 1788–1790. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Artificial intelligence (AI) planning models play an important role in decision support systems for disaster management e.g. typhoon contingency plan development. However, constructing an AI planning model always requires significant amount of manual effort, which becomes a bottleneck to emergency response in a time-critical situation. In this demonstration, we present a framework of automating a domain model of planning domain definition language from natural language input through deep learning techniques. We implement this framework in a typhoon response system and demonstrate automatic generation of typhoon contingency plan from official typhoon plan documents.},
  archive   = {C_AAMAS},
  author    = {Zeng, Yifeng and Yao, Zhangrui and Pan, Yinghui and Chen, Wanqing and Zhou, Junxin and Chen, Junhan and Ma, Biyang and Ming, Zhong},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1788–1790},
  title     = {ATPT: Automate typhoon contingency plan generation from text},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464238},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TORS: A train unit shunting and servicing simulator.
<em>AAMAS</em>, 1785–1787. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When trains are finished with their transportation tasks during the day, they are moved to a shunting yard where they are routed, parked, cleaned, subject to regular maintenance checks and repaired during the night. The resulting Train Unit Shunting and Servicing problem motivates advanced research in planning and scheduling in general since it integrates several known individually hard problems while incorporating many real-life details. We developed an event-based simulator called TORS (Dutch acronym for Train Shunting and Servicing Simulator), that provides the user with a state and all feasible actions. After an action is picked, TORS calculates the result and the process repeats. This simulator facilitates research into a realistic application of multi-agent path finding.},
  archive   = {C_AAMAS},
  author    = {van der Linden, Jacobus G.M. and Mulderij, Jesse and Huisman, Bob and den Ouden, Joris W. and van den Akker, Marjan and Hoogeveen, Han and de Weerdt, Mathijs M.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1785–1787},
  title     = {TORS: A train unit shunting and servicing simulator},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464237},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Symbolic reinforcement learning for safe RAN control.
<em>AAMAS</em>, 1782–1784. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we demonstrate a Symbolic Reinforcement Learning (SRL) architecture for safe control in Radio Access Network (RAN) applications. In our automated tool, a user can select a high-level safety specifications expressed in Linear Temporal Logic (LTL) to shield an RL agent running in a given cellular network with aim of optimizing network performance, as measured through certain Key Performance Indicators (KPIs). In the proposed architecture, network safety shielding is ensured through model-checking techniques over combined discrete system models (automata) that are abstracted through reinforcement learning. We demonstrate the user interface (UI) helping the user set intent specifications to the architecture and inspect the difference in allowed and blocked actions.},
  archive   = {C_AAMAS},
  author    = {Nikou, Alexandros and Mujumdar, Anusha and Orli\&#39;{c}, Marin and Vulgarakis Feljan, Aneta},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1782–1784},
  title     = {Symbolic reinforcement learning for safe RAN control},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464236},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A framework for integrating gesture generation models into
interactive conversational agents. <em>AAMAS</em>, 1779–1781. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Embodied conversational agents (ECAs) benefit from non-verbal behavior for natural and efficient interaction with users. Gesticulation -- hand and arm movements accompanying speech -- is an essential part of non-verbal behavior. Gesture generation models have been developed for several decades: starting with rule-based and ending with mainly data-driven methods. To date, recent end-to-end gesture generation methods have not been evaluated in a real-time interaction with users. We present a proof-of-concept framework, which is intended to facilitate evaluation of modern gesture generation models in interaction.We demonstrate an extensible open-source framework that contains three components: 1) a 3D interactive agent; 2) a chatbot backend; 3) a gesticulating system. Each component can be replaced, making the proposed framework applicable for investigating the effect of different gesturing models in real-time interactions with different communication modalities, chatbot backends, or different agent appearances. The code and video are available at the project page: https://nagyrajmund.github.io/project/gesturebot.},
  archive   = {C_AAMAS},
  author    = {Nagy, Rajmund and Kucherenko, Taras and Moell, Birger and Pereira, Andr\&#39;{e} and Kjellstr\&quot;{o}m, Hedvig and Bernardet, Ulysses},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1779–1781},
  title     = {A framework for integrating gesture generation models into interactive conversational agents},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464235},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scalable multiple robot task planning with plan merging and
conflict resolution. <em>AAMAS</em>, 1776–1778. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Agents can individually devise plans and coordinate to achieve common goals. Methods exist to factor planning problems into separate tasks and distribute the plan synthesis process, while reducing the overall planning complexity. Merging distributedly generated plans becomes computationally costly when task plans are tightly coupled, and conflicts arise due to dependencies between plan actions. New plan merging algorithms allow factoring and solving large problems with a growing number of agents and tasks, but are yet to be demonstrated in physical real-world systems. This Demo presents an architecture that deploys plan merging algorithms in a physical multi-robot setting and emulates a First Response Domain.},
  archive   = {C_AAMAS},
  author    = {Marcon dos Santos, Gilberto and Adams, Julie A.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1776–1778},
  title     = {Scalable multiple robot task planning with plan merging and conflict resolution},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464234},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A collaborative platform for identifying context-specific
values. <em>AAMAS</em>, 1773–1775. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Value alignment is a crucial aspect of ethical multiagent systems. An important step toward value alignment is identifying values specific to an application context. However, identifying context-specific values is complex and cognitively demanding. To support this process, we develop a methodology and a collaborative web platform that employs AI techniques. We describe this platform, highlighting its intuitive design and implementation.},
  archive   = {C_AAMAS},
  author    = {Liscio, Enrico and van der Meer, Michiel and Jonker, Catholijn M. and Murukannaiah, Pradeep K.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1773–1775},
  title     = {A collaborative platform for identifying context-specific values},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464233},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). STV+reductions: Towards practical verification of strategic
ability using model reductions. <em>AAMAS</em>, 1770–1772. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a substantially expanded version of our tool STV for strategy synthesis and verification of strategic abilities. The new version adds user-definable models and support for model reduction through partial order reduction and checking for bisimulation.},
  archive   = {C_AAMAS},
  author    = {Kurpiewski, Damian and Pazderski, Witold and Jamroga, Wojciech and Kim, Yan},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1770–1772},
  title     = {STV+Reductions: Towards practical verification of strategic ability using model reductions},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464232},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Autonomous agents on the edge of things. <em>AAMAS</em>,
1767–1769. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper describes a demonstration setup that integrates cognitive agents with the latest W3C standardization efforts for the Web of Things (WoT). The conceptual foundations of the implemented system are the integration of cognitive agent abstractions with W3C Web Things, which are generic abstractions of devices and virtual services that provide agents with various interaction affordances (e.g., actions, events). Together with the W3C WoT Scripting API, which is an ECMAScript-compatible API for W3C WoT environments, these standards allow JavaScript-based agents to be deployed and to operate in heterogeneous WoT environments. The agents can then be effectively distributed across the physical-virtual space in a write once, run anywhere manner: we deploy agents across a heterogeneous information system landscape that includes Web servers, browser-based front-ends, and constrained devices (microcontrollers). The deployment only requires minor platform-specific adjustments to consider resource and performance limitations on constrained devices. As a running example, we demonstrate a semi-autonomous assembly scenario with human-in-the-loop support.},
  archive   = {C_AAMAS},
  author    = {Kampik, Timotheus and Gomez, Andres and Ciortea, Andrei and Mayer, Simon},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1767–1769},
  title     = {Autonomous agents on the edge of things},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464231},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Strategy RV: A tool to approximate ATL model checking under
imperfect information and perfect recall. <em>AAMAS</em>, 1764–1766. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present Strategy RV, a tool that allows to approximate the verification of Alternating-time Temporal Logic under imperfect information and perfect recall, which is known to be undecidable, by using Runtime Verification. The tool uses an interface to enter the game model and the specifications and to provide results. We test Strategy RV in a variant of the Curiosity rover scenario and provide some experimental results.},
  archive   = {C_AAMAS},
  author    = {Ferrando, Angelo and Malvone, Vadim},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1764–1766},
  title     = {Strategy RV: A tool to approximate ATL model checking under imperfect information and perfect recall},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464230},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Argflow: A toolkit for deep argumentative explanations for
neural networks. <em>AAMAS</em>, 1761–1763. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, machine learning (ML) models have been successfully applied in a variety of real-world applications. However, they are often complex and incomprehensible to human users. This can decrease trust in their outputs and render their usage in critical settings ethically problematic. As a result, several methods for explaining such ML models have been proposed recently, in particular for black-box models such as deep neural networks (NNs). Nevertheless, these methods predominantly explain model outputs in terms of inputs, disregarding the inner workings of the ML model computing those outputs. We present Argflow, a toolkit enabling the generation of a variety of &#39;deep&#39; argumentative explanations (DAXs) for outputs of NNs on classification tasks.},
  archive   = {C_AAMAS},
  author    = {Dejl, Adam and He, Peter and Mangal, Pranav and Mohsin, Hasan and Surdu, Bogdan and Voinea, Eduard and Albini, Emanuele and Lertvittayakumjorn, Piyawat and Rago, Antonio and Toni, Francesca},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1761–1763},
  title     = {Argflow: A toolkit for deep argumentative explanations for neural networks},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464229},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An online human-agent interaction system: A brain-controlled
agent playing games in unity. <em>AAMAS</em>, 1758–1760. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human-agent interactions present people guide an object or agent to act as human intentions. This demonstration work develops an online human-agent interaction system, particularly targeting the brain-computer interface (BCI), which uses real-time brain cortex signals: electroencephalogram (EEG) to control the agent in Unity3D game platform. The developed system also provides the online visualisation of EEG signals, including pre-processed temporal data and power spectral in three frequency bands (theta, alpha, and beta). To build this systematic work, we firstly collect wireless EEG signals via the Bluetooth transmission from a commercially available 14-channel brainware headset (Emotiv). EEG signals are then pre-processed and fed into a trained deep learning model to predict the human intentions, which will be sent to Unity3D platform to control an agent&#39;s movements in game playing, such as a karting game scenario. The online testing results show the feasibility of our systematic work that will benefit for human-agent interaction community. The demonstration video can be viewed at the following link: https://youtu.be/9AWKHeatc6I},
  archive   = {C_AAMAS},
  author    = {Cao, Zehong and Yun, Jie},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1758–1760},
  title     = {An online human-agent interaction system: A brain-controlled agent playing games in unity},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464228},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Ship-GAN: Generative modeling based maritime traffic
simulator. <em>AAMAS</em>, 1755–1757. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modeling vessel movement in a maritime environment is an extremely challenging task given the complex nature of vessel behavior. Several existing multiagent maritime decision making frameworks require access to an accurate traffic simulator. We develop a system using electronic navigation charts to generate realistic and high fidelity vessel traffic data using Generative Adversarial Networks (GANs). Our proposed Ship-GAN uses a conditional Wasserstein GAN to model a vessel&#39;s behavior. The generator can simulate the travel time of vessels across different maritime zones conditioned on vessels&#39; speeds and traffic intensity. Furthermore, it can be used as an accurate simulator for prior decision making approaches for maritime traffic coordination, which used less accurate model than our approach. Experiments performed on the historical data from heavily trafficked Singapore strait show that our Ship-GAN system generates data whose statistical distribution is close to the real data distribution, and better fit than prior methods.},
  archive   = {C_AAMAS},
  author    = {Basrur, Chaithanya and Singh, Arambam James and Sinha, Arunesh and Kumar, Akshat},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1755–1757},
  title     = {Ship-GAN: Generative modeling based maritime traffic simulator},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464227},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributing responsibilities for exception handling in
JaCaMo. <em>AAMAS</em>, 1752–1754. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an extension of the organizational model and infrastructure adopted in JaCaMo, that explicitly encompasses the notion of exception. We propose an exception handling mechanism for organization management in multi-agent systems. This mechanism relies on abstractions that are seamlessly integrated with organizational concepts, such as responsibilities, goals and norms.},
  archive   = {C_AAMAS},
  author    = {Baldoni, Matteo and Baroglio, Cristina and Boissier, Olivier and Micalizio, Roberto and Tedeschi, Stefano},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1752–1754},
  title     = {Distributing responsibilities for exception handling in JaCaMo},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464226},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ADT2AMAS: Managing agents in attack-defence scenarios.
<em>AAMAS</em>, 1749–1751. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Expressing attack-defence trees (ADTrees) in a multi-agent setting allows for studying a new aspect of security scenarios, namely how the number of agents and their task assignment impact the performance of attacking and defending strategies executed by agent coalitions. Our tool ADT2AMAS allows for transforming ADTrees into extended asynchronous multi-agent systems and computing an optimal schedule with the minimal number of agents. ADT2AMAS is integrated within the graphical verification platform CosyVerif, but can also be run standalone.},
  archive   = {C_AAMAS},
  author    = {Arias, Jaime and Penczek, Wojciech and Petrucci, Laure and Sidoruk, Teofil},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1749–1751},
  title     = {ADT2AMAS: Managing agents in attack-defence scenarios},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464225},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Constructing junction tree agent organization with privacy.
<em>AAMAS</em>, 1746–1748. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Several frameworks for decentralized reasoning assume a junction tree agent organization (JT-org). JT-org construction involves 3 related tasks on existence recognition, construction, and environment re-decomposition, where re-decomposition incurs loss of JT-org linked privacy, including privacy on agent, topology, private and shared variables. We propose a novel algorithm DAER that accomplishes all 3 tasks distributively. For Tasks 1 and 2, DAER incurs no loss of JT-org linked privacy.For Task 3, it incurs significantly less privacy loss than existing JT-org construction methods. Its performance is formally analyzed and empirically evaluated.},
  archive   = {C_AAMAS},
  author    = {Xiang, Yang and Alshememry, Abdulrahman},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1746–1748},
  title     = {Constructing junction tree agent organization with privacy},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464223},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A decentralised self-healing approach for network topology
maintenance. <em>AAMAS</em>, 1743–1745. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many distributed systems, from cloud to sensor networks, different configurations impact system performance, while strongly depending on the network topology. Hence, topological changes may entail costly reconfiguration and optimisation processes. This paper proposes a multi-agent solution for recovering a network&#39;s topology in case of node failures. The proposed approach relies on local information about the network&#39;s topology, collected and disseminated at runtime. Two strategies for distributing topological data are studied: one based on Mobile Agents (our proposal) and the other based on Trickle (a reference gossiping protocol from the literature). These two strategies were adapted for our self-healing approach, to collect topological information for network recovery; and were evaluated in terms of resource overheads. Experimental results show that both variants can recover the network topology, up to a certain node failure rate, which depends on the network topology. At the same time, Mobile Agents collect less information, focusing on local dissemination, which suffices for network recovery. This entails less bandwidth overheads than when Trickle is used. Still, Mobile Agents utilise more memory and exchange more messages during data-collection than Trickle does. These results validate the viability of the proposed self-healing solution, offering two variant implementations with diverse performance characteristics, which may suit different application domains.},
  archive   = {C_AAMAS},
  author    = {Rodr\&#39;{\i}guez, Arles and G\&#39;{o}mez, Jonatan and Diaconescu, Ada},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1743–1745},
  title     = {A decentralised self-healing approach for network topology maintenance},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464222},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). STRATA: Unified framework for task assignments in large
teams of heterogeneous agents. <em>AAMAS</em>, 1740–1742. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large teams of heterogeneous agents have the potential to solve complex multi-task problems that are intractable for a single agent working independently. However, solving complex multi-task problems requires leveraging the relative strengths of the different kinds of agents in the team. We present Stochastic TRAit-based Task Assignment (STRATA), a unified framework that models large teams of heterogeneous agents and performs effective task assignments. Specifically, given information on which traits (capabilities) are required for various tasks, STRATA computes the assignments of agents to tasks such that the trait requirements are achieved. Inspired by prior work in robot swarms and biodiversity, we categorize agents into different species (groups) based on their traits. We model each trait as a continuous variable and differentiate between traits that can and cannot be aggregated from different agents. STRATA is capable of reasoning about both species-level and agent-level variability in traits. We illustrate the necessity and effectiveness of STRATA using detailed numerical simulations and in a capture-the-flag game environment.},
  archive   = {C_AAMAS},
  author    = {Ravichandar, Harish and Shaw, Kenneth and Chernova, Sonia},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1740–1742},
  title     = {STRATA: Unified framework for task assignments in large teams of heterogeneous agents},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464221},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Teaching a robot with unlabeled instructions: The TICS
architecture. <em>AAMAS</em>, 1738–1739. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we propose a framework that enables a human to teach a robot a new task by interactively providing it with unlabeled instructions. We ground the meaning of instruction signals in the task-learning process, and use them simultaneously for guiding the latter. We implement our framework as a modular architecture, named TICS (Task-Instruction-Contingency-Shaping) that combines different information sources: a predefined reward function, human evaluative feedback and unlabeled instructions. This approach provides a novel perspective for robotic task learning that lies between Reinforcement Learning and Supervised Learning paradigms. We evaluate our framework both in simulation and with a real robot. The experimental results demonstrate the effectiveness of our framework in accelerating the task-learning process and in reducing the number of required teaching signals.},
  archive   = {C_AAMAS},
  author    = {Najar, Anis and Sigaud, Olivier and Chetouani, Mohamed},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1738–1739},
  title     = {Teaching a robot with unlabeled instructions: The TICS architecture},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464220},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Facial feedback for reinforcement learning: A case study and
offline analysis using the TAMER framework. <em>AAMAS</em>, 1735–1737.
(<a href="https://dl.acm.org/doi/10.5555/3463952.3464219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Interactive reinforcement learning provides a way for agents to learn to solve tasks from evaluative feedback provided by a human user. Previous research showed that humans give copious feedback early in training but very sparsely thereafter. In this paper, we investigate the potential of agent learning from trainers&#39; facial expressions via interpreting them as evaluative feedback. To do so, we implemented TAMER which is a popular interactive reinforcement learning method in a reinforcement-learning benchmark problem --- Infinite Mario, and conducted the first large-scale study of TAMER involving 561 participants. With designed CNN-RNN model, our analysis shows that telling trainers to use facial expressions and competition can improve the accuracies for estimating positive and negative feedback using facial expressions. In addition, our results with a simulation experiment show that learning solely from predicted feedback based on facial expressions is possible and using strong/effective prediction models or a regression method, facial responses would significantly improve the performance of agents. Furthermore, our experiment supports previous studies demonstrating the importance of bi-directional feedback and competitive elements in the training interface.},
  archive   = {C_AAMAS},
  author    = {Li, Guangliang and Dibeklio\u{g}lu, Hamdi and Whiteson, Shimon and Hung, Hayley},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1735–1737},
  title     = {Facial feedback for reinforcement learning: A case study and offline analysis using the TAMER framework},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464219},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Summarising a framework for the certification of reliable
autonomous systems. <em>AAMAS</em>, 1733–1734. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This extended abstract summarises the contributions from the journal article Fisher et al. (Autonomous Agents and Multi Agent Systems 35, 1 (2021), 8).},
  archive   = {C_AAMAS},
  author    = {Fisher, Michael and Mascardi, Viviana and Rozier, Kristin Y. and Schlingloff, Bernd-Holger and Winikoff, Michael and Yorke-Smith, Neil},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1733–1734},
  title     = {Summarising a framework for the certification of reliable autonomous systems},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464218},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On teammate-pattern-aware autonomy. <em>AAMAS</em>,
1730–1732. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We describe an approach for constraining robot autonomy based on the robot&#39;s awareness of patterns of its human teammates&#39; behaviors, rather than either ignoring its teammates (which is fast but dangerous) or inferring their plans (which is safer but slow). We evaluate this approach in a series of simulated problems where an unmanned ground vehicle and its human teammates must rapidly respond to a sudden context shift, and identify conditions that should be (purposely) met such that a pattern-aware approach is particularly effective compared to the alternatives.},
  archive   = {C_AAMAS},
  author    = {Durfee, Edmund H. and Thakur, Abhishek and Goldweber, Eli},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1730–1732},
  title     = {On teammate-pattern-aware autonomy},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464217},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Logic-based specification and verification of homogeneous
dynamic multi-agent systems. <em>AAMAS</em>, 1727–1729. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This is a short version of &#39;&#39;Logic-based specification and verification of homogeneous dynamic multi-agent systems&#39;&#39; in Autonomous Agents and Multi-Agent Systems 34, 2 (2020), where we developed a logic-based framework for formal specification and algorithmic verification of homogeneous and dynamic concurrent multi-agent transition systems (HDMAS). Homogeneity means that all agents have the same available actions at any given state and the actions have the same effects regardless of which agents perform them. Dynamicity means that the numbers of controllable and uncontrollable agents may vary throughout the system evolution, possibly at every transition. As a specification language, we extended Alternating-time Temporal Logic to express properties such as: &quot;a coalition of (at least) n controllable agents can ensure against (at most) m uncontrollable agents that any possible evolution of the system satisfies a given objective γ&quot;, where γ is specified again as a formula of that language and each of n and m is either a fixed number or a variable that can be quantified over. Finally, we show the algorithm and worst-case complexity for the model checking problem.},
  archive   = {C_AAMAS},
  author    = {De Masellis, Riccardo and Goranko, Valentin},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1727–1729},
  title     = {Logic-based specification and verification of homogeneous dynamic multi-agent systems},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464216},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Programming agent-based mobile apps: The JaCa-android
framework. <em>AAMAS</em>, 1724–1726. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A relevant application domain for agent-based software is given by smart mobile applications. In this context, the impressive progress of technologies makes it possible to explore the use of agent-oriented programming languages and frameworks based on cognitive architectures. Accordingly, here we provide a description of JaCa-Android, a framework based on the JaCaMo platform that allows for designing and programming smart mobile apps using BDI-based cognitive agents within the AandA conceptual model.},
  archive   = {C_AAMAS},
  author    = {Croatti, Angelo and Ricci, Alessandro},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1724–1726},
  title     = {Programming agent-based mobile apps: The JaCa-android framework},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464215},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Logic-based technologies for multi-agent systems: Summary of
a systematic literature review. <em>AAMAS</em>, 1721–1723. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The paper summarises a systematic literature review (SLR) over logic-based technologies for MAS.},
  archive   = {C_AAMAS},
  author    = {Calegari, Roberta and Ciatto, Giovanni and Mascardi, Viviana and Omicini, Andrea},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1721–1723},
  title     = {Logic-based technologies for multi-agent systems: Summary of a systematic literature review},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464214},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Agent programming in the cognitive era. <em>AAMAS</em>,
1718–1720. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It is claimed that, in the nascent &#39;Cognitive Era&#39;, intelligent systems will be trained using machine learning techniques rather than programmed by software developers. A contrary point of view argues that machine learning has limitations, and, taken in isolation, cannot form the basis of autonomous systems capable of intelligent behaviour in complex environments. In this paper, we argue that the unique strengths of Belief-Desire-Intention (BDI) agent programming languages provide an ideal framework for integrating the wide range of AI capabilities necessary for progress towards the next-generation of intelligent systems.},
  archive   = {C_AAMAS},
  author    = {Bordini, Rafael H. and El Fallah Seghrouchni, Amal and Hindriks, Koen and Logan, Brian and Ricci, Alessandro},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1718–1720},
  title     = {Agent programming in the cognitive era},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464213},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A norm enforcement mechanism for a time-constrained
conditional normative framework. <em>AAMAS</em>, 1715–1717. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper discusses the formalization of a norm enforcement mechanism for a regulative time constrained conditional normative framework. The norm representation captures both the condition and effect of a norm as situations and allows for rich temporal constraints between the times of the situations. As part of the enforcement process, a designated agent has an obligation within a time constraint to inform a liable agent of a reparative action they must take and the time constraint within which they must take it. That same agent must then monitor the compliance of the erring agent&#39;s obligation to carry out the reparative action, meting out sanctions in the case of violation.},
  archive   = {C_AAMAS},
  author    = {Akinkunmi, Babatunde Opeoluwa and Babalola, Florence Moyin},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1715–1717},
  title     = {A norm enforcement mechanism for a time-constrained conditional normative framework},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464212},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep interactive bayesian reinforcement learning via
meta-learning. <em>AAMAS</em>, 1712–1714. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Agents that interact with other agents often do not know a priori what the other agents&#39; strategies are, but have to maximise their own online return while interacting with and learning about others. The optimal adaptive behaviour under uncertainty over the other agents&#39; strategies w.r.t. some prior can in principle be computed using the Interactive Bayesian Reinforcement Learning framework. Unfortunately, doing so is intractable in most settings, and existing approximation methods are restricted to small tasks. To overcome this, we propose to meta-learn (alongside the policy) approximate belief inference by combining sequential and hierarchical VAEs. We show empirically that our approach can learn a factorised belief model that separates the other agent&#39;s permanent and temporal structure, and outperforms methods that sample from the approximate posterior or do not have this hierarchical structure. A full version of this work can be found in Zintgraf et al. (2021).},
  archive   = {C_AAMAS},
  author    = {Zintgraf, Luisa and Devlin, Sam and Ciosek, Kamil and Whiteson, Shimon and Hofmann, Katja},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1712–1714},
  title     = {Deep interactive bayesian reinforcement learning via meta-learning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464210},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fast adaptation to external agents via meta imitation
counterfactual regret advantage. <em>AAMAS</em>, 1709–1711. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper focuses on the multi-agent credit assignment problem. We propose a novel multi-agent reinforcement learning algorithm called meta imitation counterfactual regret advantage (MICRA) and a three-phase framework for training, adaptation, and execution of MICRA. The key features are: (1) a counterfactual regret advantage is proposed to optimize the target agents&#39; policy; (2) a meta-imitator is designed to infer the external agents&#39; policies. Results show that MICRA outperforms state-of-the-art algorithms.},
  archive   = {C_AAMAS},
  author    = {Zhang, Mingyue and Jin, Zhi and Xu, Yang and Shen, Zehan and Liu, Kun and Pan, Keyu},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1709–1711},
  title     = {Fast adaptation to external agents via meta imitation counterfactual regret advantage},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464209},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A blockchain-enabled quantitative approach to trust and
reputation management with sparse evidence. <em>AAMAS</em>, 1707–1708.
(<a href="https://dl.acm.org/doi/10.5555/3463952.3464208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The prevalence of e-commerce applications poses new trust challenges that render traditional Trust and Reputation Management (TRM) approaches inadequate. The first challenge is that TRM is built on evidence (direct or indirect observations) but evidence is becoming increasingly sparse because nowadays users have many more venues to share information. This makes it hard to derive trust models that are robust to attacks such as whitewashing and Sybil attacks. Second, the cost of attacks has reduced significantly due to the widespread presence of bots in e-commerce applications, which tends to invalidate the traditional assumption that majority users are honest.In this paper, we propose a new TRM framework called BEQA, which uses Blockchain to transform multiple disjoint and sparse sets of evidence into a single and dense evidence set. To address the second challenge, we introduce and formulate the cost of Sybil attacks using Blockchain transaction fees. In addition, we make a key observation that existing trust models have overlooked publicity (evidence originating from influencers) that exist in e-commerce applications. Thus, we formulate publicity as a whitewashing deposit such that a higher level of publicity will impose higher cost on Sybil attacks.},
  archive   = {C_AAMAS},
  author    = {Zeynalvand, Leonit and Luo, Tie and Andrejczuk, Ewa and Niyato, Dusit and Teo, Sin G. and Zhang, Jie},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1707–1708},
  title     = {A blockchain-enabled quantitative approach to trust and reputation management with sparse evidence},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464208},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimal crowdfunding design. <em>AAMAS</em>, 1704–1706. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  archive   = {C_AAMAS},
  author    = {Yan, Xiang and Chen, Yiling},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1704–1706},
  title     = {Optimal crowdfunding design},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464207},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning policies for effective incentive allocation in
unknown social networks. <em>AAMAS</em>, 1701–1703. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most existing incentive allocation approaches rely on sufficient information about users&#39; attributes, such as their preferences, followers in the social network, and activities, to customize effective incentives. However, this may lead to failure when such knowledge is unavailable. In this light, we propose an end-to-end reinforcement learning-based framework, named Geometric Actor-Critic (GAC), to discover effective incentive allocation policies towards users in a social network. More specifically, given a limited budget, the proposed approach can extract information from a high-level network representation for learning effective incentive allocation policies. The proposed GAC only requires the topology of the social network and does not rely on any prior information about users&#39; attributes. We use three real-world social network datasets to evaluate the performance of the proposed GAC. The experimental results demonstrate the effectiveness of the proposed approach.},
  archive   = {C_AAMAS},
  author    = {Wu, Shiqing and Bai, Quan and Li, Weihua},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1701–1703},
  title     = {Learning policies for effective incentive allocation in unknown social networks},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464206},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The sabre narrative planner: Multi-agent coordination with
intentions and beliefs. <em>AAMAS</em>, 1698–1700. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Narrative planning algorithms coordinate the virtual agents of interactive environments. Sabre is a single, centralized, omniscient decision maker that solves a multi-agent problem. It has a system-level author goal it must achieve, but every action taken by an agent must make sense according to that agent&#39;s individual intentions and limited, possibly wrong beliefs. We describe our motivation for solving such problems and the difficulties of comparing our planner to existing systems.},
  archive   = {C_AAMAS},
  author    = {Ware, Stephen G. and Siler, Cory},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1698–1700},
  title     = {The sabre narrative planner: Multi-agent coordination with intentions and beliefs},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464205},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The tight bound for pure price of anarchy in an extended
miner’s dilemma game. <em>AAMAS</em>, 1695–1697. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pool block withholding attack, which reduces the effective mining power in the system and leads to potential systemic instability in the blockchain, can be modeled as a non-cooperative game called &quot;the miner&#39;s dilemma&quot;. However, existing literature on the game-theoretic properties of this attack only gives a preliminary analysis. In this paper, we establish the existence and uniqueness of pure Nash equilibrium for the two-player miner&#39;s dilemma. Then we give a tight upper bound 2 for PPoA, which measures how much mining power is wasted in the game. Moreover, we show the uniqueness and the tight bound holds in a more general setting with betrayal assumption. Inspired by the experiments on the games among three mining pools, we conjecture that similar results should hold for the N-player miner&#39;s dilemma game (N ≥ 2).},
  archive   = {C_AAMAS},
  author    = {Wang, Qian and Chen, Yurong},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1695–1697},
  title     = {The tight bound for pure price of anarchy in an extended miner&#39;s dilemma game},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464204},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributed q-learning with state tracking for multi-agent
networked control. <em>AAMAS</em>, 1692–1694. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies distributed Q-learning for Linear Quadratic Regulator (LQR) in a multi-agent network. The existing results often assume that agents can observe the global system state, which may be infeasible in large-scale systems due to privacy concerns or communication constraints. In this work, we consider a setting with unknown system models and no centralized coordinator. We devise a state tracking (ST) based Q-learning algorithm to design optimal controllers for agents. Specifically, we assume that agents maintain local estimates of the global state based on their local information and communications with neighbors. At each step, every agent updates its local global state estimation, based on which it solves an approximate Q-factor locally through policy iteration. Assuming a decaying injected excitation noise during the policy evaluation, we prove that the local estimation converges to the true global state, and establish the convergence of the proposed distributed ST-based Q-learning algorithm. The experimental studies corroborate our theoretical results by showing that our proposed method achieves comparable performance with the centralized case.},
  archive   = {C_AAMAS},
  author    = {Wang, Hang and Lin, Sen and Jafarkhani, Hamid and Zhang, Junshan},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1692–1694},
  title     = {Distributed Q-learning with state tracking for multi-agent networked control},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464203},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards sample efficient learners in population based
referential games through action advising. <em>AAMAS</em>, 1689–1691.
(<a href="https://dl.acm.org/doi/10.5555/3463952.3464202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability of agents to learn to communicate through interaction has been studied through emergent communication tasks. Previous works in this domain have studied the linguistic properties of the emergent languages like compositionality, generalization, and as well as the environmental pressures that shape them. However, most of these experiments require a considerable amount of shared training time between agents to communicate successfully. Our work highlights the problem of sample inefficiency of agents in population-based referential games and proposes an Action Advising framework to counter it.},
  archive   = {C_AAMAS},
  author    = {Verma, Shresth},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1689–1691},
  title     = {Towards sample efficient learners in population based referential games through action advising},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464202},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning robust helpful behaviors in two-player cooperative
atari environments. <em>AAMAS</em>, 1686–1688. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of learning helpful behavior, specifically, learning to cooperate with differently-skilled and diverse partners in the context of two-player, cooperative Atari games. We show robust performance of these so-called Helper-AIs when paired with different kinds of partners (both human and artificial agents), including partners that they have not previously encountered during training. In particular, while pairing an expert AI with a non-expert AI leads to performance that is worse than when pairing the non-expert AI with a copy of itself, these Helper-AIs provide a substantial boost in joint performance.},
  archive   = {C_AAMAS},
  author    = {Tylkin, Paul and Radanovic, Goran and Parkes, David C.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1686–1688},
  title     = {Learning robust helpful behaviors in two-player cooperative atari environments},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464201},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Eliciting fairness in multiplayer bargaining through
network-based role assignment. <em>AAMAS</em>, 1683–1685. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {From employment contracts to climate agreements, individuals often engage in groups that must reach decisions with varying levels of fairness. These dilemmas also pervade AI, e.g. in automated negotiation, conflict resolution or resource allocation. As evidenced by the Ultimatum Game, payoff maximization is frequently at odds with fairness. Eliciting equality in populations of self-regarding agents thus requires judicious interventions. Here we use knowledge about agents&#39; social networks to implement fairness mechanisms, in the context of Multiplayer Ultimatum Games. We show that preferentially attributing the role of Proposer to low-connected nodes enhances fairness. We further show that, when high-degree must be the Proposers, stricter voting rules (i.e., requiring consensus for collectives to accept a proposal) reduce unfairness.},
  archive   = {C_AAMAS},
  author    = {Teixeira, Andreia Sofia and Santos, Francisco C. and Francisco, Alexandre P. and Santos, Fernando P.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1683–1685},
  title     = {Eliciting fairness in multiplayer bargaining through network-based role assignment},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464200},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cohorting to isolate asymptomatic spreaders: An agent-based
simulation study on the mumbai suburban railway. <em>AAMAS</em>,
1680–1682. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies cohorting in public transit systems and its usefulness in mitigating disease transmission. The Mumbai suburban railway system is used as a case study.},
  archive   = {C_AAMAS},
  author    = {Talekar, Alok and Shriram, Sharad and Vaidhiyan, Nidhin and Aggarwal, Gaurav and Chen, Jiangzhuo and Venkatramanan, Srini and Wang, Lijing and Adiga, Aniruddha and Sadilek, Adam and Tendulkar, Ashish and Marathe, Madhav and Sundaresan, Rajesh and Tambe, Milind},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1680–1682},
  title     = {Cohorting to isolate asymptomatic spreaders: An agent-based simulation study on the mumbai suburban railway},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464199},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Branch-and-bound heuristics for incomplete DCOPs.
<em>AAMAS</em>, 1677–1679. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Incomplete Distributed Constraint Optimization Problem (I-DCOP) extends the distributed constraint optimization problem, where constraint costs are allowed to be unspecified. A distributed variant of the Synchronous Branch-and-Bound (SyncBB) search algorithm has been proposed to solve I-DCOPs, where unspecified constraint costs are elicited during its execution. In this paper, we propose two heuristics that can be used in conjunction with SyncBB to solve I-DCOPs. Our proposed heuristics speed up the algorithm by pruning those parts of the search space whose solution quality is sub-optimal. Thus, our model and heuristics extend the state of the art in distributed constraint reasoning to better model and solve distributed agent-based applications with user preferences.},
  archive   = {C_AAMAS},
  author    = {Tabakhi, Atena M. and Xiao, Yuanming and Yeoh, William and Zivan, Roie},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1677–1679},
  title     = {Branch-and-bound heuristics for incomplete DCOPs},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464198},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sound algorithms in imperfect information games.
<em>AAMAS</em>, 1674–1676. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Search has played a fundamental role in computer game research since the very beginning. And while online search has been commonly used in perfect information games such as Chess and Go, online search methods for imperfect information games have only been introduced relatively recently. This paper addresses the question of what is a sound online algorithm in an imperfect information setting of two-player zero-sum games? We argue that the fixed-strategy definitions of exploitability and epsilon-Nash equilibria are ill suited to measure the worst-case performance of an online algorithm. We thus formalize epsilon-soundness, a concept that connects the worst-case performance of an online algorithm to the performance of an epsilon-Nash equilibrium. Our definition of soundness and the consistency hierarchy finally provide appropriate tools to analyze online algorithms in repeated imperfect information games. We thus inspect some of the previous online algorithms in a new light, bringing new insights into their worst case performance guarantees.},
  archive   = {C_AAMAS},
  author    = {\v{S}ustr, Michal and Schmid, Martin and Morav\v{c}\&#39;{\i}k, Matej and Burch, Neil and Lanctot, Marc and Bowling, Michael},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1674–1676},
  title     = {Sound algorithms in imperfect information games},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464197},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A distributional perspective on value function factorization
methods for multi-agent reinforcement learning. <em>AAMAS</em>,
1671–1673. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Distributional reinforcement learning (RL) provides beneficial impacts for the single-agent domain. However, distributional RL methods are not directly compatible with value function factorization methods for multi-agent reinforcement learning. This work provides a distributional perspective on value function factorization, offering a solution for bridging the gap between distributional RL and value function factorization methods.},
  archive   = {C_AAMAS},
  author    = {Sun, Wei-Fang and Lee, Cheng-Kuang and Lee, Chun-Yi},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1671–1673},
  title     = {A distributional perspective on value function factorization methods for multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464196},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Intrinsic motivated multi-agent communication.
<em>AAMAS</em>, 1668–1670. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Efficient communication is a promising way to achieve cooperation among agents in many real-world scenarios. However, aimless and motiveless information sharing may not work or even degrade the cooperative performance. Typically, the multi-agent communication behaviors are motivated by extrinsic rewards from environment. We conclude the mechanism as &#39;Communicate what rewards you&#39;. In this work, we present a novel communication mechanism called Intrinsic Motivated Multi-Agent Communication (IMMAC). Our key insight can be summarized as &#39;Communicate what surprises you&#39;. Concretely, we use an observation-dependent intrinsic value to represent the importance of observed information. Then a gating mechanism and an attentional mechanism based on intrinsic values are designed to control communication. By encouraging agent to communicate and focus on the observations with uncertain and important information, our algorithm achieves superior communication efficiency and cooperative performance. We evaluate IMMAC on a variety of challenging tasks, and demonstrate that intrinsic values are sufficient to drive efficient communication behaviors. Moreover, we found that the combination of intrinsic values and extrinsic values can further improve the communication efficiency. Consequently, intrinsic motivation is a promising way to control communication and it is capable of being a good complement to the existing extrinsic motivated communication methods.},
  archive   = {C_AAMAS},
  author    = {Sun, Chuxiong and Wu, Bo and Wang, Rui and Hu, Xiaohui and Yang, Xiaoya and Cong, Cong},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1668–1670},
  title     = {Intrinsic motivated multi-agent communication},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464195},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Gambler bandits and the regret of being ruined.
<em>AAMAS</em>, 1664–1667. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we consider a particular class of problems calledmultiarmed gambler bandits (MAGB) which constitutes a modified version of the Bernoulli MAB problem where two new elements must be taken into account:thebudget and therisk of ruin. The agent has an initial budget that evolves in time following the received rewards, which can be either +1 after asuccess or -1 after afailure. The problem can also be seen as a MAB version of the classicgambler&#39;s ruin game. The contribution of this paper is a preliminary analysis on the probability of being ruined given the current budget and observations, and the proposition of an alternative regret formulation, combining the classic regret notion with the expected loss due to the probability of being ruined. Finally, standard state-of-the-art methods are experimentally compared using the proposed metric.},
  archive   = {C_AAMAS},
  author    = {Studzinski Perotto, Filipo and Vakili, Sattar and Gajane, Pratik and Faghan, Yaser and Bourgais, Mathieu},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1664–1667},
  title     = {Gambler bandits and the regret of being ruined},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464194},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A succinct representation scheme for cooperative games under
uncertainty. <em>AAMAS</em>, 1661–1663. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work we present a novel succinct representation for large partially observed cooperative games. The proposed representation exploits estimates over marginal contributions to form compact rules representing collaboration patterns with uncertain value. Specifically, given an initial set of MC-nets rules that use prior beliefs over values instead of the actual ones, we propose two types of merging that lead to a new set of even more compact rules.},
  archive   = {C_AAMAS},
  author    = {Streviniotis, Errikos and Georgara, Athina and Chalkiadakis, Georgios},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1661–1663},
  title     = {A succinct representation scheme for cooperative games under uncertainty},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464193},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-attention meta-learner for continual learning.
<em>AAMAS</em>, 1658–1660. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Continual learning aims to provide intelligent agents capable of learning multiple tasks sequentially with neural networks. In most settings of the current approaches, the agent starts from randomly initialized parameters and is optimized to master the current task regardless of the usefulness of the learned representation for future tasks. Moreover, each of the future tasks uses all the previously learned knowledge although parts of this knowledge might not be helpful for its learning. These cause interference among tasks, especially when the data of previous tasks is not accessible. In this paper, we propose a new method, named Self-Attention Meta-Learner (SAM), which learns a prior knowledge for continual learning that permits learning a sequence of tasks, while avoiding catastrophic forgetting. SAM incorporates an attention mechanism that learns to select the particular relevant representation for each future task. We empirically show that we can achieve a better performance than several state-of-the-art methods for continual learning by building on top of the selected representation learned by SAM. We also show the role of the meta-attention mechanism in boosting informative features corresponding to the input task and identifying the correct target in the task agnostic inference. Finally, we demonstrate that popular existing continual learning methods gain a performance boost when they adopt SAM as a starting point.},
  archive   = {C_AAMAS},
  author    = {Sokar, Ghada and Mocanu, Decebal Constantin and Pechenizkiy, Mykola},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1658–1660},
  title     = {Self-attention meta-learner for continual learning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464192},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Approximate difference rewards for scalable multigent
reinforcement learning. <em>AAMAS</em>, 1655–1657. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the problem ofmultiagent credit assignment in a large scale multiagent system. Difference rewards (DRs) are an effective tool to tackle this problem, but their exact computation is known to be challenging even for small number of agents. We propose a scalable method to compute difference rewards based on aggregate information in a multiagent system with large number of agents by exploiting the symmetry present in several practical applications. Empirical evaluation on two multiagent domains---air-traffic control and cooperative navigation, shows better solution quality than previous approaches.},
  archive   = {C_AAMAS},
  author    = {Singh, Arambam James and Kumar, Akshat and Lau, Hoong Chuin},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1655–1657},
  title     = {Approximate difference rewards for scalable multigent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464191},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MAS-bench: Parameter optimization benchmark for multi-agent
crowd simulation. <em>AAMAS</em>, 1652–1654. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent crowd simulation is generally used to construct an environment suitable for reality by parameter estimation. This estimation is to modify a model defined by human movements and behaviors to fit real-world data by means of statistics or optimization [2, 11]. In the field of traffic engineering, for example, parameter estimation is used to modify speed models and collision models of pedestrians and vehicles [10, 13]. For different simulation settings, it is not easy to obtain original data of each real-world environment. Few existing estimation methods have been fairly evaluated. Therefore, there is a need for a benchmark to discuss the applicability of certain estimation methods to other use cases.},
  archive   = {C_AAMAS},
  author    = {Shigenaka, Shusuke and Takami, Shunki and Watanabe, Shuhei and Tanigaki, Yuki and Ozaki, Yoshihiko and Onishi, Masaki},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1652–1654},
  title     = {MAS-bench: Parameter optimization benchmark for multi-agent crowd simulation},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464190},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Egalitarian and just digital currency networks.
<em>AAMAS</em>, 1649–1651. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We explore the possibility of an alternative cryptocurrency that is egalitarian in control, just in the distribution of its created wealth, and forms and grows in a grassroots way; we analyze it via a mathematical entity called a currency network. Our main result states sufficient conditions for the establishment of 1:1 exchange rates and distributive justice in a currency network.},
  archive   = {C_AAMAS},
  author    = {Shahaf, Gal and Shapiro, Ehud and Talmon, Nimrod},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1649–1651},
  title     = {Egalitarian and just digital currency networks},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464189},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). HOAD: The hanabi open agent dataset. <em>AAMAS</em>,
1646–1648. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work we present the Hanabi Open Agent Dataset (HOAD)- meant to address the current lack of Hanabi datasets, HOAD is an easily extensible, open-sourced, and comprehensive collection of existing Hanabi playing agents, all ported to the Hanabi Learning Environment (HLE). We give a description and analysis of each agent&#39;s strategy, and we also show cross-play performance between all the agents, demonstrating both their high quality and diversity of strategy. These properties make HOAD especially well suited to studies involving meta-learning and transfer learning. Finally, we describe in detail an easy way to add new agents to HOAD regardless of the origin codebase of the agent and make our code and dataset publicly available at https://github.com/aronsar/hoad.},
  archive   = {C_AAMAS},
  author    = {Sarmasi, Aron and Zhang, Timothy and Cheng, Chu-Hung and Pham, Huyen and Zhou, Xuanchen and Nguyen, Duong and Shekdar, Soumil and McCoy, Joshua},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1646–1648},
  title     = {HOAD: The hanabi open agent dataset},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464188},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Social network interventions to prevent reciprocity-driven
polarization. <em>AAMAS</em>, 1643–1645. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Complex networks and reputation systems are fundamental mechanisms to sustain cooperation in populations of self-regarding agents. These mechanisms are typically studied in isolation. In online social platforms, however, behavioral dynamics are likely to result from their combination. Here we investigate the relationship between social networks and reputation-based cooperation (in a Prisoner&#39;s Dilemma setting) in large populations. We develop a new evolutionary game-theoretical model and study dynamics in networks with varying degrees of community structure. We show that networks exhibiting modular structures hamper global cooperation: reputation-based group identities emerge in different communities and strategies that uniquely cooperate with in-group members fixate, sustaining polarization and group bias. Global cooperation is recovered provided that inter-community edges are added.},
  archive   = {C_AAMAS},
  author    = {Santos, Fernando P. and Santos, Francisco C. and Pacheco, Jorge M. and Levin, Simon A.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1643–1645},
  title     = {Social network interventions to prevent reciprocity-driven polarization},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464187},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mitigating negative side effects via environment shaping.
<em>AAMAS</em>, 1640–1642. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Agents operating in the open world often produce negative side effects (NSE), which are difficult to identify at design time. We examine how a human can assist an agent, beyond providing feedback, and exploit their broader scope of knowledge to mitigate the impacts of NSE. We formulate this problem as a human-agent team with decoupled objectives. The agent optimizes its assigned task, during which its actions may produce NSE. The human shapes the environment through minor reconfiguration actions so as to mitigate the impacts of agent&#39;s side effects, without significantly degrading agent performance. We present an algorithm to solve this problem. Empirical evaluation shows that the proposed framework can successfully mitigate NSE, without affecting the agent&#39;s ability to complete its assigned task.},
  archive   = {C_AAMAS},
  author    = {Saisubramanian, Sandhya and Zilberstein, Shlomo},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1640–1642},
  title     = {Mitigating negative side effects via environment shaping},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464186},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Dynamic skill selection for learning joint actions.
<em>AAMAS</em>, 1637–1639. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning in tightly coupled multiagent settings with sparse rewards is challenging because multiple agents must reach the goal state simultaneously for the team to receive a reward. This is even more challenging under temporal coupling constraints - where agents need to sequentially complete different components of a task in a particular order. Here, a single local reward is inadequate for learning an effective policy. We introduce MADyS, Multiagent Learning via Dynamic Skill Selection, a bi-level optimization framework that learns to dynamically switch between multiple local skills to optimize sparse team objectives. MADyS adopts fast policy gradients to learn local skills using local rewards and an evolutionary algorithm to optimize the sparse team objective by recruiting the most optimal skill at any given time. This eliminates the need to generate a single dense reward via reward shaping or other mixing functions. In environments with both spatial and temporal coupling requirements, we outperform prior methods and provides intuitive visualizations of its skill switching strategy.},
  archive   = {C_AAMAS},
  author    = {Sachdeva, Enna and Khadka, Shauharda and Majumdar, Somdeb and Tumer, Kagan},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1637–1639},
  title     = {Dynamic skill selection for learning joint actions},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464185},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Combining LSTMs and symbolic approaches for robust plan
recognition. <em>AAMAS</em>, 1634–1636. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Plan recognition is the task of inferring the actual plan an observed agent is performing to achieve a goal, given domain theory and a partial, possibly noisy, sequence of observations. Applications include natural language processing, elder-care, multi-agent systems, collaborative problem-solving, epistemic problems, and more. Real-world plan recognition problems impose limitations on the quality and quantity of the observations, which may be missing or faulty from silent errors in the sensors. While recent approaches to goal and plan recognition have substantially improved performance under partial observability and noisy conditions, dealing with these problems remains a challenge. Recent work on goal and plan recognition use machine learning to assist planning-based approaches in modeling domains. Such techniques yield robust models capable of accurate predictions with missing or noisy data. Thus inspired, we develop a novel approach to solve both goal and plan recognition tasks simultaneously by combining planning and machine learning techniques to mitigate problems of low and faulty observability.},
  archive   = {C_AAMAS},
  author    = {Rosa Amado, Leonardo and Fraga Pereira, Ramon and Meneguzzi, Felipe},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1634–1636},
  title     = {Combining LSTMs and symbolic approaches for robust plan recognition},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464184},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An autonomous drive balancing strategy for the design of
purpose in open-ended learning robots. <em>AAMAS</em>, 1631–1633. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper is concerned with designing purpose in autonomous robots for open-ended learning settings. Unconstrained human robot interaction situations and robotic systems that must operate in dynamic multi-robot scenarios are paradigmatic examples of open-endedness. An approach to the appropriate design and engineering of motivational structures to endow robots with a particular purpose is proposed and tested. This approach focuses on the drive structure and how it can be made to autonomously adapt to changing circumstances. Specifically, a simple evolutionary strategy for the autonomous regulation of multiple drives in order to optimize long-term operation is defined. The experimental results have been obtained on a Baxter robot facing changing situations in real setups.},
  archive   = {C_AAMAS},
  author    = {Romero, Alejandro and Bellas, Francisco and Duro, Richard J.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1631–1633},
  title     = {An autonomous drive balancing strategy for the design of purpose in open-ended learning robots},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464183},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiagent task allocation and planning with multi-objective
requirements. <em>AAMAS</em>, 1628–1630. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In service robot applications, planning is often integrated with task allocation. Linear Temporal Logic (LTL) as an expressive high-level formalism is widely used for task specification, and allows for formalised restrictions on temporal sequences of tasks. In multiagent planning, a Multi-Objective Markov Decision Process extends the standard model with vector rewards capturing possibly conflicting planning objectives. Such objectives include the success rates of accomplishing individual tasks, and the cost budgets for individual agents. In this paper, we consider the problem of concurrently allocating LTL task sequences to a team of agents and calculating optimal task schedulers simultaneously, satisfying cost and probability thresholds. We reduce this problem to multi-objective scheduler synthesis for a team MDP structure, whose size is linear in the number of agents. Our preliminary experiment demonstrates the scalability of our approach.},
  archive   = {C_AAMAS},
  author    = {Robinson, Thomas and Su, Guoxin and Zhang, Minjie},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1628–1630},
  title     = {Multiagent task allocation and planning with multi-objective requirements},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464182},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Finite-time consensus in the presence of malicious agents.
<em>AAMAS</em>, 1625–1627. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A finite-time consensus protocol is developed for a connected network of agents, where communication between agents occurs locally, some of the agents are malicious, and the non-malicious or cooperative agents do not know the identities of the malicious ones. The agents are modeled with first-order dynamics, and the inputs to each agent that enable consensus are designed using the principles of sliding mode control (SMC). The use of the SMC algorithm guarantees finite-time consensus and ensures that in the transient stage, the agents&#39; states are contained within the convex hull formed by their initial conditions (ICs). With this feature and by modeling the network as a connected graph, the protocol guarantees consensus amongst the cooperative agents when the malicious agents transmit values of their states lying outside the convex hull of ICs, and the graph formed by the cooperative agents with the removal of the malicious agents is strongly connected. The protocol does not require a cooperative agent to know the number of malicious or other cooperative agents in the network, and is based only on local communication.},
  archive   = {C_AAMAS},
  author    = {Rao, Sachit and Rao, Shrisha},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1625–1627},
  title     = {Finite-time consensus in the presence of malicious agents},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464181},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Personalising the dialogue of relational agents for
first-time users. <em>AAMAS</em>, 1622–1624. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Relational agents who personalise their message to individual users need to learn about the user first. In the context of an agent who personalises the inclusion of 10 relational cues in its dialogue, we explore training the agent based on first time user&#39;s responses to a single example of each cue prior to the session with the virtual advisor. We designed a between subjects study with three groups: Empathic (all relational cues); Neutral (no relational cues) and Adaptive (only helpful cues included). We found that in the Adaptive group, students received what they found helpful more often than in the other two groups. Analysis of the discrepancy between what user&#39;s found helpful and what they received was least in the Adaptive group and greatest in the Neutral group.},
  archive   = {C_AAMAS},
  author    = {Ranjbartabar, Hedieh and Richards, Deborah and Bilgin, Ayse Aysin and Kutay, Cat},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1622–1624},
  title     = {Personalising the dialogue of relational agents for first-time users},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464180},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toward a self-learning governance loop for competitive
multi-attribute MAS. <em>AAMAS</em>, 1619–1621. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Competitive Multi-Agent Systems (MAS) are inherently hard to control due to agent autonomy and strategic behavior, which is particularly problematic when there are system-level objectives to be achieved or specific environmental states to be avoided.Existing methods mostly assume specific knowledge about agent preferences, utilities and strategies, neglecting the fact that actions are not always directly linked to genuine agent preferences, but can also reflect anticipated competitor behavior, be a concession to a superior adversary or simply be intended to mislead other agents. This assumption both reduces applicability to real-world systems and opens room for manipulation.We therefore propose a new governance approach for Multi-Attribute MAS which relies exclusively on publicly observable actions and transitions, and uses the acquired knowledge to purposefully restrict action spaces, thereby achieving the system&#39;s objectives while preserving a high level of autonomy for the agents.},
  archive   = {C_AAMAS},
  author    = {Pernpeintner, Michael},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1619–1621},
  title     = {Toward a self-learning governance loop for competitive multi-attribute MAS},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464179},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Attention actor-critic algorithm for multi-agent constrained
co-operative reinforcement learning. <em>AAMAS</em>, 1616–1618. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we consider the problem of computing optimal actions for Reinforcement Learning (RL) agents in a co-operative setting, where the objective is to optimize a common goal. However, in many real-life applications, the agents are also required to satisfy certain constraints specified on their actions. Under this setting, the objective of the agents is to not only learn the actions that optimize the common objective but also meet the specified constraints. In recent times, the Actor-Critic algorithm with an attention mechanism has been successfully applied to obtain optimal actions for RL agents in multi-agent environments. In this work, we extend this algorithm to the constrained multi-agent RL setting.},
  archive   = {C_AAMAS},
  author    = {Parnika, P. and Diddigi, Raghuram Bharadwaj and Danda, Sai Koti Reddy and Bhatnagar, Shalabh},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1616–1618},
  title     = {Attention actor-critic algorithm for multi-agent constrained co-operative reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464178},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Online learning of shaping reward with subgoal knowledge.
<em>AAMAS</em>, 1613–1615. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {SARSA-RS is a reward shaping method that updates the shaping through learning. However, the bottleneck of this method is the aggregation of states since designers need to design mappings from all states to abstract states. We propose a dynamic trajectory aggregation that uses subgoal series. The designer&#39;s effort becomes minimal because only human input is the subgoal series. This makes application to environments with high-dimensional observations possible. We compared our method by using participants&#39; subgoal series with a baseline reinforcement learning algorithm and other subgoal-based methods in a navigation task. As a result, our reward shaping outperformed all other methods in learning efficiency.},
  archive   = {C_AAMAS},
  author    = {Okudo, Takato and Yamada, Seiji},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1613–1615},
  title     = {Online learning of shaping reward with subgoal knowledge},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464177},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tunable behaviours in sequential social dilemmas using
multi-objective reinforcement learning. <em>AAMAS</em>, 1610–1612. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this study, we leverage multi-objective reinforcement learning to create tunable agents, i.e., agents that can adopt a range of different behaviours according to the designer&#39;s preferences, without the need for retraining. We apply this technique to sequential social dilemmas, settings where there is inherent tension between individual and collective rationality. Learning a single fixed policy in such settings leaves one at a significant disadvantage if the opponents&#39; strategies change after learning is complete. In our work, we demonstrate empirically that the tunable agents framework allows easy adaption between cooperative and competitive behaviours in sequential social dilemmas without the need for retraining, allowing a single trained agent model to be adjusted to cater for a wide range of behaviours and opponent strategies.},
  archive   = {C_AAMAS},
  author    = {O&#39;Callaghan, David and Mannion, Patrick},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1610–1612},
  title     = {Tunable behaviours in sequential social dilemmas using multi-objective reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464176},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SIBRE: Self improvement based REwards for adaptive feedback
in reinforcement learning. <em>AAMAS</em>, 1607–1609. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a generic reward shaping approach for improving the rate of convergence in reinforcement learning (RL), called Self Improvement Based REwards, or SIBRE. The approach is designed for use in conjunction with any existing RL algorithm, and consists of rewarding improvement over the agent&#39;s own past performance. We prove that SIBRE converges in expectation under the same conditions as the original RL algorithm. The reshaped rewards help discriminate between policies when the original rewards are weakly discriminated or sparse. Experiments on several well-known benchmark environments with different RL algorithms show that SIBRE converges to the optimal policy faster and more stably. We also perform sensitivity analysis with respect to hyper-parameters, in comparison with baseline RL algorithms.},
  archive   = {C_AAMAS},
  author    = {Nath, Somjit and Verma, Richa and Ray, Abhik and Khadilkar, Harshad},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1607–1609},
  title     = {SIBRE: Self improvement based REwards for adaptive feedback in reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464175},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Privacy-preserving and accountable multi-agent learning.
<em>AAMAS</em>, 1605–1606. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Distributed multi-agent learning enables agents to cooperatively train a model without requiring to share their datasets. While this setting ensures some level of privacy, it has been shown that, even when data is not directly shared, the training process is vulnerable to privacy attacks including data reconstruction and model inversion attacks. Additionally, malicious agents that train on inverted labels or random data, may arbitrarily weaken the accuracy of the global model. This paper addresses these challenges and presents Privacy-preserving and Accountable Distributed Learning (PA-DL), a fully decentralized framework that relies on Differential Privacy to guarantee strong privacy protection of the agents data, and Ethereum smart contracts to ensure accountability.},
  archive   = {C_AAMAS},
  author    = {Nagar, Anudit and Tran, Cuong and Fioretto, Ferdinando},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1605–1606},
  title     = {Privacy-preserving and accountable multi-agent learning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464174},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A general framework for the logical representation of
combinatorial exchange protocols. <em>AAMAS</em>, 1602–1604. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The goal of this work is to propose a framework for representing and reasoning about the rules governing a combinatorial exchange. Such a framework is at first interest as long as we want to build up digital marketplaces based on auction, a widely used mechanism for automated transactions. Hence the framework should fulfill two requirements: (i) it should enable bidders to express their bids on combinations of goods and (ii) it should allow describing the rules governing some market, namely the legal bids, the allocation and payment rules. To do so, we define a logical language in the spirit of the Game Description Language: the Combinatorial Exchange Description Language is the first language for describing combinatorial exchange in a logical framework. The contribution is two-fold: first, we illustrate the general dimension by representing different kinds of protocols, and second, we show how to reason about auction properties in this machine-processable language.},
  archive   = {C_AAMAS},
  author    = {Mittelmann, Munyque and Bouveret, Sylvain and Perrussel, Laurent},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1602–1604},
  title     = {A general framework for the logical representation of combinatorial exchange protocols},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464173},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A strategic analysis of portfolio compression.
<em>AAMAS</em>, 1599–1601. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We analyze portfolio compression, the netting of cycles in a financial network, as a strategic decision made by firms within a debt network. We define a network game in which firms have only local information and ask what criteria the firms should consider in their decision to compress. We propose a variety of heuristic strategies and evaluate them using agent-based simulation and empirical game-theoretic analysis. Our results show that some simple strategies based on local information perform better than the unconditional strategies of always agreeing or disagreeing to a compression and that when the decision is made strategically, the price of anarchy is always high.},
  archive   = {C_AAMAS},
  author    = {Mayo, Katherine and Wellman, Michael P.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1599–1601},
  title     = {A strategic analysis of portfolio compression},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464172},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimized execution of PDDL plans using behavior trees.
<em>AAMAS</em>, 1596–1598. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots need task planning to sequence and execute actions toward achieving their goals. On the other hand, Behavior Trees provide a mathematical model for specifying plan execution in an intrinsically composable, reactive, and robust way. PDDL (Planning Domain Definition Language) has become the standard description language for most planners. In this paper, we present a novel algorithm to systematically create behavior trees from PDDL plans to execute them. This approach uses the execution graph of the plan to generate a behavior tree. The most remarkable contribution of this approach is the algorithm to build a Behavior Tree that optimizes its execution by paralyzing actions, applicable to any plan, taking into account the actions&#39; causal relationships. We demonstrate the improvement in the execution of plans in mobile robots using the ROS2 Planning System framework.},
  archive   = {C_AAMAS},
  author    = {Mart\&#39;{\i}n Rico, Francisco and Morelli, Matteo and Espinoza, Huascar and Rodr\&#39;{\i}guez-Lera, Francisco J. and Matell\&#39;{a}n Olivera, Vicente},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1596–1598},
  title     = {Optimized execution of PDDL plans using behavior trees},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464171},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Trajectory diversity for zero-shot coordination.
<em>AAMAS</em>, 1593–1595. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of zero-shot coordination (ZSC), where agents must independently produce strategies for a collaborative game that are compatible with novel partners not seen during training. In particular, our first contribution is to consider the need for diversity in generating such agents. Because self-play agents control their own trajectory distribution during training, their policy only performs well on this exact distribution. As a result, they achieve low scores in ZSC, since playing with another agent is likely to put them in situations they have not encountered during training. To address this issue, we train a common best response (BR) to a population of agents, which we regulate to be as diverse as possible. For that purpose, we introduce Trajectory Diversity (TrajeDi) - a differentiable objective for generating diverse reinforcement learning (RL) policies. We present TrajeDi as a generalization of the Jensen-Shannon divergence (JSD) between policies and motivate it experimentally in a simple matrix game, where it allows to find the unique ZSC-optimal solution.},
  archive   = {C_AAMAS},
  author    = {Lupu, Andrei and Hu, Hengyuan and Foerster, Jakob},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1593–1595},
  title     = {Trajectory diversity for zero-shot coordination},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464170},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Solid semantics and extension aggregation using quota rules
under integrity constraints. <em>AAMAS</em>, 1590–1592. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose solid semantics to obtain the most acceptable sets of arguments. Besides, we study the application of solid semantics in the field of judgement aggregation.},
  archive   = {C_AAMAS},
  author    = {Liu, Xiaolong and Chen, Weiwei},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1590–1592},
  title     = {Solid semantics and extension aggregation using quota rules under integrity constraints},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464169},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Call markets with adaptive clearing intervals.
<em>AAMAS</em>, 1587–1589. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trading mechanisms play a fundamental role in the health of financial markets. For example, it is believed that continuous double auctions constitute fertile soil for predatory behaviour and toxic order flows. To this end, frequent call markets have been proposed as an alternative design choice to address the latency arbitrage opportunities built in those markets. This paper studies the extent to which adaptive rules to define the length of the clearing intervals affect the performance of frequent call markets.},
  archive   = {C_AAMAS},
  author    = {Liu, Buhong and Polukarov, Maria and Ventre, Carmine and Li, Lingbo and Kanthan, Leslie},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1587–1589},
  title     = {Call markets with adaptive clearing intervals},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464168},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reliability-aware multi-UAV coverage path planning using a
genetic algorithm. <em>AAMAS</em>, 1584–1586. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graceful degradation is a desirable trait in applications that require coverage with real, failure-prone robots. Thus paper uses methods informed by Reliability Engineering to study the Reliability-Aware Multi-Agent Coverage Path Planning (RA-MCPP) problem. An augmented stochastic framework is applied to evaluate a strategy&#39;s probability of mission completion. An augmented stochastic framework is applied to evaluate a strategy&#39;s probability of mission completion (PoC) on 3D lattice graph environments. A Genetic Algorithm optimisation approach is then proposed to find RA-MCPP path plans which maximise PoC. It is shown that the GA provides good solutions at reasonable runtimes, complementing previous approaches which focused on global optimality guarantees at the cost of massive computation, especially for medium and large environments.},
  archive   = {C_AAMAS},
  author    = {Li, Mickey and Richards, Arthur and Sooriyabandara, Mahesh},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1584–1586},
  title     = {Reliability-aware multi-UAV coverage path planning using a genetic algorithm},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464167},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Anytime multi-agent path finding via large neighborhood
search. <em>AAMAS</em>, 1581–1583. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-Agent Path Finding (MAPF) is the challenging problem of computing collision-free paths for a cooperative team of moving agents. Algorithms for solving MAPF can be categorized on a spectrum. At one end are (bounded-sub)optimal algorithms that can find high-quality solutions for small problems. At the other end are unbounded-suboptimal algorithms (including prioritized and rule-based algorithms) that can solve very large practical problems but usually find low-quality solutions. In this paper, we consider a third approach that combines both advantages: anytime algorithms that quickly find an initial solution, including for large problems, and that subsequently improve the solution to near-optimal as time progresses. To improve the solution, we replan subsets of agents using Large Neighborhood Search, a popular meta-heuristic often applied in combinatorial optimization. Empirically, we compare our algorithm MAPF-LNS to the state-of-the-art anytime MAPF algorithm anytime BCBS and report significant gains in scalability, runtime to the first solution, and speed of improving solutions.},
  archive   = {C_AAMAS},
  author    = {Li, Jiaoyang and Chen, Zhe and Harabor, Daniel and Stuckey, Peter J. and Koenig, Sven},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1581–1583},
  title     = {Anytime multi-agent path finding via large neighborhood search},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464166},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Object allocation over a network of objects: Mobile agents
with strict preferences. <em>AAMAS</em>, 1578–1580. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent work, Gourv\`{e}s, Lesca, and Wilczynski propose a variant of the classic housing markets model where the matching between agents and objects evolves through Pareto-improving swaps between pairs of adjacent agents in a social network. To explore the swap dynamics of their model, they pose several basic questions concerning the set of reachable matchings. In their work and other follow-up works, these questions have been studied for various classes of graphs: stars, paths, generalized stars (i.e., trees where at most one vertex has degree greater than two), trees, and cliques. For generalized stars and trees, it remains open whether a Pareto-efficient reachable matching can be found in polynomial time.In this paper, we pursue the same set of questions under a natural variant of their model. In our model, the social network is replaced by a network of objects, and a swap is allowed to take place between two agents if it is Pareto-improving and the associated objects are adjacent in the network. In those cases where the question of polynomial-time solvability versus NP-hardness has been resolved for the social network model, we are able to show that the same result holds for the network-of-objects model. In addition, for our model, we present a polynomial-time algorithm for computing a Pareto-efficient reachable matching in generalized star networks. Moreover, the object reachability algorithm that we present for path networks is significantly faster than the known polynomial-time algorithms for the same question in the social network model.},
  archive   = {C_AAMAS},
  author    = {Li, Fu and Plaxton, C. Gregory and Sinha, Vaibhav B.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1578–1580},
  title     = {Object allocation over a network of objects: Mobile agents with strict preferences},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464165},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Partial disclosure of private dependencies in privacy
preserving planning. <em>AAMAS</em>, 1575–1577. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Designing autonomous agents that act collaboratively to achieve a common set of goals has been a major goal for Artificial Intelligence research for many years. Collaborative Privacy-Preserving Planning (cppp) is a multi-agent planning task in which agents need to achieve a common set of goals without revealing certain private information [2]. In particular, in cppp an individual agent may have a set of private facts and actions that it does not share with the other agents. cppp has important motivating examples, such as planning for organizations that outsource some of their tasks, and has recently received considerable attention from the academic community.},
  archive   = {C_AAMAS},
  author    = {Lev Lehman, Rotem and Shani, Guy and Stern, Roni},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1575–1577},
  title     = {Partial disclosure of private dependencies in privacy preserving planning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464164},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning cooperative solution concepts from voting behavior:
A case study on the israeli knesset. <em>AAMAS</em>, 1572–1574. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most frameworks for computing solution concepts in hedonic games are theoretical in nature, and require complete knowledge of all agent preferences, an impractical assumption in real-world settings. This paper presents the first application of strategic hedonic game models on real-world data. We show that PAC stable solutions can reflect Members of Knesset&#39; political positions and reveal politicians who are known to deviate from party lines. Moreover, these models compare favorably to machine learning models.},
  archive   = {C_AAMAS},
  author    = {Lev, Omer and Lu, Wei and Tsang, Alan and Zick, Yair},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1572–1574},
  title     = {Learning cooperative solution concepts from voting behavior: A case study on the israeli knesset},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464163},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). RPPLNS: Pay-per-last-n-shares with a randomised twist.
<em>AAMAS</em>, 1569–1571. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {&quot;Pay-per-last-N-shares&#39;&#39; (PPLNS) is one of the most common payout strategies used by mining pools in Proof-of-Work (PoW) cryptocurrencies such as Bitcoin. As with any payment scheme, it is imperative to study issues of incentive compatibility of miners within the pool. For PPLNS this question has only been partially answered; we know that reasonably-sized miners within a PPLNS pool prefer following the pool protocol over employing em specific deviations. In this paper, we present a novel modification to PPLNS where we randomise the protocol in a natural way. We call our protocol &quot;Randomised pay-per-last-N-shares&#39;&#39; (RPPLNS), and note that the randomised structure of the protocol greatly simplifies the study of its incentive compatibility. We show that RPPLNS maintains the strengths of PPLNS (i.e., fairness, variance reduction, and resistance to pool hopping), while also being robust against a richer class of strategic mining than what has been shown for PPLNS.},
  archive   = {C_AAMAS},
  author    = {Lazos, Philip and Marmolejo Coss\&#39;{\i}o, Francisco J. and Zhou, Xinyu and Katz, Jonathan},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1569–1571},
  title     = {RPPLNS: Pay-per-last-N-shares with a randomised twist},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464162},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fairness in long-term participatory budgeting.
<em>AAMAS</em>, 1566–1568. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Participatory Budgeting processes are usually designed to span several years, with referenda for new budget allocations taking place regularly. This paper presents the first formalization of long-term PB. We introduce a theory of fairness for this setting, investigate under which conditions our fairness criteria can be satisfied, and analyze the computational complexity of verifying them.},
  archive   = {C_AAMAS},
  author    = {Lackner, Martin and Maly, Jan and Rey, Simon},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1566–1568},
  title     = {Fairness in long-term participatory budgeting},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464161},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On weakly and strongly popular rankings. <em>AAMAS</em>,
1563–1565. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Van Zuylen et al. introduced the notion of a popular ranking in a voting context, where each voter submits a strictly-ordered list of all candidates. A popular ranking pi of the candidates is at least as good as any other ranking sigma in the following sense: if we compare pi to sigma, at least half of all voters will always weakly prefer pi. Whether a voter prefers one ranking to another is calculated based on the Kendall distance.A more traditional definition of popularity---as applied to popular matchings, a well-established topic in computational social choice---is stricter, because it requires at least half of the voters who are not indifferent between pi and sigma to prefer pi. In this paper, we derive structural and algorithmic results in both settings, also improving upon the results by van Zylen et al. We also point out connections to the famous open problem of finding a Kemeny consensus with 3 voters.},
  archive   = {C_AAMAS},
  author    = {Kraiczy, Sonja and Cseh, \&#39;{A}gnes and Manlove, David},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1563–1565},
  title     = {On weakly and strongly popular rankings},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464160},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evaluating the robustness of collaborative agents.
<em>AAMAS</em>, 1560–1562. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Artificial agents trained by deep reinforcement learning will likely encounter novel situations after deployment that were never seen during training. Our agent must be robust to handle such situations well. However, if we cannot rely on the average training or validation reward as a metric, then how can we effectively evaluate robustness? We take inspiration from the practice of unit testing in software engineering. Specifically, we suggest that when designing AI agents that collaborate with humans, designers should search for potential edge cases in possible partner behavior and possible states encountered, and write tests which check that the behavior of the agent in these edge cases is reasonable. We apply this methodology to build a suite of unit tests for the Overcooked-AI environment, and use this test suite to evaluate three proposals for improving robustness. We find that the test suite provides significant insight into the effects of these proposals that were generally not revealed by looking solely at the average validation reward. For our full paper, see https://arxiv.org/abs/2101.05507 arxiv.org/abs/2101.05507},
  archive   = {C_AAMAS},
  author    = {Knott, Paul and Carroll, Micah and Devlin, Sam and Ciosek, Kamil and Hofmann, Katja and Dragan, Anca and Shah, Rohin},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1560–1562},
  title     = {Evaluating the robustness of collaborative agents},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464159},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Solver agent: Towards emotional and opponent-aware agent for
human-robot negotiation. <em>AAMAS</em>, 1557–1559. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Negotiation is one of the crucial processes for resolving conflicts between parties. In automated negotiation, agent designers mostly take opponent&#39;s offers and the remaining time into account while designing their strategies. While designing a negotiating agent interacting with a human directly, other information such as opponent&#39;s emotional changes during the negotiation can establish a better interaction and reach an admissible settlement for joint interests. Accordingly, this paper proposes a bidding strategy for humanoid robots, which incorporates their opponents&#39; emotional states and awareness of the agent&#39;s changing behavior.},
  archive   = {C_AAMAS},
  author    = {Keskin, Mehmet Onur and \c{C}akan, Umut and Aydo\u{g}an, Reyhan},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1557–1559},
  title     = {Solver agent: Towards emotional and opponent-aware agent for human-robot negotiation},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464158},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Coverage control under connectivity constraints.
<em>AAMAS</em>, 1554–1556. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, centralized and decentralized laws for the coverage problem under connectivity constraints are proposed. We formulated the problem as a continuous optimization task with the inequality constraint that algebraic connectivity must not be less than a small positive number, and solved the problem based on the active set method. This formulation can cause agents to be trapped in undesired local minima; to address this, we added the squared distance between the centroid of the coverage area and that of the agent system to the objective function of the coverage control. To derive the decentralized law, we employed an average consensus estimator that determines the algebraic connectivity and system centroid. When the algebraic connectivity is greater than the threshold, the proposed control laws allow agents to advance along the direction of steepest descent of the objective function. Once the connectivity is equal to the threshold, agents maintain the connectivity while decreasing the objective function by moving along the projection vector of the steepest descent in the direction in which the connectivity is constant. Simulation results confirmed the effectiveness of our proposed laws.},
  archive   = {C_AAMAS},
  author    = {Kawajiri, Shota and Hirashima, Kazuki and Shiraishi, Masashi},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1554–1556},
  title     = {Coverage control under connectivity constraints},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464157},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Toward consistent agreement approximation in abstract
argumentation and beyond. <em>AAMAS</em>, 1551–1553. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In cooperative human decision-making, agreements are often not total; a partial degree of agreement is sufficient to commit to a decision and move on, as long as one is somewhat confident that the involved parties are likely to stand by their commitment in the future, given no drastic unexpected changes. In this work, we introduce models that allow autonomous agents to reach such agreements, using abstract argumentation as the underlying model.},
  archive   = {C_AAMAS},
  author    = {Kampik, Timotheus and Nieves, Juan Carlos},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1551–1553},
  title     = {Toward consistent agreement approximation in abstract argumentation and beyond},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464156},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Solving 3D bin packing problem via multimodal deep
reinforcement learning. <em>AAMAS</em>, 1548–1550. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, there is growing attention on applying deep reinforcement learning (DRL) to solve the 3D bin packing problem (3D BPP), given its favorable generalization and independence of ground-truth label. However, due to the relatively less informative yet computationally heavy encoder, and considerably large action space inherent to the 3D BPP, existing methods are only able to handle up to 50 boxes. In this paper, we propose to alleviate this issue via an end-to-end multimodal DRL agent, which sequentially addresses three sub-tasks of sequence, orientation and position, respectively. The resulting architecture enables the agent to solve large-scale instances of 100 boxes or more. Experiments show that the agent could learn highly efficient policies that deliver superior performance against all the baselines on instances of various scales.},
  archive   = {C_AAMAS},
  author    = {Jiang, Yuan and Cao, Zhiguang and Zhang, Jie},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1548–1550},
  title     = {Solving 3D bin packing problem via multimodal deep reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464155},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Strategic abilities of asynchronous agents: Semantic side
effects. <em>AAMAS</em>, 1545–1547. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, we have proposed a framework for verification of agents&#39; abilities in asynchronous multi-agent systems, together with an algorithm for automated reduction of models. The semantics was built on the modeling tradition of distributed systems. As we show here, this can sometimes lead to counterintuitive interpretation of formulas when reasoning about the outcome of strategies.},
  archive   = {C_AAMAS},
  author    = {Jamroga, Wojciech and Penczek, Wojciech and Sidoruk, Teofil},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1545–1547},
  title     = {Strategic abilities of asynchronous agents: Semantic side effects},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464154},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Preserving consistency for liquid knapsack voting.
<em>AAMAS</em>, 1542–1544. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Liquid Democracy (LD) uses transitive delegations in voting. In its simplest form, it is used for binary decisions, however its promise holds also for more advanced voting settings. Here we consider LD in the context of Participatory Budgeting (PB), which is a direct democracy approach to budgeting, most usually done in municipal budgeting processes. In particular, we study Knapsack Voting, in which PB voters approve projects, such that the sum of costs of projects each voter approves must respect the budget limit. We observe possible inconsistencies, as the cost of voter-approved projects may go over the budget limit after resolving delegations. We offer ways to overcome them by studying the computational complexity of updating as few delegations as possible to arrive---after following all project delegations---to a consistent profile.},
  archive   = {C_AAMAS},
  author    = {Jain, Pallavi and Sornat, Krzysztof and Talmon, Nimrod},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1542–1544},
  title     = {Preserving consistency for liquid knapsack voting},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464153},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). We might walk together, but i run faster: Network fairness
and scalability in blockchains. <em>AAMAS</em>, 1539–1541. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Blockchain-based Distributed Ledgers (DLs) promise to transform the existing financial system by making it truly democratic. In the past decade, blockchain technology has seen many novel applications ranging from the banking industry to real estate. However, in order to be adopted universally, blockchain systems must be scalable to support a high volume of transactions. As we increase the throughput of the DL system, the underlying peer-to-peer network might face multiple levels of challenges to keep up with the requirements. Due to varying network capacities, the slower nodes would be at a relative disadvantage compared to the faster ones, which could negatively impact their revenue. In order to quantify their relative advantage or disadvantage, we introduce two measures of network fairness, $p_f$, the probability of frontrunning and α_f$, the publishing fairness. We show that as we scale the blockchain, both these measures deteriorate, implying that the slower nodes face a disadvantage at higher throughputs. It results in the faster nodes getting more than their fair share of the reward while the slower nodes (slow in terms of network quality) get less. Thus, fairness and scalability in blockchain systems do not go hand in hand.In a setting with rational miners, lack of fairness causes miners to deviate from the &quot;longest chain rule&quot; or undercut, which would reduce the blockchain&#39;s resilience against byzantine adversaries. Hence, fairness is not only a desirable property for a blockchain system but also essential for the security of the blockchain and any scalable blockchain protocol proposed must ensure fairness.},
  archive   = {C_AAMAS},
  author    = {Jain, Anurag and Siddiqui, Shoeb and Gujar, Sujit},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1539–1541},
  title     = {We might walk together, but i run faster: Network fairness and scalability in blockchains},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464152},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Balancing rational and other-regarding preferences in
cooperative-competitive environments. <em>AAMAS</em>, 1536–1538. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent reinforcement learning studies extensively explore the interplay between cooperative and competitive behaviour in mixed environments. Unlike cooperative environments where agents strive towards a common goal, mixed environments are notorious for the conflicts of selfish and social interests. As a consequence, purely rational agents often struggle to maintain cooperation. A prevalent approach to induce cooperative behaviour is to assign additional rewards based on other agents&#39; well-being. However, this approach suffers from the issue of multi-agent credit assignment, which can hinder performance. This issue is efficiently alleviated in cooperative setting with such state-of-the-art algorithms as QMIX and COMA. Still, when applied to mixed environments, these algorithms may result in unfair allocation of rewards. We propose BAROCCO, an extension of these algorithms capable to balance individual and social incentives. The mechanism behind BAROCCO is to train two distinct but interwoven components that jointly affect agents&#39; decisions. We experimentally confirm the advantages of BAROCCO.},
  archive   = {C_AAMAS},
  author    = {Ivanov, Dmitry and Egorov, Vladimir and Shpilman, Aleksei},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1536–1538},
  title     = {Balancing rational and other-regarding preferences in cooperative-competitive environments},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464151},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Approximating spatial evolutionary games using bayesian
networks. <em>AAMAS</em>, 1533–1535. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Evolutionary Game Theory is an application of game theory to evolving populations of organisms. Of recent interest are EGT models situated on structured populations or spatial evolutionary games. Due to the complexity added by introducing a population structure, model analysis is usually performed through agent-based Monte-Carlo simulations. However, it can be difficult to obtain desired quantities of interest from these simulations due to stochastic effects. We define a framework for modeling spatial evolutionary games using Dynamic Bayesian Networks that capture the underlying stochastic process. The resulting Dynamic Bayesian Networks can be queried for quantities of interest by performing exact inference on the network. We then propose a method for producing approximations of the spatial evolutionary game through the truncation of the corresponding DBN, taking advantage of the high symmetry of the model. This method generalizes mean-field and pair approximations in the literature for spatial evolutionary games. Furthermore, we show empirical results demonstrating the capability of the method to obtain much better accuracy than pair approximation with respect to stochastic simulations.},
  archive   = {C_AAMAS},
  author    = {Hsiao, Vincent and Pan, Xinyue and Nau, Dana and Dechter, Rina},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1533–1535},
  title     = {Approximating spatial evolutionary games using bayesian networks},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464150},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Distributional monte carlo tree search for risk-aware and
multi-objective reinforcement learning. <em>AAMAS</em>, 1530–1532. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many risk-aware and multi-objective reinforcement learning settings, the utility of the user is derived from the single execution of a policy. In these settings, making decisions based on the average future returns is not suitable. For example, in a medical setting a patient may only have one opportunity to treat their illness. When making a decision, just the expected return -- known in reinforcement learning as the value -- cannot account for the potential range of adverse or positive outcomes a decision may have. Our key insight is that we should use the distribution over expected future returns differently to represent the critical information that the agent requires at decision time.In this paper, we propose Distributional Monte Carlo Tree Search, an algorithm that learns a posterior distribution over the utility of the different possible returns attainable from individual policy executions, resulting in good policies for risk-aware settings. Moreover, our algorithm outperforms the state-of-the-art in multi-objective reinforcement learning for the expected utility of the returns.},
  archive   = {C_AAMAS},
  author    = {Hayes, Conor F. and Reymond, Mathieu and Roijers, Diederik M. and Howley, Enda and Mannion, Patrick},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1530–1532},
  title     = {Distributional monte carlo tree search for risk-aware and multi-objective reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464149},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Simultaneous learning of moving and active perceptual
policies for autonomous robot. <em>AAMAS</em>, 1527–1529. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans can move their bodies and eyes actively to perceive the state of the environment they are surrounded by. Autonomous robots are needed to learn this ability, so called active perception, to behave as humans do. In this paper, we propose a reinforcement learning algorithm to make the robots have the perceptual ability. In our algorithm, we simultaneously train two agents which control the robot and its sensor on the robot to achieve a task. We conducted experiments on navigation tasks in a 3D environment where useful information for the task achievement is partially occluded. The experimental results show that our algorithm can obtain better perceptual behavior and achieve higher success rates than conventional reinforcement learning algorithms.},
  archive   = {C_AAMAS},
  author    = {Hatanaka, Wataru and Sasaki, Fumihiro and Yamashina, Ryota and Kawaguchi, Atsuo},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1527–1529},
  title     = {Simultaneous learning of moving and active perceptual policies for autonomous robot},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464148},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Teaching unknown learners to classify via feature
importance. <em>AAMAS</em>, 1524–1526. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work we introduce an interactive machine teaching approach that teaches a classification task to the learner. Our adaptive approach - Feature Importance Teaching (FIT) - does not assume perfect knowledge about the learner, as most machine teaching approaches do. It chooses, online, which sample to show next, as it updates the learner&#39;s model based on feedback from the student on the weights attributed to the features. We present simulated results where the student has a different prior knowledge from the one assumed by the teacher. The results have shown that our teaching approach can mitigate this mismatch and lead to a significantly faster learning curve than the ones obtained in conditions where the teacher randomly selects the samples or does not consider this feedback from the student.},
  archive   = {C_AAMAS},
  author    = {Guerra, Carla and Melo, Francisco S. and Lopes, Manuel},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1524–1526},
  title     = {Teaching unknown learners to classify via feature importance},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464147},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sequential and swap mechanisms for public housing allocation
with quotas and neighbourhood-based utilities. <em>AAMAS</em>,
1521–1523. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the problem of allocating indivisible items to agents where both agents and items are partitioned into disjoint groups. Following previous works on public housing allocation, each item (or house) belongs to a block and each agent is assigned a type. The allocation problem consists in assigning at most one item to each agent in a good way while respecting diversity constraints. Based on Schelling&#39;s seminal work, we introduce a generic individual utility function where the welfare of an agent not only relies on her preferences over the items but also takes into account the fraction of agents of her own type in her own block. In this context, we investigate the issue of stability, and study two existing allocation mechanisms: a sequential mechanism used in Singapore and a distributed procedure based on mutually improving swaps of items.},
  archive   = {C_AAMAS},
  author    = {Gross-Humbert, Nathana\&quot;{e}l and Benabbou, Nawal and Beynier, Aur\&#39;{e}lie and Maudet, Nicolas},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1521–1523},
  title     = {Sequential and swap mechanisms for public housing allocation with quotas and neighbourhood-based utilities},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464146},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rank aggregation by dissatisfaction minimisation in the
unavailable candidate model. <em>AAMAS</em>, 1518–1520. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we extend the unavailable candidate model (Tyler Lu and Craig Boutilier, The Unavailable Candidate Model : a Decision-Theoretic View of Social Choice, 2010) and present two new voting rules based on a finer notion of disagreement, called dissatisfaction, which depends on the ranks of the candidates, considered among all the candidates ( ante dissatisfaction rule) or only among the available candidates ( post dissatisfaction rule). We provide algorithmic results for the two rules and show that apparently very different voting rules such as scoring rules or Kemeny rule can be unified under the same aggregation concept: expectation of dissatisfaction under the availability distribution.},
  archive   = {C_AAMAS},
  author    = {Grivet S\&#39;{e}bert, Arnaud and Maudet, Nicolas and Perny, Patrice and Viappiani, Paolo},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1518–1520},
  title     = {Rank aggregation by dissatisfaction minimisation in the unavailable candidate model},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464145},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A global multi-sided market with ascending-price mechanism.
<em>AAMAS</em>, 1515–1517. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an ascending-price mechanism for a multi-sided market with a variety of participants, such as manufacturers, logistics agents, insurance providers, and assemblers. Each deal in the market may consist of a combination of agents from separate categories, and different such combinations are simultaneously allowed. This flexibility lets multiple intersecting markets be resolved as a single global market. Our mechanism is obviously-truthful, strongly budget-balanced, individually rational, and attains almost the optimal gain-from-trade when the market is sufficiently large.},
  archive   = {C_AAMAS},
  author    = {Gonen, Rica and Segal-Halevi, Erel},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1515–1517},
  title     = {A global multi-sided market with ascending-price mechanism},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464144},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards decentralized social reinforcement learning via
ego-network extrapolation. <em>AAMAS</em>, 1512–1514. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we consider the problem of multi-agent reinforcement learning in directed social networks with a large number of agents. Network dependencies among user activities impact the reward for individual actions and need to be incorporated into policy learning, however, directed interactions entail that the network is partially observable to each user. When estimating policies locally, the insufficient state information makes it challenging for users to effectively learn network dependencies. To address this, we use parameter sharing and ego-network extrapolation in a decentralized policy learning and execution framework. This is in contrast to previous work on social RL that assumes a centralized controller to capture inter-agent dependencies for joint policy learning. We evaluate our proposed approach on Twitter datasets and show that our decentralized learning approach achieves performance nearly equivalent to that of centralized learning approach and superior performance to other baselines.},
  archive   = {C_AAMAS},
  author    = {Goindani, Mahak and Neville, Jennifer},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1512–1514},
  title     = {Towards decentralized social reinforcement learning via ego-network extrapolation},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464143},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Comparison of desynchronization methods for a decentralized
swarm on a logistical resupply problem. <em>AAMAS</em>, 1510–1511. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we investigate the impact of two approaches to agent desynchronization on task allocation in decentralized swarms: variation in response threshold and variation in response duration. We focus on swarms consisting of simple threshold-based stimulus response agents and examine how they respond to dynamically changing task demands in a logistics re-supply problem.Ideally, a swarm will be able to divide the labor of its workers appropriately such that task demands are satisfied in a timely manner. In general, agent resources should be conserved to minimize waste resulting from overdelivery. Because task switching may incur physical or time costs, stable distributions of agents are desirable. We predict that agent performance will be more consistent when agent desynchronization is highest and that the need for sufficient desynchronization is more critical for more difficult problems. To provide grounds for this investigation, we examine primary effects for each method of desynchronization in combination with different types of logistic schedules on agent performance.},
  archive   = {C_AAMAS},
  author    = {Giordano, Joseph P. and Wu, Annie S. and Pherwani, Arjun and Mathias, H. David},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1510–1511},
  title     = {Comparison of desynchronization methods for a decentralized swarm on a logistical resupply problem},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464142},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Shielding atari games with bounded prescience.
<em>AAMAS</em>, 1507–1509. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present the first explicit-state method for analysing and ensuring the safety of DRL agents for Atari games. Our method only requires access to the emulator. We give a suite of 42 properties that characterise &quot;safe behaviour&quot; for 31 games. We evaluate the safety of the best available DRL agents which, as our experiments show, violate most of our properties. We propose a countermeasure that implements shielding using bounded explicit-state exploration. Our method improved their overall safety, producing the safest DRL agents for Atari games currently available.},
  archive   = {C_AAMAS},
  author    = {Giacobbe, Mirco and Hasanbeig, Mohammadhosein and Kroening, Daniel and Wijk, Hjalmar},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1507–1509},
  title     = {Shielding atari games with bounded prescience},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464141},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards a competence-based approach to allocate teams to
tasks. <em>AAMAS</em>, 1504–1506. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We tackle the problem of allocating teams to tasks. For that, we propose a novel allocation problem that matches teams with tasks based on the similarity between the competencies required by the tasks and the competencies offered by teams. We formally cast our problem as an optimisation problem, characterise the size of its search space and outline a heuristic approach to solve it.},
  archive   = {C_AAMAS},
  author    = {Georgara, Athina and Rodr\&#39;{\i}guez-Aguilar, Juan A. and Sierra, Carles},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1504–1506},
  title     = {Towards a competence-based approach to allocate teams to tasks},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464140},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Pick your battles: Interaction graphs as population-level
objectives for strategic diversity. <em>AAMAS</em>, 1501–1503. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Strategic diversity is often essential in games: in multi-player games, for example, evaluating a player against a diverse set of strategies will yield a more accurate estimate of its performance. Furthermore, in games with non-transitivities diversity allows a player to cover several winning strategies. However, despite the significance of strategic diversity, training agents that exhibit diverse behaviour remains a challenge. In this paper we study how to construct diverse populations of agents by carefully structuring how individuals within a population interact. Our approach is based on interaction graphs, which control the flow of information between agents during training and can encourage agents to specialise on different strategies, leading to improved overall performance. We provide evidence for the importance of diversity in multi-agent training and analyse the effect of applying different interaction graphs on the training trajectories, diversity and performance of populations in a range of games.},
  archive   = {C_AAMAS},
  author    = {Garnelo, Marta and Czarnecki, Wojciech Marian and Liu, Siqi and Tirumala, Dhruva and Oh, Junhyuk and Gidel, Gauthier and van Hasselt, Hado and Balduzzi, David},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1501–1503},
  title     = {Pick your battles: Interaction graphs as population-level objectives for strategic diversity},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464139},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Partially cooperative multi-agent periodic indivisible
resource allocation. <em>AAMAS</em>, 1498–1500. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Asymmetric distributed constraint optimization problems (ADCOPs) in which agents are partially cooperative, is a model for representing multi-agent optimization problems in which agents, are willing to cooperate in order to achieve a global goal, as long as some minimal threshold on their personal utility is satisfied.We contribute by: 1) extending the ADCOP model to represent resource allocation problems in which indivisible resources are periodically allocated, e.g., meeting rooms, operating rooms, etc. 2) adjusting partially cooperative local search algorithms to solve problems represented by the extended model. 3) presenting an implementation of a realistic problem that is represented by the proposed model, and empirical evidence of the compatibility of partially cooperative algorithms for this scenario.},
  archive   = {C_AAMAS},
  author    = {Gabai Schlosberg, Yuval and Zivan, Roie},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1498–1500},
  title     = {Partially cooperative multi-agent periodic indivisible resource allocation},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464138},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). It’s a match! Gesture generation using expressive parameter
matching. <em>AAMAS</em>, 1495–1497. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automatic gesture generation from speech generally relies on implicit modelling of the nondeterministic speech-gesture relationship and can result in averaged motion lacking defined form. Here, we propose a database-driven approach of selecting gestures based on specific motion characteristics that have been shown to be associated with the speech audio. We extend previous work that identified expressive parameters of gesture motion that can both be predicted from speech and are perceptually important for a good speech-gesture match, such as gesture velocity and finger extension. A perceptual study was performed to evaluate the appropriateness of the gestures selected with our method. We compare our method with two baseline selection methods. The first respects timing, the desired onset and duration of a gesture, but does not match gesture form in other ways. The second baseline additionally disregards the original gesture timing for selecting gestures. The gesture sequences from our method were rated as a significantly better match to the speech than gestures selected by either baseline method.},
  archive   = {C_AAMAS},
  author    = {Ferstl, Ylva and Neff, Michael and McDonnell, Rachel},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1495–1497},
  title     = {It&#39;s a match! gesture generation using expressive parameter matching},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464137},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A multi-arm bandit approach to subset selection under
constraints. <em>AAMAS</em>, 1492–1494. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We explore the class of problems where a central planner needs to select a subset of agents, each with different quality and cost. The planner wants to maximize its utility while ensuring that the average quality of the selected agents is above a certain threshold. When the agents&#39; quality is known, we formulate our problem as an integer linear program (ILP) and propose a deterministic algorithm, namely dpss that provides an exact solution to our ILP.We then consider the setting when the qualities of the agents are unknown. We model this as a Multi-Arm Bandit (MAB) problem and propose newalgo to learn the qualities over multiple rounds. We show that after a certain number of rounds, τ, newalgo outputs a subset of agents that satisfy the average quality constraint with a high probability. Next, we provide bounds on τ and prove that after τ rounds, the algorithm incurs a regret of $O(\l{}n T)$, where T is the total number of rounds. We further illustrate the efficacy of newalgo through simulations. To overcome the computational limitations of dpss, we propose a polynomial-time greedy algorithm, namely greedy, that provides an approximate solution to our ILP. We also compare the performance of dpss and greedy through experiments.},
  archive   = {C_AAMAS},
  author    = {Deva, Ayush and Abhishek, Kumar and Gujar, Sujit},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1492–1494},
  title     = {A multi-arm bandit approach to subset selection under constraints},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464136},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A generic multi-agent model for resource allocation
strategies in online on-demand transport with autonomous vehicles.
<em>AAMAS</em>, 1489–1491. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The introduction of driver-less technologies can improve on-demand transport (ODT) systems and help make passenger transportation and logistics more efficient. Here, we aim to provide a generic model of the online ODT with autonomous vehicles problem and a multi-agent model specific to resource allocation and scheduling in vehicle fleets. Our model considers autonomous vehicles that communicate via peer-to-peer radio channels to meet passenger requirements and satisfy trip requests in an online ODT system. We experiment this model with several allocation mechanisms (mathematical programming, greedy heuristic, distributed constraint optimization, and auctions) and compare their performance on synthetic scenarios on a real-world city road network.},
  archive   = {C_AAMAS},
  author    = {Daoud, Alaa and Balbo, Flavien and Gianessi, Paolo and Picard, Gauthier},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1489–1491},
  title     = {A generic multi-agent model for resource allocation strategies in online on-demand transport with autonomous vehicles},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464135},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Stratified experience replay: Correcting multiplicity bias
in off-policy reinforcement learning. <em>AAMAS</em>, 1486–1488. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep Reinforcement Learning (RL) methods rely on experience replay to approximate the minibatched supervised learning setting; however, unlike supervised learning where access to lots of training data is crucial to generalization, replay-based deep RL appears to struggle in the presence of extraneous data. Recent works have shown that the performance of Deep Q-Network (DQN) degrades when its replay memory becomes too large.This suggests that outdated experiences somehow impact the performance of deep RL, which should not be the case for off-policy methods like DQN. Consequently, we re-examine the motivation for sampling uniformly over a replay memory, and find that it may be flawed when using function approximation. We show that -- despite conventional wisdom -- sampling from the uniform distribution does not yield uncorrelated training samples and therefore biases gradients during training. Our theory prescribes a special non-uniform distribution to cancel this effect, and we propose a stratified sampling scheme to efficiently implement it.},
  archive   = {C_AAMAS},
  author    = {Daley, Brett and Hickert, Cameron and Amato, Christopher},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1486–1488},
  title     = {Stratified experience replay: Correcting multiplicity bias in off-policy reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464134},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A logic of inferable in multi-agent systems with budget and
costs. <em>AAMAS</em>, 1483–1485. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In AI, Multi-Agent Systems are able to model many kind of collective behavior and have therefore a wide range of application. In this paper, we propose a logical framework (Logic of &quot;Inferable&quot;) which enable reasoning about whether a group of agents can perform an action, highlighting the concepts of cost of actions and of budget that agents have available to perform actions. The focus is on modeling the group dynamics of cooperative agents.},
  archive   = {C_AAMAS},
  author    = {Costantini, Stefania and Formisano, Andrea and Pitoni, Valentina},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1483–1485},
  title     = {A logic of inferable in multi-agent systems with budget and costs},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464133},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Promoting fair proposers, fair responders or both?
Cost-efficient interference in the spatial ultimatum game.
<em>AAMAS</em>, 1480–1482. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Institutions and investors face the constant challenge of making accurate decisions and predictions regarding how best they should distribute their endowments. The problem of achieving an optimal outcome at minimal cost has been extensively studied and resolved using several heuristics. However, these works fail to address how an external decision maker can target different types of fair behaviour and how limited information can shape this complex interplay. Here, we consider the well-known Ultimatum game in a spatial setting and propose a hierarchy of interference mechanisms based on the amount of information available to the external decision maker and desired standards of fairness. Our key findings show that asymmetric interactions have drastically different dynamics when compared to symmetric games, such as the Prisoner&#39;s Dilemma, and discuss why gathering information about the agents&#39; behaviour allows for the most efficient investment strategies.},
  archive   = {C_AAMAS},
  author    = {Cimpeanu, Theodor and Perret, Cedric and Han, The Anh},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1480–1482},
  title     = {Promoting fair proposers, fair responders or both? cost-efficient interference in the spatial ultimatum game},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464132},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning to cooperate with unseen agents through
meta-reinforcement learning. <em>AAMAS</em>, 1478–1479. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ad hoc teamwork problem describes situations where an agent has to cooperate with previously unseen agents to achieve a common goal. For an agent to be successful in these scenarios, it has to have cooperative skills. One could implement cooperative skills into an agent by using domain knowledge (e.g., goals, roles, and protocols) to design the agent&#39;s behaviours. However, in complex domains, domain knowledge might not be available. Therefore, it is interesting to explore how to directly learn cooperative skills from data. In this work, we apply meta-reinforcement learning (meta-RL) formulation in the context of ad hoc teamwork problem. Our experiments show that such a method could produce cooperative agents in two cooperative environments with different cooperative circumstances.},
  archive   = {C_AAMAS},
  author    = {Charakorn, Rujikorn and Manoonpong, Poramate and Dilokthanakul, Nat},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1478–1479},
  title     = {Learning to cooperate with unseen agents through meta-reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464131},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Difference rewards policy gradients. <em>AAMAS</em>,
1475–1477. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Policy gradient methods have become one of the most popular classes of algorithms for multi-agent reinforcement learning. A key challenge, however, that is not addressed by many of these methods is multi-agent credit assignment: assessing an agent&#39;s contribution to the overall performance, which is crucial for learning good policies. We propose a novel algorithm called Dr.Reinforce that explicitly tackles this by combining difference rewards with policy gradients to allow for learning decentralized policies when the reward function is known. By differencing the reward function directly, Dr.Reinforce avoids difficulties associated with learning the Q-function as done by Counterfactual Multiagent Policy Gradients (COMA), a state-of-the-art difference rewards method. For applications where the reward function is unknown, we show the effectiveness of a version of Dr.Reinforce that learns a reward network that is used to estimate the difference rewards.},
  archive   = {C_AAMAS},
  author    = {Castellini, Jacopo and Devlin, Sam and Oliehoek, Frans A. and Savani, Rahul},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1475–1477},
  title     = {Difference rewards policy gradients},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464130},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the sensory commutativity of action sequences for
embodied agents. <em>AAMAS</em>, 1472–1474. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study perception in the scenario of an embodied agent equipped with first-person sensors and a continuous motor space with multiple degrees of freedom. We consider the commutative properties of action sequences with respect to sensory information perceived by such an embodied agent. We introduce the Sensory Commutativity Probability (SCP) criterion which measures how much an agent&#39;s degree of freedom affects the environment in embodied scenarios. We show how to compute this criterion in different environments, including realistic robotic setups and discuss how SCP and the commutative properties of action sequences can be used to learn about objects in the environment and improve sample-efficiency in Reinforcement Learning.},
  archive   = {C_AAMAS},
  author    = {Caselles-Dupr\&#39;{e}, Hugo and Garcia-Ortiz, Michael and Filliat, David},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1472–1474},
  title     = {On the sensory commutativity of action sequences for embodied agents},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464129},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CHARET: Character-centered approach to emotion tracking in
stories. <em>AAMAS</em>, 1469–1471. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous agents that can engage in social interactions with a human is the ultimate goal of a myriad of applications. A key challenge in the design of these applications is to define the social behavior of the agent, which requires extensive content creation. In this research, we explore how we can leverage current state-of-the-art tools to make inferences about the emotional state of a character in a story as events unfold, in a coherent way. We propose a character role-labelling approach to emotion tracking that accounts for the semantics of emotions. We show that, by identifying actors and objects of events and considering the emotional state of the characters, we can achieve better performance in this task when compared to end-to-end approaches.},
  archive   = {C_AAMAS},
  author    = {Carvalho, Diogo S. and Campos, Joana and Guimar\~{a}es, Manuel and Antunes, Ana and Dias, Jo\~{a}o and Santos, Pedro A.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1469–1471},
  title     = {CHARET: Character-centered approach to emotion tracking in stories},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464128},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning index policies for restless bandits with
application to maternal healthcare. <em>AAMAS</em>, 1467–1468. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many community health settings, it is crucial to have a systematic monitoring and intervention process to ensure that the patients adhere to healthcare programs, such as periodic health checks or taking medications. When these interventions are expensive, they can be provided to only a fixed small fraction of the patients at any period of time. Hence, it is important to carefully choose the beneficiaries who should be provided with interventions and when. We model this scenario as a restless multi-armed bandit (RMAB) problem, where each beneficiary is assumed to transition from one state to another depending on the intervention provided to them. In practice, the transition probabilities are unknown a priori, and hence, we propose a mechanism for the problem of balancing the explore-exploit trade-off. Empirically, we find that our proposed mechanism outperforms the baseline intervention scheme maternal healthcare dataset.},
  archive   = {C_AAMAS},
  author    = {Biswas, Arpita and Aggarwal, Gaurav and Varakantham, Pradeep and Tambe, Milind},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1467–1468},
  title     = {Learning index policies for restless bandits with application to maternal healthcare},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464127},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). How to guide a non-cooperative learner to cooperate:
Exploiting no-regret algorithms in system design. <em>AAMAS</em>,
1464–1466. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate a repeated two-player game setting where the column player is also a designer of the system, and has full control over payoff matrices. In addition, we assume that the row player uses a no-regret algorithm to efficiently learn how to adapt their strategy to the column player&#39;s behaviour over time. The goal of the column player is to guide her opponent into picking a mixed strategy which is preferred by the system designer. Therefore, she needs to: (i) design appropriate payoffs for both players; and (ii) strategically interact with the row player during a sequence of plays in order to guide her opponent to converge to the desired mixed strategy. To design appropriate payoffs, we propose a novel zero-sum game construction whose unique minimax solution contains the desired behaviour. We also propose another construction in which only the minimax strategy of the row player is unique. Finally, we propose a new game playing algorithm for the system designer and show that it can guide the row player to its minimax strategy, under the assumption that the row player adopts a stable no-regret algorithm.},
  archive   = {C_AAMAS},
  author    = {Bishop, Nicholas and Dinh, Le Cong and Tran-Thanh, Long},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1464–1466},
  title     = {How to guide a non-cooperative learner to cooperate: Exploiting no-regret algorithms in system design},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464126},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Maximizing influence-based group shapley centrality.
<em>AAMAS</em>, 1461–1463. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A key problem in network analysis is the influence maximization problem, which consists of finding a set S of at most k seed users in a social network, such that the spread of information from S is maximized. We investigate the problem of choosing the best set of seeds when there exists an unknown pre-existing set of seed nodes. Our work extends the one of Chen and Teng (WWW&#39;17) who introduced the so-called Shapley centrality of a node to measure the efficiency of nodes acting as seeds within a pre-existing but unknown set of seeds. We instead consider the question: Which set of cardinality k to target in this kind of scenario? The resulting optimization problem reveals very challenging, that is, assuming common computational complexity conjectures, we obtain strong hardness of approximation results. Nevertheless, we design a greedy algorithm which achieves an approximation factor of 1-1/ek -ε for any ε &amp;gt; 0, showing that not all is lost in settings where k is bounded.},
  archive   = {C_AAMAS},
  author    = {Becker, Ruben and D&#39;Angelo, Gianlorenzo and Gilbert, Hugo},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1461–1463},
  title     = {Maximizing influence-based group shapley centrality},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464125},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Image sequence understanding through narrative sensemaking.
<em>AAMAS</em>, 1458–1460. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For intelligent conversational agents to speak about albums of images with users as humans do, they must be able to make sense of images as humans do. Computer vision methods can report directly observable information, but human beings care about more than the directly observable; they value holistic narratives that include affective and motivational evaluations, casual connections, and other inferred relationships from external knowledge. Drawing from theories in cognitive sensemaking and narrative coherence, we propose an approach for image sequence understanding that strives to generate and evaluate hypotheses about the relationships between people, events, and objects in images using commonsense knowledge, which are formed into a consistent network of hypotheses and observed facts via multi-objective optimization. The result is an enriched knowledge representation in the form of a knowledge graph which may later be used by a conversational agent.},
  archive   = {C_AAMAS},
  author    = {Battad, Zev and Si, Mei},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1458–1460},
  title     = {Image sequence understanding through narrative sensemaking},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464124},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modelling cooperation in network games with spatio-temporal
complexity. <em>AAMAS</em>, 1455–1457. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The real world is awash with multi-agent problems that require collective action by self-interested agents, from the routing of packets across a computer network to the management of irrigation systems. Such systems have local incentives for individuals, whose behavior has an impact on the global outcome for the group. Given appropriate mechanisms describing agent interaction, groups may achieve socially beneficial outcomes, even in the face of short-term selfish incentives. In many cases, collective action problems possess an underlying graph structure, whose topology crucially determines the relationship between local decisions and emergent global effects. Such scenarios have received great attention through the lens of network games. However, this abstraction typically collapses important dimensions, such as geometry and time, relevant to the design of mechanisms promoting cooperation. In parallel work, multi-agent deep reinforcement learning has shown great promise in modelling the emergence of self-organized cooperation in complex gridworld domains. Here we apply this paradigm in graph-structured collective action problems.},
  archive   = {C_AAMAS},
  author    = {Bakker, Michiel A. and Everett, Richard and Weidinger, Laura and Gabriel, Iason and Isaac, William S. and Leibo, Joel Z. and Hughes, Edward},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1455–1457},
  title     = {Modelling cooperation in network games with spatio-temporal complexity},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464123},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quantifying human perception with multi-armed bandits.
<em>AAMAS</em>, 1452–1454. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study a variant of the continuous multi-armed bandits problem, where the objective is to estimate the sensitivity threshold for an unknown psychometric function ¶si. This setting models the conduct of a psychometric experiment, which aims at quantifying human perception. We show that this setting is akin to hierarchical multi-armed bandits and Black-box optimization of noisy functions, with both significant similarities and key differences. We introduce a new algorithm, DOS, for Dichotomous Optimistic Search, that efficiently solves this task, and show that DOS outperforms recent methods from both Psychophysics and Global Optimization for non Gaussian Psychometric functions in our experiments.},
  archive   = {C_AAMAS},
  author    = {Audiffren, Julien},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1452–1454},
  title     = {Quantifying human perception with multi-armed bandits},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464122},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interpretive blindness and the impossibility of learning
from testimony. <em>AAMAS</em>, 1449–1451. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We model interpretive blindness, a type of epistemic bias that poses a problem for learning from testimony, in which one acquires information from text or conversation but lacks direct access to ground truth. Interpretive blindness arises when a co-dependence between background beliefs and interpretation leads to a dynamic process of bias hardening that impedes or precludes learning. We argue that when bodies of data are argumentatively complete, even constraints from hierarchical Bayesian learning designed to promote good epistemic practices will fail to stop interpretive blindness.},
  archive   = {C_AAMAS},
  author    = {Asher, Nicholas and Hunter, Julie},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1449–1451},
  title     = {Interpretive blindness and the impossibility of learning from testimony},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464121},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning competitive equilibria in noisy combinatorial
markets. <em>AAMAS</em>, 1446–1448. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a methodology to robustly estimate the competitive equilibria (CE) of combinatorial markets under the assumption that buyers do not know their precise valuations for bundles of goods, but instead can only provide noisy estimates. We first show tight lower- and upper-bounds on the buyers&#39; utility loss, and hence the set of CE, given a uniform approximation of one market by another. We then present two probably-approximately-correct algorithms for learning CE with finite-sample guarantees. The first is a baseline and the second leverages a connection between the first welfare theorem of economics and uniform approximations to adaptively prune value queries when it is determined that they are provably not part of a CE. Extensive experimentation shows that pruning achieves better estimates than the baseline with far fewer samples.},
  archive   = {C_AAMAS},
  author    = {Areyan Viqueira, Enrique and Cousins, Cyrus and Greenwald, Amy},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1446–1448},
  title     = {Learning competitive equilibria in noisy combinatorial markets},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464120},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). How to amend a constitution? Model, axioms, and
supermajority rules. <em>AAMAS</em>, 1443–1445. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A self-governed society must have decision rules by which group decisions are made, and these rules are often codified in a written constitution. One of the defining features of a constitution is its degree of entrenchment, or how hard it is to change by amendment. If it is too easy to make amendments, then the constitution can change too frequently, leading to chaos. On the other hand, if it is too hard to make amendments, then this can also be destabilizing, as voters may begin to see the rules as less legitimate, or even seek to overturn the status quo in a revolt. As norms, priorities, and circumstances change over time and over generations a constitution must be able to adapt. Our work considers a stylized model of constitutions that use reality-aware supermajority rules to make decisions. We propose principles for designing amendment procedures for changing decision rules in these constitutions and propose a novel procedure based on these principles.},
  archive   = {C_AAMAS},
  author    = {Abramowitz, Ben and Shapiro, Ehud and Talmon, Nimrod},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1443–1445},
  title     = {How to amend a constitution? model, axioms, and supermajority rules},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464119},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Evolution of strategies in sequential security games.
<em>AAMAS</em>, 1434–1442. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we introduce a generic approach to solving Sequential Security Games (SGs) based on Evolutionary Algorithms. The method (named EASG) is general and largely game-independent, which allows for its application to a wide range of SGs with only slight adjustments. The efficacy of EASG is verified on 3 types of games from different domains: patrolling, surveillance, and cybersecurity. Comprehensive experiments performed on more than $300$ test games demonstrate robustness and stability of EASG, manifested by repetitively achieving optimal or near-optimal solutions. The main advantage of EASG compared to alternative approaches to solving SGs is its time efficiency. EASG finds high-quality solutions with approximately linear time scalability and can therefore be applied to solving SG instances which are beyond capabilities of the state-of-the-art exact methods. Due to anytime characteristics, EASG is particularly well suited for time-critical applications.},
  archive   = {C_AAMAS},
  author    = {\.{Z}ychowski, Adam and Ma\&#39;{n}dziuk, Jacek},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1434–1442},
  title     = {Evolution of strategies in sequential security games},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464117},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A computational model of coping for simulating human
behavior in high-stress situations. <em>AAMAS</em>, 1425–1433. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {People often encounter high-stress situations. Modeling and being able to predict people&#39;s behavior in such situations, how they cope, is a critical research topic. To that end, we propose a computational model of coping that casts Lazarus&#39; theory of coping into a Partial Observable Markov Decision Process (POMDP) framework. This includes an appraisal process that models the factors that lead to stress by assessing a person&#39;s relation to the environment and a coping process that models people&#39;s behavior in the face of such stress. This coping process includes problem-focused coping, whereby people seek to alter the external environment, and emotion-focused coping, whereby people alter their internal beliefs, goals, and intentions in the face of stress. We evaluate the model&#39;s assumptions and predictions in the context of a high-stress situation that is increasingly common, the extreme conditions of a hurricane. We collected human survey data from the last several years of major U.S. hurricanes to evaluate the features in the models used for appraisal calculation. Additionally, we conducted a controlled human-subject experiment simulating a hurricane experience to investigate the prediction of the model on how people change their beliefs and goals to cope with the situation. The results show that, as predicted by the model, the proposed model features are significantly associated with the evacuation decisions and post-decision people also change their beliefs and goals in the directions that align with their prior decisions. Lastly, we conduct a simulation study showing that the proposed model is qualitatively closer to the experiment data than the baseline models that do not incorporate coping effects.},
  archive   = {C_AAMAS},
  author    = {Yongsatianchot, Nutchanon and Marsella, Stacy},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1425–1433},
  title     = {A computational model of coping for simulating human behavior in high-stress situations},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464116},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Intention progression using quantitative summary
information. <em>AAMAS</em>, 1416–1424. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A key problem for Belief-Desire-Intention (BDI) agents is intention progression, i.e., which plans should be selected and how the execution of these plans should be interleaved so as to achieve the agent&#39;s goals. Monte-Carlo Tree Search (MCTS) has been shown to be a promising approach to the intention progression problem, out-performing other approaches in the literature. However, MCTS relies on runtime simulation of possible interleavings of the plans in each intention, which may be computationally costly. In this paper, we introduce the notion of quantitative summary information which can be used to estimate the likelihood of conflicts between an agent&#39;s intentions. We show how offline simulation can be used to precompute quantitative summary information prior to execution of the agent&#39;s program, and how the precomputed summary information can be used at runtime to guide the expansion of the MCTS search tree and avoid unnecessary runtime simulation. We compare the performance of our approach with standard MCTS in a range of scenarios of increasing difficulty. The results suggest our approach can significantly improve the efficiency of MCTS in terms of the number of runtime simulations performed.},
  archive   = {C_AAMAS},
  author    = {Yao, Yuan and Alechina, Natasha and Logan, Brian and Thangarajah, John},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1416–1424},
  title     = {Intention progression using quantitative summary information},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464115},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Drone formation control via belief-correlated imitation
learning. <em>AAMAS</em>, 1407–1415. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The proliferation of unmanned aerial vehicles (UAVs) has flourished various intelligent services, in which the effective coordination plays a significant role in enhancing swarm execution efficiency. However, due to the unreliable communication in the air as well as the heterogeneity in operation mode, it is challenging to achieve highly coordinated actions, particularly in the fully distributed environment with incomplete observations. In this paper, we leverage the generative adversarial imitation learning (GAIL) technique to coordinate the drones&#39; actions by directly imitating the peer&#39;s demonstrations. In order to characterize the true environment state under local incomplete observations, we transform historical observation-action trajectories into belief representations, which are trained in conjunction with the imitation policies. We also gain regularized belief representations by correlating the prediction of future states, the trace of historical contexts, and the action-assisted guidance information, which contribute to more accurate imitation policies. We evaluate the proposed algorithm on the drones&#39; formation control scenario. Evaluation results show the superiorities on imitation accuracy, teamwork execution time and energy cost.},
  archive   = {C_AAMAS},
  author    = {Yang, Bo and Ma, Chaofan and Xia, Xiaofang},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1407–1415},
  title     = {Drone formation control via belief-correlated imitation learning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464114},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Transferable environment poisoning: Training-time attack on
reinforcement learning. <em>AAMAS</em>, 1398–1406. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Studying adversarial attacks on Reinforcement Learning (RL) agents has become a key aspect of developing robust, RL-based solutions. Test-time attacks, which target the post-learning performance of an RL agent&#39;s policy, have been well studied in both white- and black-box settings. More recently, however, state-of-the-art works have shifted to investigate training-time attacks on RL agents, i.e., forcing the learning process towards a target policy designed by the attacker. Alas, these SOTA works continue to rely on white-box settings and/or use a reward-poisoning approach. In contrast, this paper studies environment-dynamics poisoning attacks at training time. Furthermore, while environment-dynamics poisoning presumes a transfer-learning capable agent, it also allows us to expand our approach to black-box attacks. Our overall framework, inspired by hierarchical RL, seeks the minimal environment-dynamics manipulation that will prompt the momentary policy of the agent to change in a desired manner. We show the attack efficiency by comparing it with the reward-poisoning approach, and empirically demonstrate the transferability of the environment-poisoning attack strategy. Finally, we seek to exploit the transferability of the attack strategy to handle black-box settings.},
  archive   = {C_AAMAS},
  author    = {Xu, Hang and Wang, Rundong and Raizman, Lev and Rabinovich, Zinovi},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1398–1406},
  title     = {Transferable environment poisoning: Training-time attack on reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464113},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Strategic evasion of centrality measures. <em>AAMAS</em>,
1389–1397. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Among the most fundamental tools for social network analysis are centrality measures, which quantify the importance of every node in the network. This centrality analysis typically disregards the possibility that the network may have been deliberately manipulated to mislead the analysis. To solve this problem, a recent study attempted to understand how a member of a social network could rewire the connections therein to avoid being identified as a leader of that network. However, the study was based on the assumption that the network analyzer-the seeker-is oblivious to any evasion attempts by the evader. In this paper, we relax this assumption by modelling the seeker and evader as strategic players in a Bayesian Stackelberg game. In this context, we study the complexity of various optimization problems, and analyze the equilibria of the game under different assumptions, thereby drawing the first conclusions in the literature regarding which centralities the seeker should use to maximize the chances of detecting a strategic evader.},
  archive   = {C_AAMAS},
  author    = {Waniek, Marcin and Wo\&#39;{z}nica, Jan and Zhou, Kai and Vorobeychik, Yevgeniy and Rahwan, Talal and Michalak, Tomasz P.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1389–1397},
  title     = {Strategic evasion of centrality measures},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464112},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mechanism design for public projects via neural networks.
<em>AAMAS</em>, 1380–1388. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study mechanism design for nonexcludable and excludable binary public project problems. We aim to maximize the expected number of consumers and the expected agents&#39; welfare. For the nonexcludable public project model, we identify a sufficient condition on the prior distribution for the conservative equal costs mechanism to be the optimal strategy-proof and individually rational mechanism. For general distributions, we propose a dynamic program that solves for the optimal mechanism. For the excludable public project model, we identify a similar sufficient condition for the serial cost sharing mechanism to be optimal for 2 and 3 agents. We derive a numerical upper bound. Experiments show that for several common distributions, the serial cost sharing mechanism is close to optimality.The serial cost sharing mechanism is not optimal in general. We design better performing mechanisms via neural networks. Our approach involves several technical innovations that can be applied to mechanism design in general. We interpret the mechanisms as price-oriented rationing-free (PORF) mechanisms, which enables us to move the mechanism&#39;s complex (e.g., iterative) decision making off the neural network, to a separate simulation process. We feed the prior distribution&#39;s analytical form into the cost function to provide high-quality gradients for efficient training. We use supervision to manual mechanisms as a systematic way for initialization. Our approach of &quot;supervision and then gradient descent&#39;&#39; is effective for improving manual mechanisms&#39; performances. It is also effective for fixing constraint violations for heuristic-based mechanisms that are infeasible.},
  archive   = {C_AAMAS},
  author    = {Wang, Guanhua and Guo, Runqi and Sakurai, Yuko and Ali Babar, Muhammad and Guo, Mingyu},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1380–1388},
  title     = {Mechanism design for public projects via neural networks},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464111},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Fairness and efficiency in facility location problems with
continuous demands. <em>AAMAS</em>, 1371–1379. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the facility location problem with continuous demands, where customers are continuously distributed on an area, a planner wants to locate the facilities and allocate the customers to their closest facilities under the proximity rule. In this work, we focus on the fairness and system efficiency from the facility&#39;s perspective. Each facility is assumed to have a preference (represented as valuation function) over the subsets of customers. For the fairness of facilities, we provide approximation guarantees for the proportionality and envy-freeness. For the efficiency, we study the utilitarian and egalitarian social welfare. In addition, we are interested in quantifying the possible trade-offs between meeting the fairness criteria and maximizing social welfare, measured by the price of fairness.},
  archive   = {C_AAMAS},
  author    = {Wang, Chenhao and Zhang, Mengqi},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1371–1379},
  title     = {Fairness and efficiency in facility location problems with continuous demands},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464110},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scalable optimization for wind farm control using
coordination graphs. <em>AAMAS</em>, 1362–1370. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Wind farms are a crucial driver toward the generation of ecological and renewable energy. Due to their rapid increase in capacity, contemporary wind farms need to adhere to strict constraints on power output to ensure stability of the electricity grid. Specifically, a wind farm controller is required to match the farm&#39;s power production with a power demand imposed by the grid operator. This is a non-trivial optimization problem, as complex dependencies exist between the wind turbines. State-of-the-art wind farm control typically relies on physics-based heuristics that fail to capture the full load spectrum that defines a turbine&#39;s health status. When this is not taken into account, the long-term viability of the farm&#39;s turbines is put at risk. Given the complex dependencies that determine a turbine&#39;s lifetime, learning a flexible and optimal control strategy requires a data-driven approach. However, as wind farms are large-scale multi-agent systems, optimizing control strategies over the full joint action space is intractable. We propose a new learning method for wind farm control that leverages the sparse wind farm structure to factorize the optimization problem. Using a Bayesian approach, based on multi-agent Thompson sampling, we explore the factored joint action space for configurations that match the demand, while considering the lifetime of turbines. We apply our method to a grid-like wind farm layout, and evaluate configurations using a state-of-the-art wind flow simulator. Our results are competitive with a physics-based heuristic approach in terms of demand error, while, contrary to the heuristic, our method prolongs the lifetime of high-risk turbines.},
  archive   = {C_AAMAS},
  author    = {Verstraeten, Timothy and Daems, Pieter-Jan and Bargiacchi, Eugenio and Roijers, Diederik M. and Libin, Pieter J.K. and Helsen, Jan},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1362–1370},
  title     = {Scalable optimization for wind farm control using coordination graphs},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464109},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reinforcement learning for unified allocation and patrolling
in signaling games with uncertainty. <em>AAMAS</em>, 1353–1361. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Green Security Games (GSGs) have been successfully used in the protection of valuable resources such as fisheries, forests, and wildlife. Real-world deployment involves both resource allocation and subsequent coordinated patrolling with communication in the presence real-time, uncertain information. Previous game models do not address both of these stages simultaneously. Furthermore, adopting existing solution strategies is difficult since they do not scale well for larger, more complex variants of the game models. We propose a novel GSG model to address these challenges. We also present a novel algorithm, CombSGPO, to compute a defender strategy for this game model. CombSGPO performs policy search over a multidimensional, discrete action space to compute an allocation strategy that is best suited to a best-response patrolling strategy for the defender, learnt by training a multi-agent Deep Q-Network. We show via experiments that CombSGPO converges to better strategies and is more scalable than comparable approaches. From a detailed analysis of the coordination and signaling behavior learnt by CombSGPO, we find that strategic signaling emerges in the final learnt strategy.},
  archive   = {C_AAMAS},
  author    = {Venugopal, Aravind and Bondi, Elizabeth and Kamarthi, Harshavardhan and Dholakia, Keval and Ravindran, Balaraman and Tambe, Milind},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1353–1361},
  title     = {Reinforcement learning for unified allocation and patrolling in signaling games with uncertainty},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464108},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). No more hand-tuning rewards: Masked constrained policy
optimization for safe reinforcement learning. <em>AAMAS</em>, 1344–1352.
(<a href="https://dl.acm.org/doi/10.5555/3463952.3464107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In safe Reinforcement Learning (RL), the agent attempts to find policies which maximize the expectation of accumulated rewards and guarantee its safety to remain above a given threshold. Hence, it is straightforward to formalize safe RL problems by both a reward function and a safety constraint. We define safety as the probability of survival in environments where taking risky actions could lead to early termination of the task. Although the optimization problem is already constrained by a safety threshold, reward signals related to unsafe terminal states influence the original maximization objective of the task. Selecting the appropriate value of these signals is often a time consuming and challenging reward engineering task, which requires expert knowledge of the domain. This paper presents a safe RL algorithm, called Masked Constrained Policy Optimization (MCPO), in which the learning process is constrained by safety and excludes the risk reward signals. We develop MCPO as an extension of gradient-based policy search methods, in which the updates of the policy and the expected reward models are masked. Our method benefits from having a high probability of satisfying the given constraints for every policy in the learning process. We validate the proposed algorithm in two continuous tasks. Our findings prove the proposed algorithm is able to neglect risk reward signals, and thereby resolving the desired safety-performance trade-off without having the need for hand-tuning rewards.},
  archive   = {C_AAMAS},
  author    = {Van Havermaet, Stef and Khaluf, Yara and Simoens, Pieter},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1344–1352},
  title     = {No more hand-tuning rewards: Masked constrained policy optimization for safe reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464107},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Collaborative multiagent decision making for lane-free
autonomous driving. <em>AAMAS</em>, 1335–1343. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the problem of collaborative multi-agent autonomous driving of connected and automated vehicles (CAVs) in lane-free highway scenarios. We eliminate the lane-changing task, i.e., CAVs may be located in any arbitrary lateral position within the road boundaries, hence allowing for better utilization of the available road capacity. As a consequence, vehicles operate in a much more complex environment, and the need for the individual CAVs to select actions that are efficient for the group as a whole is highly desired. We formulate this environment as a multiagent collaboration problem represented via a coordination graph, thus decomposing the problem with local utility functions, based on the interactions between vehicles. We produce a tractable and scalable solution by estimating the joint action of all vehicles via the anytime max-plus algorithm, with local utility functions provided by potential fields, designed to promote collision avoidance. Specifically, the fields have an ellipsoid form that is most suitable for lane-free highway environments. This novel use of max-plus with potential fields gives rise to a coordinated control policy that exploits only local information specific to each CAV. Our experimental evaluation confirms the effectiveness of our approach: lane-free movement allows for increased traffic flow rates, and vehicles are able to achieve speeds that are both high and close to their desired ones, even in demanding environments with high traffic flow.},
  archive   = {C_AAMAS},
  author    = {Troullinos, Dimitrios and Chalkiadakis, Georgios and Papamichail, Ioannis and Papageorgiou, Markos},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1335–1343},
  title     = {Collaborative multiagent decision making for lane-free autonomous driving},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464106},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient exact computation of setwise minimax regret for
interactive preference elicitation. <em>AAMAS</em>, 1326–1334. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A key issue in artificial intelligence methods for interactive preference elicitation is choosing at each stage an appropriate query to the user, in order to find a near-optimal solution as quickly as possible. A theoretically attractive method is to choose a query that minimises max setwise regret (which corresponds to the worst case loss response in terms of value of information). We focus here on the situation in which the choices are represented explicitly in a database, and with a model of user utility as a weighted sum of the criteria; in this case when the user makes a choice, an agent learns a linear constraint on the unknown vector of weights. We develop an algorithmic method for computing minimax setwise regret for this form of preference model, by making use of a SAT solver with cardinality constraints to prune the search space, and computing max setwise regret using an extreme points method. Our experimental results demonstrate the feasibility of the approach and the very substantial speed up over the state of the art.},
  archive   = {C_AAMAS},
  author    = {Toffano, Federico and Viappiani, Paolo and Wilson, Nic},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1326–1334},
  title     = {Efficient exact computation of setwise minimax regret for interactive preference elicitation},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464105},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Guiding evolutionary strategies with off-policy
actor-critic. <em>AAMAS</em>, 1317–1325. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Evolutionary strategies (ES) and off-policy learning algorithms are two major workhorses of Reinforcement learning (RL): ES adopt a simple blackbox approach to optimization but it can be slightly more sample inefficient; off-policy learning is by design more sample efficient but the updates can be unstable. Motivated by their trade-offs, we propose CEM-ACER, a combination of Cross-entropy method, a standard ES algorithm, and Actor-critic with experience replay (ACER), an off-policy actor-critic algorithm. Our proposal relies on a key insight: off-policy algorithms provide a natural mechanism to efficiently evolve parameter populations as part of an ES algorithm. Across a wide range of benchmark control tasks, we show that CEM-ACER balances the strengths of CEM and ACER, leading to an algorithm that consistently outperforms its individual building blocks, as well as other competitive baseline algorithms.},
  archive   = {C_AAMAS},
  author    = {Tang, Yunhao},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1317–1325},
  title     = {Guiding evolutionary strategies with off-policy actor-critic},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464104},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning complex policy distribution with CEM guided
adversarial hypernetwork. <em>AAMAS</em>, 1308–1316. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cross-Entropy Method (CEM) is a gradient-free direct policy search method, which has greater stability and is insensitive to hyper-parameter tuning. CEM bears similarity to population-based evolutionary methods, but, rather than using a population it uses a distribution over candidate solutions (policies in our case). Usually, a natural exponential family distribution such as multivariate Gaussian is used to parameterize the policy distribution. Using a multivariate Gaussian limits the quality of CEM policies as the search becomes confined to a less representative subspace. We address this drawback by using an adversarially-trained hypernetwork, enabling a richer and complex representation of the policy distribution. To achieve better training stability and faster convergence, we use a multivariate Gaussian CEM policy to guide our adversarial training process. Experiments demonstrate that our approach outperforms state-of-the-art CEM-based methods by $15.8\%$ in terms of rewards while achieving faster convergence. Results also show that our approach is less sensitive to hyper-parameters than other deep-RL methods such as REINFORCE, DDPG and DQN.},
  archive   = {C_AAMAS},
  author    = {Tang, Shi Yuan and Irissappane, Athirai A. and Oliehoek, Frans A. and Zhang, Jie},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1308–1316},
  title     = {Learning complex policy distribution with CEM guided adversarial hypernetwork},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464103},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive cascade submodular maximization. <em>AAMAS</em>,
1299–1307. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose and study the cascade submodular maximization problem under the adaptive setting. The input of our problem is a set of items, each item is in a particular state (i.e., the marginal contribution of an item) which is drawn from a known probability distribution. However, we can not know its actual state before selecting it. As compared with existing studies on stochastic submodular maximization, one unique setting of our problem is that each item is associated with a continuation probability which represents the probability that one is allowed to continue to select the next item after selecting the current one. Intuitively, this term captures the externality of selecting one item to all its subsequent items in terms of the opportunity of being selected. Therefore, the actual set of items that can be selected by a policy depends on the specific ordering it adopts to select items, this makes our problem fundamentally different from classical submodular set optimization problems. Our objective is to identify the best sequence of selecting items so as to maximize the expected utility of the selected items. We propose a class of stochastic utility functions, adaptive cascade submodular functions, and show that the objective functions in many practical application domains satisfy adaptive cascade submodularity. Then we develop a $0.12$ approximation algorithm to the adaptive cascade submodular maximization problem.},
  archive   = {C_AAMAS},
  author    = {Tang, Shaojie and Yuan, Jing},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1299–1307},
  title     = {Adaptive cascade submodular maximization},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464102},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Grab the reins of crowds: Estimating the effects of crowd
movement guidance using causal inference. <em>AAMAS</em>, 1290–1298. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Crowd movement guidance has been a fascinating problem in various fields, such as easing traffic congestion in unusual events and evacuating people from an emergency-affected area. To grab the reins of crowds, there has been considerable demand for a decision support system that can answer a typical question: &quot;what will be the outcomes of each of the possible options in the current situation&quot;. In this paper, we consider the problem of estimating the effects of crowd movement guidance from past data. To cope with limited amount of available data biased by past decision-makers, we leverage two recent techniques in deep representation learning for spatial data analysis and causal inference. We use a spatial convolutional operator to extract effective spatial features of crowds from a small amount of data and use balanced representation learning based on the integral probability metrics to mitigate the selection bias and missing counterfactual outcomes. To evaluate the performance on estimating the treatment effects of possible guidance, we use a multi-agent simulator to generate realistic data on evacuation scenarios in a crowded theater, since there are no available datasets recording outcomes of all possible crowd movement guidance. The results of three experiments demonstrate that our proposed method reduces the estimation error by at most 56\% from state-of-the-art methods.},
  archive   = {C_AAMAS},
  author    = {Takeuchi, Koh and Nishida, Ryo and Kashima, Hisashi and Onishi, Masaki},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1290–1298},
  title     = {Grab the reins of crowds: Estimating the effects of crowd movement guidance using causal inference},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464101},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Connections between fairness criteria and efficiency for
allocating indivisible chores. <em>AAMAS</em>, 1281–1289. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study several fairness notions in allocating indivisible chores (i.e., items with non-positive values): envy-freeness and its relaxations. For allocations under each fairness criterion, we establish their approximation guarantees for other fairness criteria. Under the setting of additive cost functions, our results show strong connections between these fairness criteria and, at the same time, reveal intrinsic differences between goods allocation and chores allocation. Furthermore, we investigate the efficiency loss under these fairness constraints and establish their prices of fairness.},
  archive   = {C_AAMAS},
  author    = {Sun, Ankang and Chen, Bo and Vinh Doan, Xuan},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1281–1289},
  title     = {Connections between fairness criteria and efficiency for allocating indivisible chores},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464100},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mean-payoff games with ω-regular specifications.
<em>AAMAS</em>, 1272–1280. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-player mean-payoff games are a natural formalism to model concurrent and multi-agent systems with self-interested players. Players in such a game traverse a graph, while trying to maximise a mean-payoff function that depends on the plays so generated. As with all games, the equilibria that could arise may have undesirable properties. However, as system designers, we typically wish to ensure that equilibria in such systems correspond to desirable system behaviours, for example, satisfying certain safety or liveness properties. One natural way to do this would be to specify such desirable properties using temporal logic. Unfortunately, the use of temporal logic specifications causes game theoretic verification problems to have very high computational complexity. To this end, we consider \o{}mega-regular specifications, which offer a concise and intuitive way of specifying desirable behaviours of a system. The main results of this work are characterisation and complexity bounds for the problem of determining if there are equilibria that satisfy a given \o{}mega-regular specification in a multi-player mean-payoff game in a number of computationally relevant game-theoretic settings.},
  archive   = {C_AAMAS},
  author    = {Steeples, Thomas and Gutierrez, Julian and Wooldridge, Michael},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1272–1280},
  title     = {Mean-payoff games with ω-regular specifications},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464099},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Achieving sybil-proofness in distributed work systems.
<em>AAMAS</em>, 1263–1271. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In a multi-agent system where agents provide quantifiable work for each other on a voluntary basis, reputation mechanisms are incorporated to induce cooperation. Hereby agents assign their peers numerical scores based on their reported transaction histories. In such systems, adversaries can launch an attack by creating fake identities called Sybils, who report counterfeit transactions among one another, with the aim of increasing their own scores in the eyes of others. This paper provides new results about the Sybil-proofness of reputation mechanisms. We revisit the impossibility result of Seuken and Parkes (2011), who show that strongly-beneficial Sybil attacks cannot be prevented on reputation mechanisms satisfying three particular requirements. We prove that, under a more rigorous set of definitions of Sybil attack benefit, this result no longer holds. We characterise properties under which reputation mechanisms are susceptible to strongly-beneficial Sybil attacks. Building on our results, we propose a minimal set of requirements for reputation mechanisms to achieve resistance to such attacks, which are stronger than the results by Cheng and Friedman (2005), who show Sybil-proofness of certain asymmetric reputation mechanisms.},
  archive   = {C_AAMAS},
  author    = {Stannat, Alexander and Umut Ileri, Can and Gijswijt, Dion and Pouwelse, Johan},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1263–1271},
  title     = {Achieving sybil-proofness in distributed work systems},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464098},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Regular model checking approach to knowledge reasoning over
parameterized systems. <em>AAMAS</em>, 1254–1262. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a general framework for modelling and verifying epistemic properties over parameterized multi-agent systems that communicate by truthful public announcements. In our framework, the number of agents or the amount of certain resources are parameterized (i.e. not known a priori), and the corresponding verification problem asks whether a given epistemic property is true regardless of the instantiation of the parameters. For example, in a muddy children puzzle, one could ask whether each child will eventually find out whether (s)he is muddy, regardless of the number of children. Our framework is regular model checking (RMC)-based, wherein synchronous finite-state automata (equivalently, monadic second-order logic over words) are used to specify the systems. We propose an extension of public announcement logic as specification language. Of special interests is the addition of the so-called iterated public announcement operators, which are crucial for reasoning about knowledge in parameterized systems. Although the operators make the model checking problem undecidable, we show that this becomes decidable when an appropriate &quot;disappearance relation&quot; is given. Further, we show how Angluin&#39;s L*-algorithm for learning finite automata can be applied to find a disappearance relation, which is guaranteed to terminate if it is regular. We have implemented the algorithm and apply this to such examples as the Muddy Children Puzzle, the Russian Card Problem, and Large Number Challenge.},
  archive   = {C_AAMAS},
  author    = {Stan, Daniel and Lin, Anthony W.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1254–1262},
  title     = {Regular model checking approach to knowledge reasoning over parameterized systems},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464097},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Towards transferrable personalized student models in
educational games. <em>AAMAS</em>, 1245–1253. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To help facilitate play and learning, game-based educational activities often feature a computational agent as a co-player. Personalizing this agent&#39;s behavior to the student player is an active area of research, and prior work has demonstrated the benefits of personalized educational interaction across a variety of domains. A critical research challenge for personalized educational agents is real-time student modeling. Most student models are designed for and trained on only a single task, which limits the variety, flexibility, and efficiency of student player model learning. In this paper we present a research project applying transfer learning methods to student player models over different educational tasks, studying the effects of an algorithmic &quot;multi-task personalization&quot; approach on the accuracy and data efficiency of student model learning. We describe a unified robotic game system for studying multi-task personalization over two different educational games, each emphasizing early language and literacy skills such as rhyming and spelling. We present a flexible Gaussian Process-based approach for rapidly learning student models from interactive play in each game, and a method for transferring each game&#39;s learned student model to the other via a novel instance-weighting protocol based on task similarity. We present results from a simulation-based investigation of the impact of multi-task personalization, establishing the core viability and benefits of transferrable student models and outlining new questions for future in-person research.},
  archive   = {C_AAMAS},
  author    = {Spaulding, Samuel and Shen, Jocelyn and Park, Haewon and Breazeal, Cynthia},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1245–1253},
  title     = {Towards transferrable personalized student models in educational games},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464096},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rankings for bipartite tournaments via chain editing.
<em>AAMAS</em>, 1236–1244. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ranking the participants of a tournament has applications in voting, paired comparisons analysis, sports and other domains. In this paper we introduce bipartite tournaments, which model situations in which two different kinds of entity compete indirectly via matches against players of the opposite kind; examples include education (students/exam questions) and solo sports (golfers/courses). In particular, we look to find rankings via chain graphs, which correspond to bipartite tournaments in which the sets of adversaries defeated by the players on one side are nested with respect to set inclusion. Tournaments of this form have a natural and appealing ranking associated with them. We apply chain editing -- finding the minimum number of edge changes required to form a chain graph -- as a new mechanism for tournament ranking. The properties of these rankings are investigated in a probabilistic setting, where they arise as maximum likelihood estimators, and through the axiomatic method of social choice theory. Despite some nice properties, two problems remain: an important anonymity axiom is violated, and chain editing is NP-hard. We address both issues by relaxing the minimisation constraint in chain editing, and characterise the resulting ranking methods via a greedy approximation algorithm.},
  archive   = {C_AAMAS},
  author    = {Singleton, Joseph and Booth, Richard},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1236–1244},
  title     = {Rankings for bipartite tournaments via chain editing},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464095},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). AlwaysSafe: Reinforcement learning without safety constraint
violations during training. <em>AAMAS</em>, 1226–1235. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deploying reinforcement learning (RL) involves major concerns around safety. Engineering a reward signal that allows the agent to maximize its performance while remaining safe is not trivial. Safe RL studies how to mitigate such problems. For instance, we can decouple safety from reward using constrained Markov decision processes (CMDPs), where an independent signal models the safety aspects. In this setting, an RL agent can autonomously find tradeoffs between performance and safety. Unfortunately, most RL agents designed for CMDPs only guarantee safety after the learning phase, which might prevent their direct deployment. In this work, we investigate settings where a concise abstract model of the safety aspects is given, a reasonable assumption since a thorough understanding of safety-related matters is a prerequisite for deploying RL in typical applications. Factored CMDPs provide such compact models when a small subset of features describe the dynamics relevant for the safety constraints. We propose an RL algorithm that uses this abstract model to learn policies for CMDPs safely, that is without violating the constraints. During the training process, this algorithm can seamlessly switch from a conservative policy to a greedy policy without violating the safety constraints. We prove that this algorithm is safe under the given assumptions. Empirically, we show that even if safety and reward signals are contradictory, this algorithm always operates safely and, when they are aligned, this approach also improves the agent&#39;s performance.},
  archive   = {C_AAMAS},
  author    = {Sim\~{a}o, Thiago D. and Jansen, Nils and Spaan, Matthijs T. J.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1226–1235},
  title     = {AlwaysSafe: Reinforcement learning without safety constraint violations during training},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464094},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Active perception within BDI agents reasoning cycle.
<em>AAMAS</em>, 1218–1225. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In multi-agent systems the main process responsible for obtaining information about the environment is perception, generally this process is performed passively regardless the agent&#39;s intentional state. However, especially when inserted in the real world, a frequent problem is that agents have partial perception of the environment, failing to perceive some relevant information. To circumvent this problem, a solution is to actively take actions to perceive what is of interest to the agent, for example, in a computer vision system, the camera can be repositioned to have a better view of an object. This work aims to develop an active perception model integrated with the reasoning cycle of BDI agents. Experiments are performed using BDI agents with ROS to command unmanned aerial vehicles to analyze the benefits and impacts of using cognitive agents with active perception to program robot intelligence.},
  archive   = {C_AAMAS},
  author    = {Silva, Gustavo R. and H\&quot;{u}bner, Jomi F. and Becker, Leandro B.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1218–1225},
  title     = {Active perception within BDI agents reasoning cycle},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464093},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sequential mechanisms for multi-type resource allocation.
<em>AAMAS</em>, 1209–1217. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Several resource allocation problems involve multiple types of resources, with a different agency being responsible for &quot;locally&quot; allocating the resources of each type, while a central planner wishes to provide a guarantee on the properties of the final allocation given agents&#39; preferences. We study the relationship between properties of the local mechanisms, each responsible for assigning all of the resources of a designated type, and the properties of a sequential mechanism which is composed of these local mechanisms, one for each type, applied sequentially, under lexicographic preferences, a well studied model of preferences over multiple types of resources in artificial intelligence and economics. We show that when preferences are O-legal, meaning that agents share a common importance order on the types, sequential mechanisms satisfy the desirable properties of anonymity, neutrality, non-bossiness, or Pareto-optimality if and only if every local mechanism also satisfies the same property, and they are applied sequentially according to the order O. Our main results are that under O-legal lexicographic preferences, every mechanism satisfying strategyproofness and a combination of these properties must be a sequential composition of local mechanisms that are also strategyproof, and satisfy the same combinations of properties.},
  archive   = {C_AAMAS},
  author    = {Sikdar, Sujoy and Guo, Xiaoxi and Wang, Haibin and Xia, Lirong and Cao, Yongzhi},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1209–1217},
  title     = {Sequential mechanisms for multi-type resource allocation},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464092},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cyber attack intent recognition and active deception using
factored interactive POMDPs. <em>AAMAS</em>, 1200–1208. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an intelligent and adaptive agent that employs deception to recognize a cyber adversary&#39;s intent on a honeypot host. Unlike previous approaches to cyber deception, which mainly focus on delaying or confusing the attackers, we focus on engaging with them to learn their intent. We model cyber deception as a sequential decision-making problem in a two-agent context. We introduce factored finitely-nested interactive POMDPs (I-POMDPX) and use this framework to model the problem with multiple attacker types. Our approach models cyber attacks on a single honeypot host across multiple phases from the attacker&#39;s initial entry to reaching its adversarial objective. The defending I-POMDPX-based agent uses decoys to engage with the attacker at multiple phases to form increasingly accurate predictions of the attacker&#39;s behavior and intent. The use of I-POMDPs also enables us to model the adversary&#39;s mental state and investigate how deception affects their beliefs. Our experiments in both simulation and with the agent deployed on a host system show that the I-POMDPX-based agent performs significantly better at intent recognition than commonly used deception strategies on honeypots. This emerging application of autonomous agents offers a new approach that contrasts with the traditional action-reaction dynamic that has defined interactions between cyber attackers and defenders for years.},
  archive   = {C_AAMAS},
  author    = {Shinde, Aditya and Doshi, Prashant and Setayeshfar, Omid},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1200–1208},
  title     = {Cyber attack intent recognition and active deception using factored interactive POMDPs},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464091},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cooperative policy learning with pre-trained heterogeneous
observation representations. <em>AAMAS</em>, 1191–1199. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent reinforcement learning (MARL) has been increasingly explored to learn the cooperative policy towards maximizing a certain global reward. Many existing studies take advantage of graph neural networks (GNN) in MARL to propagate critical collaborative information over the interaction graph, built upon inter-connected agents. Nevertheless, the vanilla GNN approach yields substantial defects in dealing with complex real-world scenarios since the generic message passing mechanism is ineffective between heterogeneous vertices and, moreover, simple message aggregation functions are incapable of accurately modeling the combinational interactions from multiple neighbors. While adopting complex GNN models with more informative message passing and aggregation mechanisms can obviously benefit heterogeneous vertex representations and cooperative policy learning, it could, on the other hand, increase the training difficulty of MARL and demand more intense and direct reward signals compared to the original global reward. To address these challenges, we propose a new cooperative learning framework with pre-trained heterogeneous observation representations. Particularly, we employ an encoder-decoder based graph attention to learn the intricate interactions and heterogeneous representations that can be more easily leveraged by MARL. Moreover, we design a pre-training with local actor-critic algorithm to ease the difficulty in cooperative policy learning. Extensive experiments over real-world scenarios demonstrate that our new approach can significantly outperform existing MARL baselines as well as operational research solutions that are widely-used in industry.},
  archive   = {C_AAMAS},
  author    = {Shi, Wenlei and Wei, Xinran and Zhang, Jia and Ni, Xiaoyuan and Jiang, Arthur and Bian, Jiang and Liu, Tie-Yan},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1191–1199},
  title     = {Cooperative policy learning with pre-trained heterogeneous observation representations},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464090},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multiagent epidemiologic inference through realtime contact
tracing. <em>AAMAS</em>, 1182–1190. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses an epidemiologic inference problem where, given realtime observation of test results, presence of symptoms, and physical contacts, the most likely infected individuals need to be inferred. The inference problem is modeled as a hidden Markov model where infection probabilities are updated at every time step and evolve between time steps. We suggest a unique inference approach that avoids storing the given observations explicitly. Theoretical justification for the proposed model is provided under specific simplifying assumptions. To complement these theoretical results, a comprehensive experimental study is performed using a custom-built agent-based simulator that models inter-agent contacts. The reported results show the effectiveness of the proposed inference model when considering more realistic scenarios -- where the simplifying assumptions do not hold. When pairing the proposed inference model with a simple testing and quarantine policy, promising trends are obtained where the epidemic progression is significantly slowed down while quarantining a bounded number of individuals.},
  archive   = {C_AAMAS},
  author    = {Sharon, Guni and Ault, James and Stone, Peter and Kompella, Varun and Capobianco, Roberto},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1182–1190},
  title     = {Multiagent epidemiologic inference through realtime contact tracing},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464089},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sequential ski rental problem. <em>AAMAS</em>, 1173–1181.
(<a href="https://dl.acm.org/doi/10.5555/3463952.3464088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The classical &#39;buy or rent&#39; ski-rental problem was recently considered in the setting where multiple experts (such as Machine learning algorithms) advice on the length of the ski season. Here, robust algorithms were developed with improved theoretical performance over adversarial scenarios where such expert predictions were unavailable. We consider a variant of this problem which we call the &#39;sequential ski-rental&#39; problem. Here, a sequence of ski-rental problems has to be solved in an online fashion where both the buy cost and the length of ski season are unknown to the learner. The learner has access to two sets of experts, one set who advise on the true cost of buying the ski and another set who advise on the length of the ski season. Under certain stochastic assumptions on the experts who predict the buy costs, we develop online algorithms and prove regret bounds for the same. Our experimental evaluations confirm our theoretical results.},
  archive   = {C_AAMAS},
  author    = {Shah, Anant and Rajkumar, Arun},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1173–1181},
  title     = {Sequential ski rental problem},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464088},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An autonomous negotiating agent framework with reinforcement
learning based strategies and adaptive strategy switching mechanism.
<em>AAMAS</em>, 1163–1172. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite abundant negotiation strategies in literature, the complexity of automated negotiation forbids a single strategy from being dominant against all others in different negotiation scenarios. To overcome this, one approach is to use mixture of experts, but at the same time, one problem of this method is the selection of experts, as this approach is limited by the competency of the experts selected. Another problem with most negotiation strategies is their incapability of adapting to dynamic variation of the opponent&#39;s behaviour within a single negotiation session resulting in poor performance. This work focuses on both, solving the problem of expert selection and adapting to the opponent&#39;s behaviour with our Autonomous Negotiating Agent Framework. This framework allows real-time classification of opponent&#39;s behaviour and provides a mechanism to select, switch or combine strategies within a single negotiation session. Additionally, our framework has a reviewer component which enables self-enhancement capability by deciding to include new strategies or replace old ones with better strategies periodically. We demonstrate an instance of our framework by implementing maximum entropy reinforcement learning based strategies with a deep learning based opponent classifier. Finally, we evaluate the performance of our agent against state-of-the-art negotiators under varied negotiation scenarios.},
  archive   = {C_AAMAS},
  author    = {Sengupta, Ayan and Mohammad, Yasser and Nakadai, Shinji},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1163–1172},
  title     = {An autonomous negotiating agent framework with reinforcement learning based strategies and adaptive strategy switching mechanism},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464087},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Partial robustness in team formation: Bridging the gap
between robustness and resilience. <em>AAMAS</em>, 1154–1162. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Team formation is the problem of deploying the least expensive team of agents while covering a set of skills. Once a team has been formed, some of the agents considered at start may be finally defective and some skills may become uncovered. Two solution concepts have been recently introduced to deal with this issue in a proactive manner: one may form a team which is robust to changes so that after some agent losses, all skills remain covered; or one may opt for a recoverable team, i.e., it can be &quot;repaired&quot; in the worst case by hiring new agents while keeping the overall deployment cost minimal. In this paper, we introduce the problem of partially robust team formation (PR-TF). Partial robustness is a weaker form of robustness which guarantees a certain degree of skill coverage after some agents are lost. We analyze the computational complexity of PR-TF, and provide a complete algorithm for it. The performance of our algorithm is empirically compared with the existing methods for robust and recoverable team formation, on a number of existing benchmarks and some newly introduced ones. Partial robustness is shown to be an interesting trade-off notion between (full) robustness and recoverability in terms of computational efficiency, skill coverage guarantees after agent losses, and repairability.},
  archive   = {C_AAMAS},
  author    = {Schwind, Nicolas and Demirovi\&#39;{c}, Emir and Inoue, Katsumi and Lagniez, Jean-Marie},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1154–1162},
  title     = {Partial robustness in team formation: Bridging the gap between robustness and resilience},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464086},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Timely information from prediction markets. <em>AAMAS</em>,
1145–1153. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Prediction markets are powerful tools to elicit and aggregate beliefs from strategic agents. However, in current prediction markets, agents may exhaust the social welfare by competing to be the first to update the market. We initiate the study of the trade-off between how quickly information is aggregated by the market, and how much this information costs. We design markets to aggregate timely information from strategic agents to maximize social welfare. To this end, the market must incentivize agents to invest the correct amount of effort to acquire information: quickly enough to be useful, but not faster (and more expensively) than necessary. The market also must ensure that agents report their information truthfully and on time. We consider two settings: in the first, information is only valuable before a deadline; in the second, the value of information decreases as time passes. We use both theorems and simulations to demonstrate the mechanisms.},
  archive   = {C_AAMAS},
  author    = {Schoenebeck, Grant and Yu, Chenkai and Yu, Fang-Yi},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1145–1153},
  title     = {Timely information from prediction markets},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464085},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). CMCF: An architecture for realtime gesture generation by
clustering gestures by motion and communicative function.
<em>AAMAS</em>, 1136–1144. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Gestures augment speech by performing a variety of communicative functions in humans and virtual agents, and are often related to speech by complex semantic, rhetorical, prosodic, and affective elements. In this paper we briefly present an architecture for human-like gesturing in virtual agents that is designed to realize complex speech-to-gesture mappings by exploiting existing machine-learning based parsing tools and techniques to extract these functional elements from speech. We then deeply explore the rhetorical branch of this architecture, objectively assessing specifically whether existing rhetorical parsing techniques can classify gestures into classes with distinct movement properties. To do this, we take a corpus of spontaneously generated gestures and correlate their movement to co-speech utterances. We cluster gestures based on their rhetorical properties, and then by their movement. Our objective analysis suggests that some rhetorical structures are identifiable by our movement features while others require further exploration. We explore possibilities behind these findings and propose future experiments that may further reveal nuances of the richness of the mapping between speech and motion. This work builds towards a real-time gesture generator which performs gestures that effectively convey rich communicative functions.},
  archive   = {C_AAMAS},
  author    = {Saund, Carolyn and B\^{\i}rl\u{a}deanu, Andrei and Marsella, Stacy},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1136–1144},
  title     = {CMCF: An architecture for realtime gesture generation by clustering gestures by motion and communicative function},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464084},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A local search based approach to solve continuous DCOPs.
<em>AAMAS</em>, 1127–1135. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Distributed Constraint Optimization Problems (DCOPs) are a suitable formulation for coordinating interactions (i.e. constraints) in cooperative multi-agent systems. The traditional DCOP model assumes that variables owned by the agents can take only discrete values and constraints&#39; cost functions are defined for every possible value assignment of a set of variables. While this formulation is often reasonable, there are many applications where the decision variables are continuous-valued and constraints are in functional form. To overcome this limitation, Continuous DCOPs (C-DCOPs), an extension of the DCOPs model has been proposed that is able to formulate problems having continuous variables. The existing methods for solving C-DCOPs come with a huge computation and communication overhead. In this paper, we apply continuous non-linear optimization methods on Cooperative Constraint Approximation (CoCoA) algorithm, which is a non-iterative, fast incomplete local search approach for solving DCOPs. We empirically show that our algorithm is able to provide high-quality solutions at the expense of smaller communication cost and execution time compared to the state-of-the-art C-DCOP algorithms.},
  archive   = {C_AAMAS},
  author    = {Sarker, Amit and Choudhury, Moumita and Khan, Md. Mosaddek},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1127–1135},
  title     = {A local search based approach to solve continuous DCOPs},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464083},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SPOTTER: Extending symbolic planning operators through
targeted reinforcement learning. <em>AAMAS</em>, 1118–1126. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Symbolic planning models allow decision-making agents to sequence actions in arbitrary ways to achieve a variety of goals in dynamic domains. However, they are typically handcrafted and tend to require precise formulations that are not robust to human error. Reinforcement learning (RL) approaches do not require such models, and instead learn domain dynamics by exploring the environment and collecting rewards. However, RL approaches tend to require millions of episodes of experience and often learn policies that are not easily transferable to other tasks. In this paper, we address one aspect of the open problem of integrating these approaches: how can decision-making agents resolve discrepancies in their symbolic planning models while attempting to accomplish goals? We propose an integrated framework named SPOTTER that uses RL to augment and support (&quot;spot&quot;) a planning agent by discovering new operators needed by the agent to accomplish goals that are initially unreachable for the agent. SPOTTER outperforms pure-RL approaches while also discovering transferable symbolic knowledge and does not require supervision, successful plan traces or any a priori knowledge about the missing planning operator.},
  archive   = {C_AAMAS},
  author    = {Sarathy, Vasanth and Kasenberg, Daniel and Goel, Shivam and Sinapov, Jivko and Scheutz, Matthias},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1118–1126},
  title     = {SPOTTER: Extending symbolic planning operators through targeted reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464082},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficiently guiding imitation learning agents with human
gaze. <em>AAMAS</em>, 1109–1117. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human gaze is known to be an intention-revealing signal in human demonstrations of tasks. In this work, we use gaze cues from human demonstrators to enhance the performance of agents trained via three popular imitation learning methods --- behavioral cloning (BC), behavioral cloning from observation (BCO), and Trajectory-ranked Reward EXtrapolation (T-REX). Based on similarities between the attention of reinforcement learning agents and human gaze, we propose a novel approach for utilizing gaze data in a computationally efficient manner, as part of an auxiliary loss function, which guides a network to have higher activations in image regions where the human&#39;s gaze fixated. This work is a step towards augmenting any existing convolutional imitation learning agent&#39;s training with auxiliary gaze data. Our auxiliary coverage-based gaze loss (CGL) guides learning toward a better reward function or policy, without adding any additional learnable parameters and without requiring gaze data at test time. We find that our proposed approach improves the performance by 95\% for BC, 343\% for BCO, and 390\% for TREX, averaged over 20 different Atari games. We also find that compared to a prior state-of-the-art imitation learning method assisted by human gaze (AGIL), our method achieves better performance, and is more efficient in terms of learning with fewer demonstrations. We further interpret trained CGL agents with a saliency map visualization method to explain their performance. At last, we show that CGL can help alleviate a well-known causal confusion problem in imitation learning.},
  archive   = {C_AAMAS},
  author    = {Saran, Akanksha and Zhang, Ruohan and Short, Elaine S. and Niekum, Scott},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1109–1117},
  title     = {Efficiently guiding imitation learning agents with human gaze},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464081},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). SEERL: Sample efficient ensemble reinforcement learning.
<em>AAMAS</em>, 1100–1108. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ensemble learning is a very prevalent method employed in machine learning. The relative success of ensemble methods is attributed to their ability to tackle a wide range of instances and complex problems that require different low-level approaches. However, ensemble methods are relatively less popular in reinforcement learning owing to the high sample complexity and computational expense involved in obtaining a diverse ensemble. We present a novel training and model selection framework for model-free reinforcement algorithms that use ensembles of policies obtained from a single training run. These policies are diverse in nature and are learned through directed perturbation of the model parameters at regular intervals. We show that learning and selecting an adequately diverse set of policies is required for a good ensemble while extreme diversity can prove detrimental to overall performance. Selection of an adequately diverse set of policies is done through our novel policy selection framework. We evaluate our approach on challenging discrete and continuous control tasks and also discuss various ensembling strategies. Our framework is substantially sample efficient, computationally inexpensive and is seen to outperform state-of-the-art (SOTA) scores in Atari 2600 and Mujoco.},
  archive   = {C_AAMAS},
  author    = {Saphal, Rohan and Ravindran, Balaraman and Mudigere, Dheevatsa and Avancha, Sasikant and Kaul, Bharat},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1100–1108},
  title     = {SEERL: Sample efficient ensemble reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464080},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cooperative and competitive biases for multi-agent
reinforcement learning. <em>AAMAS</em>, 1091–1099. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Training a multi-agent reinforcement learning (MARL) algorithm is more challenging than training a single-agent reinforcement learning algorithm, because the result of a multi-agent task strongly depends on the complex interactions among agents and their interactions with a stochastic and dynamic environment. We propose an algorithm that boosts MARL training using the biased action information of other agents based on a friend-or-foe concept. For a cooperative and competitive environment, there are generally two groups of agents: cooperative-agents and competitive-agents. In the proposed algorithm, each agent updates its value function using its own action and the biased action information of other agents in the two groups. The biased joint action of cooperative agents is computed as the sum of their actual joint action and the imaginary cooperative joint action, by assuming all the cooperative agents jointly maximize the target agent&#39;s value function. The biased joint action of competitive agents can be computed similarly. Each agent then updates its own value function using the biased action information, resulting in a biased value function and corresponding biased policy. Subsequently, the biased policy of each agent is inevitably subjected to recommend an action to cooperate and compete with other agents, thereby introducing more active interactions among agents and enhancing the MARL policy learning. We empirically demonstrate that our algorithm outperforms existing algorithms in various mixed cooperative-competitive environments. Furthermore, the introduced biases gradually decrease as the training proceeds and the correction based on the imaginary assumption vanishes.},
  archive   = {C_AAMAS},
  author    = {Ryu, Heechang and Shin, Hayong and Park, Jinkyoo},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1091–1099},
  title     = {Cooperative and competitive biases for multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464079},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). TDprop: Does adaptive optimization with jacobi
preconditioning help temporal difference learning? <em>AAMAS</em>,
1082–1090. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate whether Jacobi preconditioning, accounting for the bootstrap term in temporal difference (TD) learning, can help boost performance of adaptive optimizers. Our method, TDprop, computes a per-parameter learning rate based on the diagonal preconditioning of the TD update rule. We show how this can be used in both n-step returns and TD(λ). Our theoretical findings demonstrate that including this additional preconditioning information is comparable to normal semi-gradient TD if the optimal learning rate is found for both via a hyperparameter search. This matches our experimental results. In Deep RL experiments using Expected SARSA, TDprop meets or exceeds the performance of Adam in all tested games under near-optimal learning rates, but a well-tuned SGD can yield similar performance in most settings. Our findings suggest that Jacobi preconditioning may improve upon Adam in Deep RL, but despite incorporating additional information from the TD bootstrap term, may not always be better than SGD. Moreover, they suggest that more theoretical investigations are needed to understand adaptive optimizers under optimal hyperparameter regimes in TD learning: simpler methods may, surprisingly, be theoretically comparable after a hyperparameter search.},
  archive   = {C_AAMAS},
  author    = {Romoff, Joshua and Henderson, Peter and Kanaa, David and Bengio, Emmanuel and Touati, Ahmed and Bacon, Pierre-Luc and Pineau, Joelle},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1082–1090},
  title     = {TDprop: Does adaptive optimization with jacobi preconditioning help temporal difference learning?},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464078},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accumulating risk capital through investing in cooperation.
<em>AAMAS</em>, 1073–1081. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent work on promoting cooperation in multi-agent learning has resulted in many methods which successfully promote cooperation at the cost of becoming more vulnerable to exploitation by malicious actors. We show that this is an unavoidable trade-off and propose an objective which balances these concerns, promoting both safety and long-term cooperation. Moreover, the trade-off between safety and cooperation is not severe, and you can receive exponentially large returns through cooperation from a small amount of risk. We study both an exact solution method and propose a method for training policies that targets this objective, Accumulating Risk Capital Through Investing in Cooperation (ARCTIC), and evaluate them in iterated Prisoner&#39;s Dilemma and Stag Hunt.},
  archive   = {C_AAMAS},
  author    = {Roman, Charlotte and Dennis, Michael and Critch, Andrew and Russell, Stuart},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1073–1081},
  title     = {Accumulating risk capital through investing in cooperation},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464077},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). User and system stories: An agile approach for managing
requirements in AOSE. <em>AAMAS</em>, 1064–1072. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The agile software development life cycle is widely used in industry today due to its highly flexible and iterative processes that facilitate rapid prototyping. There has been recent work in bringing concepts and processes from agile methodologies to agent-oriented software engineering (AOSE). We contribute to this effort by presenting in this paper a novel approach to capturing requirements of agent systems in AOSE using and extending agile concepts. In this paper, we propose to adopt and extend the well-known concept of User Stories to facilitate the development of agent systems. We introduce a novel concept, System Story, that defines requirements from the perspective of the system. These System Stories are refinements of User Stories and provide more intuitive mappings to agent concepts in the design and implementation. We show how our approach allows better traceability of requirements between stories and the different software development artifacts. We validate our proposal with a feature-based comparison to recent related work, and a preliminary user evaluation based on a drone simulation of a simple search and rescue case study.},
  archive   = {C_AAMAS},
  author    = {Rodriguez, Sebastian and Thangarajah, John and Winikoff, Michael},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1064–1072},
  title     = {User and system stories: An agile approach for managing requirements in AOSE},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464076},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MAPFAST: A deep algorithm selector for multi agent path
finding using shortest path embeddings. <em>AAMAS</em>, 1055–1063. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Solving the Multi-Agent Path Finding (MAPF) problem optimally is known to be NP-Hard for both make-span and total arrival time minimization. While many algorithms have been developed to solve MAPF problems, there is no dominating optimal MAPF algorithm that works well in all types of problems and no standard guidelines for when to use which algorithm. In this work, we develop the deep convolutional network MAPFAST (Multi-Agent Path Finding Algorithm SelecTor), which takes a MAPF problem instance and attempts to select the fastest algorithm to use from a portfolio of algorithms. We improve the performance of our model by including single-agent shortest paths in the instance embedding given to our model and by utilizing supplemental loss functions in addition to a classification loss. We evaluate our model on a large and diverse dataset of MAPF instances, showing that it outperforms all individual algorithms in its portfolio as well as the state-of-the-art optimal MAPF algorithm selector. We also provide an analysis of algorithm behavior in our dataset to gain a deeper understanding of optimal MAPF algorithms&#39; strengths and weaknesses to help other researchers leverage different heuristics in algorithm designs.},
  archive   = {C_AAMAS},
  author    = {Ren, Jingyao and Sathiyanarayanan, Vikraman and Ewing, Eric and Senbaslar, Baskin and Ayanian, Nora},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1055–1063},
  title     = {MAPFAST: A deep algorithm selector for multi agent path finding using shortest path embeddings},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464075},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Nash equilibria in finite-horizon multiagent concurrent
games. <em>AAMAS</em>, 1046–1054. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The problem of finding pure strategy Nash equilibria in multiagent concurrent games with finite-horizon temporal goals has received some recent attention. Earlier work solved this problem through the use of Rabin automata. In this work, we take advantage of the finite-horizon nature of the agents&#39; goals and show that checking for and finding pure strategy Nash equilibria can be done using a combination of safety games and lasso testing in B\&quot;{u}chi automata. To separate strategic reasoning from temporal reasoning, we model agents&#39; goals by deterministic finite-word automata (DFAs), since finite-horizon logics such as LTLf and LDLf are reasoned about through conversion to equivalent DFAs. This allow us characterize the complexity of the problem as PSPACE complete.},
  archive   = {C_AAMAS},
  author    = {Rajasekaran, Senthil and Vardi, Moshe Y.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1046–1054},
  title     = {Nash equilibria in finite-horizon multiagent concurrent games},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464074},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Peer-to-peer autonomous agent communication network.
<em>AAMAS</em>, 1037–1045. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reliable and secure communication between heterogeneously resourced autonomous agents controlled by competing stakeholders in a decentralized environment is a challenge. Agents require a means to find each other and communicate without reliance on a centralized party and participation in the system must be permissionless. We present the Agent Communication Network (ACN), a peer-to-peer lookup system that provides a distributed overlay to the Internet and addresses this problem. The ACN enables agents to find each other and to communicate safely. It achieves this by leveraging a distributed hash table for agent lookup, maintained by participating peers and through the use of public-key cryptography. The paper discusses the properties of the system and its guarantees as well as its integration with a novel multi-agent system. Preliminary benchmark results demonstrate the feasibility of the system, its performance, and its scalability.},
  archive   = {C_AAMAS},
  author    = {Rahmani, Lokman and Minarsch, David and Ward, Jonathan},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1037–1045},
  title     = {Peer-to-peer autonomous agent communication network},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464073},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Accelerating recursive partition-based causal structure
learning. <em>AAMAS</em>, 1028–1036. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Causal structure discovery from observational data is fundamental to the causal understanding of autonomous systems such as medical decision support systems, advertising campaigns and self-driving cars. This is essential to solve well-known causal decision making and prediction problems associated with those real-world applications. Recently, recursive causal discovery algorithms have gained particular attention among the research community due to their ability to provide good results by using Conditional Independent (CI) tests in smaller sub-problems. However, each of such algorithms needs a refinement function to remove undesired causal relations of the discovered graphs. Notably, with the increase of the problem size, the computation cost (i.e., the number of CI-tests) of the refinement function makes an algorithm expensive to deploy in practice. This paper proposes a generic causal structure refinement strategy that can locate the undesired relations with a small number of CI-tests, thus speeding up the algorithm for large and complex problems. We theoretically prove the correctness of our algorithm. We then empirically evaluate its performance against the state-of-the-art algorithms in terms of solution quality and completion time in synthetic and real datasets.},
  archive   = {C_AAMAS},
  author    = {Rahman, Md. Musfiqur and Rasheed, Ayman and Khan, Md. Mosaddek and Javidian, Mohammad Ali and Jamshidi, Pooyan and Mamun-Or-Rashid, Md.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1028–1036},
  title     = {Accelerating recursive partition-based causal structure learning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464072},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Latency-aware local search for distributed constraint
optimization. <em>AAMAS</em>, 1019–1027. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most studies investigating models and algorithms for distributed constraint optimization problems (DCOPs) assume messages arrive instantaneously or within a (short) bounded delay.Specifically, distributed local search DCOP algorithms have been designed as synchronous algorithms, performing in an asynchronous environment, i.e., algorithms that perform in synchronous iterations in which each agent exchanges messages with all its neighbors. This is true also for an anytime mechanism that reports the best solution explored during the run of synchronous distributed local search algorithms. Thus, when the assumptions on instantaneous message arrival are relaxed, the state of the art local search algorithms and mechanism do not apply.In this work, we address this limitation by: (1) Investigating the performance of existing local search DCOP algorithms in the presence of message latency; (2) Proposing an asynchronous monotonic distributed local search DCOP algorithm; and (3) Proposing an asynchronous anytime framework for reporting the best solution explored by non-monotonic asynchronous local search DCOP algorithms. Our empirical results demonstrate that, up to some extent, message delays have a positive effect on distributed local search algorithms due to increased exploration. The asynchronous anytime framework proposed, allows a maximal benefit from such latency based explorative heuristics.},
  archive   = {C_AAMAS},
  author    = {Rachmut, Ben and Zivan, Roie and Yeoh, William},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1019–1027},
  title     = {Latency-aware local search for distributed constraint optimization},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464071},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An agent-based model to predict pedestrians trajectories
with an autonomous vehicle in shared spaces. <em>AAMAS</em>, 1010–1018.
(<a href="https://dl.acm.org/doi/10.5555/3463952.3464070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses modeling and simulating pedestrian trajectories when interacting with an autonomous vehicle in a shared space. Pedestrian motion models integrating pedestrians interactions with an autonomous vehicle already exist. However, they fail to accurately predict the individual trajectory of each pedestrian, and they do not deal with the diversity of possible pedestrian interactions with the vehicle in a shared space (front, back or lateral). Moreover, previous works do not sufficiently provide a quantitative evaluation of the model&#39;s predictions. In this paper, we propose an hybrid pedestrian model that combines the social force model and a new decision model for conflicting pedestrian-vehicle interactions. The proposed model integrates different observed pedestrians behaviors, as well as the behaviors of the social groups of pedestrians. We validate the model and evaluate its predictive potential through qualitative and quantitative comparisons with ground truth trajectories. The proposed model reproduces observed behaviors that have not been replicated by the social force model and outperforms the social force model at predicting pedestrians trajectories on the used dataset. This model will be used by an autonomous vehicle in a shared space to predict the trajectories of surrounding pedestrians.},
  archive   = {C_AAMAS},
  author    = {Pr\&#39;{e}dhumeau, Manon and Mancheva, Lyuba and Dugdale, Julie and Spalanzani, Anne},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1010–1018},
  title     = {An agent-based model to predict pedestrians trajectories with an autonomous vehicle in shared spaces},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464070},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Group fairness for knapsack problems. <em>AAMAS</em>,
1001–1009. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the knapsack problem with group fairness constraints. The input of the problem consists of a knapsack of bounded capacity and a set of items. Each item belongs to a particular category and has an associated weight and value. The goal of this problem is to select a subset of items such that all categories are fairly represented, the total weight of the selected items does not exceed the capacity of the knapsack, and the total value is maximized. We study the fairness parameters such as the bounds on the total value of items from each category, the total weight of items from each category, and the total number of items from each category. We give approximation algorithms for these problems. We also give experimental validation for the efficiency of our algorithms. These fairness notions could also be extended to the min-knapsack problem. The fair knapsack problems encompass various important problems, such as participatory budgeting, fair budget allocation, and advertising.},
  archive   = {C_AAMAS},
  author    = {Patel, Deval and Khan, Arindam and Louis, Anand},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1001–1009},
  title     = {Group fairness for knapsack problems},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464069},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Active screening for recurrent diseases: A reinforcement
learning approach. <em>AAMAS</em>, 992–1000. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Active screening is a common approach in controlling the spread of recurring infectious diseases such as tuberculosis and influenza. In this approach, health workers periodically select a subset of population for screening. However, given the limited number of health workers, only a small subset of the population can be visited in any given time period. Given the recurrent nature of the disease and rapid spreading, the goal is to minimize the number of infections over a long time horizon. Active screening can be formalized as a sequential combinatorial optimization over the network of people and their connections. The main computational challenges in this formalization arise from i) the combinatorial nature of the problem, ii) the need of sequential planning and iii) the uncertainties in the infectiousness states of the population. Previous works on active screening fail to scale to large time horizon while fully considering the future effect of current interventions. In this paper, we propose a novel reinforcement learning (RL) approach based on Deep Q-Networks (DQN), with several innovative adaptations that are designed to address the above challenges. First, we use graph convolutional networks (GCNs) to represent the Q-function that exploit the node correlations of the underlying contact network. Second, to avoid solving a combinatorial optimization problem in each time period, we decompose the node set selection as a sub-sequence of decisions, and further design a two-level RL framework that solves the problem in a hierarchical way. Finally, to speed-up the slow convergence of RL which arises from reward sparseness, we incorporate ideas from curriculum learning into our hierarchical RL approach. We evaluate our RL algorithm on several real-world networks. Results show that our RL algorithm can scale up to 10 times the problem size of state-of-the-art (the variant that considers the effect of future interventions but unscalable) in terms of planning time horizon. Meanwhile, it outperforms state-of-the-art (the variant that scales up but does not consider the effect of future interventions) by up to 33\% in solution quality.},
  archive   = {C_AAMAS},
  author    = {Ou, Han-Ching and Chen, Haipeng and Jabbari, Shahin and Tambe, Milind},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {992–1000},
  title     = {Active screening for recurrent diseases: A reinforcement learning approach},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464068},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Safe pareto improvements for delegated game playing.
<em>AAMAS</em>, 983–991. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A set of players delegate playing a game to a set of representatives, one for each player. We imagine that each player trusts their respective representative&#39;s strategic abilities. Thus, we might imagine that per default, the original players would simply instruct the representatives to play the original game as best as they can. In this paper, we ask: are there safe Pareto improvements on this default way of giving instructions? That is, we imagine that the original players can coordinate to tell their representatives to only consider some subset of the available strategies and to assign utilities to outcomes differently than the original players. Then can the original players do this in such a way that the payoff is guaranteed to be weakly higher than under the default instructions for all the original players? In particular, can they Pareto-improve without probabilistic assumptions about how the representatives play games? In this paper, we give some examples of safe Pareto improvements. We prove that the notion of safe Pareto improvements is closely related to a notion of outcome correspondence between games. We also show that under some specific assumptions about how the representatives play games, finding safe Pareto improvements is NP-complete.},
  archive   = {C_AAMAS},
  author    = {Oesterheld, Caspar and Conitzer, Vincent},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {983–991},
  title     = {Safe pareto improvements for delegated game playing},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464067},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Emergent communication under competition. <em>AAMAS</em>,
974–982. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The literature in modern machine learning has only negative results for learning to communicate between competitive agents using standard RL. We introduce a modified sender-receiver game to study the spectrum of partially-competitive scenarios and show communication can indeed emerge in a competitive setting. We empirically demonstrate three key takeaways for future research. First, we show that communication is proportional to cooperation, and it can occur for partially competitive scenarios using standard learning algorithms. Second, we highlight the difference between communication and manipulation and extend previous metrics of communication to the competitive case. Third, we investigate the negotiation game where previous work failed to learn communication between independent agents (Cao et al, 2018). We show that, in this setting, both agents must benefit from communication for it to emerge; and, with a slight modification to the game, we demonstrate successful communication between competitive agents. We hope this work overturns misconceptions and inspires more research in competitive emergent communication. Code is available at https://github.com/mnoukhov/emergent-compete},
  archive   = {C_AAMAS},
  author    = {Noukhovitch, Michael and LaCroix, Travis and Lazaridou, Angeliki and Courville, Aaron},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {974–982},
  title     = {Emergent communication under competition},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464066},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-agent graph-attention communication and teaming.
<em>AAMAS</em>, 964–973. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {High-performing teams learn effective communication strategies to judiciously share information and reduce the cost of communication overhead. Within multi-agent reinforcement learning, synthesizing effective policies requires reasoning about when to communicate, whom to communicate with, and how to process messages. We propose a novel multi-agent reinforcement learning algorithm, Multi-Agent Graph-attentIon Communication (MAGIC), with a graph-attention communication protocol in which we learn 1) a Scheduler to help with the problems of when to communicate and whom to address messages to, and 2) a Message Processor using Graph Attention Networks (GATs) with dynamic graphs to deal with communication signals. The Scheduler consists of a graph attention encoder and a differentiable attention mechanism, which outputs dynamic, differentiable graphs to the Message Processor, which enables the Scheduler and Message Processor to be trained end-to-end. We evaluate our approach on a variety of cooperative tasks, including Google Research Football. Our method outperforms baselines across all domains, achieving ~10.5\% increase in reward in the most challenging domain. We also show MAGIC communicates $27.4\%$ more efficiently on average than baselines, is robust to stochasticity, and scales to larger state-action spaces. Finally, we demonstrate MAGIC on a physical, multi-robot testbed.},
  archive   = {C_AAMAS},
  author    = {Niu, Yaru and Paleja, Rohan and Gombolay, Matthew},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {964–973},
  title     = {Multi-agent graph-attention communication and teaming},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464065},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adversarial learning in revenue-maximizing auctions.
<em>AAMAS</em>, 955–963. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a new numerical framework to learn optimal bidding strategies in repeated auctions when the seller uses past bids to optimize her mechanism. Crucially, we do not assume that the bidders know which optimization mechanism is used by the seller. We recover essentially all state-of-the-art analytical results for the single-item framework derived previously in the setup where the bidder knows the optimization mechanism used by the seller and extend our approach to multi-item settings, in which no optimal shading strategies were previously known. Our approach yields substantial increases in bidder utility in all settings and has a strong potential for practical usage since it provides a simple way to optimize bidding strategies on modern marketplaces where buyers face unknown data-driven mechanisms.},
  archive   = {C_AAMAS},
  author    = {Nedelec, Thomas and Baudet, Jules and Perchet, Vianney and El Karoui, Noureddine},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {955–963},
  title     = {Adversarial learning in revenue-maximizing auctions},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464064},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reward machines for cooperative multi-agent reinforcement
learning. <em>AAMAS</em>, 934–942. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In cooperative multi-agent reinforcement learning, a collection of agents learns to interact in a shared environment to achieve a common goal. We propose the use of reward machines (RM) -- Mealy machines used as structured representations of reward functions -- to encode the team&#39;s task. The proposed novel interpretation of RMs in the multi-agent setting explicitly encodes required teammate interdependencies, allowing the team-level task to be decomposed into sub-tasks for individual agents. We define such a notion of RM decomposition and present algorithmically verifiable conditions guaranteeing that distributed completion of the sub-tasks leads to team behavior accomplishing the original task. This framework for task decomposition provides a natural approach to decentralized learning: agents may learn to accomplish their sub-tasks while observing only their local state and abstracted representations of their teammates. We accordingly propose a decentralized q-learning algorithm. Furthermore, in the case of undiscounted rewards, we use local value functions to derive lower and upper bounds for the global value function corresponding to the team task. Experimental results in three discrete settings exemplify the effectiveness of the proposed RM decomposition approach, which converges to a successful team policy an order of magnitude faster than a centralized learner and significantly outperforms hierarchical and independent q-learning approaches.},
  archive   = {C_AAMAS},
  author    = {Neary, Cyrus and Xu, Zhe and Wu, Bo and Topcu, Ufuk},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {934–942},
  title     = {Reward machines for cooperative multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464063},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A novelty-centric agent architecture for changing worlds.
<em>AAMAS</em>, 925–933. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Open-world AI requires artificial agents to cope with novelties that arise during task performance, i.e., they must (1) detect novelties, (2) characterize them, in order to (3) accommodate them, especially in cases where sudden changes to the environment make task accomplishment impossible without utilizing the novelty. We present a formal framework and implementation thereof in a cognitive agent for novelty handling and demonstrate the efficacy of the proposed methods for detecting and handling a large set of novelties in a crafting task in a simulated environment. We discuss the success of the proposed knowledge-based methods and propose heuristic extensions that will further improve novelty handling in open-worlds tasks.},
  archive   = {C_AAMAS},
  author    = {Muhammad, Faizan and Sarathy, Vasanth and Tatiya, Gyan and Goel, Shivam and Gyawali, Saurav and Guaman, Mateo and Sinapov, Jivko and Scheutz, Matthias},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {925–933},
  title     = {A novelty-centric agent architecture for changing worlds},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464062},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). ELVIRA: An explainable agent for value and utility-driven
multiuser privacy. <em>AAMAS</em>, 916–924. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Online social networks fail to support users to adequately share co-owned content, which leads to privacy violations. Scholars proposed collaborative mechanisms to support users, but they did not satisfy one or more requirements needed according to empirical evidence in this domain, such as explainability, role-agnosticism, adaptability, and being utility- and value-driven. We present ELVIRA, an agent that supports multiuser privacy, whose design meets all these requirements. By considering the sharing preferences and the moral values of users, ELVIRA identifies the optimal sharing policy. Furthermore, ELVIRA justifies the optimality of the solution through explanations based on argumentation. We prove via simulations that ELVIRA provides solutions with the best trade-off between individual utility and value adherence. We also show through a user study that ELVIRA suggests solutions that are more acceptable than existing approaches and that its explanations are also more satisfactory.},
  archive   = {C_AAMAS},
  author    = {Mosca, Francesca and Such, Jose M.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {916–924},
  title     = {ELVIRA: An explainable agent for value and utility-driven multiuser privacy},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464061},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Value-guided synthesis of parametric normative systems.
<em>AAMAS</em>, 907–915. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years there has been an increasing interest in ensuring that autonomous systems behave consistently with human values. A popular approach to this challenge is through the incorporation of norms that regulate behaviour in an ethical way. However, such norms must be effective at promoting the values we consider most important. In this work, we introduce a systematic methodology for the automated synthesis of parametric normative systems based on value promotion. We introduce the new concepts of Shapley values of norms and value compatibility. To quantify the effectiveness of norms at upholding the values we consider relevant, we adopt the value alignment indicator from a previously established framework. We apply our model to a toy system which we use to illustrate our approach from end to end.},
  archive   = {C_AAMAS},
  author    = {Montes, Nieves and Sierra, Carles},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {907–915},
  title     = {Value-guided synthesis of parametric normative systems},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464060},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cooperation between independent reinforcement learners under
wealth inequality and collective risks. <em>AAMAS</em>, 898–906. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study how wealth inequality influences behavioral dynamics in groups of independent reinforcement learners facing a threshold public goods dilemma with uncertain returns. The game allows individuals to contribute or not to a common pool to reduce their chances of future losses. The non-linearity introduced by the threshold, the stochasticity introduced by the risk and the wealth heterogeneity of players result in a game setting with multiple equilibria. We find that the learners&#39; dynamics in this case play a major role in determining the attained equilibrium point. Our results suggest that, under individual-based learning, wealth inequality can have sizable effects on the emerging collective behaviors, decreasing the overall chances of group success. Moreover, we compute the class-based Nash equilibria (i.e., where same wealth-class agents are assumed to play the same strategy) for this game and compare the performance of groups composed of independent learning agents with the performance obtained under the payoff maximizing class-based Nash equilibrium. We find that the learned strategies never really match optimal performance for all tested values of risk.},
  archive   = {C_AAMAS},
  author    = {Merhej, Ramona and Santos, Fernando P. and Melo, Francisco S. and Santos, Francisco C.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {898–906},
  title     = {Cooperation between independent reinforcement learners under wealth inequality and collective risks},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464059},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Identification of unexpected decisions in partially
observable monte-carlo planning: A rule-based approach. <em>AAMAS</em>,
889–897. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Partially Observable Monte-Carlo Planning (POMCP) is a powerful online algorithm able to generate approximate policies for large Partially Observable Markov Decision Processes. The online nature of this method supports scalability by avoiding complete policy representation. The lack of an explicit representation however hinders interpretability. In this work, we propose a methodology based on Satisfiability Modulo Theory (SMT) for analyzing POMCP policies by inspecting their traces, namely sequences of belief-action-observation triplets generated by the algorithm. The proposed method explores local properties of policy behavior to identify unexpected decisions. We propose an iterative process of trace analysis consisting of three main steps, i) the definition of a question by means of a parametric logical formula describing (probabilistic) relationships between beliefs and actions, ii) the generation of an answer by computing the parameters of the logical formula that maximize the number of satisfied clauses (solving a MAX-SMT problem), iii) the analysis of the generated logical formula and the related decision boundaries for identifying unexpected decisions made by POMCP with respect to the original question. We evaluate our approach on Tiger, a standard benchmark for POMDPs, and a real-world problem related to mobile robot navigation. Results show that the approach can exploit human knowledge on the domain, outperforming state-of-the-art anomaly detection methods in identifying unexpected decisions. An improvement of the Area Under Curve up to 47\% has been achieved in our tests.},
  archive   = {C_AAMAS},
  author    = {Mazzi, Giulio and Castellini, Alberto and Farinelli, Alessandro},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {889–897},
  title     = {Identification of unexpected decisions in partially observable monte-carlo planning: A rule-based approach},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464058},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Risk-aware interventions in public health: Planning with
restless multi-armed bandits. <em>AAMAS</em>, 880–888. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Community Health Workers (CHWs) form an important component of health-care systems globally, especially in low-resource settings. CHWs are often tasked with monitoring the health of and intervening on their patient cohort. Previous work has developed several classes of Restless Multi-Armed Bandits (RMABs) that are computationally tractable and indexable, a condition that guarantees asymptotic optimality, for solving such health monitoring and intervention problems (HMIPs). However, existing solutions to HMIPs fail to account for risk-sensitivity considerations of CHWs in the planning stage and may run the danger of ignoring some patients completely because they are deemed less valuable to intervene on. Additionally, these also rely on patients reporting their state of adherence accurately when intervened upon. Towards tackling these issues, our contributions in this paper are as follows: (1) We develop an RMAB solution to HMIPs that allows for reward functions that are monotone increasing, rather than linear, in the belief state and also supports a wider class of observations. (2) We prove theoretical guarantees on the asymptotic optimality of our algorithm for any arbitrary reward function. Additionally, we show that for the specific reward function considered in previous work, our theoretical conditions are stronger than the state-of-the-art guarantees. (3) We show the applicability of these new results for addressing the three issues pertaining to: risk-sensitive planning, equitable allocation and reliance on perfect observations as highlighted above. We evaluate these techniques on both simulated as well as real data from a prevalent CHW task of monitoring adherence of tuberculosis patients to their prescribed medication in Mumbai, India and show improved performance over the state-of-the-art. The simulation code is available at: https://github.com/AdityaMate/risk-aware-bandits.},
  archive   = {C_AAMAS},
  author    = {Mate, Aditya and Perrault, Andrew and Tambe, Milind},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {880–888},
  title     = {Risk-aware interventions in public health: Planning with restless multi-armed bandits},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464057},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Extended goal recognition: A planning-based model for
strategic deception. <em>AAMAS</em>, 871–879. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Goal recognition is the problem of determining an agent&#39;s intent by observing its actions. In the context of AI research, the problem is tackled for two quite different purposes: to determine an agent&#39;s most probable goal or, for human-aware planning including planned---or strategic---deception, to determine an observer&#39;s most likely belief about that goal. Making no distinction, contemporary models tend to assume an infallible observer, deceived only while it has limited access to information or if the environment itself is only partially observable. Focusing on the second purpose, we propose an extended framework that incorporates formal definitions of confirmation bias, selective attention and memory decay. In contrast to pre-existing models, our approach combines explicit consideration of prior probabilities with a principled representation of observer confidence and distinguishes between potential observations---i.e., every observable event within the observer&#39;s frame of reference---and recalled observations which we model as a function of attention and memory. We show that when these factors are taken into consideration, false beliefs may arise and can be made to persist, even in a fully observable environment---thus providing a perceptual model readily incorporated into the &quot;thinking&quot; of an adversarial agent for the purpose of strategic deception.},
  archive   = {C_AAMAS},
  author    = {Masters, Peta and Kirley, Michael and Smith, Wally},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {871–879},
  title     = {Extended goal recognition: A planning-based model for strategic deception},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464056},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). To hold or not to hold? - reducing passenger missed
connections in airlines using reinforcement learning. <em>AAMAS</em>,
862–870. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Missed connections at transit airports are a source of both poor customer experience and reduced airline operational efficiency. Airlines typically handle missed connections by rebooking customers. Recently, airlines have started holding departing flights for some time in a rule-based manner to avoid missed connections. However, rule-based heuristics typically use information local to a flight and do not learn in a globally informed way across the entire network.We complement existing approaches by learning a policy for holding a flight to avoid misconnections, using reinforcement learning (RL). The state presented to the RL agent uses forecasted flight-specific context; and measured network-wide context. The reward uses components that trade off the decrease in on-time performance due to the hold decisions, for a decrease in missed connections. We attribute the global rewards to individual local hold actions through a novel delay tree that approximates the network interactions. Multiple flights are handled through the same instance of the agent handling them in sequence with varying state information.We evaluate our approach for two different airlines with training and testing over a microsimulator that uses real-world data for calibration. Across different algorithms (DQN, AC, A2C, DDPG), we find that the best performing RL-based agent is able to reduce significantly more (up to 50\%) missed connections for a minimal decrease (~5\%) in on-time performance; when compared with a current rule-based heuristic. Further, the approach is tunable and able to transfer learn across different airlines.},
  archive   = {C_AAMAS},
  author    = {Malladi, Tejasvi and Murugappan, Karpagam and Sudarsanam, Depak and Suriyanarayanan, Ramasubramanian and Vasan, Arunchandar},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {862–870},
  title     = {To hold or not to hold? - reducing passenger missed connections in airlines using reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464055},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling the interaction between agents in cooperative
multi-agent reinforcement learning. <em>AAMAS</em>, 853–861. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Value-based methods of multi-agent reinforcement learning (MARL), especially the value decomposition methods, have been demonstrated on a range of challenging cooperative tasks. However, current methods pay little attention to the interaction between agents, which is essential to teamwork in games or real life. This limits the efficiency of value-based MARL algorithms in the two aspects: collaborative exploration and value function estimation. In this paper, we propose a novel cooperative MARL algorithm named as interactive actor-critic (IAC), which models the interaction of agents from the perspectives of policy and value function. On the policy side, a multi-agent joint stochastic policy is introduced by adopting a collaborative exploration module, which is trained by maximizing the entropy-regularized expected return. On the value side, we use the shared attention mechanism to estimate the value function of each agent, which takes the impact of the teammates into consideration. At the implementation level, we extend the value decomposition methods to continuous control tasks and evaluate IAC on benchmark tasks including classic control and multi-agent particle environments. Experimental results indicate that our method outperforms the state-of-the-art approaches and achieves better performance in terms of cooperation.},
  archive   = {C_AAMAS},
  author    = {Ma, Xiaoteng and Yang, Yiqin and Li, Chenghao and Lu, Yiwen and Zhao, Qianchuan and Yang, Jun},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {853–861},
  title     = {Modeling the interaction between agents in cooperative multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464054},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Contrasting centralized and decentralized critics in
multi-agent reinforcement learning. <em>AAMAS</em>, 844–852. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Centralized Training for Decentralized Execution, where agents are trained offline using centralized information but execute in a decentralized manner online, has gained popularity in the multi-agent reinforcement learning community. In particular, actor-critic methods with a centralized critic and decentralized actors are a common instance of this idea. However, the implications of using a centralized critic in this context are not fully discussed and understood even though it is the standard choice of many algorithms. We therefore formally analyze centralized and decentralized critic approaches, providing a deeper understanding of the implications of critic choice. Because our theory makes unrealistic assumptions, we also empirically compare the centralized and decentralized critic methods over a wide set of environments to validate our theories and to provide practical advice. We show that there exist misconceptions regarding centralized critics in the current literature and show that the centralized critic design is not strictly beneficial, but rather both centralized and decentralized critics have different pros and cons that should be taken into account by algorithm designers.},
  archive   = {C_AAMAS},
  author    = {Lyu, Xueguang and Xiao, Yuchen and Daley, Brett and Amato, Christopher},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {844–852},
  title     = {Contrasting centralized and decentralized critics in multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464053},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Exploration of indoor environments through predicting the
layout of partially observed rooms. <em>AAMAS</em>, 836–843. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider exploration tasks in which an autonomous mobile robot incrementally builds maps of initially unknown indoor environments. In such tasks, the robot makes a sequence of decisions on where to move next that, usually, are based on knowledge about the observed parts of the environment. In this paper, we present an approach that exploits a prediction of the geometric structure of the unknown parts of an environment to improve exploration performance. In particular, we leverage an existing method that reconstructs the layout of an environment starting from a partial grid map and that predicts the shape of partially observed rooms on the basis of geometric features representing the regularities of the indoor environment. Then, we originally employ the predicted layout to estimate the amount of new area the robot would observe from candidate locations in order to inform the selection of the next best location and to early stop the exploration when no further relevant area is expected to be discovered. Experimental activities show that our approach is able to exploit the predicted layout of partially observed rooms in order to speed up the exploration.},
  archive   = {C_AAMAS},
  author    = {Luperto, Matteo and Fochetta, Luca and Amigoni, Francesco},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {836–843},
  title     = {Exploration of indoor environments through predicting the layout of partially observed rooms},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464052},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A logic of evaluation. <em>AAMAS</em>, 827–835. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a logic of evaluation which clarifies the relationship between knowledge, values and preferences of multiple agents in an interactive setting. Evaluation is a fundamental concept for understanding how an ethical agent&#39;s decision is affected by her values. We provide a complete axiomatics for the logic and present a dynamic extension by the concept of value expansion. We show that value expansion indirectly affects the agents&#39; preferences by inducing a preference upgrade operation.},
  archive   = {C_AAMAS},
  author    = {Lorini, Emiliano},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {827–835},
  title     = {A logic of evaluation},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464051},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deceptive reinforcement learning for privacy-preserving
planning. <em>AAMAS</em>, 818–826. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we study the problem of deceptive reinforcement learning to preserve the privacy of a reward function. Reinforcement learning is the problem of finding a behaviour policy based on rewards received from exploratory behaviour. A key ingredient in reinforcement learning is a reward function, which determines how much reward (negative or positive) is given and when. However, in some situations, we may want to keep a reward function private; that is, to make it difficult for an observer to determine the reward function used. We define the problem of privacy-preserving reinforcement learning, and present two models for solving it. These models are based on dissimulation -- a form of deception that &#39;hides the truth&#39;. We evaluate our models both computationally and via human behavioural experiments. Results show that the resulting policies are indeed deceptive, and that participants can determine the true reward function less reliably than that of an honest agent.},
  archive   = {C_AAMAS},
  author    = {Liu, Zhengshang and Yang, Yue and Miller, Tim and Masters, Peta},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {818–826},
  title     = {Deceptive reinforcement learning for privacy-preserving planning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464050},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Energy-based imitation learning. <em>AAMAS</em>, 809–817.
(<a href="https://dl.acm.org/doi/10.5555/3463952.3464049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We tackle a common scenario in imitation learning (IL), where agents try to recover the optimal policy from expert demonstrations without further access to the expert or environment reward signals. Except the simple Behavior Cloning (BC) that adopts supervised learning followed by the problem of compounding error, previous solutions like inverse reinforcement learning (IRL) and recent generative adversarial methods involve a bi-level or alternating optimization for updating the reward function and the policy, suffering from high computational cost and training instability. Inspired by recent progress in energy-based model (EBM), in this paper, we propose a simplified IL framework named Energy-Based Imitation Learning (EBIL). Instead of updating the reward and policy iteratively, EBIL breaks out of the traditional IRL paradigm by a simple and flexible two-stage solution: first estimating the expert energy as the surrogate reward function through score matching, then utilizing such a reward for learning the policy by reinforcement learning algorithms. EBIL combines the idea of both EBM and occupancy measure matching, and via theoretic analysis we reveal that EBIL and Max-Entropy IRL (MaxEnt IRL) approaches are two sides of the same coin, and thus EBIL could be an alternative of adversarial IRL methods. Extensive experiments on qualitative and quantitative evaluations indicate that EBIL is able to recover meaningful and interpretative reward signals while achieving effective and comparable performance against existing algorithms on IL benchmarks.},
  archive   = {C_AAMAS},
  author    = {Liu, Minghuan and He, Tairan and Xu, Minkai and Zhang, Weinan},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {809–817},
  title     = {Energy-based imitation learning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464049},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Axies: Identifying and evaluating context-specific values.
<em>AAMAS</em>, 799–808. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The pursuit of values drives human behavior and promotes cooperation. Existing research is focused on general (e.g., Schwartz) values that transcend contexts. However, context-specific values are necessary to (1) understand human decisions, and (2) engineer intelligent agents that can elicit human values and take value-aligned actions.We propose Axies, a hybrid (human and AI) methodology to identify context-specific values. Axies simplifies the abstract task of value identification as a guided value annotation process involving human annotators. Axies exploits the growing availability of value-laden text corpora and Natural Language Processing to assist the annotators in systematically identifying context-specific values.We evaluate Axies in a user study involving 60 subjects. In our study, six annotators generate value lists for two timely and important contexts: COVID-19 measures, and sustainable energy. Then, two policy experts and 52 crowd workers evaluate Axies value lists. We find that Axies yields values that are context-specific, consistent across different annotators, and comprehensible to end users.},
  archive   = {C_AAMAS},
  author    = {Liscio, Enrico and van der Meer, Michiel and Siebert, Luciano C. and Jonker, Catholijn M. and Mouter, Niek and Murukannaiah, Pradeep K.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {799–808},
  title     = {Axies: Identifying and evaluating context-specific values},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464048},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Let the DOCTOR decide whom to test: Adaptive testing
strategies to tackle the COVID-19 pandemic. <em>AAMAS</em>, 790–798. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A robust testing program is necessary for containing the spread of COVID-19 infections before a vaccine becomes available. However, due to an acute shortage of testing kits (especially in low-resource developing countries), designing an optimal testing program/strategy is a challenging problem to solve. Prior literature on testing strategies suffers from two major limitations: (i) it does not account for the trade-off between testing of symptomatic and asymptomatic individuals, and (ii) it primarily focuses on static testing strategies, which leads to significant shortcomings in the testing program&#39;s effectiveness. In this paper, we address these limitations by making five novel contributions. (i) We formally define the optimal testing problem and propose the DOCTOR POMDP model to tackle it. (ii) We solve the DOCTOR POMDP using a scalable Monte Carlo tree search based algorithm. (iii) We provide a rigorous experimental analysis of DOCTOR&#39;s testing strategies against static baselines - our results show that when applied to the city of Santiago in Panama, DOCTOR&#39;s strategies result in approximately 40\% fewer COVID-19 infections (over one month) as compared to state-of-the-art static baselines. (iv) In addition, we analyze DOCTOR&#39;s testing policy to derive insights about the reasons behind the optimality of DOCTOR&#39;s testing policy. (v) Finally, we characterize conditions (of the real world) under which DOCTOR&#39;s optimization would be of most benefit to government policy makers, and thus requires significant attention from researchers in this area. Our work complements the growing body of research on COVID-19, and serves as a proof-of-concept that illustrates the benefit of having an AI-driven adaptive testing strategy for COVID-19.},
  archive   = {C_AAMAS},
  author    = {Liang, Yu and Yadav, Amulya},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {790–798},
  title     = {Let the DOCTOR decide whom to test: Adaptive testing strategies to tackle the COVID-19 pandemic},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464047},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Parallel curriculum experience replay in distributed
reinforcement learning. <em>AAMAS</em>, 782–789. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Distributed training architectures have been shown to be effective to improve the performance of reinforcement learning algorithms. However, their performances are still poor for problems with sparse rewards, e.g., the scoring task with or without goalkeeper for robots in RoboCup soccer. It is challenging to solve these tasks in reinforcement learning, especially for those that require combining high-level actions with flexible control. To address these challenges, we introduce a distributed training framework with parallel curriculum experience replay that can collect different experiences in parallel and then automatically identify the difficulty of these subtasks. Experiments on the domain of simulated RoboCup soccer show that, the approach is effective and outperforms existing reinforcement learning methods.},
  archive   = {C_AAMAS},
  author    = {Li, Yuyu and Ji, Jianmin},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {782–789},
  title     = {Parallel curriculum experience replay in distributed reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464046},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Structured diversification emergence via reinforced
organization control and hierachical consensus learning. <em>AAMAS</em>,
773–781. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When solving a complex task, humans will spontaneously form teams and to complete different parts of the whole task, respectively. Meanwhile, the cooperation between teammates will improve efficiency. However, for current cooperative MARL methods, the cooperation team is constructed through either heuristics or end-to-end blackbox optimization. In order to improve the efficiency of cooperation and exploration, we propose a structured diversification emergence MARL framework named Rochico based on reinforced organization control and hierarchical consensus learning. Rochico first learns an adaptive grouping policy through the organization control module, which is established by independent multi-agent reinforcement learning. Further, the hierarchical consensus module based on the hierarchical intentions with consensus constraint is introduced after team formation. Simultaneously, utilizing the hierarchical consensus module and a self-supervised intrinsic reward enhanced decision module, the proposed cooperative MARL algorithm Rochico can output the final diversified multi-agent cooperative policy.},
  archive   = {C_AAMAS},
  author    = {Li, Wenhao and Wang, Xiangfeng and Jin, Bo and Sheng, Junjie and Hua, Yun and Zha, Hongyuan},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {773–781},
  title     = {Structured diversification emergence via reinforced organization control and hierachical consensus learning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464045},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Deep implicit coordination graphs for multi-agent
reinforcement learning. <em>AAMAS</em>, 764–772. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent reinforcement learning (MARL) requires coordination to efficiently solve certain tasks. Fully centralized control is often infeasible in such domains due to the size of joint action spaces. Coordination graph based formalization allows reasoning about the joint action based on the structure of interactions. However,they often require domain expertise in their design. This paper introduces the deep implicit coordination graph (DICG) architecture for such scenarios. DICG consists of a module for inferring the dynamic coordination graph structure which is then used by a graph neural network based module to learn to implicitly reason about the joint actions or values. DICG allows learning the tradeoff between full centralization and decentralization via standard actor-critic methods to significantly improve coordination for domains with large number of agents. We apply DICG to both centralized-training-centralized-execution and centralized-training-decentralized-execution regimes. We demonstrate that DICG solves the relative over generalization pathology in predatory-prey tasks as well as outperforms various MARL baselines on the challenging StarCraft II Multi-agent Challenge (SMAC) and traffic junction environments.},
  archive   = {C_AAMAS},
  author    = {Li, Sheng and Gupta, Jayesh K. and Morales, Peter and Allen, Ross and Kochenderfer, Mykel J.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {764–772},
  title     = {Deep implicit coordination graphs for multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464044},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The price is (probably) right: Learning market equilibria
from samples. <em>AAMAS</em>, 755–763. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Equilibrium computation in markets usually considers settings where player valuation functions are known. We consider the setting where player valuations are unknown; using a PAC learning-theoretic framework, we analyze some classes of common valuation functions, and provide algorithms which output direct PAC equilibrium allocations, not estimates based on attempting to learn valuation functions. Since there exist trivial PAC market outcomes with an unbounded worst-case efficiency loss, we lower-bound the efficiency of our algorithms. While the efficiency loss under general distributions is rather high, we show that in some cases (e.g., unit-demand valuations), it is possible to find a PAC market equilibrium with significantly better utility.},
  archive   = {C_AAMAS},
  author    = {Lev, Omer and Patel, Neel and Viswanathan, Vignesh and Zick, Yair},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {755–763},
  title     = {The price is (Probably) right: Learning market equilibria from samples},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464043},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Aggregating bipolar opinions. <em>AAMAS</em>, 746–754. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a novel method to aggregate Bipolar Argumentation (BA) Frameworks expressing opinions by different parties in debates. We use Bipolar Assumption-based Argumentation (ABA) as an all-encompassing formalism for BA under different semantics. By leveraging on recent results on judgement aggregation in Social Choice Theory, we prove several preservation results, both positive and negative, for relevant properties of Bipolar ABA.},
  archive   = {C_AAMAS},
  author    = {Lauren, Stefan and Belardinelli, Francesco and Toni, Francesca},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {746–754},
  title     = {Aggregating bipolar opinions},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464042},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Approval-based shortlisting. <em>AAMAS</em>, 737–745. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Shortlisting is the task of reducing a long list of alternatives to a (smaller) set of best or most suitable alternatives from which a final winner will be chosen. Shortlisting is often used in the nomination process of awards or in recommender systems to display featured objects. In this paper, we analyze shortlisting methods that are based on approval data, a common type of preferences. Furthermore, we assume that the size of the shortlist, i.e., the number of best or most suitable alternatives, is not fixed but determined by the shortlisting method. We axiomatically analyze established and new shortlisting methods and complement this analysis with an experimental evaluation based on imperfect quality estimates. Our results lead to recommendations which shortlisting methods to use, depending on the desired properties.},
  archive   = {C_AAMAS},
  author    = {Lackner, Martin and Maly, Jan},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {737–745},
  title     = {Approval-based shortlisting},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464041},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Adaptive operating hours for improved performance of taxi
fleets. <em>AAMAS</em>, 728–736. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Taxi fleets and car aggregation systems are an important component of the urban public transportation system. Taxis and cars in taxi fleets and car aggregation systems (e.g., Uber) are dependent on a large number of self-controlled and profit-driven taxi drivers, which introduces inefficiencies in the system. There are two ways in which taxi fleet performance can be optimized: (i) Operational decision making: improve assignment of taxis/cars to customers, while accounting for future demand; (ii) strategic decision making: optimize operating hours of (taxi and car) drivers. Existing research has primarily focused on the operational decisions in (i) and we focus on the strategic decisions in (ii).We first model this complex real world decision making problem (with thousands of taxi drivers) as a multi-stage stochastic congestion game with a non dedicated set of agents (i.e., agents start operation at a random stage and exit the game after a fixed time), where there is a dynamic population of agents (constrained by the maximum number of drivers). We provide planning and learning methods for computing the ideal operating hours in such a game, so as to improve efficiency of the overall fleet. In our experimental results, we demonstrate that our planning based approach provides up to 16\% improvement in revenue over existing method on a real world taxi dataset. The learning based approach further improves the performance and achieves up to 10\% more revenue than the planning approach.},
  archive   = {C_AAMAS},
  author    = {Kumar, Rajiv Ranjan and Varakantham, Pradeep and Cheng, Shih-Fen},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {728–736},
  title     = {Adaptive operating hours for improved performance of taxi fleets},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464040},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Feasible coalition sequences. <em>AAMAS</em>, 719–727. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce the idea of a finite sequence of coalition formation games over a set of agents, and we call it Sequential Characteristic-Function Game (SCFG). We define the solution of such a game as a corresponding sequence of coalition structures that must be related by a given feasibility relation, so no coalition structure can be evaluated in isolation. A sequence satisfying this condition is called Feasible Coalition-Structure Sequence (FCSS). Such games can be a useful abstraction for modelling various scenarios, in particular those for real-world disaster management that we consider in this paper. We give an algorithm for computing an FCSS and evaluate it experimentally. Our results show that an SCFG can represent various classical variations of characteristic-function games, and our algorithm solves instances with a reasonable number of agents.},
  archive   = {C_AAMAS},
  author    = {Krausburg, Tabajara and Dix, J\&quot;{u}rgen and Bordini, Rafael H.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {719–727},
  title     = {Feasible coalition sequences},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464039},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Beyond “to act or not to act”: Fast lagrangian approaches to
general multi-action restless bandits. <em>AAMAS</em>, 710–718. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents new algorithms and theoretical results for solutions to Multi-action Multi-armed Restless Bandits, an important but insufficiently studied generalization of traditional Multi-armed Restless Bandits (MARBs). Though MARBs are popular for modeling many problems, they are restricted to binary actions, i.e., &quot;to act or not to act&quot;. This renders them unable to capture critical complexities faced by planners in real domains, such as a system manager balancing maintenance, repair, and job scheduling, or a health worker deciding among treatments for a given patient. Limited previous work on Multi-action MARBs has only been specialized to sub-problems. Here we derive multiple algorithms for use on general Multi-action MARBs using Lagrangian relaxation techniques, leading to the following contributions: (i) We develop BLam, a bound optimization algorithm which leverages problem convexity to quickly and provably converge to the well-performing Lagrange policy; (ii) We develop SampleLam, a fast sampling technique for estimating the Lagrange policy, and derive a concentration bound to investigate its convergence properties; (iii) We derive best and worst case computational complexities for our algorithms as well as our main competitor; (iv) We provide experimental results comparing our algorithms to baselines on simulated distributions, including one motivated by a real-world community health intervention task. Our approach achieves significant, up to ten-fold speedups over more general methods without sacrificing performance and is widely applicable across general Multi-action MARBs. Code is available at https://github.com/killian-34/MAMARB-Lagrange-Policies.},
  archive   = {C_AAMAS},
  author    = {Killian, Jackson A. and Perrault, Andrew and Tambe, Milind},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {710–718},
  title     = {Beyond &quot;To act or not to act&quot;: Fast lagrangian approaches to general multi-action restless bandits},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464038},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Knowing why — on the dynamics of knowledge about actual
causes in the situation calculus. <em>AAMAS</em>, 701–709. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reasoning about observed effects and their causes is important in many applications. For instance, understanding why a plan failed can aid the task of replanning by allowing the agent to tailor a better plan. But under incomplete information, an agent may be unable to determine which actions/events caused an effect. To overcome this, the agent may be able to perform some sensing actions that allow him to figure out what caused the effect. This becomes even more important in multiagent contexts, where an agent may want to identify which agents caused some effect, or possibly prevent other agents from determining who caused something. The effects involved may even be epistemic effects, such as an agent coming to know the PIN of a bank card, and the causes may be sensing actions. Reasoning about such causes is a key part of &quot;theory of mind&quot; and understanding other agents&#39; behaviour. While there has been much work on causality from an objective standpoint, causality from the point of view of individual agents has received much less attention. In this paper, we develop a formalization of knowledge about actual causes in the situation calculus, and how it is affected by actions including sensing. We show that the proposed framework has some intuitive properties and study the conditions under which an agent can be expected to come to know the causes of an effect.},
  archive   = {C_AAMAS},
  author    = {Khan, Shakil M. and Lesp\&#39;{e}rance, Yves},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {701–709},
  title     = {Knowing why — on the dynamics of knowledge about actual causes in the situation calculus},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464037},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mechanism design for housing markets over social networks.
<em>AAMAS</em>, 692–700. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we investigate the effect of an underlying social network over agents in a well-known multi-agent resource allocation problem; the housing market. We first show that, when a housing market takes place over a social network with more than two agents and these agents have an option to avoid forwarding information about it to their followers, there does not exist an exchange mechanism that simultaneously satisfies strategy-proofness, Pareto efficiency, and individual rationality. It is also impossible to find a strategy-proof exchange mechanism that always chooses an outcome in a weakened core. These results highlight the difficulty of taking into account the agents&#39; incentive of information diffusion in the resource allocation. To overcome these negative results, we consider two different ways of restricting the problem; limiting the domain of preferences and the structure of social networks.},
  archive   = {C_AAMAS},
  author    = {Kawasaki, Takehiro and Wada, Ryoji and Todo, Taiki and Yokoo, Makoto},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {692–700},
  title     = {Mechanism design for housing markets over social networks},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464036},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Committee selection using attribute approvals.
<em>AAMAS</em>, 683–691. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the problem of committee selection from a fixed set of candidates where each candidate has multiple quantifiable attributes. Instead of voting for a candidate, a voter is allowed to approve the preferred attributes of a given candidate. Though attribute-based preferences capture several important real-life scenarios, committee selection problem with attribute approval of voters has not properly formalized or studied. We present a detailed study of axioms, rules, algorithms, and computational complexity for the setting. Apart from extending previous axioms and rules, we design two new algorithms that are especially appropriate for our model.},
  archive   = {C_AAMAS},
  author    = {Kagita, Venkateswara Rao and Pujari, Arun K. and Padmanabhan, Vineet and Aziz, Haris and Kumar, Vikas},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {683–691},
  title     = {Committee selection using attribute approvals},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464035},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Grid-to-graph: Flexible spatial relational inductive biases
for reinforcement learning. <em>AAMAS</em>, 674–682. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although reinforcement learning has been successfully applied in many domains in recent years, we still lack agents that can systematically generalize. While relational inductive biases that fit a task can improve generalization of RL agents, these biases are commonly hard-coded directly in the agent&#39;s neural architecture. In this work, we show that we can incorporate relational inductive biases, encoded in the form of relational graphs, into agents. Based on this insight, we propose Grid-to-Graph (GTG), a mapping from grid structures to relational graphs that carry useful spatial relational inductive biases when processed through a Relational Graph Convolution Network (R-GCN). We show that, with GTG, R-GCNs generalize better both in terms of in-distribution and out-of-distribution compared to baselines based on Convolutional Neural Networks and Neural Logic Machines on challenging procedurally generated environments and MinAtar. Furthermore, we show that GTG produces agents that can jointly reason over observations and environment dynamics encoded in knowledge bases.},
  archive   = {C_AAMAS},
  author    = {Jiang, Zhengyao and Minervini, Pasquale and Jiang, Minqi and Rockt\&quot;{a}schel, Tim},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {674–682},
  title     = {Grid-to-graph: Flexible spatial relational inductive biases for reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464034},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Partition aggregation for participatory budgeting.
<em>AAMAS</em>, 665–673. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, Jain et al. [IJCAI, 2019] studied the effect of project interactions in participatory budgeting (PB) by assuming an existing partition of the projects to interaction structures, namely a grouping of the projects into substitution and complementarity groups. Motivated by their study, here we take voter preferences to find such interaction structures. In our model, voters submit interaction structures, and the goal is to find an aggregated structure. Formally, given a set P of m projects, and n partitions of P, the task is to aggregate these n partitions into one aggregated partition. We consider this partition aggregation task both for substitution structures and for complementarity structures, studying several aggregation methods for each, including utility-based methods and Condorcet-based methods; we evaluate these methods by analyzing their computational complexity and their behavior with respect to certain relevant axiomatic properties.},
  archive   = {C_AAMAS},
  author    = {Jain, Pallavi and Talmon, Nimrod and Bulteau, Laurent},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {665–673},
  title     = {Partition aggregation for participatory budgeting},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464033},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Trader-company method: A metaheuristics for interpretable
stock price prediction. <em>AAMAS</em>, 656–664. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Investors try to predict returns of financial assets to make successful investment. Many quantitative analysts have used machine learning-based methods to find unknown profitable market rules from large amounts of market data. However, there are several challenges in financial markets hindering practical applications of machine learning-based models. First, in financial markets, there is no single model that can consistently make accurate prediction because traders in markets quickly adapt to newly available information. Instead, there are a number of ephemeral and partially correct models called &quot;alpha factors&quot;. Second, since financial markets are highly uncertain, ensuring interpretability of prediction models is quite important to make reliable trading strategies. To overcome these challenges, we propose the Trader-Company method, a novel evolutionary model that mimics the roles of a financial institute and traders belonging to it. Our method predicts future stock returns by aggregating suggestions from multiple weak learners called Traders. A Trader holds a collection of simple mathematical formulae, each of which represents a candidate of an alpha factor and would be interpretable for real-world investors. The aggregation algorithm, called a Company, maintains multiple Traders. By randomly generating new Traders and retraining them, Companies can efficiently find financially meaningful formulae whilst avoiding overfitting to a transient state of the market. We show the effectiveness of our method by conducting experiments on real market data.},
  archive   = {C_AAMAS},
  author    = {Ito, Katsuya and Minami, Kentaro and Imajo, Kentaro and Nakagawa, Kei},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {656–664},
  title     = {Trader-company method: A metaheuristics for interpretable stock price prediction},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464032},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021b). Probabilistic inference of winners in elections by
independent random voters. <em>AAMAS</em>, 647–655. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate the problem of computing the probability of winning in an election where voter attendance is uncertain. More precisely, we study the setting where, in addition to a total ordering of the candidates, each voter is associated with a probability of attending the poll, and the attendances of different voters are probabilistically independent. We show that the probability of winning can be computed in polynomial time for the plurality and veto rules. However, it is computationally hard (#P-hard) for various other rules, including k-approval and k-veto for $k&amp;gt;1$, Borda, Condorcet, and Maximin. For some of these rules, it is even hard to find a multiplicative approximation since it is already hard to determine whether this probability is nonzero. In contrast, we devise a fully polynomial-time randomized approximation scheme (FPRAS) for the complement probability, namely the probability of losing, for every positional scoring rule (with polynomial scores), as well as for the Condorcet rule.},
  archive   = {C_AAMAS},
  author    = {Imber, Aviram and Kimelfeld, Benny},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {647–655},
  title     = {Probabilistic inference of winners in elections by independent random voters},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464031},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021a). Computing the extremal possible ranks with incomplete
preferences. <em>AAMAS</em>, 638–646. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Various voting rules are based on ranking the candidates by scores induced by aggregating voter preferences. A winner (respectively, unique winner) is a candidate who receives a score not smaller than (respectively, strictly greater than) the remaining candidates. Examples of such rules include the positional scoring rules and the Bucklin, Copeland, and Maximin rules. When voter preferences are known in an incomplete manner as partial orders, a candidate can be a possible/necessary winner based on the possibilities of completing the partial votes. Past research has studied in depth the computational problems of determining the possible and necessary winners and unique winners. These problems are all special cases of reasoning about the range of possible positions of a candidate under different tiebreakers. We investigate the complexity of determining this range, and particularly the extremal positions. Among our results, we establish that finding each of the minimal and maximal positions is NP-hard for each of the above rules, including all positional scoring rules, pure or not. Hence, none of the tractable variants of necessary/possible winner determination remain tractable for extremal position determination. Tractability can be retained when reasoning about the top-k positions for a fixed k. Yet, exceptional is Maximin where it is tractable to decide whether the maximal rank is k for $k=1$ (necessary winning) but it becomes intractable for all $k&amp;gt;1$.},
  archive   = {C_AAMAS},
  author    = {Imber, Aviram and Kimelfeld, Benny},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {638–646},
  title     = {Computing the extremal possible ranks with incomplete preferences},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464030},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Action advising with advice imitation in deep reinforcement
learning. <em>AAMAS</em>, 629–637. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Action advising is a peer-to-peer knowledge exchange technique built on the teacher-student paradigm to alleviate the sample inefficiency problem in deep reinforcement learning. Recently proposed student-initiated approaches have obtained promising results. However, due to being in the early stages of development, these also have some substantial shortcomings. One of the abilities that are absent in the current methods is further utilising advice by reusing, which is especially crucial in the practical settings considering the budget constraints in peer-to-peer interactions. In this study, we present an approach to enable the student agent to imitate previously acquired advice to reuse them directly in its exploration policy, without any interventions in the learning mechanism itself. In particular, we employ a behavioural cloning module to imitate the teacher policy and use dropout regularisation to have a notion of epistemic uncertainty to keep track of which state-advice pairs are actually collected. As the results of experiments we conducted in three Atari games show, advice reusing via imitation is indeed a feasible option in deep RL and our approach can successfully achieve this while significantly improving the learning performance, even when it is paired with a simple early advising heuristic.},
  archive   = {C_AAMAS},
  author    = {Ilhan, Ercument and Gow, Jeremy and Perez Liebana, Diego},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {629–637},
  title     = {Action advising with advice imitation in deep reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464029},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Show me the way: Intrinsic motivation from demonstrations.
<em>AAMAS</em>, 620–628. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The study of exploration in the domain of decision making has a long history but remains actively debated. From the vast literature that addressed this topic for decades under various points of view (e.g., developmental psychology, experimental design, artificial intelligence), intrinsic motivation emerged as a concept that can practically be transferred to artificial agents. Especially, in the recent field of Deep Reinforcement Learning (RL), agents implement such a concept (mainly using a novelty argument) in the shape of an exploration bonus, added to the task reward, that encourages visiting the whole environment. This approach is supported by the large amount of theory on RL for which convergence to optimality assumes exhaustive exploration. Yet, Human Beings and mammals do not exhaustively explore the world and their motivation is not only based on novelty but also on various other factors (e.g., curiosity, fun, style, pleasure, safety, competition, etc.). They optimize for life-long learning and train to learn transferable skills in playgrounds without obvious goals. They also apply innate or learned priors to save time and stay safe. For these reasons, we propose to learn an exploration bonus from demonstrations that could transfer these motivations to an artificial agent with little assumptions about their rationale. Using an inverse RL approach, we show that complex exploration behaviors, reflecting different motivations, can be learnt and efficiently used by RL agents to solve tasks for which exhaustive exploration is prohibitive.},
  archive   = {C_AAMAS},
  author    = {Hussenot, L\&#39;{e}onard and Dadashi, Robert and Geist, Matthieu and Pietquin, Olivier},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {620–628},
  title     = {Show me the way: Intrinsic motivation from demonstrations},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464028},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning node-selection strategies in bounded-suboptimal
conflict-based search for multi-agent path finding. <em>AAMAS</em>,
611–619. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-Agent Path Finding (MAPF) is an NP-hard problem that has important applications for distribution centers, traffic management and computer games, and it is still difficult for current solvers to solve large instances optimally. Bounded suboptimal solvers, such as Enhanced Conflict-Based Search (ECBS) and its variants, are more efficient than optimal solvers in finding a solution with suboptimality guarantees. ECBS is a tree search algorithm that expands the search tree by repeatedly selecting search tree nodes from a focal list. In this work, we propose to use machine learning (ML) to learn a node-selection strategy to speed up ECBS. In the first phase of our framework, we use imitation learning and curriculum learning to learn node-selection strategies iteratively for different numbers of agents from training instances. In the second phase, we deploy the learned models in ECBS and test their solving performance on unseen instances drawn from the same distribution as the training instances. We demonstrate that our solver, ML, shows substantial improvement in terms of the success rates and runtimes over ECBS on five different types of grid maps from the MAPF benchmark.},
  archive   = {C_AAMAS},
  author    = {Huang, Taoan and Dilkina, Bistra and Koenig, Sven},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {611–619},
  title     = {Learning node-selection strategies in bounded-suboptimal conflict-based search for multi-agent path finding},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464027},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cooperative-competitive reinforcement learning with
history-dependent rewards. <em>AAMAS</em>, 602–610. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Consider a typical organization whose worker agents seek to collectively cooperate for its general betterment. However, each individual agent simultaneously seeks to act to secure a larger chunk than its co-workers of the annual increment in compensation, which usually comes from a fixed pot. As such, the agents in an organization must cooperate and compete. Another feature of many organizations is that a worker receives a bonus, which is often a fraction of previous year&#39;s total profit. As such, the agent derives a reward that is also partly dependent on historical performance. How should the individual agent decide to act in this context? Few methods for the mixed cooperative-competitive setting have been presented in recent years, but these are challenged by problem domains whose reward functions additionally depend on historical information. Recent deep multi-agent reinforcement learning (MARL) methods using long short-term memory (LSTM) may be used, but these adopt a joint perspective to the interaction or require explicit exchange of information among the agents to promote cooperation, which may not be possible under competition. In this paper, we first show that the agent&#39;s decision-making problem can be modeled as an interactive partially observable Markov decision process (I-POMDP) that captures the dynamic of a history-dependent reward. We present an interactive advantage actor-critic method (IA2C+), which combines the independent advantage actor-critic network with a belief filter that maintains a belief distribution over other agents&#39; models. Empirical results show that IA2C+ learns the optimal policy faster and more robustly than several baselines.},
  archive   = {C_AAMAS},
  author    = {He, Keyang and Banerjee, Bikramjit and Doshi, Prashant},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {602–610},
  title     = {Cooperative-competitive reinforcement learning with history-dependent rewards},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464026},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A hotelling-downs framework for party nominees.
<em>AAMAS</em>, 593–601. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a model for the strategic selection of party nominees, where competing groups choose their representatives based on the expected electoral returns. Technically, we look at a generalisation of the Hotelling-Downs model, where each nominee has a predefined position on the political spectrum and attracts the closest voters compared to all other representatives. Within this framework we explore the algorithmic properties of Nash equilibria, which are not guaranteed to exist even in two party competitions. We show that finding a Nash equilibrium is NP-complete for the general case. However, if there are only two competing parties, this can be achieved in linear time. The results readily extend to games with restricted positioning options for the players involved, such as facility location and Voronoi games.},
  archive   = {C_AAMAS},
  author    = {Harrenstein, Paul and Lisowski, Grzegorz and Sridharan, Ramanujan and Turrini, Paolo},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {593–601},
  title     = {A hotelling-downs framework for party nominees},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464025},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-agent reinforcement learning with temporal logic
specifications. <em>AAMAS</em>, 583–592. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we study the problem of learning to satisfy temporal logic specifications with a group of agents in an unknown environment, which may exhibit probabilistic behaviour. From a learning perspective these specifications provide a rich formal language with which to capture tasks or objectives, while from a logic and automated verification perspective the introduction of learning capabilities allows for practical applications in large, stochastic, unknown environments. The existing work in this area is, however, limited. Of the frameworks that consider full linear temporal logic or have correctness guarantees, all methods thus far consider only the case of a single temporal logic specification and a single agent. In order to overcome this limitation, we develop the first multi-agent reinforcement learning technique for temporal logic specifications, which is also novel in its ability to handle multiple specifications. We provide correctness and convergence guarantees for our main algorithm - ALMANAC (Automaton/Logic Multi-Agent Natural Actor-Critic) - even when using function approximation. Alongside our theoretical results, we further demonstrate the applicability of our technique via a set of preliminary experiments.},
  archive   = {C_AAMAS},
  author    = {Hammond, Lewis and Abate, Alessandro and Gutierrez, Julian and Wooldridge, Michael},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {583–592},
  title     = {Multi-agent reinforcement learning with temporal logic specifications},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464024},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Equilibrium refinements for multi-agent influence diagrams:
Theory and practice. <em>AAMAS</em>, 574–582. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent influence diagrams (MAIDs) are a popular form of graphical model that, for certain classes of games, have been shown to offer key complexity and explainability advantages over traditional extensive form game (EFG) representations. In this paper, we extend previous work on MAIDs by introducing the concept of a MAID subgame, as well as subgame perfect and trembling hand perfect equilibrium refinements. We then prove several equivalence results between MAIDs and EFGs. Finally, we describe an open source implementation for reasoning about MAIDs and computing their equilibria.},
  archive   = {C_AAMAS},
  author    = {Hammond, Lewis and Fox, James and Everitt, Tom and Abate, Alessandro and Wooldridge, Michael},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {574–582},
  title     = {Equilibrium refinements for multi-agent influence diagrams: Theory and practice},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464023},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Action selection for composable modular deep reinforcement
learning. <em>AAMAS</em>, 565–573. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In modular reinforcement learning (MRL), a complex decision making problem is decomposed into multiple simpler subproblems each solved by a separate module. Often, these subproblems have conflicting goals, and incomparable reward scales. A composable decision making architecture requires that even the modules authored separately with possibly misaligned reward scales can be combined coherently. An arbitrator should consider different module&#39;s action preferences to learn effective global action selection. We present a novel framework called GRACIAS that assigns fine-grained importance to the different modules based on their relevance in a given state, and enables composable decision making based on modern deep RL methods such as deep deterministic policy gradient (DDPG) and deep Q-learning. We provide insights into the convergence properties of GRACIAS and also show that previous MRL algorithms reduce to special cases of our framework. We experimentally demonstrate on several standard MRL domains that our approach works significantly better than the previous MRL methods, and is highly robust to incomparable reward scales. Our framework extends MRL to complex Atari games such as Qbert, and has a better learning curve than the conventional RL algorithms.},
  archive   = {C_AAMAS},
  author    = {Gupta, Vaibhav and Anand, Daksh and Paruchuri, Praveen and Kumar, Akshat},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {565–573},
  title     = {Action selection for composable modular deep reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464022},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multivariate analysis of scheduling fair competitions.
<em>AAMAS</em>, 555–564. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A fair competition, based on the concept of envy-freeness, is a non-eliminating competition where each contestant (team or individual player) may not play against all other contestants, but the total difficulty for each contestant is the same: the sum of the initial rankings of the opponents for each contestant is the same. Similar to other non-eliminating competitions like the Round-robin competition or the Swiss-system competition, the winner of the fair competition is the contestant who wins the most games. The Fair Non-Eliminating Tournament (Fair-NET ) problem can be used to schedule fair competitions whose infrastructure is known. In the Fair-NET problem, we are given an infrastructure of a tournament represented by a graph G and the initial rankings of the contestants represented by a multiset of integers S. The objective is to decide whether G is S-fair, i.e., there exists an assignment of the contestants to the vertices of G such that the sum of the rankings of the neighbors of each contestant in G is the same constant k∈ N. We initiate a study of the classical and parameterized complexity of Fair-NET with respect to several central structural parameters motivated by real world scenarios, thereby presenting a comprehensive picture of it.},
  archive   = {C_AAMAS},
  author    = {Gupta, Siddharth and Zehavi, Meirav},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {555–564},
  title     = {Multivariate analysis of scheduling fair competitions},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464021},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On a notion of monotonic support for bipolar argumentation
frameworks. <em>AAMAS</em>, 546–554. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The bipolar argumentation framework (BAF) setting is an extension of Dung&#39;s setting for abstract argumentation, that considers an additional relation, called support relation. Several interpretations of such a support relation have been pointed out so far, including deductive, necessity, general and backing supports. These notions of support capture different kinds of interactions between arguments, that do not primarily correspond to attacks. In this paper, we propose a new notion of support, called monotonic support. Our approach is axiomatic: two postulates are introduced for capturing the intuition that underlies this notion of support in formal terms. The first postulate, monotony, prevents the support relation from downgrading the acceptance status of the supported argument. The second postulate, non-triviality, requires the existence of BAFs for which supporting an argument leads to increase its acceptance status. We present a general family of extension-based semantics for BAFs, called support score-based (SSB) semantics, that satisfy the two postulates and are parameterized by some aggregation functions. We prove a characterisation result linking the postulates that a SBB semantics satisfies with the properties of the aggregation functions used to define it. We also show that none of the previously introduced semantics for BAFs satisfies the monotony postulate.},
  archive   = {C_AAMAS},
  author    = {Gargouri, Anis and Konieczny, S\&#39;{e}bastien and Marquis, Pierre and Vesic, Srdjan},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {546–554},
  title     = {On a notion of monotonic support for bipolar argumentation frameworks},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464020},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Partially observable mean field reinforcement learning.
<em>AAMAS</em>, 537–545. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traditional multi-agent reinforcement learning algorithms are not scalable to environments with more than a few agents, since these algorithms are exponential in the number of agents. Recent research has introduced successful methods to scale multi-agent reinforcement learning algorithms to many agent scenarios using mean field theory. Previous work in this field assumes that an agent has access to exact cumulative metrics regarding the mean field behaviour of the system, which it can then use to take its actions. In this paper, we relax this assumption and maintain a distribution to model the uncertainty regarding the mean field of the system. We consider two different settings for this problem. In the first setting, only agents in a fixed neighbourhood are visible, while in the second setting, the visibility of agents is determined at random based on distances. For each of these settings, we introduce a Q-learning based algorithm that can learn effectively. We prove that this Q-learning estimate stays very close to the Nash Q-value (under a common set of assumptions) for the first setting. We also empirically show our algorithms outperform multiple baselines in three different games in the MAgents framework, which supports large environments with many agents learning simultaneously to achieve possibly distinct goals.},
  archive   = {C_AAMAS},
  author    = {Ganapathi Subramanian, Sriram and Taylor, Matthew E. and Crowley, Mark and Poupart, Pascal},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {537–545},
  title     = {Partially observable mean field reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464019},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Quantified announcements and common knowledge.
<em>AAMAS</em>, 528–536. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Public announcement logic (PAL) extends multi-agent epistemic logic with dynamic operators modelling the effects of public communication. Allowing quantification over public announcements lets us reason about theexistence of an announcement to reach a certain epistemic goal. Two notable examples of logics of quantified announcements are arbitrary public announcement logic (APAL) and group announcement logic (GAL). The notion of common knowledge plays an important role in PAL, and in particular in characterisations of epistemic states that an agent or a group of agents might make come about by performing public announcements. In this paper, we study extensions of APAL and GAL with common knowledge, which has not been done before. We consider both conservative extensions where the semantics of the quantifiers is not changed, as well as extensions where the scope of quantification also includes common knowledge formulas. We compare the expressivity of these extensions relative to each other and other connected logics, and provide sound and complete axiomatisations.},
  archive   = {C_AAMAS},
  author    = {Galimullin, Rustam and \r{A}gotnes, Thomas},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {528–536},
  title     = {Quantified announcements and common knowledge},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464018},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Probabilistic control argumentation frameworks.
<em>AAMAS</em>, 519–527. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we present Probabilistic Control Argumentation Frameworks (PCAFs) that extend classical Control Argumentation Frameworks (CAFs) to take into account probabilistic information in the reasoning process. We show that probabilities can be used to optimally control CAFs that cannot be controlled otherwise. We introduce the notion of controlling power, that represents the probability that a control configuration reaches its target. A computational method based on Monte Carlo simulations for computing the controlling power of control configurations is defined. We experimentally show that PCAFs outperform w.r.t runtime classical CAFs and in a large number of situations they can reach the target with a high probability while the classical CAFs fail.},
  archive   = {C_AAMAS},
  author    = {Gaignier, Fabrice and Dimopoulos, Yannis and Mailly, Jean-Guy and Moraitis, Pavlos},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {519–527},
  title     = {Probabilistic control argumentation frameworks},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464017},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Strategyproof facility location mechanisms on discrete
trees. <em>AAMAS</em>, 510–518. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the problem of strategyproof (SP) facility location mechanisms on discrete trees. Our main result is a full characterization of onto and SP mechanisms. In particular, we prove that when a single agent significantly affects the outcome, the trajectory of the facility is almost contained in the trajectory of the agent, and both move in the same direction along the common edges. We show tight relations of our characterization to previous results on discrete lines and on continuous trees. We then derive further implications of the main result for infinite discrete lines.},
  archive   = {C_AAMAS},
  author    = {Filimonov, Alina and Meir, Reshef},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {510–518},
  title     = {Strategyproof facility location mechanisms on discrete trees},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464016},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Self-imitation advantage learning. <em>AAMAS</em>, 501–509.
(<a href="https://dl.acm.org/doi/10.5555/3463952.3464015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-imitation learning is a Reinforcement Learning (RL) method that encourages actions whose returns were higher than expected, which helps in hard exploration and sparse reward problems. It was shown to improve the performance of on-policy actor-critic methods in several discrete control tasks. Nevertheless, applying self-imitation to the mostly action-value based off-policy RL methods is not straightforward. We propose SAIL, a novel generalization of self-imitation learning for off-policy RL, based on a modification of the Bellman optimality operator that we connect to Advantage Learning. Crucially, our method mitigates the problem of stale returns by choosing the most optimistic return estimate between the observed return and the current action-value for self-imitation. We demonstrate the empirical effectiveness of SAIL on the Arcade Learning Environment, with a focus on hard exploration games.},
  archive   = {C_AAMAS},
  author    = {Ferret, Johan and Pietquin, Olivier and Geist, Matthieu},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {501–509},
  title     = {Self-imitation advantage learning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464015},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A knowledge compilation map for conditional preference
statements-based languages. <em>AAMAS</em>, 492–500. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Conditional preference statements have been used to compactly represent preferences over combinatorial domains. They are at the core of CP-nets and their generalizations, and lexicographic preference trees. Several works have addressed the complexity of some queries (optimization, dominance in particular). We extend in this paper some of these results, and study other queries which have not been addressed so far, like equivalence, thereby contributing to a knowledge compilation map for languages bas­ed on conditional preference statements. We also introduce a new parameterised family of languages, which enables to balance expressiveness against the complexity of some queries.},
  archive   = {C_AAMAS},
  author    = {Fargier, H\&#39;{e}l\`{e}ne and Mengin, J\&#39;{e}r\^{o}me},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {492–500},
  title     = {A knowledge compilation map for conditional preference statements-based languages},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464014},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Safe multi-agent reinforcement learning via shielding.
<em>AAMAS</em>, 483–491. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent reinforcement learning (MARL) has been increasingly used in a wide range of safety-critical applications, which require guaranteed safety (e.g., no unsafe states are ever visited) during the learning process.Unfortunately, current MARL methods do not have safety guarantees. Therefore, we present two shielding approaches for safe MARL. In centralized shielding, we synthesize a single shield to monitor all agents&#39; joint actions and correct any unsafe action if necessary.In factored shielding, we synthesize multiple shields based on a factorization of the joint state space observed by all agents; the set of shields monitors agents concurrently and each shield is only responsible for a subset of agents at each step.Experimental results show that both approaches can guarantee the safety of agents during learning without compromising the quality of learned policies; moreover, factored shielding is more scalable in the number of agents than centralized shielding.},
  archive   = {C_AAMAS},
  author    = {ElSayed-Aly, Ingy and Bharadwaj, Suda and Amato, Christopher and Ehlers, R\&quot;{u}diger and Topcu, Ufuk and Feng, Lu},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {483–491},
  title     = {Safe multi-agent reinforcement learning via shielding},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464013},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). An abstraction-based method to check multi-agent deep
reinforcement-learning behaviors. <em>AAMAS</em>, 474–482. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent reinforcement learning (RL) often struggles to ensure the safe behaviours of the learning agents, and therefore it is generally not adapted to safety-critical applications. To address this issue, we present a methodology that combines formal verification with (deep) RL algorithms to guarantee the satisfaction of formally-specified safety constraints both in training and testing. The approach we propose expresses the constraints to verify inProbabilistic Computation Tree Logic (PCTL) and builds an abstract representation of the system to reduce the complexity of the verification step. This abstract model allows for model checking techniques to identify a set of abstract policies that meet the safety constraints expressed in PCTL. Then, the agents&#39; behaviours are restricted according to these safe abstract policies. We provide formal guarantees that by using this method, the actions of the agents always meet the safety constraints, and provide a procedure to generate an abstract model automatically. We empirically evaluate and show the effectiveness of our method in a multi-agent environment.},
  archive   = {C_AAMAS},
  author    = {El Mqirmi, Pierre and Belardinelli, Francesco and Le\&#39;{o}n, Borja G.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {474–482},
  title     = {An abstraction-based method to check multi-agent deep reinforcement-learning behaviors},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464012},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Log-time prediction markets for interval securities.
<em>AAMAS</em>, 465–473. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We design a prediction market to recover a complete and fully general probability distribution over a random variable. Traders buy and sellinterval securities that pay $1 if the outcome falls into an interval and $0 otherwise. Our market takes the form of a centralautomated market maker and allows traders to express interval endpoints of arbitrary precision. We present two designs in both of which market operations take time logarithmic in the number of intervals (that traders distinguish), providing the first computationally efficient market for a continuous variable. Our first design replicates the popularlogarithmic market scoring rule (LMSR), but operates exponentially faster than a standard LMSR by exploiting its modularity properties to construct a balanced binary tree and decompose computations along the tree nodes. The second design consists of two or more parallel LMSR market makers that mediate submarkets of increasingly fine-grained outcome partitions. This design remains computationally efficient for all operations, including arbitrage removal across submarkets. It adds two additional benefits for the market designer: (1)~the ability to express utility for information at various resolutions by assigning different liquidity values, and (2)~the ability to guarantee a true constant bounded loss by appropriately decreasing the liquidity in each submarket.},
  archive   = {C_AAMAS},
  author    = {Dud\&#39;{\i}k, Miroslav and Wang, Xintong and Pennock, David M. and Rothschild, David M.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {465–473},
  title     = {Log-time prediction markets for interval securities},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464011},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Learning correlated communication topology in multi-agent
reinforcement learning. <em>AAMAS</em>, 456–464. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Communication improves the efficiency and convergence of multi-agent learning. Existing study of agent communication has been limited on predefined fixed connections. While an attention mechanism exists and is useful for scheduling the communication between agents, it, however, largely ignores the dynamical nature of communication and thus the correlation between agents&#39; connections. In this work, we adopt a normalizing flow to encode correlation between agents interactions. The dynamical communication topology is directly learned by maximizing the agent rewards. In our end-to-end formulation, the communication structure is learned by considering it as a hidden dynamical variable. We realize centralized training of critics and graph reasoning policy, and decentralized execution from local observation and message that are received through the learned dynamical communication topology. Experiments on cooperative navigation in the particle world and adaptive traffic control tasks demonstrate the effectiveness of our method.},
  archive   = {C_AAMAS},
  author    = {Du, Yali and Liu, Bo and Moens, Vincent and Liu, Ziqi and Ren, Zhicheng and Wang, Jun and Chen, Xu and Zhang, Haifeng},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {456–464},
  title     = {Learning correlated communication topology in multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464010},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Efficient nonmyopic online allocation of scarce reusable
resources. <em>AAMAS</em>, 447–455. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study settings where a set of identical, reusable resources must be allocated in an online fashion to arriving agents. Each arriving agent is patient and willing to wait for some period of time to be matched. When matched, each agent occupies a resource for a certain amount of time, and then releases it, gaining some utility from having done so. The goal of the system designer is to maximize overall utility given some prior knowledge of the distribution of arriving agents. We are particularly interested in settings where demand for the resources far outstrips supply, as is typical in the provision of social services, for example homelessness resources. We formulate this problem as online bipartite matching with reusable resources and patient agents. We develop new, efficient nonmyopic algorithms for this class of problems, and compare their performance with that of greedy algorithms in a variety of simulated settings, as well as in a setting calibrated to real-world data on household demand for homelessness services. We find substantial overall welfare benefits to using our nonmyopic algorithms, particularly in more extreme settings -- those where agents are unwilling or unable to wait for resources, and where the ratio of resource demand to supply is particularly high.},
  archive   = {C_AAMAS},
  author    = {Dong, Zehao and Das, Sanmay and Fowler, Patrick and Ho, Chien-Ju},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {447–455},
  title     = {Efficient nonmyopic online allocation of scarce reusable resources},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464009},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Network robustness via global k-cores. <em>AAMAS</em>,
438–446. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Network robustness is a measure a network&#39;s ability to survive adversarial attacks. But not all parts of a network are equal. K-cores, which are dense subgraphs, are known to capture some of the key properties of many real-life networks. Therefore, previous work has attempted to model network robustness via the stability of its k-core. However, these approaches account for a single core value and thus fail to encode a global network resilience measure.In this paper, we address this limitation by proposing a novel notion of network resilience that is defined over all cores. In particular, we evaluate the stability of the network under node removals with respect to each node&#39;s initial core. Our goal is to compute robustness via a combinatorial problem: find b most critical nodes to delete such that the number of nodes that fall from their initial cores is maximized. One of our contributions is showing that it is NP-hard to achieve any polynomial factor approximation of the given objective. We also present a fine-grained complexity analysis of this problem under the lens of parameterized complexity theory for several natural parameters. Moreover, we show two applications of our notion of robustness: measuring the evolution of species and characterizing networks arising from different domains.},
  archive   = {C_AAMAS},
  author    = {Dey, Palash and Maity, Suman Kalyan and Medya, Sourav and Silva, Arlei},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {438–446},
  title     = {Network robustness via global K-cores},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464008},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Explaining BDI agent behaviour through dialogue.
<em>AAMAS</em>, 429–437. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {BDI agents act in response to external inputs and their internal plan library. Understanding the root cause of BDI agent action is often difficult, and in this paper we present a dialogue based approach for explaining the behaviour of a BDI agent. We consider two dialogue participants who may have different views regarding the beliefs, plans and external events which drove agent action (encoded via traces). These participants make utterances which incrementally reveal their traces to each other, allowing them to identify divergences in the traces, or to conclude that their traces agree. In practice, we envision a human taking on the role of a dialogue participant, with the BDI agent itself acting as the other participant. The dialogue then facilitates explanation, understanding and debugging of BDI agent behaviour. After presenting our formalism and its properties, we describe our implementation of the system and provide an example of its use in a simple scenario.},
  archive   = {C_AAMAS},
  author    = {Dennis, Louise A. and Oren, Nir},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {429–437},
  title     = {Explaining BDI agent behaviour through dialogue},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464007},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Modeling replicator dynamics in stochastic games using
markov chain method. <em>AAMAS</em>, 420–428. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In stochastic games, individuals need to make decisions in multiple states and transitions between states influence the dynamics of strategies significantly. In this work, by describing the dynamic process in stochastic game as a Markov chain and utilizing the transition matrix, we introduce a new method, named state-transition replicator dynamics, to obtain the replicator dynamics of a stochastic game. Based on our proposed model, we can gain qualitative and detailed insights into the influence of transition probabilities on the dynamics of strategies. We illustrate that a set of unbalanced transition probabilities can help players to overcome the social dilemmas and lead to mutual cooperation in a cooperation back state, even if the stochastic game has the same social dilemmas in each state. Moreover, we also present that a set of specifically designed transition probabilities can fix the expected payoffs of one player and make him lose the motivation to update his strategies in the stochastic game.},
  archive   = {C_AAMAS},
  author    = {Deng, Chuang and Rong, Zhihai and Wang, Lin and Wang, Xiaofan},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {420–428},
  title     = {Modeling replicator dynamics in stochastic games using markov chain method},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464006},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Walrasian equilibria in markets with small demands.
<em>AAMAS</em>, 413–419. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the complexity of finding a Walrasian equilibrium in markets where the agents have k-demand valuations. These valuations are an extension of unit-demand valuations where a bundle&#39;s value is the maximum of its k-subsets&#39; values. For unit-demand agents, where the existence of a Walrasian equilibrium is guaranteed, we show that the problem is in quasi-NC. For $k=2$, we show that it is NP-hard to decide if a Walrasian equilibrium exists even if the valuations are submodular, while for $k=3$ the hardness carries over to budget-additive valuations. In addition, we give a polynomial-time algorithm for markets with 2-demand single-minded valuations, or unit-demand valuations.},
  archive   = {C_AAMAS},
  author    = {Deligkas, Argyrios and Melissourgos, Themistoklis and Spirakis, Paul G.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {413–419},
  title     = {Walrasian equilibria in markets with small demands},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464005},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A heuristic algorithm for multi-agent vehicle routing with
automated negotiation. <em>AAMAS</em>, 404–412. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate a problem that lies at the intersection of three research areas, namely Automated Negotiation, Vehicle Routing, and Multi-Objective Optimization. Specifically, we investigate the scenario that multiple competing logistics companies aim to cooperate by delivering truck loads for one another, in order to improve efficiency and reduce the distance they drive. In order to do so, these companies need to find ways to exchange their truck loads such that each of them individually benefits. We present a new heuristic algorithm that, given one set of orders to deliver for each company, tries to find the set of all order-exchanges that are Pareto-optimal and individually rational. Furthermore, we present experiments based on real-world test data from two major logistics companies, which show that our algorithm is able to find hundreds of solutions in a matter of minutes.},
  archive   = {C_AAMAS},
  author    = {de Jonge, Dave and Bistaffa, Filippo and Levy, Jordi},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {404–412},
  title     = {A heuristic algorithm for multi-agent vehicle routing with automated negotiation},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464004},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Improved cooperation by exploiting a common signal.
<em>AAMAS</em>, 395–403. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Can artificial agents benefit from human conventions? Human societies manage to successfully self-organize and resolve the tragedy of the commons in common-pool resources, in spite of the bleak prediction of non-cooperative game theory. On top of that, real-world problems are inherently large-scale and of low observability. One key concept that facilitates human coordination in such settings is the use of conventions. Inspired by human behavior, we investigate the learning dynamics and emergence of temporal conventions, focusing on common-pool resources. Extra emphasis was given in designing a realistic evaluation setting: (a) environment dynamics are modeled on real-world fisheries, (b) we assume decentralized learning, where agents can observe only their own history, and (c) we run large-scale simulations (up to 64 agents).Uncoupled policies and low observability make cooperation hard to achieve; as the number of agents grow, the probability of taking a correct gradient direction decreases exponentially. By introducing an arbitrary common signal (e.g., date, time, or any periodic set of numbers) as a means to couple the learning process, we show that temporal conventions can emerge and agents reach sustainable harvesting strategies. The introduction of the signal consistently improves the social welfare (by 258\% on average, up to 3306\%), the range of environmental parameters where sustainability can be achieved (by 46\% on average, up to 300\%), and the convergence speed in low abundance settings (by 13\% on average, up to 53\%).},
  archive   = {C_AAMAS},
  author    = {Danassis, Panayiotis and Erden, Zeki Doruk and Faltings, Boi},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {395–403},
  title     = {Improved cooperation by exploiting a common signal},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464003},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scalable multiagent driving policies for reducing traffic
congestion. <em>AAMAS</em>, 386–394. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traffic congestion is a major challenge in modern urban settings. The industry-wide development of autonomous and automated vehicles (AVs) motivates the question of how can AVs contribute to congestion reduction. Past research has shown that in small scale mixed traffic scenarios with both AVs and human-driven vehicles, a small fraction of AVs executing a controlled multiagent driving policy can mitigate congestion. In this paper, we scale up existing approaches and develop new multiagent driving policies for AVs in scenarios with greater complexity. We start by showing that a congestion metric used by past research is manipulable in open road network scenarios where vehicles dynamically join and leave the road. We then propose using a different metric that is robust to manipulation and reflects open network traffic efficiency. Next, we propose a modular transfer learning approach and use it to scale up the multiagent driving policy to a realistic simulated scenario that is an order of magnitude larger than past scenarios (hundreds rather than tens of vehicles). Our experimental study shows that the resulting policy improves traffic efficiency over human-driven traffic in a large open network, where existing approaches fail to do so. Another key advantage of our modular transfer learning approach is that it avoids collecting samples from entire network, which saves up to 80\% of training and data collection time in our experiments. Finally, we show for the first time a distributed multiagent policy that improves congestion over human-driven traffic. The distributed approach is more realistic as it does not require adding communication infrastructure, it relies on existing sensing capabilities, and its complexity scales linearly with the number of vehicles.},
  archive   = {C_AAMAS},
  author    = {Cui, Jiaxun and Macke, William and Yedidsion, Harel and Goyal, Aastha and Urieli, Daniel and Stone, Peter},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {386–394},
  title     = {Scalable multiagent driving policies for reducing traffic congestion},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464002},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Loss bounds for approximate influence-based abstraction.
<em>AAMAS</em>, 377–385. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sequential decision making techniques hold great promise to improve the performance of many real-world systems, but computational complexity hampers their principled application. Influence-based abstraction aims to gain leverage by modeling local subproblems together with the &#39;influence&#39; that the rest of the system exerts on them. While computing exact representations of such influence might be intractable, learning approximate representations offers a promising approach to enable scalable solutions. This paper investigates the performance of such approaches from a theoretical perspective. The primary contribution is the derivation of sufficient conditions on approximate influence representations that can guarantee solutions with small value loss. In particular we show that neural networks trained with cross entropy are well suited to learn approximate influence representations. Moreover, we provide a sample based formulation of the bounds, which reduces the gap to applications. Finally, driven by our theoretical insights, we propose approximation error estimators, which empirically reveal to correlate well with the value loss.},
  archive   = {C_AAMAS},
  author    = {Congeduti, Elena and Mey, Alexander and Oliehoek, Frans A.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {377–385},
  title     = {Loss bounds for approximate influence-based abstraction},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464001},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Rational synthesis in the commons with careless and careful
agents. <em>AAMAS</em>, 368–376. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3464000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Turn-based games on graphs are games where the states are controlled by one and only one player who decides which edge to follow. Each player has a temporal objective that he tries to achieve. One player is the designated &#39;controller&#39;, whose objective captures the desirable outcomes of the whole system. Cooperative rational synthesis is the problem of computing a Nash equilibrium that satisfies the controller&#39;s objective. In this paper, we tackle this problem in the context of a commons, where each action has a cost or a benefit on one shared common pool energy resource. The paper investigates the problem of synthesising the controller in a commons such that there exists an individually rational behaviour of all the agents in the commons that satisfies the controller&#39;s objective and does not deplete the resource. We consider two types of agents: careless and careful. Careless agents only care for their temporal objective, while careful agents also pay attention not to deplete the system&#39;s resource. We solve the problem of cooperative rational synthesis in these games, focusing on parity objectives.},
  archive   = {C_AAMAS},
  author    = {Condurache, Rodica and Dima, Catalin and Oualhadj, Youssouf and Troquard, Nicolas},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {368–376},
  title     = {Rational synthesis in the commons with careless and careful agents},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464000},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Spatial consensus-prevention in robotic swarms.
<em>AAMAS</em>, 359–367. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we define the consensus-prevention problem, which examines the canonical swarm robotic consensus problem from an adversarial point of view: how (if at all) is it possible to lead a swarm into a disagreement, that is, prevent them from reaching an agreement. We focus on consensus-prevention in physically grounded tasks, concentrating on influencing the direction of movement of a flocking swarm and guaranteeing that the swarm will never converge to the same direction by the use of external, predefined agents, referred to as diverting agents.We formally define the notion of disagreement within a flock, and propose a way of measuring it. We show a correlation between the consensus-prevention problem and the coalition formation problem, whose players aim at maximizing the disagreement measure. While the general problem of optimizing disagreement between flocking agents is NP-hard, we focus on a case which is solvable in polynomial time, using a variant of the graph clustering problem where the clusters constitute the desired coalitions. This allows us to determine both the number of coalitions that optimize disagreement, and the behavior of the diverting agents for a given number of coalitions that will lead to optimal disagreement. Finally, we demonstrate in simulation the impact of the number of diverting agents on the disagreement measure in different scenarios, and discuss the limitations of the diverting agents in dynamic settings.},
  archive   = {C_AAMAS},
  author    = {Cohen, Saar and Agmon, Noa},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {359–367},
  title     = {Spatial consensus-prevention in robotic swarms},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463999},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). MOBLOT: Molecular oblivious robots. <em>AAMAS</em>, 350–358.
(<a href="https://dl.acm.org/doi/10.5555/3463952.3463998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In swarm robotics, research mainly follows a theoretical approach that considers robot systems in the abstract, where the complexity and capabilities of the underlying model are often reduced to their minimum. One general and well-investigated model is \o{}blot, where the robots are silent, anonymous, and oblivious.In this work, we introduce MOBLOT, a model that extends \o{}blot to address a larger spectrum of cases. MOBLOT stands for molecular oblivious robots: like atoms combine themselves to form molecules, in MOBLOT simple robots can move to form more complex computational units, having an extent and different capabilities with respect to robots; like molecules combine themselves to form the matter, in MOBLOT the complex structures can exploit their own capabilities to arrange themselves to form any shape defining an acceptable final structure. In MOBLOT, we formally define the Matter Formation (MF) problem and, as a preliminary general result, we provide a necessary condition for its solvability which relies on symmetricity. Informally, the symmetricity of a configuration measures the amount of symmetries of the robots&#39; disposal. We actually show how dealing with molecules can resolve in some cases the symmetry breaking issue where \o{}blot cannot. Finally, we provide a case study for MOBLOT, that is, a representative MF problem along with a resolution distributed algorithm.},
  archive   = {C_AAMAS},
  author    = {Cicerone, Serafino and Di Fonso, Alessia and Di Stefano, Gabriele and Navarra, Alfredo},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {350–358},
  title     = {MOBLOT: Molecular oblivious robots},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463998},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Scalable anytime planning for multi-agent MDPs.
<em>AAMAS</em>, 341–349. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a scalable tree search planning algorithm for large multi-agent sequential decision problems that require dynamic collaboration. Teams of agents need to coordinate decisions in many domains, but naive approaches fail due to the exponential growth of the joint action space with the number of agents. We circumvent this complexity through an anytime approach that allows us to trade computation for approximation quality and also dynamically coordinate actions. Our algorithm comprises three elements: online planning with Monte Carlo Tree Search (MCTS), factored representations of local agent interactions with coordination graphs, and the iterative Max-Plus method for joint action selection. We evaluate our approach on the benchmark SysAdmin domain with static coordination graphs and achieve comparable performance with much lower computation cost than our MCTS baselines. We also introduce a multi-drone delivery domain with dynamic, i.e., state-dependent coordination graphs, and demonstrate how our approach scales to large problems on this domain that are intractable for other MCTS methods. We provide an open-source implementation of our algorithm at https://github.com/JuliaPOMDP/FactoredValueMCTS.jl.},
  archive   = {C_AAMAS},
  author    = {Choudhury, Shushman and Gupta, Jayesh K. and Morales, Peter and Kochenderfer, Mykel J.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {341–349},
  title     = {Scalable anytime planning for multi-agent MDPs},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463997},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A general trust framework for multi-agent systems.
<em>AAMAS</em>, 332–340. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Transportation systems of the future can be best modeled as multi-agent systems. A number of coordination protocols such as autonomous intersection management (AIM), adaptive cooperative traffic light control (TLC), cooperative adaptive cruise control (CACC), among others have been developed with the goal of improving the safety and efficiency of such systems. The overall goal in these systems is to provide behavioral guarantees under the assumption that the participating agents work in concert with a centralized (or distributed) coordinator. While there is work on analyzing such systems from a security perspective, we argue that there is limited work on quantifying trustworthiness of individual agents in a multi-agent system. We propose a framework that uses an epistemic logic to quantify trustworthiness of agents, and embed the use of quantitative trustworthiness values into control and coordination policies. Our modified control policies can help the multi-agent system improve its safety in the presence of untrustworthy agents (and under certain assumptions, including malicious agents). We empirically show the effectiveness of our proposed trust framework by embedding it into AIM, TLC, and CACC platooning algorithms. In our experiments, our trust framework accurately detects attackers in CACC platoons; mitigates the effect of untrustworthy agents in AIM; and trust-aware TLC and AIM reduce collisions in all cases compared to the vanilla versions of these algorithms.},
  archive   = {C_AAMAS},
  author    = {Cheng, Mingxi and Yin, Chenzhong and Zhang, Junyao and Nazarian, Shahin and Deshmukh, Jyotirmoy and Bogdan, Paul},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {332–340},
  title     = {A general trust framework for multi-agent systems},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463996},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). A game theoretical analysis of non-linear blockchain system.
<em>AAMAS</em>, 323–331. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in the blockchain research have been made in two important directions. One is refined resilience analysis utilizing game theory to study the consequences of selfish behavior of users (miners), and the other is the extension from a linear (chain) structure to a non-linear (graphical) structure for performance improvements, such as IOTA and Graphcoin. The first question that comes to mind is what improvements that a blockchain system would see by leveraging these new advances. In this paper, we consider three major properties for a blockchain system: α-partial verification, scalability, and finality-duration. We establish a formal framework and prove that no blockchain system can achieve ?-partial verification for any fixed constant ?, high scalability, and low finality-duration simultaneously. We observe that classical blockchain systems like Bitcoin achieves full verification (α=1) and low finality-duration, Ethereum 2.0 Sharding achieves low finality-duration and high scalability. We are interested in whether it is possible to partially satisfy the three properties.},
  archive   = {C_AAMAS},
  author    = {Chen, Lin and Xu, Lei and Gao, Zhimin and Sunny, Ahmed Imtiaz and Kasichainula, Keshav and Shi, Weidong},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {323–331},
  title     = {A game theoretical analysis of non-linear blockchain system},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463995},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Temporal watermarks for deep reinforcement learning models.
<em>AAMAS</em>, 314–322. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Watermarking has become a popular and attractive technique to protect the Intellectual Property (IP) of Deep Learning (DL) models. However, very few studies explore the possibility of watermarking Deep Reinforcement Learning (DRL) models. Common approaches in the DL context embed backdoors into the protected model and use special samples to verify the model ownership. These solutions are easy to be detected, and can potentially affect the performance and behaviors of the target model. Such limitations make existing solutions less applicable to safety- and security-critical tasks and scenarios, where DRL has been widely used.In this work, we propose a novel watermarking scheme for DRL protection. Instead of using spatial watermarks as in DL models, we introduce temporal watermarks, which can reduce the potential impact and damage to the target model, while achieving ownership verification with high fidelity. Specifically, (1) we design a new damage metric to select sequential states for watermark generation; (2) we introduce a new reward function to efficiently alter the model&#39;s behaviors for watermark embedding; (3) we propose to utilize a predefined probability density function of actions over the watermark states as the verification evidence. The integration of these techniques enables a DRL model owner to embed the watermarks for ownership verification and IP protection. Our method is general and can be applied to various DRL tasks with either deterministic or stochastic reinforcement learning algorithms. Extensive experimental results show that it can effectively preserve the functionality of DRL models and exhibit significant robustness against common model modifications, e.g., fine-tuning and model compression.},
  archive   = {C_AAMAS},
  author    = {Chen, Kangjie and Guo, Shangwei and Zhang, Tianwei and Li, Shuxin and Liu, Yang},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {314–322},
  title     = {Temporal watermarks for deep reinforcement learning models},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463994},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Tractable mechanisms for computing near-optimal utility
functions. <em>AAMAS</em>, 306–313. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large scale multiagent systems must rely on distributed decision making, as centralized coordination is either impractical or impossible. Recent works approach this problem under a game theoretic lens, whereby utility functions are assigned to each of the agents with the hope that their local optimization approximates the centralized optimal solution. Yet, formal guarantees on the resulting performance cannot be obtained for broad classes of problems without compromising on their accuracy. In this work, we address this concern relative to the well-studied problem of resource allocation with nondecreasing concave welfare functions. We show that optimally designed local utilities achieve an approximation ratio (price of anarchy) of 1-c/e, where c is the function&#39;s curvature and e is Euler&#39;s constant. The upshot of our contributions is the design of approximation algorithms that are distributed and efficient, and whose performance matches that of the best existing polynomial-time (and centralized) schemes.},
  archive   = {C_AAMAS},
  author    = {Chandan, Rahul and Paccagnan, Dario and Marden, Jason R.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {306–313},
  title     = {Tractable mechanisms for computing near-optimal utility functions},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463993},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Classifying the complexity of the possible winner problem on
partial chains. <em>AAMAS</em>, 297–305. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Possible Winner (PW) problem, a fundamental algorithmic problem in computational social choice, concerns elections where voters express only partial preferences between candidates. A sequence of investigations led to a complete classification of the complexity of this problem for all pure positional scoring rules: the PW problem is in P for the plurality and veto rules, and NP-complete for all other such rules. The PW problem has also been studied on classes of restricted partial orders, such as partitioned partial orders and truncated partial orders; one of the findings is that there are positional scoring rules for which the complexity of the PW problem drops from NP-complete to P on such restricted partial orders. Here, we investigate the PW problem on partial chains, i.e., partial orders that consist of a total order on a subset of their domains. Such orders arise naturally in a variety of settings, including rankings of movies or restaurants. We classify the complexity of the PW problem on partial chains by establishing that, perhaps surprisingly, this restriction does not change the complexity of the problem. Specifically, we show that the PW problem on partial chains is NP-complete for all pure positional scoring rules other than the plurality rule and the veto rule, while, of course, for the latter two rules this problem remains in P.},
  archive   = {C_AAMAS},
  author    = {Chakraborty, Vishal and Kolaitis, Phokion G.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {297–305},
  title     = {Classifying the complexity of the possible winner problem on partial chains},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463992},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Worst-case bounds for spending a common budget.
<em>AAMAS</em>, 288–296. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of spending a budget that is common to n agents. Agents submit demands to a central planner who uses the budget to fund a subset of them. The utility of an agent is the part of the budget spent on her own accepted demands. In a fair solution, the successful demands of each agent would represent a 1/n fraction of the budget. However, this is rarely possible because every demand is indivisible, i.e. either accepted in its entirety or rejected. We are interested in worst-case bounds on the largest proportion of the budget that is dedicated to the least funded agent. Our approach is not to solve the corresponding max min problem for every instance, but to tackle the problem from a higher level. The size of the largest demand compared to the budget and the number of agents, are two parameters that significantly influence how much the worst-off agent gets. We propose worst-case bounds on the best utility of the least funded agent for the class of instances where the number of agents and the most expensive demand are fixed to given values. A characterization of this quantity is provided for 1 and 2 agents. For more than 2 agents,\%our results we propose lower and upper bounds that constitute a 14/15-approximation of the optimal value. Every existence result is complemented with a polynomial algorithm that builds a feasible solution satisfying our bounds.},
  archive   = {C_AAMAS},
  author    = {Cardi, Pierre and Gourv\`{e}s, Laurent and Lesca, Julien},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {288–296},
  title     = {Worst-case bounds for spending a common budget},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463991},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Imitation learning from pixel-level demonstrations by
HashReward. <em>AAMAS</em>, 279–287. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One of the key issues for imitation learning lies in making policy learned from limited samples to generalize well in the whole state-action space. This problem is much more severe in high-dimensional state environments, such as game playing with raw pixel inputs. Under this situation, even state-of-the-art adversary-based imitation learning algorithms fail. Through empirical studies, we find that the main cause lies in the failure of training a powerful discriminator to generate meaningful rewards in high-dimensional environments. Although it seems that dimensionality reduction can help, a straightforward application of off-the-shelf methods cannot achieve good performance. In this work, we show in theory that the balance between dimensionality reduction and discriminative training is essential for effective learning. To achieve this target, we propose HashReward, which utilizes the idea of supervised hashing to realize such an ideal balance. Experimental results show that HashReward could outperform state-of-the-art methods for a large gap under the challenging high-dimensional environments.},
  archive   = {C_AAMAS},
  author    = {Cai, Xin-Qiang and Ding, Yao-Xiang and Jiang, Yuan and Zhou, Zhi-Hua},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {279–287},
  title     = {Imitation learning from pixel-level demonstrations by HashReward},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463990},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-agent coordination in adversarial environments through
signal mediated strategies. <em>AAMAS</em>, 269–278. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many real-world scenarios involve teams of agents that have to coordinate their actions to reach a shared goal. We focus on the setting in which a team of agents faces an opponent in a zero-sum, imperfect-information game. Team members can coordinate their strategies before the beginning of the game, but are unable to communicate during the playing phase of the game. This is the case, for example, in Bridge, collusion in poker, and collusion in bidding.In this setting, model-free RL methods are oftentimes unable to capture coordination because agents&#39; policies are executed in a decentralized fashion. Our first contribution is a game-theoretic centralized training regimen to effectively perform trajectory sampling so as to foster team coordination. When team members can observe each other actions, we show that this approach provably yields equilibrium strategies. Then, we introduce a signaling-based framework to represent team coordinated strategies given a buffer of past experiences. Each team member&#39;s policy is parametrized as a neural network whose output is conditioned on a suitable exogenous signal, drawn from a learned probability distribution. By combining these two elements, we empirically show convergence to coordinated equilibria in cases where previous state-of-the-art multi-agent RL algorithms did not.},
  archive   = {C_AAMAS},
  author    = {Cacciamani, Federico and Celli, Andrea and Ciccone, Marco and Gatti, Nicola},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {269–278},
  title     = {Multi-agent coordination in adversarial environments through signal mediated strategies},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463989},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). High-multiplicity fair allocation made more practical.
<em>AAMAS</em>, 260–268. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The envy-free, Pareto-efficient allocation of indivisible goods leads to computationally hard problems. There is a big variety of modeling issues, such as agent-specific utility functions or (high numbers of) different types of goods. In recent work, Bredereck et al. [ACM EC~2019] addressed this topic by showing (theoretical) fixed-parameter tractability results for &quot;high-multiplicity fair allocation&quot;, exploiting parameters such as number of agents or maximum absolute utility values. To this end, they used a number of tools from (theoretical) integer linear programming. We &quot;engineer&quot; their work towards practical usefulness, thereby being able to solve all real-world instances from the state-of-art online platform &quot;spliddit.org for provably fair solutions&quot;. Besides providing the foundations for a fast tool for fair allocations, we also offer a flexible framework with the possibility to relax fairness or efficiency demands so to, e.g., allow tradeoffs between fairness and social welfare. Moreover, our framework provides ways to interpret and explain &quot;solution paths&quot; which makes it possible to perform further explorations in cases when no envy-free and efficient allocations exist.},
  archive   = {C_AAMAS},
  author    = {Bredereck, Robert and Figiel, Aleksander and Kaczmarczyk, Andrzej and Knop, Du\v{s}an and Niedermeier, Rolf},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {260–268},
  title     = {High-multiplicity fair allocation made more practical},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463988},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). On the indecisiveness of kelly-strategyproof social choice
functions. <em>AAMAS</em>, 251–259. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Social choice functions (SCFs) map the preferences of a group of agents over some set of alternatives to a non-empty subset of alternatives. The Gibbard-Satterthwaite theorem has shown that only extremely unattractive single-valued SCFs are strategyproof when there are more than two alternatives. For set-valued SCFs, or so-called social choice correspondences, the situation is less clear. There are miscellaneous--mostly negative--results using a variety of strategyproofness notions and additional requirements. The simple and intuitive notion of Kelly-strategyproofness has turned out to be particularly compelling because it is weak enough to still allow for positive results. For example, the Pareto rule is strategyproof even when preferences are weak, and a number of attractive SCFs (such as the top cycle, the uncovered set, and the essential set) are strategyproof for strict preferences. In this paper, we show that, for weak preferences, only indecisive SCFs can satisfy strategyproofness. In particular, (i) every strategyproof rank-based SCF violates Pareto-optimality, (ii) every strategyproof support-based SCF (which generalize Fishburn&#39;s C2 SCFs) that satisfies Pareto-optimality returns at least one most preferred alternative of every voter, and (iii) every strategyproof non-imposing SCF returns a Condorcet loser in at least one profile.},
  archive   = {C_AAMAS},
  author    = {Brandt, Felix and Bullinger, Martin and Lederer, Patrick},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {251–259},
  title     = {On the indecisiveness of kelly-strategyproof social choice functions},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463987},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Knowledge improvement and diversity under interaction-driven
adaptation of learned ontologies. <em>AAMAS</em>, 242–250. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When agents independently learn knowledge, such as ontologies, about their environment, it may be diverse, incorrect or incomplete. This knowledge heterogeneity could lead agents to disagree, thus hindering their cooperation. Existing approaches usually deal with this interaction problem by relating ontologies, without modifying them, or, on the contrary, by focusing on building common knowledge. Here, we consider agents adapting ontologies learned from the environment in order to agree with each other when cooperating. In this scenario, fundamental questions arise: Do they achieve successful interaction? Can this process improve knowledge correctness? Do all agents end up with the same ontology? To answer these questions, we design a two-stage experiment. First, agents learn to take decisions about the environment by classifying objects and the learned classifiers are turned into ontologies. In the second stage, agents interact with each other to agree on the decisions to take and modify their ontologies accordingly. We show that agents indeed reduce interaction failure, most of the time they improve the accuracy of their knowledge about the environment, and they do not necessarily opt for the same ontology.},
  archive   = {C_AAMAS},
  author    = {Bourahla, Yasser and Atencia, Manuel and Euzenat, J\&#39;{e}r\^{o}me},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {242–250},
  title     = {Knowledge improvement and diversity under interaction-driven adaptation of learned ontologies},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463986},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Decision model for a virtual agent that can touch and be
touched. <em>AAMAS</em>, 232–241. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Touch is an essential sense to the social development and well-being of individuals, acting as a communicative channel for emotions and empathy. Our overall objective is to enhance the ability of embodied conversational agents (ECAs) to bond with humans. To reach such an objective, we have endowed an ECA with the capacity to touch and be touched in a social interaction with a human in an immersive environment. By drawing inspiration from literature on human-human social touch and related works on human-agent tactile interactions, we have developed a framework for a touching ECA able to perceive when and how a user touches it and decide when and how to respond accordingly. This paper focuses on going beyond a simple bi-directional touch interaction through a decision model that actually adapts its behaviour to the content of the interaction, the level of rapport and the human&#39;s touch avoidance sensibility. This enables an actual interactive loop between the agent and the human.},
  archive   = {C_AAMAS},
  author    = {Boucaud, Fabien and Pelachaud, Catherine and Thouvenin, Indira},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {232–241},
  title     = {Decision model for a virtual agent that can touch and be touched},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463985},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Manipulability of thiele methods on party-list profiles.
<em>AAMAS</em>, 223–231. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent impossibility results have shown that strategyproofness is difficult to obtain for multiwinner voting rules, especially in combination with proportionality. In this paper, we attempt to identify cases where strategyproofness can be established by considering manipulation on party-list profiles. We distinguish between three types of manipulation---subset-manipulation, superset-manipulation, and disjoint-set-manipulation. Our focus is the class of irresolute Thiele rules. For all three types of manipulation, we are able to establish that Thiele rules are strategyproof on party-list profiles for several well-known preference extensions. For superset- and disjoint-set-strategyproofness, we can extend this result to all preference extensions. We are also able to show that Thiele rules are fully strategyproof for optimistic agents on these profiles.},
  archive   = {C_AAMAS},
  author    = {Botan, Sirin},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {223–231},
  title     = {Manipulability of thiele methods on party-list profiles},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463984},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Egalitarian judgment aggregation. <em>AAMAS</em>, 214–222.
(<a href="https://dl.acm.org/doi/10.5555/3463952.3463983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Egalitarian considerations play a central role in many areas of social choice theory. Applications of egalitarian principles range from ensuring everyone gets an equal share of a cake when deciding how to divide it, to guaranteeing balance with respect to gender or ethnicity in committee elections. Yet, the egalitarian approach has received little attention in judgment aggregation---a powerful framework for aggregating logically interconnected issues. We make the first steps towards filling that gap. We introduce axioms capturing two classical interpretations of egalitarianism in judgment aggregation and situate these within the context of existing axioms in the pertinent framework of belief merging. We then explore the relationship between these axioms and several notions of strategyproofness from social choice theory at large. Finally, a novel egalitarian judgment aggregation rule stems from our analysis; we present complexity results concerning both outcome determination and strategic manipulation for that rule.},
  archive   = {C_AAMAS},
  author    = {Botan, Sirin and de Haan, Ronald and Slavkovik, Marija and Terzopoulou, Zoi},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {214–222},
  title     = {Egalitarian judgment aggregation},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463983},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Action priors for large action spaces in robotics.
<em>AAMAS</em>, 205–213. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In robotics, it is often not possible to learn useful policies using pure model-free reinforcement learning without significant reward shaping or curriculum learning. As a consequence, many researchers rely on expert demonstrations to guide learning. However, acquiring expert demonstrations can be expensive. This paper proposes an alternative approach where the solutions of previously solved tasks are used to produce an action prior that can facilitate exploration in future tasks. The action prior is a probability distribution over actions that summarizes the set of policies found solving previous tasks. Our results indicate that this approach can be used to solve robotic manipulation problems that would otherwise be infeasible without expert demonstrations. Source code is available at https://github.com/ondrejba/action_priors.},
  archive   = {C_AAMAS},
  author    = {Biza, Ondrej and Wang, Dian and Platt, Robert and van de Meent, Jan-Willem and Wong, Lawson L.S.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {205–213},
  title     = {Action priors for large action spaces in robotics},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463982},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Optimising long-term outcomes using real-world fluent
objectives: An application to football. <em>AAMAS</em>, 196–204. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a novel approach for optimising long-term tactical and strategic decision-making in football (soccer) by encapsulating events in a league environment across a given time frame. We model the teams&#39; objectives for a season and track how these evolve as games unfold to give a fluent objective that can aid in decision-making games. We develop Markov chain Monte Carlo and deep learning-based algorithms that make use of the fluent objectives in order to learn from prior games and other games in the environment and increase the teams&#39; long-term performance. Simulations of our approach using real-world datasets from 760 matches shows that by using optimised tactics with our fluent objective and prior games, we can on average increase teams mean expected finishing distribution in the league by up to 35.6\%.},
  archive   = {C_AAMAS},
  author    = {Beal, Ryan and Chalkiadakis, Georgios and Norman, Timothy J. and Ramchurn, Sarvapali D.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {196–204},
  title     = {Optimising long-term outcomes using real-world fluent objectives: An application to football},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463981},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Complexity of sequential rules in judgment aggregation.
<em>AAMAS</em>, 187–195. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The task in judgment aggregation is to find a collective judgment set based on the views of individual judges about a given set of propositional formulas. One way of guaranteeing consistent outcomes is the use of sequential rules. In each round, the decision on a single formula is made either because the outcome is entailed by the already obtained judgment set, or, if this is not the case, by some underlying rule, e.g. the majority rule. Such rules are especially useful for cases, where the agenda is not fixed in advance, and formulas are added one by one. This paper investigates the computational complexity of winner determination under a family of sequential rules, and the manipulative influence of the processing order on the final outcome.},
  archive   = {C_AAMAS},
  author    = {Baumeister, Dorothea and Boes, Linus and Weishaupt, Robin},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {187–195},
  title     = {Complexity of sequential rules in judgment aggregation},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463980},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Complexity of scheduling and predicting round-robin
tournaments. <em>AAMAS</em>, 178–186. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tournaments are commonly used to identify winners, for example in sports. We study the computational complexity of problems related to scheduling the matches in a tournament and predicting the outcome with a special focus on round-robin tournaments, which is the most prominent tournament type used in sports. Besides the general financial and intrinsically motivated interest of various agents in tournament prediction, the recently very relevant winner determination for suddenly discontinued tournaments is a strong motivation. We show the immense theoretical complexity of predicting the winners in round-robin tournaments even under the assumption that only three matchdays remain to be played. On the other hand, we present an FPT algorithm and analyze its practical complexity using experiments on real-world and generated data, showing the applicability of the algorithm in praxis. To the best of our knowledge, this is the first exact and not purely brute-force oriented approach for predicting round-robin tournaments.},
  archive   = {C_AAMAS},
  author    = {Baumeister, Dorothea and Hogrebe, Tobias Alexander},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {178–186},
  title     = {Complexity of scheduling and predicting round-robin tournaments},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463979},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Existence and computation of maximin fair allocations under
matroid-rank valuations. <em>AAMAS</em>, 169–177. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study fair and economically efficient allocation of indivisible goods among agents whose valuations are rank functions of matroids. Such valuations constitute a well-studied class of submodular functions (i.e., they exhibit a diminishing returns property) and model preferences in several resource-allocation settings. We prove that, for matroid-rank valuations, a social welfare-maximizing allocation that gives each agent her maximin share always exists. Furthermore, such an allocation can be computed in polynomial time. We establish similar existential and algorithmic results for the pairwise maximin share guarantee as well.To complement these results, we show that if the agents have binary XOS valuations or weighted-rank valuations, then maximin fair allocations are not guaranteed to exist. Both of these valuation classes are immediate generalizations of matroid-rank functions.},
  archive   = {C_AAMAS},
  author    = {Barman, Siddharth and Verma, Paritosh},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {169–177},
  title     = {Existence and computation of maximin fair allocations under matroid-rank valuations},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463978},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cooperative prioritized sweeping. <em>AAMAS</em>, 160–168.
(<a href="https://dl.acm.org/doi/10.5555/3463952.3463977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel model-based algorithm, Cooperative Prioritized Sweeping, for sample-efficient learning in large multi-agent Markov decision processes. Our approach leverages domain knowledge about the structure of the problem in the form of a dynamic decision network. Using this information, our method learns a model of the environment to determine which state-action pairs are the most likely in need to be updated, significantly increasing learning speed. Batch updates can then be performed which efficiently back-propagate knowledge throughout the value function. Our method outperforms the state-of-the-art sparse cooperative Q-learning and QMIX algorithms, both on the well-known SysAdmin benchmark, randomized environments and a fully-observable variation of the well-known firefighter benchmark from Dec-POMDP literature.},
  archive   = {C_AAMAS},
  author    = {Bargiacchi, Eugenio and Verstraeten, Timothy and Roijers, Diederik M.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {160–168},
  title     = {Cooperative prioritized sweeping},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463977},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Predicting voting outcomes in presence of communities.
<em>AAMAS</em>, 151–159. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Individuals in a social network may form their views as a result of the influence exerted by their connections. In elections, for example, while they might initially support one candidate, social influence may lead them to support another. Here, we investigate whether a recently proposed metric, influence gap, designed to measure the effect of social influence in voting on social networks, is able to predict the outcome of a vote on networks exhibiting community structure, i.e., made of highly interconnected components, and therefore more resembling of real-world interaction.To encode communities, we extend the classical model of caveman graphs to a richer graph family that displays levels of homophily, i.e., where connections and opinions are highly intertwined. We show that, across these graphs, there are important cases when the influence gap correlation is a weak predictor due to communities, and a simpler metric, counting the initial partisan majority, provides a more accurate prediction overall. Using regression models, we further demonstrate that the influence gap combined with the more successful metrics does increase their predictive power for some levels of homophily.},
  archive   = {C_AAMAS},
  author    = {Bara, Jacques and Lev, Omer and Turrini, Paolo},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {151–159},
  title     = {Predicting voting outcomes in presence of communities},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463976},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Robustness based on accountability in multiagent
organizations. <em>AAMAS</em>, 142–150. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multiagent system models do not typically encompass tools for tackling abnormal or exceptional situations. When a critical situation arises, and individual agents fail, the system as a whole usually fails, too. For realizing robust distributed systems, conceived as agent systems or organizations, it is necessary to keep a right level of situational awareness (of both agents and environment), through the introduction of the means for gathering and propagating feedback, upon which actions can be taken. This work proposes a notion of accountability that encompasses both a normative dimension, and a structural dimension, that serves for the purpose of robustness.},
  archive   = {C_AAMAS},
  author    = {Baldoni, Matteo and Baroglio, Cristina and Micalizio, Roberto and Tedeschi, Stefano},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {142–150},
  title     = {Robustness based on accountability in multiagent organizations},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463975},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-robot task allocation-complexity and approximation.
<em>AAMAS</em>, 133–141. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-robot task allocation is one of the most fundamental classes of problems in robotics and is crucial for various real-world robotic applications such as search, rescue and area exploration. We consider the Single-Task robots and Multi-Robot tasks Instantaneous Assignment (ST-MR-IA) setting where each task requires at least a certain number of robots and each robot can work on at most one task and incurs an operational cost for each task. Our aim is to consider a natural computational problem of allocating robots to complete the maximum number of tasks subject to budget constraints. We consider budget constraints of three different kinds: (1) total budget, (2) task budget, and (3) robot budget. We provide a detailed complexity analysis including results on approximations as well as polynomial-time algorithms for the general setting and important restricted settings.},
  archive   = {C_AAMAS},
  author    = {Aziz, Haris and Chan, Hau and Cseh, \&#39;{A}gnes and Li, Bo and Ramezani, Fahimeh and Wang, Chenhao},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {133–141},
  title     = {Multi-robot task allocation-complexity and approximation},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463974},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). State-aware variational thompson sampling for deep
q-networks. <em>AAMAS</em>, 124–132. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Thompson sampling is a well-known approach for balancing exploration and exploitation in reinforcement learning. It requires the posterior distribution of value-action functions to be maintained; this is generally intractable for tasks that have a high dimensional state-action space. We derive a variational Thompson sampling approximation for DQNs which uses a deep network whose parameters are perturbed by a learned variational noise distribution. We interpret the successful NoisyNets method citefortunato2018noisy as an approximation to the variational Thompson sampling method that we derive. Further, we propose State Aware Noisy Exploration (SANE) which seeks to improve on NoisyNets by allowing a non-uniform perturbation, where the amount of parameter perturbation is conditioned on the state of the agent. This is done with the help of an auxiliary perturbation module, whose output is state dependent and is learnt end to end with gradient descent. We hypothesize that such state-aware noisy exploration is particularly useful in problems where exploration in certainhigh risk states may result in the agent failing badly. We demonstrate the effectiveness of the state-aware exploration method in the off-policy setting by augmenting DQNs with the auxiliary perturbation module.},
  archive   = {C_AAMAS},
  author    = {Aravindan, Siddharth and Lee, Wee Sun},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {124–132},
  title     = {State-aware variational thompson sampling for deep Q-networks},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463973},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cooperation and reputation dynamics with reinforcement
learning. <em>AAMAS</em>, 115–123. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Creating incentives for cooperation is a challenge in natural and artificial systems. One potential answer is reputation, whereby agents trade the immediate cost of cooperation for the future benefits of having a good reputation. Game theoretical models have shown that specific social norms can make cooperation stable, but how agents can independently learn to establish effective reputation mechanisms on their own is less understood. We use a simple model of reinforcement learning to show that reputation mechanisms generate two coordination problems: agents need to learn how to coordinate on the meaning of existing reputations and collectively agree on a social norm to assign reputations to others based on their behavior. These coordination problems exhibit multiple equilibria, some of which effectively establish cooperation. When we train agents with a standard Q-learning algorithm in an environment with the presence of reputation mechanisms, convergence to undesirable equilibria is widespread. We propose two mechanisms to alleviate this: (i) seeding a proportion of the system with fixed agents that steer others towards good equilibria; and (ii), intrinsic rewards based on the idea of introspection, i.e., augmenting agents&#39; rewards by an amount proportionate to the performance of their own strategy against themselves. A combination of these simple mechanisms is successful in stabilizing cooperation, even in a fully decentralized version of the problem where agents learn to use and assign reputations simultaneously. We show how our results relate to the literature in Evolutionary Game Theory, and discuss implications for artificial, human and hybrid systems, where reputations can be used as a way to establish trust and cooperation.},
  archive   = {C_AAMAS},
  author    = {Anastassacos, Nicolas and Garc\&#39;{\i}a, Julian and Hailes, Stephen and Musolesi, Mirco},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {115–123},
  title     = {Cooperation and reputation dynamics with reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463972},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Interrogating the black box: Transparency through
information-seeking dialogues. <em>AAMAS</em>, 106–114. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper is preoccupied with the following question: given a (possibly opaque) learning system, how can we understand whether its behaviour adheres to governance constraints? The answer can be quite simple: we just need to &quot;ask&quot; the system about it. We propose to construct an investigator agent to query a learning agent-- the suspect agent-- to investigate its adherence to a given ethical policy in the context of an information-seeking dialogue, modeled in formal argumentation settings. This formal dialogue framework is the main contribution of this paper. Through it, we break down compliance checking mechanisms into three modular components, each of which can be tailored to various needs in a vast amount of ways: an investigator agent, a suspect agent, and an acceptance protocol determining whether the responses of the suspect agent comply with the policy. This acceptance protocol presents a fundamentally different approach to aggregation: rather than using quantitative methods to deal with the non-determinism of a learning system, we leverage the use of argumentation semantics to investigate the notion of properties holding consistently. Overall, we argue that the introduced formal dialogue framework opens many avenues both in the area of compliance checking and in the analysis of properties of opaque systems.},
  archive   = {C_AAMAS},
  author    = {Aler Tubella, Andrea and Theodorou, Andreas and Nieves, Juan Carlos},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {106–114},
  title     = {Interrogating the black box: Transparency through information-seeking dialogues},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463971},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Minimum-delay adaptation in non-stationary reinforcement
learning via online high-confidence change-point detection.
<em>AAMAS</em>, 97–105. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Non-stationary environments are challenging for reinforcement learning algorithms. If the state transition and/or reward functions change based on latent factors, the agent is effectively tasked with optimizing a behavior that maximizes performance over a possibly infinite random sequence of Markov Decision Processes (MDPs), each of which drawn from some unknown distribution. We call each such MDP acontext. Most related works make strong assumptions such as knowledge about the distribution over contexts, the existence of pre-training phases, ora priori knowledge about the number, sequence, or boundaries between contexts. We introduce an algorithm that efficiently learns policies in non-stationary environments. It analyzes a possibly infinite stream of data and computes, in real-time, high-confidence change-point detection statistics that reflect whether novel, specialized policies need to be created and deployed to tackle novel contexts, or whether previously-optimized ones might be reused. We show that(i) this algorithm minimizes the delay until unforeseen changes to a context are detected, thereby allowing for rapid responses; and(ii) it bounds the rate of false alarm, which is important in order to minimize regret. Our method constructs a mixture model composed of a (possibly infinite) ensemble of probabilistic dynamics predictors that model the different modes of the distribution over underlying latent MDPs. We evaluate our algorithm on high-dimensional continuous reinforcement learning problems and show that it outperforms state-of-the-art (model-free and model-based) RL algorithms, as well as state-of-the-art meta-learning methods specially designed to deal with non-stationarity.},
  archive   = {C_AAMAS},
  author    = {Alegre, Lucas N. and Bazzan, Ana L. C. and da Silva, Bruno C.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {97–105},
  title     = {Minimum-delay adaptation in non-stationary reinforcement learning via online high-confidence change-point detection},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463970},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Siting and sizing of charging infrastructure for shared
autonomous electric fleets. <em>AAMAS</em>, 88–96. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Business models rooted in shared economy, electrification, and automation are transforming urban mobility. Accounting for how these transformations interact is crucial if synergies are to be exploited. In this paper, we focus on how a cost-effective charging infrastructure for e-mobility can support the emergence of shared, autonomous mobility. This study addresses the problem of siting and sizing of charging stations for a fleet of shared autonomous electric vehicles (SAEVs). We develop a hybrid simulation-optimization model to find locations and numbers of chargers needed to serve charging demands. Our agent-based model provides an enhanced representation of SAEV operations allowing for smart charging and vehicle cruising when parking/charging is not available. Also, we model charging station placement as full covering optimization and solve the location-allocation problem simultaneously. Finally, we employ real-world trip data from ShareNow in Berlin to evaluate our approach for realistic demand patterns under different charging strategies and fleet sizes. The results show that charging station locations depend mostly on the spatial distribution of installation costs and charging demands. Moreover, charging strategies and fleet size affect the charging patterns and the required number of chargers as well as fleet performance.},
  archive   = {C_AAMAS},
  author    = {Ahadi, Ramin and Ketter, Wolfgang and Collins, John and Daina, Nicol\`{o}},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {88–96},
  title     = {Siting and sizing of charging infrastructure for shared autonomous electric fleets},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463969},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Off-policy exploitability-evaluation in two-player zero-sum
markov games. <em>AAMAS</em>, 78–87. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Off-policy evaluation (OPE) is the problem of evaluating new policies using historical data obtained from a different policy. In the recent OPE context, most studies have focused on single-player cases, and not on multi-player cases. In this study, we propose OPE estimators constructed by the doubly robust and double reinforcement learning estimators in two-player zero-sum Markov games. The proposed estimators project exploitability that is often used as a metric for determining how close a policy profile (i.e., a tuple of policies) is to a Nash equilibrium in two-player zero-sum games. We prove the exploitability estimation error bounds for the proposed estimators. We then propose the methods to find the best candidate policy profile by selecting the policy profile that minimizes the estimated exploitability from a given policy profile class. We prove the regret bounds of the policy profiles selected by our methods. Finally, we demonstrate the effectiveness and performance of the proposed estimators through experiments.},
  archive   = {C_AAMAS},
  author    = {Abe, Kenshi and Kaneko, Yusuke},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {78–87},
  title     = {Off-policy exploitability-evaluation in two-player zero-sum markov games},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463968},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Reason explanation for encouraging behaviour change
intention. <em>AAMAS</em>, 68–77. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The demand for intelligent virtual advisors in our rapidly advancing world is rising and, consequently, the need for understanding the reasoning process to answer why a particular piece of advice is provided to the user is directly increasing. Personalized explanation is regarded as a reliable way to improve the user&#39;s understanding and trust in the virtual advisor. So far, cognitive explainable agents utilize reason explanation by referring to their own mental state (beliefs and goals) to explain their own behaviour. However, when the explainable agent plays the role of a virtual advisor and recommends a behaviour for the human to perform, it is best to refer to the user&#39;s mental state, rather than the agent&#39;s mental state, to form a reason explanation. In this paper, we are developing an explainable virtual advisor (XVA) that communicates with the user to elicit the user&#39;s beliefs and goals and then tailors its advice and explains it according to the user&#39;s mental state. We tested the proposed XVA with university students where the XVA provides tips to reduce the students&#39; study stress. We measured the impact of receiving three different patterns of tailored explanations (belief-based, goal-based, and beliefandgoal-based explanation) in terms of the students&#39; intentions to change their behaviours. The results showed that the intention to change is not only related to the explanation pattern but also to the user context, the relationship built with the agent, the type of behaviour recommended and the user&#39;s current intention to do the behaviour.},
  archive   = {C_AAMAS},
  author    = {Abdulrahman, Amal and Richards, Deborah and Bilgin, Ayse Aysin},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {68–77},
  title     = {Reason explanation for encouraging behaviour change intention},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463967},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Mechanism design powered by social interactions.
<em>AAMAS</em>, 63–67. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mechanism design has traditionally assumed that the set of participants are fixed and known to the mechanism (the market owner) in advance. However, in practice, the market owner can only directly reach a small number of participants (her neighbours). Hence the owner often needs costly promotions to recruit more participants in order to get desirable outcomes such as social welfare or revenue maximization. In this paper, we propose to incentivize existing participants to invite their neighbours to attract more participants. However, they would not invite each other if they are competitors. We discuss how to utilize the conflict of interest between the participants to incentivize them to invite each other to form larger markets. We will highlight the early solutions and open the floor for discussing the fundamental open questions in the settings of auctions, coalitional games, matching and voting.},
  archive   = {C_AAMAS},
  author    = {Zhao, Dengji},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {63–67},
  title     = {Mechanism design powered by social interactions},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463965},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Responsibility research for trustworthy autonomous systems.
<em>AAMAS</em>, 57–62. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To develop and effectively deploy Trustworthy Autonomous Systems (TAS), we face various social, technological, legal, and ethical challenges in which different notions of responsibility can play a key role. In this work, we elaborate on these challenges, discuss research gaps, and show how the multidimensional notion of responsibility can play a role to bridge them. We argue that TAS requires operational tools to represent and reason about responsibilities of humans as well as AI agents. We review major challenges to which responsibility reasoning can contribute, highlight open research problems, and argue for the application of multiagent responsibility models in a variety of TAS domains.},
  archive   = {C_AAMAS},
  author    = {Yazdanpanah, Vahid and Gerding, Enrico H. and Stein, Sebastian and Dastani, Mehdi and Jonker, Catholijn M. and Norman, Timothy J.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {57–62},
  title     = {Responsibility research for trustworthy autonomous systems},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463964},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Diverse auto-curriculum is critical for successful
real-world multiagent learning systems. <em>AAMAS</em>, 51–56. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multiagent reinforcement learning (MARL) has achieved a remarkable amount of success in solving various types of video games. A cornerstone of this success is the auto-curriculum framework, which shapes the learning process by continually creating new challenging tasks for agents to adapt to, thereby facilitating the acquisition of new skills. In order to extend MARL methods to real-world domains outside of video games, we envision in this blue sky paper that maintaining a diversity-aware auto-curriculum is critical for successful MARL applications. Specifically, we argue that behavioural diversity is a pivotal, yet under-explored, component for real-world multiagent learning systems, and that significant work remains in understanding how to design a diversity-aware auto-curriculum. We list four open challenges for auto-curriculum techniques, which we believe deserve more attention from this community. Towards validating our vision, we recommend modelling realistic interactive behaviours in autonomous driving as an important test bed, and recommend the SMARTS/ULTRA benchmark.},
  archive   = {C_AAMAS},
  author    = {Yang, Yaodong and Luo, Jun and Wen, Ying and Slumbers, Oliver and Graves, Daniel and Bou Ammar, Haitham and Wang, Jun and Taylor, Matthew E.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {51–56},
  title     = {Diverse auto-curriculum is critical for successful real-world multiagent learning systems},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463963},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Better metrics for evaluating explainable artificial
intelligence. <em>AAMAS</em>, 45–50. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents objective metrics for how explainable artificial intelligence (XAI) can be quantified. Through an overview of current trends, we show that many explanations are generated post-hoc and independent of the agent&#39;s logical process, which in turn creates explanations with limited meaning as they lack transparency and fidelity. While user studies are a known basis for evaluating XAI, studies that do not consider objective metrics for evaluating XAI may have limited meaning and may suffer from confirmation bias, particularly if they use low fidelity explanations unnecessarily. To avoid this issue, this paper suggests a paradigm shift in evaluating XAI that focuses on metrics that quantify the explanation itself and its appropriateness given the XAI goal. We suggest four such metrics based on performance differences, D, between the explanation&#39;s logic and the agent&#39;s actual performance, the number of rules, R, outputted by the explanation, the number of features, F, used to generate that explanation, and the stability, S, of the explanation. We believe that user studies that focus on these metrics in their evaluations are inherently more valid and should be integrated in future XAI research.},
  archive   = {C_AAMAS},
  author    = {Rosenfeld, Avi},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {45–50},
  title     = {Better metrics for evaluating explainable artificial intelligence},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463962},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Autonomous agents and multiagent systems challenges in earth
observation satellite constellations. <em>AAMAS</em>, 39–44. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We identify several challenges and opportunities opened to agent and multiagent systems, following the recent developments in the domain of Earth observation constellations. We focus on three challenge categories that manifest in this field: (i) configuration problems of constellations and ground stations used to operate them, potentially owned by different actors, as to provide better services and coordination; (ii) offline planning and scheduling problems, which consist in finding solution methods to schedule observation and upload/download tasks over the constellation; (iii) the design of efficient and reactive online operation methods as to adapt schedules in dynamic settings. Being naturally distributed and composed of multiple entities and users, these problems clearly fit the multiagent paradigm, and may challenge researchers for many years.},
  archive   = {C_AAMAS},
  author    = {Picard, Gauthier and Caron, Cl\&#39;{e}ment and Farges, Jean-Loup and Guerra, Jonathan and Pralet, C\&#39;{e}dric and Roussel, St\&#39;{e}phanie},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {39–44},
  title     = {Autonomous agents and multiagent systems challenges in earth observation satellite constellations},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463961},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Sparse training theory for scalable and efficient agents.
<em>AAMAS</em>, 34–38. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A fundamental task for artificial intelligence is learning. Deep Neural Networks have proven to cope perfectly with all learning paradigms, i.e. supervised, unsupervised, and reinforcement learning. Nevertheless, traditional deep learning approaches make use of cloud computing facilities and do not scale well to autonomous agents with low computational resources. Even in the cloud, they suffer from computational and memory limitations, and they cannot be used to model adequately large physical worlds for agents which assume networks with billions of neurons. These issues are addressed in the last few years by the emerging topic of sparse training, which trains sparse networks from scratch. This paper discusses sparse training state-of-the-art, its challenges and limitations while introducing a couple of new theoretical research directions which has the potential of alleviating sparse training limitations to push deep learning scalability well beyond its current boundaries. Nevertheless, the theoretical advancements impact in complex multi-agents settings is discussed from a real-world perspective, using the smart grid case study.},
  archive   = {C_AAMAS},
  author    = {Mocanu, Decebal Constantin and Mocanu, Elena and Pinto, Tiago and Curci, Selima and Nguyen, Phuong H. and Gibescu, Madeleine and Ernst, Damien and Vale, Zita A.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {34–38},
  title     = {Sparse training theory for scalable and efficient agents},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463960},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). The seeing-eye robot grand challenge: Rethinking automated
care. <em>AAMAS</em>, 28–33. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automated care systems are becoming more tangible than ever: recent breakthroughs in robotics and machine learning can be used to address the need for automated care created by the increasing aging population. However, such systems require overcoming several technological, ethical, and social challenges. One inspirational manifestation of these challenges can be observed in the training of seeing-eye dogs for visually impaired people. A seeing-eye dog is not just trained to obey its owner, but also to &quot;intelligently disobey&quot;: if it is given an unsafe command from its handler, it is taught to disobey it or even insist on a different course of action. This paper proposes the challenge of building a seeing-eye robot, as a thought-provoking use-case that helps identify the challenges to be faced when creating behaviors for robot assistants in general. Through this challenge, this paper delineates the prerequisites that an automated care system will need to have in order to perform intelligent disobedience and to serve as a true agent for its handler.},
  archive   = {C_AAMAS},
  author    = {Mirsky, Reuth and Stone, Peter},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {28–33},
  title     = {The seeing-eye robot grand challenge: Rethinking automated care},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463959},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Environment shift games: Are multiple agents the solution,
and not the problem, to non-stationarity? <em>AAMAS</em>, 23–27. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Machine learning and artificial intelligence models that interact with and in an environment will unavoidably have impact on this environment and change it. This is often a problem as many methods do not anticipate such a change in the environment and thus may start acting sub-optimally. Although efforts are made to deal with this problem, we believe that a lot of potential is unused. Driven by the recent success of predictive machine learning, we believe that in many scenarios one can predict when and how a change in the environment will occur. In this paper we introduce a blueprint that intimately connects this idea to the multiagent setting, showing that the multiagent community has a pivotal role to play in addressing the challenging problem of changing environments.},
  archive   = {C_AAMAS},
  author    = {Mey, Alexander and Oliehoek, Frans A.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {23–27},
  title     = {Environment shift games: Are multiple agents the solution, and not the problem, to non-stationarity?},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463958},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Multi-modal agents for business intelligence.
<em>AAMAS</em>, 17–22. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Given their installation on nearly a billion consumer devices around the world, consumers clearly enjoy using voice-driven assistants such as Alexa, Siri and Google Home, and find it compelling to interact with AI agents as quasi-human entities. Inevitably, people who are accustomed to using voice-driven assistants at home and in the car will expect to use such technologies in the workplace. What form will this take? Simple extrapolations from consumer space (e.g. running meetings or presentations) promise only modest value. I propose that AAMAS and the AI research community should pursue a bolder vision in which software agents act as quasi-human collaborators on core business intelligence tasks that entail analyzing data, diagnosing problems, and making decisions. Moreover, these business intelligence agents should communicate multi-modally, i.e. they must understand and employ speech complemented by non-verbal behaviors such as pointing, eye gaze, and facial expression. I outline requisite agent, MAS and AI technologies and pose several fundamental research challenges raised by this vision.},
  archive   = {C_AAMAS},
  author    = {Kephart, Jeffrey O.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {17–22},
  title     = {Multi-modal agents for business intelligence},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463957},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Cognitive homeostatic agents. <em>AAMAS</em>, 12–16. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human brain has been used as an inspiration for building autonomous agents, but it is not obvious what level of computational description of the brain one should use. This has led to overly opinionated symbolic approaches and overly unstructured connectionist approaches. We propose that using homeostasis as the computational description provides a good compromise. Similar to how physiological homeostasis is the regulation of certain homeostatic variables, cognition can be interpreted as the regulation of certain &#39;cognitive homeostatic variables&#39;. We present an outline of a Cognitive Homeostatic Agent, built as a hierarchy of physiological and cognitive homeostatic subsystems and describe structures and processes to guide future exploration. We expect this to be a fruitful line of investigation towards building sophisticated artificial agents that can act flexibly in complex environments, and produce behaviors indicating planning, thinking and feelings.},
  archive   = {C_AAMAS},
  author    = {Kelkar, Amol},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {12–16},
  title     = {Cognitive homeostatic agents},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463956},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Models we can trust: Toward a systematic discipline of
(agent-based) model interpretation and validation. <em>AAMAS</em>, 6–11.
(<a href="https://dl.acm.org/doi/10.5555/3463952.3463955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We advocate the development of a discipline of interacting with and extracting information from models, both mathematical (e.g. game-theoretic ones) and computational (e.g. agent-based models). We outline some directions for the development of a such a discipline: - the development of logical frameworks for the systematic formal specification of stylized facts and social mechanisms in (mathematical and computational) social science. Such frameworks would bring to attention new issues, such as phase transitions, i.e. dramatical changes in the validity of the stylized facts beyond some critical values in parameter space. We argue that such statements are useful for those logical frameworks describing properties of ABM. - the adaptation of tools from the theory of reactive systems (such as bisimulation) to obtain practically relevant notions of two systems &quot;having the same behavior&quot;. - the systematic development of an adversarial theory of model perturbations, that investigates the robustness of conclusions derived from models of social behavior to variations in several features of the social dynamics. These may include: activation order, the underlying social network, individual agent behavior.},
  archive   = {C_AAMAS},
  author    = {Istrate, Gabriel},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {6–11},
  title     = {Models we can trust: Toward a systematic discipline of (Agent-based) model interpretation and validation},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463955},
  year      = {2021},
}
</textarea>
</details></li>
<li><details>
<summary>
(2021). Broadening the research agenda for computational social
choice: Multiple preference profiles and multiple solutions.
<em>AAMAS</em>, 1–5. (<a
href="https://dl.acm.org/doi/10.5555/3463952.3463954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The area of computational social choice (COMSOC) analyzes collective decision problems from an algorithmic perspective. So far, the main focus in this area lied on analyzing problems where a single preference relation for each agent is given and a single solution reflecting all agents&#39; preferences needs to be found. However, this modeling is often not rich enough to capture the changing and ambivalent nature of real-world problems. We will argue that one possibility to incorporate such aspects is to allow for multiple preference profiles in the input and multiple solutions in the output. We systematically review different types of arising settings, point out how classical problems and solution concepts can be generalized, and identify several research challenges.},
  archive   = {C_AAMAS},
  author    = {Boehmer, Niclas and Niedermeier, Rolf},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1–5},
  title     = {Broadening the research agenda for computational social choice: Multiple preference profiles and multiple solutions},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3463954},
  year      = {2021},
}
</textarea>
</details></li>
</ul>

</body>
</html>
