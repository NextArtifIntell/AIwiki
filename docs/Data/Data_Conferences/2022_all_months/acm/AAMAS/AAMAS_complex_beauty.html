<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>AAMAS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="aamas---306">AAMAS - 306</h2>
<ul>
<li><details>
<summary>
(2022). Towards addressing dynamic multi-agent task allocation in
law enforcement. <em>AAMAS</em>, 1950–1951. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To deal with the underlying heterogeneous law enforcement problem (LEPH), one needs to allocate police officers to dynamic tasks whose locations, arrival times, and importance levels are unknown a priory. Addressing this challenge and inspired by real police logs, this research aims to solve the LEPH problem by using and comparing three methods: Fisher market-based FMC_TAH+, swarm intelligence HDBA, and Simulated Annealing SA algorithms. The three methods were compared in this study for the performance measures that are commonly used by law enforcement authorities. The results indicate an advantage for FMC_TAH+ both in total utility and in the average arrival time to tasks. Also, compared respectively to HDBA and SA, FMC_TAH+ leads to 34\% and 32\% higher team utility in the highest shift workload.},
  archive   = {C_AAMAS},
  author    = {Tkach, Itshak and Amador Nelke, Sofia},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1950–1951},
  title     = {Towards addressing dynamic multi-agent task allocation in law enforcement},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536163},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Concurrent negotiations with global utility functions.
<em>AAMAS</em>, 1947–1949. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automated Negotiation is attracting more attention from researchers recently as it is becoming more relevant to industrial and business applications with increased reliance on automated systems. Most research in this area assumes either a single negotiation thread with a well-defined utility function for each agent involved or a set of concurrent negotiations with an ordering of outcomes in each local negotiation. In this paper, we consider an agent engaged in a set of concurrent negotiations with a utility function defined only for the complete set of agreements in all of them and no locally defined ordering of outcomes in any negotiation independent from what happens in the others. We argue that this problem setting is interesting both from the academic and the industrial points of view. The paper then presents an algorithm that allows such agent to maximize its expected global utility by orchestrating its behavior in all negotiation threads. The performance of the proposed method is analyzed theoretically and empirically using simulation.},
  archive   = {C_AAMAS},
  author    = {Mohammad, Yasser and Nakadai, Shinji},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1947–1949},
  title     = {Concurrent negotiations with global utility functions},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536162},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Trust repair in human-agent teams: The effectiveness of
explanations and expressing regret. <em>AAMAS</em>, 1944–1946. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trust is a fundamental aspect of teamwork in Human-Agent Teams (HATs). Trust violations are an inevitable aspect of the cycle of trust, so effective trust repair strategies are needed to ensure durable and successful team performance. This study explores the effectiveness of four trust repair strategies. In a first-person shooter resembling HAT task, a trust violation was provoked when the robotic agent failed to detect an approaching enemy. After this, the agent offered an apology composed of an explanation and/or an expression of regret (either one alone, both or neither). Our results indicated that expressing regret was crucial for effective trust repair, and that trust repair was most effective when the apology contained both components.},
  archive   = {C_AAMAS},
  author    = {Kox, E.S. and Kerstholt, J.H. and Hueting, T.F. and de Vries, P.W.},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1944–1946},
  title     = {Trust repair in human-agent teams: The effectiveness of explanations and expressing regret},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536161},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic calibration framework of agent-based models for
dynamic and heterogeneous parameters. <em>AAMAS</em>, 1941–1943. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Agent-based models (ABMs) highlight the importance of simulation validation, such as qualitative face validation and quantitative empirical validation. In particular, we focused on quantitative validation by adjusting simulation input parameters of the ABM. This study introduces anautomatic calibration framework that combines the suggested dynamic and heterogeneous calibration methods. Specifically, the dynamic calibration fits the simulation results to the real-world data by automatically capturing suitable simulation time to adjust the simulation parameters. Meanwhile, the heterogeneous calibration reduces the distributional discrepancy between individuals in the simulation and the real world by adjusting agent related parameters cluster-wisely.},
  archive   = {C_AAMAS},
  author    = {Kim, Dongjun and Yun, Tae-Sub and Moon, Il-Chul and Bae, Jang Won},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1941–1943},
  title     = {Automatic calibration framework of agent-based models for dynamic and heterogeneous parameters},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536160},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Designing efficient and fair mechanisms for multi-type
resource allocation. <em>AAMAS</em>, 1938–1940. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the multi-type resource allocation problem (MTRA), there are d ≥ 2 types of items, and n agents who each demand one unit of items of each type and have strict linear preferences over bundles consisting of one item of each type. For MTRAs with indivisible items, we first present an impossibility result that no mechanism can satisfy both sd-efficiency and sd-envy-freeness. We show that this impossibility result is circumvented under the natural assumption of lexicographic preferences by providing lexicographic probabilistic serial (LexiPS) as an extension of the probabilistic serial (PS) mechanism. We also prove that LexiPS satisfies sd-efficiency and sd-envy-freeness. Moreover, LexiPS satisfies sd-weak-strategyproofness when agents are not allowed to misreport their importance orders. The multi-type probabilistic serial cannot deal with indivisible items, but provides a stronger efficiency guarantee under the unrestricted domain of strict linear preferences for divisible items, while also retaining desirable fairness guarantees.},
  archive   = {C_AAMAS},
  author    = {Guo, Xiaoxi and Sikdar, Sujoy and Wang, Haibin and Xia, Lirong and Cao, Yongzhi and Wang, Hanpin},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1938–1940},
  title     = {Designing efficient and fair mechanisms for multi-type resource allocation},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536159},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GDL as a unifying domain description language for
declarative automated negotiation. <em>AAMAS</em>, 1935–1937. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We show that Game Description Language (GDL) can be used to describe some of the most commonly used test-beds in the automated negotiations literature, namely Genius and Colored Trails. This opens up an entirely new, declarative, approach to automated negotiation, in which a single algorithm can negotiate over a very broad class of different negotiation domains. We formally prove that the set of possible agreements of any negotiation domain from Genius (either linear or non-linear) can be modeled as a set of strategies over a deterministic extensive-form game that can be described efficiently in GDL. Furthermore, we show experimentally that, given only this GDL description, we can explore the agreement space efficiently using entirely generic domain-independent algorithms. In addition, we show that the same also holds for negotiation domains in the Colored Trails framework. This means we have the basic ingredients to implement a single negotiating agent that is capable of negotiating over many different kinds of negotiation domains, including Genius and Colored Trails.},
  archive   = {C_AAMAS},
  author    = {de Jonge, Dave and Zhang, Dongmo},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1935–1937},
  title     = {GDL as a unifying domain description language for declarative automated negotiation},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536158},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enabling BDI group plans with coordination middleware:
Semantics and implementation. <em>AAMAS</em>, 1932–1934. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This extended abstract summarises an article with the same title published in the journal Autonomous Agents and Multi-Agent Systems.},
  archive   = {C_AAMAS},
  author    = {Cranefield, Stephen},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1932–1934},
  title     = {Enabling BDI group plans with coordination middleware: Semantics and implementation},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536157},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Voting with random classifiers (VORACE): Theoretical and
experimental analysis. <em>AAMAS</em>, 1929–1931. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ensemble methods are built by training many different models and aggregating their outputs to output the prediction of the whole system. In this work, we study the behavior of an ensemble method where voting rules are used to aggregate the output of a set of randomly-generated classifiers. We provide both a theoretical and an empirical analysis of this method, showing that it performs comparably with other state-of-the-art ensemble methods, while not requiring any domain expertise to fine-tune the individual classifiers.},
  archive   = {C_AAMAS},
  author    = {Cornelio, Cristina and Donini, Michele and Loreggia, Andrea and Pini, Maria Silvia and Rossi, Francesca},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1929–1931},
  title     = {Voting with random classifiers (VORACE): Theoretical and experimental analysis},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536156},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Combining quantitative and qualitative reasoning in
concurrent multi-player games. <em>AAMAS</em>, 1926–1928. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose and study a general framework for modelling and formal reasoning about multi-agent systems and, in particular, multi-stage games where both quantitative and qualitative objectives and constraints are involved. Our models enrich concurrent game models with payoffs and guards on actions associated with each state of the model. We propose a quantitative extension of the logic ATLs that enables combination of quantitative and qualitative reasoning. We illustrate the framework with some examples and then consider the model-checking problems arising in it and establish some general undecidability and decidability results for them.},
  archive   = {C_AAMAS},
  author    = {Bulling, Nils and Goranko, Valentin},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1926–1928},
  title     = {Combining quantitative and qualitative reasoning in concurrent multi-player games},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536155},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Goal-driven active learning. <em>AAMAS</em>, 1923–1925. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite recent breakthroughs for learning a rich set of behaviors in simulated tasks, reinforcement learning agents are not yet in widespread use in the real world where rewards are naturally sparse. In fact, efficient exploration remains a key challenge in sparse-reward tasks as it requires quickly finding informative and task-relevant experiences. While cloning behaviors provided by an expert is a promising approach to the exploration problem, learning from a fixed set of demonstrations may be impracticable due to lack of state coverage or distribution mismatch - when the learner&#39;s goal deviates from the demonstrated behaviors. Moreover, we aim to obtain a policy that can accomplish a variety of goals guided by the same set of demonstrations (i.e. without additional human effort). We present a goal-conditioned method that leverages very small sets of goal-driven demonstrations to significantly accelerate learning. Crucially, we present the concept of active goal-driven demonstrations to query the demonstrator only in hard-to-learn and uncertain regions of the state space. We evaluate our framework on a set of robot control tasks. Our method outperforms prior imitation learning approaches in most of the tasks in terms of data efficiency and scores while reducing the amount of human effort.},
  archive   = {C_AAMAS},
  author    = {Bougie, Nicolas and Ichise, Ryutaro},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1923–1925},
  title     = {Goal-driven active learning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536154},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reaching consensus under a deadline. <em>AAMAS</em>,
1920–1922. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Group decisions are often complicated by a deadline. For example, in committee hiring decisions, the deadline might be a budget&#39;s start date or the beginning of a semester. It may be that if no candidate is supported by a strong majority, the default is to hire no one - an option that may cost dearly. Hence, committee members might prefer to agree on a reasonable, rather than the best, candidate, to avoid unfilled positions. Here, we propose a model for the above scenario -Consensus Under a Deadline (CUD)- based on a time-bounded iterative voting process. We provide theoretical convergence guarantees and an analysis of the resulting decision quality. An extensive experimental study demonstrates more subtle features of CUDs, e.g., the difference between two simple types of committee member behavior, lazy vs. proactive voters. Finally, a user study examines the differences between the behavior of rational voting bots and human voters, concluding that it may often be best to have bots play on the humans&#39; behalf.},
  archive   = {C_AAMAS},
  author    = {B\&#39;{a}nnikova, Marina and Dery, Lihi and Obraztsova, Svetlana and Rabinovich, Zinovi and Rosenschein, Jeffrey S.},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1920–1922},
  title     = {Reaching consensus under a deadline},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536153},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). JEDAI: A system for skill-aligned explainable robot
planning. <em>AAMAS</em>, 1917–1919. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents JEDAI, an AI system designed for outreach and educational efforts aimed at non-AI experts. JEDAI features a novel synthesis of research ideas from integrated task and motion planning and explainable AI. JEDAI helps users create high-level, intuitive plans while ensuring that they will be executable by the robot. It also provides users customized explanations about errors and helps improve their understanding of AI planning as well as the limits and capabilities of the underlying robot system.},
  archive   = {C_AAMAS},
  author    = {Shah, Naman and Verma, Pulkit and Angle, Trevor and Srivastava, Siddharth},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1917–1919},
  title     = {JEDAI: A system for skill-aligned explainable robot planning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536151},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LBfT: Learning bayesian network structures from text in
autonomous typhoon response systems. <em>AAMAS</em>, 1914–1916. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Discovering variables and understanding their relations, which impacts emergency response, provide important knowledge to the development of decision models, e.g. Bayesian networks, in autonomous typhoon response systems (ATRS). Given the text inputs (containing natural language), learning the network structures still remains a challenge although learning Bayesian networks from data has been extensively investigated in various fields. In this demo, we develop a deep learning based framework for identifying typhoon relevant variables and build their causal relations from text. We use the CausalBank dataset and typhoon specific relation rules to refine the learned relations and allow users to further improve the models through their domain knowledge. We integrate the new learning tool into the existing ATRS and demonstrate the empirical results through real-world typhoon reports.},
  archive   = {C_AAMAS},
  author    = {Pan, Yinghui and Chen, Junhan and Zeng, Yifeng and Yao, Zhangrui and Li, Qianwen and Ma, Biyang and Ji, Yi and Ming, Zhong},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1914–1916},
  title     = {LBfT: Learning bayesian network structures from text in autonomous typhoon response systems},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536150},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ev-IDID: Enhancing solutions to interactive dynamic
influence diagrams through evolutionary algorithms. <em>AAMAS</em>,
1911–1913. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Interactive dynamic influence diagrams~(I-DIDs) are a general framework for multiagent sequential decision making under uncertainty. Due to the model complexity, a significant amount of research has been invested into solving the model through various types of either exact or approximate algorithms. However, there is no tool that allows users to specify the algorithm parameters and visualise the model solutions. In this demo, we develop an interactive I-DID system that implements most the state-of-art I-DID algorithms and develops a new type of algorithms based on evolutionary computation. In particular, we propose a multi-population genetic algorithm for solving the I-DID models and automate the generation of behavioural models in the solutions. This demo will facilitate the I-DID research development and practical applications, and elicit a new wave of I-DID solutions based on evolutionary algorithms.},
  archive   = {C_AAMAS},
  author    = {Ma, Biyang and Pan, Yinghui and Zeng, Yifeng and Ming, Zhong},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1911–1913},
  title     = {Ev-IDID: Enhancing solutions to interactive dynamic influence diagrams through evolutionary algorithms},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536149},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cellulan world: Interactive platform to learn swarm
behaviors. <em>AAMAS</em>, 1908–1910. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Swarming behaviors are ubiquitous in nature, and an important topic to teach and learn. In the learning sciences domain of complex systems understanding, studies have shown that novices tend to assume that centralized control and leadership should exist whenever they see complex patterns. In this paper, we present an educational framework consisting of a swarm of virtual and/or physical agents (robots) as a game-based learning environment to introduce the underlying rules of swarm behaviors.},
  archive   = {C_AAMAS},
  author    = {Khodr, Hala and Bruno, Barbara and Kothiyal, Aditi and Dillenbourg, Pierre},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1908–1910},
  title     = {Cellulan world: Interactive platform to learn swarm behaviors},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536148},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SIERRA: A modular framework for research automation.
<em>AAMAS</em>, 1905–1907. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern intelligent systems researchers form hypotheses about system behavior, and then run experiments using one or more independent variables to test their hypotheses. We present SIERRA, a novel framework structured around that idea for accelerating research developments and improving reproducibility of results. SIERRA makes it easy to quickly specify the independent variable(s) for an experiment, generate experimental inputs, automatically run the experiment, and process the results to generate deliverables such as graphs and videos. SIERRA provides reproducible automation independent of the execution environment (HPC hardware, real robots, etc.) and targeted platform (arbitrary simulator or real robots), enabling exact experiment replication (up to the limit of the execution environment and platform). It employs a deeply modular approach that allows easy customization and extension of automation for the needs of individual researchers, thereby eliminating manual experiment configuration and result processing via throw-away scripts.},
  archive   = {C_AAMAS},
  author    = {Harwell, John and Lowmanstone, London and Gini, Maria},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1905–1907},
  title     = {SIERRA: A modular framework for research automation},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536147},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Demonstrating the rapid integration and development
environment (RIDE): Embodied conversational agent (ECA) and multiagent
capabilities. <em>AAMAS</em>, 1902–1904. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We demonstrate the Rapid Integration and Development Environment (RIDE), a research and development platform that enables rapid prototyping in support of multiagents and embodied conversational agents. RIDE is based on commodity game engines and includes a flexible architecture, system interoperability, and native support for artificial intelligence and machine learning frameworks.},
  archive   = {C_AAMAS},
  author    = {Hartholt, Arno and Fast, Ed and Leeds, Andrew and Kim, Kevin and Gordon, Andrew and McCullough, Kyle and Ustun, Volkan and Mozgai, Sharon},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1902–1904},
  title     = {Demonstrating the rapid integration and development environment (RIDE): Embodied conversational agent (ECA) and multiagent capabilities},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536146},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multi-agent system for automated machine learning.
<em>AAMAS</em>, 1899–1901. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Machine Learning (ML) focuses on giving machines the ability to forecast, predict, or classify without being explicitly programmed to do so. To achieve such goals, large amounts of data are used to conceive models that can adapt to unseen data and to new scenarios. However, applying ML models to real-world business domains is a resource-intensive and time-consuming effort. Automated machine learning (AutoML) emerged as a way to ease such processes. With this in mind, this study introduces a multi-agent system (MAS) that autonomously go through the entire ML pipeline, with different entities being responsible for the data collection process, for pre-processing the data, and for deploying the best candidate ML model. The conceived MAS is currently implemented in a real-world setting, addressing important societal challenges raised by big urban centers. The obtained results show that this solution proved to be beneficial not only for the data collection and pre-processing tasks, but also for the automated execution of ML models.},
  archive   = {C_AAMAS},
  author    = {Fernandes, Bruno and Novais, Paulo and Analide, Cesar},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1899–1901},
  title     = {A multi-agent system for automated machine learning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536145},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). KnowLedger - a multi-agent system blockchain for smart
cities data. <em>AAMAS</em>, 1896–1898. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Smart Cities introduce novel problems related to the management of inordinate amounts of data of different types and sources. Hence, solutions are required where data may be committed by any actor, being, at the same time, freely available to the community. With this in mind, this study introduces KnowLedger, a multi-agent system (MAS) blockchain to store structured data, where a new protocol, entitled as Proof-of-Confidence, is used to award those who commit accurate and reliable data. The obtained results show that the MAS proved to be beneficial not only for data collection but mainly for the maintenance of the blockchain, with the agents being responsible for the entire workload, from data collection to mining.},
  archive   = {C_AAMAS},
  author    = {Fernandes, Bruno and Diogo, Andr\&#39;{e} and Silva, F\&#39;{a}bio and Neves, Jos\&#39;{e} and Analide, Cesar},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1896–1898},
  title     = {KnowLedger - a multi-agent system blockchain for smart cities data},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536144},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AdLeap-MAS: An open-source multi-agent simulator for ad-hoc
reasoning. <em>AAMAS</em>, 1893–1895. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ad-hoc reasoning models are recurrently used to solve some of our daily tasks. Intending to avoid worthless investments or spend valuable resources, these smart systems requires a proper evaluation before acting in the real-world. In this paper, we demonstrate AdLeap-MAS, a novel framework focused on enabling quick and easy testing of smart algorithms in ad-hoc reasoning domains.},
  archive   = {C_AAMAS},
  author    = {do Carmo Alves, Matheus Aparecido and Varma, Amokh and Elkhatib, Yehia and Soriano Marcolino, Leandro},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1893–1895},
  title     = {AdLeap-MAS: An open-source multi-agent simulator for ad-hoc reasoning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536143},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An agent-based simulator for maritime transport
decarbonisation. <em>AAMAS</em>, 1890–1892. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Greenhouse gas (GHG) emission reduction is an important and necessary goal; currently, different policies to reduce GHG emissions in maritime transport are being discussed. Amongst policies, like carbon taxes or carbon intensity targets, it is hard to determine which policies can successfully reduce GHG emissions while allowing the industry to be profitable. We introduce an agent-based maritime transport simulator to investigate the effectiveness of two decarbonisation policies by simulating a maritime transport operator&#39;s trade pattern and fleet make-up changes as a reaction to taxation and fixed targets. This first of its kind simulator allows to compare and quantify the difference of carbon reduction policies and how they affect shipping operations.},
  archive   = {C_AAMAS},
  author    = {Buermann, Jan and Georgiev, Dimitar and Gerding, Enrico H. and Hill, Lewis and Malik, Obaid and Pop, Alexandru and Pun, Matthew and Ramchurn, Sarvapali D. and Salisbury, Elliot and Stojanovic, Ivan},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1890–1892},
  title     = {An agent-based simulator for maritime transport decarbonisation},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536142},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Chameleon - a framework for developing conversational agents
for medical training purposes. <em>AAMAS</em>, 1887–1889. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Objective Clinical Structured Examination (OSCE) is used to assess multiple competencies in medical pedagogy such as efficiently eliciting relevant clinical history from a patient. Clinical interviewing is done in a question and answer fashion, making it amenable to computer simulation. We introduce Chameleon, a framework to create virtual patients in an OSCE setting using the conversational agent platform Dialogflow CX. Our framework consists of a generic chatbot that is capable of answering most questions (in a classic non-specific clinical interview) and which can be expanded to capture any clinical presentation, e.g. a patient with backpain.},
  archive   = {C_AAMAS},
  author    = {Abutaleb, Al-Hussein and Yun, Bruno},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1887–1889},
  title     = {Chameleon - a framework for developing conversational agents for medical training purposes},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536141},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Transferable environment poisoning: Training-time attack on
reinforcement learner with limited prior knowledge. <em>AAMAS</em>,
1884–1886. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As reinforcement learning (RL) systems are deployed in various safety-critical applications, it is imperative to understand how vulnerable they are to adversarial attacks. Of these, an environment-poisoning attack is considered particularly insidious, since environment hyper-parameters are significant in determining an RL policy yet prone to be accessed by third parties. In this work, we study an environment-poisoning attack (EPA) against RL at training time. Considering that environment alteration comes at a cost, we seek minimal poisoning in an unknown environment and aim to force a black-box RL agent to learn an attacker-designed policy.},
  archive   = {C_AAMAS},
  author    = {Xu, Hang},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1884–1886},
  title     = {Transferable environment poisoning: Training-time attack on reinforcement learner with limited prior knowledge},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536139},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The coaching scenario: Recommender systems with a long term
goal. A case study in changing dietary habits. <em>AAMAS</em>,
1881–1883. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This PhD work explores the way automated recommender systems can be built to help users develop healthier food consumption habits. The main focus is on developing methods of recommendation that allow long-term modifications in user&#39;s consumption habits and lasting changes in user&#39;s behaviour. We proposed a recommendation scenario were the user and the recommender can be seen as two agents interacting with each other. We also proposed a reinforcement learning formalism of the recommendation problem faced by the recommender system, as well as a choice criterion for recommendation and heuristics derived from that optimal criterion to guide recommendation.},
  archive   = {C_AAMAS},
  author    = {Vandeputte, Jules},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1881–1883},
  title     = {The coaching scenario: Recommender systems with a long term goal. a case study in changing dietary habits},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536138},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Engineering normative and cognitive agents with emotions and
values. <em>AAMAS</em>, 1878–1880. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While Artificial Intelligence (AI) has become part of our daily lives, there are emerging expectations for these AI systems to (1) reason over human factors with humans-in-the-loop (2) adapt to the changing environment or requirement in the real world. To achieve this goal, an agent must first incorporate emotions in its decision-making. Further, the agent must be capable of interpreting normative information from expressed emotions. Specifically, expressed emotions as information enable inference of non-observable mental states. Furthermore, the agent must consider human values and preferences. The research presented here proposes an agent architecture to accommodate these factors.},
  archive   = {C_AAMAS},
  author    = {Tzeng, Sz-Ting},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1878–1880},
  title     = {Engineering normative and cognitive agents with emotions and values},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536137},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Using multi-objective optimization to generate timely
responsive BDI agents. <em>AAMAS</em>, 1875–1877. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A BDI agent&#39;s ability to perform well depends on its reasoning time. If the reasoning is slow, it is possible that the environment has changed and the action selected is no longer optimal by the time the agent has finished to deliberate. This work then builds a BDI architecture using Anytime Algorithms that can control the amount of time used by the agent to reason and act on the environment. I briefly describe the proposed architecture and its implementation in the Jason agent language.},
  archive   = {C_AAMAS},
  author    = {Stabile Junior, M\&#39;{a}rcio Fernando},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1875–1877},
  title     = {Using multi-objective optimization to generate timely responsive BDI agents},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536136},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The reputation lag attack. <em>AAMAS</em>, 1872–1874. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reputation systems and distributed networks are increasingly common. Examples include electronic marketplaces, online social networks, the Internet of Things and ad-hoc networks (such as VANETs and MANETs). Such systems have great inherent complexity. As a result, they challenge typical methods for defining and verifying desired behaviour. For example, actions such as leaving a smart home appliance open to network connections, connecting to a hotel WiFi network or interacting with a potentially pseudonymous identity (on an online marketplace or social network, for example) all entail an inherent component of uncertainty and, therefore, risk. As such, trust and reputation are increasingly crucial concepts for understanding modern socio-technical systems. Broadly, we consider trust to occur whenever an agent interacts with another party without a guarantee of the other party&#39;s &quot;good behaviour&quot;. A guarantee in this case depends on context. Information system guarantees are normally cryptographic whilst social guarantees can come in the form of face-to-face meetings; contracts; signatures; or &quot;difficult to obtain/counterfeit information&quot; such as bank cards, passports or driving licenses. However, the users (or trustors) in systems with weakened or nonexistent guarantees may still attempt to protect themselves when considering an interaction. Commonly, this comes in the form of attempting to predict the future behaviour of potential interactive parties (or trustees). For example, they may try to infer a trustee&#39;s future behaviour from their past behaviour. In doing so, they form an opinion of the trustee&#39;s trustworthiness. This distribution of opinions about a trustee constitutes their reputation.},
  archive   = {C_AAMAS},
  author    = {Sirur, Sean},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1872–1874},
  title     = {The reputation lag attack},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536135},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Embodied team intelligence in multi-robot systems.
<em>AAMAS</em>, 1869–1871. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {High-performing human teams leverage intelligent and efficient communication and coordination strategies to collaboratively maximize their joint utility. Inspired by teaming behaviors among humans, I seek to develop computational methods for synthesizing intelligent communication and coordination strategies for collaborative multi-robot systems. I leverage both classical model-based control and planning approaches as well as data-driven methods such as Multi-Agent Reinforcement Learning (MARL) to provide several contributions towards enabling emergent cooperative teaming behavior across both homogeneous and heterogeneous (including agents with different capabilities) robot teams. In future work, I aim to investigate efficient ways to incorporate humans&#39; teaming strategies for robot teams and directly learn team coordination policies from human experts.},
  archive   = {C_AAMAS},
  author    = {Seraj, Esmaeil},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1869–1871},
  title     = {Embodied team intelligence in multi-robot systems},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536134},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Empathetic reinforcement learning agents. <em>AAMAS</em>,
1866–1868. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the increased interaction between artificial agents and humans, the need to have agents who can respond to their human counterparts appropriately will be crucial for the deployment of trustworthy systems. A key behaviour to permit this, one which humans and other living beings exhibit naturally, is empathy. In my research I explore the potential for agents to behave in ways that may be considered empathetic. Empathy is a two stage process involving the identification of the feelings or goals of the other, and having that same feeling be evoked in oneself. I began my work towards this objective by initially designing an agent who exhibits sympathy - the ability to identify the goals of another. Empathy is slightly more complex as it involves a process of projecting the state of the other back onto oneself and observing one&#39;s own response. In my research I hope to draw inspiration from this and evoke empathy through a process of mapping the other&#39;s goals back to oneself. By drawing upon empathetic responses, the hope is that this will lead to a faster and deeper understanding of the other.},
  archive   = {C_AAMAS},
  author    = {Senadeera, Manisha},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1866–1868},
  title     = {Empathetic reinforcement learning agents},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536133},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Task generalisation in multi-agent reinforcement learning.
<em>AAMAS</em>, 1863–1865. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent reinforcement learning agents are typically trained in a single environment. As a consequence, they overfit to the training environment which results in sensitivity to perturbations and inability to generalise to similar environments. For multi-agent reinforcement learning approaches to be applicable in real-world scenarios, generalisation and robustness need to be addressed. However, unlike in supervised learning, generalisation lacks a clear definition in multi-agent reinforcement learning. We discuss the problem of task generalisation and demonstrate the difficulty of zero-shot generalisation and finetuning at the example of multi-robot warehouse coordination with preliminary results. Lastly, we discuss promising directions of research working towards generalisation of multi-agent reinforcement learning.},
  archive   = {C_AAMAS},
  author    = {Sch\&quot;{a}fer, Lukas},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1863–1865},
  title     = {Task generalisation in multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536132},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Designing mechanisms for participatory budgeting.
<em>AAMAS</em>, 1860–1862. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When seeking for suitable mechanisms for participatory budgeting (PB), one has to decide on which criteria to assess them. In this paper, I present several appealing criteria for PB mechanisms. I briefly introduce each of them and discuss their impact on the design of PB mechanisms.},
  archive   = {C_AAMAS},
  author    = {Rey, Simon},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1860–1862},
  title     = {Designing mechanisms for participatory budgeting},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536131},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fair allocation problems in reviewer assignment.
<em>AAMAS</em>, 1857–1859. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Peer review is an incredibly important and revered process for establishing scientific rigor and facilitating effective discourse. However, many conventions of this process are ad-hoc and could benefit heavily from a more intentional design. I present our existing work applying fair allocation techniques to a major problem in peer review, the assignment of reviewers. We propose a simple but flexible algorithm for fair reviewer assignment, based on the classic Round Robin picking sequence. In addition, I discuss two other areas in peer review that are ripe for fair allocation and mechanism design, reviewer bidding and two-sided fair reviewer assignment.},
  archive   = {C_AAMAS},
  author    = {Payan, Justin},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1857–1859},
  title     = {Fair allocation problems in reviewer assignment},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536130},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Budget feasible mechanisms in auction markets: Truthfulness,
diffusion and fairness. <em>AAMAS</em>, 1854–1856. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Budget feasible mechanism design, as a sub-field of auction domain, has been well studied in many previous works [5]. In budget-feasible mechanism design problem, there is a buyer with a budget and multiple strategic sellers owning items with private costs. The goal of the buyer is to procure as many items as possible from sellers. Thus, the designed mechanisms should ensure desirable properties, e.g., truthfulness of sellers, individual rationality, budget feasibility that the total payment cannot exceed buyer&#39;s budget. Although many works focused on budget-feasible mechanism design, there are still three further directions: 1) Budget feasible mechanisms in social networks where all participants are connected by their neighbors via social network and the buyer wants her neighbors to further diffuse auction information to other potential sellers to improve her utility; 2) Budget feasible mechanisms with fair representation where the agents may belong to different groups and the buyer wants to fairly select agents from different groups; 3) Budget feasible mechanisms in two-sided markets where there are multiple strategic buyers with diverse budgets and multiple strategic sellers with private costs.},
  archive   = {C_AAMAS},
  author    = {Liu, Xiang},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1854–1856},
  title     = {Budget feasible mechanisms in auction markets: Truthfulness, diffusion and fairness},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536129},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data-driven approaches for formal synthesis of dynamical
systems. <em>AAMAS</em>, 1852–1853. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {My research lies at the intersection of control theory, machine learning and formal methods. This paper presents part of the work developed so far within the scope of my PhD and suggests possible future research directions. Towards trustworthy computing, my research has focused on simplifying the designing pipeline of safe and reliable AI systems. I have worked on data-driven controller synthesis, i.e., the automated generation of control systems from a given high-level specification with theoretical guarantees of correctness. In this way, I analyze the satisfaction of properties in both episodic and continual settings. Moreover, in my research I provide correctness for satisfying specifications using different approaches including abstraction-based techniques, game-theoretic techniques, and model-free reinforcement learning.},
  archive   = {C_AAMAS},
  author    = {Kazemi, Milad},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1852–1853},
  title     = {Data-driven approaches for formal synthesis of dynamical systems},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536128},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Model-free and model-based reinforcement learning, the
intersection of learning and planning. <em>AAMAS</em>, 1849–1851. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {My doctoral dissertation is intended as the compound of four publications considering: structure and randomness in planning and reinforcement learning, continuous control with ensemble deep deterministic policy gradients, toddler-inspired active representation learning, and large-scale deep reinforcement learning costs.},
  archive   = {C_AAMAS},
  author    = {Januszewski, Piotr},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1849–1851},
  title     = {Model-free and model-based reinforcement learning, the intersection of learning and planning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536127},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Incentive design for equitable resource allocation:
Artificial currencies and allocation constraints. <em>AAMAS</em>,
1846–1848. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The proliferation of algorithmic decision-making systems in resource allocation applications has enabled the efficient allocation of scarce resources. However, in the pursuit of an efficient outcome, such systems often discriminate against some users who may be made disproportionately worse off. To address this concern, in this work, we aim to develop equitable allocation mechanisms that respect the often complex preferences of users and associated allocation constraints while catering to the needs of all groups of society. In particular, we will develop conceptual frameworks to embed fairness and equity constraints in designing resource allocation mechanisms for emerging transportation and labor market applications. Furthermore, we will devise artificial currency market mechanisms that ensure users have an equal opportunity to avail resources. The results of this research will help (i) catalyze support from multidisciplinary stakeholders at the intersection of economics and optimization, (ii) inform policymakers about how to improve regulation in transportation, labor, and artificial currency markets, and (iii) lay the groundwork to pilot the mechanisms to tackle important resource inequity challenges in real systems.},
  archive   = {C_AAMAS},
  author    = {Jalota, Devansh},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1846–1848},
  title     = {Incentive design for equitable resource allocation: Artificial currencies and allocation constraints},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536126},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Non-cooperative multi-robot planning under shared resources.
<em>AAMAS</em>, 1843–1845. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As more and more single-use robots are introduced to private and public spaces, it will become essential to mediate the interaction between robots. In particular, we consider the problem of resource sharing in non-cooperative multi-robot systems. We discuss the motivation for different types of shared resources and share how we used auctions to address the non-cooperative multi-agent pathfinding problem. We summarize those result, which are presented in full in a separate paper. Finally, we discuss some avenues for future work, including the application of auctions to allocate multi-unit chance-constrained resources under the presence of uncertainty.},
  archive   = {C_AAMAS},
  author    = {Gautier, Anna},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1843–1845},
  title     = {Non-cooperative multi-robot planning under shared resources},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536125},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online learning against strategic adversary. <em>AAMAS</em>,
1841–1842. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Our work considers repeated games in which one player has a different objective than others. In particular, we investigate repeated two-player zero-sum games where the column player not only aims to minimize her regret but also stabilize the actions. Suppose that while repeatedly playing this game, the row player chooses her strategy at each round by using a no-regret algorithm to minimize her regret. We develop a no-dynamic regret algorithm for the column player to exhibit last round convergence to a minimax equilibrium. We show that our algorithm is efficient against a large set of popular no-regret algorithms the row player can use, including the multiplicative weights update algorithm, general follow-the-regularized-leader and any no-regret algorithms satisfy a property so called &quot;stability&#39;&#39;. Our algorithm can be applied to the game setting where the column player is also a designer of the system, and has full control over payoff matrices.},
  archive   = {C_AAMAS},
  author    = {Dinh, Le Cong},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1841–1842},
  title     = {Online learning against strategic adversary},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536124},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards multi-agent interactive reinforcement learning for
opportunistic software composition in ambient environments.
<em>AAMAS</em>, 1839–1840. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In order to manage the ever-growing number of devices present in modern and future ambient environments, as well as their dynamics and openness, we aim to propose a distributed multi-agent system that learns, in interaction with a human user, what would be their preferred applications given the services available.The goal of this Ph.D. thesis is to focus on the interaction between a reinforcement learning system and the human user, to improve the system&#39;s learning capabilities as well as the user&#39;s ease with the system, and ultimately build a working prototype, usable by end-users.},
  archive   = {C_AAMAS},
  author    = {Delcourt, Kevin},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1839–1840},
  title     = {Towards multi-agent interactive reinforcement learning for opportunistic software composition in ambient environments},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536123},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Collaborative training of multiple autonomous agents.
<em>AAMAS</em>, 1836–1838. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Exploration in multi-agent reinforcement learning is a challenging problem, especially with a large number of agents. Parameter sharing between agents is often used since it significantly decreases the number of trainable parameters, shortening training times to tractable levels and improving exploration efficiency. We present two algorithms that aim to be a middle ground between not sharing parameters and fully sharing parameters. These proposed algorithms show advantages of the baselines at the two ends of the spectrum and minimise their drawbacks. First, Shared Experience Actor-Critic [Christianos et al. 2020], applies the basic idea of off-policy correction via importance weighting and combines the experiences generated by different agents into more informative and effective learning gradients. Then, Selective Parameter Sharing [Christianos et al. 2021], based on rigorous empirical analysis of the impact of parameter sharing proposes a novel parameter sharing method that can be coupled with existing multi-agent reinforcement learning algorithms.},
  archive   = {C_AAMAS},
  author    = {Christianos, Filippos},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1836–1838},
  title     = {Collaborative training of multiple autonomous agents},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536122},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Manipulation of machine learning algoirhtms. <em>AAMAS</em>,
1833–1835. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As data becomes increasingly available, individuals, organisations and companies are increasingly applying machine learning algorithms to make decisions. In many cases, those decisions have a direct effect on those who provided the data to the decision maker. In other words, data providers often have a vested interest in the decisions made based on the data provided. Therefore, decision makers should anticipate that data providers may alter or change the data they provide in order to achieve a preferential outcome. Such strategic behaviour is not adequately modelled by classical machine learning settings in the literature. As a result, new machine learning algorithms are required, which take into the account the incentives and capabilities of data providers when making decisions. This paper summarises a PhD project which attempts to address this problem in a number of contexts.},
  archive   = {C_AAMAS},
  author    = {Bishop, Nicholas},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1833–1835},
  title     = {Manipulation of machine learning algoirhtms},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536121},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploration and communication for partially observable
collaborative multi-agent reinforcement learning. <em>AAMAS</em>,
1829–1832. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent reinforcement learning (MARL) enables us to create adaptive agents in challenging environments, even when the agents have limited observation. The cooperative multi-agent setting introduces numerous challenges compared to the single-agent setting, such as the moving target problem and the curse of dimensionality with respect to the action space. It also aggravates the credit assignment problem, as the credit is not only spread across a sequence of actions, but also multiple agents. This setting also introduces new possibilities, such as task parallelization, specialization, and communication. The Centralized Training with Decentralized Execution paradigm has emerged as a popular strategy to mitigate some of the difficulties in MARL, while still ensuring that the policy of the agents is only conditioned on their local history. However, how to fully leverage this paradigm is still an open question. During the first year of my Ph.D., I developed a novel algorithm, Local Advantage Networks (LAN), that proposes an alternative direction to value factorization, that is more scalable, not limited in its representation and state-of-the-art. The next parts of my research will focus on multi-agent exploration and learning to communicate.},
  archive   = {C_AAMAS},
  author    = {Avalos, Rapha\&quot;{e}l},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1829–1832},
  title     = {Exploration and communication for partially observable collaborative multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536120},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Macro ethics for governing equitable sociotechnical systems.
<em>AAMAS</em>, 1824–1828. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The evolving relationship between humans and technology entails increasing concerns about the impact on ethical issues such as bias, unfairness, and lack of accountability. There is thus a need for consistent responses to multiple-user social dilemmas that arise during interactions in sociotechnical systems, where combinations of humans and technical agents work together as ethical duos. The outcomes of these systems can be evaluated by the values that participating humans hold, which in turn influence the development of norms used to guide acceptable behaviour. However, when values are misjudged or norms conflict, dilemmas arise that must be resolved in satisfactory ways. To examine these dilemmas, we adopt a macro ethics perspective where ethics is addressed via the governance of sociotechnical systems with multiple agents (rather than through the actions of single agents). We propose that to produce satisfactory outcomes, systematic methodologies be developed to consistently integrate normative ethical principles in reasoning capacities. The application of these ethical principles would enable practitioners to think analytically and systematically about the multiple-user social dilemmas that occur in these systems, in order to resolve them in satisfactory ways. To achieve this, we need to (1) categorize ethical principles not yet used in AI, form new ways of (2) systematically integrating ethical principles into reasoning, and use these new ways to (3) develop consistent responses to multiple-user social dilemmas.},
  archive   = {C_AAMAS},
  author    = {Woodgate, Jessica and Ajmeri, Nirav},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1824–1828},
  title     = {Macro ethics for governing equitable sociotechnical systems},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536118},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Agent-assisted life-long education and learning.
<em>AAMAS</em>, 1819–1823. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel approach to education and learning, in which digital agents take care of the life-long education and career development of their human partners. Agent consults requirements and preferences and proactively searches for opportunities by monitoring job markets, reflecting on educational options and challenges. Then, it adjusts the current academic program and coordinates the pathway facilitating the optimal desired outcome. We envision a Distributed Educational Agent Network (DEAN) of connected, trustworthy educational agents that securely communicate with each other to collectively carry out the above-mentioned tasks. We discuss the benefits and current and future challenges associated with the implementation of the proposed agent-assisted educational approach. Research and development of DEAN provide a unique opportunity for the agent community to design, implement and evaluate agent technologies in complex, large scale scenarios. In this paper, we also specify a proposal for a design of a DEAN agent, using skill development to monitor and drive education and career.},
  archive   = {C_AAMAS},
  author    = {Trescak, Tomas and Lera-Leri, Roger and Bistaffa, Filippo and Rodriguez-Aguilar, Juan A.},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1819–1823},
  title     = {Agent-assisted life-long education and learning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536117},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Foundations for grassroots democratic metaverse.
<em>AAMAS</em>, 1814–1818. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While the physical lives of many of us are in democracies (one person, one vote - e.g., the EU and the US), our digital lives are mostly in autocracies (one person, all votes - e.g., Facebook). Cryptocurrencies promise liberation but stop short, at plutocracy (one coin, one vote). What would it take for us to live in a digital democracy? This paper offers a vision, a theoretical framework, and an architecture for a grassroots network of autonomous, people-owned, people-operated, and people-governed digital communities, namely a grassroots democratic metaverse. It also charts a roadmap towards realizing it, and identifies unexplored territory for MAS research.},
  archive   = {C_AAMAS},
  author    = {Shapiro, Ehud and Talmon, Nimrod},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1814–1818},
  title     = {Foundations for grassroots democratic metaverse},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536116},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). “Go to the children”: Rethinking intelligent agent design
and programming in a developmental learning perspective. <em>AAMAS</em>,
1809–1813. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we propose to rethink the development of intelligent agents based on cognitive architectures as a developmental learning process, inspired by theories of learning in children and cognitive development. The idea is targeted to explore architectures, methods and tools to systematically develop intelligent agents capable of integrating both practical knowledge and skills designed by developers and knowledge and skills acquired by interacting in properly designed learning environments.},
  archive   = {C_AAMAS},
  author    = {Ricci, Alessandro},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1809–1813},
  title     = {&quot;Go to the children&quot;: Rethinking intelligent agent design and programming in a developmental learning perspective},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536115},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The holy grail of multi-robot planning: Learning to generate
online-scalable solutions from offline-optimal experts. <em>AAMAS</em>,
1804–1808. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many multi-robot planning problems are burdened by the curse of dimensionality, which compounds the difficulty of applying solutions to large-scale problem instances. The use of learning-based methods in multi-robot planning holds great promise as it enables us to offload the online computational burden of expensive centralized, yet optimal solvers, to an offline learning procedure. The hope is that by training a policy to copy an optimal pattern generated by a small-scale (centralized) system, we can transfer that policy to much larger, decentralized systems while maintaining near-optimal performance. Yet, a number of issues impede us from leveraging this idea to its full potential. This blue-sky paper elaborates some of the key challenges that remain.},
  archive   = {C_AAMAS},
  author    = {Prorok, Amanda and Blumenkamp, Jan and Li, Qingbiao and Kortvelesy, Ryan and Liu, Zhe and Stump, Ethan},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1804–1808},
  title     = {The holy grail of multi-robot planning: Learning to generate online-scalable solutions from offline-optimal experts},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536114},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards anomaly detection in reinforcement learning.
<em>AAMAS</em>, 1799–1803. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Identifying datapoints that substantially differ from normality is the task of anomaly detection (AD). While AD has gained widespread attention in rich data domains such as images, videos, audio and text, it has has been studied less frequently in the context of reinforcement learning (RL). This is due to the additional layer of complexity that RL introduces through sequential decision making. Developing suitable anomaly detectors for RL is of particular importance in safety-critical scenarios where acting on anomalous data could result in hazardous situations. In this work, we address the question of what AD means in the context of RL. We found that current research trains and evaluates on overly simplistic and unrealistic scenarios which reduce to classic pattern recognition tasks. We link AD in RL to various fields in RL such as lifelong RL and generalization. We discuss their similarities, differences, and how the fields can benefit from each other. Moreover, we identify non-stationarity to be one of the key drivers for future research on AD in RL and make a first step towards a more formal treatment of the problem by framing it in terms of the recently introduced block contextual Markov decision process. Finally, we define a list of practical desiderata for future problems.},
  archive   = {C_AAMAS},
  author    = {M\&quot;{u}ller, Robert and Illium, Steffen and Phan, Thomy and Haider, Tom and Linnhoff-Popien, Claudia},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1799–1803},
  title     = {Towards anomaly detection in reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536113},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Augmented democratic deliberation: Can conversational agents
boost deliberation in social media? <em>AAMAS</em>, 1794–1798. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Online social media are currently perceived as the new means of providing a platform for participation among citizens. This comes at a time when people are led to alternative spaces of political expression as traditional channels become strained. New technologies appear to have a transformational potential that could lead to social change and achieve deeper and wider mobilisation in political processes. However, people still face challenges when seeking information, disseminating information, or engaging in online deliberation. Allowing every citizen to participate in discussions and thus influence the final decision requires countless interactions that take considerable amounts of time and energy. This process is cognitively demanding due to linguistic barriers or when the problems on the table are multidisciplinary. We envision in this blue sky paper the development of autonomous and intelligent conversational agents that can augment the deliberative capacities of citizens in social media. To implement our vision, we start by proposing an approach that quantifies deliberation in online argumentative discussions. Then, we propose a methodology to optimise deliberation across discussion threads. The proposed concept is expected to pave the way to a form of augmented democratic deliberation built on the cooperation of humans and conversational agents.},
  archive   = {C_AAMAS},
  author    = {Hadfi, Rafik and Ito, Takayuki},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1794–1798},
  title     = {Augmented democratic deliberation: Can conversational agents boost deliberation in social media?},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536112},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Social choice around the block: On the computational social
choice of blockchain. <em>AAMAS</em>, 1788–1793. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One of the most innovative aspects of blockchain technology consists in the introduction of an incentive layer to regulate the behavior of distributed protocols. The designer of a blockchain system faces therefore issues that are akin to those relevant for the design of economic mechanisms, and faces them in a computational setting. From this perspective the present paper argues for the importance of computational social choice in blockchain research. It identifies a few challenges at the interface of the two fields that illustrate the strong potential for cross-fertilization between them.},
  archive   = {C_AAMAS},
  author    = {Grossi, Davide},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1788–1793},
  title     = {Social choice around the block: On the computational social choice of blockchain},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536111},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robots teaching humans: A new communication paradigm via
reverse teleoperation. <em>AAMAS</em>, 1783–1787. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simulators offer a scalable platform to train robots, offering a path to creative and innovative solutions that are difficult for humans to envision a priori. We introduce a way to leverage this property, along with a new paradigm where robots discover creative solutions in simulation, then teach humans or other agents to physically execute the learned solutions via reverse teleoperation. We provide various examples ranging from learning new skills, to rehabilitation, to everyday activities, where such a system would be valuable.},
  archive   = {C_AAMAS},
  author    = {Antonova, Rika and Handa, Ankur},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1783–1787},
  title     = {Robots teaching humans: A new communication paradigm via reverse teleoperation},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536110},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Irrational behaviour and globalisation. <em>AAMAS</em>,
1780–1782. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper seeks to predict globalisation using agent models and a combination of evolutionary game theory and irrational bias. In this paper, we use a new evolutionary process in the model, where we set several complex parameters and trade-offs to set the payoff matrix and its change in real-time. We define a new dynamic evolutionary process to ensure that each agent can choose its interests in the simulation of globalisation and also the model include irrational agents in the model to test the usefulness of the new dynamics against a control group without irrational agents.},
  archive   = {C_AAMAS},
  author    = {Zhu, Yuanzi and Ventre, Carmine},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1780–1782},
  title     = {Irrational behaviour and globalisation},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536108},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An agent-based model for emergency evacuation from a
multi-floor building. <em>AAMAS</em>, 1777–1779. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This extended abstract presents an agent-based model to enable simulations to be performed of evacuations from a multi-floor building, thus including the movement in staircases. In the model, agents select exits and staircases based on familiarity level with the building layout, spatial distance to exits, preference to evacuate via the main entrance/exit, and visibility of exits and staircases. The agent-based model has been validated using the empirical findings of an evacuation drill in the staircase of a four-floor building. The novel contribution of this research is the development of a more realistic agent-based model aimed at providing decision support for the layout design of buildings and evacuation behavioral management in emergency evacuation. That is, the agent-based model mimics people&#39;s natural movement in staircases, and involves agents&#39; exit and staircase selection, and movement on floors. Simulations have been performed to investigate the effect on evacuation time of (i) number of agents and their familiarity with the building layout, and (ii) exit selection of agents.},
  archive   = {C_AAMAS},
  author    = {Zhang, Xiaoyan and Coates, Graham and Dunn, Sarah and Hall, Jean},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1777–1779},
  title     = {An agent-based model for emergency evacuation from a multi-floor building},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536107},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Off-policy correction for multi-agent reinforcement
learning. <em>AAMAS</em>, 1774–1776. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent reinforcement learning (MARL) provides a framework for problems involving multiple interacting agents. Despite similarity to the single-agent case, multi-agent problems are often harder to train and analyze theoretically. In this work, we propose MA-Trace, a new on-policy actor-critic algorithm, which extends V-Trace to the MARL setting. The key advantage of our algorithm is its high scalability in a multi-worker setting. To this end, MA-Trace utilizes importance sampling as an off-policy correction method, which allows distributing the computations with negligible impact on the quality of training. Furthermore, our algorithm is theoretically grounded -- we provide a fixed-point theorem that guarantees convergence. We evaluate the algorithm extensively on the StarCraft Multi-Agent Challenge, a standard benchmark for multi-agent algorithms. MA-Trace achieves high performance on all its tasks and exceeds state-of-the-art results on some of them.},
  archive   = {C_AAMAS},
  author    = {Zawalski, Micha\l{} and Osi\&#39;{n}ski, B\l{}azej and Michalewski, Henryk and Mi\l{}o\&#39;{n}, Piotr},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1774–1776},
  title     = {Off-policy correction for multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536106},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On-the-fly strategy adaptation for ad-hoc agent
coordination. <em>AAMAS</em>, 1771–1773. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Training agents in cooperative settings offers the promise of AI agents able to interact effectively with humans (and other agents) in the real world. Multi-agent reinforcement learning (MARL) has the potential to achieve this goal, demonstrating success in a series of challenging problems. However, whilst these advances are significant, the vast majority of focus has been on the self-play paradigm. This often results in a coordination problem, caused by agents learning to make use of arbitrary conventions when playing with themselves. This means that even the strongest self-play agents may have very low cross-play with other agents, including other initializations of the same algorithm. In this paper we propose to solve this problem by adapting agent strategies on the fly, using a posterior belief over the other agents&#39; strategy. Concretely, we consider the problem of selecting a strategy from a finite set of previously trained agents, to play with an unknown partner. We propose an extension of the classic statistical technique, Gibbs sampling, to update beliefs about other agents and obtain close to optimal ad-hoc performance. Despite its simplicity, our method is able to achieve strong cross-play with unseen partners in the challenging card game of Hanabi, achieving successful ad-hoc coordination without knowledge of the partner&#39;s strategy a priori.},
  archive   = {C_AAMAS},
  author    = {Zand, Jaleh and Parker-Holder, Jack and Roberts, Stephen J.},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1771–1773},
  title     = {On-the-fly strategy adaptation for ad-hoc agent coordination},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536105},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the complexity of controlling amendment and successive
winners. <em>AAMAS</em>, 1768–1770. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Successive and amendment are two important sequential voting procedures widely used in parliamentary and legislative decision making. They have been extensively studied in the literature from different perspectives. However, investigating them through the lens of computational complexity theory has not been well conducted heretofore. This paper studies the parameterized complexity of constructive/destructive control by adding/deleting voters/candidates for these two procedures and provides a comprehensive parameterized complexity landscape of these problems.},
  archive   = {C_AAMAS},
  author    = {Yang, Yongjie},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1768–1770},
  title     = {On the complexity of controlling amendment and successive winners},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536104},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Performance of deep reinforcement learning for high
frequency market making on actual tick data. <em>AAMAS</em>, 1765–1767.
(<a href="https://dl.acm.org/doi/10.5555/3535850.3536103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {High frequency market making is a trading strategy in which agent quotes passively at both ask and bid sides. In this paper, by applying Dueling Double Deep Q Network (D3QN) and a novel reward function, we develop market making agents who can balance profit and inventory robustly, flexibly and full-automatically. Thanks to the actual tick data of stock, we are able to train and test D3QN agents in a relatively realistic environment. To further explore the agent&#39;s performance, we also consider a double-agent situation, where the agent competes with a special-designed market maker. And we find in this case, with further training, the D3QN agent learns to quote more narrowly to increase transaction probability. Furthermore, we analyze the impact of high frequency market making on market quality in both single-agent and double-agent cases.},
  archive   = {C_AAMAS},
  author    = {Xu, Ziyi and Cheng, Xue and He, Yangbo},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1765–1767},
  title     = {Performance of deep reinforcement learning for high frequency market making on actual tick data},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536103},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). How to train PointGoal navigation agents on a (sample and
compute) budget. <em>AAMAS</em>, 1762–1764. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {PointGoal navigation has seen significant recent interest and progress, spurred on by the Habitat platform and associated challenge. In this paper, we study PointGoal navigation under both a sample budget (75 million frames) and a compute budget (1 GPU for 1 day). We conduct an extensive set of experiments, cumulatively totaling over 50,000 GPU-hours, that let us identify and discuss a number of ostensibly minor but significant design choices -- the advantage estimation procedure (a key component in training), and visual encoder architecture. Overall, these design choices lead to considerable and consistent improvements. Under a sample budget, performance for RGB-D agents improves 3 SPL on Gibson (4\% relative improvement) and 20 SPL on Matterport3D (43\% relative improvement). Under a compute budget, performance for RGB-D agents improves by 3 SPL on Gibson (5\% relative improvement) and 15 SPL on Matterport3D (50\% relative improvement). Our findings and recommendations will serve to make the community&#39;s experiments more efficient -- to reach 50 SPL with RGB-D on Matterport3D, they reduce the samples needed by 3x and the training time 2x.},
  archive   = {C_AAMAS},
  author    = {Wijmans, Erik and Essa, Irfan and Batra, Dhruv},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1762–1764},
  title     = {How to train PointGoal navigation agents on a (Sample and compute) budget},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536102},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On agent incentives to manipulate human feedback in
multi-agent reward learning scenarios. <em>AAMAS</em>, 1759–1761. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In settings without well-defined goals, methods for reward learning allow reinforcement learning agents to infer goals from human feedback. Existing work has discussed the problem that such agents may manipulate humans, or the reward learning process, in order to gain higher reward. We introduce the neglected problem that, in multi-agent settings, agents may have incentives to manipulate one another&#39;s reward functions in order to change each other&#39;s behavioral policies. We focus on the setting with humans acting alongside assistive (artificial) agents who must learn the reward function by interacting with these humans. We propose a possible solution to manipulation of human feedback in this setting: the Shared Value Prior (SVP). The SVP equips agents with an assumption that the reward functions of all humans are similar. Given this assumption, the actions of any human provide information to an agent about its reward, and so the agent is incentivised to observe these actions rather than to manipulate them. We present an expository example in which the SVP prevents manipulation.},
  archive   = {C_AAMAS},
  author    = {Ward, Francis Rhys and Toni, Francesca and Belardinelli, Francesco},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1759–1761},
  title     = {On agent incentives to manipulate human feedback in multi-agent reward learning scenarios},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536101},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Near on-policy experience sampling in multi-objective
reinforcement learning. <em>AAMAS</em>, 1756–1758. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In multi-objective decision problems, the same state-action pair under different preference weights between the objectives, constitutes different optimal policies. The introduction of changing preference weights interferes with the convergence of the network, and can even stop the network from converging. In this paper, we propose a novel experience sampling strategy for multi-objective RL problems, which samples transitions based on the weight and state similarities, to get the sampled experiences close to on-policy. We apply our sampling strategy in multi-objective deep RL algorithms on known benchmark problems, and show that this strongly improves performance.},
  archive   = {C_AAMAS},
  author    = {Wang, Shang and Reymond, Mathieu and Irissappane, Athirai A. and Roijers, Diederik M.},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1756–1758},
  title     = {Near on-policy experience sampling in multi-objective reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536100},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The ethical acceptability of artificial social agents.
<em>AAMAS</em>, 1753–1755. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Artificial social agents (ASAs) are computer-based autonomous entities who interact with humans in a range of social roles, including advising, coaching, and consumer support in education and health. While there are many discussions around the ethical use of Artificial Intelligence in general, such as the AI4People Ethical Framework, the ethical ramifications that ASAs will have on human relationships as we share not only personal data but our inner most thoughts and feelings, is less explored. We conducted a study with 199 student participants exposed to Sam, an ASA which acts as a personal guide to support the student during their studies. During the interaction, Sam elicits private information, takes decisions for the user and speaks of its own study experiences. Our results indicate that (loss of) autonomy raised the strongest ethical concerns. These results confirm the importance of informed consent, transparency and accountability of ASAs and question the ethics of false memories and emotion sharing.},
  archive   = {C_AAMAS},
  author    = {Vythilingam, Ravi and Richards, Deborah and Formosa, Paul},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1753–1755},
  title     = {The ethical acceptability of artificial social agents},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536099},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Moving target defense under uncertainty for web
applications. <em>AAMAS</em>, 1750–1752. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Moving target defense (MTD) has emerged as a key technique that can be used in various security applications to reduce the threat of attackers by taking away their ability to perform reconnaissance and exploit vulnerabilities. However, most of the existing research in the field assumes unrealistic access to information about the attacker&#39;s motivations and/or actions when developing MTD strategies. Many of the existing approaches also assume complete knowledge regarding the vulnerabilities of a particular application and how each of these vulnerabilities can be exploited by an attacker. In this work, we propose an algorithm that generates effective MTD strategies for web applications that does not rely on prior knowledge about the attackers. Our approach assumes that the only information the defender receives about its own reward function, is via interaction with the attacker in a repeated game setting. We evaluate our algorithm using data which is mined from the National Vulnerability Database to show that it matches the performance of the state of the art techniques, despite using much less information.},
  archive   = {C_AAMAS},
  author    = {Viswanathan, Vignesh and Bose, Megha and Paruchuri, Praveen},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1750–1752},
  title     = {Moving target defense under uncertainty for web applications},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536098},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Non-parametric neuro-adaptive coordination of multi-agent
systems. <em>AAMAS</em>, 1747–1749. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We develop a learning-based algorithm for the distributed formation control of networked multi-agent systems governed by unknown, nonlinear dynamics. The proposed algorithm integrates neural network-based learning with adaptive control in a two-step procedure. In the first step, each agent learns a controller, represented as a neural network, using training data that correspond to a collection of formation tasks and agent parameters. These parameters and tasks are derived by varying the nominal agent parameters and the formation specifications of the task in hand, respectively. In the second step of the algorithm, each agent incorporates the trained neural network into an online and adaptive control policy in such a way that the behavior of the multi-agent closed-loop system satisfies a user-defined formation task. Both the learning phase and the adaptive control policy are distributed, in the sense that each agent computes its own actions using only local information from its neighboring agents.},
  archive   = {C_AAMAS},
  author    = {Verginis, Christos K. and Xu, Zhe and Topcu, Ufuk},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1747–1749},
  title     = {Non-parametric neuro-adaptive coordination of multi-agent systems},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536097},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Autonomous flight arcade challenge: Single- and multi-agent
learning environments for aerial vehicles. <em>AAMAS</em>, 1744–1746.
(<a href="https://dl.acm.org/doi/10.5555/3535850.3536096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Autonomous Flight Arcade (AFA) is a novel suite of single- and multi-agent learning environments for control of aerial vehicles. These environments incorporate realistic physics using the Unity game engine with diverse objectives and levels of decision-making sophistication. In addition to the environments themselves, we introduce an interface for interacting with them, including the ability to vary key parameters, thereby both changing the difficulty and the core challenges. We also introduce a pipeline for collecting human gameplay within the environments. We demonstrate the performance of artificial agents in these environments trained using deep reinforcement learning, and also motivate these environments as a benchmark for designing non-learned classical control policies and agents trained using imitation learning from human demonstrations. Finally, we motivate the use of AFA environments as a testbed for training artificial agents capable of cooperative human-AI decision making, including parallel autonomy.},
  archive   = {C_AAMAS},
  author    = {Tylkin, Paul and Wang, Tsun-Hsuan and Seyde, Tim and Palko, Kyle and Allen, Ross and Amini, Alexander and Rus, Daniela},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1744–1746},
  title     = {Autonomous flight arcade challenge: Single- and multi-agent learning environments for aerial vehicles},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536096},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Max-sum with quadtrees for continuous DCOPs with application
to lane-free autonomous driving. <em>AAMAS</em>, 1741–1743. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we put forward a novel extension of the classic Max-Sum algorithm to the framework of Continuous Distributed Constrained Optimization Problems (Continuous DCOPs), in which we model the exchanged messages by means of a popular geometric algorithm, Quadtrees. As such, the discretization process is dynamic and embedded in the internal Max-Sum operations (addition and marginal maximization). We apply our Max-Sum with Quadtrees approach to Lane-Free Autonomous Driving in a highway populated with vehicles. Our experimental evaluation verifies the efficiency of our approach in this challenging dynamic coordination domain, demonstrating its superior performance with respect to the standard Max-Sum algorithm.},
  archive   = {C_AAMAS},
  author    = {Troullinos, Dimitrios and Chalkiadakis, Georgios and Samoladas, Vasilis and Papageorgiou, Markos},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1741–1743},
  title     = {Max-sum with quadtrees for continuous DCOPs with application to lane-free autonomous driving},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536095},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Maximizing resource allocation likelihood with minimum
compromise. <em>AAMAS</em>, 1738–1740. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many scenarios where agents with preferences compete for resources can be cast as maximum matching problems on bipartite graphs. Our focus is on resource allocation problems where agents may have preferences that make them incompatible with some resources. We assume that a Principal chooses a maximum matching randomly so that each agent is matched to a resource with some probability. Agents would like to improve their chances of being matched by modifying their preferences within certain limits. The Principal&#39;s goal is to advise an unsatisfied agent to relax its restrictions so that the total cost of relaxation is within a budget (chosen by the agent) and the increase in the probability of being assigned a resource is maximized. We develop efficient algorithms for some variants of this budget-constrained maximization problem and establish hardness results for other variants. For the latter variants, we also develop algorithms with performance guarantees. We experimentally evaluate our methods on synthetic datasets as well as on two novel real-world datasets: a vacation activities dataset and a classrooms dataset.},
  archive   = {C_AAMAS},
  author    = {Trabelsi, Yohai and Adiga, Abhijin and Kraus, Sarit and Ravi, S. S.},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1738–1740},
  title     = {Maximizing resource allocation likelihood with minimum compromise},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536094},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Speeding up deep reinforcement learning through
influence-augmented local simulators. <em>AAMAS</em>, 1735–1737. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning effective policies for real-world problems is still an open challenge for the field of reinforcement learning (RL). The main limitation being the amount of data needed and the pace at which that data can be obtained. In this paper, we study how to build lightweight simulators of complicated systems that can run sufficiently fast for deep RL to be applicable. We focus on domains where agents interact with a reduced portion of a larger environment while still being affected by the global dynamics. Our method combines the use of local simulators with learned models that mimic the influence of the global system. The experiments reveal that incorporating this idea into the deep RL workflow can considerably accelerate the training process and presents several opportunities for the future.},
  archive   = {C_AAMAS},
  author    = {Suau, Miguel and He, Jinke and Spaan, Matthijs T. J. and Oliehoek, Frans A.},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1735–1737},
  title     = {Speeding up deep reinforcement learning through influence-augmented local simulators},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536093},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Resource-aware adaptation of heterogeneous strategies for
coalition formation. <em>AAMAS</em>, 1732–1734. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing approaches to coalition formation assume that task requirements are precisely specified by the human operator. Further, existing approaches ignore the fact that tasks could often be performed by following one of many equivalent strategies. However, prior work has demonstrated that humans, while extremely adept at solving complex problems, struggle to explicitly state the intuition that led to their solution. In this work, we propose a two-part framework to i) learn implicit heterogeneous strategies for coalition formation directly from expert demonstrations, and ii) adaptively select one of the inferred strategies based on available resources, without additional training.},
  archive   = {C_AAMAS},
  author    = {Srikanthan, Anusha and Ravichandar, Harish},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1732–1734},
  title     = {Resource-aware adaptation of heterogeneous strategies for coalition formation},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536092},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pre-trained language models as prior knowledge for playing
text-based games. <em>AAMAS</em>, 1729–1731. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, text world games have been proposed to enable artificial agents to understand and reason about real-world scenarios. These text-based games are challenging for artificial agents, as it requires an understanding of and interaction using natural language in a partially observable environment. Past approaches have paid less attention to the language understanding capability of the proposed agents. In this paper, we improve the semantic understanding of the agent by proposing a simple RL with LM framework where we use transformer-based language models with Deep RL models. Overall, our proposed approach outperforms on 4 games out of the 14 text-based games, while performing comparable to the state-of-the-art models on the remaining games.},
  archive   = {C_AAMAS},
  author    = {Singh, Ishika and Singh, Gargi and Modi, Ashutosh},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1729–1731},
  title     = {Pre-trained language models as prior knowledge for playing text-based games},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536091},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Environment guided interactive reinforcement learning:
Learning from binary feedback in high-dimensional robot task
environments. <em>AAMAS</em>, 1726–1728. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Continuous Action-space Interactive Reinforcement learning (CAIR) is the first continuous action-space interactive reinforcement learning algorithm that can out-preform state-of-the-art reinforcement learning algorithms early on in training. We test CAIR in two simulated robotics environments with intuitive and easy to design heuristic teachers.},
  archive   = {C_AAMAS},
  author    = {Sheidlower, Isaac and Short, Elaine Schaertl and Moore, Allison},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1726–1728},
  title     = {Environment guided interactive reinforcement learning: Learning from binary feedback in high-dimensional robot task environments},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536090},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Agent-time attention for sparse rewards multi-agent
reinforcement learning. <em>AAMAS</em>, 1723–1725. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sparse and delayed rewards pose a challenge to single agent reinforcement learning. This challenge is amplified in multi-agent reinforcement learning (MARL) where credit assignment of these rewards needs to happen not only across time, but also across agents. We propose Agent-Time Attention, a neural network model with auxiliary losses for redistributing sparse and delayed rewards in collaborative MARL. We provide a simple example to demonstrate how providing agents with their own local redistributed rewards over shared global redistributed rewards leads to better policies.},
  archive   = {C_AAMAS},
  author    = {She, Jennifer and Gupta, Jayesh K. and Kochenderfer, Mykel J.},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1723–1725},
  title     = {Agent-time attention for sparse rewards multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536089},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Behavior vs appearance: What type of adaptations are more
socially motivated? <em>AAMAS</em>, 1720–1722. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For autonomous agents to successfully perform in open hybrid scenarios (with people and agents), they need to adapt to different social contexts. Simultaneously, the people involved must understand the observed changes in the agents, particularly to identify the social motivations for such behavior. In this paper, we report an experimental study conducted to assess if an agent&#39;s behavioral changes (gait), are perceived as more socially motivated than its appearance adaptations (color). Furthermore, we explore if other agents&#39; presence and behavior affect the attribution of motivations. Our findings suggest that external observers identify agents&#39; appearance adaptations as more socially and spatially motivated than behavioral changes. Furthermore, the agents&#39; awareness is rated higher when the agents adjust their physical characteristics compared to behavioral adaptations.},
  archive   = {C_AAMAS},
  author    = {Rato, Diogo and Couto, Marta and Prada, Rui},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1720–1722},
  title     = {Behavior vs appearance: What type of adaptations are more socially motivated?},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536088},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A stit logic of responsibility. <em>AAMAS</em>, 1717–1719.
(<a href="https://dl.acm.org/doi/10.5555/3535850.3536087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a logic of responsibility. Extending stit theory with epistemic, doxastic, deontic, and intentional modalities, we provide logic-based characterizations of several modes of responsibility.},
  archive   = {C_AAMAS},
  author    = {Ram\&#39;{\i}rez Abarca, Aldo Iv\&#39;{a}n and Broersen, Jan},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1717–1719},
  title     = {A stit logic of responsibility},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536087},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Concise representations and complexity of combinatorial
assignment problems. <em>AAMAS</em>, 1714–1716. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the computational problem of partitioning items into bundles among alternatives to maximize social welfare. Unfortunately, many important classes of this problem are computationally hard, including well-known instances in the multi-agent systems literature. In this paper we analyze novel concise representations and restrictions that admit polynomial-time algorithms for many such combinatorial assignment problems, and prove several complexity results for them. We provide efficient approximation algorithms and non-trivial exponential-time algorithms for the hard cases.},
  archive   = {C_AAMAS},
  author    = {Pr\&quot;{a}ntare, Fredrik and Osipov, George and Eriksson, Leif},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1714–1716},
  title     = {Concise representations and complexity of combinatorial assignment problems},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536086},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). I will have order! Optimizing orders for fair reviewer
assignment. <em>AAMAS</em>, 1711–1713. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study mechanisms that allocate reviewers to papers in a fair and efficient manner. We model reviewer assignment as an instance of a fair allocation problem, presenting an extension of the classic round-robin mechanism, called Reviewer Round Robin (RRR). Round-robin mechanisms are a standard tool to ensure envy-free up to one item (EF1) allocations. However, fairness often comes at the cost of decreased efficiency. To overcome this challenge, we carefully select an approximately optimal round-robin order. Applying a relaxation of submodularity, γ-weak submodularity, we show that greedily inserting papers into an order yields a (1+γ2)-approximation to the maximum welfare attainable by our round-robin mechanism under any order. Our approach outputs highly efficient EF1 allocations for three real conference datasets, offering comparable performance to state-of-the-art paper assignment methods in fairness, efficiency, and runtime, while providing the only EF1 guarantee.},
  archive   = {C_AAMAS},
  author    = {Payan, Justin and Zick, Yair},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1711–1713},
  title     = {I will have order! optimizing orders for fair reviewer assignment},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536085},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards an enthymeme-based communication framework.
<em>AAMAS</em>, 1708–1710. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we give an operational semantics for speech acts that BDI agents can use to communicate enthymemes. The approach uses argumentation schemes as common organisational knowledge to guide the construction of enthymemes by the proponents of arguments. Such schemes are also used to guide the reconstruction of the intended argument by the recipients of such enthymemes.},
  archive   = {C_AAMAS},
  author    = {Panisson, Alison R. and McBurney, Peter and Bordini, Rafael H.},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1708–1710},
  title     = {Towards an enthymeme-based communication framework},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536084},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On achieving leximin fairness and stability in many-to-one
matchings. <em>AAMAS</em>, 1705–1707. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The past few years have seen a surge of work on fairness in allocation problems where items must be fairly divided among agents having individual preferences. In comparison, fairness in matching settings with preferences on both sides, that is, where agents have to be matched to other agents, has received much less attention. Moreover, the two-sided matching literature has largely focused on ordinal preferences. We study leximin optimality over stable many-to-one matchings under cardinal preferences. We first investigate matching problems with ranked valuations for which we give efficient algorithms to find the leximin optimal matching over the space of stable matchings. We complement these results by showing that relaxing the ranked valuations condition in any way, makes finding the leximin optimal stable matching intractable.},
  archive   = {C_AAMAS},
  author    = {Narang, Shivika and Biswas, Arpita and Narahari, Yadati},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1705–1707},
  title     = {On achieving leximin fairness and stability in many-to-one matchings},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536083},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards assume-guarantee verification of strategic ability.
<em>AAMAS</em>, 1702–1704. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Formal verification of strategic abilities is a hard problem. We propose to use the methodology of assume-guarantee reasoning in order to facilitate model checking of alternating-time temporal logic with imperfect information and imperfect recall.},
  archive   = {C_AAMAS},
  author    = {Mikulski, \L{}ukasz and Jamroga, Wojciech and Kurpiewski, Damian},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1702–1704},
  title     = {Towards assume-guarantee verification of strategic ability},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536082},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reinforcement learning for traffic signal control
optimization: A concept for real-world implementation. <em>AAMAS</em>,
1699–1701. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The improvement of traffic control is one of the most important but also most ambitious goals in the field of urban traffic today. Due to the domain&#39;s long history, new methods must first establish themselves in practice and, above all, demonstrate reasonable and robust optimization results. Potential methods which came up in recent years are Reinforcement Learning (RL) and Multi-Agent Reinforcement Learning (MARL). In research, the use of RL/MARL for traffic optimization is widely spread. However, it has not yet managed to make it from simulations into practical implementation due to: (1) The lack of real data for the online estimation of the state space. (2) The compatibility to real controllers. (3) The necessity of guarantees to ensure the resilience of the controls. Enabled by a project developing testbeds and practical approaches to optimize traffic through AI, we present a concept to close this gap to the online control of real networks and to overcome the stated issues.},
  archive   = {C_AAMAS},
  author    = {Meess, Henri and Gerner, Jeremias and Hein, Daniel and Schmidtner, Stefanie and Elger, Gordon},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1699–1701},
  title     = {Reinforcement learning for traffic signal control optimization: A concept for real-world implementation},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536081},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Active generation of logical rules for POMCP shielding.
<em>AAMAS</em>, 1696–1698. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the popular Partially Observable Monte-Carlo Planning (POMCP) algorithm and propose a methodology, called Active XPOMCP, for generating compact logical rules that represent properties of the control policy. These rules are then used as shields to prevent POMCP from selecting unexpected actions, with useful implications on the security and trustworthiness of the algorithm. Contrary to state-of-the-art methods, Active XPOMCP does not require a previously generated set of belief-action pairs to generate the logical rule, but it actively generates this data in an information-efficient way by querying the algorithm. Active XPOMCP reduces the number of beliefs needed to generate accurate rules with respect to state-of-the-art methods, and it allows to produce more accurate shields when few belief-action samples are available.},
  archive   = {C_AAMAS},
  author    = {Mazzi, Giulio and Castellini, Alberto and Farinelli, Alessandro},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1696–1698},
  title     = {Active generation of logical rules for POMCP shielding},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536080},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Parameterized algorithms for kidney exchange.
<em>AAMAS</em>, 1693–1695. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In kidney exchange programs, multiple patient-donor pairs each of whom are otherwise incompatible, exchange their donors to receive compatible kidneys. The Kidney Exchange problem is typically modelled as a directed graph where every vertex is either an altruistic donor or a pair of patient and donor; directed edges are added from a donor to its compatible patients. The computational task is to find if there exists a collection of disjoint cycles and paths starting from altruistic donor vertices of length at most $ell_c$ and $ell_p$ respectively that covers at least some specific number t of non-altruistic vertices (patients). We study parameterized algorithms for the kidney exchange problem in this paper. Specifically, we design FPT algorithms parameterized by each of the following parameters: (1) the number of patients who receive kidney, (2) treewidth of the input graph + $maxell_p,ell_c $, and (3) the number of vertex types in the input graph when $ell_p\l{}eq ell_c$. We also present interesting algorithmic and hardness results on the kernelization complexity of the problem. Finally, we present an approximation algorithm for an important special case of Kidney Exchange.},
  archive   = {C_AAMAS},
  author    = {Maiti, Arnab and Dey, Palash},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1693–1695},
  title     = {Parameterized algorithms for kidney exchange},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536079},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A simulation based online planning algorithm for multi-agent
cooperative environments. <em>AAMAS</em>, 1690–1692. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent Markov Decision Process (MMDP) has been an effective way of modelling sequential decision making algorithms for multi-agent cooperative environments. However, challenges such as exponential size of action space and dynamic changes limit the efficacy of proposed solutions. This paper propose a scalable and robust algorithm that can effectively solve MMDPs in real time. Simulation, pruning, and prediction are the three key components of the algorithm. The simulation component enables real time solutions by using a novel iterative pruning technique which in turn makes use of the prediction component trained with self play data. The algorithm is self-sustained as it generates new training data from simulation and gradually becomes better. Furthermore, we show empirical results demonstrating the capabilities of the algorithm and compare them with existing MMDP solvers.},
  archive   = {C_AAMAS},
  author    = {Mahmud, Rafid Ameer and Faisal, Fahim and Mahmud, Saaduddin and Khan, Md. Mosaddek},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1690–1692},
  title     = {A simulation based online planning algorithm for multi-agent cooperative environments},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536078},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Group-level fairness maximization in online bipartite
matching. <em>AAMAS</em>, 1687–1689. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the allocation of limited resources to heterogeneous customers who arrive in an online fashion. We would like to allocate the resources &#39;&#39;fairly&#39;&#39;, so that no group of customers is marginalized in terms of their overall service rate. We study whether this is possible to do so in an online fashion, and if so, what a good online allocation policy is.We model this problem using online bipartite matching under stationary arrivals, a fundamental model in the literature typically studied under the objective of maximizing the total number of customers served. We instead study the objective of maximizing the minimum service rate across all groups, and propose two notions of fairness: long-run and short-run.For these fairness objectives, we analyze how competitive online algorithms can be, in comparison to offline algorithms which know the sequence of demands in advance. For long-run fairness, we propose two online heuristics (Sampling and Pooling) which establish asymptotic optimality in different regimes (no specialized supplies, no rare demand types, or imbalanced supply/demand). By contrast, outside all of these regimes, we show that the competitive ratio of online algorithms is between 0.632 and 0.732. For short-run fairness, we show for complete bipartite graphs that the competitive ratio of online algorithms is between 0.863 and 0.942; we also derive a probabilistic rejection algorithm which is asymptotically optimal in the total demand.Depending on the overall scarcity of resources, either our Sampling or Pooling heuristics could be desirable. The most difficult situation for online allocation occurs when the total supply is just enough to serve the total demand, in which case an organization could try to make allocations offline instead.We simulate our algorithms on a public ride-hailing dataset, which both demonstrates the efficacy of our heuristics and validates our managerial insights.},
  archive   = {C_AAMAS},
  author    = {Ma, Will and Xu, Pan and Xu, Yifan},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1687–1689},
  title     = {Group-level fairness maximization in online bipartite matching},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536077},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multimodal reinforcement learning with effective state
representation learning. <em>AAMAS</em>, 1684–1686. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many real-world applications require an agent to make robust and deliberate decisions with multimodal information (e.g., robots with multi-sensory inputs). However, it is very challenging to train the agent via reinforcement learning (RL) due to the heterogeneity and dynamic importance of different modalities. Specifically, we observe that these issues make conventional RL methods difficult to learn a useful state representation in the end-to-end training with multimodal information. To address this, we propose a novel multimodal RL approach that can do multimodal alignment and importance enhancement according to their similarity and importance in terms of RL tasks respectively. By doing so, we are able to learn an effective state representation and consequentially improve the RL training process. We test our approach on several multimodal RL domains, showing that it outperforms state-of-the-art methods in terms of learning speed and policy quality.},
  archive   = {C_AAMAS},
  author    = {Ma, Jinming and Chen, Yingfeng and Wu, Feng and Ji, Xianpeng and Ding, Yu},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1684–1686},
  title     = {Multimodal reinforcement learning with effective state representation learning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536076},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling affective reaction in multi-agent systems.
<em>AAMAS</em>, 1681–1683. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Affective reaction is a mechanism that is triggered by a perceived emotion and results in the observer&#39;s emotion, subsequently steering the behavior of the observer. It is crucial to understand this mechanism as it constitutes a mediator between the emotional expressions of one agent and the (behavioral) responses of another thus facilitating social interaction among agents. This paper reports on a formal specification of affective reaction based on its psychological analysis using values and value systems as agents&#39; appraisal standards.},
  archive   = {C_AAMAS},
  author    = {Luo, Jieting and Dastani, Mehdi},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1681–1683},
  title     = {Modeling affective reaction in multi-agent systems},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536075},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). (Almost) envy-free, proportional and efficient allocations
of an indivisible mixed manna. <em>AAMAS</em>, 1678–1680. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of finding fair and efficient allocations of a set of indivisible items to a set of agents, where each item may be a good (positively valued) for some agents and a bad (negatively valued) for others, i.e., a mixed manna. As fairness notions, we consider arguably the strongest possible relaxations of envy-freeness and proportionality, namely envy-free up to any item (EFX and EFX0), and proportional up to the maximin good or any bad (PropMX and PropMX0). Our efficiency notion is Pareto-optimality (PO). We study two types of instances: (i) Separable, where the item set can be partitioned into goods and bads, and (ii) Restricted mixed goods (RMG), where for each item j, every agent has either a non-positive value for j, or values j at the same vj&amp;gt;0. We obtain polynomial-time algorithms for the following: Separable instances: PropMX0 allocation. RMG instances: Let pure bads be the set of items that everyone values negatively. PropMX allocation for general pure bads. EFX+PropMX allocation for identically-ordered pure bads. EFX+PropMX+PO allocation for identical pure bads. Finally, if the RMG instances are further restricted to binary mixed goods where all the vj&#39;s are the same, we strengthen the results to guarantee EFX0 and PropMX0 respectively.},
  archive   = {C_AAMAS},
  author    = {Livanos, Vasilis and Mehta, Ruta and Murhekar, Aniket},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1678–1680},
  title     = {(Almost) envy-free, proportional and efficient allocations of an indivisible mixed manna},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536074},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving generalization with cross-state behavior matching
in deep reinforcement learning. <em>AAMAS</em>, 1675–1677. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Representation learning on visualized input is an essential yet challenging task for deep reinforcement learning (RL). To help the RL agent learn more general and discriminative representation among various states, we present cross-state self-constraint (CSSC). This novel technique regularizes representation learning by comparing state embedding similarities across different state-action pairs. We test our proposed method on the OpenAI Procgen benchmark with Rainbow and PPO and demonstrate significant improvement across most Procgen environments.},
  archive   = {C_AAMAS},
  author    = {Liu, Guan-Ting and Lin, Guan-Yu and Cheng, Pu-Jen},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1675–1677},
  title     = {Improving generalization with cross-state behavior matching in deep reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536073},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Theoretical models and preliminary results for contact
tracing and isolation. <em>AAMAS</em>, 1672–1674. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Efficient contact tracing and isolation is an effective strategy to control epidemics, as seen in the Ebola epidemic and COVID-19 pandemic. An important consideration in contact tracing is the budget on the number of individuals asked to quarantine---the budget is limited for socioeconomic reasons (e.g., having a limited number of contact tracers). Here, we present a Markov Decision Process (MDP) framework to formulate the problem of using contact tracing to reduce the size of an outbreak while limiting the number of people quarantined. We formulate each step of the MDP as a combinatorial problem, MinExposed, which we demonstrate is NP-Hard. Next, we develop two approximation algorithms, one based on rounding the solutions of a linear program and another (greedy algorithm) based on choosing nodes with a high (weighted) degree. A key feature of the greedy algorithm is that it does not need complete information of the underlying social contact network, making it implementable in practice. Using simulations over realistic networks, we show how the algorithms can help in bending the epidemic curve with a limited number of isolated individuals.},
  archive   = {C_AAMAS},
  author    = {Li, George Z. and Haddadan, Arash and Li, Ann and Marathe, Madhav and Srinivasan, Aravind and Vullikanti, Anil and Zhao, Zeyu},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1672–1674},
  title     = {Theoretical models and preliminary results for contact tracing and isolation},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536072},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automated story sifting using story arcs. <em>AAMAS</em>,
1669–1671. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Story sifting (or story recognition) allows for the exploration of events, stories and patterns that emerge from agent-based simulations. The goal of this work is to automate and reduce the authoring burden for writing sifting queries. In this paper, we use the event traces of agent-based simulations to create Dynamic Character Networks that track the changing relationship scores between every agent in a simulation. These networks allow for the fortunes between any two agents to be plotted against time as a story arc. Similarity scores between story arcs from the simulation and a user&#39;s query arc can be calculated using the Dynamic Time Warping technique. Events corresponding to the story arc that best matches the query arc can then be returned to the user, thus providing an intuitive means for users to sift a variety of stories without coding a search query. These components are implemented in our experimental prototype Arc Sift. The results of a user study support our expectation that Arc Sift is an intuitive and accurate tool that allows human users to sift stories out from a larger chronicle of events produced by an agent-based simulation.},
  archive   = {C_AAMAS},
  author    = {Leong, Wilkins and Porteous, Julie and Thangarajah, John},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1669–1671},
  title     = {Automated story sifting using story arcs},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536071},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Measuring resilience in collective robotic algorithms.
<em>AAMAS</em>, 1666–1668. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Measuring and comparing resilience is crucial for evaluating different algorithms&#39; performance. Existing resilience metrics focus on a system&#39;s ability to maintain a particular state, but are inadequate to evaluate whether a system can achieve a novel state after an unexpected disturbance. The presentedresilience power metric is used to analyze two best-of-N algorithms. Both algorithms exhibited high resilience power when changing the collective&#39;s population size, but this result did not correlate with high overall task performance.},
  archive   = {C_AAMAS},
  author    = {Leaf, Jennifer and Adams, Julie A.},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1666–1668},
  title     = {Measuring resilience in collective robotic algorithms},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536070},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning generalizable multi-lane mixed-autonomy behaviors
in single lane representations of traffic. <em>AAMAS</em>, 1663–1665.
(<a href="https://dl.acm.org/doi/10.5555/3535850.3536069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper tackles the problem of learning generalizable congestion-mitigation strategies in simple representations of traffic. In particular, we look to mixed-autonomy ring roads as depictions of instabilities common to many generic settings, and ask the question: What features are needed to ensure that policies here can be adapted to typical multi-lane highways? To answer this, we study the implications of the scale of the source task and the modeling of pseudo-lane change events within it on the transferability of policies learned to complex networks. Our findings suggest that negating the effects of boundary conditions and introducing lane changes that approximately match trends in more complex systems can significantly improve the generalizability of learned behaviors.},
  archive   = {C_AAMAS},
  author    = {Kreidieh, Abdul Rahman and Zhao, Yibo and Parajuli, Samyak and Bayen, Alexandre M.},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1663–1665},
  title     = {Learning generalizable multi-lane mixed-autonomy behaviors in single lane representations of traffic},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536069},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Minimizing expected intrusion detection time in adversarial
patrolling. <em>AAMAS</em>, 1660–1662. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In adversarial patrolling games, a mobile Defender strives to discover intrusions at vulnerable targets initiated by an Attacker.The Attacker&#39;s utility is traditionally defined as the probability of completing an attack, possibly weighted by target costs.However, in many real-world scenarios, the actual damage caused by the Attacker depends on the time elapsed since the attack&#39;s initiation to its detection. We introduce a formal model for such scenarios, and we show that the Defender always has an optimal strategy achieving maximal protection. We also prove that finite-memory Defender&#39;s strategies are sufficient for achieving protection arbitrarily close to the optimum. Then, we design an efficient strategy synthesis algorithm based on differentiable programming and gradient descent. We evaluate the efficiency of our method experimentally.},
  archive   = {C_AAMAS},
  author    = {Kla\v{s}ka, David and Ku\v{c}era, Anton\&#39;{\i}n and Musil, Vit and Reh\&#39;{a}k, Vojtech},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1660–1662},
  title     = {Minimizing expected intrusion detection time in adversarial patrolling},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536068},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Popularity and strict popularity in altruistic hedonic games
and minimum-based altruistic hedonic games. <em>AAMAS</em>, 1657–1659.
(<a href="https://dl.acm.org/doi/10.5555/3535850.3536067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider average-based and min-based altruistic hedonic games and study the problem of verifying popular and strictly popular coalition structures. While strict popularity verification has been shown to be coNP-complete in min-based altruistic hedonic games, this problem has been open for equal-treatment and altruistic-treatment average-based altruistic hedonic games. We solve these two open cases of strict popularity verification and then provide the first complexity results for popularity verification in (both average- and min-based) altruistic hedonic games, where we cover all three degrees of altruism.},
  archive   = {C_AAMAS},
  author    = {Kerkmann, Anna Maria and Rothe, J\&quot;{o}rg},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1657–1659},
  title     = {Popularity and strict popularity in altruistic hedonic games and minimum-based altruistic hedonic games},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536067},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). How to train your agent: Active learning from human
preferences and justifications in safety-critical environments.
<em>AAMAS</em>, 1654–1656. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Training reinforcement learning agents in real-world environments is costly, particularly for safety-critical applications. Human input can enable an agent to learn a good policy while avoiding unsafe actions, but at the cost of bothering the human with repeated queries. We present a model for safe learning in safety-critical environments from human input that minimises bother cost. Our model, JPAL-HA, proposes an efficient mechanism to harness human preferences and justifications to significantly improve safety during the learning process without increasing the number of interactions with a user. We show this with both simulation and human experiments.},
  archive   = {C_AAMAS},
  author    = {Kazantzidis, Ilias and Norman, Timothy J. and Du, Yali and Freeman, Christopher T.},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1654–1656},
  title     = {How to train your agent: Active learning from human preferences and justifications in safety-critical environments},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536066},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Forgiving debt in financial network games. <em>AAMAS</em>,
1651–1653. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A financial system is represented by a network, where nodes correspond to banks, and directed labeled edges correspond to debt contracts between banks. Once a payment schedule has been defined, where we assume that a bank cannot refuse a payment towards one of its lenders if it has sufficient funds, the liquidity of the system is defined as the sum of total payments made in the network. Maximizing systemic liquidity is a natural objective of any financial authority, so, we study the setting where the financial authority offers bailout money to some bank(s) or forgives the debts of others in order to maximize liquidity, and examine efficient ways to achieve this. We investigate the approximation ratio provided by the greedy bailout policy compared to the optimal one, and we study the computational hardness of finding the optimal debt removal and budget-constrained optimal bailout policy, respectively.We also study financial systems from a game-theoretic standpoint. We observe that the removal of some incoming debt might be in the best interest of a bank, if that helps one of its borrowers remain solvent and avoid costs related to default. Assuming that a bank&#39;s well-being (i.e., utility) is aligned with the incoming payments they receive from the network, we define and analyze a game among banks who want to maximize their utility by strategically giving up some incoming payments. In addition, we extend the previous game by considering bailout payments. After formally defining the above games, we prove results about the existence and quality of pure Nash equilibria, as well as the computational complexity of finding such equilibria.},
  archive   = {C_AAMAS},
  author    = {Kanellopoulos, Panagiotis and Kyropoulou, Maria and Zhou, Hao},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1651–1653},
  title     = {Forgiving debt in financial network games},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536065},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). REFORM: Reputation based fair and temporal reward framework
for crowdsourcing. <em>AAMAS</em>, 1648–1650. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Crowdsourcing is an effective method to collect data by employing distributed human population. Researchers introduce Peer-Based Mechanisms (PBMs) in crowdsourcing settings to incentivize agents to report accurately. We observe that with PBMs, crowdsourcing systems may not be fair. Unfair rewards for the agents may discourage participation. This work aims to build a general framework that assures fairness for PBMs in a temporal setting, i.e., where reports are time-sensitive. Towards this, we introduce two notions of fairness for PBMs, namely γ-fairness and qualitative fairness. To satisfy these notions, our framework provides trustworthy agents with additional chances of pairing. We introduce Temporal Reputation Model (TERM) to quantify agents&#39; trustworthiness across tasks. Having TERM, we present our iterative framework, REFORM, that can adopt the reward scheme of any existing PBM. We demonstrate REFORM&#39;s significance by deploying the framework with RPTSC&#39;s reward scheme and prove that REFORM with RPTSC considerably improves fairness; while incentivizing truthful and early reports.},
  archive   = {C_AAMAS},
  author    = {Kanaparthy, Samhita and Damle, Sankarshan and Gujar, Sujit},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1648–1650},
  title     = {REFORM: Reputation based fair and temporal reward framework for crowdsourcing},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536064},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to advise and learning from advice in cooperative
multiagent reinforcement learning. <em>AAMAS</em>, 1645–1647. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel policy-level generative adversarial learning framework to enhance cooperative multiagent reinforcement learning (MARL), which consists of a centralized advisor, MARL agents and discriminators. The advisor is realized through a dual graph convolutional network (DualGCN) to give advice to agents from a global perspective via fusing decision information, resolving spatial conflicts, and maintaining temporal continuity. Each discriminator trained can distinguish between the policies of the advisor and an agent. Leveraging the discriminator&#39;s judgment, each agent learns to match with the advised policy in addition to learning by its own exploration, which accelerates learning and enhances policy performance. Additionally, an advisor boosting method which incorporates the relevant suggestion made by the discriminators into the training of DualGCN is proposed to further help improve MARL agents. We validate our methods in cooperative navigation tasks. Results demonstrate that our method outperforms baseline methods in terms of both learning efficiency and policy efficacy.},
  archive   = {C_AAMAS},
  author    = {Jin, Yue and Wei, Shuangqing and Yuan, Jian and Zhang, Xudong},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1645–1647},
  title     = {Learning to advise and learning from advice in cooperative multiagent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536063},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Near-optimal reviewer splitting in two-phase paper reviewing
and conference experiment design. <em>AAMAS</em>, 1642–1644. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many scientific conferences employ a two-phase paper review process, where some papers are assigned additional reviewers after the initial reviews are submitted. Many conferences also design and run experiments on their paper review process, where some papers are assigned reviewers who provide reviews under an experimental condition. In this paper, we consider the question: how should reviewers be divided between phases or conditions in order to maximize total assignment similarity? We show both empirically (on real conference data) and theoretically (under certain natural conditions) that dividing reviewers uniformly at random is near-optimal. The full paper is available at urlhttps://arxiv.org/abs/2108.06371.},
  archive   = {C_AAMAS},
  author    = {Jecmen, Steven and Zhang, Hanrui and Liu, Ryan and Fang, Fei and Conitzer, Vincent and Shah, Nihar B.},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1642–1644},
  title     = {Near-optimal reviewer splitting in two-phase paper reviewing and conference experiment design},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536062},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data-driven agent-based models for optimal evacuation of
large metropolitan areas for improved disaster planning. <em>AAMAS</em>,
1639–1641. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Evacuation plans are designed to move people to safety in case of a disaster. It mainly consists of two components: routing and scheduling. Joint optimization of these two components with the goal of minimizing total evacuation time is a computationally hard problem, specifically when the problem instance is large. Moreover, often in disaster situations, there is uncertainty regarding the passability of roads throughout the evacuation time period. In this paper, we present a way to model the time-varying risk associated with roads in disaster situations. We also design a heuristic method based on the well known Large Neighborhood Search framework to perform the joint optimization task. We use real-world road network and population data from Harris County in Houston, Texas and apply our heuristic to find evacuation routes and schedules for the area. We show that the proposed method is able to find good solutions within a reasonable amount of time. We also perform agent-based simulations of the evacuation using these solutions to evaluate their quality and efficacy.},
  archive   = {C_AAMAS},
  author    = {Islam, Kazi Ashik and Marathe, Madhav and Mortveit, Henning and Swarup, Samarth and Vullikanti, Anil},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1639–1641},
  title     = {Data-driven agent-based models for optimal evacuation of large metropolitan areas for improved disaster planning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536061},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Argumentative forecasting. <em>AAMAS</em>, 1636–1638. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce the Forecasting Argumentation Framework (FAF), a novel argumentation framework for forecasting informed by recent judgmental forecasting research. FAFs comprise update frameworks which empower (human or artificial) agents to argue over time with and about probability of scenarios, whilst flagging perceived irrationality in their behaviour with a view to improving their forecasting accuracy. FAFs include three argument types with future forecasts and aggregate the strength of these arguments to inform estimates of the likelihood of scenarios. We describe an implementation of FAFs for supporting forecasting agents.},
  archive   = {C_AAMAS},
  author    = {Irwin, Benjamin and Rago, Antonio and Toni, Francesca},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1636–1638},
  title     = {Argumentative forecasting},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536060},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Guaranteeing half-maximin shares under cardinality
constraints. <em>AAMAS</em>, 1633–1635. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of fair allocation of a set of indivisible items among agents with additive valuations, under cardinality constraints. In this setting, the items are partitioned into categories, each with its own limit on the number of items it may contribute to any bundle. We consider the fairness measure known as the maximin share (MMS) guarantee, and propose a novel polynomial-time algorithm for finding 1/2-approximate MMS allocations---an improvement from the previously best available guarantee of 11/30.},
  archive   = {C_AAMAS},
  author    = {Hummel, Halvard and Hetland, Magnus Lie},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1633–1635},
  title     = {Guaranteeing half-maximin shares under cardinality constraints},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536059},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiagent q-learning with sub-team coordination.
<em>AAMAS</em>, 1630–1632. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For cooperative mutliagent reinforcement learning tasks, we propose a novel value factorization framework in the popular centralized training with decentralized execution paradigm, called multiagent Q-learning with sub-team coordination (QSCAN). This framework could flexibly exploit local coordination within sub-teams for effective factorization while honoring the individual-global-max (IGM) condition. QSCAN encompasses the full spectrum of sub-team coordination according to sub-team size, ranging from the monotonic value function class to the entire IGM function class, with familiar methods such as QMIX and QPLEX located at the respective extremes of the spectrum. Empirical results show that QSCAN&#39;s performance dominates state-of-the-art methods in predator-prey tasks and the Switch challenge in MA-Gym.},
  archive   = {C_AAMAS},
  author    = {Huang, Wenhan and Li, Kai and Shao, Kun and Zhou, Tianze and Luo, Jun and Wang, Dongge and Mao, Hangyu and Hao, Jianye and Wang, Jun and Deng, Xiaotie},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1630–1632},
  title     = {Multiagent Q-learning with sub-team coordination},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536058},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intelligent communication over realistic wireless networks
in multi-agent cooperative games. <em>AAMAS</em>, 1627–1629. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In MARL, communication among agents is essential to establish cooperation. Over the realistic wireless network, many factors can affect transmission reliability, especially considering that the wireless network condition varies with agents&#39; mobility. We propose a framework that improves the intelligence of communication over realistic wireless networks in two fundamental aspects: (1) When: Agents learn the timing of communication based on message importance and wireless channel condition. We further propose a communication lagging technique to make the training end-to-end differentiable. (2) What: Agents augment message contents with wireless network measurements. The messages improve both the game and communication actions of the agents. Experiments on a standard environment show that compared with state-of-the-art, our framework enables more intelligent collaboration and thus achieves significantly better game performance, convergence speed and communication efficiency.},
  archive   = {C_AAMAS},
  author    = {Hu, Diyi and Zhang, Chi and Prasanna, Viktor and Krishnamachari, Bhaskar},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1627–1629},
  title     = {Intelligent communication over realistic wireless networks in multi-agent cooperative games},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536057},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Implementation of actual data for artificial market
simulation. <em>AAMAS</em>, 1624–1626. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study proposes a new scheme for implementing actual data into artificial market simulations at the level of trader agents. Because humans can introduce bias or overlook the important features of actual traders, we implemented the actual data and automated the strategy learning (imitating) of agents using machine learning (ML). We then ran artificial market simulations in the treader model, which imitates the actual trading behaviors in an ML architecture. Through this study, we demonstrate the potentials and limitations of the proposed scheme.},
  archive   = {C_AAMAS},
  author    = {Hirano, Masanori and Izumi, Kiyoshi and Sakaji, Hiroki},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1624–1626},
  title     = {Implementation of actual data for artificial market simulation},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536056},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Decision-theoretic planning for the expected scalarised
returns. <em>AAMAS</em>, 1621–1623. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In sequential multi-objective decision making (MODeM) settings, when the utility of a user is derived from a single execution of a policy, policies for the expected scalarised returns (ESR) criterion should be computed. In multi-objective settings, a user&#39;s preferences over objectives, or utility function, may be unknown at the time of planning. When the utility function of a user is unknown, multi-policy methods are deployed to compute a set of optimal policies. However, the state-of-the-art sequential MODeM multi-policy algorithms compute a set of optimal policies for the scalarised expected returns (SER) criterion. Algorithms that compute a set of optimal policies for the SER criterion utilise expected value vectors which cannot be used when optimising for the ESR criterion. We propose multi-objective distributional value iteration (MODVI) that replaces value vectors with distributions over the returns and computes a set of optimal policies for the ESR criterion.},
  archive   = {C_AAMAS},
  author    = {Hayes, Conor F. and Roijers, Diederik M. and Howley, Enda and Mannion, Patrick},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1621–1623},
  title     = {Decision-theoretic planning for the expected scalarised returns},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536055},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-agent task allocation for fruit picker team formation.
<em>AAMAS</em>, 1618–1620. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent task allocation methods seek to distribute a set of tasks fairly amongst a set of agents. In real-world settings, such as fruit farms, human labourers undertake harvesting tasks, organised each day by farm manager(s) who assign workers to the fields that are ready to be harvested. The work presented here considers three challenges identified in the adaptation of a multi-agent task allocation methodology applied to the problem of distributing workers to fields. First, the methodology must be fast to compute so that it can be applied on a daily basis. Second, the incremental acquisition of harvesting data used to make decisions about worker-task assignments means that a data-backed approach must be derived from incomplete information as the growing season unfolds. Third, the allocation must take &quot;fairness&#39;&#39; into account and consider worker motivation. Solutions to these challenges are demonstrated, showing statistically significant results based on the operations at a soft fruit farm during their 2020 and 2021 harvesting seasons.},
  archive   = {C_AAMAS},
  author    = {Harman, Helen and Sklar, Elizabeth I.},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1618–1620},
  title     = {Multi-agent task allocation for fruit picker team formation},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536054},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Capacitated network design games on a generalized fair
allocation model. <em>AAMAS</em>, 1616–1617. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The cost-sharing connection game is a variant of routing games on a network. In this model, given a directed graph with edge costs and capacities, each agent wants to construct a path from a source to a sink with low cost. The cost of each edge is shared by the users based on a cost-sharing function. One of simple cost-sharing functions is defined as the cost divided by the number of users. It models an ideal setting, where no overhead arises when people share things, though it might be quite rare in real life. In this paper, we model more realistic scenarios of cost-sharing connection games by generalizing the cost-sharing function. The arguments do not depend on specific cost-sharing functions and are applicable for a class of all natural cost-sharing scenarios, which include equal divisions with any natural functional overheads. We show that many bounds of the Price of Anarchy and the Price of Stability under sum-cost and max-cost criteria inherit the no-overhead case.},
  archive   = {C_AAMAS},
  author    = {Hanaka, Tesshu and Hirose, Toshiyuki and Ono, Hirotaka},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1616–1617},
  title     = {Capacitated network design games on a generalized fair allocation model},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536053},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Proof-of-work as a stigmergic consensus algorithm.
<em>AAMAS</em>, 1613–1615. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we make a theoretical analysis from a coordination perspective and conclude that the Proof-of-Work (PoW) algorithm is a stigmergic consensus algorithm where the trace left by an action in the blockchain through indirect coordination of agents stimulates subsequent actions and eventually creates a single chain of blocks.},
  archive   = {C_AAMAS},
  author    = {G\&quot;{u}rcan, \&quot;{O}nder},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1613–1615},
  title     = {Proof-of-work as a stigmergic consensus algorithm},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536052},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive aggregation weight assignment for federated
learning: A deep reinforcement learning approach. <em>AAMAS</em>,
1610–1612. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Federated learning (FL) has recently received considerable attention due to its capability of allowing distributed clients to collaboratively train a global model without sharing their private data. However, due to the heterogeneous data distributions/contents of clients, it is non-trivial to accurately evaluate the contributions of local models in global model aggregation. Most existing works simply use the amount of data to assign weights of clients in global model aggregation, e.g., FedAvg, which however is not effective, especially when the data of clients is non-IID. To address this issue, this paper aims to propose a novel FL algorithm, which can accurately evaluate the contributions of clients and aggregate global model in a more efficient manner. More specifically, this paper proposes a Deep Reinforcement Learning (DRL) method to dynamically learn the contributions of clients in each communication round. Based on this, we adaptively assign appropriate weights to clients, which will be used in global model aggregation. By improving the process of global mode aggregation, our proposed scheme greatly improves the performance of federated learning.},
  archive   = {C_AAMAS},
  author    = {Guo, Enwei and Wang, Xiumin and Wu, Weiwei},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1610–1612},
  title     = {Adaptive aggregation weight assignment for federated learning: A deep reinforcement learning approach},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536051},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A graph neural network reasoner for game description
language. <em>AAMAS</em>, 1607–1609. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {General Game Playing (GGP) aims to develop agents that are able to play any game with only rules given. The game rules are encoded in the Game Description Language (GDL). A GGP player processes the game rules to obtain game states and expand the game tree search for an optimal move. The recent accomplishments of AlphaGo and AlphaZero have triggered new works in extending neural network approaches to GGP. In these works, the neural networks are used only for optimal move selection, while the components dealing with GDL still use logic-based methods. This motivates us to explore if a neural network based method would be able to approximate the logical inference in GDL with a high accuracy. The structured nature of logic tends to be a difficulty for neural networks, which rely heavily on statistical features. Inspired by the recent works on neural network learning for logical entailments, we propose a neural network based reasoner that is able to learn logical inferences for GDL. We present three key contributions: (i) a general, game-agnostic graph-based representation for game states described in GDL, (ii) methods for generating samples and datasets to frame the GDL inference task as a neural network based machine learning problem and (iii) a GNN based neural reasoner that is able to learn and infer various game states with a high accuracy and has some capability of transfer learning across games.},
  archive   = {C_AAMAS},
  author    = {Gunawan, Alvaro and Ruan, Ji and Huang, Xiaowei},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1607–1609},
  title     = {A graph neural network reasoner for game description language},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536050},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Minimizing robot navigation graph for position-based
predictability by humans. <em>AAMAS</em>, 1604–1606. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When multiple humans and robots are moving in spaces like restaurants, hospitals, or banks, making the robot&#39;s movements easy to predict can help the humans co-navigate the space with the robots. Since people would be busy with their own goals, they are not paying close attention to the prior movements, or goals of multiple robots. So predictability from the robot&#39;s current position alone would help. With this in mind, we propose using an algorithm to lay out fixed paths for the different tasks the robots would do, such that predictability from only the current position alone is optimized, and motion costs are kept within acceptable bounds.},
  archive   = {C_AAMAS},
  author    = {Gopalakrishnan, Sriram and Kambhampati, Subbarao},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1604–1606},
  title     = {Minimizing robot navigation graph for position-based predictability by humans},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536049},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Influencing emergent self-assembled structures in robotic
collectives through traffic control. <em>AAMAS</em>, 1601–1603. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multiagent self-assembly allows collectives to reach areas otherwise inaccessible to any particular agent. However, the coordination of this collective is not trivial, so each agent&#39;s position in the structure is usually determined apriori. In our approach, we take inspiration from army ants and use a simulated model of the Eciton Robotica robot to form emergent structures with bio-inspired local rules. We demonstrate that by coupling this with traffic control, we can induce the formation of a structure and control certain characteristics without pre-computed paths or central coordination.},
  archive   = {C_AAMAS},
  author    = {Gonzalez, Everardo and Houel, Lucie and Nagpal, Radhika and Malley, Melinda},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1601–1603},
  title     = {Influencing emergent self-assembled structures in robotic collectives through traffic control},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536048},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An anytime heuristic algorithm for allocating many teams to
many tasks. <em>AAMAS</em>, 1598–1600. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many practical applications, we often need to form a team of agents to solve a task, since no agent alone has the full set of required competences or the power to complete the task on time. Here, we address the problem of distributing individuals in teams, with each team being in charge of a specific task. In particular, we formalise the problem and propose a heuristic approach to solve it.},
  archive   = {C_AAMAS},
  author    = {Georgara, Athina and Rodr\&#39;{\i}guez-Aguilar, Juan A. and Sierra, Carles and Mich, Ornella and Kazhamiakin, Raman and Palmero Approsio, Alessio and Pazzaglia, Jean-Christophe},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1598–1600},
  title     = {An anytime heuristic algorithm for allocating many teams to many tasks},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536047},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stable matching games. <em>AAMAS</em>, 1595–1597. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Gale and Shapley introduced a matching problem between two sets of agents where each agent on one side has an exogenous preference ordering over the agents on the other side. They defined a matching as stable if no unmatched pair can both improve their utility by forming a new pair. They proved, algorithmically, the existence of a stable matching. Shapley and Shubik, Demange and Gale, and many others extended the model by allowing monetary transfers. We offer a further extension by assuming that matched couples obtain their payoff endogenously as the outcome of a strategic game they have to play in a usual non-cooperative sense (without commitment) or in a semi-cooperative way (with commitment, as the outcome of a bilateral binding contract in which each player is responsible for his/her part of the contract). Depending on whether the players can commit or not, we define in each case a solution concept that combines Gale-Shapley pairwise stability with a (generalized) Nash equilibrium stability. In each case, we give the necessary and sufficient conditions for the set of stable allocations to be non-empty, we study its lattice structure, and provide an algorithm that converges to its maximal element. Finally, we prove that our second model -with commitment- encompasses and refines most of the literature.},
  archive   = {C_AAMAS},
  author    = {Garrido-Lucero, Felipe and Laraki, Rida},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1595–1597},
  title     = {Stable matching games},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536046},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Beyond uninformed search: Improving branch-and-bound based
acceleration algorithms for belief propagation via heuristic strategies.
<em>AAMAS</em>, 1592–1594. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Belief propagation algorithms including Max-sum and its variants are important methods for solving DCOPs. However, they may face a tough challenge when handling n-ary constraints since the computational overheads grow exponentially with the number of variables that a utility function holds.In this paper, we update the state-of-the-art technique called Function Decomposing and State Pruning (FDSP) which can significantly reduce such an expenditure, by introducing two heuristic techniques. By introducing a round-robin mechanism to control the order of exploration, we propose Concurrent-search-based FDSP (CONC-FDSP). Besides, we propose Best-first-search-based FDSP (BFS-FDSP) by using the A* search to find the optimal path to the solution. Finally, we demonstrate their efficiency in solving the benchmarks compared with the state-of-the-art.},
  archive   = {C_AAMAS},
  author    = {Gao, Junsong and Chen, Ziyu and Chen, Dingding and Zhang, Wenxin},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1592–1594},
  title     = {Beyond uninformed search: Improving branch-and-bound based acceleration algorithms for belief propagation via heuristic strategies},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536045},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Safety shields, an automated failure handling mechanism for
BDI agents. <em>AAMAS</em>, 1589–1591. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Belief Desire Intention model is a widely used architecture for developing rational agents. Because of its expressiveness, the task of programming a BDI agent can be challenging, especially when applied to safety-critical scenarios. In such scenarios, it is important to provide a safeguard for the critical behaviour of the agent. In this paper, we summarise how to extend the agent&#39;s reasoning cycle in the BDI model with safety shields. A safety shield works as a sandbox for the agents&#39; plans that is enforced at runtime so that the agent behaves according to a safety formal specification. A runtime monitor is automatically synthesised from a shield to detect any failure that is within the scope of the shielded plan.},
  archive   = {C_AAMAS},
  author    = {Ferrando, Angelo and Cardoso, Rafael C.},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1589–1591},
  title     = {Safety shields, an automated failure handling mechanism for BDI agents},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536044},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Approaching the overbidding puzzle in all-pay auctions:
Explaining human behavior through bayesian optimization and equilibrium
learning. <em>AAMAS</em>, 1586–1588. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It is an established fact in behavioral economics that in lab experiments of auctions, human subjects do not adhere to the risk-neutral Bayesian Nash equilibria of such games. Several attempts at explaining this Overbidding Puzzle focus on the bidders&#39; psychology and suggest they may have parametrized utility functions that differ from the risk-neutral payoff. However, analytical equilibria of the resulting modified games are generally not available. Consequently, it has been difficult to identify the specific parameters and assess the merits of these proposed modifications in explaining empirical observations.With recent advances in equilibrium learning, it has become tractable to compute approximations of Bayesian Nash equilibria. Building on these advances and Bayesian optimization, we propose a novel regression framework to infer unobserved parameters of Bayesian games from behavioral data. We apply our method to two data sets of human bidding behavior in all-pay auctions. For the first time, this makes it possible to directly compare the goodness-of-fit of several proposed qualitative explanations of overbidding.},
  archive   = {C_AAMAS},
  author    = {Ewert, Markus and Heidekr\&quot;{u}ger, Stefan and Bichler, Martin},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1586–1588},
  title     = {Approaching the overbidding puzzle in all-pay auctions: Explaining human behavior through bayesian optimization and equilibrium learning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536043},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rawlsian fairness in online bipartite matching: Two-sided,
group, and individual. <em>AAMAS</em>, 1583–1585. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Online bipartite-matching platforms are ubiquitous and find applications in important areas such as crowdsourcing and ridesharing. In the most general form, the platform consists of three entities: two sides to be matched and a platform operator that decides the matching. The design of algorithms for such platforms has traditionally focused on the operator&#39;s (expected) profit. Recent reports have shown that certain demographic groups may receive less favorable treatment under pure profit maximization. As a result, a collection of online matching algorithms have been developed that give a fair treatment guarantee for one side of the market at the expense of a drop in the operator&#39;s profit. In this paper, we generalize the existing work to offer fair treatment guarantees to both sides of the market simultaneously, at a calculated worst case drop to operator profit. We consider group and individual Rawlsian fairness criteria. Moreover, our algorithms have theoretical guarantees and have adjustable parameters that can be tuned as desired to balance the trade-off between the utilities of the three sides. We also derive hardness results that give clear upper bounds over the performance of any algorithm.},
  archive   = {C_AAMAS},
  author    = {Esmaeili, Seyed A. and Duppala, Sharmila and Nanda, Vedant and Srinivasan, Aravind and Dickerson, John P.},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1583–1585},
  title     = {Rawlsian fairness in online bipartite matching: Two-sided, group, and individual},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536042},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-agent adversarial attacks for multi-channel
communications. <em>AAMAS</em>, 1580–1582. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently Reinforcement Learning (RL) has been applied as an anti-adversarial remedy in wireless communication networks. However studying the RL-based approaches from the adversary&#39;s perspective has received little attention. Additionally, RL-based approaches in an anti-adversary or adversarial paradigm mostly consider single-channel communication (either channel selection or single channel power control), while multi-channel communication is more common in practice. In this paper, we propose a multi-agent adversary system (MAAS) for modeling and analyzing adversaries in a wireless communication scenario by careful design of the reward function under realistic communication scenarios. In particular, by modeling the adversaries as learning agents, we show that the proposed MAAS is able to successfully choose the transmitted channel(s) and their respective allocated power(s) without any prior knowledge of the sender strategy. Compared to the single-agent adversary (SAA), multi-agents in MAAS can achieve significant reduction in signal-to-noise ratio (SINR) under the same power constraints and partial observability, while providing improved stability and a more efficient learning process. Moreover, through empirical studies we show that the results in simulation are close to the ones in communication in reality, a conclusion that is pivotal to the validity of performance of agents evaluated in simulations.},
  archive   = {C_AAMAS},
  author    = {Dong, Juncheng and Wu, Suya and Soltani, Mohammadreza and Tarokh, Vahid},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1580–1582},
  title     = {Multi-agent adversarial attacks for multi-channel communications},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536041},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Behavior exploration and team balancing for heterogeneous
multiagent coordination. <em>AAMAS</em>, 1578–1579. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Diversity in behaviors is instrumental for robust team performance in many multiagent tasks which require agents to coordinate. Unfortunately, exhaustive search through the agents&#39; behavior spaces is often intractable. This paper introduces Behavior Exploration for Heterogeneous Teams (BEHT), a multi-level learning framework that enables agents to progressively explore regions of the behavior space that promote team coordination on diverse goals. By combining diversity search to maximize agent-specific rewards and evolutionary optimization to maximize the team-based fitness, our method effectively filters regions of the behavior space that are conducive to agent coordination. We demonstrate the diverse behaviors and synergies that are method allows agents to learn on a multiagent exploration problem.},
  archive   = {C_AAMAS},
  author    = {Dixit, Gaurav and Tumer, Kagan},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1578–1579},
  title     = {Behavior exploration and team balancing for heterogeneous multiagent coordination},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536040},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Priced gerrymandering. <em>AAMAS</em>, 1575–1577. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We initiate the study of the gerrymandering problem when changing the district of a voter incurs a certain cost. In this problem, the input is a set of voters having votes over a set of alternatives, a graph on the voters, a partition of voters into connected districts, a cost of every voter for changing her district, a budget, and a target winner. We need to compute if the given partition can be modified so that (i) the target alternative wins the resulting election, (ii) the modification is budget feasible, and (iii) every new district is connected. We study four natural variants of the above problem -- the graph on the voters being arbitrary vs complete graph (corresponds to removing the connectivity requirement for districts) and the cost of moving every voter being uniform vs non-uniform. We show that all the four problems are NP-complete even under quite restrictive scenarios. Hence, our results show that district based elections are quite resistant under this new kind of electoral attack. We complement our intractability results by showing that two of our problems admit polynomial-time algorithms if the budget or the number of districts is a constant.},
  archive   = {C_AAMAS},
  author    = {Dey, Palash},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1575–1577},
  title     = {Priced gerrymandering},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536039},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-agent covering option discovery through kronecker
product of factor graphs. <em>AAMAS</em>, 1572–1574. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Covering option discovery has been developed to improve the exploration of reinforcement learning in single-agent scenarios with sparse reward signals, through connecting the most distant states in the embedding space provided by the Fiedler vector of the state transition graph. However, these option discovery methods cannot be directly extended to multi-agent scenarios, since the joint state space grows exponentially with the number of agents in the system. Thus, existing researches on adopting options in multi-agent scenarios still rely on single-agent option discovery and fail to directly discover the joint options that can improve the connectivity of the joint state space of agents. In this paper, we show that it is indeed possible to directly compute multi-agent options with collaborative exploratory behaviors among the agents, while still enjoying the ease of decomposition. Our key idea is to approximate the joint state space as a Kronecker graph - the Kronecker product of individual agents&#39; state transition graphs, based on which we can directly estimate the Fiedler vector of the joint state space using the Laplacian spectrum of individual agents&#39; transition graphs. This decomposition enables us to efficiently construct multi-agent joint options by encouraging agents to connect the sub-goal joint states which are corresponding to the minimum or maximum values of the estimated joint Fiedler vector. The evaluation based on multi-agent collaborative tasks shows that the proposed algorithm can successfully identify multi-agent options, and significantly outperforms prior works using single-agent options or no options, in terms of both faster exploration and higher cumulative rewards.},
  archive   = {C_AAMAS},
  author    = {Chen, Jiayu and Chen, Jingdi and Lan, Tian and Aggarwal, Vaneet},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1572–1574},
  title     = {Multi-agent covering option discovery through kronecker product of factor graphs},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536038},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-unit double auctions: Equilibrium analysis and bidding
strategy using DDPG in smart-grids. <em>AAMAS</em>, 1569–1571. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a Nash equilibrium analysis for single-buyer single-seller multi-unit k-double auctions for scaling-based bidding strategies. We then design a Deep Deterministic Policy Gradient (DDPG) based learning strategy, DDPGBBS, for a participating agent to suggest bids that approximately achieve the above Nash equilibrium. We expand DDPGBBS to be helpful in more complex settings with multiple buyers/sellers trading multiple units in a Periodic Double Auction (PDA), such as the wholesale market in smart-grids. We demonstrate the efficacy of DDPGBBS with Power Trading Agent Competition&#39;s (PowerTAC) wholesale market PDA as a testbed.},
  archive   = {C_AAMAS},
  author    = {Chandlekar, Sanjay and Subramanian, Easwar and Bhat, Sanjay and Paruchuri, Praveen and Gujar, Sujit},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1569–1571},
  title     = {Multi-unit double auctions: Equilibrium analysis and bidding strategy using DDPG in smart-grids},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536037},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Augmented reality visualizations using imitation learning
for collaborative warehouse robots. <em>AAMAS</em>, 1566–1568. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Augmented reality (AR) technologies have been applied to human-robot collaboration (HRC) domains to enable people to visualize the state of the robots. Current AR-based visualization strategies are manually designed. This design process requires a lot of human efforts, and domain knowledge. When too little information is visualized, human users find the AR interface not useful; when too much is visualized, they find it difficult to process the visualized information. In this paper, we develop an intelligent AR agent that learns visualization policies (what to visualize, when, and how) from demonstrations. We developed a Unity-based platform for simulating warehouse environments where human-robot teammates work on collaborative delivery tasks. We have collected a dataset that includes 6000 demonstrations of visualizing robots&#39; current and planned behaviors. Our results from experiments with real human participants show that, compared with competitive baselines from the literature, our learned visualization strategy significantly increases the efficiency of human-robot teams in delivery tasks.},
  archive   = {C_AAMAS},
  author    = {Chandan, Kishan and Albertson, Jack and Zhang, Shiqi},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1566–1568},
  title     = {Augmented reality visualizations using imitation learning for collaborative warehouse robots},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536036},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal local bayesian differential privacy over markov
chains. <em>AAMAS</em>, 1563–1565. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we focus on data generated from a Markov chain and provide optimal mechanisms for local Bayesian differential privacy (BDP) guarantees. Our main theoretical contribution is to provide a mechanism for achieving BDP when data is drawn from a binary Markov chain. We improve on the state-of-the-art BDP mechanism and show that our mechanism provides the optimal noise-privacy tradeoffs for any local mechanism up to negligible factors. We perform experiments on synthetic data to show that a correlation aware adversary can launch successful attacks on data that satisfies only the vanilla differential privacy guarantees. Finally, we perform experiments on real data to show that our privacy guarantees are robust to underlying distributions that are not simple Markov chains.},
  archive   = {C_AAMAS},
  author    = {Chakrabarti, Darshan and Gao, Jie and Saraf, Aditya and Schoenebeck, Grant and Yu, Fang-Yi},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1563–1565},
  title     = {Optimal local bayesian differential privacy over markov chains},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536035},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On fair and efficient solutions for budget apportionment.
<em>AAMAS</em>, 1560–1562. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This works deals with an apportionment problem. In this problem involving multiple agents, it is desirable to propose fair and efficient solutions. Several alternative notions of fairness exist but combining efficiency with fairness is often impossible, and a trade-off has to be made. We first study the computation of almost fair and approximately efficient solutions, and we determine when these two goals can be met. Afterwards, we characterize the price of fairness which bounds the loss of efficiency caused by imposing fairness or one of its relaxations.},
  archive   = {C_AAMAS},
  author    = {Cardi, Pierre and Gourves, Laurent and Lesca, Julien},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1560–1562},
  title     = {On fair and efficient solutions for budget apportionment},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536034},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Solving n-player dynamic routing games with congestion: A
mean-field approach. <em>AAMAS</em>, 1557–1559. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The recent emergence of navigational tools has changed traffic patterns and has now enabled new types of congestion-aware routing control like dynamic road pricing. Using the fundamental diagram of traffic flows - applied in macroscopic and mesoscopic traffic modeling - the article introduces a new N-player dynamic routing game with explicit congestion dynamics. The model is well-posed and can reproduce heterogeneous departure times and congestion spill back phenomena. However, as Nash equilibrium computations are PPAD-complete, solving the game becomes intractable for large but realistic numbers of vehicles N. Therefore, the corresponding mean field game is also introduced. Experiments were performed on several classical benchmark networks of the traffic community: the Pigou, Braess, and Sioux Falls networks with heterogeneous origin, destination and departure time tuples. The Pigou and the Braess examples reveal that the mean field approximation is generally very accurate and computationally efficient as soon as the number of vehicles exceeds a few dozen. On the Sioux Falls network (76 links, 100 time steps), this approach enables learning traffic dynamics with more than 14,000 vehicles.},
  archive   = {C_AAMAS},
  author    = {Cabannes, Theophile and Lauri\`{e}re, Mathieu and Perolat, Julien and Marinier, Raphael and Girgin, Sertan and Perrin, Sarah and Pietquin, Olivier and Bayen, Alexandre M. and Goubault, Eric and Elie, Romuald},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1557–1559},
  title     = {Solving N-player dynamic routing games with congestion: A mean-field approach},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536033},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Voting for centrality. <em>AAMAS</em>, 1554–1556. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In network science, centrality indices are used to determine important nodes by virtue of their position in a network. In social choice theory, voting rules are used to aggregate preferences of voters to determine the winners of an election. Exploiting parallels between these two fields, we propose a novel approach to define network centrality indices based on voting rules. Since formal properties of voting rules have been studied in much greater depth, this will not only lead to new applications of social choice theory but also facilitate a deeper understanding of centrality in networks.},
  archive   = {C_AAMAS},
  author    = {Brandes, Ulrik and Lau\ss{}mann, Christian and Rothe, J\&quot;{o}rg},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1554–1556},
  title     = {Voting for centrality},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536032},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Contrastive explanations for argumentation-based
conclusions. <em>AAMAS</em>, 1551–1553. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we discuss contrastive explanations for formal argumentation - the question why one argument (the fact) can be accepted, whilst another argument (the foil) cannot be accepted. We show under which conditions contrastive explanations in abstract argumentation are meaningful, and how argumentation allows us to make implicit foils explicit.},
  archive   = {C_AAMAS},
  author    = {Borg, AnneMarie and Bex, Floris},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1551–1553},
  title     = {Contrastive explanations for argumentation-based conclusions},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536031},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A refined complexity analysis of fair districting over
graphs. <em>AAMAS</em>, 1548–1550. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the NP-hard Fair Connected Districting problem: Partition a vertex-colored graph into k connected components (subsequently referred to as districts) so that in every district the most frequent color occurs at most a given number of times more often than the second most frequent color. Fair Connected Districting is motivated by various real-world scenarios, such as district-based elections, where agents of different types, which are one-to-one represented by nodes in a network, have to be partitioned into disjoint districts. We conduct a fine-grained analysis of the (parameterized) computational complexity of Fair Connected Districting: We study its parameterized complexity with respect to various graph parameters, including treewidth, and problem-specific parameters, including the numbers of colors and districts, and its complexity on graphs from different classes (such as paths, stars, and trees).},
  archive   = {C_AAMAS},
  author    = {Boehmer, Niclas and Koana, Tomohiro and Niedermeier, Rolf},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1548–1550},
  title     = {A refined complexity analysis of fair districting over graphs},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536030},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The evolutionary dynamics of soft-max policy gradient in
multi-agent settings. <em>AAMAS</em>, 1545–1547. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate the mean dynamics of the soft-max policy gradient algorithm in multi-agent normal-form games by resorting to evolutionary game theory and dynamical system tools. First, we consider the best-response problem analysis, where a single learner plays against a fixed opponent in continuous time. For such dynamics, we provide a complete characterization of the set of bad initializations (points for which the dynamics initially move towards sub-optimal strategies). Then, we resort to models based on single- and multi-population games, showing that the dynamics preserve the volume and, in arbitrary instances, it is impossible to obtain last-iterate convergence when the equilibrium of the game is fully mixed. Furthermore, we give empirical evidence that dynamics starting from close initial points may expand over time, thus showing that the behavior of the dynamics in games with fully-mixed equilibrium is chaotic.},
  archive   = {C_AAMAS},
  author    = {Bernasconi, Martino and Cacciamani, Federico and Fioravanti, Simone and Gatti, Nicola and Trov\`{o}, Francesco},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1545–1547},
  title     = {The evolutionary dynamics of soft-max policy gradient in multi-agent settings},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536029},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the average-case complexity of predicting round-robin
tournaments. <em>AAMAS</em>, 1542–1544. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Round-robin tournaments are, besides single-elimination tournaments, by far the most prominent and widely used tournament format in sports and other competitions. We study the average-case complexity of two problems related to the prediction of round-robin tournaments, namely first the problem of calculating the championship probability of a team and second the well-known sports elimination problem where one has to decide whether a team still has the possibility to become champion. We show that, under certain assumptions, these problems are solvable in expected polynomial time for a distribution which, for the algorithm used, seems to dominate the distribution of real instances in terms of complexity, despite their computational worst-case hardness.},
  archive   = {C_AAMAS},
  author    = {Baumeister, Dorothea and Hogrebe, Tobias},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1542–1544},
  title     = {On the average-case complexity of predicting round-robin tournaments},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536028},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new porous structure for modular robots. <em>AAMAS</em>,
1539–1541. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a new self-reconfiguration method based on organizing modules in meta-modules that form a 3D porous structure allowing modules to flow in parallel inside of it without blocking. We first propose a new meta-module model which can be used to fill in its internal volume an additional number of modules and describe the structure that can be built with our meta-modules which allow it to be compressible and expandable. We then show how to self-reconfigure the structure from an initial shape to a given goal shape.},
  archive   = {C_AAMAS},
  author    = {Bassil, Jad and Piranda, Beno\^{\i}t and Makhoul, Abdallah and Bourgeois, Julien},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1539–1541},
  title     = {A new porous structure for modular robots},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536027},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Can algorithms be explained without compromising efficiency?
The benefits of detection and imitation in strategic classification.
<em>AAMAS</em>, 1536–1538. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Given the ubiquity of AI-based decisions that affect individuals&#39; lives, providing transparent explanations about algorithms is ethically sound and often legally mandatory. How do individuals strategically adapt following explanations? What are the consequences of adaptation for algorithmic accuracy? We simulate the interplay between explanations shared by an Institution (e.g. a bank) and the dynamics of strategic adaptation by Individuals reacting to such feedback. Resorting to an agent-based approach, our model scrutinizes the role of: i) transparency in explanations, ii) detection capacity and iii) behavior imitation. We find that the risks of transparent explanations are alleviated if effective methods to detect faking behaviors are in place. Furthermore, we observe that social learning and imitation --- as often observed across societies --- is likely to alleviate the impacts of (malicious) adaptation.},
  archive   = {C_AAMAS},
  author    = {Barsotti, Flavia and Ko\c{c}er, R\&quot;{u}ya G\&quot;{o}khan and Santos, Fernando P.},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1536–1538},
  title     = {Can algorithms be explained without compromising efficiency? the benefits of detection and imitation in strategic classification},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536026},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep learnable strategy templates for multi-issue bilateral
negotiation. <em>AAMAS</em>, 1533–1535. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose the notion of deep reinforcement learning-based strategy templates for multi-issue bilateral negotiation. Each strategy template consists of a set of interpretable parameterized tactics that are used to decide an optimal action at any time. This contrasts with existing work that only estimates the threshold utility for those tactics that require it. As a result, we build automated agents for multi-issue negotiations that can adapt to different negotiation domains without the need to be pre-programmed.},
  archive   = {C_AAMAS},
  author    = {Bagga, Pallavi and Paoletti, Nicola and Stathis, Kostas},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1533–1535},
  title     = {Deep learnable strategy templates for multi-issue bilateral negotiation},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536025},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Status-quo policy gradient in multi-agent reinforcement
learning. <em>AAMAS</em>, 1530–1532. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Individual rationality, which involves maximizing expected individual returns, does not always lead to high-utility individual or group outcomes in multi-agent problems. For instance, in multi-agent social dilemmas, Reinforcement Learning (RL) agents trained to maximize individual rewards converge to a low-utility mutually harmful equilibrium. In contrast, humans evolve useful strategies in such social dilemmas. Inspired by ideas from human psychology that attribute this behavior to the status-quo bias, we present a status-quo loss (SQLoss) and the corresponding policy gradient algorithm that incorporates this bias in an RL agent. We demonstrate that agents trained with SQLoss learn high-utility policies in several social dilemma matrix games (Prisoner&#39;s Dilemma, Matching Pennies, Chicken Game). To apply SQLoss to visual input games where cooperation and defection are determined by a sequence of lower-level actions, we present GameDistill, an algorithm that reduces a visual input game to a matrix game. We empirically show how agents trained with SQLoss on GameDistill reduced versions of Coin Game and Stag Hunt learn high-utility policies. Finally, we show that SQLoss extends to a 4-agent setting by demonstrating the emergence of cooperative behavior in the popular Braess&#39; paradox.},
  archive   = {C_AAMAS},
  author    = {Badjatiya, Pinkesh and Sarkar, Mausoom and Puri, Nikaash and Subramanian, Jayakumar and Sinha, Abhishek and Singh, Siddharth and Krishnamurthy, Balaji},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1530–1532},
  title     = {Status-quo policy gradient in multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536024},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Advising agent for service-providing live-chat operators.
<em>AAMAS</em>, 1527–1529. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Call centers, in which human operators attend clients using textual chat, are very common in modern e-commerce. Training enough skilled operators who are able to provide good service is a challenge. We propose a methodology for the development of an assisting agent that provides online advice to operators while they attend clients. The agent is easy-to-build and can be introduced to new domains without major effort in design, training and organizing structured knowledge of the professional discipline. We demonstrate the applicability of the system in an experiment that realizes its full life-cycle on a specific domain, and analyze its capabilities.},
  archive   = {C_AAMAS},
  author    = {Aviv, Aviram and Oshrat, Yaniv and Assefa, Samuel and Mustapha, Toby and Borrajo, Daniel and Veloso, Manuela and Kraus, Sarit},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1527–1529},
  title     = {Advising agent for service-providing live-chat operators},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536023},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Local advantage networks for cooperative multi-agent
reinforcement learning. <em>AAMAS</em>, 1524–1526. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent reinforcement learning (MARL) enables us to create adaptive agents in challenging environments, even when the agents have limited observation. Modern MARL methods have focused on finding factorized value functions. While successful, the resulting methods have convoluted network structures. We take a radically different approach and build on the structure of independent Q-learners. Our algorithm LAN leverages a dueling architecture to represent decentralized policies as separate individual advantage functions w.r.t. a centralized critic that is cast aside after training. The critic works as a stabilizer that coordinates the learning and to formulate DQN targets. This enables LAN to keep the number of parameters of its centralized network independent in the number of agents, without imposing additional constraints like monotonic value functions. When evaluated on the SMAC, LAN shows SOTA performance overall and scores more than 80\% wins in two super-hard maps where even QPLEX does not obtain almost any wins. Moreover, when the number of agents becomes large, LAN uses significantly fewer parameters than QPLEX or even QMIX. We thus show that LAN&#39;s structure forms a key improvement that helps MARL methods remain scalable.},
  archive   = {C_AAMAS},
  author    = {Avalos, Rapha\&quot;{e}l and Reymond, Mathieu and Now\&#39;{e}, Ann and Roijers, Diederik M.},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1524–1526},
  title     = {Local advantage networks for cooperative multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536022},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RASS: Risk-aware swarm storage. <em>AAMAS</em>, 1521–1523.
(<a href="https://dl.acm.org/doi/10.5555/3535850.3536021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In robotics, data acquisition often plays a key part in unknown environment exploration. For example, storing information about the topography of the explored terrain can inform the decision-making process of the robots. Therefore, it is crucial to store these data safely and to make it available quickly to the operators of the robotic system. In a decentralized system like a swarm of robots, this entails several challenges. To address them, we propose RASS, a decentralized risk-aware swarm storage and routing mechanism, which relies exclusively on local information sharing between neighbours to establish storage and routing fitness. We test our system through thorough experiments in a physics-based simulator and obtain convincing reliability, routing speeds, and swarm storage capacity results.},
  archive   = {C_AAMAS},
  author    = {Arseneault, Samuel and Vielfaure, David and Beltrame, Giovanni},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1521–1523},
  title     = {RASS: Risk-aware swarm storage},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536021},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). R-CHECK: A model checker for verifying reconfigurable MAS.
<em>AAMAS</em>, 1518–1520. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reconfigurable multi-agent systems consist of a set of autonomous agents, with integrated interaction capabilities that feature opportunistic interaction. Agents seemingly reconfigure their interactions interfaces by forming collectives, and interact based on mutual interests. Finding ways to design and analyse the behaviour of these systems is a vigorously pursued research goal. We propose a model checker, named R-CHECK, to allow reasoning about these systems both from an individual- and a system-level. R-CHECK also permits reasoning about interaction protocols and joint missions. R-CHECK supports a high-level input language with symbolic semantics, and provides a modelling convenience for interaction features such as reconfiguration, coalition formation, self-organisation, etc.},
  archive   = {C_AAMAS},
  author    = {Abd Alrahman, Yehia and Azzopardi, Shaun and Piterman, Nir},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1518–1520},
  title     = {R-CHECK: A model checker for verifying reconfigurable MAS},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536020},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Incentives to invite others to form larger coalitions.
<em>AAMAS</em>, 1509–1517. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study a cooperative game setting where players form a network and each player only knows the existence of the players to whom she connects. Initially, only a subset of the players are in the game. Our goal is to design a reward distribution mechanism to incentivize the players to use their connections to invite more players to join the game. We show that the existing solutions such as the Shapley value cannot achieve this. Hence, to combat this problem, we propose a solution called weighted permission Shapley value (inspired by permission structure and the weighted Shapley value). Under this solution, for each player, inviting all her neighbors is a dominant strategy in all monotone games. We further prove that the solution is unique for tree networks. Our solution offers the very first attempt to incentivize the players to invite others to form a larger coalition in cooperative games.},
  archive   = {C_AAMAS},
  author    = {Zhang, Yao and Zhao, Dengji},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1509–1517},
  title     = {Incentives to invite others to form larger coalitions},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536018},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Centralized model and exploration policy for multi-agent RL.
<em>AAMAS</em>, 1500–1508. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning (RL) in partially observable, fully cooperative multi-agent settings (Dec-POMDPs) can in principle be used to address many real-world challenges such as controlling a swarm of rescue robots or a team of quadcopters. However, Dec-POMDPs are significantly harder to solve than single-agent problems, with the former being NEXP-complete and the latter, MDPs, being just P-complete. Hence, current RL algorithms for Dec-POMDPs suffer from poor sample complexity, which greatly reduces their applicability to practical problems where environment interaction is costly. Our key insight is that using just a polynomial number of samples, one can learn a centralized model that generalizes across different policies. We can then optimize the policy within the learned model instead of the true system, without requiring additional environment interactions. We also learn a centralized exploration policy within our model that learns to collect additional data in state-action regions with high model uncertainty. We empirically evaluate the proposed model-based algorithm, MARCO, in three cooperative communication tasks, where it improves sample efficiency by up to 20x. Finally, to investigate the theoretical sample complexity, we adapt an existing model-based method for tabular MDPs to Dec-POMDPs, and prove that it achieves polynomial sample complexity.},
  archive   = {C_AAMAS},
  author    = {Zhang, Qizhen and Lu, Chris and Garg, Animesh and Foerster, Jakob},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1500–1508},
  title     = {Centralized model and exploration policy for multi-agent RL},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536017},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A deeper look at discounting mismatch in actor-critic
algorithms. <em>AAMAS</em>, 1491–1499. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate the discounting mismatch in actor-critic algorithm implementations from a representation learning perspective. Theoretically, actor-critic algorithms usually have discounting for both the actor and critic, i.e., there is a γt term in the actor update for the transition observed at time t in a trajectory and the critic is a discounted value function. Practitioners, however, usually ignore the discounting (γt) for the actor while using a discounted critic. We investigate this mismatch in two scenarios. In the first scenario, we consider optimizing an undiscounted objective (γ = 1) where γt disappears naturally (1t = 1). We then propose to interpret the discounting in the critic in terms of a bias-variance-representation trade-off and provide supporting empirical results. In the second scenario, we consider optimizing a discounted objective (γ &amp;lt; 1) and propose to interpret the omission of the discounting in the actor update from an auxiliary task perspective and provide supporting empirical results.},
  archive   = {C_AAMAS},
  author    = {Zhang, Shangtong and Laroche, Romain and van Seijen, Harm and Whiteson, Shimon and Tachet des Combes, Remi},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1491–1499},
  title     = {A deeper look at discounting mismatch in actor-critic algorithms},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536016},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tracking truth by weighting proxies in liquid democracy.
<em>AAMAS</em>, 1482–1490. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study wisdom-of-the-crowd effects in liquid democracy on networks where agents are allowed to apportion parts of their voting weight to different proxies. We show that in this setting---unlike in the standard one where voting weight is delegated in full to only one proxy---it becomes possible to construct delegation structures that optimize the truth-tracking ability of the group. Focusing on group accuracy we contrast this centralized solution with the setting in which agents are free to choose their weighted delegations by greedily trying to maximize their own individual accuracy. While equilibria with weighted delegations may be as bad as with standard delegations, they are never worse and may sometimes be better. To gain further insights into this model we experimentally study quantal response delegation strategies on random networks. We observe that weighted delegations can lead, under specific conditions, to higher group accuracy than simple majority voting.},
  archive   = {C_AAMAS},
  author    = {Zhang, Yuzhe and Grossi, Davide},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1482–1490},
  title     = {Tracking truth by weighting proxies in liquid democracy},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536015},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The competition and inefficiency in urban road last-mile
delivery. <em>AAMAS</em>, 1473–1481. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The last-mile delivery market is highly competitive and is saturated with numerous small operators. In this context, the fierce competition between operators, joint with the rapid increase in the demand for home-delivery, resulted in a significant increase in urban freight traffic further worsening congestion and pollution. To tackle these issues, previous research has studied the implementation of collaborative last-mile operations, with organisations sharing resources in the form of inventory space or transportation capacity. However, a common limitation of the proposed models is ignoring time windows and the effects of externalities such as network congestion. In this work, we propose a framework to quantify the efficiency loss in urban last-mile delivery system by comparing the solutions of a fully-decentralised and fully-centralised last-mile delivery problem. In doing so, we develop a Multi-depot Vehicle Routing Problem with Time Windows and Congestible Network that is solved using a bespoke Parallel Hybrid Genetic Algorithm that accounts for the non-linearities arising from modelling endogenous network congestion. The model is evaluated on a case study based on central London to assess the efficiency gaps of realistic last-mile delivery operations. When time window constraints are not included, our results show that the efficiency loss fluctuates the most with a small number of customers, while it stabilises to less than 15\% for instances with over 100 customers. However, time windows could significantly exacerbate this issue, resulting in an additional 25\% of efficiency loss.},
  archive   = {C_AAMAS},
  author    = {Zhang, Keyang and Escribano Macias, Jose Javier and Paccagnan, Dario and Angeloudis, Panagiotis},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1473–1481},
  title     = {The competition and inefficiency in urban road last-mile delivery},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536014},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-agent path finding for precedence-constrained goal
sequences. <em>AAMAS</em>, 1464–1472. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the rising demand for deploying robot teams in autonomous warehouses and factories, the Multi-Agent Path Finding (MAPF) problem has drawn more and more attention. The classical MAPF problem and most of its variants focus on navigating agent teams to goal locations while avoiding collisions. However, they do not take into account any precedence constraints that agents should respect when reaching their goal locations. Planning with precedence constraints is important for real-world multi-agent systems. For example, a mobile robot can only pick up a package at a station after it has been delivered by another robot. In this paper, we study the Multi-Agent Path Finding with Precedence Constraints (MAPF-PC) problem, in which agents need to visit sequences of goal locations while satisfying precedence constraints between the goal locations. We propose two algorithms for solving this problem systematically: Conflict-Based Search with Precedence Constraints (CBS-PC) is complete and optimal, and Priority-Based Search with Precedence Constraints (PBS-PC) is incomplete but more efficient in finding near-optimal solutions in practice. Our experimental results show that CBS-PC scales to dozens of agents and hundreds of goal locations and precedence constraints, and PBS-PC scales to hundreds of agents, around one thousand goal locations, and hundreds of precedence constraints.},
  archive   = {C_AAMAS},
  author    = {Zhang, Han and Chen, Jingkai and Li, Jiaoyang and Williams, Brian C. and Koenig, Sven},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1464–1472},
  title     = {Multi-agent path finding for precedence-constrained goal sequences},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536013},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Segregation in social networks of heterogeneous agents
acting under incomplete information. <em>AAMAS</em>, 1455–1463. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose an agent-based network formation model under uncertainty with the objective of relaxing the common assumption of complete information, calling attention to the role beliefs may play in segregation. We demonstrate that our model is capable of generating a set of networks that encompasses those of a complete information model. Further, we show that by allowing agents to hold biased beliefs toward each other based on social group membership, individual utility-maximising decisions may lead to group segregation at the cost of social welfare. We accompany our theoretical results with a simulation-based investigation of the relationship between beliefs and segregation and show that biased beliefs are an important driver of segregation under incomplete information.},
  archive   = {C_AAMAS},
  author    = {Zhang, D. Kai and Carver, Alexander},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1455–1463},
  title     = {Segregation in social networks of heterogeneous agents acting under incomplete information},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536012},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Strategy-proof house allocation with existing tenants over
social networks. <em>AAMAS</em>, 1446–1454. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mechanism design over social networks, whose goal is to incentivize agents to diffuse the information of a mechanism to their followers, as well as to report their true preferences, is one of the new trends in market design. In this paper, we reconsider the traditional house allocation problem with existing tenants from the perspective of mechanism design over social networks. Since our model is a generalization of the networked housing market investigated by Kawasaki et al., no mechanism simultaneously satisfies strategy-proofness, individual rationality and Pareto efficiency for general social network structures. We therefore examine the cases where the social network has a tree structure. We first show that even for the restricted structure, a weaker welfare requirement called non-wastefulness is not achievable by any strategy-proof and individually rational mechanism. We then show that a non-trivial modification of You Request My House - I Get Your Turn mechanism (YRMH-IGYT) is individually rational, strategy-proof, and weakly non-wasteful. Furthermore, it chooses an allocation in the strict core for neighbors and satisfies weak group strategy-proofness.},
  archive   = {C_AAMAS},
  author    = {You, Bo and Dierks, Ludwig and Todo, Taiki and Li, Minming and Yokoo, Makoto},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1446–1454},
  title     = {Strategy-proof house allocation with existing tenants over social networks},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536011},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive incentive design with multi-agent meta-gradient
reinforcement learning. <em>AAMAS</em>, 1436–1445. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Critical sectors of human society are progressing toward the adoption of powerful artificial intelligence (AI) agents, which are trained individually on behalf of self-interested principals but deployed in a shared environment. Short of direct centralized regulation of AI, which is as difficult an issue as regulation of human actions, one must design institutional mechanisms that indirectly guide agents&#39; behaviors to safeguard and improve social welfare in the shared environment. Our paper focuses on one important class of such mechanisms: the problem of adaptive incentive design, whereby a central planner intervenes on the payoffs of an agent population via incentives in order to optimize a system objective. To tackle this problem in high-dimensional environments whose dynamics may be unknown or too complex to model, we propose a model-free meta-gradient method to learn an adaptive incentive function in the context of multi-agent reinforcement learning. Via the principle of online cross-validation, the incentive designer explicitly accounts for its impact on agents&#39; learning and, through them, the impact on future social welfare. Experiments on didactic benchmark problems show that the proposed method can induce selfish agents to learn near-optimal cooperative behavior and significantly outperform learning-oblivious baselines. When applied to a complex simulated economy, the proposed method finds tax policies that achieve better trade-off between economic productivity and equality than baselines, a result that we interpret via a detailed behavioral analysis.},
  archive   = {C_AAMAS},
  author    = {Yang, Jiachen and Wang, Ethan and Trivedi, Rakshit and Zhao, Tuo and Zha, Hongyuan},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1436–1445},
  title     = {Adaptive incentive design with multi-agent meta-gradient reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536010},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Standby-based deadlock avoidance method for multi-agent
pickup and delivery tasks. <em>AAMAS</em>, 1427–1435. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The multi-agent pickup and delivery (MAPD) problem, in which multiple agents iteratively carry materials without collisions, has received significant attention. However, many conventional MAPD algorithms assume a specifically designed grid-like environment, such as an automated warehouse. Therefore, they have many pickup and delivery locations where agents can stay for a lengthy period, as well as plentiful detours to avoid collisions owing to the freedom of movement in a grid. By contrast, because a maze-like environment such as a search-and-rescue or construction site has fewer pickup/delivery locations and their numbers may be unbalanced, many agents concentrate on such locations resulting in inefficient operations, often becoming stuck or deadlocked. Thus, to improve the transportation efficiency even in a maze-like restricted environment, we propose a deadlock avoidance method, called standby-based deadlock avoidance (SBDA). SBDA uses standby nodes determined in real-time using the articulation-point-finding algorithm, and the agent is guaranteed to stay there for a finite amount of time. We demonstrated that our proposed method outperforms a conventional approach. We also analyzed how the parameters used for selecting standby nodes affect the performance.},
  archive   = {C_AAMAS},
  author    = {Yamauchi, Tomoki and Miyashita, Yuki and Sugawara, Toshiharu},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1427–1435},
  title     = {Standby-based deadlock avoidance method for multi-agent pickup and delivery tasks},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536009},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mis-spoke or mis-lead: Achieving robustness in multi-agent
communicative reinforcement learning. <em>AAMAS</em>, 1418–1426. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent studies in multi-agent communicative reinforcement learning (MACRL) have demonstrated that multi-agent coordination can be greatly improved by allowing communication between agents. Meanwhile, adversarial machine learning (ML) have shown that ML models are vulnerable to attacks. Despite the increasing concern about the robustness of ML algorithms, how to achieve robust communication in multi-agent reinforcement learning has been largely neglected. In this paper, we systematically explore the problem of adversarial communication in MACRL. Our main contributions are threefold. First, we propose an effective method to perform attacks in MACRL, by learning a model to generate optimal malicious messages. Second, we develop a defence method based on message reconstruction, to maintain multi-agent coordination under message attacks. Third, we formulate the adversarial communication problem as a two-player zero-sum game and propose a game-theoretical method R-MACRL to improve the worst-case defending performance. Empirical results demonstrate that many state-of-the-art MACRL methods are vulnerable to message attacks, and our method can significantly improve their robustness.},
  archive   = {C_AAMAS},
  author    = {Xue, Wanqi and Qiu, Wei and An, Bo and Rabinovich, Zinovi and Obraztsova, Svetlana and Yeo, Chai Kiat},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1418–1426},
  title     = {Mis-spoke or mis-lead: Achieving robustness in multi-agent communicative reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536008},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spiking pitch black: Poisoning an unknown environment to
attack unknown reinforcement learners. <em>AAMAS</em>, 1409–1417. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As reinforcement learning (RL) systems are deployed in various safety-critical applications, it is imperative to understand how vulnerable they are to adversarial attacks. Of these, an environment-poisoning attack (EPA) is considered particularly insidious, since environment hyper-parameters are significant factors in determining an RL policy, yet prone to be accessed by third parties. The success of EPAs relies on comprehensive prior knowledge of the attacked RL system, including RL agent&#39;s learning mechanism and/or its environment model. Unfortunately, such an assumption of prior knowledge creates an unrealistic attack, one that poses limited threat to real-world RL systems. In this paper, we propose a Double-Black-Box EPA framework, only assuming the attacker&#39;s ability to alter environment hyper-parameters. Considering that environment alteration comes at a cost, we seek minimal poisoning in an unknown environment and aim to force a black-box RL agent to learn an attacker-designed policy. To this end, we incorporate an inference module in our framework to capture the internal information of an unknown RL system and, accordingly, learn an adaptive strategy based on an approximation of our attack objective. We empirically show the threat posed by our attack to both tabular-RL and deep-RL algorithms, in both discrete and continuous environments.},
  archive   = {C_AAMAS},
  author    = {Xu, Hang and Qu, Xinghua and Rabinovich, Zinovi},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1409–1417},
  title     = {Spiking pitch black: Poisoning an unknown environment to attack unknown reinforcement learners},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536007},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SIDE: State inference for partially observable cooperative
multi-agent reinforcement learning. <em>AAMAS</em>, 1400–1408. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As one of the solutions to the decentralized partially observable Markov decision process (Dec-POMDP) problems, the value decomposition method has achieved significant results recently. However, most value decomposition methods require the fully observable state of the environment during training, but this is not feasible in some scenarios where only incomplete and noisy observations can be obtained. Therefore, we propose a novel value decomposition framework, named State Inference for value DEcomposition (SIDE), which eliminates the need to know the global state by simultaneously seeking solutions to the two problems of optimal control and state inference. SIDE can be extended to any value decomposition method to tackle partially observable problems. By comparing with the performance of different algorithms in StarCraft II micromanagement tasks, we verified that though without accessible states, SIDE can infer the current state that contributes to the reinforcement learning process based on past local observations and even achieve superior results to many baselines in some complex scenarios.},
  archive   = {C_AAMAS},
  author    = {Xu, Zhiwei and Bai, Yunpeng and Li, Dapeng and Zhang, Bin and Fan, Guoliang},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1400–1408},
  title     = {SIDE: State inference for partially observable cooperative multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536006},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Agent-temporal attention for reward redistribution in
episodic multi-agent reinforcement learning. <em>AAMAS</em>, 1391–1399.
(<a href="https://dl.acm.org/doi/10.5555/3535850.3536005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper considers multi-agent reinforcement learning (MARL) tasks where agents receive a shared global reward at the end of an episode. The delayed nature of this reward affects the ability of the agents to assess the quality of their actions at intermediate time-steps. This paper focuses on developing methods to learn a temporal redistribution of the episodic reward to obtain a dense reward signal. Solving such MARL problems requires addressing two challenges: identifying (1) relative importance of states along the length of an episode (along time), and (2) relative importance of individual agents&#39; states at any single time-step (among agents). In this paper, we introduce Agent-Temporal Attention for Reward Redistribution in Episodic Multi-Agent Reinforcement Learning (AREL) to address these two challenges. AREL uses attention mechanisms to characterize the influence of actions on state transitions along trajectories (temporal attention), and how each agent is affected by other agents at each time-step (agent attention). The redistributed rewards predicted by AREL are dense, and can be integrated with any given MARL algorithm. We evaluate AREL on challenging tasks from the Particle World environment and the StarCraft Multi-Agent Challenge. AREL results in higher rewards in Particle World, and improved win rates in StarCraft compared to three state-of-the-art reward redistribution methods. Our code is available at https://github.com/baicenxiao/AREL.},
  archive   = {C_AAMAS},
  author    = {Xiao, Baicen and Ramasubramanian, Bhaskar and Poovendran, Radha},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1391–1399},
  title     = {Agent-temporal attention for reward redistribution in episodic multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536005},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Empirical estimates on hand manipulation are recoverable: A
step towards individualized and explainable robotic support in everyday
activities. <em>AAMAS</em>, 1382–1390. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A key challenge for robotic systems is to figure out the behavior of another agent. The capability to draw correct inferences is crucial to derive human behavior from examples.Processing correct inferences is especially challenging when (confounding) factors are not controlled experimentally (observational evidence). For this reason, robots that rely on inferences that are correlational risk a biased interpretation of the evidence.We propose equipping robots with the necessary tools to conduct observational studies on people. Specifically, we propose and explore the feasibility of structural causal models with non-parametric estimators to derive empirical estimates on hand behavior in the context of object manipulation in a virtual kitchen scenario. In particular, we focus on inferences under (the weaker) conditions of partial confounding (the model covering only some factors) and confront estimators with hundreds of samples instead of the typical order of thousands. Studying these conditions explores the boundaries of the approach and its viability.Despite the challenging conditions, the estimates inferred from the validation data are correct. Moreover, these estimates are stable against three refutation strategies where four estimators are in agreement. Furthermore, the causal quantity for two individuals reveals the sensibility of the approach to detect positive and negative effects.The validity, stability, and explainability of the approach are encouraging and serve as the foundation for further research.},
  archive   = {C_AAMAS},
  author    = {Wich, Alexander and Schultheis, Holger and Beetz, Michael},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1382–1390},
  title     = {Empirical estimates on hand manipulation are recoverable: A step towards individualized and explainable robotic support in everyday activities},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536004},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Position-based matching with multi-modal preferences.
<em>AAMAS</em>, 1373–1381. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many models have been proposed for computing a one-to-one matching between two equal-sized sets/sides of agents, each assigned with one preference list of the agents in the opposite side. The most prominent one might be the Stable Matching model. Recently, the Stable Matching model has been extended to the multi-modal setting, where each agent has more than one preference lists, each representing a criterion based on which the agents of the opposite side are evaluated. We use a layer to denote the set of preference lists of agents, which are based on the same criterion. Thus, the single modal matching problem has only one layer. This setting finds applications in many real-world scenarios. However, it turns out that stable matchings might not exist with multi-modal preferences and the determination is NP-hard and W-hard with respect to several natural parameters. Here, we introduce three position-based matching models, which minimize the &#39;&#39;dissatisfaction score&#39;&#39;. We define four dissatisfaction scores, which measure matchings from different perspectives. The first model minimizes the total respective dissatisfaction score over all layers, while the second minimizes the maximum of the respective score over all layers. The third model seeks for a matching M which is Layer Pareto-optimal, meaning that there does not exist a matching M&#39;, which is at least as good as M with respect to the respective dissatisfaction score in all layers, but is strictly better in at least one layer. We present diverse complexity results for these three models, among others, polynomial-time solvability for the first model. We also investigate the generalization which given an upper bound on the dissatisfaction score, computes a matching involving subsets of agents and a subset of layers. Hereby, we mainly focus on the parameterized complexity with respect to parameters such as the size of agent subsets, or the size of the layer subset and achieve fixed-parameter tractability as well as intractability results.},
  archive   = {C_AAMAS},
  author    = {Wen, Yinghui and Zhou, Aizhong and Guo, Jiong},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1373–1381},
  title     = {Position-based matching with multi-modal preferences},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536003},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online collective multiagent planning by offline policy
reuse with applications to city-scale mobility-on-demand systems.
<em>AAMAS</em>, 1364–1372. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The popularity of mobility-on-demand (MoD) systems boosts the need for online collective multiagent planning, where spatially distributed servicing agents are planned to meet dynamically arriving demands. For city-scale MoDs with a population of agents, it is necessary to find a balance between computation time (i.e., real-time) and solution quality (i.e., the number of demands served). Directly using an offline policy can guarantee real-time, but cannot be dynamically adjusted to real agent and demand distributions. On the other hand, search-based online planning methods are adaptive. However, they are computationally expensive and cannot scale up.In this paper, we propose a principled online multiagent planning method, which reuses and improves the offline policy in an anytime manner. We first model MoDs as a collective Markov Decision Process (C-MDP) where the history collective behavior of agents affects the joint reward. We propose a novel state value function to evaluate the policy, and a gradient ascent (GA) technique to improve the policy. We show that GA-based policy iteration (GA-PI) on local policy can converge. Finally, given real-time information, the offline policy is used as the default plan and GA-PI is used to improve it and generate an online plan. Experimentally, the proposed offline policy reuse method significantly outperforms standard online multiagent planning methods on MoD systems like ride-sharing and security traffic patrolling in terms of computation time and solution quality.},
  archive   = {C_AAMAS},
  author    = {Wang, Wanyuan and Wu, Gerong and Wu, Weiwei and Jiang, Yichuan and An, Bo},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1364–1372},
  title     = {Online collective multiagent planning by offline policy reuse with applications to city-scale mobility-on-demand systems},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536002},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FCMNet: Full communication memory net for team-level
cooperation in multi-agent systems. <em>AAMAS</em>, 1355–1363. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Decentralized cooperation in partially-observable multi-agent systems requires effective communications among agents. To support this effort, this work focuses on the class of problems where global communications are available but may be unreliable, thus precluding differentiable communication learning methods. We introduce FCMNet, a reinforcement learning based approach that allows agents to simultaneously learn a) an effective multi-hop communications protocol and b) a common, decentralized policy that enables team-level decision-making. Specifically, our proposed method utilizes the hidden states of multiple directional recurrent neural networks as communication messages among agents. Using a simple multi-hop topology, we endow each agent with the ability to receive information sequentially encoded by every other agent at each time step, leading to improved global cooperation. We demonstrate FCMNet on a challenging set of StarCraft II micromanagement tasks with shared rewards, as well as a collaborative multi-agent pathfinding task with individual rewards. There, our comparison results show that FCMNet outperforms state-of-the-art communication-based reinforcement learning methods in all StarCraft II micromanagement tasks, and value decomposition methods in certain tasks. We further investigate the robustness of FCMNet under realistic communication disturbances, such as random message loss or binarized messages (i.e., non-differentiable communication channels), to showcase FMCNet&#39;s potential applicability to robotic tasks under a variety of real-world conditions.},
  archive   = {C_AAMAS},
  author    = {Wang, Yutong and Sartoretti, Guillaume},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1355–1363},
  title     = {FCMNet: Full communication memory net for team-level cooperation in multi-agent systems},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536001},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluating strategy exploration in empirical game-theoretic
analysis. <em>AAMAS</em>, 1346–1354. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3536000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In empirical game-theoretic analysis (EGTA), game models are extended iteratively through a process of generating new strategies based on experience with prior strategies. The strategy exploration problem in EGTA is how to direct this process so to construct effective models with minimal iteration. A variety of approaches have been proposed in the literature, including methods based on classic techniques and novel concepts. Comparing the performance of these alternatives can depend sensitively on criteria adopted and measures employed. We investigate some of the methodological considerations in evaluating strategy exploration, proposing and justifying new evaluation methods based on examples and experimental observations. In particular, we emphasize the fact that empirical games create a space of strategies and evaluation should reflect how well it covers the strategically relevant space. Based on this fact, we suggest that the minimum regret constrained profile(MRCP) provides a particularly robust basis for evaluating a space of strategies, and propose a local search method for computing MRCP. However, MRCP computation is not always feasible especially in large games. To evaluate strategy exploration in large games, we propose a new evaluation scheme that measures the strategic coverage of an empirical game. Specifically, we highlight consistency considerations for comparing across different approaches. We show that violation of the consistency considerations could yield misleading conclusions on the performance of different approaches. In accord with consistency considerations, we propose a profile-selection method, which effectively discovers the profile that can represent the strategic coverage of an empirical game through its regret information. We show that our evaluation scheme reveals the authentic learning performance of different approaches compared to previous evaluation methods.},
  archive   = {C_AAMAS},
  author    = {Wang, Yongzhao and Ma, Qiurui and Wellman, Michael P.},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1346–1354},
  title     = {Evaluating strategy exploration in empirical game-theoretic analysis},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3536000},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust learning from observation with model
misspecification. <em>AAMAS</em>, 1337–1345. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Imitation learning (IL) is a popular paradigm for training policies in robotic systems when specifying the reward function is difficult. However, despite the success of IL algorithms, they impose the somewhat unrealistic requirement that the expert demonstrations must come from the same domain in which a new imitator policy is to be learned. We consider a practical setting, where (i) state-only expert demonstrations from the real (deployment) environment are given to the learner, (ii) the imitation learner policy is trained in a simulation (training) environment whose transition dynamics is slightly different from the real environment, and (iii) the learner does not have any access to the real environment during the training phase beyond the batch of demonstrations given. Most of the current IL methods, such as generative adversarial imitation learning and its state-only variants, fail to imitate the optimal expert behavior under the above setting. By leveraging insights from the Robust reinforcement learning (RL) literature and building on recent adversarial imitation approaches, we propose a robust IL algorithm to learn policies that can effectively transfer to the real environment without fine-tuning. Furthermore, we empirically demonstrate on continuous-control benchmarks that our method outperforms the state-of-the-art state-only IL method in terms of the zero-shot transfer performance in the real environment and robust performance under different testing conditions.},
  archive   = {C_AAMAS},
  author    = {Viano, Luca and Huang, Yu-Ting and Kamalaruban, Parameswaran and Innes, Craig and Ramamoorthy, Subramanian and Weller, Adrian},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1337–1345},
  title     = {Robust learning from observation with model misspecification},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535999},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Epistemic reasoning in jason. <em>AAMAS</em>, 1328–1336. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an extension to the Jason BDI language to allow qualitative reasoning under uncertainty. We demonstrate the need for such an extension using a challenge from the 2019 Multi-Agent Programming Contest (MAPC), namely localization for navigation. Given the ability to qualitatively reason about what the agent knows and what it considers possible (or impossible), these challenges become easier to express, reason about, and act upon in a Jason program.Through the use of epistemic logic and the epistemic reasoner in Hintikka&#39;s World, our extension allows agents to express epistemic queries; specifically, utilizing the class of single-agent S5 epistemic models to model-check queries about the agent&#39;s uncertainty. This paper also provides an evaluation of the overall performance and scalability of the extension&#39;s implementation to show how it impacts the agent&#39;s reasoning time; from the evaluation results, we use the official 2019 MAPC time constraints to examine the performance tradeoffs of using the presented extension to model and reason about uncertainty.},
  archive   = {C_AAMAS},
  author    = {Vezina, Michael and Esfandiari, Babak},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1328–1336},
  title     = {Epistemic reasoning in jason},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535998},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graphical representation enhances human compliance with
principles for graded argumentation semantics. <em>AAMAS</em>,
1319–1327. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We examined principles of graded argumentation semantics (independence, anonymity, void precedence, and maximality) to explore if (a) they realistically model human reasoning, (b) graphical representation of arguments facilitates compliance with the principles, (c) there is a positive correlation between compliance with different principles, and (d) this compliance is related to cognitive reflection, need for cognition and faith in intuition. The participants (N = 96) were randomly assigned to one of two experimental conditions - the graph group was presented with textual and graphical representations, while the second group was presented only with textual arguments. Our results indicate that there are major differences in the compliance with the several argumentation principles studied in this paper. However, compliance with argumentation principles was consistently better and more consistent in the graph group. Moreover, cognitive reflection correlated with compliance to some principles, but only in the graph group.},
  archive   = {C_AAMAS},
  author    = {Vesic, Srdjan and Yun, Bruno and Teovanovic, Predrag},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1319–1327},
  title     = {Graphical representation enhances human compliance with principles for graded argumentation semantics},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535997},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Controller synthesis for omega-regular and steady-state
specifications. <em>AAMAS</em>, 1310–1318. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Given a Markov decision process (MDP) and a linear-time ($\o{}mega$-regular or Linear Temporal Logic) specification which reasons about the infinite-trace behavior of a system, the controller synthesis problem aims to compute the optimal policy that satisfies said specification. Recently, problems that reason over the complementary infinite-frequency behavior of systems have been proposed through the lens of steady-state planning or steady-state policy synthesis. This entails finding a control policy for an MDP such that the Markov chain induced by the solution policy satisfies a given set of constraints on its steady-state distribution. This paper studies a generalization of the controller synthesis problem for a linear-time specification under steady-state constraints on the asymptotic behavior of the agent. We present an algorithm to find a deterministic policy satisfying $\o{}mega$-regular and steady-state constraints by characterizing the solutions as an integer linear program, and experimentally evaluate our approach.},
  archive   = {C_AAMAS},
  author    = {Velasquez, Alvaro and Alkhouri, Ismail and Beckus, Andre and Trivedi, Ashutosh and Atia, George},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1310–1318},
  title     = {Controller synthesis for omega-regular and steady-state specifications},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535996},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). How to sense the world: Leveraging hierarchy in multimodal
perception for robust reinforcement learning agents. <em>AAMAS</em>,
1301–1309. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work addresses the problem of sensing the world: how to learn a multimodal representation of a reinforcement learning agent&#39;s environment that allows the execution of tasks under incomplete perceptual conditions. To address such problem, we argue for hierarchy in the design of representation models and contribute with a novel multimodal representation model, MUSE. The proposed model learns a hierarchy of representations: low-level modality-specific representations, encoded from raw observation data, and a high-level multimodal representation, encoding joint-modality information to allow robust state estimation. We employ MUSE as the perceptual model of deep reinforcement learning agents provided with multimodal observations in Atari games. We perform a comparative study over different designs of reinforcement learning agents, showing that MUSE allows agents to perform tasks under incomplete perceptual experience with minimal performance loss. Finally, we also evaluate the generative performance of MUSE in literature-standard multimodal scenarios with higher number and more complex modalities, showing that it outperforms state-of-the-art multimodal variational autoencoders in single and cross-modality generation.},
  archive   = {C_AAMAS},
  author    = {Vasco, Miguel and Yin, Hang and Melo, Francisco S. and Paiva, Ana},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1301–1309},
  title     = {How to sense the world: Leveraging hierarchy in multimodal perception for robust reinforcement learning agents},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535995},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Coaching agent: Making recommendations for behavior change.
A case study on improving eating habits. <em>AAMAS</em>, 1292–1300. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many applications, the desire is to change the behavior of users over a period of repeated episodes of similar decision making. For example, in a nutritionist scenario, one may try to encourage the user to adopt better food consumption habits. In this paper, we propose to model this recommendation scenario with a long-term goal, that we call coaching, as an iterative two-player game. At each decision time, the user makes a proposal (e.g., a meal) based on her/his preferences, the coach can then suggest a change in this proposal that the user may or may not accept. After each episode, the user updates her/his preferences and the coach adapts its model of the user. We propose a formalization of the coaching problem and discuss several possible criteria to measure the performance of a coach. Different coaching strategies are described in the paper. They are then tested and compared using a real-world dataset in the field of nutrition, where a user is simulated using a simple, but general, model of decision making and adaptation. Results show that it pays to adapt to user characteristics and use non-myopic strategies, which aim for long-term gains, when the number of interactions becomes large. Although illustrated on choice sequences for food consumption, the scope of the proposed method goes far beyond this use case, as in sports, health, entertainment or tourist activity choices, etc.},
  archive   = {C_AAMAS},
  author    = {Vandeputte, Jules and Cornu\&#39;{e}jols, Antoine and Darcel, Nicolas and Delaere, Fabien and Martin, Christine},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1292–1300},
  title     = {Coaching agent: Making recommendations for behavior change. a case study on improving eating habits},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535994},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Corruption in auctions: Social welfare loss in hybrid
multi-unit auctions. <em>AAMAS</em>, 1283–1291. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We initiate the study of the social welfare loss caused by corrupt auctioneers, both in single-item and multi-unit auctions. In our model, the auctioneer may collude with the winning bidders by letting them lower their bids in exchange for a (possibly bidder-dependent) fraction - of the surplus. We consider different corruption schemes. In the most basic one, all winning bidders lower their bid to the highest losing bid. We show that this setting is equivalent to a y-hybrid auction in which the payments are a convex combination of first-price and the second-price payments. More generally, we consider corruption schemes that can be related to y-approximate first-price auctions (y-FPA), where the payments recover at least a y-fraction of the first-price payments. Our goal is to obtain a precise understanding of the robust price of anarchy (POA) of such auctions. If no restrictions are imposed on the bids, we prove a bound on the robust POA of y-FPA which is tight (over the entire range of y) for the single-item and the multi-unit auction setting. On the other hand, if the bids satisfy the no-overbidding assumption a more fine-grained landscape of the price of anarchy emerges, depending on the auction setting and the equilibrium notion. Albeit being more challenging, we derive (almost) tight bounds for both auction settings and several equilibrium notions, basically leaving open some (small) gaps for the coarse-correlated price of anarchy only.},
  archive   = {C_AAMAS},
  author    = {van Beek, Andries and Brokkelkamp, Ruben and Sch\&quot;{a}fer, Guido},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1283–1291},
  title     = {Corruption in auctions: Social welfare loss in hybrid multi-unit auctions},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535993},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Socially supervised representation learning: The role of
subjectivity in learning efficient representations. <em>AAMAS</em>,
1274–1282. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite its rise as a prominent solution to the data inefficiency of today&#39;s machine learning models, self-supervised learning has yet to be studied from a purely multi-agent perspective. In this work, we propose that aligning internal subjective representations, which naturally arise in a multi-agent setup where agents receive partial observations of the same underlying environmental state, can lead to more data-efficient representations. We propose that multi-agent environments, where agents do not have access to the observations of others but can communicate within a limited range, guarantees a common context that can be leveraged in individual representation learning. The reason is that subjective observations necessarily refer to the same subset of the underlying environmental states and that communication about these states can freely offer a supervised signal. To highlight the importance of communication, we refer to our setting associally supervised representation learning. We present a minimal architecture comprised of a population of autoencoders, where we define loss functions, capturing different aspects of effective communication, and examine their effect on the learned representations. We show that our proposed architecture allows the emergence of aligned representations. The subjectivity introduced by presenting agents with distinct perspectives of the environment state contributes to learning abstract representations that outperform those learned by a single autoencoder and a population of autoencoders, presented with identical perspectives of the environment state. Altogether, our results demonstrate how communication from subjective perspectives can lead to the acquisition of more abstract representations in multi-agent systems, opening promising perspectives for future research at the intersection of representation learning and emergent communication.},
  archive   = {C_AAMAS},
  author    = {Taylor, Julius and Nisioti, Eleni and Moulin-Frier, Cl\&#39;{e}ment},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1274–1282},
  title     = {Socially supervised representation learning: The role of subjectivity in learning efficient representations},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535992},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). How hard is bribery in elections with randomly selected
voters. <em>AAMAS</em>, 1265–1273. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many research works in computational social choice assume a fixed set of voters in an election and study the resistance of different voting rules against electoral manipulation. In recent years, however, a new technique known as random sample voting has been adopted in many multi-agent systems. One of the most prominent examples is blockchain. Many proof-of-stake based blockchain systems like Algorand will randomly select a subset of participants of the system to form a committee, and only the committee members will be involved in the decision of some important system parameters. This can be viewed as running an election where the voter committee (i.e., the voters whose votes will be counted) is randomly selected. It is generally expected that the introduction of such randomness should make the election more resistant to electoral manipulation, despite the lack of theoretical analysis. In this paper, we present a systematic study on the resistance of an election with a randomly selected voter committee against bribery. Since the committee is randomly generated, by bribing any fixed subset of voters, the designated candidate may or may not win. Consequently, we consider the problem of finding a feasible solution that maximizes the winning probability of the designated candidate. We show that for most voting rules, this problem becomes extremely difficult for the briber as even finding any non-trivial solution with non-zero objective value becomes NP-hard. However, for plurality and veto, there exists a polynomial time approximation scheme that computes a near-optimal solution efficiently. The algorithm builds upon a novel integer programming formulation together with techniques from n-fold integer programming, which may be of a separate interest.},
  archive   = {C_AAMAS},
  author    = {Tao, Liangde and Chen, Lin and Xu, Lei and Shi, Weidong and Sunny, Ahmed and Zaman, Md Mahabub Uz},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1265–1273},
  title     = {How hard is bribery in elections with randomly selected voters},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535991},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Descriptive and prescriptive visual guidance to improve
shared situational awareness in human-robot teaming. <em>AAMAS</em>,
1256–1264. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In collaborative tasks involving human and robotic teammates, live communication between agents has potential to substantially improve task efficiency and fluency. Effective communication provides essential situational awareness to adapt successfully during uncertain situations and encourage informed decision-making. In contrast, poor communication can lead to incongruous mental models resulting in mistrust and failures. In this work, we first introduce characterizations of and generative algorithms for two complementary modalities of visual guidance: prescriptive guidance (visualizing recommended actions), and descriptive guidance (visualizing state space information to aid in decision-making). Robots can communicate this guidance to human teammates via augmented reality (AR) interfaces, facilitating synchronization of notions of environmental uncertainty and offering more collaborative and interpretable recommendations. We also introduce a min-entropy multi-agent collaborative planning algorithm for uncertain environments, informing the generation of these proactive visual recommendations for more informed human decision-making. We illustrate the effectiveness of our algorithm and compare these different modalities of AR-based guidance in a human subjects study involving a collaborative, partially observable search task. Finally, we synthesize our findings into actionable insights informing the use of prescriptive and descriptive visual guidance.},
  archive   = {C_AAMAS},
  author    = {Tabrez, Aaquib and Luebbers, Matthew B. and Hayes, Bradley},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1256–1264},
  title     = {Descriptive and prescriptive visual guidance to improve shared situational awareness in human-robot teaming},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535990},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Justifying social-choice mechanism outcome for improving
participant satisfaction. <em>AAMAS</em>, 1246–1255. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many social-choice mechanisms the resulting choice is not the most preferred one for some of the participants, thus the need for methods to justify the choice made in a way that improves the acceptance and satisfaction of said participants. One natural method for providing such explanations is to ask people to provide them, e.g., through crowdsourcing, and choosing the most convincing arguments among those received. In this paper we propose the use of an alternative approach, one that automatically generates explanations based on desirable mechanism features found in theoretical mechanism design literature. We test the effectiveness of both of the methods through a series of extensive experiments conducted with over 600 participants in ranked voting, a classic social choice mechanism. The analysis of the results reveals that explanations indeed affect both average satisfaction from and acceptance of the outcome in such settings. In particular, explanations are shown to have a positive effect on satisfaction and acceptance when the outcome (the winning candidate in our case) is the least desirable choice for the participant. A comparative analysis reveals that the automatically generated explanations result in similar levels of satisfaction from and acceptance of an outcome as with the more costly alternative of crowdsourced explanations, hence eliminating the need to keep humans in the loop. Furthermore, the automatically generated explanations significantly reduce participants&#39; belief that a different winner should have been elected compared to crowdsourced explanations.},
  archive   = {C_AAMAS},
  author    = {Suryanarayana, Sharadhi Alape and Sarne, David and Kraus, Sarit},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1246–1255},
  title     = {Justifying social-choice mechanism outcome for improving participant satisfaction},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535989},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Off-policy evolutionary reinforcement learning with maximum
mutations. <em>AAMAS</em>, 1237–1245. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Advances in Reinforcement Learning (RL) have demonstrated data efficiency and optimal control over large state spaces at the cost of scalable performance. Genetic methods, on the other hand, provide scalability but depict hyperparameter sensitivity towards evolutionary operations. A combination of the two methods has recently demonstrated success in scaling RL agents to high-dimensional action spaces. Parallel to recent developments, we present the Evolution-based Soft Actor-Critic (ESAC), a scalable RL algorithm. We abstract exploration from exploitation by combining Evolution Strategies (ES) with Soft Actor-Critic (SAC). Through this lens, we enable dominant skill transfer between offsprings by making use of soft winner selections and genetic crossovers in hindsight. Simultaneously we improve hyperparameter sensitivity in evolutions using the novel Automatic Mutation Tuning (AMT). AMT gradually replaces the entropy framework of SAC allowing the population to succeed at the task whileacting as randomly as possible, without making use of backpropagation updates. In a study of challenging locomotion tasks consisting of high-dimensional action spaces and sparse rewards, ESAC demonstrates improved performance and sample efficiency in comparison to the Maximum Entropy framework. Additionally, ESAC presents efficacious use of hardware resources and algorithm overhead. Our implementation is available at the https://karush17.github.io/esac-web/ Project Website.},
  archive   = {C_AAMAS},
  author    = {Suri, Karush},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1237–1245},
  title     = {Off-policy evolutionary reinforcement learning with maximum mutations},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535988},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Context-aware modelling for multi-robot systems under
uncertainty. <em>AAMAS</em>, 1228–1236. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Formal models of multi-robot behaviour are fundamental to planning, simulation, and model checking techniques. However, existing models are invalidated by strong assumptions that fail to capture execution-time multi-robot behaviour, such as simplistic duration models or synchronisation constraints. In this paper we propose a novel multi-robot Markov automaton formulation which models asynchronous multi-robot execution in continuous time. Robot dynamics are captured using phase-type distributions over action durations. Moreover, we explicitly model the effects of robot interactions, as they are a key factor for the duration of action execution. We also present a scalable discrete-event simulator which yields realistic statistics over execution-time robot behaviour by sampling through the Markov automaton. We validate our model and simulator against a Gazebo simulation in a range of multi-robot navigation scenarios, demonstrating that our model accurately captures high-level multi-robot behaviour.},
  archive   = {C_AAMAS},
  author    = {Street, Charlie and Lacerda, Bruno and Staniaszek, Michal and M\&quot;{u}hlig, Manuel and Hawes, Nick},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1228–1236},
  title     = {Context-aware modelling for multi-robot systems under uncertainty},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535987},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The generalized magician problem under unknown distributions
and related applications. <em>AAMAS</em>, 1219–1227. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Magician Problem (MP) and its generalization, the Generalized Magician Problem (GMP), were introduced by Alaei et al. (APPROX-RANDOM 2013) and Alaei (SICOMP 2014) and have been used as powerful ingredients in online-algorithm design for many hard problems such as the k-choice prophet inequality, mechanism design in Bayesian combinatorial auctions, and the generalized assignment problem. The adversarial model here is essentially that of an oblivious adversary. In this paper, we introduce generalizations of GMP (MP) under two different arrival settings (by making the adversary stronger): unknown independent identical distributions (UIID) and unknown adversarial distributions (UAD). Different adversary models capture a range of arrival patterns. For GMP under UIID, we show that a natural greedy algorithm Greedy is optimal. For the case of MP under UIID, we show that Greedy has an optimal performance of 1 - BB/B! eB ≥ 1 - 1/√2π B, where B is the budget, and show an application to online B-matching with stochastic rewards. For GMP under UAD, we present a simple algorithm, which is near-optimal among all non-adaptive algorithms. We consider the simple case of MP under UAD with B=1, and give an exact characterization of the respective optimal adaptive and optimal non-adaptive algorithms for any finite time horizon. We offer an example of MP under UAD on which there is a provable gap between the classical MP under adversarial order and MP under UAD even with a time horizon T=4.},
  archive   = {C_AAMAS},
  author    = {Srinivasan, Aravind and Xu, Pan},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1219–1227},
  title     = {The generalized magician problem under unknown distributions and related applications},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535986},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Properties of reputation lag attack strategies.
<em>AAMAS</em>, 1210–1218. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The trustors in a reputation system share trust-relevant information about trustees; their reputation. The presence of lag in the sharing mechanism can be exploited by malicious trustees to perform otherwise impermissible additional bad actions. This is the reputation lag attack. In this paper, we use simulations to explore properties of the reputation lag attack and strategies which improve the attacker&#39;s success. We demonstrate the following key findings: Attackers in lagged systems clearly outperform attackers in lag-free systems. The attacker&#39;s success is proportional to the rate at which they can interact with victims, plateauing at a maximum. Attackers who wait for their good reputation to disseminate outperform those who do not. Attackers who perform only good actions, wait for dissemination and then perform only bad actions outperform attackers who do not follow this ordering. This implies reputation-lag attacks are effective exit strategies. In typical social networks, smart attackers may cheat users with a low centrality, but in a homogeneous network, this strategy is ineffectual. Our findings help allow developers of reputation systems defend against a class of attacks that has not yet received a great deal of attention.},
  archive   = {C_AAMAS},
  author    = {Sirur, S. and Muller, Tim},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1210–1218},
  title     = {Properties of reputation lag attack strategies},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535985},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Anti-malware sandbox games. <em>AAMAS</em>, 1201–1209. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We develop a game theoretic model of malware protection using the state-of-the-art sandbox method, to characterize and compute optimal defense strategies for anti-malware. We model the strategic interaction between developers of malware (M) and anti-malware (AM) as a two player game, where AM commits to a strategy of generating sandbox environments, and M responds by choosing to either attack or hide malicious activity based on the environment it senses. We characterize the condition for AM to protect all its machines, and identify conditions under which an optimal AM strategy can be computed efficiently. For other cases, we provide a quadratically constrained quadratic program (QCQP)-based optimization framework to compute the optimal AM strategy. In addition, we identify a natural and easy to compute strategy for AM, which as we show empirically, achieves AM utility that is close to the optimal AM utility, in equilibrium.},
  archive   = {C_AAMAS},
  author    = {Sikdar, Sujoy and Ruan, Sikai and Han, Qishen and Pitimanaaree, Paween and Blackthorne, Jeremy and Yener, Bulent and Xia, Lirong},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1201–1209},
  title     = {Anti-malware sandbox games},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535984},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ACuTE: Automatic curriculum transfer from simple to complex
environments. <em>AAMAS</em>, 1192–1200. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite recent advances in Reinforcement Learning (RL), many problems, especially real-world tasks, remain prohibitively expensive to learn. To address this issue, several lines of research have explored how tasks, or data samples themselves, can be sequenced into a curriculum to learn a problem that may otherwise be too difficult to learn from scratch. However, generating and optimizing a curriculum in a realistic scenario still requires extensive interactions with the environment. To address this challenge, we formulate the curriculum transfer problem, in which the schema of a curriculum optimized in a simpler, easy-to-solve environment (e.g., a grid world) is transferred to a complex, realistic scenario (e.g., a physics-based robotics simulation or the real world). We present &quot;ACuTE&quot;, Automatic Curriculum Transfer from Simple to Complex Environments, a novel framework to solve this problem, and evaluate our proposed method by comparing it to other baseline approaches (e.g., domain adaptation) designed to speed up learning. We observe that our approach produces improved jumpstart and time-to-threshold performance even when adding task elements that further increase the difficulty of the realistic scenario. Finally, we demonstrate that our approach is independent of the learning algorithm used for curriculum generation, and is Sim2Real transferable to a real world scenario using a physical robot.},
  archive   = {C_AAMAS},
  author    = {Shukla, Yash and Thierauf, Christopher and Hosseini, Ramtin and Tatiya, Gyan and Sinapov, Jivko},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1192–1200},
  title     = {ACuTE: Automatic curriculum transfer from simple to complex environments},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535983},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Using deep learning to bootstrap abstractions for
hierarchical robot planning. <em>AAMAS</em>, 1183–1191. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the problem of learning abstractions that boost robot planning performance while providing strong guarantees of reliability. Although state-of-the-art hierarchical robot planning algorithms allow robots to efficiently compute long-horizon motion plans for achieving user desired tasks, these methods typically rely upon environment-dependent state and action abstractions that need to be hand-designed by experts.We present a new approach for bootstrapping the entire hierarchical planning process. This allows us to compute abstract states and actions for new environments automatically using the critical regions predicted by a deep neural network with an auto-generated robot-specific architecture. We show that the learned abstractions can be used with a novel multi-source bi-directional hierarchical robot planning algorithm that is sound and probabilistically complete. An extensive empirical evaluation on twenty different settings using holonomic and non-holonomic robots shows that (a) our learned abstractions provide the information necessary for efficient multi-source hierarchical planning; and that (b) this approach of learning, abstractions, and planning outperforms state-of-the-art baselines by nearly a factor of ten in terms of planning time on test environments not seen during training.},
  archive   = {C_AAMAS},
  author    = {Shah, Naman and Srivastava, Siddharth},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1183–1191},
  title     = {Using deep learning to bootstrap abstractions for hierarchical robot planning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535982},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning efficient diverse communication for cooperative
heterogeneous teaming. <em>AAMAS</em>, 1173–1182. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {High-performing teams learn intelligent and efficient communication and coordination strategies to maximize their joint utility. These teams implicitly understand the different roles of heterogeneous team members and adapt their communication protocols accordingly. Multi-Agent Reinforcement Learning (MARL) seeks to develop computational methods for synthesizing such coordination strategies, but formulating models for heterogeneous teams with different state, action, and observation spaces has remained an open problem. Without properly modeling agent heterogeneity, as in prior MARL work that leverages homogeneous graph networks, communication becomes less helpful and can even deteriorate the cooperativity and team performance. We propose Heterogeneous Policy Networks (HetNet) to learn efficient and diverse communication models for coordinating cooperative heterogeneous teams. Building on heterogeneous graph-attention networks, we show that HetNet not only facilitates learning heterogeneous collaborative policies per existing agent-class but also enables end-to-end training for learning highly efficient binarized messaging. Our empirical evaluation shows that HetNet sets a new state of the art in learning coordination and communication strategies for heterogeneous multi-agent teams by achieving an 8.1\% to 434.7\% performance improvement over the next-best baseline across multiple domains while simultaneously achieving a 200X reduction in the required communication bandwidth.},
  archive   = {C_AAMAS},
  author    = {Seraj, Esmaeil and Wang, Zheyuan and Paleja, Rohan and Martin, Daniel and Sklar, Matthew and Patel, Anirudh and Gombolay, Matthew},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1173–1182},
  title     = {Learning efficient diverse communication for cooperative heterogeneous teaming},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535981},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sympathy-based reinforcement learning agents.
<em>AAMAS</em>, 1164–1172. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As artificial agents become increasingly prevalent in our daily lives, it becomes imperative to equip them with an awareness of societal norms; specifically, the ability to account for and be considerate towards others they may cohabit with. In this work, we explore the ability for an agent trained through reinforcement learning to exhibit sympathetic behaviours towards another (independent) agent in the environment. We propose to achieve such behaviours by first inferring the reward function of the independent agent, through inverse reinforcement learning, and subsequently learning a policy based on a sympathetic reward function - a convex combination of the inferred rewards and the agent&#39;s own rewards. The corresponding weighting is determined by a sympathy function which is computed based on the estimated return of the agent&#39;s current action relative to that of all possible actions it could have taken. We evaluate our approach on adversarial as well as assistive environment settings, and demonstrate the ability of our sympathetic agent to perform well at its own goal, while simultaneously giving due consideration to another agent in its environment. We also empirically examine and report the sensitivity of our agent&#39;s performance to the hyperparameters introduced in our proposed framework.},
  archive   = {C_AAMAS},
  author    = {Senadeera, Manisha and Karimpanal, Thommen George and Gupta, Sunil and Rana, Santu},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1164–1172},
  title     = {Sympathy-based reinforcement learning agents},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535980},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Group fairness in bandits with biased feedback.
<em>AAMAS</em>, 1155–1163. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel formulation of group fairness with biased feedback in the contextual multi-armed bandit (CMAB) setting. In the CMAB setting, a sequential decision maker must, at each time step, choose an arm to pull from a finite set of arms after observing some context for each of the potential arm pulls. In our model, arms are partitioned into two or more sensitive groups based on some protected feature(s) (e.g., age, race, or socio-economic status). Initial rewards received from pulling an arm may be distorted due to some unknown societal or measurement bias. We assume that in reality these groups are equal despite the biased feedback received by the agent. To alleviate this, we learn a societal bias term which can be used to both find the source of bias and to potentially fix the problem outside of the algorithm. We provide a novel algorithm that can accommodate this notion of fairness for an arbitrary number of groups, and provide a theoretical bound on the regret for our algorithm. We validate our algorithm using synthetic data and two real-world datasets for intervention settings wherein we want to allocate resources fairly across groups.},
  archive   = {C_AAMAS},
  author    = {Schumann, Candice and Lang, Zhi and Mattei, Nicholas and Dickerson, John P.},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1155–1163},
  title     = {Group fairness in bandits with biased feedback},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535979},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Decoupled reinforcement learning to stabilise
intrinsically-motivated exploration. <em>AAMAS</em>, 1146–1154. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Intrinsic rewards can improve exploration in reinforcement learning, but the exploration process may suffer from instability caused by non-stationary reward shaping and strong dependency on hyperparameters. In this work, we introduce Decoupled RL (DeRL) as a general framework which trains separate policies for intrinsically-motivated exploration and exploitation. Such decoupling allows DeRL to leverage the benefits of intrinsic rewards for exploration while demonstrating improved robustness and sample efficiency. We evaluate DeRL algorithms in two sparse-reward environments with multiple types of intrinsic rewards. Our results show that DeRL is more robust to varying scale and rate of decay of intrinsic rewards and converges to the same evaluation returns than intrinsically-motivated baselines in fewer interactions. Lastly, we discuss the challenge of distribution shift and show that divergence constraint regularisers can successfully minimise instability caused by divergence of exploration and exploitation policies.},
  archive   = {C_AAMAS},
  author    = {Sch\&quot;{a}fer, Lukas and Christianos, Filippos and Hanna, Josiah P. and Albrecht, Stefano V.},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1146–1154},
  title     = {Decoupled reinforcement learning to stabilise intrinsically-motivated exploration},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535978},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). REMAX: Relational representation for multi-agent
exploration. <em>AAMAS</em>, 1137–1145. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Training a multi-agent reinforcement learning (MARL) model with a sparse reward is generally difficult because numerous combinations of interactions among agents induce a certain outcome (i.e., success or failure). Earlier studies have tried to resolve this issue by employing an intrinsic reward to induce interactions that are helpful for learning an effective policy. However, this approach requires extensive prior knowledge for designing an intrinsic reward. To train the MARL model effectively without designing the intrinsic reward, we propose a learning-based exploration strategy to generate the initial states of a game. The proposed method adopts a variational graph autoencoder to represent a game state such that (1) the state can be compactly encoded to a latent representation by considering relationships among agents, and (2) the latent representation can be used as an effective input for a coupled surrogate model to predict an exploration score. The proposed method then finds new latent representations that maximize the exploration scores and decodes these representations to generate initial states from which the MARL model starts training in the game and thus experiences novel and rewardable states. We demonstrate that our method improves the training and performance of the MARL model more than the existing exploration methods.},
  archive   = {C_AAMAS},
  author    = {Ryu, Heechang and Shin, Hayong and Park, Jinkyoo},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1137–1145},
  title     = {REMAX: Relational representation for multi-agent exploration},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535977},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GCS: Graph-based coordination strategy for multi-agent
reinforcement learning. <em>AAMAS</em>, 1128–1136. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many real-world scenarios involve a team of agents that have to coordinate their policies to achieve a shared goal. Previous studies mainly focus on decentralized control to maximize a common reward and barely consider the coordination among control policies, which is critical in dynamic and complicated environments. In this work, we propose factorizing the joint team policy into graph generator and graph-based coordinated policy to enable coordinated behaviours among agents. The graph generator adopts an encoder-decoder framework that outputs directed acyclic graphs (DAGs) to capture the underlying dynamic decision structure. We also apply the DAGness-constrained and DAG depth-constrained optimization in the graph generator to balance efficiency and performance. The graph-based coordinated policy exploits the generated decision structure. The graph generator and coordinated policy are trained simultaneously to maximize the discounted return. Empirical evaluations on Collaborative Gaussian Squeeze, Cooperative Navigation, and Google Research Football demonstrate the superiority of the proposed method. The code is available at urlhttps://github.com/Amanda-1997/GCS_aamas337.},
  archive   = {C_AAMAS},
  author    = {Ruan, Jingqing and Du, Yali and Xiong, Xuantang and Xing, Dengpeng and Li, Xiyun and Meng, Linghui and Zhang, Haifeng and Wang, Jun and Xu, Bo},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1128–1136},
  title     = {GCS: Graph-based coordination strategy for multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535976},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Testing requirements via user and system stories in agent
systems. <em>AAMAS</em>, 1119–1127. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Agile software development is a popular and widely adopted practice due to its flexible and iterative nature that facilitates rapid prototyping. Recent work presented an agile approach to capturing requirements in agent systems via user and system stories. User and system stories present the requirements from the user and system perspective, respectively. Each story contains a set of acceptance criteria, which are a set of statements that identify the conditions under which the system behaviour can be accepted by the users or stakeholders. In this paper, we present a novel approach to testing the requirements that are specified via User and System stories in an agent system. We do this by developing a systematic approach to validating the execution traces output by the system against the specified acceptance criteria for each story. The approach identifies acceptance criteria that are met successfully in execution and those that fail. We present a fault model that categorizes the failures providing insight to the developers to address the failed cases. We classify three kinds of faults for a given acceptance criterion: (a) the trigger condition is never met; (b) when the trigger occurs the preconditions are not met; or (c) the trigger and preconditions are met but the resulting actions are not as expected. The motivating application of our work, which is also the test-bed for evaluation, is an agent-based simulation application for modelling the behaviours of civilians in a bushfire emergency scenario that is used in practice. We show our approach is able to successfully test and uncover requirements that were not met in this application.},
  archive   = {C_AAMAS},
  author    = {Rodriguez, Sebastian and Thangarajah, John and Winikoff, Michael and Singh, Dhirendra},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1119–1127},
  title     = {Testing requirements via user and system stories in agent systems},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535975},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pareto conditioned networks. <em>AAMAS</em>, 1110–1118. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In multi-objective optimization, learning all the policies that reach Pareto-efficient solutions is an expensive process. The set of optimal policies can grow exponentially with the number of objectives, and recovering all solutions requires an exhaustive exploration of the entire state space. We propose Pareto Conditioned Networks (PCN), a method that uses a single neural network to encompass all non-dominated policies. PCN associates every past transition with its episode&#39;s return. It trains the network such that, when conditioned on this same return, it should reenact said transition. In doing so we transform the optimization problem into a classification problem. We recover a concrete policy by conditioning the network on the desired Pareto-efficient solution. Our method is stable as it learns in a supervised fashion, thus avoiding moving target issues. Moreover, by using a single network, PCN scales efficiently with the number of objectives. Finally, it makes minimal assumptions on the shape of the Pareto front, which makes it suitable to a wider range of problems than previous state-of-the-art multi-objective reinforcement learning algorithms.},
  archive   = {C_AAMAS},
  author    = {Reymond, Mathieu and Bargiacchi, Eugenio and Now\&#39;{e}, Ann},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1110–1118},
  title     = {Pareto conditioned networks},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535974},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automated configuration and usage of strategy portfolios for
mixed-motive bargaining. <em>AAMAS</em>, 1101–1109. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bargaining can be used to resolve mixed-motive games in multi-agent systems. Although there is an abundance of negotiation strategies implemented in automated negotiating agents, most agents are based on single fixed strategies, while it is acknowledged that there is no single best-performing strategy for all negotiation settings.In this paper, we focus on bargaining settings where opponents are repeatedly encountered, but the bargaining problems change. We introduce a novel method that automatically creates and deploys a portfolio of complementary negotiation strategies using a training set and optimise pay-off in never-before-seen bargaining settings through per-setting strategy selection. Our method relies on the following contributions. We introduce a feature representation that captures characteristics for both the opponent and the bargaining problem. We model the behaviour of an opponent during a negotiation based on its actions, which is indicative of its negotiation strategy, in order to be more effective in future encounters.Our combination of feature-based methods generalises to new negotiation settings, as in practice, over time, it selects effective counter strategies in future encounters. Our approach is tested in an ANAC-like tournament, and we show that we are capable of winning such a tournament with a (5.6\%) increase in pay-off compared to the runner-up agent.},
  archive   = {C_AAMAS},
  author    = {Renting, Bram M. and Hoos, Holger H. and Jonker, Catholijn M.},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1101–1109},
  title     = {Automated configuration and usage of strategy portfolios for mixed-motive bargaining},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535973},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Revenue and user traffic maximization in mobile short-video
advertising. <em>AAMAS</em>, 1092–1100. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A new mobile attention economy has emerged with the explosive growth of short-video apps such as TikTok. In this internet market, three types of agents interact with each other: the platform, influencers, and advertisers. A short-video platform encourages its influencers to attract users by creating appealing content through short-form videos and allows advertisers to display their ads in short-form videos. There are two options for the advertisers: one is to bid for platform advert slots in a similar way to search engine auctions; the other is to pay an influencer to make engaging short videos and promote them through the influencer&#39;s channel. The second option will generate a higher conversion ratio if advertisers choose the right influencers whose followers match their target market. Although displaying influencer ads will generate less revenue, it is more engaging than platform ads, which is better for maintaining user traffic. Therefore, it is crucial for a platform to balance these factors by establishing a sustainable business agreement with its influencers and advertisers. In this paper, we develop a two-stage solution for a platform to maximize short-term revenue and long-term user traffic maintenance. In the first stage, we estimate the impact of user traffic generated by displaying influencer ads and characterize the user traffic the platform should allocate to influencers for overall revenue maximization. In the second stage, we devise an optimal (1-1/e)-competitive algorithm for ad slot allocation. To complement this analysis, we examine the ratio of the revenue generated by our online algorithm to the optimal offline revenue. Our simulation results show that this ratio is 0.94 on average, which is much higher than (1-1/e) and outperforms four baseline algorithms.},
  archive   = {C_AAMAS},
  author    = {Ran, Dezhi and Zheng, Weiqiang and Li, Yunqi and Bian, Kaigui and Zhang, Jie and Deng, Xiaotie},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1092–1100},
  title     = {Revenue and user traffic maximization in mobile short-video advertising},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535972},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluating the role of interactivity on improving
transparency in autonomous agents. <em>AAMAS</em>, 1083–1091. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous agents are increasingly being deployed amongst human end-users. Yet, human users often have little knowledge of how these agents work or what they will do next. This lack of transparency has already resulted in unintended consequences during AI use: a concerning trend which is projected to increase with the proliferation of autonomous agents. To curb this trend and ensure safe use of AI, assisting users in establishing an accurate understanding of agents that they work with is essential. In this work, we present AI teacher, a user-centered Explainable AI framework to address this need for autonomous agents that follow a Markovian policy. Our framework first computes salient instructions of agent behavior by estimating a user&#39;s mental model and utilizing algorithms for sequential decision-making. Next, in contrast to existing solutions, these instructions are presented interactively to the end-users, thereby enabling a personalized approach to improving AI transparency. We evaluate our framework, with emphasis on its interactive features, through experiments with human participants. The experiment results suggest that, relative to non-interactive approaches, interactive teaching can both reduce the amount of time it takes for humans to create accurate mental models of these agents and is subjectively preferred by human users.},
  archive   = {C_AAMAS},
  author    = {Qian, Peizhu and Unhelkar, Vaibhav},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1083–1091},
  title     = {Evaluating the role of interactivity on improving transparency in autonomous agents},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535971},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning heuristics for combinatorial assignment by
optimally solving subproblems. <em>AAMAS</em>, 1074–1082. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hand-crafting accurate heuristics for optimization problems is often costly due to requiring expert knowledge and time-consuming parameter tuning. Automating this procedure using machine learning has in recent years shown great promise. However, a large number of important problem classes remain unexplored. This paper investigates one such class by exploring learning-based methods for generating heuristics to perform value-maximizing combinatorial assignment (the partitioning of elements among alternatives). In more detail, we use machine learning leveraged by generating and optimally solving subproblems to produce heuristics that can, for example, be used with search algorithms to find feasible solutions of higher quality more quickly. Our results show that our learned heuristics outperform the state of the art in several benchmarks.},
  archive   = {C_AAMAS},
  author    = {Pr\&quot;{a}ntare, Fredrik and Appelgren, Herman and Tiger, Mattias and Bergstr\&quot;{o}m, David and Heintz, Fredrik},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1074–1082},
  title     = {Learning heuristics for combinatorial assignment by optimally solving subproblems},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535970},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Trajectory coordination based on distributed constraint
optimization techniques in unmanned air traffic management.
<em>AAMAS</em>, 1065–1073. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we explore a multiagent-based deconfliction strategy in Unmanned Air Traffic Management (UTM). We equip autonomous unmanned aerial vehicles (UAV) with decision capabilities to update their trajectories (4D contracts) when facing unpredictable events or when priority trajectories are added to the airspace. Current initiatives for UTM envision a multi-layered management system where the lowest one is composed of either autonomous UAV or human-operated one, that may have to react to incidents and emergencies, while sticking to the contracted 4D trajectories. We propose an agent-based approach, where UAVs, being aware of close potential conflicts provided by the UTM communication layer, can coordinate directly, without central UTM control, to update trajectories and solve resulting conflicts in a decentralized manner. This reduces the access to a central decision bottleneck and permits reactive deconfliction. We propose both uncoordinated and DCOP-based coordinated behaviors, that we experimentally evaluate on dense scenarios within a limited area with numerous 4D contracts, potential incidents and emergency procedures.},
  archive   = {C_AAMAS},
  author    = {Picard, Gauthier},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1065–1073},
  title     = {Trajectory coordination based on distributed constraint optimization techniques in unmanned air traffic management},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535969},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Auction-based and distributed optimization approaches for
scheduling observations in satellite constellations with exclusive orbit
portions. <em>AAMAS</em>, 1056–1064. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate the use of cooperative multi-agent allocation techniques on problems related to Earth observation scenarios with multiple users and satellites. We focus on the problem of coordinating users having reserved exclusive orbit portions and one central planner having several requests that may use some intervals of these exclusives. We define this problem as Earth Observation Satellite Constellation Scheduling Problem (EOSCSP) and map it to a Mixed Integer Linear Program. As to solve EOSCSP, we propose market-based techniques and a distributed problem solving technique based on Distributed Constraint Optimization (DCOP), where agents cooperate to allocate requests without sharing their own schedules. These contributions are experimentally evaluated on randomly generated EOSCSP instances based on real large-scale or highly conflicting observation order books.},
  archive   = {C_AAMAS},
  author    = {Picard, Gauthier},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1056–1064},
  title     = {Auction-based and distributed optimization approaches for scheduling observations in satellite constellations with exclusive orbit portions},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535968},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Emergent cooperation from mutual acknowledgment exchange.
<em>AAMAS</em>, 1047–1055. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Peer incentivization (PI) is a recent approach, where all agents learn to reward or to penalize each other in a distributed fashion which often leads to emergent cooperation. Current PI mechanisms implicitly assume a flawless communication channel in order to exchange rewards. These rewards are directly integrated into the learning process without any chance to respond with feedback. Furthermore, most PI approaches rely on global information which limits scalability and applicability to real-world scenarios, where only local information is accessible. In this paper, we propose Mutual Acknowledgment Token Exchange (MATE), a PI approach defined by a two-phase communication protocol to mutually exchange acknowledgment tokens to shape individual rewards. Each agent evaluates the monotonic improvement of its individual situation in order to accept or reject acknowledgment requests from other agents. MATE is completely decentralized and only requires local communication and information. We evaluate MATE in three social dilemma domains. Our results show that MATE is able to achieve and maintain significantly higher levels of cooperation than previous PI approaches. In addition, we evaluate the robustness of MATE in more realistic scenarios, where agents can defect from the protocol and where communication failures can occur.},
  archive   = {C_AAMAS},
  author    = {Phan, Thomy and Sommer, Felix and Altmann, Philipp and Ritz, Fabian and Belzner, Lenz and Linnhoff-Popien, Claudia},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1047–1055},
  title     = {Emergent cooperation from mutual acknowledgment exchange},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535967},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MORAL: Aligning AI with human norms through multi-objective
reinforced active learning. <em>AAMAS</em>, 1038–1046. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inferring reward functions from demonstrations and pairwise preferences are auspicious approaches for aligning Reinforcement Learning (RL) agents with human intentions. However, state-of-the art methods typically focus on learning a single reward model, thus rendering it difficult to trade off different reward functions from multiple experts. We propose Multi-Objective Reinforced Active Learning (MORAL), a novel method for combining diverse demonstrations of social norms into a Pareto-optimal policy. Through maintaining a distribution over scalarization weights, our approach is able to interactively tune a deep RL agent towards a variety of preferences, while eliminating the need for computing multiple policies. We empirically demonstrate the effectiveness of MORAL in two scenarios, which model a delivery and an emergency task that require an agent to act in the presence of normative conflicts. Overall, we consider our research a step towards multi-objective RL with learned rewards, bridging the gap between current reward learning and machine ethics literature.},
  archive   = {C_AAMAS},
  author    = {Peschl, Markus and Zgonnikov, Arkady and Oliehoek, Frans A. and Siebert, Luciano C.},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1038–1046},
  title     = {MORAL: Aligning AI with human norms through multi-objective reinforced active learning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535966},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scaling mean field games by online mirror descent.
<em>AAMAS</em>, 1028–1037. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the scaling of equilibrium computation in Mean Field Games (MFGs) by using Online Mirror Descent (OMD). We show that continuous-time OMD provably converges to a Nash equilibrium under a natural and well-motivated set of monotonicity assumptions. A thorough experimental investigation on various single and multi-population MFGs shows that OMD outperforms traditional algorithms such as Fictitious Play. We empirically show that OMD scales and converges significantly faster than Fictitious Play by solving, for the first time to our knowledge, examples of MFGs with hundreds of billions states.},
  archive   = {C_AAMAS},
  author    = {P\&#39;{e}rolat, Julien and Perrin, Sarah and Elie, Romuald and Lauri\`{e}re, Mathieu and Piliouras, Georgios and Geist, Matthieu and Tuyls, Karl and Pietquin, Olivier},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1028–1037},
  title     = {Scaling mean field games by online mirror descent},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535965},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BOID*: Autonomous goal deliberation through abduction.
<em>AAMAS</em>, 1019–1027. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The original BOID [5] is a cognitive architecture that unifies Belief, Obligation, Intention and Desire rules to calculate which actions should an agent undertake next. In the current paper, we adapt the original BOID with an aim to model autonomous agency. The new BOID* architecture is able to capture anticipation that we believe to be one of the hallmarks of autonomous agency. We focus on developing algorithms for anticipatory reasoning through a new BOID* goal deliberation component. The key method that BOID* introduces is abductive reasoning as a way to represent motivational attitudes, such as desires and obligations. As a result of deliberation via abduction, BOID* specifies intention revision procedures that connect motivational and informational attitudes. The BOID* is a part of the project to build autonomous AI models that make explicit the reasoning behind adopting future goals, prioritizing selected goals and forming intentions.},
  archive   = {C_AAMAS},
  author    = {Pand\v{z}i\&#39;{c}, Stipe and Broersen, Jan and Aarts, Henk},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1019–1027},
  title     = {BOID*: Autonomous goal deliberation through abduction},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535964},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Characterizing attacks on deep reinforcement learning.
<em>AAMAS</em>, 1010–1018. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent studies show that Deep Reinforcement Learning (DRL) models are vulnerable to adversarial attacks, which attack DRL models by adding small perturbations to the observations. However, some attacks assume full availability of the victim model, and some require a huge amount of computation, making them less feasible for real world applications. In this work, we make further explorations of the vulnerabilities of DRL by studying other aspects of attacks on DRL using realistic and efficient attacks. First, we adapt and propose efficient black-box attacks when we do not have access to DRL model parameters. Second, to address the high computational demands of existing attacks, we introduce efficient online sequential attacks that exploit temporal consistency across consecutive steps. Third, we explore the possibility of an attacker perturbing other aspects in the DRL setting, such as the environment dynamics. Finally, to account for imperfections in how an attacker would inject perturbations in the physical world, we devise a method for generating a robust physical perturbations to be printed. The attack is evaluated on a real-world robot under various conditions. We conduct extensive experiments both in simulation such as Atari games, robotics and autonomous driving, and on real-world robotics, to compare the effectiveness of the proposed attacks with baseline approaches. To the best of our knowledge, we are the first to apply adversarial attacks on DRL systems to physical robots.},
  archive   = {C_AAMAS},
  author    = {Pan, Xinlei and Xiao, Chaowei and He, Warren and Yang, Shuang and Peng, Jian and Sun, Mingjie and Liu, Mingyan and Li, Bo and Song, Dawn},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1010–1018},
  title     = {Characterizing attacks on deep reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535963},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Networked restless multi-armed bandits for mobile
interventions. <em>AAMAS</em>, 1001–1009. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motivated by a broad class of mobile intervention problems, we propose and study restless multi-armed bandits (RMABs) with network effects. In our model, arms are partially recharging and connected through a graph, so that pulling one arm also improves the state of neighboring arms, significantly extending the previously studied setting of fully recharging bandits with no network effects. In mobile interventions, network effects may arise due to regular population movements (such as commuting between home and work). We show that network effects in RMABs induce strong reward coupling that is not accounted for by existing solution methods. We propose a new solution approach for networked RMABs, exploiting concavity properties which arise under natural assumptions on the structure of intervention effects. We provide sufficient conditions for optimality of our approach in idealized settings and demonstrate that it empirically outperforms state-of-the art baselines in three mobile intervention domains using real-world graphs.},
  archive   = {C_AAMAS},
  author    = {Ou, Han-Ching and Siebenbrunner, Christoph and Killian, Jackson and Brooks, Meredith B. and Kempe, David and Vorobeychik, Yevgeniy and Tambe, Milind},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1001–1009},
  title     = {Networked restless multi-armed bandits for mobile interventions},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535962},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Factorial agent markov model: Modeling other agents’
behavior in presence of dynamic latent decision factors. <em>AAMAS</em>,
982–990. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous agents operating in the real world often need to interact with other agents to accomplish their tasks. For such agents, the ability to model behavior of other agents -- both human and artificial -- without complete knowledge of their decision factors is essential. Towards realizing this ability, we present Factorial Agent Markov Model (FAMM), a model to represent behavior of other agents performing sequential tasks. In contrast with most existing models, FAMM allows for behavior of other agents to depend on multiple, time-varying latent decision factors and does not assume rationality. To enable learning of famm parameters by observing behavior of other agents, we provide a set of variational inference algorithms for the unsupervised, semi-supervised, and supervised settings. These Bayesian learning algorithms for the FAMM enable agents to model other agents using execution traces and domain-specific priors. We demonstrate the utility of FAMM and corresponding learning algorithms using three synthetic domains and benchmark them against existing algorithms for modeling agent behavior. Our numerical experiments demonstrate that, despite the presence of multiple and time-varying latent states, our approach is capable of learning predictive models of other agents with semi-supervision.},
  archive   = {C_AAMAS},
  author    = {Orlov-Savko, Liubove and Jain, Abhinav and Gremillion, Gregory M. and Neubauer, Catherine E. and Canady, Jonroy D. and Unhelkar, Vaibhav},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {982–990},
  title     = {Factorial agent markov model: Modeling other agents&#39; behavior in presence of dynamic latent decision factors},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535960},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CTRMs: Learning to construct cooperative timed roadmaps for
multi-agent path planning in continuous spaces. <em>AAMAS</em>, 972–981.
(<a href="https://dl.acm.org/doi/10.5555/3535850.3535959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent path planning (MAPP) in continuous spaces is a challenging problem with significant practical importance. One promising approach is to first construct graphs approximating the spaces, called roadmaps, and then apply multi-agent pathfinding (MAPF) algorithms to derive a set of conflict-free paths. While conventional studies have utilized roadmap construction methods developed for single-agent planning, it remains largely unexplored how we can construct roadmaps that work effectively for multiple agents. To this end, we propose a novel concept of roadmaps called cooperative timed roadmaps (CTRMs). CTRMs enable each agent to focus on its important locations around potential solution paths in a way that considers the behavior of other agents to avoid inter-agent collisions (i.e., &quot;cooperative&quot;), while being augmented in the time direction to make it easy to derive a &quot;timed&quot; solution path. To construct CTRMs, we developed a machine-learning approach that learns a generative model from a collection of relevant problem instances and plausible solutions and then uses the learned model to sample the vertices of CTRMs for new, previously unseen problem instances. Our empirical evaluation revealed that the use of CTRMs significantly reduced the planning effort with acceptable overheads while maintaining a success rate and solution quality comparable to conventional roadmap construction approaches.},
  archive   = {C_AAMAS},
  author    = {Okumura, Keisuke and Yonetani, Ryo and Nishimura, Mai and Kanezaki, Asako},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {972–981},
  title     = {CTRMs: Learning to construct cooperative timed roadmaps for multi-agent path planning in continuous spaces},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535959},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to transfer role assignment across team sizes.
<em>AAMAS</em>, 963–971. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent reinforcement learning holds the key for solving complex tasks that demand the coordination of learning agents. However, strong coordination often leads to expensive exploration over the exponentially large state-action space. A powerful approach is to decompose team works into roles, which are ideally assigned to agents with the relevant skills. Training agents to adaptively choose and play emerging roles in a team thus allows the team to scale to complex tasks and quickly adapt to changing environments. These promises, however, have not been fully realised by current role-based multi-agent reinforcement learning methods as they assume either a pre-defined role structure or a fixed team size. We propose a framework to learn role assignment and transfer across team sizes. In particular, we train a role assignment network for small teams by demonstration and transfer the network to larger teams, which continue to learn through interaction with the environment. We demonstrate that re-using the role-based credit assignment structure can foster the learning process of larger reinforcement learning teams to achieve tasks requiring different roles. Our proposal outperforms competing techniques in enriched role-enforcing Prey-Predator games and in new scenarios in the StarCraft II Micro-Management benchmark.},
  archive   = {C_AAMAS},
  author    = {Nguyen, Dung and Nguyen, Phuoc and Venkatesh, Svetha and Tran, Truyen},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {963–971},
  title     = {Learning to transfer role assignment across team sizes},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535958},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning theory of mind via dynamic traits attribution.
<em>AAMAS</em>, 954–962. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Machine learning of Theory of Mind (ToM) is essential to build social agents that co-live with humans and other agents. This capacity, once acquired, will help machines infer the mental states of others from observed contextual action trajectories, enabling future prediction of goals, intention, actions and successor representations. The underlying mechanism for such a prediction remains unclear, however. Inspired by the observation that humans often infer the character traits of others, then use it to explain behaviour, we propose a new neural ToM architecture that learns to generate a latent trait vector of an actor from the past trajectories. This trait vector then multiplicatively modulates the prediction mechanism via a &#39;fast weights&#39; scheme in the prediction neural network, which reads the current context and predicts the behaviour. We empirically show that the fast weights provide a good inductive bias to model the character traits of agents and hence improves mindreading ability. On the indirect assessment of false-belief understanding, the new ToM model enables more efficient helping behaviours.},
  archive   = {C_AAMAS},
  author    = {Nguyen, Dung and Nguyen, Phuoc and Le, Hung and Do, Kien and Venkatesh, Svetha and Tran, Truyen},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {954–962},
  title     = {Learning theory of mind via dynamic traits attribution},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535957},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep reinforcement learning for active wake control.
<em>AAMAS</em>, 944–953. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Wind farms suffer from so-called wake effects: when turbines are located in the wind shadows of other turbines, their power output is substantially reduced. These losses can be partially mitigated via actively changing the yaw from the individually optimal direction. Most existing wake control techniques have two major limitations: they use simplified wake models to optimize the control strategy, and they assume that the atmospheric conditions remain stable. In this paper, we address these limitations by applying reinforcement learning (RL). RL forgoes the wake model entirely and learns an optimal control strategy based on the observed atmospheric conditions and a reward signal, in this case the power output of the farm. It also accounts for random transitions in the observations, such as turbulent fluctuations in the wind. To evaluate RL for active wake control, we provide a simulator based on the state-of-the-art FLORIS model in the OpenAI gym format. Next, we propose three different state-action representations of the active wake control problem and investigate their effect on the performance of RL-based wake control. Finally, we compare RL to a state-of-the-art wake control strategy based on FLORIS and show that RL is less sensitive to changes in unobservable data.},
  archive   = {C_AAMAS},
  author    = {Neustroev, Grigory and Andringa, Sytze P. E. and Verzijlbergh, Remco A. and De Weerdt, Mathijs M.},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {944–953},
  title     = {Deep reinforcement learning for active wake control},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535956},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A graph-based algorithm for the automated justification of
collective decisions. <em>AAMAS</em>, 935–943. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We develop an algorithm for the axiomatic justification problem in social choice that is sufficiently efficient to be applicable in decision making scenarios of real practical interest. Given a profile of individual preferences, a suggested election outcome, and a corpus of axioms encoding fundamental normative principles of electoral fairness, solving this justification problem involves computing a minimal set of instances of some of the axioms in the corpus that together rule out any outcome that is different from the one we want to justify. Our approach combines the use of state-of-the-art tools for computing minimally unsatisfiable sets of constraints with a graph-search algorithm. The latter searches the graph induced by the set of all axiom instances in an incremental manner and relies on a number of heuristics to further improve performance.},
  archive   = {C_AAMAS},
  author    = {Nardi, Oliviero and Boixel, Arthur and Endriss, Ulle},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {935–943},
  title     = {A graph-based algorithm for the automated justification of collective decisions},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535955},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning equilibria in mean-field games: Introducing
mean-field PSRO. <em>AAMAS</em>, 926–934. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in multiagent learning have seen the introduction of a family of algorithms that revolve around the population-based training method PSRO, showing convergence to Nash, correlated and coarse correlated equilibria. Notably, when the number of agents increases, learning best-responses becomes exponentially more difficult, and as such hampers PSRO training methods. The field of mean-field games provides an asymptotic solution to this problem when the considered games are anonymous-symmetric. Unfortunately, the mean-field approximation introduces non-linearities which prevent a straightforward adaptation of PSRO. Building upon optimization and adversarial regret minimization, this paper sidesteps this issue and introduces mean-field PSRO, an adaptation of PSRO which learns Nash, coarse correlated and correlated equilibria in mean-field games. The key is to replace the exact distribution computation step by newly-defined mean-field no-adversarial-regret learners, or by black-box optimization. We compare the asymptotic complexity of the approach to standard PSRO, greatly improve empirical bandit convergence speed by compressing temporal mixture weights, and ensure it is theoretically robust to payoff noise. Finally, we illustrate the speed and accuracy of mean-field PSRO on several mean-field games, demonstrating convergence to strong and weak equilibria.},
  archive   = {C_AAMAS},
  author    = {Muller, Paul and Rowland, Mark and Elie, Romuald and Piliouras, Georgios and Perolat, Julien and Lauriere, Mathieu and Marinier, Raphael and Pietquin, Olivier and Tuyls, Karl},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {926–934},
  title     = {Learning equilibria in mean-field games: Introducing mean-field PSRO},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535954},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Preference-based goal refinement in BDI agents.
<em>AAMAS</em>, 917–925. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Computational agents based on the BDI framework typically rely on abstract plans and plan refinement to reach a degree of autonomy in dynamic environments: agents are provided with the ability to selecthow-to achieve their goals by choosing from a set of options. In this work we focus on a related, yet under-studied feature:abstract goals. These constructs refer to the ability of agents to adopt goals that are not fully grounded at the moment of invocation, refining them only when and where needed: the ability to selectwhat-to (concretely) achieve at run-time. We present a preference-based approach to goal refinement, defining preferences based on extendedCeteris Paribus Networks (CP-Nets) for an AgentSpeak(L)-like agent programming language, and mapping the established CP-Nets logic and algorithms to guide the goal refinement step. As a technical contribution, we present an implementation of this method that solely uses a Prolog-like inference engine of the agent&#39;s belief-base to reason about preferences, thus minimally affecting the decision-making mechanisms hard-coded in the agent framework.},
  archive   = {C_AAMAS},
  author    = {Mohajeriparizi, Mostafa and Sileno, Giovanni and van Engers, Tom},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {917–925},
  title     = {Preference-based goal refinement in BDI agents},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535953},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cooperation and learning dynamics under risk diversity and
financial incentives. <em>AAMAS</em>, 908–916. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we investigate the role of risk diversity in groups of agents learning to play collective risk dilemmas (CRDs). We show that risk diversity poses new challenges to cooperation that are not observed in homogeneous groups. While increasing average risk contributes, in general, for agents to cooperate with higher probability, increasing risk diversity significantly reduces a population&#39;s ability to achieve a collective target. Risk diversity leads to asymmetrical changes in agents policies --- i.e. the increase in contributions from individuals at high risk is unable to compensate for the decrease in contributions from individuals at low risk --- which reduces the total contributions in a population and overall social welfare. At the same time, risk diversity offers novel opportunities to design financial incentives, which, as we show, can improve cooperation, target achievement and global welfare beyond the levels obtained in the absence of diversity. Our results highlight the need to align risk perceptions among agents and implement diversity-based incentive policies in order to improve collectives&#39; abilities to avoid future catastrophic events.},
  archive   = {C_AAMAS},
  author    = {Merhej, Ramona and Santos, Fernando P. and Melo, Francisco S. and Chetouani, Mohamed and Santos, Francisco C.},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {908–916},
  title     = {Cooperation and learning dynamics under risk diversity and financial incentives},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535952},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Warmth and competence in human-agent cooperation.
<em>AAMAS</em>, 898–907. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Interaction and cooperation with humans are overarching aspirations of artificial intelligence (AI) research. Recent studies demonstrate that AI agents trained with deep reinforcement learning are capable of collaborating with humans. These studies primarily evaluate human compatibility through &quot;objective&quot; metrics such as task performance, obscuring potential variation in the levels of trust and subjective preference that different agents garner. To better understand the factors shaping subjective preferences in human-agent cooperation, we train deep reinforcement learning agents in Coins, a two-player social dilemma. We recruit participants for a human-agent cooperation study and measure their impressions of the agents they encounter. Participants&#39; perceptions of warmth and competence predict their stated preferences for different agents, above and beyond objective performance metrics. Drawing inspiration from social science and biology research, we subsequently implement a new &quot;partner choice&quot; framework to elicit revealed preferences: after playing an episode with an agent, participants are asked whether they would like to play the next round with the same agent or to play alone. As with stated preferences, social perception better predicts participants&#39; revealed preferences than does objective performance. Given these results, we recommend human-agent interaction researchers routinely incorporate the measurement of social perception and subjective preferences into their studies.},
  archive   = {C_AAMAS},
  author    = {McKee, Kevin R. and Bai, Xuechunzi and Fiske, Susan T.},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {898–907},
  title     = {Warmth and competence in human-agent cooperation},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535951},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CAPS: Comprehensible abstract policy summaries for
explaining reinforcement learning agents. <em>AAMAS</em>, 889–897. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As reinforcement learning (RL) continues to improve and be applied in situations alongside humans, the need to explain the learned behaviors of RL agents to end-users becomes more important. Strategies for explaining the reasoning behind an agent&#39;s policy, called policy-level explanations, can lead to important insights about both the task and the agent&#39;s behaviors. Following this line of research, in this work, we propose a novel approach, named as CAPS, that summarizes an agent&#39;s policy in the form of a directed graph with natural language descriptions. A decision tree based clustering method is utilized to abstract the state space of the task into fewer, condensed states which makes the policy graphs more digestible to end-users. This abstraction allows the users to control the size of the policy graph to achieve their desired balance between comprehensibility and accuracy. In addition, we develop a heuristic optimization method to find the most explainable graph policy and present it to the users. Finally, we use the user-defined predicates to enrich the abstract states with semantic meaning. We test our approach on five RL tasks, using both deterministic and stochastic policies, and show that our method is: (1) agnostic to the algorithms used to train the policies, and (2) comparable in accuracy and superior in explanation capabilities to existing baselines. Especially, when provided with our explanation graph, end-users are able to accurately interpret policies of trained RL agents 80\% of the time, compared to 10\% when provided with the next best baseline. We make our code and datasets available to ensure the reproducibility of our research findings: https://github.com/mccajl/CAPS},
  archive   = {C_AAMAS},
  author    = {McCalmon, Joe and Le, Thai and Alqahtani, Sarra and Lee, Dongwon},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {889–897},
  title     = {CAPS: Comprehensible abstract policy summaries for explaining reinforcement learning agents},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535950},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient algorithms for finite horizon and streaming
restless multi-armed bandit problems. <em>AAMAS</em>, 880–888. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose Streaming Bandits, a Restless Multi-Armed Bandit (RMAB) framework in which heterogeneous arms may arrive and leave the system after staying on for a finite lifetime. Streaming Bandits naturally capture the health-intervention planning problem, where health workers must manage the health outcomes of a patient cohort while new patients join and existing patients leave the cohort each day. Our contributions are as follows: (1) We derive conditions under which our problem satisfies indexability, a pre-condition that guarantees the existence and asymptotic optimality of the Whittle Index solution for RMABs. We establish the conditions using a polytime reduction of the Streaming Bandit setup to regular RMABs. (2) We further prove a phenomenon that we call index decay - whereby the Whittle index values are low for short residual lifetimes - driving the intuition underpinning our algorithm. (3) We propose a novel and efficient algorithm to compute the index-based solution for Streaming Bandits. Unlike previous methods, our algorithm does not rely on solving the costly finite horizon problem on each arm of the RMAB, thereby lowering the computational complexity compared to existing methods. (4) Finally, we evaluate our approach via simulations run on real-world data sets from a tuberculosis patient monitoring task and an intervention planning task for improving maternal healthcare, in addition to other synthetic domains. Across the board, our algorithm achieves a 2-orders-of-magnitude speed-up over existing methods while maintaining the same solution quality. The full paper is available at: https://arxiv.org/pdf/2103.04730.pdf},
  archive   = {C_AAMAS},
  author    = {Mate, Aditya S. and Biswas, Arpita and Siebenbrunner, Christoph and Ghosh, Susobhan and Tambe, Milind},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {880–888},
  title     = {Efficient algorithms for finite horizon and streaming restless multi-armed bandit problems},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535949},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). On parameterized complexity of binary networked public
goods game. <em>AAMAS</em>, 871–879. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the Binary Networked Public Goods (BNPG for short) game, every player needs to decide if she participates in a public project whose utility is shared equally by the community. We study the problem of deciding if there exists a pure strategy Nash equilibrium (PSNE) in such games. The problem is already known to be NP-complete. This casts doubt on predictive power of PSNE in BNPG games. We provide fine-grained analysis of this problem under the lens of parameterized complexity theory. We consider various natural graph parameters and show W[1]-hardness, XP, and FPT results. Hence, our work significantly improves our understanding of BNPG games where PSNE serves as a reliable solution concept. We finally prove that some graph classes, for example path, cycle, bi-clique, and complete graph, always have a PSNE if the utility function of the players are same.},
  archive   = {C_AAMAS},
  author    = {Maiti, Arnab and Dey, Palash},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {871–879},
  title     = {On parameterized complexity of binary networked public goods game},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535948},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Coalition formation games and social ranking solutions.
<em>AAMAS</em>, 862–870. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A social ranking (solution) over a set N is defined as a map assigning to each coalitional relation (i.e. a ranking over subsets of N) another ranking over the single elements in N. Differently, coalition formation situations, and, in particular, hedonic games, mainly focus on partitions of the set N into disjoint coalitions, which are in general referred to as coalition structures. A coalition structure may be stable according to various notions of stability and the objective is to understand under which conditions a coalition structure is stable.In this paper we merge the framework of coalition formation with the one of social rankings to keep into account the effect of hierarchies within coalitions on the stability of coalition structures. We consider alternative classes of coalition formation games where the preferences of the players over coalitions are induced by a social ranking. More precisely, players compare coalition structures keeping into account both the relative ranking of coalitions to which they belong (according to a coalitional relation) and their position in the social ranking within each coalition. Constructive characterizations of the set of stable coalition structures are provided for alternative classes of hedonic games, together with an impossibility result on the existence of stable coalition structures for (non-hedonic) coalition formation situations.},
  archive   = {C_AAMAS},
  author    = {Lucchetti, Roberto and Moretti, Stefano and Rea, Tommaso},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {862–870},
  title     = {Coalition formation games and social ranking solutions},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535947},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Any-play: An intrinsic augmentation for zero-shot
coordination. <em>AAMAS</em>, 853–861. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cooperative artificial intelligence with human or superhuman proficiency in collaborative tasks stands at the frontier of machine learning research. Prior work has tended to evaluate cooperative AI performance under the restrictive paradigms of self-play (teams composed of agents trained together) and cross-play (teams of agents trained independently but using the same algorithm). Recent work has indicated that AI optimized for these narrow settings may make for undesirable collaborators in the real-world. We formalize an alternative criteria for evaluating cooperative AI, referred to as inter-algorithm cross-play, where agents are evaluated on teaming performance with all other agents within an experiment pool with no assumption of algorithmic similarities between agents. We show that existing state-of-the-art cooperative AI algorithms, such as Other-Play and Off-Belief Learning, under-perform in this paradigm. We propose the Any-Play learning augmentation---a multi-agent extension of diversity-based intrinsic rewards for zero-shot coordination (ZSC)---for generalizing self-play-based algorithms to the inter-algorithm cross-play setting. We apply the Any-Play learning augmentation to the Simplified Action Decoder (SAD) and demonstrate state-of-the-art performance in the collaborative card game Hanabi.},
  archive   = {C_AAMAS},
  author    = {Lucas, Keane and Allen, Ross E.},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {853–861},
  title     = {Any-play: An intrinsic augmentation for zero-shot coordination},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535946},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lyapunov exponents for diversity in differentiable games.
<em>AAMAS</em>, 842–852. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ridge Rider (RR) is an algorithm for finding diverse solutions to optimization problems by following eigenvectors of the Hessian (&quot;ridges&quot;). RR is designed for conservative gradient systems (i.e., settings involving a single loss function), where it branches at saddles - easy-to-find bifurcation points. We generalize this idea to non-conservative, multi-agent gradient systems by proposing a method - denoted Generalized Ridge Rider (GRR) - for finding arbitrary bifurcation points. We give theoretical motivation for our method by leveraging machinery from the field of dynamical systems. We construct novel toy problems where we can visualize new phenomena while giving insight into high-dimensional problems of interest. Finally, we empirically evaluate our method by finding diverse solutions in the iterated prisoners&#39; dilemma and relevant machine learning problems including generative adversarial networks.},
  archive   = {C_AAMAS},
  author    = {Lorraine, Jonathan and Vicol, Paul and Parker-Holder, Jack and Kachman, Tal and Metz, Luke and Foerster, Jakob},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {842–852},
  title     = {Lyapunov exponents for diversity in differentiable games},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535945},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Logical theories of collective attitudes and the belief base
perspective. <em>AAMAS</em>, 833–841. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present two logics of collective belief with a semantics exploiting the notion of belief base. The semantics distinguishes explicit from implicit belief: an agent&#39;s belief of explicit type is a piece of information contained in the agent&#39;s belief base, while a belief of implicit type corresponds to a piece of information that is derivable from the agent&#39;s belief base. The first part of the paper is devoted to the logic of implicit common belief, while the second presents the logic of explicit common belief. Implicit common belief is defined as a mutual belief of any order. This leads to the usual fixpoint construction of common belief. Explicit common belief is the collective counterpart of explicit individual belief and has a public nature. It moreover implies implicit common belief. We study axiomatic aspects of our logics as well as complexity of satisfiability checking. We show that, while the satisfiability checking problem is EXPTIME-hard for the logic of implicit common belief, it is in PSPACE for the logic of explicit common belief. This makes the latter logic a natural candidate for reasoning about collective attitudes in multi-agent scenarios and applications. We also study a dynamic extension of the logic of explicit common belief in which private and public forms of information dynamics can be modeled.},
  archive   = {C_AAMAS},
  author    = {Lorini, Emiliano and Rapion, \&#39;{E}loan},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {833–841},
  title     = {Logical theories of collective attitudes and the belief base perspective},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535944},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The spoofing resistance of frequent call markets.
<em>AAMAS</em>, 825–832. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the effects of spoofing attacks on frequent call markets (FCMs). Spoofing---a strategic manipulation to mislead market participants by creating spurious limit orders---could harm the market efficiency and has been declared illegal in many countries. However, this practice is still very common, which inspired extensive research on measuring, detecting and curbing such manipulation in the popular market model of continuous double auctions (CDAs). In this paper, we extend this research to frequent call markets and use agent-based modelling to simulate spoofing in this context. Specifically, we apply empirical game-theoretic analysis (EGTA) to compute equilibria of agent-based markets, and demonstrate that while spoofing may be profitable in both market models, it has less impact on FCMs as opposed to CDAs. Finally, we explore several FCM mechanism designs to help to curb this type of market manipulation even further.},
  archive   = {C_AAMAS},
  author    = {Liu, Buhong and Polukarov, Maria and Ventre, Carmine and Li, Lingbo and Kanthan, Leslie and Wu, Fan and Basios, Michail},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {825–832},
  title     = {The spoofing resistance of frequent call markets},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535943},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Residual entropy-based graph generative algorithms.
<em>AAMAS</em>, 816–824. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Classification and clustering are crucial tasks that recognize the identities and the communities of nodes in a graph. Several methods have been proposed to reduce the accuracy of node classification and clustering through graph neural networks (GNN). Existing defense methods usually modify the model architecture and adopt countermeasure training to enhance the robustness of the node classification and clustering. However, these defense methods are model-oriented and not robust.To alleviate the problem, this paper first proposes a robust node classification metric based on residual entropy. More concretely, we prove that maximizing the residual entropy helps to improve the robustness of the classification accuracy. We them propose two graph generative algorithms to resist against two kinds of GNN-based attacks, the untargeted and the targeted attacks. Finally, experimental analysis show that the proposed algorithms outperform the existing defense works under five classic datasets.},
  archive   = {C_AAMAS},
  author    = {Liu, Wencong and Liu, Jiamou and Zhang, Zijian and Liu, Yiwei and Zhu, Liehuang},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {816–824},
  title     = {Residual entropy-based graph generative algorithms},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535942},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Equilibrium computation for knockout tournaments played by
groups. <em>AAMAS</em>, 807–815. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In single-elimination knockout tournaments, participants face each other based on a starting seeding and progress to the next rounds by beating their direct opponents. In this paper we initiate the study of coalitional knockout tournaments, which generalise single-elimination knockout tournaments by allowing groups of players, or coalitions, to strategically select one of their members to take part in the tournament, following the starting seeding. We investigate the algorithmic properties of pure strategies Nash equilibria in these games under various setups, i.e., whether or not choices can be made at each round and whether or not tournament progression is important to the group. Despite the more complex tournament structure when compared to single-elimination, we provide (quasi-)polynomial-time algorithms for all cases. Our results can be applied to those tournaments where pre-play selection plays an important role, such as sport events or elections with run-off.},
  archive   = {C_AAMAS},
  author    = {Lisowski, Grzegorz and Ramanujan, M. S. and Turrini, Paolo},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {807–815},
  title     = {Equilibrium computation for knockout tournaments played by groups},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535941},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ASM-PPO: Asynchronous and scalable multi-agent PPO for
cooperative charging. <em>AAMAS</em>, 798–806. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Wireless Rechargeable Sensor Networks (WRSNs) are especially promising in large-area monitoring tasks that are previously impossible to complete by traditional Wireless Sensor Networks (WSNs). Mobile Chargers (MCs) in WRSNs are to cooperatively charge battery drained sensor nodes high efficiently and with a guarantee of sensors survival. Considering the unpredictability and high dynamics of WRSNs during the charging process, Multi-Agent Reinforcement Learning (MARL) is an attractive alternative to schedule the cooperation among MCs. However, most existing MARL methods are based on Decentralized Partially Observable Markov Decision Processes (Dec-POMDP), a general framework to describe decentralized agents making decisions at the same time step. Nevertheless, MCs in WRSNs perform charging asynchronously since the charging time of each sensor node varies. To address the problem of asynchronous behavior, we first formulate an Asynchronous Dec-POMDP (AD-POMDP). We then propose an algorithm called Asynchronous and Scalable Multi-agent Proximal Policy Optimization (ASM-PPO) that allows asynchronous learning and decision-making in AD-POMDP based on two popular multi-agent reinforcement learning methods in Dec-POMDP. Furthermore, ASM-PPO takes advantage of the translation invariance in WRSNs to avoid the huge input space dimensions caused by centralized training. The evaluation results not only indicate that our method achieves much charging efficiency and the longer lifetime of sensor nodes, but also demonstrate that ASM-PPO has advantages in terms of stability and scalability over existing methods.},
  archive   = {C_AAMAS},
  author    = {Liang, Yongheng and Wu, Hejun and Wang, Haitao},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {798–806},
  title     = {ASM-PPO: Asynchronous and scalable multi-agent PPO for cooperative charging},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535940},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deploying vaccine distribution sites for improved
accessibility and equity to support pandemic response. <em>AAMAS</em>,
789–797. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In response to COVID-19, many countries have mandated social distancing and banned large group gatherings in order to slow down the spread of SARS-CoV-2. These social interventions along with vaccines remain the best way forward to reduce the spread of SARS CoV-2. In order to increase vaccine accessibility, states such as Virginia have deployed mobile vaccination centers to distribute vaccines across the state. When choosing where to place these sites, there are two important factors to take into account: accessibility and equity. We formulate a combinatorial problem that captures these factors and then develop efficient algorithms with theoretical guarantees on both of these aspects. Furthermore, we study the inherent hardness of the problem, and demonstrate strong impossibility results. Finally, we run computational experiments on real-world data to show the efficacy of our methods.},
  archive   = {C_AAMAS},
  author    = {Li, George Z. and Li, Ann and Marathe, Madhav and Srinivasan, Aravind and Tsepenekas, Leonidas and Vullikanti, Anil},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {789–797},
  title     = {Deploying vaccine distribution sites for improved accessibility and equity to support pandemic response},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535939},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards pluralistic value alignment: Aggregating value
systems through ℓp-regression. <em>AAMAS</em>, 780–788. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dealing with the challenges of an interconnected globalised world requires to handle plurality. This is no exception when considering value-aligned intelligent systems, since the values to align with should capture this plurality. So far, most literature on value-alignment has just considered a single value system. Thus, this paper advances the state of the art by proposing a method for the aggregation of value systems. By exploiting recent results in the social choice literature, we formalise our aggregation problem as an optimisation problem. We then cast such problem as an ℓp-regression problem. By doing so, we provide a general theoretical framework that allows us to model and solve the above-mentioned problem. Our aggregation method allows us to consider a range of ethical principles, from utilitarian (maximum utility) to egalitarian (maximum fairness). We illustrate the aggregation of value systems by considering real-world data from the European Value Study. Specifically, we show how different consensus value systems can be obtained depending on the ethical principle of choice.},
  archive   = {C_AAMAS},
  author    = {Lera-Leri, Roger and Bistaffa, Filippo and Serramia, Marc and Lopez-Sanchez, Maite and Rodriguez-Aguilar, Juan},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {780–788},
  title     = {Towards pluralistic value alignment: Aggregating value systems through ℓp-regression},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535938},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multimodal analysis of the predictability of hand-gesture
properties. <em>AAMAS</em>, 770–779. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Embodied conversational agents benefit from being able to accompany their speech with gestures. Although many data-driven approaches to gesture generation have been proposed in recent years, it is still unclear whether such systems can consistently generate gestures that convey meaning. We investigate which gesture properties (phase, category, and semantics) can be predicted from speech text and/or audio using contemporary deep learning. In extensive experiments, we show that gesture properties related to gesture meaning (semantics and category) are predictable from text features (time-aligned FastText embeddings) alone, but not from prosodic audio features, while rhythm-related gesture properties (phase) on the other hand can be predicted from audio features better than from text. These results are encouraging as they indicate that it is possible to equip an embodied agent with content-wise meaningful co-speech gestures using a machine-learning model.},
  archive   = {C_AAMAS},
  author    = {Kucherenko, Taras and Nagy, Rajmund and Neff, Michael and Kjellstr\&quot;{o}m, Hedvig and Henter, Gustav Eje},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {770–779},
  title     = {Multimodal analysis of the predictability of hand-gesture properties},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535937},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Equilibria in schelling games: Computational hardness and
robustness. <em>AAMAS</em>, 761–769. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the simplest game-theoretic formulation of Schelling&#39;s model of segregation on graphs, agents of two different types each select their own vertex in a given graph so as to maximize the fraction of agents of their type in their occupied neighborhood. Two ways of modeling agent movement here are either to allow two agents to swap their vertices or to allow an agent to jump to a free vertex.The contributions of this paper are twofold. First, we prove that deciding the existence of a swap-equilibrium and a jump-equilibrium in this simplest model of Schelling games is NP-hard, thereby answering questions left open by Agarwal et al. [AAAI &#39;20] and Elkind et al. [IJCAI &#39;19]. Second, we introduce two measures for the robustness of equilibria in Schelling games in terms of the minimum number of edges or the minimum number of vertices that need to be deleted to make an equilibrium unstable. We prove tight lower and upper bounds on the edge- and vertex-robustness of swap-equilibria in Schelling games on different graph classes.},
  archive   = {C_AAMAS},
  author    = {Kreisel, Luca and Boehmer, Niclas and Froese, Vincent and Niedermeier, Rolf},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {761–769},
  title     = {Equilibria in schelling games: Computational hardness and robustness},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535936},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Disentangling successor features for coordination in
multi-agent reinforcement learning. <em>AAMAS</em>, 751–760. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent reinforcement learning (MARL) is a promising framework for solving complex tasks with many agents. However, a key challenge in MARL is defining private utility functions that ensure coordination when training decentralized agents. This challenge is especially prevalent in unstructured tasks with sparse rewards and many agents. We show that successor features can help address this challenge by disentangling an individual agent&#39;s impact on the global value function from that of all other agents. We use this disentanglement to compactly represent private utilities that support stable training of decentralized agents in unstructured tasks. We implement our approach using a centralized training, decentralized execution architecture and test it in a variety of multi-agent environments. Our results show improved performance and training time relative to existing methods and suggest that disentanglement of successor features offers a promising approach to coordination in MARL.},
  archive   = {C_AAMAS},
  author    = {Kim, Seung Hyun and Van Stralen, Neale and Chowdhary, Girish and Tran, Huy T.},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {751–760},
  title     = {Disentangling successor features for coordination in multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535935},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tactile pose estimation and policy learning for unknown
object manipulation. <em>AAMAS</em>, 742–750. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Object pose estimation methods allow finding locations of objects in unstructured environments. This is a highly desired skill for autonomous robot manipulation as robots need to estimate the precise poses of the objects in order to manipulate them. In this paper, we investigate the problems of tactile pose estimation and manipulation for category-level objects. Our proposed method uses a Bayes filter with a learned tactile observation model and a deterministic motion model. Later, we train policies using deep reinforcement learning where the agents use the belief estimation from the Bayes filter. Our models are trained in simulation and transferred to the real world. We analyze the reliability and the performance of our framework through a series of simulated and real-world experiments and compare our method to the baseline work. Our results show that the learned tactile observation model can localize the pose of novel objects at 2-mm and 1-degree resolution for position and orientation, respectively. Furthermore, we experiment on a bottle opening task where the gripper needs to reach the desired grasp state.},
  archive   = {C_AAMAS},
  author    = {Kelestemur, Tarik and Platt, Robert and Padir, Taskin},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {742–750},
  title     = {Tactile pose estimation and policy learning for unknown object manipulation},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535934},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Translating omega-regular specifications to average
objectives for model-free reinforcement learning. <em>AAMAS</em>,
732–741. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent success in reinforcement learning (RL) has brought renewed attention to the design of reward functions by which agent behavior is reinforced or deterred. Manually designing reward functions is tedious and error-prone. An alternative approach is to specify a formal, unambiguous logic requirement, which is automatically translated into a reward function to be learned from. Omega-regular languages, of which Linear Temporal Logic (LTL) is a subset, are a natural choice for specifying such requirements due to their use in verification and synthesis. However, current techniques based on omega-regular languages learn in an episodic manner whereby the environment is periodically reset to an initial state during learning. In some settings, this assumption is challenging or impossible to satisfy. Instead, in the continuing setting the agent explores the environment without resets over a single lifetime. This is a more natural setting for reasoning about omega-regular specifications defined over infinite traces of agent behavior. Optimizing the average reward instead of the usual discounted reward is more natural in this case due to the infinite-horizon objective that poses challenges to the convergence of discounted RL solutions.We restrict our attention to the omega-regular languages which correspond to absolute liveness specifications. These specifications cannot be invalidated by any finite prefix of agent behavior, in accordance with the spirit of a continuing problem. We propose a translation from absolute liveness omega-regular languages to an average reward objective for RL. Our reduction can be done on-the-fly, without full knowledge of the environment, thereby enabling the use of model-free RL algorithms. Additionally, we propose a reward structure that enables RL without episodic resetting in communicating MDPs, unlike previous approaches. We demonstrate empirically with various benchmarks that our proposed method of using average reward RL for continuing tasks defined by omega-regular specifications is more effective than competing approaches that leverage discounted RL.},
  archive   = {C_AAMAS},
  author    = {Kazemi, Milad and Perez, Mateo and Somenzi, Fabio and Soudjani, Sadegh and Trivedi, Ashutosh and Velasquez, Alvaro},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {732–741},
  title     = {Translating omega-regular specifications to average objectives for model-free reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535933},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BADDr: Bayes-adaptive deep dropout RL for POMDPs.
<em>AAMAS</em>, 723–731. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While reinforcement learning (RL) has made great advances in scalability, exploration and partial observability are still active research topics. In contrast, Bayesian RL (BRL) provides a principled answer to both state estimation and the exploration-exploitation trade-off, but struggles to scale. To tackle this challenge, BRL frameworks with various prior assumptions have been proposed, with varied success. This work presents a representation-agnostic formulation of BRL under partially observability, unifying the previous models under one theoretical umbrella. To demonstrate its practical significance we also propose a novel derivation, Bayes-Adaptive Deep Dropout rl (BADDr), based on dropout networks. Under this parameterization, in contrast to previous work, the belief over the state and dynamics is a more scalable inference problem. We choose actions through Monte-Carlo tree search and empirically show that our method is competitive with state-of-the-art BRL methods on small domains while being able to solve much larger ones.},
  archive   = {C_AAMAS},
  author    = {Katt, Sammie and Nguyen, Hai and Oliehoek, Frans A. and Amato, Christopher},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {723–731},
  title     = {BADDr: Bayes-adaptive deep dropout RL for POMDPs},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535932},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). How hard is safe bribery? <em>AAMAS</em>, 714–722. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bribery in an election is one of the well-studied control problems in computational social choice. In this paper, we propose and study the safe bribery problem. Here the goal of the briber is to ask the bribed voters to vote in such a way that the briber never prefers the original winner (of the unbribed election) more than the new winner, even if the bribed voters do not fully follow the briber&#39;s advice. Indeed, in many applications of bribery, campaigning for example, the briber often has limited control on whether the bribed voters eventually follow her recommendation and thus it is conceivable that the bribed voters can either partially or fully ignore the briber&#39;s recommendation. We provide a comprehensive complexity theoretic landscape of the safe bribery problem for many common voting rules in this paper.},
  archive   = {C_AAMAS},
  author    = {Karia, Neel and Mallick, Faraaz and Dey, Palash},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {714–722},
  title     = {How hard is safe bribery?},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535931},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Planning not to talk: Multiagent systems that are robust to
communication loss. <em>AAMAS</em>, 705–713. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In a cooperative multiagent system, a collection of agents executes a joint policy in order to achieve some common objective. The successful deployment of such systems hinges on the availability of reliable inter-agent communication. However, many sources of potential disruption to communication exist in practice, such as radio interference, hardware failure, and adversarial attacks. In this work, we develop joint policies for cooperative multiagent systems that are robust to potential losses in communication. More specifically, we develop joint policies for cooperative Markov games with reach-avoid objectives. First, we propose an algorithm for the decentralized execution of joint policies during periods of communication loss. Next, we use the total correlation of the state-action process induced by a joint policy as a measure of the intrinsic dependencies between the agents. We then use this measure to lower-bound the performance of a joint policy when communication is lost. Finally, we present an algorithm that maximizes a proxy to this lower bound in order to synthesize minimum-dependency joint policies that are robust to communication loss. Numerical experiments show that the proposed minimum-dependency policies require minimal coordination between the agents while incurring little to no loss in performance; the total correlation value of the synthesized policy is one fifth of the total correlation value of the baseline policy which does not take potential communication losses into account. As a result, the performance of the minimum-dependency policies remains consistently high regardless of whether or not communication is available. By contrast, the performance of the baseline policy decreases by twenty percent when communication is lost.},
  archive   = {C_AAMAS},
  author    = {Karabag, Mustafa O. and Neary, Cyrus and Topcu, Ufuk},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {705–713},
  title     = {Planning not to talk: Multiagent systems that are robust to communication loss},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535930},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal matchings with one-sided preferences: Fixed and
cost-based quotas. <em>AAMAS</em>, 696–704. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the well-studied many-to-one bipartite matching problem of assigning applicants A to posts P where applicants rank posts in the order of preference. This setting models many important real-world allocation problems like assigning students to courses, applicants to jobs, amongst many others. In such scenarios, it is natural to ask for an allocation that satisfies guarantees of the form &quot;match at least 80\% of applicants to one of their top three choices&quot; or &quot;it is unacceptable to leave more than 10\% of applicants unassigned&quot;. The well-studied notions of rank-maximality and fairness fail to capture such requirements due to their property of optimizing extreme ends of the signature of a matching. We, therefore, propose a novel optimality criterion, which we call as the &quot;cumulative better signature&quot;.We investigate the computational complexity of the new notion of optimality in the setting where posts have associated fixed quotas. We prove that under the fixed quota setting, the problem turns out to be NP-hard under natural restrictions. We provide randomized algorithms in the fixed quota setting when the number of ranks is constant. We also study the problem under a cost-based quota setting and show that min-cost cumulative better matching can be computed efficiently. Apart from circumventing the hardness, the cost-based quota setting is motivated by real-world applications like course allocation or school choice where the capacities or quotas need not be rigid.},
  archive   = {C_AAMAS},
  author    = {K. A., Santhini and Sankar, Govind S. and Nasre, Meghana},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {696–704},
  title     = {Optimal matchings with one-sided preferences: Fixed and cost-based quotas},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535929},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Selecting PhD students and projects with limited funding.
<em>AAMAS</em>, 687–695. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In some universities, there is a fixed number of PhD grants, but a larger number of eligible projects and students, each student being allowed to apply on several projects, and a committee builds up a ranking (masterlist) over student-project pairs. The paper analyses three mechanisms to choose the pairs to be funded. The first one, used in some universities, is a greedy mechanism that gives a huge priority to the masterlist and very little to student&#39;s preferences. At the other extremity of the spectrum, we have a priority-list variant of student-oriented Gale-Shapley. It is strategyproof and optimal for students, but has one drawback: it pays too little attention to the masterlist, and thus to the committee. Inbetween, we have an intermediate mechanism which can be seen as a good trade-off. Among the properties we study, one is specific to our setting: dynamic monotonicity, which deals with cases when a student suddenly leaves the system in the middle of the process.},
  archive   = {C_AAMAS},
  author    = {Jindal, Jatin and Lang, J\&#39;{e}r\^{o}me and Cechl\&#39;{a}rov\&#39;{a}, Katar\&#39;{\i}na and Lesca, Julien},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {687–695},
  title     = {Selecting PhD students and projects with limited funding},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535928},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Balancing fairness and efficiency in traffic routing via
interpolated traffic assignment. <em>AAMAS</em>, 678–686. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {System optimum (SO) routing, wherein the total travel time of all users is minimized, is a holy grail for transportation authorities. However, SO routing may discriminate against users who incur much larger travel times than others to achieve high system efficiency, i.e., low total travel times. To address the inherent unfairness of SO routing, we study the β-fair SO problem whose goal is to minimize the total travel time while guaranteeing a β ≥ 1 level of unfairness, which specifies the maximum possible ratio between the travel times of different users with shared origins and destinations. To obtain feasible solutions to the β-fair SO problem while achieving high system efficiency, we develop a new convex program, the Interpolated Traffic Assignment Problem (I-TAP), which interpolates between a fairness-promoting and an efficiency-promoting traffic-assignment objective. We evaluate the efficacy of I-TAP through theoretical bounds on the total system travel time and level of unfairness in terms of its interpolation parameter, as well as present a numerical comparison between I-TAP and a state-of-the-art algorithm on a range of transportation networks. The numerical results indicate that our approach is faster by several orders of magnitude as compared to the benchmark algorithm, while achieving higher system efficiency for all desirable levels of unfairness. We further leverage the structure of I-TAP to develop two pricing mechanisms to collectively enforce the I-TAP solution in the presence of selfish homogeneous and heterogeneous users, respectively, that independently choose routes to minimize their own travel costs. We mention that this is the first study of pricing in the context of fair routing for general road networks.},
  archive   = {C_AAMAS},
  author    = {Jalota, Devansh and Solovey, Kiril and Tsao, Matthew and Zoepf, Stephen and Pavone, Marco},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {678–686},
  title     = {Balancing fairness and efficiency in traffic routing via interpolated traffic assignment},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535927},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lazy-MDPs: Towards interpretable RL by learning when to act.
<em>AAMAS</em>, 669–677. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traditionally, Reinforcement Learning (RL) aims at deciding how to act optimally for an artificial agent. We argue that deciding when to act is equally important. As humans, we drift from default, instinctive or memorized behaviors to focused, thought-out behaviors when required by the situation. To enhance RL agents with this aptitude, we propose to augment the standard Markov Decision Process and make a new mode of action available: being lazy, which defers decision-making to a default policy. In addition, we penalize non-lazy actions in order to enforce minimal effort and have agents focus on critical decisions only. We name the resulting formalism lazy-MDPs. We study the theoretical properties of lazy-MDPs, expressing value functions and characterizing greediness and optimal solutions. Then we empirically demonstrate that policies learned in lazy-MDPs generally come with a form of interpretability: by construction, they show us the states where the agent takes control over the default policy. We deem those states and corresponding actions important since they explain the difference in performance between the default and the new, lazy policy. With suboptimal policies (even uniform random) as default, we observe that agents are still able to get close to and sometimes outperform DQN on Atari games while only taking control in a limited subset of states.},
  archive   = {C_AAMAS},
  author    = {Jacq, Alexis and Ferret, Johan and Pietquin, Olivier and Geist, Matthieu},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {669–677},
  title     = {Lazy-MDPs: Towards interpretable RL by learning when to act},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535926},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A declarative framework for maximal k-plex enumeration
problems. <em>AAMAS</em>, 660–668. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It is widely accepted that an ideal community in networks is the one whose structure is closest to a (maximal) clique. However, inmost real-world graphs the clique model is too restrictive, as it requires complete pairwise interactions. More relaxed cohesive subgraph models were then studied. A k-plex is one of the arguably most studied pseudo-clique model. A k-plex of size n is a subgraph where any vertex is adjacent to at least (n-k) vertices. Unfortunately, some maximal k-plexes, by involving irrelevant subgraphs, are far from designing meaningful communities in real-world networks. In this paper, we first introduce a novel variant of k-plex model, called cohesive k-plex, which is more appropriate for modeling closely interacting communities. Then, we reduce the problem of enumerating maximal (cohesive) k-plexes in a graphto those of enumerating the models of a formula in propositional logic. Afterwards, to make our approach more efficient, we provide a decomposition technique that is particularly suitable for deriving smaller and independent sub-problems easy to resolve. Lastly, our extensive experiments on various real-world graphs demonstrate the efficiency of the proposed approach w.r.t state-of-the-art algorithms.},
  archive   = {C_AAMAS},
  author    = {Jabbour, Said and Mhadhbi, Nizar and Raddaoui, Badran and Sais, Lakhdar},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {660–668},
  title     = {A declarative framework for maximal K-plex enumeration problems},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535925},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Being central on the cheap: Stability in heterogeneous
multiagent centrality games. <em>AAMAS</em>, 651–659. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study strategic network formation games in which agents attempt to form (costly) links in order to maximize their network centrality. Our model derives from Jackson and Wolinsky&#39;s symmetric connection model, but allows for heterogeneity in agent utilities by replacing decay centrality (implicit in the J-W model) by a variety of classical centrality measures, as well as game-theoretic measures of centrality. We are primarily interested in characterizing the asymptotically pairwise stable networks, i.e. those networks that are pairwise stable for all sufficiently small, positive edge costs. We uncover a rich typology of stability: we give an axiomatic approach to network centrality that allows us to predict the stable network for a rich set of combination of centrality utility functions, yielding stable networks with features reminiscent of structural properties such as &quot;core periphery&quot; and &quot;rich club&quot; networks. We show that a simple variation on the model renders it universal, i.e. every network may be a stable network. We also show that often we can infer a significant amount about agent utilities from the structure of stable networks.},
  archive   = {C_AAMAS},
  author    = {Istrate, Gabriel and Bonchis, Cosmin},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {651–659},
  title     = {Being central on the cheap: Stability in heterogeneous multiagent centrality games},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535924},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cascades and overexposure in social networks: The budgeted
case. <em>AAMAS</em>, 642–650. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Influence maximization (IM) has now been a widely studied topic, but only in recent years have studies considered overexposure. Overexposure is usually measured as the negative cost associated with reaching unintended recipients during an information cascade. A polynomial-time algorithm is known for cascades with overexposure when we can seed as many nodes as we want. This paper focuses on overexposure for the budgeted case of seeding, which has received little to no attention. We show that the problem is NP-hard even for restricted cases. For various special cases, we devise provable approximation algorithms, dynamic programming solutions, linear programming solutions, and heuristics. For the general case, we provide a linear programming solution and several fast and effective heuristics, mostly of the greedy flavor. We perform an extensive experimental study using synthetic and real-world networks. We investigate how network properties and model parameters impact our algorithms. It brings out interesting findings like why a low-quality product needs a smarter algorithm, and why certain algorithms do well on some networks but not others.},
  archive   = {C_AAMAS},
  author    = {Irfan, Mohammad T. and Hancock, Kim and Friel, Laura M.},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {642–650},
  title     = {Cascades and overexposure in social networks: The budgeted case},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535923},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Autonomous swarm shepherding using curriculum-based
reinforcement learning. <em>AAMAS</em>, 633–641. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous shepherding is a bio-inspired swarm guidance approach, whereby an artificial sheepdog guides a swarm of artificial or biological agents, such as sheep, towards a goal. While the success in this guidance depends on the set of behaviours exhibited by the sheepdog, the main source of complexity for learning effective behaviours lies within the highly non-linear dynamics featured among the swarm members as well as between the swarm and the sheepdog. Attempts to apply reinforcement learning (RL) to shepherding have so far relied greatly on rule-based algorithms for calculating waypoints to guide the RL algorithm. In this paper, we propose a curriculum-based approach for RL that does not rely on any external algorithm to pre-determine waypoints for the sheepdog. Instead, the approach uses task decomposition by formulating shepherding in terms of two sub-tasks: (1) pushing an agent from a start to a target location and (2) selecting between collecting scattered agents or driving the biggest cluster of agents to the goal. Simple-to-complex curriculum learning is used to accelerate the learning of each sub-task. For the first sub-task, the complexity is gradually increased over training time, whereas for the second sub-task a simplified environment is designed for initial learning before proceeding with the main environment. The proposed approach results in high-performance shepherding with a success rate of about 96\%. While curriculum learning was found to expedite the learning of the first sub-task, it was not as efficient for the second sub-task. Our analyses highlight the need for the careful design of the curriculum to ensure that skills acquired in intermediate tasks are useful for the main tasks.},
  archive   = {C_AAMAS},
  author    = {Hussein, Aya and Petraki, Eleni and Elsawah, Sondoss and Abbass, Hussein A.},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {633–641},
  title     = {Autonomous swarm shepherding using curriculum-based reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535922},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reduction-based solving of multi-agent pathfinding on large
maps using graph pruning. <em>AAMAS</em>, 624–632. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent pathfinding is the problem of finding collision-free paths for a set of agents. Solving this problem optimally is computationally hard, therefore many techniques based on reductions to other formalisms were developed. In comparison to search-based techniques, the reduction-based techniques fall behind on large maps even for a small number of agents. To combat this phenomenon, we propose several strategies for pruning vertices off large instances that will most likely not be used by agents. First, we introduce these strategies conceptually and prove which of them maintain completeness and optimality. Eventually, we conduct an exhaustive evaluation and show that graph pruning strategies make reduction-based solvers comparable to search-based techniques on large maps while maintaining their advantage on small dense maps.},
  archive   = {C_AAMAS},
  author    = {Hus\&#39;{a}r, Matej and \v{S}vancara, Jir\&#39;{\i} and Obermeier, Philipp and Bart\&#39;{a}k, Roman and Schaub, Torsten},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {624–632},
  title     = {Reduction-based solving of multi-agent pathfinding on large maps using graph pruning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535921},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The dynamics of q-learning in population games: A
physics-inspired continuity equation model. <em>AAMAS</em>, 615–623. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although learning has found wide application in multi-agent systems, its effects on the temporal evolution of a system are far from understood. This paper focuses on the dynamics of Q-learning in large-scale multi-agent systems modeled as population games. We revisit the replicator equation model for Q-learning dynamics and observe that this model is inappropriate for our concerned setting. Motivated by this, we develop a new formal model, which bears a formal connection with the continuity equation in physics. We show that our model always accurately describes the Q-learning dynamics in population games across different initial settings of MASs and game configurations. We also show that our model can be applied to different exploration mechanisms, describe the mean dynamics, and be extended to Q-learning in 2-player and n-player games. Last but not least, we show that our model can provide insights into algorithm parameters and facilitate parameter tuning.},
  archive   = {C_AAMAS},
  author    = {Hu, Shuyue and Leung, Chin-Wing and Leung, Ho-fung and Soh, Harold},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {615–623},
  title     = {The dynamics of Q-learning in population games: A physics-inspired continuity equation model},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535920},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A mean field game model of spatial evolutionary games.
<em>AAMAS</em>, 606–614. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Evolutionary Game Theory (EGT) studies evolving populations of agents that interact through normal form games whose outcome determines each individual&#39;s evolutionary fitness. In many applications, EGT models are extended to include spatial effects in which the agents are located in a structured population such as a graph. We propose a Mean Field Game (MFG) generalization, denoted Pair-MFG, of the spatial evolutionary game model such that the behavior of a given spatial evolutionary game (or more specifically the behavior of its pair approximation) is a special case trajectory of the corresponding MFG. The proposed Pair-MFG model also allows for the formulation of the spatial evolutionary game as a control problem, opening up additional avenues of research into controlling the outcomes of these games. The state evolution equations of the proposed model are highly nonlinear and none of the equations in the system are necessarily convex. This necessitates different numerical methods as compared to those for traditional Linear Quadratic Gaussian MFGs. We provide a method for solving this new Pair-MFG model using fixed point iteration with time-dependent proximal terms and show empirically that this method is capable of finding a solution to a selection of EGT games.},
  archive   = {C_AAMAS},
  author    = {Hsiao, Vincent and Nau, Dana},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {606–614},
  title     = {A mean field game model of spatial evolutionary games},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535919},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ordinal maximin share approximation for chores.
<em>AAMAS</em>, 597–605. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of fairly allocating a set of m indivisible chores (items with non-positive value) to n agents. We consider the desirable fairness notion of 1-out-of-d maximin share (MMS)---the minimum value that an agent can guarantee by partitioning items into d bundles and receiving the least valued bundle---and focus on ordinal approximation of MMS that aims at finding the largest d\l{}eq n for which 1-out-of-d MMS allocation exists. Our main contribution is a polynomial-time algorithm for 1-out-of-\l{} 2n/3 MMS allocation, and a proof of existence of 1-out-of-\l{}floor 3n/4 MMS allocation of chores. Furthermore, we show how to use recently-developed algorithms for bin-packing to approximate the latter bound up to a logarithmic factor in polynomial time.},
  archive   = {C_AAMAS},
  author    = {Hosseini, Hadi and Searns, Andrew and Segal-Halevi, Erel},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {597–605},
  title     = {Ordinal maximin share approximation for chores},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535918},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computing nash equilibria for district-based nominations.
<em>AAMAS</em>, 588–596. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study political parties that strategically place their candidates in districts so to maximise the number of their nominees that get elected. In each district, voters rank the nominated candidates and elect the plurality winners. After studying equilibrium existence in restricted instances, we show that deciding the existence of pure Nash equilibria for these games is NP-complete if party size is bounded by a constant and Σ^P _2-complete for the general case. For the hardness part of the latter result we reduce from EEthreeSAT.},
  archive   = {C_AAMAS},
  author    = {Harrenstein, Paul and Turrini, Paolo},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {588–596},
  title     = {Computing nash equilibria for district-based nominations},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535917},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical value decomposition for effective on-demand
ride-pooling. <em>AAMAS</em>, 580–587. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {On-demand ride-pooling (e.g., UberPool, GrabShare) services focus on serving multiple different customer requests using each vehicle, i.e., an empty or partially filled vehicle can be assigned requests from different passengers with different origins and destinations. On the other hand, in Taxi on Demand (ToD) services (e.g., UberX), one vehicle is assigned to only one request at a time. On-demand ride pooling is not only beneficial to customers (lower cost), drivers (higher revenue per trip) and aggregation companies (higher revenue), but is also of crucial importance to the environment as it reduces the number of vehicles required on the roads. Since each vehicle has to be matched with a combination of customer requests, the matching problem in ride pooling is significantly more challenging. Due to this complexity, most existing solutions to ride-pooling problem are myopic in that they either ignore future impact of current matches or the impact of other taxis in the expected revenue earned by a taxi. In this paper, we build on an approximate dynamic programming framework to consider impact of other taxis on the value of a taxi (expected revenue earned until end of horizon) through a novel hierarchical value decomposition framework. On a real world city scale taxi data set, we show a significant improvement of up to 10.7\% in requests served compared to existing best method for on-demand ride pooling.},
  archive   = {C_AAMAS},
  author    = {Hao, Jiang and Varakantham, Pradeep},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {580–587},
  title     = {Hierarchical value decomposition for effective on-demand ride-pooling},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535916},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiagent model-based credit assignment for continuous
control. <em>AAMAS</em>, 571–579. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep reinforcement learning (RL) has recently shown great promise in robotic continuous control tasks. Nevertheless, prior research in this vein center around the centralized learning setting that largely relies on the communication availability among all the components of a robot. However, agents in the real world often operate in a decentralised fashion without communication due to latency requirements, limited power budgets and safety concerns. By formulating robotic components as a system of decentralised agents, this work presents a decentralised multiagent reinforcement learning framework for continuous control. To this end, we first develop a cooperative multiagent PPO framework that allows for centralized optimisation during training and decentralised operation during execution. However, the system only receives a global reward signal which is not attributed towards each agent. To address this challenge, we further propose a generic game-theoretic credit assignment framework which computes agent-specific reward signals. Last but not least, we also incorporate a model-based RL module into our credit assignment framework, which leads to significant improvement in sample efficiency. Finally, we empirically demonstrate the effectiveness of our framework on Mujoco locomotion control tasks.},
  archive   = {C_AAMAS},
  author    = {Han, Dongge and Lu, Chris Xiaoxuan and Michalak, Tomasz and Wooldridge, Michael},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {571–579},
  title     = {Multiagent model-based credit assignment for continuous control},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535915},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intention-aware navigation in crowds with extended-space
POMDP planning. <em>AAMAS</em>, 562–570. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a hybrid online Partially Observable Markov Decision Process (POMDP) planning system that addresses the problem of autonomous navigation in the presence of multi-modal uncertainty introduced by other agents in the environment. As a particular example, we consider the problem of autonomous navigation in dense crowds of pedestrians and among obstacles. Popular approaches to this problem first generate a path using a complete planner (e.g., Hybrid A*) with ad-hoc assumptions about uncertainty, then use online tree-based POMDP solvers to reason about uncertainty with control over a limited aspect of the problem (i.e. speed along the path). We present a more capable and responsive real-time approach enabling the POMDP planner to control more degrees of freedom (e.g., both speed AND heading) to achieve more flexible and efficient solutions. This modification greatly extends the region of the state space that the POMDP planner must reason over, significantly increasing the importance of finding effective roll-out policies within the limited computational budget that real time control affords. Our key insight is to use multi-query motion planning techniques (e.g., Probabilistic Roadmaps or Fast Marching Method) as priors for rapidly generating efficient roll-out policies for every state that the POMDP planning tree might reach during its limited horizon search. Our proposed approach generates trajectories that are safe and significantly more efficient than the previous approach, even in densely crowded dynamic environments with long planning horizons.},
  archive   = {C_AAMAS},
  author    = {Gupta, Himanshu and Hayes, Bradley and Sunberg, Zachary},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {562–570},
  title     = {Intention-aware navigation in crowds with extended-space POMDP planning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535914},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-agent curricula and emergent implicit signaling.
<em>AAMAS</em>, 553–561. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Emergent communication has made strides towards learning communication from scratch, but has focused primarily on protocols that resemble human language. In nature, multi-agent cooperation gives rise to a wide range of communication that varies in structure and complexity. In this work, we recognize the full spectrum of communication that exists in nature and propose studying lower-level communication. Specifically, we study emergent implicit signaling in the context of decentralized multi-agent learning in difficult, sparse reward environments. However, learning to coordinate in such environments is challenging. We propose a curriculum-driven strategy that combines: (i) velocity-based environment shaping, tailored to the skill level of the multi-agent team; and (ii) a behavioral curriculum that helps agents learn successful single-agent behaviors as a precursor to learning multi-agent behaviors. Pursuit-evasion experiments show that our approach learns effective coordination, significantly outperforming sophisticated analytical and learned policies. Our method completes the pursuit-evasion task even when pursuers move at half of the evader&#39;s speed, whereas the highest-performing baseline fails at 80\% of the evader&#39;s speed. Moreover, we examine the use of implicit signals in coordination through position-based social influence. We show that pursuers trained with our strategy exchange more than twice as much information (in bits) than baseline methods, indicating that our method has learned, and relies heavily on, the exchange of implicit signals.},
  archive   = {C_AAMAS},
  author    = {Grupen, Niko A. and Lee, Daniel D. and Selman, Bart},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {553–561},
  title     = {Multi-agent curricula and emergent implicit signaling},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535913},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust no-regret learning in min-max stackelberg games.
<em>AAMAS</em>, 543–552. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The behavior of no-regret learning algorithms is well understood in two-player min-max (i.e, zero-sum) games. In this paper, we investigate the behavior of no-regret learning in min-max games with dependent strategy sets, where the strategy of the first player constrains the behavior of the second. Such games are best understood as sequential, i.e., min-max Stackelberg, games. We consider two settings, one in which only the first player chooses their actions using a no-regret algorithm while the second player best responds, and one in which both players use no-regret algorithms. For the former case, we show that no-regret dynamics converge to a Stackelberg equilibrium. For the latter case, we introduce a new type of regret, which we call Lagrangian regret, and show that if both players minimize their Lagrangian regrets, then play converges to a Stackelberg equilibrium. We then observe that online mirror descent (OMD) dynamics in these two settings correspond respectively to a known nested (i.e., sequential) gradient descent-ascent (GDA) algorithm and a new simultaneous GDA-like algorithm, thereby establishing convergence of these algorithms to Stackelberg equilibrium. Finally, we analyze the robustness of OMD dynamics to perturbations by investigating online min-max Stackelberg games. We prove that OMD dynamics are robust for a large class of online min-max games with independent strategy sets. In the dependent case, we demonstrate the robustness of OMD dynamics experimentally by simulating them in online Fisher markets, a canonical example of a min-max Stackelberg game with dependent strategy sets.},
  archive   = {C_AAMAS},
  author    = {Goktas, Denizalp and Zhao, Jiayi and Greenwald, Amy},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {543–552},
  title     = {Robust no-regret learning in min-max stackelberg games},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535912},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fair and truthful mechanism with limited subsidy.
<em>AAMAS</em>, 534–542. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The notion of envy-freeness is a natural and intuitive fairness requirement in resource allocation. With indivisible goods, such fair allocations are unfortunately not guaranteed to exist. Classical works have avoided this issue by introducing an additional divisible resource, i.e., money, to subsidize envious agents. In this paper, we aim to design a truthful allocation mechanism of indivisible goods to achieve both fairness and efficiency criteria with a limited amount of subsidy. Following the work of Halpern and Shah, our central question is as follows: to what extent do we need to rely on the power of money to accomplish these objectives? We show that, when agents have matroidal valuations, there is a truthful allocation mechanism that achieves envy-freeness and utilitarian optimality by subsidizing each agent with at most 1, the maximum marginal contribution of each item for each agent. The design of the mechanism rests crucially on the underlying matroidal M-convexity of the Lorenz dominating allocations. For superadditive valuations, we show that there is a truthful mechanism that achieves envy-freeness and utilitarian optimality, with each agent receiving a subsidy of at most m; furthermore, we show that the amount m is necessary even when agents have additive valuations.},
  archive   = {C_AAMAS},
  author    = {Goko, Hiromichi and Igarashi, Ayumi and Kawase, Yasushi and Makino, Kazuhisa and Sumita, Hanna and Tamura, Akihisa and Yokoi, Yu and Yokoo, Makoto},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {534–542},
  title     = {Fair and truthful mechanism with limited subsidy},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535911},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Long-term resource allocation fairness in average markov
decision process (AMDP) environment. <em>AAMAS</em>, 525–533. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fairness has emerged as an important concern in automated decision-making in recent years, especially when these decisions affect human welfare. In this work, we study fairness in temporally extended decision-making settings, specifically those formulated as Markov Decision Processes (MDPs). Our proposed notion of fairness ensures that each state&#39;s long-term visitation frequency is at least a specified fraction. This quota-based notion of fairness is natural in many resource-allocation settings where the dynamics of a single resource being allocated is governed by an MDP and the distribution of the shared resource is captured by its state-visitation frequency. In an average-reward MDP (AMDP) setting, we formulate the problem as a bilinear saddle point program and, for a generative model, solve it using a Stochastic Mirror Descent (SMD) based algorithm. The proposed solution guarantees a simultaneous approximation on the expected average-reward and fairness requirement. We give sample complexity bounds for the proposed algorithm and validate our theoretical results with experiments on simulated data.},
  archive   = {C_AAMAS},
  author    = {Ghalme, Ganesh and Nair, Vineet and Patil, Vishakha and Zhou, Yilun},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {525–533},
  title     = {Long-term resource allocation fairness in average markov decision process (AMDP) environment},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535910},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Building contrastive explanations for multi-agent team
formation. <em>AAMAS</em>, 516–524. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As more and more hard and complex procedures are being automated with the aid of artificial intelligence, the need for humans to understand the rationale behind AI decisions becomes imperative. Adequate explanations for decisions made by an intelligent system do not just help describing how the system works, they also earn users&#39; trust. In this work we focus on a general methodology for justifying why certain teams are formed and others are not by a team formation algorithm (TFA). Specifically, we introduce an algorithm that wraps up any existing TFA and builds justifications regarding the teams formed by such TFA. This is done without modifying the TFA in any way. Our algorithm offers users a collection of commonly-asked questions within a team formation scenario and builds justifications as contrastive explanations. We also report on an empirical evaluation to determine the quality of the explanations provided by our algorithm.},
  archive   = {C_AAMAS},
  author    = {Georgara, Athina and Rodriguez Aguilar, Juan A. and Sierra, Carles},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {516–524},
  title     = {Building contrastive explanations for multi-agent team formation},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535909},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sample-based approximation of nash in large many-player
games via gradient descent. <em>AAMAS</em>, 507–515. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Nash equilibrium is a central concept in game theory. Several Nash solvers exist, yet none scale to normal-form games with many actions and many players, especially those with payoff tensors too big to be stored in memory. In this work, we propose an approach that iteratively improves an approximation to a Nash equilibrium through joint play. It accomplishes this by tracing a previously established homotopy that defines a continuum of equilibria for the game regularized with decaying levels of entropy. This continuum asymptotically approaches the limiting logit equilibrium, proven by MKelvey and Palfrey (1995) to be unique in almost all games, thereby partially circumventing the well-known equilibrium selection problem of many-player games. To encourage iterates to remain near this path, we efficiently minimize average deviation incentive via stochastic gradient descent, intelligently sampling entries in the payoff tensor as needed. Monte Carlo estimates of the stochastic gradient from joint play are biased due to the appearance of a nonlinear max operator in the objective, so we introduce additional innovations to the algorithm to alleviate gradient bias. The descent process can also be viewed as repeatedly constructing and reacting to a polymatrix approximation to the game. In these ways, our proposed approach, average deviation incentive descent with adaptive sampling (ADIDAS), is most similar to three classical approaches, namely homotopy-type, Lyapunov, and iterative polymatrix solvers. The lack of local convergence guarantees for biased gradient descent prevents guaranteed convergence to Nash, however, we demonstrate through extensive experiments the ability of this approach to approximate a unique Nash equilibrium in normal-form games with as many as seven players and twenty one actions (several billion outcomes) that are orders of magnitude larger than those possible with prior algorithms.},
  archive   = {C_AAMAS},
  author    = {Gemp, Ian and Savani, Rahul and Lanctot, Marc and Bachrach, Yoram and Anthony, Thomas and Everett, Richard and Tacchetti, Andrea and Eccles, Tom and Kram\&#39;{a}r, J\&#39;{a}nos},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {507–515},
  title     = {Sample-based approximation of nash in large many-player games via gradient descent},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535908},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). D3C: Reducing the price of anarchy in multi-agent learning.
<em>AAMAS</em>, 498–506. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In multiagent systems, the complex interaction of fixed incentives can lead agents to outcomes that are poor (inefficient) not only for the group, but also for each individual. Price of anarchy is a technical, game-theoretic definition that quantifies the inefficiency arising in these scenarios-- it compares the welfare that can be achieved through perfect coordination against that achieved by self-interested agents at a Nash equilibrium. We derive a differentiable, upper bound on a price of anarchy that agents can cheaply estimate during learning. Equipped with this estimator, agents can adjust their incentives in a way that improves the efficiency incurred at a Nash equilibrium. Agents do so by learning to mix their reward (equiv. negative loss) with that of other agents by following the gradient of our derived upper bound. We refer to this approach as D3C. In the case where agent incentives are differentiable, D3C resembles the celebrated Win-Stay, Lose-Shift strategy from behavioral game theory, thereby establishing a connection between the global goal of maximum welfare and an established agent-centric learning rule. In the non-differentiable setting, as is common in multiagent reinforcement learning, we show the upper bound can be reduced via evolutionary strategies, until a compromise is reached in a distributed fashion. We demonstrate that D3C improves outcomes for each agent and the group as a whole on several social dilemmas including a traffic network exhibiting Braess&#39;s paradox, a prisoner&#39;s dilemma, and several multiagent domains.},
  archive   = {C_AAMAS},
  author    = {Gemp, Ian and McKee, Kevin R. and Everett, Richard and Du\&#39;{e}\~{n}ez-Guzm\&#39;{a}n, Edgar and Bachrach, Yoram and Balduzzi, David and Tacchetti, Andrea},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {498–506},
  title     = {D3C: Reducing the price of anarchy in multi-agent learning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535907},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Concave utility reinforcement learning: The mean-field game
viewpoint. <em>AAMAS</em>, 489–497. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Concave Utility Reinforcement Learning (CURL) extends RL from linear to concave utilities in the occupancy measure induced by the agent&#39;s policy. This encompasses not only RL but also imitation learning and exploration, among others. Yet, this more general paradigm invalidates the classical Bellman equations, and calls for new algorithms. Mean-field Games (MFGs) are a continuous approximation of many-agent RL. They consider the limit case of a continuous distribution of identical agents, anonymous with symmetric interests, and reduce the problem to the study of a single representative agent in interaction with the full population. Our core contribution consists in showing that CURL is a subclass of MFGs. We think this important to bridge together both communities. It also allows to shed light on aspects of both fields: we show the equivalence between concavity in CURL and monotonicity in the associated MFG, between optimality conditions in CURL and Nash equilibrium in MFG, or that Fictitious Play (FP) for this class of MFGs is simply Frank-Wolfe, bringing the first convergence rate for discrete-time FP for MFGs. We also experimentally demonstrate that, using algorithms recently introduced for solving MFGs, we can address the CURL problem more efficiently.},
  archive   = {C_AAMAS},
  author    = {Geist, Matthieu and P\&#39;{e}rolat, Julien and Lauri\`{e}re, Mathieu and Elie, Romuald and Perrin, Sarah and Bachem, Oliver and Munos, R\&#39;{e}mi and Pietquin, Olivier},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {489–497},
  title     = {Concave utility reinforcement learning: The mean-field game viewpoint},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535906},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Refined hardness of distance-optimal multi-agent path
finding. <em>AAMAS</em>, 481–488. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the computational complexity of multi-agent path finding (MAPF). Given a graph G and a set of agents, each having a start and target vertex, the goal is to find collision-free paths minimizing the total distance traveled. To better understand the source of difficulty of the problem, we aim to study the simplest and least constrained graph class for which it remains hard. To this end, we restrict G to be a 2D grid, which is a ubiquitous abstraction, as it conveniently allows for modeling well-structured environments (e.g., warehouses). Previous hardness results considered highly constrained 2D grids having only one vertex unoccupied by an agent, while the most restricted hardness result that allowed multiple empty vertices was for (non-grid) planar graphs. We therefore refine previous results by simultaneously considering both 2D grids and multiple empty vertices. We show that even in this case distance-optimal MAPF remains NP-hard, which settles an open problem posed by Banfi et al. [4]. We present a reduction directly from 3-SAT using simple gadgets, making our proof arguably more informative than previous work in terms of potential progress towards positive results. Furthermore, our reduction is the first linear one for the case where G is planar, appearing nearly four decades after the first related result. This allows us to go a step further and exploit the Exponential Time Hypothesis (ETH) to obtain an exponential lower bound for the running time of the problem. Finally, as a stepping stone towards our main results, we prove the NP-hardness of the monotone case, in which agents move one by one with no intermediate stops.},
  archive   = {C_AAMAS},
  author    = {Geft, Tzvika and Halperin, Dan},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {481–488},
  title     = {Refined hardness of distance-optimal multi-agent path finding},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535905},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Negotiated path planning for non-cooperative multi-robot
systems. <em>AAMAS</em>, 472–480. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As autonomous systems are deployed at a large scale in both public and private spaces, robots owned and operated by competing organisations will be required to interact. Interactions in such settings will be inherently non-cooperative. In this paper, we address the problem of non-cooperative multi-agent path finding. We design an auction mechanism that allows a group of agents to reach their goals whilst minimising the total cost of the system. In particular, we aim to design a mechanism such that rational agents are incentivised to participate. Our privileged knowledge auction consists of a modified combinatorial Vickrey-Clarke-Groves auction. Our approach limits the initial number of bids in the Vickrey-Clarke-Groves auction, then uses the privileged knowledge of the auctioneer to identify and solve path conflicts. In order to maintain agent autonomy in the non-cooperative system, individual agents are provided with final say over paths. The mechanism provides a heuristic method to maximise social welfare whilst remaining computationally efficient. We also consider single-agent bid generation and propose a similarity metric to use in dissimilar shortest path generation. We then show this bid generation method increases the success likelihood of both the limited-bid VCG auction and our novel approach on synthetic data. Our experiments with synthetic data outperform existing work on the non-cooperative problem.},
  archive   = {C_AAMAS},
  author    = {Gautier, Anna and Stephens, Alex and Lacerda, Bruno and Hawes, Nick and Wooldridge, Michael},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {472–480},
  title     = {Negotiated path planning for non-cooperative multi-robot systems},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535904},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). One-sided matching markets with endowments: Equilibria and
algorithms. <em>AAMAS</em>, 463–471. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Arrow-Debreu extension of the classic Hylland-Zeckhauser scheme [27] for a one-sided matching market -- called ADHZ in this paper -- has natural applications but has instances which do not admit equilibria. By introducing approximation, we define the ε-approximate ADHZ model, and we give the following results.(1) Existence of equilibrium under linear utility functions. We prove that the equilibrium satisfies Pareto optimality, approximate envy-freeness, and approximate weak core stability.(2) A combinatorial polynomial-time algorithm for an ε-approximate ADHZ equilibrium for the case of dichotomous, and more generally bi-valued, utilities.(3) An instance of ADHZ, with dichotomous utilities and a strongly connected demand graph, which does not admit an equilibrium.(4) A rational convex program for HZ under dichotomous utilities; a combinatorial polynomial-time algorithm for this case was given in [35].The ε-approximate ADHZ model fills a void in the space of general mechanisms for one-sided matching markets; see details in the paper.},
  archive   = {C_AAMAS},
  author    = {Garg, Jugal and Tr\&quot;{o}bst, Thorben and Vazirani, Vijay V.},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {463–471},
  title     = {One-sided matching markets with endowments: Equilibria and algorithms},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535903},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fully-autonomous, vision-based traffic signal control: From
simulation to reality. <em>AAMAS</em>, 454–462. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ineffective traffic signal control is one of the major causes of congestion in urban road networks. Dynamically changing traffic conditions and live traffic state estimation are fundamental challenges that limit the ability of the existing signal infrastructure in rendering individualized signal control in real-time. We use deep reinforcement learning (DRL) to address these challenges. Due to economic and safety constraints associated with training such agents in the real world, a practical approach is to do so in simulation before deployment. Domain randomisation is an effective technique for bridging thereality gap and ensuring effective transfer of simulation-trained agents to the real world. In this paper, we develop a fully-autonomous, vision-based DRL agent that achieves adaptive signal control in the face of complex, imprecise, and dynamic traffic environments. Our agent uses live visual data (i.e. a stream of real-time RGB footage) from an intersection to extensively perceive and subsequently act upon the traffic environment. Employing domain randomisation, we examine our agent&#39;s generalisation capabilities under varying traffic conditions in both the simulation and the real-world environments. In a diverse validation set independent of training data, our traffic control agent reliably adapted to novel traffic situations and demonstrated a positive transfer to previously unseen real intersections despite being trained entirely in simulation.},
  archive   = {C_AAMAS},
  author    = {Garg, Deepeka and Chli, Maria and Vogiatzis, George},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {454–462},
  title     = {Fully-autonomous, vision-based traffic signal control: From simulation to reality},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535902},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A symbolic representation for probabilistic dynamic
epistemic logic. <em>AAMAS</em>, 445–453. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Probabilistic Dynamic Epistemic Logic (PDEL) is a formalism for reasoning about the higher-order probabilistic knowledge of agents, and about how this knowledge changes when events occur. While PDEL has been studied for its theoretical appeal, it was only ever applied to toy examples: the combinatorial explosion of probabilistic Kripke structures makes the PDEL framework impractical for realistic applications, such as card games.This paper is a first step towards the use of PDEL in more practical settings: in line with recent work applying ideas from symbolic model checking to (non-probabilistic) DEL, we propose a &#39;&#39;symbolic&#39;&#39; representation of probabilistic Kripke structures as pseudo-Boolean functions, which can be represented with several data structures of the decision diagram family, in particular Algebraic Decision Diagrams (ADDs). We show that ADDs scale much better than explicit Kripke structures, and that they allow for efficient symbolic model checking, even on the realistic example of the Hanabi card game, thus paving the way towards the practical application of epistemic planning techniques.},
  archive   = {C_AAMAS},
  author    = {Gamblin, S\&#39;{e}bastien and Niveau, Alexandre and Bouzid, Maroua},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {445–453},
  title     = {A symbolic representation for probabilistic dynamic epistemic logic},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535901},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The price of majority support. <em>AAMAS</em>, 436–444. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the problem of finding a compromise between the opinions of a group of individuals on a number of mutually independent, binary topics. In this paper, we quantify the loss in representativeness that results from requiring the outcome to have majority support, in other words, the &quot;price of majority support&quot;. Each individual is assumed to support an outcome if they agree with the outcome on at least as many topics as they disagree on. Our results can also be seen as quantifying Anscombes paradox which states that topic-wise majority outcome may not be supported by a majority. To measure the representativeness of an outcome, we consider two metrics. First, we look for an outcome that agrees with a majority on as many topics as possible. We prove that the maximum number such that there is guaranteed to exist an outcome that agrees with a majority on this number of topics and has majority support, equals [(t+1)/2] where t is the total number of topics. Second, we count the number of times a voter opinion on a topic matches the outcome on that topic. The goal is to find the outcome with majority support with the largest number of matches. We consider the ratio between this number and the number of matches of the overall best outcome which may not have majority support. We try to find the maximum ratio such that an outcome with majority support and this ratio of matches compared to the overall best is guaranteed to exist. For 3 topics, we show this ratio to be 5/6≈0.83. In general, we prove an upper bound that comes arbitrarily close to 2√6-4≈0.90 as t tends to infinity. Furthermore, we numerically compute a better upper and a non-matching lower bound in the relevant range for t.},
  archive   = {C_AAMAS},
  author    = {Fritsch, Robin and Wattenhofer, Roger},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {436–444},
  title     = {The price of majority support},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535900},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ensemble and incremental learning for norm violation
detection. <em>AAMAS</em>, 427–435. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The use of norms to guide and coordinate interactions has gained tremendous attention in the multiagent community. However, as the interest moves towards dynamic socio-technical systems, where human and software agents interact and interactions are required to adapt to the human&#39;s changing needs, new challenges arise. For instance, different agents (human or software) might not have the same understanding of what it means to violate a norm (e.g., what characterizes hate speech), or that their understanding of a norm might change over time (e.g., what constitutes an acceptable response time). The challenge is to address these issues by learning the meaning of a norm violation from the limited interaction data. For this, we propose a framework that learns and updates the meaning of a norm violation from interaction data through a combination of ensemble and incremental learning techniques. Ensemble learning handles the imbalance class distribution of the interaction stream, and incremental learning is used to continuously update the ensemble models as community members interact, which is an essential feature to keep the ensemble models in accordance with the latest community view on the meaning of norm violation. We evaluate the proposed approach in the use case of Wikipedia article edits, where interactions revolve around editing articles and the norm in question is prohibiting vandalism. Results show that the proposed framework can learn the meaning of a norm violation in a setting with data imbalance and concept drift.},
  archive   = {C_AAMAS},
  author    = {Freitas dos Santos, Thiago and Osman, Nardine and Schorlemmer, Marco},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {427–435},
  title     = {Ensemble and incremental learning for norm violation detection},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535899},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A path-following polynomial equations systems approach for
computing nash equilibria. <em>AAMAS</em>, 418–426. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a path-following combinatorial framework based on systems of polynomial equations to compute a mixed Nash equilibrium in N-person games. We provide the first detailed implementable description of Wilson&#39;s path-following method, extending Lemke-Howson&#39;s algorithm to N-person games and handling degenerate games. Our approach resembles, in some respects, support enumeration methods. We thus compare both approaches, theoretically and experimentally. Then, we show that the path-following approach allows to deal with a large family of succinctly expressed games: hypergraphical games, graphical games and polymatrix games. The described algorithms have been implemented in Python, making use of Sagemath libraries to solve systems of polynomial equations, allowing an experimental comparison of the different combinatorial approaches on a large variety of games.},
  archive   = {C_AAMAS},
  author    = {Fargier, H\&#39;{e}l\`{e}ne and Jourdan, Paul and Sabbadin, R\&#39;{e}gis},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {418–426},
  title     = {A path-following polynomial equations systems approach for computing nash equilibria},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535898},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Welfare vs. Representation in participatory budgeting.
<em>AAMAS</em>, 409–417. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Participatory budgeting (PB) is a democratic process for allocating funds to projects based on the votes of members of the community. Different rules have been used to aggregate participants&#39; votes.A recent paper by Lackner and Skowron [12] studied the tradeoff between notions of social welfare and representation in the multi-winner voting, which is a special case of participatory budgeting with identical project costs. But there is little understanding of this trade-off in the more general PB setting. This paper provides a theoretical and empirical study of the worst-case guarantees of several common rules to better understand the trade-off between social welfare and representation. We show that many of the guarantees from the multi-winner setting do not generalize to the PB setting, and that the introduction of costs leads to substantially worse guarantees, thereby exacerbating the welfare-representation trade-off. We further study how the requirement of proportionality over voting rules effects the guarantees on social welfare and representation. We study the latter point also empirically, both on real and synthetic datasets. We show that variants of the recently suggested voting rule Rule-X (which satisfies proportionality) do very well in practice both with respect to social welfare and representation.},
  archive   = {C_AAMAS},
  author    = {Fairstein, Roy and Vilenchik, Dan and Meir, Reshef and Gal, Kobi},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {409–417},
  title     = {Welfare vs. representation in participatory budgeting},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535897},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Betweenness centrality in multi-agent path finding.
<em>AAMAS</em>, 400–408. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-Agent Path Finding (MAPF) is a well studied problem with many existing optimal algorithms capable of solving a wide variety of instances, each with its own strengths and weaknesses. While for some instances the fastest algorithm can be easily determined, not enough is known about their performance to predict the fastest algorithm for every MAPF instance, or what makes some instances more difficult than others. There is no clear answer for which features dominate the hardness of MAPF instances. In this work, we study how betweenness centrality affects the empirical difficulty of MAPF instances. To that end, we benchmark the largest and most complete optimal MAPF algorithm portfolio to date. We analyze the algorithms&#39; performance independently and as part of the portfolio, and discuss how betweenness centrality can be used to improve estimations of algorithm performance on a given instance of MAPF.},
  archive   = {C_AAMAS},
  author    = {Ewing, Eric and Ren, Jingyao and Kansara, Dhvani and Sathiyanarayanan, Vikraman and Ayanian, Nora},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {400–408},
  title     = {Betweenness centrality in multi-agent path finding},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535896},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Facility location with approval preferences:
Strategyproofness and fairness. <em>AAMAS</em>, 391–399. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We develop a formal model of multiwinner facility location with approval preferences in one dimension: there is a set of facilities, a set of potential locations, and the goal is to build k facilities at these locations. Agents have approval preferences over &#39;facility, location&#39; pairs, and may misreport their preferences if they can benefit from doing so. We consider both unit-demand agents and agents with additive demands, and the social objectives of coverage and utilitarian welfare. We ask whether these social objectives can be satisfied in a computationally efficient and strategyproof way. We also initiate the study of proportional representation in the context of facility location. We show that the axiom of justified representation, which is used to capture proportionality in multiwinner voting with approval preferences, is not well-suited for the facility location setting, and provide a relaxation of this axiom that can handle incompatibilities and may be of broader interest.},
  archive   = {C_AAMAS},
  author    = {Elkind, Edith and Li, Minming and Zhou, Houyu},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {391–399},
  title     = {Facility location with approval preferences: Strategyproofness and fairness},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535895},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scalable multi-agent model-based reinforcement learning.
<em>AAMAS</em>, 381–390. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent Multi-Agent Reinforcement Learning (MARL) literature has been largely focused on Centralized Training with Decentralized Execution (CTDE) paradigm. CTDE has been a dominant approach for both cooperative and mixed environments due to its capability to efficiently train decentralized policies. While in mixed environments full autonomy of the agents can be a desirable outcome, cooperative environments allow agents to share information to facilitate coordination. Approaches that leverage this technique are usually referred as communication methods, as full autonomy of agents is compromised for better performance. Although communication approaches have shown impressive results, they do not fully leverage this additional information during training phase. In this paper, we propose a new method called MAMBA which utilizes Model-Based Reinforcement Learning (MBRL) to further leverage centralized training in cooperative environments. We argue that communication between agents is enough to sustain a world model for each agent during execution phase while imaginary rollouts can be used for training, removing the necessity to interact with the environment. These properties yield sample efficient algorithm that can scale gracefully with the number of agents. We empirically confirm that MAMBA achieves good performance while reducing the number of interactions with the environment up to an orders of magnitude compared to Model-Free state-of-the-art approaches in challenging domains of SMAC and Flatland.},
  archive   = {C_AAMAS},
  author    = {Egorov, Vladimir and Shpilman, Alexei},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {381–390},
  title     = {Scalable multi-agent model-based reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535894},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). How to fairly allocate easy and difficult chores.
<em>AAMAS</em>, 372–380. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A major open question in fair allocation of indivisible items is whether there always exists an allocation of chores that is Pareto optimal (PO) and envy-free up to one item (EF1). We answer this question affirmatively for the natural class of bivalued utilities, where each agent partitions the chores into easy and difficult ones, and has cost p &amp;gt; 1 for chores that are difficult for her and cost 1 for chores that are easy for her. Such an allocation can be found in polynomial time using an algorithm based on the Fisher market.We also show that for a slightly broader class of utilities, where each agent i can have a potentially different integer pi, an allocation that is maximin share fair (MMS) always exists and can be computed in polynomial time, provided that each pi is an integer. Our MMS arguments also hold when allocating goods instead of chores, and extend to another natural class of utilities, namely weakly lexicographic utilities.},
  archive   = {C_AAMAS},
  author    = {Ebadian, Soroush and Peters, Dominik and Shah, Nisarg},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {372–380},
  title     = {How to fairly allocate easy and difficult chores},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535893},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiagent dynamics of gradual argumentation semantics.
<em>AAMAS</em>, 363–371. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the abstract argumentation setting, gradual semantics have been proposed to assess the individual strength of arguments. A number of such semantics have been proposed recently, and their formal properties have been studied. While these semantics are sometimes motivated by their better adequacy to capture debates, their behaviour in such multiagent settings is largely unexplored. In this paper, we undertake a study of the multiagent dynamics of a standard gradual semantics. We propose a simple protocol, where agents exchange arguments in order to provide a collective evaluation of the value of a given argument (i.e an issue), and may learn new arguments from the other agents, as well as an extended version allowing votes. The debate proceeds following a better response dynamics. We study how the value of the issue and the agents opinion evolve, depending on various parameters of this setting.},
  archive   = {C_AAMAS},
  author    = {Dupuis de Tarl\&#39;{e}, Louise and Bonzon, Elise and Maudet, Nicolas},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {363–371},
  title     = {Multiagent dynamics of gradual argumentation semantics},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535892},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient approximation algorithms for the inverse semivalue
problem. <em>AAMAS</em>, 354–362. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Weighted voting games are typically used to model situations where a number of agents vote against or for a proposal. In such games, a proposal is accepted if a weighted sum of the votes exceeds a specified threshold. As the influence of a player over the outcome is not in general proportional to her assigned weight, various power indices have been proposed to measure each player&#39;s influence. The inverse power index problem is the problem of designing a weighted voting game that achieves a set of desirable influences as they are measured by a predefined power index. Recent work has shown that exactly solving the inverse power index problem is computationally intractable when the power index is in the class of semivalues --- a broad class that includes the popular Shapley value and Banzhaf index. In this work, we design efficient approximation algorithms for the inverse semivalue problem. We develop a unified methodology that leads to computationally efficient algorithms that solve the inverse semivalue problem to any desired accuracy. We perform an extensive experimental evaluation of our algorithms on both synthetic and real inputs. Our experiments show that our algorithms are scalable and achieve higher accuracy compared to previous methods in the literature.},
  archive   = {C_AAMAS},
  author    = {Diakonikolas, Ilias and Pavlou, Chrystalla and Peebles, John and Stewart, Alistair},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {354–362},
  title     = {Efficient approximation algorithms for the inverse semivalue problem},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535891},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Budgeted combinatorial multi-armed bandits. <em>AAMAS</em>,
345–353. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider a budgeted combinatorial multi-armed bandit setting where, in every round, the algorithm selects a super-arm consisting of one or more arms. The goal is to minimize the total expected regret after all rounds within a limited budget. Existing techniques in this literature either fix the budget per round or fix the number of arms pulled in each round. Our setting is more general where based on the remaining budget and remaining number of rounds, the algorithm can decide how many arms to be pulled in each round. First, we propose CBwK-Greedy-UCB algorithm, which uses a greedy technique, CBwK-Greedy, to allocate the arms to the rounds. Next, we propose a reduction of this problem to Bandits with Knapsacks (BwK) with a single pull. With this reduction, we propose CBwK-LP-UCB that uses PrimalDualBwK ingeniously. We rigorously prove regret bounds for CBwK-LP-UCB. We experimentally compare the two algorithms and observe that CBwK-Greedy-UCB performs incrementally better than CBwK-LP-UCB. We also show that for very high budgets, the regret goes to zero.},
  archive   = {C_AAMAS},
  author    = {Das, Debojit and Jain, Shweta and Gujar, Sujit},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {345–353},
  title     = {Budgeted combinatorial multi-armed bandits},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535890},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computation and bribery of voting power in delegative simple
games. <em>AAMAS</em>, 336–344. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Following Zhang and Grossi~(AAAI 2021), we study in more depth a variant of weighted voting games in which agents&#39; weights are induced by a transitive support structure. This class of simple games is notably well suited to study the relative importance of agents in the liquid democracy framework. We first propose a pseudo-polynomial time algorithm to compute the Banzhaf and Shapley-Shubik indices for this class of game. Then, we study a bribery problem, in which one tries to maximize/minimize the voting power/weight of a given agent by changing the support structure under a budget constraint. We show that these problems are computationally hard and provide several parameterized complexity results.},
  archive   = {C_AAMAS},
  author    = {D&#39;Angelo, Gianlorenzo and Delfaraz, Esmaeil and Gilbert, Hugo},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {336–344},
  title     = {Computation and bribery of voting power in delegative simple games},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535889},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A distributed differentially private algorithm for resource
allocation in unboundedly large settings. <em>AAMAS</em>, 327–335. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a practical and scalable algorithm (PALMA) for solving one of the fundamental problems of multi-agent systems -- finding matches and allocations -- in unboundedly large settings (e.g., resource allocation in urban environments, mobility-on-demand systems, etc.), while providing strong worst-case privacy guarantees. PALMA is decentralized, runs on-device, requires no inter-agent communication, and converges in constant time under reasonable assumptions. We evaluate PALMA in a mobility-on-demand and a paper assignment scenario, using real data in both, and demonstrate that it provides a strong level of privacy ($varepsilon \l{}eq 1$ and median as low as $varepsilon = 0.5$ across agents) and high-quality matchings (up to $86\%$ of the non-private optimal, outperforming even the privacy-preserving centralized maximum-weight matching baseline).},
  archive   = {C_AAMAS},
  author    = {Danassis, Panayiotis and Triastcyn, Aleksei and Faltings, Boi},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {327–335},
  title     = {A distributed differentially private algorithm for resource allocation in unboundedly large settings},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535888},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Poincaré-bendixson limit sets in multi-agent learning.
<em>AAMAS</em>, 318–326. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A key challenge of evolutionary game theory and multi-agent learning is to characterize the limit behavior of game dynamics. Whereas convergence is often a property of learning algorithms in games satisfying a particular reward structure (e.g., zero-sum games), even basic learning models, such as the replicator dynamics, are not guaranteed to converge for general payoffs. Worse yet, chaotic behavior is possible even in rather simple games, such as variants of the Rock-Paper-Scissors game. Although chaotic behavior in learning dynamics can be precluded by the celebrated Poincar\&#39;{e}-Bendixson theorem, it is only applicable to low-dimensional settings. Are there other characteristics of a game that can force regularity in the limit sets of learning? We show that behavior consistent with the Poincar\&#39;{e}-Bendixson theorem (limit cycles, but no chaotic attractor) can follow purely from the topological structure of the interaction graph, even for high-dimensional settings with an arbitrary number of players and arbitrary payoff matrices. We prove our result for a wide class of follow-the-regularized leader (FoReL) dynamics, which generalize replicator dynamics, for binary games characterized interaction graphs where the payoffs of each player are only affected by one other player (i.e., interaction graphs of indegree one). Since chaos occurs already in games with only two players and three strategies, this class of non-chaotic games may be considered maximal. Moreover, we provide simple conditions under which such behavior translates into efficiency guarantees, implying that FoReL learning achieves time-averaged sum of payoffs at least as good as that of a Nash equilibrium, thereby connecting the topology of the dynamics to social-welfare analysis.},
  archive   = {C_AAMAS},
  author    = {Czechowski, Aleksander and Piliouras, Georgios},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {318–326},
  title     = {Poincar\&#39;{e}-bendixson limit sets in multi-agent learning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535887},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Three-dimensional popular matching with cyclic preferences.
<em>AAMAS</em>, 309–317. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Two actively researched problem settings in matchings under preferences are popular matchings and the three-dimensional stable matching problem with cyclic preferences. In this paper, we apply the optimality notion of the first topic to the input characteristics of the second one. We investigate the connection between stability, popularity, and their strict variants, strong stability and strong popularity in three-dimensional instances with cyclic preferences. Furthermore, we also derive results on the complexity of these problems when the preferences are derived from master lists.},
  archive   = {C_AAMAS},
  author    = {Cseh, \&#39;{A}gnes and Peters, Jannik},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {309–317},
  title     = {Three-dimensional popular matching with cyclic preferences},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535886},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pareto optimal and popular house allocation with lower and
upper quotas. <em>AAMAS</em>, 300–308. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the house allocation problem with lower and upper quotas, we are given a set of applicants and a set of projects. Each applicant has a strictly ordered preference list over the projects she finds acceptable, while the projects are equipped with a lower and an upper quota. A feasible matching assigns the applicants to the projects in such a way that a project is either matched to no applicant or to a number of applicants between its lower and upper quota.In this model we study two classic optimality concepts: Pareto optimality and popularity. We show that finding a popular matching is hard even if the maximum lower quota is 2 and that finding a perfect Pareto optimal matching, verifying Pareto optimality, and verifying popularity are all NP-complete even if the maximum lower quota is 3. We complement the last three negative results by showing that the problems become polynomial-time solvable when the maximum lower quota is 2, thereby answering two open questions of Cechl\&#39;{a}rov\&#39;{a} and Fleiner. Finally, we also study the parameterized complexity of all four mentioned problems.},
  archive   = {C_AAMAS},
  author    = {Cseh, \&#39;{A}gnes and Friedrich, Tobias and Peters, Jannik},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {300–308},
  title     = {Pareto optimal and popular house allocation with lower and upper quotas},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535885},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimizing multi-agent coordination via hierarchical graph
probabilistic recursive reasoning. <em>AAMAS</em>, 290–299. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent reinforcement learning (MARL) requires coordination by some means of interaction between agents to efficiently solve tasks. Interaction graphs allow reasoning about joint actions based on the local structure of interactions, but they disregard the potential impact of an agent&#39;s action on its neighbors&#39; behaviors, which could rapidly alter in dynamic settings. In this paper, we thus present a novel perspective on opponent modeling in domains with only local interactions using (level-1) Graph Probabilistic Recursive Reasoning (GrPR2). Unlike previous work on recursive reasoning, each agent iteratively best-responds to other agents&#39; policies over all possible local interactions. Agents&#39; policies are approximated via a variational Bayes scheme for capturing their uncertainties, and we prove that an induced variant of Q-learning converges under self-play when there exists only one Nash equilibrium. In cooperative settings, we further devise a variational lower bound on the likelihood of each agent&#39;s optimality. Opposed to other models, optimizing the resulting objective prevents each agent from attaining an unrealistic modelling of others, and yields an exact tabular Q-iteration method that holds convergence guarantees. Then, we deepen the recursion to level-k via Cognitive Hierarchy GrPR2 (GrPR2-CH), which lets each level-k player best-respond to a mixture of strictly lower levels in the hierarchy. We prove that: (1) level-3 reasoning is the optimal hierarchical level, maximizing each agent&#39;s expected return; and (2) the weak spot of the classical CH models is that 0-level is uniformly distributed, as it may introduce policy bias. Finally, we propose a practical actor-critic scheme, and illustrate that GrPR2-CH outperforms strong MARL baselines in the particle environment.},
  archive   = {C_AAMAS},
  author    = {Cohen, Saar and Agmon, Noa},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {290–299},
  title     = {Optimizing multi-agent coordination via hierarchical graph probabilistic recursive reasoning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535884},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pippi: Practical protocol instantiation. <em>AAMAS</em>,
281–289. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A protocol specifies interactions between roles, which together constitute a multiagent system (MAS). Enacting a protocol presupposes that agents are bound to the its roles. Existing protocol-based approaches, however, do not adequately treat the practical aspects of how roles bindings come about.Pippi addresses this problem of MAS instantiation. It proposes the notion of a metaprotocol, enacting which instantiates a MAS suitable for enacting a given protocol. Pippi demonstrates the subtleties involved in instantiating MAS arising from protocol composition, correlation, and decentralization. To address these subtleties and further support practical application patterns, we introduce an enhanced protocol language, with support for parameter types (including role and protocol typed parameters, for metaprotocols), interface flexibility, and binding constraints. We discuss the realization of our approach through an extended agent architecture, including the novel concept of a MAS adapter for contact management. We evaluate Pippi&#39;s expressiveness by demonstrating common patterns for agent discovery.},
  archive   = {C_AAMAS},
  author    = {Christie, Samuel H. and Chopra, Amit K. and Singh, Munindar P.},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {281–289},
  title     = {Pippi: Practical protocol instantiation},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535883},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Coordinated multi-agent pathfinding for drones and trucks
over road networks. <em>AAMAS</em>, 272–280. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the problem of routing a team of drones and trucks over large-scale urban road networks. To conserve their limited flight energy, drones can use trucks as temporary modes of transit en route to their own destinations. Such coordination can yield significant savings in total vehicle distance traveled, i.e., truck travel distance and drone flight distance, compared to operating drones and trucks independently. But it comes at the potentially prohibitive computational cost of deciding which trucks and drones should coordinate and when and where it is most beneficial to do so. We tackle this fundamental trade-off by decoupling our overall intractable problem into tractable sub-problems that we solve stage-wise. The first stage solves only for trucks, by computing paths that make them more likely to be useful transit options for drones. The second stage solves only for drones, by routing them over a composite of the road network and the transit network defined by truck paths from the first stage. We design a comprehensive algorithmic framework that frames each stage as a multi-agent path-finding problem and implement two distinct methods for solving them. We evaluate our approach on extensive simulations with up to 100 agents on the real-world Manhattan road network containing nearly 4500 vertices and 10000 edges. Our framework saves on more than 50\% of vehicle distance traveled compared to independently solving for trucks and drones, and computes solutions for all settings within 5 minutes on commodity hardware.},
  archive   = {C_AAMAS},
  author    = {Choudhury, Shushman and Solovey, Kiril and Kochenderfer, Mykel and Pavone, Marco},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {272–280},
  title     = {Coordinated multi-agent pathfinding for drones and trucks over road networks},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535882},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simulating multiwinner voting rules in judgment aggregation.
<em>AAMAS</em>, 263–271. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We simulate voting rules for multiwinner elections in a model of judgment aggregation that distinguishes between rationality and feasibility constraints. These constraints restrict the structure of the individual judgments and of the collective outcome computed by the rule, respectively. We extend known results regarding the simulation of single-winner voting rules to the multiwinner setting, both for elections with ordinal preferences and for elections with approval-based preferences. This not only provides us with a new tool to analyse multiwinner elections, but it also suggests the definition of new judgment aggregation rules, by generalising some of the principles at the core of well-known multiwinner voting rules to this richer setting. We explore this opportunity with regards to the principle of proportionality. Finally, in view of the computational difficulty associated with many judgment aggregation rules, we investigate the computational complexity of our embeddings and of the new judgment aggregation rules we put forward.},
  archive   = {C_AAMAS},
  author    = {Chingoma, Julian and Endriss, Ulle and de Haan, Ronald},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {263–271},
  title     = {Simulating multiwinner voting rules in judgment aggregation},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535881},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Individual-level inverse reinforcement learning for mean
field games. <em>AAMAS</em>, 253–262. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The recent mean field game (MFG) formalism has enabled the application of inverse reinforcement learning (IRL) methods in large-scale multi-agent systems, with the goal of inferring reward signals that can explain demonstrated behaviours of large populations. The existing IRL methods for MFGs are built upon reducing an MFG to a Markov decision process (MDP) defined on the collective behaviours and average rewards of the population. However, this paper reveals that the reduction from MFG to MDP holds only for the fully cooperative setting. This limitation invalidates existing IRL methods on MFGs with non-cooperative environments. To measure more general behaviours in large populations, we study the use of individual behaviours to infer ground-truth reward functions for MFGs. We propose Mean Field IRL (MFIRL), the first dedicated IRL framework for MFGs that can handle both cooperative and non-cooperative environments. Based on this theoretically justified framework, we develop a practical algorithm effective for MFGs with unknown dynamics. We evaluate MFIRL on both cooperative and mixed cooperative-competitive scenarios with many agents. Results demonstrate that MFIRL excels in reward recovery, sample efficiency and robustness in the face of changing dynamics.},
  archive   = {C_AAMAS},
  author    = {Chen, Yang and Zhang, Libo and Liu, Jiamou and Hu, Shuyue},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {253–262},
  title     = {Individual-level inverse reinforcement learning for mean field games},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535880},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Anomaly guided policy learning from imperfect
demonstrations. <em>AAMAS</em>, 244–252. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning from Demonstrations (LfD) refers to using expert demonstrations combined with the reward information given by the environment to jointly guide the learning of policy in Reinforcement Learning. Previous LfD methods usually assume that provided demonstrations are perfect., while in real-world applications, demonstrations are often collected from multiple sources, which may contain imperfect ones. In this work, we aim to deal with the latter situation, i.e., Learning from Imperfect Demonstrations (LfID), where demonstrations only include trajectories with state-action pairs. To this end, two challenges need to be solved: evaluation for the demonstrations and calibration for the bonus model. Both challenges can be more severe in sparse reward environments, since the exploration problem will appear while learning. In this work, we focus on bridging the exploration and LfID problems in view of anomaly detection, and further proposing AGPO method to deal with these problems. Compared with state-of-the-art methods, empirical studies on some challenging continuous control benchmarks show the superiority of AGPO in this scenario.},
  archive   = {C_AAMAS},
  author    = {Chen, Zi-Xuan and Cai, Xin-Qiang and Jiang, Yuan and Zhou, Zhi-Hua},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {244–252},
  title     = {Anomaly guided policy learning from imperfect demonstrations},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535879},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Best-response bayesian reinforcement learning with
bayes-adaptive POMDPs for centaurs. <em>AAMAS</em>, 235–243. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Centaurs are half-human, half-AI decision-makers where the AI&#39;s goal is to complement the human. To do so, the AI must be able to recognize the goals and constraints of the human and have the means to help them. We present a novel formulation of the interaction between the human and the AI as a sequential game where the agents are modelled using Bayesian best-response models. We show that in this case the AI&#39;s problem of helping bounded-rational humans make better decisions reduces to a Bayes-adaptive POMDP. In our simulated experiments, we consider an instantiation of our framework for humans who are subjectively optimistic about the AI&#39;s future behaviour. Our results show that when equipped with a model of the human, the AI can infer the human&#39;s bounds and nudge them towards better decisions. We discuss ways in which the machine can learn to improve upon its own limitations as well with the help of the human. We identify a novel trade-off for centaurs in partially observable tasks: for the AI&#39;s actions to be acceptable to the human, the machine must make sure their beliefs are sufficiently aligned, but aligning beliefs might be costly. We present a preliminary theoretical analysis of this trade-off and its dependence on task structure.},
  archive   = {C_AAMAS},
  author    = {\c{C}elikok, Mustafa Mert and Oliehoek, Frans A. and Kaski, Samuel},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {235–243},
  title     = {Best-response bayesian reinforcement learning with bayes-adaptive POMDPs for centaurs},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535878},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian persuasion meets mechanism design: Going beyond
intractability with type reporting. <em>AAMAS</em>, 226–234. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bayesian persuasion studies how an informed sender should partially disclose information so as to influence the behavior of self-interested receivers. In the last years, a growing attention has been devoted to relaxing the assumption that the sender perfectly knows receiver&#39;s payoffs. The first crucial step towards such an achievement is to study settings where each receiver&#39;s payoffs depend on their unknown type, which is randomly determined by a known finite-supported probability distribution. This begets considerable computational challenges, as computing a sender-optimal signaling scheme is inapproximable up to within any constant factor, even in basic settings with a single receiver. In this work, we circumvent this issue by leveraging ideas from mechanism design. In particular, we introduce a type reporting step in which the receiver is asked to report their type to the sender, after the latter has committed to a menu defining a signaling scheme for each possible receiver&#39;s type. Surprisingly, we prove that, with a single receiver, the addition of this type reporting stage makes the sender&#39;s computational problem tractable. Then, we extend our Bayesian persuasion framework with type reporting to settings with multiple receivers, focusing on the widely-studied case of no inter-agent externalities and binary actions. In such setting, we show that it is possible to find a sender-optimal solution in polynomial-time by means of the ellipsoid method, given access to a suitable polynomial-time separation oracle. This can be implemented for supermodular and anonymous sender&#39;s utility functions. As for the case of submodular sender&#39;s utility functions, we first approximately cast the sender&#39;s problem into a linearly-constrained mathematical program whose objective function is the multi-linear extension of the sender&#39;s utility. Then, we show how to find in polynomial-time an approximate solution to the program by means of a continuous greedy algorithm. This provides a (1 -1/e)-approximation to the problem, which is tight.},
  archive   = {C_AAMAS},
  author    = {Castiglioni, Matteo and Marchesi, Alberto and Gatti, Nicola},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {226–234},
  title     = {Bayesian persuasion meets mechanism design: Going beyond intractability with type reporting},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535877},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Planning, execution, and adaptation for multi-robot systems
using probabilistic and temporal planning. <em>AAMAS</em>, 217–225. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Planning for multi-robot coordination during long horizon missions in complex environments need to consider resources, temporal constraints, and uncertainty. This could be computationally expensive and impractical for online planning and execution. We propose a decoupled framework to address this. At the high-level, we plan for multi-robot missions that require coordination amongst robots considering temporal and numeric constraints. The temporal plan is decomposed into low-level plans for individual robots. At the low-level, we perform online learning and adaptation due to unexpected probabilistic outcomes to achieve mission goals. Our framework learns over time to improve the performance by (1) updating the learned domain model to reduce model prediction errors and (2) constraining the robot&#39;s capabilities which in turn improves goal allocation. The approach provides a solution to planning problems that require long-term robot operability. We demonstrate the performance of our approach via experiments involving a fleet of heterogeneous robots.},
  archive   = {C_AAMAS},
  author    = {Carreno, Yaniel and Ng, Jun Hao Alvin and Petillot, Yvan and Petrick, Ron},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {217–225},
  title     = {Planning, execution, and adaptation for multi-robot systems using probabilistic and temporal planning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535876},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Beyond cake cutting: Allocating homogeneous divisible goods.
<em>AAMAS</em>, 208–216. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The problem of fair division known as &quot;cake cutting&quot; has been the focus of multiple papers spanning several decades. The most prominent problem in this line of work has been to bound the query complexity of computing an envy-free outcome in the Robertson-Webb query model. However, the root of this problem&#39;s complexity is somewhat artificial: the agents&#39; values are assumed to be additive across different pieces of the &quot;cake&quot; but infinitely complicated within each piece. This is unrealistic in most of the motivating examples, where the cake represents a finite collection of homogeneous goods.We address this issue by introducing a fair division model that more accurately captures these applications: the value that an agent gains from a given good depends only on the amount of the good they receive, yet it can be an arbitrary function of this amount, allowing the agents to express preferences that go beyond standard cake cutting. In this model, we study the query complexity of computing allocations that are not just envy-free, but also approximately Pareto optimal among all envy-free allocations. Using a novel flow-based approach, we show that we can encode the ex-post feasibility of randomized allocations via a polynomial number of constraints, which reduces our problem to solving a linear program.},
  archive   = {C_AAMAS},
  author    = {Caragiannis, Ioannis and Gkatzelis, Vasilis and Psomas, Alexandros and Schoepflin, Daniel},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {208–216},
  title     = {Beyond cake cutting: Allocating homogeneous divisible goods},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535875},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploiting causal structure for transportability in online,
multi-agent environments. <em>AAMAS</em>, 199–207. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous agents may encounter the transportability problem when they suffer performance deficits from training in an environment that differs in key respects from that in which they are deployed. Although a causal treatment of transportability has been studied in the data sciences, the present work expands its utility into online, multi-agent, reinforcement learning systems in which agents are capable of both experimenting within their own environments and observing the choices of agents in separate, potentially different ones. In order to accelerate learning, agents in these Multi-agent Transport (MAT) problems face the unique challenge of determining which agents are acting in similar environments, and if so, how to incorporate these observations into their policy. We propose and compare several agent policies that exploit local similarities between environments using causal selection diagrams, demonstrating that optimal policies are learned more quickly than in baseline agents that do not. Simulation results support the efficacy of these new agents in a novel variant of the Multi-Armed Bandit problem with MAT environments.},
  archive   = {C_AAMAS},
  author    = {Browne, Axel and Forney, Andrew},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {199–207},
  title     = {Exploiting causal structure for transportability in online, multi-agent environments},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535874},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fair stable matching meets correlated preferences.
<em>AAMAS</em>, 190–198. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Stable matching models are widely used in market design, school admission, and donor organ exchange. The classic Deferred Acceptance (DA) algorithm guarantees a stable matching that is optimal for one side (say men) and pessimal for the other (say women). A sex-equal stable matching aims at providing a fair solution to this problem. We demonstrate that under a class of correlated preferences, the DA algorithm either returns a sex-equal solution or has a very low sex-equality cost.},
  archive   = {C_AAMAS},
  author    = {Brilliantova, Angelina and Hosseini, Hadi},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {190–198},
  title     = {Fair stable matching meets correlated preferences},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535873},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Relaxed notions of condorcet-consistency and efficiency for
strategyproof social decision schemes. <em>AAMAS</em>, 181–189. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Social decision schemes (SDSs) map the preferences of a group of voters over some set of m alternatives to a probability distribution over the alternatives. A seminal characterization of strategyproof SDSs by Gibbard implies that there are no strategyproof Condorcet extensions and that only random dictatorships satisfy ex post efficiency and strategyproofness. The latter is known as the random dictatorship theorem. We relax Condorcet-consistency and ex post efficiency by introducing a lower bound on the probability of Condorcet winners and an upper bound on the probability of Pareto-dominated alternatives, respectively. We then show that the SDS that assigns probabilities proportional to Copeland scores is the only anonymous, neutral, and strategyproof SDS that can guarantee the Condorcet winner a probability of at least $2/m$. Moreover, no strategyproof SDS can exceed this bound, even when dropping anonymity and neutrality. Secondly, we prove a continuous strengthening of Gibbard&#39;s random dictatorship theorem: the less probability we put on Pareto-dominated alternatives, the closer to a random dictatorship is the resulting SDS. Finally, we show that the only anonymous, neutral, and strategyproof SDSs that maximize the probability of Condorcet winners while minimizing the probability of Pareto-dominated alternatives are mixtures of the uniform random dictatorship and the randomized Copeland rule.},
  archive   = {C_AAMAS},
  author    = {Brandt, Felix and Lederer, Patrick and Romen, Ren\&#39;{e}},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {181–189},
  title     = {Relaxed notions of condorcet-consistency and efficiency for strategyproof social decision schemes},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535872},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Explainability in multi-agent path/motion planning:
User-study-driven taxonomy and requirements. <em>AAMAS</em>, 172–180.
(<a href="https://dl.acm.org/doi/10.5555/3535850.3535871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-Agent Path Finding (MAPF) and Multi-Robot Motion Planning (MRMP) are complex problems to solve, analyze and build algorithms for. Automatically-generated explanations of algorithm output, by improving human understanding of the underlying problems and algorithms, could thus lead to better user experience, developer knowledge, and MAPF/MRMP algorithm designs. Explanations are contextual, however, and thus developers need a good understanding of the questions that can be asked about algorithm output, the kinds of explanations that exist, and the potential users and uses of explanations in MAPF/MRMP applications. In this paper we provide a first step towards establishing a taxonomy of explanations, and a list of requirements for the development of explainable MAPF/MRMP planners. We use interviews and a questionnaire with expert developers and industry practitioners to identify the kinds of questions, explanations, users, uses, and requirements of explanations that should be considered in the design of such explainable planners. Our insights cover a diverse set of applications: warehouse automation, computer games, and mining.},
  archive   = {C_AAMAS},
  author    = {Brandao, Martim and Mansouri, Masoumeh and Mohammed, Areeb and Luff, Paul and Coles, Amanda},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {172–180},
  title     = {Explainability in multi-agent Path/Motion planning: User-study-driven taxonomy and requirements},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535871},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Knowledge transmission and improvement across generations do
not need strong selection. <em>AAMAS</em>, 163–171. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Agents have been used for simulating cultural evolution and cultural evolution can be used as a model for artificial agents. Previous results have shown that horizontal, or intra-generation, knowledge transmission allows agents to improve the quality of their knowledge to a certain level. Moreover, variation generated through vertical, or inter-generation, transmission allows agents to exceed that level. Such results were obtained under specific conditions such as the drastic selection of agents allowed to transmit their knowledge, seeding the process with correct knowledge or introducing artificial noise during transmission. Here, we question the necessity of such measures and study their impact on the quality of transmitted knowledge. For that purpose, we combine the settings of two previous experiments and relax these conditions (no strong selection of teachers, no fully correct seed, no introduction of artificial noise). The rationale is that if interactions lead agents to improve their overall knowledge quality, this should be sufficient to ensure correct knowledge transmission, and that transmission mechanisms are sufficiently imperfect to produce variation. In this setting, we confirm that vertical transmission improves on horizontal transmission even without drastic selection and oriented learning. We also show that horizontal transmission is able to compensate for the lack of parent selection if it is maintained for long enough. This means that it is not necessary to take the most successful agents as teachers, neither in vertical nor horizontal transmission, to cumulatively improve knowledge.},
  archive   = {C_AAMAS},
  author    = {Bourahla, Yasser and Atencia, Manuel and Euzenat, J\&#39;{e}r\^{o}me},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {163–171},
  title     = {Knowledge transmission and improvement across generations do not need strong selection},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535870},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Little house (seat) on the prairie: Compactness,
gerrymandering, and population distribution. <em>AAMAS</em>, 154–162.
(<a href="https://dl.acm.org/doi/10.5555/3535850.3535869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Gerrymandering is the process of creating electoral districts for partisan advantage, allowing a party to win more seats than what is reasonable for their vote. While research on gerrymandering has recently grown, many issues are still not fully understood such as what influences the degree to which a party can gerrymander and what techniques can be used to counter it. One commonly suggested (and, in some US states, mandated) requirement is that districts be &quot;geographically compact&quot;. However, there are many competing compactness definitions and the impact of compactness on the gerrymandering abilities of the parties is not well understood. Also not well understood is how the growing urban-rural divide between supporters of different parties impacts redistricting.We develop a modular, scalable, and efficient algorithm that can design districts for various criteria. We confirm its effectiveness on several US states by pitting it against maps &quot;hand-drawn&quot; by political experts. Using real data from US political elections we use our algorithm to study the interaction between population distribution, partisanship, and geographic compactness. We find that compactness can lead to more fair plans (compared to implemented plans) and limit gerrymandering potential, but there is a consistent asymmetry where the party with rural supporters has an advantage. We also show there are plans which are fair from a partisan perspective, but they are far from optimally compact.},
  archive   = {C_AAMAS},
  author    = {Borodin, Allan and Lev, Omer and Shah, Nisarg and Strangway, Tyrone},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {154–162},
  title     = {Little house (Seat) on the prairie: Compactness, gerrymandering, and population distribution},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535869},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hierarchical bayesian process for inverse RL in
partially-controlled environments. <em>AAMAS</em>, 145–153. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots learning from observations in the real world may encounter objects or agents in the environment, other than the expert giving the demonstration, that cause nuisance observations. These confounding elements are typically removed in fully-controlled environments such as virtual simulations or lab settings. When complete removal is impossible the nuisance observations must be filtered out. However, identifying the sources of observations when large amounts of observations are made is difficult. To address this, we present a hierarchical Bayesian process that models both the expert&#39;s and the confounding elements&#39; observations thereby explicitly modeling the diverse observations a robot may receive. We extend an existing inverse reinforcement learning algorithm originally designed to work under partial occlusion of the expert to consider the diverse and noisy observations. In a simulated robotic produce-sorting domain containing both occlusion and confounding elements, we demonstrate the model&#39;s effectiveness. In particular, our technique outperforms several other comparative methods, second only to having perfect knowledge of the subject&#39;s trajectory.},
  archive   = {C_AAMAS},
  author    = {Bogert, Kenneth and Doshi, Prashant},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {145–153},
  title     = {A hierarchical bayesian process for inverse RL in partially-controlled environments},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535868},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Proportional representation in matching markets: Selecting
multiple matchings under dichotomous preferences. <em>AAMAS</em>,
136–144. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Given a set of agents with approval preferences over each other, we study the task of finding k matchings fairly representing everyone&#39;s preferences. We model the problem as an approval-based multiwinner election where the set of candidates consists of matchings of the agents, and agents&#39; preferences over each other are lifted to preferences over matchings. Due to the exponential number of candidates in such elections, standard algorithms for classical sequential voting rules (such as those proposed by Thiele and Phragm\&#39;{e}n) are rendered inefficient. We show that the computational tractability of these rules can be regained by exploiting the structure of the approval preferences. Moreover, we establish algorithmic results and axiomatic guarantees that go beyond those obtainable in the classical approval-based multiwinner setting: Assuming that approvals are symmetric, we show that Proportional Approval Voting (PAV), a well-established but computationally intractable voting rule, becomes polynomial-time computable, and that its sequential variant, which does not provide any proportionality guarantees in general, fulfills a rather strong guarantee known as extended justified representation. Some of our algorithmic results extend to other types of compactly representable elections with an exponential candidate space.},
  archive   = {C_AAMAS},
  author    = {Boehmer, Niclas and Brill, Markus and Schmidt-Kraepelin, Ulrike},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {136–144},
  title     = {Proportional representation in matching markets: Selecting multiple matchings under dichotomous preferences},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535867},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multivariate algorithmics for eliminating envy by donating
goods. <em>AAMAS</em>, 127–135. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fairly dividing a set of indivisible resources to a set of agents is of utmost importance in some applications. However, after an allocation has been implemented the preferences of agents might change and envy might arise. We study the following problem to cope with such situations: Given an allocation of indivisible resources to agents with additive utility-based preferences, is it possible to socially donate some of the resources (which means removing these resources from the allocation instance) such that the resulting modified allocation is envy-free (up to one good). We require that the number of deleted resources and/or the caused utilitarian welfare loss of the allocation are bounded. We conduct a thorough study of the (parameterized) computational complexity of this problem considering various natural and problem-specific parameters (e.g., the number of agents, the number of deleted resources, or the maximum number of resources assigned to an agent in the initial allocation) and different preference models, including unary and 0/1-valuations. In our studies, we obtain a rich set of (parameterized) tractability and intractability results and discover several surprising contrasts, for instance, between the two closely related fairness concepts envy-freeness and envy-freeness up to one good and between the influence of the parameters maximum number and welfare of the deleted resources.},
  archive   = {C_AAMAS},
  author    = {Boehmer, Niclas and Bredereck, Robert and Heeger, Klaus and Knop, Du\v{s}an and Luo, Junjie},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {127–135},
  title     = {Multivariate algorithmics for eliminating envy by donating goods},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535866},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interpretable preference-based reinforcement learning with
tree-structured reward functions. <em>AAMAS</em>, 118–126. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The potential of reinforcement learning (RL) to deliver aligned and performant agents is partially bottlenecked by the reward engineering problem. One alternative to heuristic trial-and-error is preference-based RL (PbRL), where a reward function is inferred from sparse human feedback. However, prior PbRL methods lack interpretability of the learned reward structure, which hampers the ability to assess robustness and alignment. We propose an online, active preference learning algorithm that constructs reward functions with the intrinsically interpretable, compositional structure of a tree. Using both synthetic and human-provided feedback, we demonstrate sample-efficient learning of tree-structured reward functions in several environments, then harness the enhanced interpretability to explore and debug for alignment.},
  archive   = {C_AAMAS},
  author    = {Bewley, Tom and Lecue, Freddy},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {118–126},
  title     = {Interpretable preference-based reinforcement learning with tree-structured reward functions},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535865},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Asynchronous opinion dynamics in social networks.
<em>AAMAS</em>, 109–117. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Opinion spreading in a society decides the fate of elections, the success of products, and the impact of political or social movements. The model by Hegselmann and Krause is a well-known theoretical model to study such opinion formation processes in social networks. In contrast to many other theoretical models, it does not converge towards a situation where all agents agree on the same opinion. Instead, it assumes that people find an opinion reasonable if and only if it is close to their own. The system converges towards a stable situation where agents sharing the same opinion form a cluster, and agents in different clusters do not influence each other. We focus on the social variant of the Hegselmann-Krause model where agents are connected by a social network and their opinions evolve in an iterative process. When activated, an agent adopts the average of the opinions of its neighbors having a similar opinion. By this, the set of influencing neighbors of an agent may change over time. To the best of our knowledge, social Hegselmann-Krause systems with asynchronous opinion updates have only been studied with the complete graph as social network. We show that such opinion dynamics with random agent activation are guaranteed to converge for any social network. We provide an upper bound of O(n|E|2 (ε/δ)2) on the expected number of opinion updates until convergence, where |E| is the number of edges of the social network. For the complete social network we show a bound of O(n3(n2 + (ε/δ)2)) that represents a major improvement over the previously best upper bound of O(n9 (ε/δ)2). Our bounds are complemented by simulations that indicate asymptotically matching lower bounds.},
  archive   = {C_AAMAS},
  author    = {Berenbrink, Petra and Hoefer, Martin and Kaaser, Dominik and Lenzner, Pascal and Rau, Malin and Schmand, Daniel},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {109–117},
  title     = {Asynchronous opinion dynamics in social networks},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535864},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quantitative group trust: A two-stage verification approach.
<em>AAMAS</em>, 100–108. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper is about modeling and verifying quantitative group trust. We present a formal analysis of this concept that allows us to express and reason about trust in multi-agent systems in a quantitative setting. We introduce GTL, the graded branching temporal logic that includes operators for quantitative aspects of trust within a group. A two-stage verification procedure of the logic is presented. The first stage is a transformation procedure, and the second stage is an indirect procedure that uses an existing model checking algorithm. Theoretical results about the soundness, completeness and complexity of the procedure are presented},
  archive   = {C_AAMAS},
  author    = {Bentahar, Jamal and Drawel, Nagat and Sadiki, Abdeladim},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {100–108},
  title     = {Quantitative group trust: A two-stage verification approach},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535863},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Agent-based modeling and simulation for malware spreading in
D2D networks. <em>AAMAS</em>, 91–99. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a new multi-agent model for simulating malware propagation in device-to-device (D2D) 5G networks. This model allows to understand and analyze mobile malware spreading dynamics in such highly dynamical networks. Additionally, we present a theoretical study to validate and benchmark our proposed approach for some basic scenarios that are less complicated to model mathematically and also to highlight the key parameters of the model. Our simulations identify critical thresholds for &quot;no propagation&quot; and for &quot;maximum malware propagation&quot; and make predictions on the malware spread velocity as well as device infection rates. To the best of our knowledge, this paper is the first study applying agent-based simulations for malware propagation in D2D.},
  archive   = {C_AAMAS},
  author    = {Benomar, Ziyad and Ghribi, Chaima and Cali, Elie and Hinsen, Alexander and Jahnel, Benedikt},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {91–99},
  title     = {Agent-based modeling and simulation for malware spreading in D2D networks},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535862},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computing balanced solutions for large international kidney
exchange schemes. <em>AAMAS</em>, 82–90. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To overcome incompatibility issues, kidney patients may swap their donors. In international kidney exchange programmes (IKEPs), countries merge their national patient-donor pools. We consider a recent credit system where in each round, countries are given an initial kidney transplant allocation which is adjusted by a credit function yielding a target allocation. The goal is to find a solution in the patient-donor compatibility graph that approaches the target allocation as closely as possible, to ensure long-term stability of the international pool. As solutions, we use maximum matchings that lexicographically minimize the country deviations from the target allocation. We first give a polynomial-time algorithm for computing such matchings. We then perform, for the first time, a computational study for a large number of countries. For the initial allocations we use, besides two easy-to-compute solution concepts, two classical concepts: the Shapley value and nucleolus. These are hard to compute, but by using state-of-the-art software we show that they are now within reach for IKEPs of up to fifteen countries. Our experiments show that using lexicographically minimal maximum matchings instead of ones that only minimize the largest deviation from the target allocation (as previously done) may make an IKEP up to 52\% more balanced.},
  archive   = {C_AAMAS},
  author    = {Benedek, M\&#39;{a}rton and Bir\&#39;{o}, P\&#39;{e}ter and Kern, Walter and Paulusma, Dani\&quot;{e}l},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {82–90},
  title     = {Computing balanced solutions for large international kidney exchange schemes},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535861},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). COPALZ: A computational model of pathological appraisal
biases for an interactive virtual alzheimer patient. <em>AAMAS</em>,
72–81. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Confronted with patients suffering from Alzheimer&#39;s disease, professional caregivers must be aware of their own non-verbal communication as well as the patient&#39;s non-verbal communication in order to respond to the behavioural disorders which are frequent in this disease (e.g. apathy, aggression, anxiety, etc...). Virtual patients, which are interactive animated characters that simulate the behaviors of a patient, are increasingly used to train caregivers to interact with patients. The virtual patient needs to dynamically generate multimodal expressions of pathological behaviors consistent with the pedagogical goals. In this article, we introduce COPALZ (COmputational model of Pathological appraisal biases for a virtual ALZheimer&#39;s patient), a new model that supports the generation of pathological behaviors identified as pedagogically relevant for caregivers interacting with Alzheimer&#39;s patients. This model is based on the theory of appraisal biases and on interaction data that we collected using a partially simulated version of our virtual patient with 31 caregivers. We describe initial evaluations and explain how we intend to evaluate the fully automatic virtual patient during training sessions.},
  archive   = {C_AAMAS},
  author    = {Benamara, Amine and Martin, Jean-Claude and Prigent, Elise and Chaby, Laurence and Chetouani, Mohamed and Zagdoun, Jean and Vanderstichel, H\&#39;{e}l\`{e}ne and Dacunha, S\&#39;{e}bastien and Ravenet, Brian},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {72–81},
  title     = {COPALZ: A computational model of pathological appraisal biases for an interactive virtual alzheimer patient},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535860},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reasoning about human-friendly strategies in repeated
keyword auctions. <em>AAMAS</em>, 62–71. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In online advertising, search engines sell ad placements for keywords continuously through auctions. This problem can be seen as an infinitely repeated game since the auction is executed whenever a user performs a query with the keyword. As advertisers may frequently change their bids, the game will have a large set of equilibria with potentially complex strategies. In this paper, we propose the use of natural strategies for reasoning in such setting as they are processable by artificial agents with limited memory and/or computational power as well as understandable by human users. To reach this goal, we introduce a quantitative version of Strategy Logic with natural strategies in the setting of imperfect information. In a first step, we show how to model strategies for repeated keyword auctions and take advantage of the model for proving properties evaluating this game. In a second step, we study the logic in relation to the distinguishing power, expressivity, and model-checking complexity for strategies with and without recall.},
  archive   = {C_AAMAS},
  author    = {Belardinelli, Francesco and Jamroga, Wojtek and Malvone, Vadim and Mittelmann, Munyque and Murano, Aniello and Perrussel, Laurent},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {62–71},
  title     = {Reasoning about human-friendly strategies in repeated keyword auctions},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535859},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-agent heterogeneous digital twin framework with
dynamic responsibility allocation for complex task simulation.
<em>AAMAS</em>, 53–61. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To become helpful assistants in our daily lives, robots must be able to understand the effects of their actions on their environment. A modern approach to this is the use of a physics simulation, where often very general simulation engines are utilized. As a result, specific modeling features, such as multi-contact simulation or fluid dynamics, may not be well represented. To improve the representativeness of simulations, we propose a framework for combining estimations of multiple heterogeneous simulations into a single one. The framework couples multiple simulations and reorganizes them based on semantically annotated action sequence information. While each object in the scene is always covered by a simulation, this simulation responsibility can be reassigned on-line. In this paper, we introduce the concept of the framework, describe the architecture, and demonstrate two example implementations. Eventually, we demonstrate how the framework can be used to simulate action executions on the humanoid robot Rollin&#39; Justin with the goal to extract the semantic state and how this information is used to assess whether an action sequence is executed successful or not.},
  archive   = {C_AAMAS},
  author    = {Bauer, Adrian Simon and K\&quot;{o}pken, Anne and Leidner, Daniel},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {53–61},
  title     = {Multi-agent heterogeneous digital twin framework with dynamic responsibility allocation for complex task simulation},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535858},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unbiased asymmetric reinforcement learning under partial
observability. <em>AAMAS</em>, 44–52. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In partially observable reinforcement learning, offline training gives access to latent information which is not available during online training and/or execution, such as the system state. Asymmetric actor-critic methods exploit such information by training a history-based policy via a state-based critic. However, many asymmetric methods lack theoretical foundation, and are only evaluated on limited domains. We examine the theory of asymmetric actor-critic methods which use state-based critics, and expose fundamental issues which undermine the validity of a common variant, and limit its ability to address partial observability. We propose an unbiased asymmetric actor-critic variant which is able to exploit state information while remaining theoretically sound, maintaining the validity of the policy gradient theorem, and introducing no bias and relatively low variance into the training process. An empirical evaluation performed on domains which exhibit significant partial observability confirms our analysis, demonstrating that unbiased asymmetric actor-critic converges to better policies and/or faster than symmetric and biased asymmetric baselines.},
  archive   = {C_AAMAS},
  author    = {Baisero, Andrea and Amato, Christopher},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {44–52},
  title     = {Unbiased asymmetric reinforcement learning under partial observability},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535857},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). State supervised steering function for sampling-based
kinodynamic planning. <em>AAMAS</em>, 35–43. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sampling-based motion planners such as RRT* and BIT*, when applied to kinodynamic motion planning, rely on steering functions to generate time-optimal solutions connecting sampled states. Implementing exact steering functions requires either analytical solutions to the time-optimal control problem, or nonlinear programming (NLP) solvers to solve the boundary value problem given the system&#39;s kinodynamic equations. Unfortunately, analytical solutions are unavailable for many real-world domains, and NLP solvers are prohibitively computationally expensive, hence fast and optimal kinodynamic motion planning remains an open problem. We provide a solution to this problem by introducing State Supervised Steering Function (S3F), a novel approach to learn time-optimal steering functions. S3F is able to produce near-optimal solutions to the steering function orders of magnitude faster than its NLP counterpart. Experiments conducted on three challenging robot domains show that RRT* using S3F significantly outperforms state-of-the-art planning approaches on both solution cost and runtime. We further provide a proof of probabilistic completeness of RRT* modified to use S3F.},
  archive   = {C_AAMAS},
  author    = {Atreya, Pranav and Biswas, Joydeep},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {35–43},
  title     = {State supervised steering function for sampling-based kinodynamic planning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535856},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hacking the colony: On the disruptive effect of misleading
pheromone and how to defend against it. <em>AAMAS</em>, 27–34. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ants have evolved to seek and retrieve food by leaving trails of pheromones. This mechanism has inspired several approaches to decentralized multi-robot coordination. However, in this paper, we show that pheromone trails are a fragile mechanism for coordination, and can be sabotaged to starve the colony. We introduce detractors: malicious agents that leave a misleading, but indistinguishable, trail of food pheromone to distract and trap cooperator ants in the nest. We analyze the effectiveness of detractors with respect to parameters such as evaporation rate of misleading pheromone and fraction of detractors in the colony. In addition, we propose a countermeasure to this attack by introducing a new type of pheromone: the cautionary pheromone. Cooperator ants secrete this type of pheromone atop existing food trails as a warning. When the cautionary pheromone intensity exceeds the food pheromone intensity, cooperator ants ignore overlapping food pheromone. We show that, despite its simplicity, this defense mechanism can limit, but not nullify, the effect of detractors. Ultimately, our work shows that pheromone-based coordination, while effective, is also fragile.},
  archive   = {C_AAMAS},
  author    = {Aswale, Ashay and L\&#39;{o}pez, Antonio and Ammartayakun, Aukkawut and Pinciroli, Carlo},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {27–34},
  title     = {Hacking the colony: On the disruptive effect of misleading pheromone and how to defend against it},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535855},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Be considerate: Avoiding negative side effects in
reinforcement learning. <em>AAMAS</em>, 18–26. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In sequential decision making -- whether it&#39;s realized with or without the benefit of a model -- objectives are often underspecified or incomplete. This gives discretion to the acting agent to realize the stated objective in ways that may result in undesirable outcomes, including inadvertently creating an unsafe environment or indirectly impacting the agency of humans or other agents that typically operate in the environment. In this paper, we explore how to build a reinforcement learning (RL) agent that contemplates the impact of its actions on the wellbeing and agency of others in the environment, most notably humans. We endow RL agents with the ability to contemplate such impact by augmenting their reward based on expectation of future return by others in the environment, providing different criteria for characterizing impact. We further endow these agents with the ability to differentially factor this impact into their decision making, manifesting behaviour that ranges from self-centred to self-less, as demonstrated by experiments in gridworld environments.},
  archive   = {C_AAMAS},
  author    = {Alizadeh Alamdari, Parand and Klassen, Toryn Q. and Toro Icarte, Rodrigo and McIlraith, Sheila A.},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {18–26},
  title     = {Be considerate: Avoiding negative side effects in reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535854},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-objective reinforcement learning with non-linear
scalarization. <em>AAMAS</em>, 9–17. (<a
href="https://dl.acm.org/doi/10.5555/3535850.3535853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-Objective Reinforcement Learning (MORL) setup naturally arises in many places where an agent optimizes multiple objectives. We consider the problem of MORL where multiple objectives are combined using a non-linear scalarization. We combine the vector objectives with a concave scalarization function and maximize this scalar objective. To work with the non-linear scalarization, in this paper, we propose a solution using steady-state occupancy measures and long-term average rewards. We show that when the scalarization function is element-wise increasing, the optimal policy for the scalarization is also Pareto optimal. To maximize the scalarized objective, we propose a model-based posterior sampling algorithm. Using a novel Bellman error analysis for infinite horizon MDPs based proof, we show that the proposed algorithm obtains a regret bound of ~O(LKDS√A/T) for K objectives, and L-Lipschitz continous scalarization function for MDP with S states, A actions, and diameter D. Additionally, we propose policy-gradient and actor-critic algorithms for MORL. For the policy gradient actor, we obtain the gradient using chain rule, and we learn different critics for each of the K objectives. Finally, we implement our algorithms on multiple environments including deep-sea treasure, and network scheduling setups to demonstrate that the proposed algorithms can optimize non-linear scalarization of multiple objectives.},
  archive   = {C_AAMAS},
  author    = {Agarwal, Mridul and Aggarwal, Vaneet and Lan, Tian},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {9–17},
  title     = {Multi-objective reinforcement learning with non-linear scalarization},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535853},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Using agent-based simulator to assess interventions against
COVID-19 in a small community generated from map data. <em>AAMAS</em>,
1–8. (<a href="https://dl.acm.org/doi/10.5555/3535850.3535852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {During the COVID-19 pandemic, governments have struggled to devise strategies to slow down the spread of the virus. This struggle happens because pandemics are complex scenarios with many unknown variables. In this context, simulated models are used to evaluate strategies for mitigating this and future pandemics. This paper proposes a simulator that analyses small communities by using real geographical data to model the road interactions and the agent&#39;s behaviors. Our simulator consists of three different modules: Environment, Mobility, and Infection module. The environment module recreates an area based on map data, including houses, restaurants, and roads. The mobility module determines the agents&#39; movement in the map based on their work schedule and needs, such as eating at restaurants, doing groceries, and going to work. The infection module simulates four cases of infection: on the road, at home, at a building, and off the map. We simulate the surrounding areas of the University of Tsukuba and design three intervention strategies, comparing them to a scenario without any intervention. The interventions are: 1) PCR testing and self-isolation if positive; 2) applying lockdown measures to restaurants and barbershops 3) closing grocery stores and restaurants and providing delivery instead. For all scenarios, we observe two areas where most infection happens: hubs, where people from different occupations can meet (e.g., restaurants), and non-hubs, where people with the same occupation meet (e.g., offices). The simulations show that most interventions reduce the total number of infected agents by a large margin. We observed that interventions targeting hubs (2-4) did not impact the infection at non-hubs. In addition, the intervention targeting people&#39;s behavior (1) ended up creating a cluster at the testing center.},
  archive   = {C_AAMAS},
  author    = {Abe, Mitsuteru and Tanaka, Fabio and Pereira Junior, Jair and Bogdanova, Anna and Sakurai, Tetsuya and Aranha, Claus},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1–8},
  title     = {Using agent-based simulator to assess interventions against COVID-19 in a small community generated from map data},
  url       = {https://dl.acm.org/doi/10.5555/3535850.3535852},
  year      = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
