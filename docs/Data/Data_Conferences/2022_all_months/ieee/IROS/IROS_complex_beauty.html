<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IROS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="iros---1214">IROS - 1214</h2>
<ul>
<li><details>
<summary>
(2022). AB-mapper: Attention and BicNet based multi-agent path
planning for dynamic environment. <em>IROS</em>, 13799–13806. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent path finding in dynamic environments is of great academic and practical value for multi-robot systems in the real world. To improve the effectiveness and efficiency of the learning process during path planning in dynamic environments, we introduce an algorithm called Attention and BicNet based Multi-agent path planning with effective reinforcement (AB-Mapper) under the actor-critic reinforcement learning framework. In this framework, on one hand, we design an actor-network that can utilize the BicNet with communication function to achieve the intra-team coordination. On the other hand, we propose a critic network that can selectively allocate attention weights to surrounding agents. This attention mechanism allows an individual agent to automatically learn a better evaluation of actions by considering the behaviours of its surrounding agents. Compared with the SOTA method Mapper in crowded environments with dynamic obstacles, our AB-Mapper is more effective (90.27±0.06\% vs. 61.65±13.90\% in terms of mean success rate) in solving the general multi-agent path finding problem.},
  archive   = {C_IROS},
  author    = {Huifeng Guan and Yuan Gao and Min Zhao and Yong Yang and Fuqin Deng and Tin Lun Lam},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981513},
  pages     = {13799-13806},
  title     = {AB-mapper: Attention and BicNet based multi-agent path planning for dynamic environment},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Collision-free minimum-time trajectory planning for multiple
vehicles based on ADMM. <em>IROS</em>, 13785–13790. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The paper presents a practical approach for planning trajectories for multiple vehicles where both collision avoidance and minimum travelling time are simultaneously considered. It is first proposed to exploit the mixed-integer programming (MIP) approach to formulate the collision avoidance paradigm, where the linear dynamic models are utilized to derive the linear constraints. Moreover, travelling time of each vehicle is compromised among them and set to be minimized so that all the vehicles can practically reach the expected destinations at the shortest time. Unfortunately, the formulated optimization problem is NP-hard. In order to effectively address it, we propose to employ the alternating direction method of multipliers (ADMM), which can share the computational burdens to distributive optimization solvers. Thus, the proposed method can enable each vehicle to obtain an expected trajectory in a practical time. Convergence of the proposed algorithm is also discussed. To verify effectiveness of our approach, we implemented it in a numerical example, where the obtained results are highly promising.},
  archive   = {C_IROS},
  author    = {Thanh Binh Nguyen and Thang Nguyen and Truong Nghiem and Linh Nguyen and Jose Baca and Pablo Rangel and Hyoung-Kyu Song},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981138},
  pages     = {13785-13790},
  title     = {Collision-free minimum-time trajectory planning for multiple vehicles based on ADMM},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inspection of ship hulls with multiple UAVs: Exploiting
prior information for online path planning. <em>IROS</em>, 13777–13784.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses a path planning problem for a fleet of Unmanned Aerial Vehicles (UAVs) that uses both prior information and online gathered data to efficiently inspect large surfaces such as ship hulls and water tanks. UAVs can detect corrosion patches and other defects on the surface from low-resolution images. If defects are detected, they get closer to the surface for a high-resolution inspection. The prior information provides expected defects locations and is affected by both false positives and false negatives. The mission objective is to prioritize the close-up inspection of defected areas while keeping a reasonable time for the coverage of the entire surface. We propose two solutions to this problem: a coverage algorithm that divides the problem into a set of Traveling Salesman Problems (Part-TSP) and a cooperative frontier approach that introduces frontier utilities to incorporate the prior information (Coop-Frontier). We finally provide extensive simulation results to analyze the performance of these approaches and compare them with alternative solutions. These results suggest that both Part-Tspand Coop-Frontier perform better than the baseline solution. Part-Tsphas the best performance in most cases. However, coop-Frontier is preferable in extreme cases because more robust to inhomogeneous corrosion distribution and imperfect information.},
  archive   = {C_IROS},
  author    = {Pasquale Grippa and Alessandro Renzaglia and Antoine Rochebois and Melanie Schranz and Olivier Simonin},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981357},
  pages     = {13777-13784},
  title     = {Inspection of ship hulls with multiple UAVs: Exploiting prior information for online path planning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FC3: Feasibility-based control chain coordination.
<em>IROS</em>, 13769–13776. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hierarchical coordination of controllers often uses symbolic state representations that fully abstract their underlying low-level controllers, treating them as “black boxes” to the symbolic action abstraction. This paper proposes a framework to realize robust behavior, which we call Feasibility-based Control Chain Coordination (FC 3 ). Our controllers expose the geometric features and constraints they operate on. Based on this, FC 3 can reason over the controllers&#39; feasibility and their sequence feasibility. For a given task, FC 3 first automatically constructs a library of potential controller chains using a symbolic action tree, which is then used to coordinate controllers in a chain, evaluate task feasibility, as well as switching between controller chains if necessary. In several real-world experiments we demonstrate FC 3 , s robustness and awareness of the task&#39;s feasibility through its own actions and gradual responses to different interferences.},
  archive   = {C_IROS},
  author    = {Jason Harris and Danny Driess and Marc Toussaint},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981758},
  pages     = {13769-13776},
  title     = {FC3: Feasibility-based control chain coordination},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RHH-LGP: Receding horizon and heuristics-based
logic-geometric programming for task and motion planning. <em>IROS</em>,
13761–13768. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sequential decision-making and motion planning for robotic manipulation induce combinatorial complexity. For long-horizon tasks, especially when the environment comprises many objects that can be interacted with, planning efficiency becomes even more important. To plan such long-horizon tasks, we present the RHH-LGP algorithm for combined task and motion planning (TAMP). First, we propose a TAMP approach (based on Logic-Geometric Programming) that effectively uses geometry-based heuristics for solving long-horizon manipulation tasks. The efficiency of this planner is then further improved by a receding horizon formulation, resulting in RHH-LGP. We demonstrate the robustness and effectiveness of our approach on a diverse range of long-horizon tasks that require reasoning about interactions with a large number of objects. Using our framework, we can solve tasks that require multiple robots, including a mobile robot and snake-like walking robots, to form novel heterogeneous kinematic structures autonomously. By combining geometry-based heuristics with iterative planning, our approach brings an order-of-magnitude reduction of planning time in all investigated problems.},
  archive   = {C_IROS},
  author    = {Cornelius V. Braun and Joaquim Ortiz-Haro and Marc Toussaint and Ozgur S. Oguz},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981797},
  pages     = {13761-13768},
  title     = {RHH-LGP: Receding horizon and heuristics-based logic-geometric programming for task and motion planning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sequence-of-constraints MPC: Reactive timing-optimal control
of sequential manipulation. <em>IROS</em>, 13753–13760. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Task and Motion Planning has made great progress in solving hard sequential manipulation problems. However, a gap between such planning formulations and control methods for reactive execution remains. In this paper we pro-pose a model predictive control approach dedicated to robustly execute a single sequence of constraints, which corresponds to a discrete decision sequence of a TAMP plan. We decompose the overall control problem into three sub-problems (solving for sequential waypoints, their timing, and a short receding horizon path) that each is a non-linear program solved online in each MPC cycle. The resulting control strategy can account for long-term interdependencies of constraints and reactively plan for a timing-optimal transition through all constraints. We additionally propose phase backtracking when running constraints of the current phase cannot be fulfilled, leading to a fluent re-initiation behavior that is robust to perturbations and interferences by an experimenter.},
  archive   = {C_IROS},
  author    = {Marc Toussaint and Jason Harris and Jung-Su Ha and Danny Driess and Wolfgang Hönig},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982236},
  pages     = {13753-13760},
  title     = {Sequence-of-constraints MPC: Reactive timing-optimal control of sequential manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Vision-based distributed multi-UAV collision avoidance via
deep reinforcement learning for navigation. <em>IROS</em>, 13745–13752.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Online path planning for multiple unmanned aerial vehicle (multi-UAV) systems is considered a challenging task. It needs to ensure collision-free path planning in real-time, especially when the multi-UAV systems can become very crowded on certain occasions. In this paper, we presented a vision-based decentralized collision-avoidance policy learning method for multi-UAV systems. The policy takes depth images and inertial measurements as sensory inputs and outputs UAV&#39;s steering commands, and it is trained together with the latent representation of depth images using a policy gradient-based reinforcement learning algorithm and autoencoder in the multi-UAV three-dimensional workspaces. Each UAV follows the same trained policy and acts independently to reach the goal without colliding or communicating with other UAVs. We validate our method in various simulated scenarios. The experimental results show that our learned policy can guarantee fully autonomous collision-free navigation for multi-UAV in three-dimensional workspaces, and its navigation performance will not be greatly affected by the increase in the number of UAVs.},
  archive   = {C_IROS},
  author    = {Huaxing Huang and Guijie Zhu and Zhun Fan and Hao Zhai and Yuwei Cai and Ze Shi and Zhaohui Dong and Zhifeng Hao},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981803},
  pages     = {13745-13752},
  title     = {Vision-based distributed multi-UAV collision avoidance via deep reinforcement learning for navigation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-UAV cooperative short-range combat via attention-based
reinforcement learning using individual reward shaping. <em>IROS</em>,
13737–13744. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel distributed method based on attention-based deep reinforcement learning using individual reward shaping, for multiple unmanned aerial vehicles (UAVs) cooperative short-range combat mission. Specifically, a two-level attention distributed policy, composed of observation-level and communication-level attention networks, is designed to enable each UAV to selectively focus on important environmental features and messages, for enhancing the effectiveness of the cooperative policy. Moreover, due to the high complexity and stochasticity of the UAV combat mission, the learning of UAVs is tricky and low efficient. To embed knowledge to accelerate the policy learning, a potential-based individual reward function is constructed by implicitly translating the individual reward into the specific form of dynamic action potentials. In addition, an actor-critic training algorithm based on the centralized training and decentralized execution framework is adopted to train the policy network of UAV maneuver decision. We build a three-dimensional UAV simulation and training platform based on Unity for multi-UAV short-range combat missions. Simulation results demonstrate the effectiveness of the proposed method and the superiority of the attention policy and individual reward shaping.},
  archive   = {C_IROS},
  author    = {Tianle Zhang and Tenghai Qiu and Zhen Liu and Zhiqiang Pu and Jianqiang Yi and Jinying Zhu and Ruiguang Hu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982096},
  pages     = {13737-13744},
  title     = {Multi-UAV cooperative short-range combat via attention-based reinforcement learning using individual reward shaping},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attention-based population-invariant deep reinforcement
learning for collision-free flocking with a scalable fixed-wing UAV
swarm. <em>IROS</em>, 13730–13736. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A swarm of fixed-wing unmanned aerial vehicles (UAVs) is expected to efficiently accomplish various tasks in complex scenarios. This paper proposes an attention-based population-invariant multi-agent deep reinforcement learning (MADRL) approach to deal with the decentralized collision-free flocking problem for a scalable fixed-wing UAV swarm. First, this problem is modeled as a decentralized partially observable Markov decision process from the perspective of each follower. Then, an improved multi-agent deep deterministic policy gradient (MADDPG) algorithm is presented to efficiently learn the population-invariant flocking policy. In this algorithm, the parameter sharing with ego-centric representation mechanism is incorporated to improve learning efficiency. Besides, the attention-based population-invariant network structure (APINet) is designed by leveraging the self-attention mechanism. With this structure, the learned flocking policy is invariant to the population of the swarm. Finally, both numerical and hardware-in-the-loop simulation results verify the efficiency and scalability of the proposed approach.},
  archive   = {C_IROS},
  author    = {Chao Yan and Kin Huat Low and Xiaojia Xiang and Tianjiang Hu and Lincheng Shen},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981429},
  pages     = {13730-13736},
  title     = {Attention-based population-invariant deep reinforcement learning for collision-free flocking with a scalable fixed-wing UAV swarm},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-robot dynamic swarm disablement. <em>IROS</em>,
13722–13729. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motivated by the use of robots for pest control in agriculture, this work introduces the Multi-Robot Dynamic Swarm Disablement problem, in which a team of robots is required to disable a swarm of agents (for example, locust agents) passing through an area while minimizing the cumulative time of the swarm members (equivalent to the cumulative damage they cause) in the area. Showing that the problem is hard even in naive settings, we turn to examine algorithms seeking to optimize the robots&#39; performance against the swarm by exploiting the known movement pattern of the swarm agents. Motivated by the poor performance when a weak group of robots attempts to catch a large swarm of agents, whether it is a significant numerical minority or poor speed gaps, we suggest the use of blocking lines: the robots form lines that block the agents along their movement in the environment. We show by both theoretical analysis and rigorous empirical evaluation in different settings that these algorithms outperform common task-assignment-based algorithms, especially for limited robots versus a large swarm.},
  archive   = {C_IROS},
  author    = {Ori Fogler and Noa Agmon},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982075},
  pages     = {13722-13729},
  title     = {Multi-robot dynamic swarm disablement},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toolbox release: A WiFi-based relative bearing framework for
robotics. <em>IROS</em>, 13714–13721. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents the WiFi-Sensor-for-Robotics (WSR) open-source toolbox 1 1 1 Code: https://github.com/Harvard-REACT/WSR-Toolbox Dataset: https://github.com/Harvard-REACT/WSR-Toolbox-Dataset Demo: https://github.com/Harvard-REACT/WSR-Toolbox/wiki/Demo. It enables robots in a team to obtain relative bearing to each other, even in nonline-of-sight (NLOS) settings which is a very challenging problem in robotics. It does so by analyzing the phase of their communicated WiFi signals as the robots traverse the environment. This capability, based on the theory developed in our prior works, is made available for the first time as an open-source toolbox. It is motivated by the lack of easily deployable solutions that use robots&#39; local resources (e.g WiFi) for sensing in NLOS. This has implications for multi-robot mapping and rendezvous, ad-hoc robot networks, and security in multi-robot teams, amongst other applications. The toolbox is designed for distributed and online deployment on robot platforms using commodity hardware and on-board sensors. We also release datasets demonstrating its performance in NLOS and line-of-sight (LOS) settings and for a multi-robot localization use case. Empirical results for hardware experiments show that the bearing estimation from our toolbox achieves accuracy with mean and standard deviation of 1.13 degrees, 11.07 degrees in LOS and 6.04 degrees, 26.4 degrees for NLOS, respectively, in an indoor office environment.},
  archive   = {C_IROS},
  author    = {Ninad Jadhav and Weiying Wang and Diana Zhang and Swarun Kumar and Stephanie Gil},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981230},
  pages     = {13714-13721},
  title     = {Toolbox release: A WiFi-based relative bearing framework for robotics},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Communication-preserving bids in market-based task
allocation. <em>IROS</em>, 13708–13713. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we study the effects of impaired communications on the performances of auction-based task allocation in a dynamic surveillance scenario. We propose a novel connectivity term to include in the bid valuation formula, that aims at improving communications in the multi-robot team. We evaluate our method as well as another state-of-the-art method using robot inter-distance to maintain communication, on randomly generated scenarios and on a real-world scenario. We demonstrate that including our connectivity term in the bid valuation formula improves the performances of the auction scheme.},
  archive   = {C_IROS},
  author    = {Felix Quinton and Christophe Grand and Charles Lesire},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981699},
  pages     = {13708-13713},
  title     = {Communication-preserving bids in market-based task allocation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Meeting-merging-mission: A multi-robot coordinate framework
for large-scale communication-limited exploration. <em>IROS</em>,
13700–13707. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This letter presents a complete framework Meeting-Merging-Mission for multi-robot exploration under communication restriction. Considering communication is limited in both bandwidth and range in the real world, we propose a lightweight environment presentation method and an efficient cooperative exploration strategy. For lower bandwidth, each robot uses specific polytopes to maintain free space and to generate Super Frontier Information (SFI), which serves as the source for exploration decision-making. To reduce repeated exploration, we develop a mission-based protocol that drives robots to share collected information in stable rendezvous. We also design a complete path planning scheme for both centralized and decentralized cases. To validate that our framework is practical and generic, we present an extensive benchmark and deploy our system into multi-UGV and multi-UAV platforms.},
  archive   = {C_IROS},
  author    = {Yuman Gao and Yingjian Wang and Xingguang Zhong and Tiankai Yang and Mingyang Wang and Zhixiong Xu and Yongchao Wang and Yi Lin and Chao Xu and Fei Gao},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981544},
  pages     = {13700-13707},
  title     = {Meeting-merging-mission: A multi-robot coordinate framework for large-scale communication-limited exploration},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MT*: Multi-robot path planning for temporal logic
specifications. <em>IROS</em>, 13692–13699. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the path planning problem for a team of robots satisfying a complex high-level mission specification given in the form of a Linear Temporal Logic (LTL) formula. The state-of-the-art approach to this problem employs the automata-theoretic model checking technique to solve this problem. This approach involves computation of a product graph of the Büchi automaton generated from the LTL specification and a joint transition system that captures the collective motion of the robots and then computation of the shortest path using Di-jkstra&#39;s shortest path algorithm. We propose MT*, an algorithm that reduces the computation burden for generating such plans for multi-robot systems significantly. Our approach generates a reduced version of the product graph without computing the complete joint transition system, which is computationally expensive. It then divides the complete mission specification among the participating robots and generates the trajectories for the individual robots independently. Our approach demonstrates substantial speedup in terms of computation time over the state-of-the-art approach and scales well with both the number of robots and the size of the workspace.},
  archive   = {C_IROS},
  author    = {Dhaval Gujarathi and Indranil Saha},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981504},
  pages     = {13692-13699},
  title     = {MT*: Multi-robot path planning for temporal logic specifications},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributed ranging SLAM for multiple robots with
ultra-WideBand and odometry measurements. <em>IROS</em>, 13684–13691.
(<a href="https://doi.org/10.1109/IROS47612.2022.9982028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To accomplish task efficiently in a multiple robots system, a problem that has to be addressed is Simultaneous Localization and Mapping (SLAM). LiDAR (Light Detection and Ranging) has been used for many SLAM solutions due to its superb accuracy, but its performance degrades in featureless environments, like tunnels or long corridors. Centralized SLAM solves the problem with a cloud server, which requires a huge amount of computational resources and lacks robustness against central node failure. To address these issues, we present a distributed SLAM solution to estimate the trajectory of a group of robots using Ultra-WideBand (UWB) ranging and odometry measurements. The proposed approach distributes the processing among the robot team and significantly mitigates the computation concern emerged from the centralized SLAM. Our solution determines the relative pose (also known as loop closure) between two robots by minimizing the UWB ranging measurements taken at different positions when the robots are in close proximity. UWB provides a good distance measure in line-of-sight conditions, but retrieving a precise pose estimation remains a challenge, due to ranging noise and unpredictable path traveled by the robot. To deal with the suspicious loop closures, we use Pairwise Consistency Maximization (PCM) to examine the quality of loop closures and perform outlier rejections. The filtered loop closures are then fused with odometry in a distributed pose graph optimization (DPGO) module to recover the full trajectory of the robot team. Extensive experiments are conducted to validate the effectiveness of the proposed approach.},
  archive   = {C_IROS},
  author    = {Ran Liu and Zhongyuan Deng and Zhiqiang Cao and Muhammad Shalihan and Billy Pik Lik Lau and Kaixiang Chen and Kaushik Bhowmik and Chau Yuen and U-Xuan Tan},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982028},
  pages     = {13684-13691},
  title     = {Distributed ranging SLAM for multiple robots with ultra-WideBand and odometry measurements},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Obstacle aware sampling for path planning. <em>IROS</em>,
13676–13683. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many path planning algorithms are based on sampling the state space. While this approach is very simple, it can become costly when the obstacles are unknown, since samples hitting these obstacles are wasted. The goal of this paper is to efficiently identify obstacles in a map and remove them from the sampling space. To this end, we propose a pre-processing algorithm for space exploration that enables more efficient sampling. We show that it can boost the performance of other space sampling methods and path planners. Our approach is based on the fact that a convex obstacle can be approximated provably well by its minimum volume enclosing ellipsoid (MVEE), and a non-convex obstacle may be partitioned into convex shapes. Our main contribution is an al-gorithm that strategically finds a small sample, called the active-coreset, that adaptively samples the space via membership-oracle such that the MVEE of the coreset approximates the MVEE of the obstacle. Experimental results confirm the ef-fectiveness of our approach across multiple planners based on rapidly-exploring random trees, showing significant improve-ment in terms of time and path length.},
  archive   = {C_IROS},
  author    = {Murad Tukan and Alaa Maalouf and Dan Feldman and Roi Poranne},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981428},
  pages     = {13676-13683},
  title     = {Obstacle aware sampling for path planning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A saturation-aware trajectory-based explicit reference
governor for a robotic arm. <em>IROS</em>, 13641–13646. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As with all actuated mechanical systems, the development of a control scheme for robotic systems must take actuator saturation into account. Furthermore, in order to properly control a robotic arm, other types of mechanical limitations must also be considered (e.g. limited operating range of the joint, speed limitations). In this paper, we propose a saturation-aware trajectory-based Explicit Reference Governor, a lightweight constrained control scheme with no online optimization. Additionally, we tested the effectiveness of the proposed control architecture on a 7 degree-of-freedom KUKA LBR IIWA14 R820. Both the performance and the computational time of our proposed solution were tested against other constrained control solutions available in the literature.},
  archive   = {C_IROS},
  author    = {Michele Ambrosino and Andres Cotorruelo and Emanuele Garone},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981483},
  pages     = {13641-13646},
  title     = {A saturation-aware trajectory-based explicit reference governor for a robotic arm},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CoMBiNED: Multi-constrained model based planning for
navigation in dynamic environments. <em>IROS</em>, 13634–13640. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent deep reinforcement learning (DRL) approaches have achieved high success rate in map-less dynamic obstacle avoidance tasks. However, navigation in unseen dynamic scenarios without a pre-built map in the presence of dynamic obstacles still remains an open challenge. Since, learning accurate models for complex robotic scenarios such as navigation directly from high dimensional sensory measurements requires a large amount of data and training. Furthermore, even a small change on robot configuration such as kino-dynamics or sensor in the inference time requires re-training of the policy. In this paper, we address these issues in a principled fashion through a multi-constraint model based online planning (CoMBiNED) framework that does not require any retraining or modifications on the existing policy. We disentangle the given task into sub-tasks and learn dynamical models for them. Treating these dynamical models as soft-constraints, we employ stochastic optimisation to jointly optimize these sub-tasks on-the-fly at the inference time. We consider navigation as central application in this work and evaluate our approach on publicly available benchmark with complex dynamic scenarios and achieved significant improvement over recent approaches both in the cases of with-and-without given map of the environment.},
  archive   = {C_IROS},
  author    = {Harit Pandya and Rudra P.K. Poudel and Stephan Liwicki},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981479},
  pages     = {13634-13640},
  title     = {CoMBiNED: Multi-constrained model based planning for navigation in dynamic environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Avoiding dynamic obstacles with real-time motion planning
using quadratic programming for varied locomotion modes. <em>IROS</em>,
13626–13633. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a real-time motion planner that avoids multiple moving obstacles without knowing their dynamics or intentions. This method uses convex optimization to generate trajectories for linear plant models over a planning horizon (i.e. model-predictive control). While convex optimizations allow for fast planning, obstacle avoidance can be challenging to incorporate because Euclidean distance calculations tend to break convexity. By using a half-space convex relaxation, our planner reasons about an approximated distance-to-obstacle measure that is linear in its decision variables and preserves convexity. Further, by iteratively updating the relaxation over the planning horizon, the half-space approximation is improved, enabling nimble avoidance maneuvers. We further augment avoidance performance with a soft penalty slack-variable for-mulation that introduces a piecewise quadratic cost. As a proof of concept, we demonstrate the planner on double-integrator models in both single-agent and multi-agent tasks-avoiding multiple obstacles and other agents in 2D and 3D environments. We show extensions to legged locomotion by bipedally walking around obstacles in simulation using the Linear Inverted Pendulum Model (LIPM). We then present two sets of hardware experiments showing real-time obstacle avoid-ance with quadcopter drones: (1) avoiding a 10m/s swinging pendulum and (2) dodging a chasing drone.},
  archive   = {C_IROS},
  author    = {Jason White and David Jay and Tianze Wang and Christian Hubicki},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981268},
  pages     = {13626-13633},
  title     = {Avoiding dynamic obstacles with real-time motion planning using quadratic programming for varied locomotion modes},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predictive angular potential field-based obstacle avoidance
for dynamic UAV flights. <em>IROS</em>, 13618–13625. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, unmanned aerial vehicles (UAVs) are used for numerous inspection and video capture tasks. Manually controlling UAVs in the vicinity of obstacles is challenging, however, and poses a high risk of collisions. Even for autonomous flight, global navigation planning might be too slow to react to newly perceived obstacles. Disturbances such as wind might lead to deviations from the planned trajectories. In this work, we present a fast predictive obstacle avoidance method that does not depend on higher-level localization or mapping and maintains the dynamic flight capabilities of UAVs. It directly operates on LiDAR range images in real time and adjusts the current flight direction by computing angular potential fields within the range image. The velocity magnitude is subsequently determined based on a trajectory prediction and time-to-contact estimation. Our method is evaluated using Hardware-in-the-Loop simulations. It keeps the UAV at a safe distance to obstacles, while allowing higher flight velocities than previous reactive obstacle avoidance methods that directly operate on sensor data.},
  archive   = {C_IROS},
  author    = {Daniel Schleich and Sven Behnke},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981677},
  pages     = {13618-13625},
  title     = {Predictive angular potential field-based obstacle avoidance for dynamic UAV flights},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Collision detection and identification for a legged
manipulator. <em>IROS</em>, 13602–13609. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To safely deploy legged robots in the real world it is necessary to provide them with the ability to reliably detect unexpected contacts and accurately estimate the corresponding contact force. In this paper, we propose a collision detection and identification pipeline for a quadrupedal manipulator. We first introduce an approach to estimate the collision time span based on band-pass filtering and show that this information is key for obtaining accurate collision force estimates. We then improve the accuracy of the identified force magnitude by compensating for model inaccuracies, unmodeled loads, and any other potential source of quasi-static disturbances acting on the robot. We validate our framework with extensive hardware experiments in various scenarios, including trotting and additional unmodeled load on the robot.},
  archive   = {C_IROS},
  author    = {Jessie Van Dam and Andreea Tulbure and Maria Vittoria Minniti and Firas Abi-Farraj and Marco Hutter},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981767},
  pages     = {13602-13609},
  title     = {Collision detection and identification for a legged manipulator},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved task space locomotion controller for a quadruped
robot with parallel mechanisms. <em>IROS</em>, 13578–13585. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, an advanced quadruped robot with abundant kinematic loops and passive joints is introduced. Due to the existence of many closed chains, the robot dynamic model is quite complex, and is derived using the Gauss&#39;s principle of least constraint. To explicitly consider the loop-closure constraints, we propose a task-space inverse dynamics based approach to obtain the robot locomotion controller. Besides, to meet the demand of high frequency (≥ 500Hz) in controller, an alternative method is provided. It uses the projected dynamics to find an analytical mapping from the desired contact force to the desired torque of actuators under full consideration of passive joints and loop-closure constraints. The effectiveness and efficiency of the proposed algorithms in this paper have been validated by simulation with a reliable physical engine MuJoCo.},
  archive   = {C_IROS},
  author    = {Shunpeng Yang and Wenchun Lin and Jaeho Noh and Cheng Luo and Bill Huang and Wei Zhang and Hua Chen},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981385},
  pages     = {13578-13585},
  title     = {Improved task space locomotion controller for a quadruped robot with parallel mechanisms},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Vision-assisted localization and terrain reconstruction
with quadruped robots. <em>IROS</em>, 13571–13577. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Legged robots, specifically quadruped robots, have good locomotion performance in complex and rugged terrain and are becoming widely used in field exploration and rescue missions. To achieve full autonomy in such scenarios, robots need not only accurate localization but also an accurate understanding of the surrounding terrain, which will be used for robots path planning and foothold planning. However, due to the kinetic characteristic and limitation of size, quadruped robots have the disadvantages of high-frequency jitter and limited field of sensors, which lead to some challenges in environmental perception. In this paper, we propose a vision-assisted rugged terrain environment reconstruction and localization method for quadruped robots. We use a depth camera to assist in the generation of high-precision localization and terrain reconstruction results, which can help achieve the autonomous mobility of quadruped robots in this environment. We test our method on a quadruped robot platform. Our experimental results show less error and lower drift in different stairs terrain types than the commonly used lidar-based localization method.},
  archive   = {C_IROS},
  author    = {Chengyang Zhang and Jiashi Zhang and Jun Wu and Qiuguo Zhu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981897},
  pages     = {13571-13577},
  title     = {Vision-assisted localization and terrain reconstruction with quadruped robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simultaneous contact-rich grasping and locomotion via
distributed optimization enabling free-climbing for multi-limbed robots.
<em>IROS</em>, 13563–13570. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While motion planning of locomotion for legged robots has shown great success, motion planning for legged robots with dexterous multi-finger grasping is not mature yet. We present an efficient motion planning framework for simultaneously solving locomotion (e.g., centroidal dynamics), grasping (e.g., patch contact), and contact (e.g., gait) problems. To accelerate the planning process, we propose distributed optimization frameworks based on Alternating Direction Methods of Multipliers (ADMM) to solve the original large-scale Mixed-Integer NonLinear Programming (MINLP). The resulting frameworks use Mixed-Integer Quadratic Programming (MIQP) to solve contact and NonLinear Programming (NLP) to solve nonlinear dynamics, which are more computationally tractable and less sensitive to parameters. Also, we explicitly enforce patch contact constraints from limit surfaces with micro-spine grippers. We demonstrate our proposed framework in the hardware experiments, showing that the multi-limbed robot is able to realize various motions including free-climbing at a slope angle 45° with a much shorter planning time.},
  archive   = {C_IROS},
  author    = {Yuki Shirai and Xuan Lin and Alexander Schperberg and Yusuke Tanaka and Hayato Kato and Varit Vichathorn and Dennis Hong},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981579},
  pages     = {13563-13570},
  title     = {Simultaneous contact-rich grasping and locomotion via distributed optimization enabling free-climbing for multi-limbed robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An online interactive approach for crowd navigation of
quadrupedal robots. <em>IROS</em>, 13556–13562. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot navigation in human crowds remains the challenge of understanding human behaviors in different scenarios. We present an approach for interactive and human-friendly crowd navigation in complex static environments. The planner models the online interactions among the robot, humans, and the static environment based on game theory. It recurrently expands and optimizes the estimated trajectories for the robot and neighboring agents and provides human-friendly navigation commands. We use various indicators to evaluate the social awareness of the planners and show that our method outperforms existing approaches in success rate to reach the goals and compatibility with humans while maintaining low navigation times. The planner is successfully deployed on a real-world quadrupedal robot, demonstrating safe and interactive crowd navigation with real-time performance.},
  archive   = {C_IROS},
  author    = {Bowen Yang and Jianhao Jiao and Lujia Wang and Ming Liu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981697},
  pages     = {13556-13562},
  title     = {An online interactive approach for crowd navigation of quadrupedal robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Db-a*: Discontinuity-bounded search for kinodynamic mobile
robot motion planning. <em>IROS</em>, 13540–13547. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider time-optimal motion planning for dynamical systems that are translation-invariant, a property that holds for many mobile robots, such as differential-drives, cars, airplanes, and multirotors. Our key insight is that we can extend graph-search algorithms to the continuous case when used symbiotically with optimization. For the graph search, we introduce discontinuity-bounded A* (db-A*), a generalization of the A* algorithm that uses concepts and data structures from sampling-based planners. Db-A* reuses short trajectories, so-called motion primitives, as edges and allows a maximum user-specified discontinuity at the vertices. These trajectories are locally repaired with trajectory optimization, which also provides new improved motion primitives. Our novel kinodynamic motion planner, kMP-db-A*, has almost surely asymptotic optimal behavior and computes near-optimal solutions quickly. For our empirical validation, we provide the first benchmark that compares search-, sampling-, and optimization- based time-optimal motion planning on multiple dynamical systems in different settings. Compared to the baselines, kMP- db-A* consistently solves more problem instances, finds lower- cost initial solutions, and converges more quickly.},
  archive   = {C_IROS},
  author    = {Wolfgang Hönig and Joaquim Ortiz-Haro and Marc Toussaint},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981577},
  pages     = {13540-13547},
  title     = {Db-a*: Discontinuity-bounded search for kinodynamic mobile robot motion planning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Motion planning by search in derivative space and convex
optimization with enlarged solution space. <em>IROS</em>, 13500–13507.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To efficiently generate safe trajectories for an autonomous vehicle in dynamic environments, a layered motion planning method with decoupled path and speed planning is widely used. This paper studies speed planning, which mainly deals with dynamic obstacle avoidance given a planned path. The main challenges lie in the optimization in a non-convex space and the trade-off between safety, comfort, and efficiency. First, this work proposes to conduct a search in second-order derivative space for generating a comfort-optimal reference trajectory. Second, by combining abstraction and refinement, an algorithm is proposed to construct a convex feasible space for optimization. Finally, a piecewise Bézier polynomial optimization approach with trapezoidal corridors is presented, which theoretically guarantees safety and significantly enlarges the solution space compared with the existing rectangular corridors-based approach. We validate the efficiency and effectiveness of the proposed approach in simulations.},
  archive   = {C_IROS},
  author    = {Jialun Li and Xiaojia Xie and Qin Lin and Jianping He and John M. Dolan},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981961},
  pages     = {13500-13507},
  title     = {Motion planning by search in derivative space and convex optimization with enlarged solution space},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conflict-based search for multi-robot motion planning with
kinodynamic constraints. <em>IROS</em>, 13494–13499. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-robot motion planning (MRMP) is the fundamental problem of finding non-colliding trajectories for multiple robots acting in an environment, under kinodynamic constraints. Due to its complexity, existing algorithms are either incomplete, or utilize simplifying assumptions. This work introduces Kinodynamic Conflict-Based Search (K-CBS), a decentralized MRMP algorithm that is general, scalable, and probabilistically complete. The algorithm takes inspiration from successful solutions to the discrete analogue of MRMP over finite graphs, known as Multi-Agent Path Finding (MAPF). Specifically, we adapt ideas from Conflict-Based Search (CBS)-a popular decentralized MAPF algorithm-to the MRMP setting. The novelty of our approach is that we work directly in the continuous domain, without discretization. In particular, the kinodynamic constraints are treated natively. K-CBS plans for each robot individually using a low-level planner and grows a conflict tree to resolve collisions between robots by defining constraints. The low-level planner can be any sampling-based, tree-search algorithm for kinodynamic robots, thus lifting existing planners for single robots to the multi-robot setting. We show that K-CBS inherits the (probabilistic) completeness of the low-level planner. We illustrate the generality and performance of K-CBS in several case studies and benchmarks.},
  archive   = {C_IROS},
  author    = {Justin Kottinger and Shaull Almagor and Morteza Lahijanian},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982018},
  pages     = {13494-13499},
  title     = {Conflict-based search for multi-robot motion planning with kinodynamic constraints},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast cost-aware lazy-theta over euclidean distance functions
for 3D planning of aerial robots in building-like environments.
<em>IROS</em>, 13486–13493. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a fast cost-aware any-angle path planning algorithm for aerial robots in 3D building-like environments. The approach integrates Euclidean Distance Fields (EDF) and Lazy Theta* algorithm to compute safe and smooth paths. We show how to consider the analytical proper-ties of EDFs for polygonal obstacles to get an approximation of the cost along the line of sight segments of the planner, reducing the computational requirements. Numerous tests in a realistic building-like environment are performed to evaluate the proposed algorithm with respect to other heuristic search algorithms considering the distance cost by using an EDF. The results show that the proposed algorithm considerably reduces the computation time in indoor and outdoor environments enabling fast, safe and smooth paths.},
  archive   = {C_IROS},
  author    = {Jose A. Cobano and Rafael Rey and L. Merino and F. Caballero},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982180},
  pages     = {13486-13493},
  title     = {Fast cost-aware lazy-theta over euclidean distance functions for 3D planning of aerial robots in building-like environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Use of action label in deep predictive learning for robot
manipulation. <em>IROS</em>, 13459–13465. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Various forms of human knowledge can be explicitly used to enhance deep robot learning from demonstrations. Annotation of subtasks from task segmentation is one type of human symbolism and knowledge. Annotated subtasks can be referred to as action labels, which are more primitive symbols that can be building blocks for more complex human reasoning, like language instructions. However, action labels are not widely used to boost learning processes because of problems that include (1) real-time annotation for online manipulation, (2) temporal inconsistency by annotators, (3) difference in data characteristics of motor commands and action labels, and (4) annotation cost. To address these problems, we propose the Gated Action Motor Predictive Learning (GAMPL) framework to leverage action labels for improved performance. GAMPL has two modules to obtain soft action labels compatible with motor commands and to generate motion. In this study, GAMPL is evaluated for towel-folding manipulation tasks in a real environment with a six degrees-of-freedom (6 DoF) robot and shows improved generalizability with action labels.},
  archive   = {C_IROS},
  author    = {Kei Kase and Chikara Utsumi and Yukiyasu Domae and Tetsuya Ogata},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982091},
  pages     = {13459-13465},
  title     = {Use of action label in deep predictive learning for robot manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive sequential composition for robot behaviours.
<em>IROS</em>, 13445–13452. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous robots are prone to fail in real world environments, where unknown factors can cause their world model to be inaccurate or incomplete. This causes robots to become stuck or repeatedly perform unsuccessful actions believing it is the best option. Static switching frameworks such as sequential composition guarantee stability of the overall system if its constituents are also stable. However, due to unknown factors, robot actions may fail when the robot senses the environment inaccurately. We propose Adaptive Sequential Composition, a novel framework that dynamically selects robot behaviours based on their utility to achieving success. We show the usefulness of the framework in a simulated second order system task as well as its application in navigating a robot through narrow gaps, where the clearance with the gap is smaller than position accuracy. Simulated results show adaptive sequential composition outperforms sequential composition by up to 30\% when presented with unknown factors leading to behaviour failure. For navigating through a narrow gap, adaptive sequential composition improved success by 65\%.},
  archive   = {C_IROS},
  author    = {Benjamin Tam and Navinda Kottege and Nicolas Hudson and Michael Brünig},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982012},
  pages     = {13445-13452},
  title     = {Adaptive sequential composition for robot behaviours},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CreativeBot: A creative storyteller agent developed by
leveraging pre-trained language models. <em>IROS</em>, 13438–13444. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In an attempt to nurture children&#39;s creativity, we developed a creative conversational agent to be used in a collaborative storytelling context with a child. We presented a novel approach to develop creative Artificial Intelligence (AI). Our approach uses the four creativity measures: fluency, flexi-bility, elaboration and originality in order to generate creative behavior. We analyzed and annotated our previously collected storytelling data sets -collected with children- according to our four creativity measures. We then used the extracted and annotated data (636 statements) in order to fine-tune two pre-trained language models (Open AI GPT-3). The two models were aimed at generating creative versus non-creative behavior in a collaborative storytelling scenario. We developed the two models to be able to assess the results and compare them together. We conducted an evaluation to assess stories generated collaboratively between a human and both agents separately (n = 26). Adult Users rated the creativity of the agent according to the stories generated. Results showed that the creative agent was perceived as significantly more creative than the non-creative agent. With the experiment results confirming the validity of our system, we may therefore proceed with testing the effects of the creative behavior of the agent on children&#39;s creativity skills.},
  archive   = {C_IROS},
  author    = {Maha Elgarf and Christopher Peters},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981033},
  pages     = {13438-13444},
  title     = {CreativeBot: A creative storyteller agent developed by leveraging pre-trained language models},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generalizability analysis of graph-based trajectory
predictor with vectorized representation. <em>IROS</em>, 13430–13437.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trajectory prediction is one of the essential tasks for autonomous vehicles. Recent progress in machine learning gave birth to a series of advanced trajectory prediction algorithms. Lately, the effectiveness of using graph neural networks (GNNs) with vectorized representations for trajec-tory prediction has been demonstrated by many researchers. Nonetheless, these algorithms either pay little attention to models&#39; generalizability across various scenarios or simply assume training and test data follow similar statistics. In fact, when test scenarios are unseen or Out-of-Distribution (OOD), the resulting train-test domain shift usually leads to significant degradation in prediction performance, which will impact downstream modules and eventually lead to severe accidents. Therefore, it is of great importance to thoroughly investigation of the prediction models in terms of their generalizability, which can not only help identify their weaknesses but also provide insights on how to improve these models. This paper proposes a generalizability analysis framework using feature attribution methods to help interpret black-box models. For the case study, we provide an in-depth generalizability analysis of one of the state-of-the-art graph-based trajectory predictors that utilize vectorized representation. Results show significant performance degradation due to domain shift, and feature attribution provides insights to identify potential causes of these problems. Finally, we conclude the common prediction challenges and how weighting biases induced by the training process can deteriorate the accuracy.},
  archive   = {C_IROS},
  author    = {Juanwu Lu and Wei Zhan and Masayoshi Tomizuka and Yeping Hu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981096},
  pages     = {13430-13437},
  title     = {Generalizability analysis of graph-based trajectory predictor with vectorized representation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Manual maneuverability: Metrics for analysing and
benchmarking kinesthetic robot guidance. <em>IROS</em>, 13414–13421. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Kinesthetic teaching of collaborative robots is applied for intuitive and flexible robot programming by demonstration. This enables non-experts to program such robots on the task-level. Multiple strategies exist to teach velocity- or torque-controlled robots and, thus, the maneuverability among commercial robots differs significantly. However, currently there exists no metric that quantifies how “well” the robot can be guided, e.g., how much effort is required to initiate a motion. In this paper, we propose standardized procedures to quantitatively assess robot manual maneuverability. First, we identify different motion phases during kinesthetic teaching. For each phase, we then propose metrics and experimental setups to evaluate them. The experimental protocols are applied to the proprietary teaching schemes of five commercial robots, namely the KUKA LWR iiwa 14, Yuanda Yu+, Franka Emika robot, and Universal Robot&#39;s UR5e and UR10e. The experimental comparison highlights distinct differences between the robots and shows that the proposed methods are a meaningful contribution to the performance and ergonomics assessment of collaborative robots.},
  archive   = {C_IROS},
  author    = {Robin Jeanne Kirschner and Florian Martineau and Nico Mansfeld and Saeed Abdolshah and Sami Haddadin},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981864},
  pages     = {13414-13421},
  title     = {Manual maneuverability: Metrics for analysing and benchmarking kinesthetic robot guidance},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Teaching robots to span the space of functional expressive
motion. <em>IROS</em>, 13406–13413. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Our goal is to enable robots to perform functional tasks in emotive ways, be it in response to their users&#39; emotional states, or expressive of their confidence levels. Prior work has proposed learning independent cost functions from user feedback for each target emotion, so that the robot may optimize it alongside task and environment specific objectives for any situation it encounters. However, this approach is inefficient when modeling multiple emotions and unable to generalize to new ones. In this work, we leverage the fact that emotions are not independent of each other: they are related through a latent space of Valence-Arousal-Dominance (VAD). Our key idea is to learn a model for how trajectories map onto VAD with user labels. Considering the distance between a trajectory&#39;s mapping and a target VAD allows this single model to represent cost functions for all emotions. As a result 1) all user feedback can contribute to learning about every emotion; 2) the robot can generate trajectories for any emotion in the space instead of only a few predefined ones; and 3) the robot can respond emotively to user-generated natural language by mapping it to a target VAD. We introduce a method that interactively learns to map trajectories to this latent space and test it in simulation and in a user study. In experiments, we use a simple vacuum robot as well as the Cassie biped.},
  archive   = {C_IROS},
  author    = {Arjun Sripathy and Andreea Bobu and Zhongyu Li and Koushil Sreenath and Daniel S. Brown and Anca D. Dragan},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981964},
  pages     = {13406-13413},
  title     = {Teaching robots to span the space of functional expressive motion},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HGCN-GJS: Hierarchical graph convolutional network with
groupwise joint sampling for trajectory prediction. <em>IROS</em>,
13400–13405. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pedestrian trajectory prediction is of great importance for downstream tasks, such as autonomous driving and mobile robot navigation. Realistic models of the social interactions within the crowd is crucial for accurate pedestrian trajectory prediction. However, most existing methods do not capture group level interactions well, focusing only on pairwise interactions and neglecting group-wise interactions. In this work, we propose a hierarchical graph convolutional network, HGCN-GJS, for trajectory prediction which well leverages group level interactions within the crowd. Furthermore, we introduce a joint sampling scheme that captures co-dependencies between pedestrian trajectories during trajectory generation. Based on group information, this scheme ensures that generated trajectories within each group are consistent with each other, but enables different groups to act more independently. We demonstrate that our proposed network achieves state of the art performance on all datasets we have considered.},
  archive   = {C_IROS},
  author    = {Yuying Chen and Congcong Liu and Xiaodong Mei and Bertram E. Shi and Ming Liu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981037},
  pages     = {13400-13405},
  title     = {HGCN-GJS: Hierarchical graph convolutional network with groupwise joint sampling for trajectory prediction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Embedding koopman optimal control in robot policy learning.
<em>IROS</em>, 13392–13399. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Embedding an optimization process has been explored for imposing efficient and flexible policy structures. Existing work often build upon nonlinear optimization with explicitly iteration steps, making policy inference prohibitively expensive for online learning and real-time control. Our approach embeds a linear-quadratic-regulator (LQR) formulation with a Koopman representation, thus exhibiting the tractability from a closed-form solution and richness from a non-convex neural network. We use a few auxiliary objectives and reparameterization to enforce optimality conditions of the policy that can be easily integrated to standard gradient-based learning. Our approach is shown to be effective for learning policies rendering an optimality structure and efficient reinforcement learning, including simulated pendulum control, 2D and 3D walking, and manipulation for both rigid and deformable objects. We also demonstrate real world application in a robot pivoting task.},
  archive   = {C_IROS},
  author    = {Hang Yin and Michael C. Welle and Danica Kragic},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981540},
  pages     = {13392-13399},
  title     = {Embedding koopman optimal control in robot policy learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward global sensing quality maximization: A configuration
optimization scheme for camera networks. <em>IROS</em>, 13386–13391. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The performance of a camera network monitoring a set of targets depends crucially on the configuration of the cameras. In this paper, we investigate the reconfiguration strategy for the parameterized camera network model, with which the sensing qualities of the multiple targets can be optimized globally and simultaneously. We first propose to use the number of pixels occupied by a unit-length object in image as a metric of the sensing quality of the object, which is determined by the parameters of the camera, such as intrinsic, extrinsic, and distortional coefficients. Then, we form a single quantity that measures the sensing quality of the targets by the camera network. This quantity further serves as the objective function of our optimization problem to obtain the optimal camera configuration. We verify the effectiveness of our approach through extensive simulations and experiments, and the results reveal its improved performance on the AprilTag detection tasks. Codes and related utilities for this work are open-sourced and available at https://github.com/sszxc/MultiCam-Simulation.},
  archive   = {C_IROS},
  author    = {Xuechao Zhang and Xuda Ding and Yi Ren and Yu Zheng and Chongrong Fang and Jianping He},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982246},
  pages     = {13386-13391},
  title     = {Toward global sensing quality maximization: A configuration optimization scheme for camera networks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Introducing force feedback in model predictive control.
<em>IROS</em>, 13379–13385. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the literature about model predictive control (MPC), contact forces are planned rather than controlled. In this paper, we propose a novel paradigm to incorporate effort measurements into a predictive controller, hence allowing to control them by direct measurement feedback. We first demonstrate why the classical optimal control formulation, based on position and velocity state feedback, cannot handle direct feedback on force information. Following previous approaches in force control, we then propose to augment the classical formulations with a model of the robot actuation, which naturally allows to generate online trajectories that adapt to sensed position, velocity and torques. We propose a complete implementation of this idea on the upper part of a real humanoid robot, and show through hardware experiments that this new formulation incorporating effort feedback outperforms classical MPC in challenging tasks where physical interaction with the environment is crucial.},
  archive   = {C_IROS},
  author    = {Sébastien Kleff and Ewen Dantec and Guilhem Saurel and Nicolas Mansard and Ludovic Righetti},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982003},
  pages     = {13379-13385},
  title     = {Introducing force feedback in model predictive control},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Constrained differential dynamic programming: A primal-dual
augmented lagrangian approach. <em>IROS</em>, 13371–13378. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trajectory optimization is an efficient approach for solving optimal control problems for complex robotic systems. It relies on two key components: first the transcription into a sparse nonlinear program, and second the corresponding solver to iteratively compute its solution. On one hand, differential dynamic programming (DDP) provides an efficient approach to transcribe the optimal control problem into a finite-dimensional problem while optimally exploiting the sparsity induced by time. On the other hand, augmented Lagrangian methods make it possible to formulate efficient algorithms with advanced constraint-satisfaction strategies. In this paper, we propose to combine these two approaches into an efficient optimal control algorithm accepting both equality and inequality constraints. Based on the augmented Lagrangian literature, we first derive a generic primal-dual augmented Lagrangian strategy for nonlinear problems with equality and inequality constraints. We then apply it to the dynamic programming principle to solve the value-greedy optimization problems inherent to the backward pass of DDP, which we combine with a dedicated globalization strategy, resulting in a Newton-like algorithm for solving constrained trajectory optimization problems. Contrary to previous attempts of formu-lating an augmented Lagrangian version of DDP, our approach exhibits adequate convergence properties without any switch in strategies. We empirically demonstrate its interest with several case-studies from the robotics literature.},
  archive   = {C_IROS},
  author    = {Wilson Jallet and Antoine Bambade and Nicolas Mansard and Justin Carpentier},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981586},
  pages     = {13371-13378},
  title     = {Constrained differential dynamic programming: A primal-dual augmented lagrangian approach},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Newton-PnP: Real-time visual navigation for autonomous
toy-drones. <em>IROS</em>, 13363–13370. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Perspective-n-Point problem aims to estimate the relative pose between a calibrated monocular camera and a known 3D model, by aligning pairs of 2D captured image points to their corresponding 3D points in the model. We suggest an algorithm that runs on weak IoT devices in real-time but still provides provable theoretical guarantees for both running time and correctness. Existing solvers provide only one of these requirements. Our main motivation was to turn the popular DJI&#39;s Tello Drone (&lt;90gr, &lt;$100) into an autonomous drone that navigates in an indoor environment with no external human/laptop/sensor, by simply attaching a Raspberry PI Zero (&lt;9gr, &lt;$25) to it. This tiny micro-processor takes as input a real-time video from a tiny RGB camera, and runs our PnP solver on-board. Extensive experimental results, open source code, and a demonstration video are included.},
  archive   = {C_IROS},
  author    = {Ibrahim Jubran and Fares Fares and Yuval Alfassi and Firas Ayoub and Dan Feldman},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981922},
  pages     = {13363-13370},
  title     = {Newton-PnP: Real-time visual navigation for autonomous toy-drones},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Refining control barrier functions through hamilton-jacobi
reachability. <em>IROS</em>, 13355–13362. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safety filters based on Control Barrier Functions (CBFs) have emerged as a practical tool for the safety-critical control of autonomous systems. These approaches encode safety through a value function and enforce safety by imposing a constraint on the time derivative of this value function. How-ever, synthesizing a valid CBF that is not overly conservative in the presence of input constraints is a notorious challenge. In this work, we propose refining a candidate CBF using formal verification methods to obtain a valid CBF. In particular, we update an expert-synthesized or backup CBF using dynamic programming (DP) based reachability analysis. Our framework, REFINECBF, guarantees that with every DP iteration the obtained CBF is provably at least as safe as the prior iteration and converges to a valid CBF. Therefore, REFINECBF can be used in-the-loop for robotic systems. We demonstrate the practicality of our method to enhance safety and/or reduce conservativeness on a range of nonlinear control-affine systems using various CBF synthesis techniques in simulation.},
  archive   = {C_IROS},
  author    = {Sander Tonkens and Sylvia Herbert},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982203},
  pages     = {13355-13362},
  title     = {Refining control barrier functions through hamilton-jacobi reachability},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An equivalent time-optimal problem to find energy-optimal
paths for skid-steer rovers. <em>IROS</em>, 13341–13346. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A skid-steer rover&#39;s power consumption is highly dependent on the turning radius of its path. For example, a point turn consumes a lot of power compared to a straight-line motion. Thus, in path planning for this kind of rover, turning radius is a factor that should be considered explicitly. There is a lack of any analytical approach in literature for finding energy-optimal paths for skid-steer rovers. The key contribution of this work is an energy-time equivalency theorem, for skid-steer rovers on obstacle-free hard ground. The theorem converts the energy-optimal problem into an equivalent time-optimal problem. This non-intuitive result stems from the fact that with this model of the system the total energy is fully parameterized by the geometry of the path alone. Hence, instead of directly solving the energy-optimal path planning problem, which is highly nonlinear, the equivalent time-optimal problem can be solved. Furthermore, experimental results are provided to experimentally prove the equivalency theorem while using Husky UGV skid-steer rover.},
  archive   = {C_IROS},
  author    = {Meysam Effati and Krzysztof Skonieczny and Tim Freiman and Devin J. Balkcom},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982069},
  pages     = {13341-13346},
  title     = {An equivalent time-optimal problem to find energy-optimal paths for skid-steer rovers},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A legendre-gauss pseudospectral collocation method for
trajectory optimization in second order systems. <em>IROS</em>,
13335–13340. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pseudospectral collocation methods have proven to be powerful tools to solve optimal control problems. While these methods generally assume the dynamics is given in the first order form $x$ = f(x, u, t), where $x$ is the state and $u$ is the control vector, robotic systems are typically governed by second order ODEs of the form $q$ = g(q, q, u, t), where $q$ is the configuration. To convert the second order ODE into a first order one, the usual approach is to introduce a velocity variable $v$ and impose its coincidence with the time derivative of q. Lobatto methods grant this constraint by construction, as their polynomials describing the trajectory for $v$ are the time derivatives of those for q, but the same cannot be said for the Gauss and Radau methods. This is problematic for such methods, as then they cannot guarantee that $q$ = g(q, q, u, t) at the collocation points. On their negative side, Lobatto methods cannot be used to solve initial value problems, as given the values of $u$ at the collocation points they generate an overconstrained system of equations for the states. In this paper, we propose a Legendre-Gauss collocation method that retains the advantages of the usual Lobatto, Gauss, and Radau methods, while avoiding their shortcomings. The collocation scheme we propose is applicable to solve initial value problems, preserves the consistency between the polynomials for $v$ and q, and ensures that $q$ = g(q, q, u, t) at the collocation points.},
  archive   = {C_IROS},
  author    = {Siro Moreno-Martín and Lluís Ros and Enric Celaya},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981255},
  pages     = {13335-13340},
  title     = {A legendre-gauss pseudospectral collocation method for trajectory optimization in second order systems},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Upper limb movement estimation and function evaluation of
the shoulder girdle by multi-sensing flexible sensor wear.
<em>IROS</em>, 13328–13334. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To extend the coverage of people able to receive high-quality rehabilitation, remote rehabilitation is required in addition to traditional face-to-face rehabilitation. Although remote rehabilitation using video conferencing systems has been realized to date, communication through physical sensations such as detailed patient motoring information and manual instructions from the therapist has not yet been realized. Therefore, the ultimate goal of this study was to develop multimodal wearable sensor system to support remote rehabilitation with a somatosensory system. To this end, we conducted a basic study of sensing technology in multimodal wear. Multiple strain sensors were attached to the shoulder to digitize the detailed behavior of the shoulder girdle. It was confirmed that the movement of the scapula can be acquired by this strain sensor. Furthermore, it was confirmed that the combination of strain sensors and an inertia measurement unit can be applied for the motion estimation of the entire upper limb.},
  archive   = {C_IROS},
  author    = {Kunihiro Ogata and Shusuke Kanazawa and Hideyuki Tanaka and Takeshi Kurata},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982102},
  pages     = {13328-13334},
  title     = {Upper limb movement estimation and function evaluation of the shoulder girdle by multi-sensing flexible sensor wear},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). From timing variations to performance degradation:
Understanding and mitigating the impact of software execution timing in
SLAM. <em>IROS</em>, 13308–13315. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Timing is an important property for robotic systems that continuously interact with our physical world. Variation in program execution time caused by limited computational resources or system resource contention can lead to significant impact on algorithmic result accuracy. Even though recent work has found Simultaneous Localization And Mapping (SLAM) to be timing-sensitive, little exists in understanding the interactions between the timing variations in SLAM systems and the corresponding degradation. In this paper we conduct a systematic analysis of nine state-of-the-art SLAM systems and dissect the root causes of their degradation. We discovered that timing-induced errors are generated either from delayed execution in certain critical tasks, or from desynchronization in sensor fusion. Based on the insights from our analysis, we propose a solution that combines selective fusion on data in the front end and temporal budget optimization on bundle adjust-ment in the backend to mitigate the impacts of unexpected timing variation adaptively. Experimental results show that our proposed method makes it possible to migrate expensive algorithms to low-cost platforms without laborious tuning, while making the SLAM system robust against the effects of abnormal timing.},
  archive   = {C_IROS},
  author    = {Ao Li and Han Liu and Jinwen Wang and Ning Zhang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981275},
  pages     = {13308-13315},
  title     = {From timing variations to performance degradation: Understanding and mitigating the impact of software execution timing in SLAM},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DeepCIR: Insights into CIR-based data-driven UWB error
mitigation. <em>IROS</em>, 13300–13307. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ultra-Wide-Band (UWB) ranging sensors have been widely adopted for robotic navigation thanks to their extremely high bandwidth and hence high resolution. However, off-the-shelf devices may output ranges with significant errors in cluttered, severe non-line-of-sight (NLOS) environments. Recently, neural networks have been actively studied to improve the ranging accuracy of UWB sensors using the channel-impulse-response (CIR) as input. However, previous works have not systematically evaluated the efficacy of various packet types and their possible combinations in a two-way-ranging transaction, including poll, response and final packets. In this paper, we firstly investigate the utility of different packet types and their combinations when used as input for a neural network. Secondly, we propose two novel data-driven approaches, namely FMCIR and WMCIR, that leverage two-sided CIRs for efficient UWB error mitigation. Our approaches outperform state-of-the-art by a significant margin, further reducing range errors up to 45\%. Finally, we create and release a dataset of transaction-level synchronized CIRs (each sample consists of the CIR of the poll, response and final packets), which will enable further studies in this area.},
  archive   = {C_IROS},
  author    = {Vu Tran and Zhuangzhuang Dai and Niki Trigoni and Andrew Markham},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981931},
  pages     = {13300-13307},
  title     = {DeepCIR: Insights into CIR-based data-driven UWB error mitigation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A LiDAR-inertial odometry with principled uncertainty
modeling. <em>IROS</em>, 13292–13299. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a LiDAR-inertial odometry that properly solves the uncertainty estimation problem, guided by the rules of designing a consistent estimator. Our system is built upon an iterated extended Kalman filter, with multiple states in an optimization window. To survive environments without distinctive geometric structures, we do not track features over time. We only extract planar primitives from the local map and use a direct point-to-plane distance metric as the measurement model. The realistic noise parameters are estimated online by modeling point distributions. We use nullspace projection to remove dependency on the feature planes, which is equivalent to transforming the pose-map measurement into relative pose constraints. To avoid reintegrating all the laser points in the local window after every state correction, we use the Schmidt Kalman update to consider the probabilistic effects of past poses while their values are left unaltered. A collection of octrees with an adaptive resolution is designed to manage measurement points and the map efficiently. The consistency and robustness of our system are verified in both simulation and real-world experiments.},
  archive   = {C_IROS},
  author    = {Binqian Jiang and Shaojie Shen},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981157},
  pages     = {13292-13299},
  title     = {A LiDAR-inertial odometry with principled uncertainty modeling},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Patchwork++: Fast and robust ground segmentation solving
partial under-segmentation using 3D point cloud. <em>IROS</em>,
13276–13283. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the field of 3D perception using 3D LiDAR sensors, ground segmentation is an essential task for various purposes, such as traversable area detection and object recognition. Under these circumstances, several ground segmentation methods have been proposed. However, some limitations are still encountered. First, some ground segmentation methods require fine-tuning of parameters depending on the surroundings, which is excessively laborious and time-consuming. Moreover, even if the parameters are well adjusted, a partial under-segmentation problem can still emerge, which implies ground segmentation failures in some regions. Finally, ground segmentation methods typically fail to estimate an appropriate ground plane when the ground is above another structure, such as a retaining wall. To address these problems, we propose a robust ground segmentation method called Patchwork++, an extension of Patchwork. Patchwork++ exploits adaptive ground likelihood estimation (A-GLE) to calculate appropriate parameters adaptively based on the previous ground segmentation results. Moreover, temporal ground revert (TGR) alleviates a partial under-segmentation problem by using the temporary ground property. Also, region-wise vertical plane fitting (R-VPF) is introduced to segment the ground plane properly even if the ground is elevated with different layers. Finally, we present reflected noise removal (RNR) to eliminate virtual noise points efficiently based on the 3D LiDAR reflection model. We demonstrate the qualitative and quantitative evaluations using a SemanticKITTI dataset. Our code is available at https://github.com/url-kaist/patchwork-plusplus},
  archive   = {C_IROS},
  author    = {Seungjae Lee and Hyungtae Lim and Hyun Myung},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981561},
  pages     = {13276-13283},
  title     = {Patchwork++: Fast and robust ground segmentation solving partial under-segmentation using 3D point cloud},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learned depth estimation of 3D imaging radar for indoor
mapping. <em>IROS</em>, 13260–13267. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D imaging radar offers robust perception capability through visually demanding environments due to the unique penetrative and reflective properties of millimeter waves (mmWave). Current approaches for 3D perception with imaging radar require knowledge of environment geometry, accumulation of data from multiple frames for perception, or access to between-frame motion. Imaging radar presents an additional difficulty due to the complexity of its data representation. To address these issues, and make imaging radar easier to use for downstream robotics tasks, we propose a learning-based method that regresses radar measurements into cylindrical depth maps using LiDAR supervision. Due to the limitation of the regression formulation, directions where the radar beam could not reach will still generate a valid depth. To address this issue, our method additionally learns a 3D filter to remove those pixels. Experiments show that our system generates visually accurate depth estimation. Furthermore, we confirm the overall ability to generalize in the indoor scene using the estimated depth for probabilistic occupancy mapping with ground truth trajectory. The code and model will be released 1 1 https://github.com/rpl-cmu/learned-depth-imaging-radar.},
  archive   = {C_IROS},
  author    = {Ruoyang Xu and Wei Dong and Akash Sharma and Michael Kaess},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981572},
  pages     = {13260-13267},
  title     = {Learned depth estimation of 3D imaging radar for indoor mapping},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Vertical bend and t-branch travels of an articulated wheeled
in-pipe inspection robot by combining its joint angle and torque
controls. <em>IROS</em>, 13254–13259. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The paper reports the performance verification of vertical bend and T-branch travels of an articulated wheeled in-pipe inspection robot. The robot is composed of only a single active compliant middle joint, two passive compliant joints, three drive wheels, and two roll wheels. The passage of the bend pipe is achieved only by the joint torque control, while the T-branch travel is achieved by controlling both joint angle and torque. Instead of using a torque sensor, a polyurethane-based series elastic actuator (SEA) is installed in the middle joint. In this paper, the travel performances of our developed in-pipe robot were tested on bend pipes and 10 types of T-branch with different gravity directions. From the experiments, in all cases, the effectiveness of the bend and T-branch travels performance was confirmed.},
  archive   = {C_IROS},
  author    = {Atsushi Kakogawa and Kenya Murata and Shugen Ma},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982260},
  pages     = {13254-13259},
  title     = {Vertical bend and T-branch travels of an articulated wheeled in-pipe inspection robot by combining its joint angle and torque controls},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An in-pipe crawling robot based on tensegrity structures.
<em>IROS</em>, 13248–13253. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel concept to develop robots capable of crawling in tubular environments, inspired by the movement of earthworms and the biological musculoskeletal systems in nature. A tensegrity structures-based robotic module with shape changeability actuated by only one linear actuator is proposed. The mechanical structure of the robotic module is determined on the basis of force density method. By serially cascading three uniform modules, the in-pipe crawling robot is designed and manufactured. The robot has the abilities to crawl in both horizontal and vertical pipes with different inner diameters, and to pass through elbow pipes adaptively under the control of a simple actuation sequence. The effectiveness of the robot is demonstrated by experimental results on the prototype. Compared with existing robots, this proposed approach enables compact yet robust structures, along with enhanced compliance, mobility, and adaptability.},
  archive   = {C_IROS},
  author    = {Yixiang Liu and Qing Bi and Xiaolin Dai and Rui Song and Xizhe Zang and Yibin Li},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981912},
  pages     = {13248-13253},
  title     = {An in-pipe crawling robot based on tensegrity structures},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Anisotropic-stiffness belt in mono wheeled flexible track
for rough terrain locomotion. <em>IROS</em>, 13227–13232. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Rescue robots that search around on debris during natural disasters require high mobility to overcome various shaped materials scattered in the environment. Our previous study developed a new tracked mechanism called Mono-wheel Track, an elastic track driven by a single wheel, having a high capability to get over obstacles. In designing the MW - Track, the track stiffness is an essential factor-the flexible track can adapt to the geometry of the obstacles, but the flexibility prevents grousers from anchoring to the environment steadily. If the track has different localized stiffnesses, both the adaptability and the stability might be archived. In this study, we developed an “anisotropic-stiffness track,” exhibiting different stiffness depending on the bending side, and investigated its deformation characteristics and the effects on mobility. The basic deformation characteristics of the track were confirmed by load tests. The effects on mobility were evaluated by step-climbing tests, ditch-crossing tests, and traction measuring with a mobile robot.},
  archive   = {C_IROS},
  author    = {Yu Ozawa and Masahiro Watanabe and Kenjiro Tadakuma and Satoshi Tadokoro},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981247},
  pages     = {13227-13232},
  title     = {Anisotropic-stiffness belt in mono wheeled flexible track for rough terrain locomotion},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Jumping on air: Design and modeling of latch-mediated,
spring-actuated air-jumpers. <em>IROS</em>, 13220–13226. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Latch-mediated spring-actuation (LaMSA) is utilized in a majority of jumping robots for its ability to slowly load and quickly release energy to generate high-power movement. Such mechanisms are found in robots that jump off of solid surfaces and even off of water. However, no robot currently employs LaMSA to jump on air. This paper presents the design, modeling, and fabrication of the first LaMSA-driven air jumper, capable of jumping mid-air. Our model informs prototype design and provides insight into the scaling properties of the wing area, wing and fuselage mass, and energy. By successfully applying LaMSA to a new domain, this work lays the foundation for future investigations into high-power airreaction maneuvers, such as in fixed-wing unmanned aerial vehicle (UAV) flight, by enabling instantaneous changes in altitude without the addition of extra on-board motors.},
  archive   = {C_IROS},
  author    = {Anna V. Alvarez and Matthew R. Devlin and Nicholas D. Naclerio and Elliot W. Hawkes},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981655},
  pages     = {13220-13226},
  title     = {Jumping on air: Design and modeling of latch-mediated, spring-actuated air-jumpers},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DeltaZ: An accessible compliant delta robot manipulator for
research and education. <em>IROS</em>, 13213–13219. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents the DeltaZ robot, a centimeter-scale, low-cost, delta-style robot that allows for a broad range of capabilities and robust functionalities. The DeltaZ robot is 3D-printed from soft and rigid materials with a design that is easy to assemble and maintain, and lowers the barriers to utilize. Functionality of the robot stems from its three translational degrees of freedom and a closed form kinematic solution which makes manipulation problems more intuitive compared to many other manipulators. Moreover, the low cost of the robot presents an opportunity to democratize manipulators for research and education settings. We describe how the robot can be used as a reinforcement learning bench-mark. Open-source 3D-printable designs and code for building and using the robot are available to the public.},
  archive   = {C_IROS},
  author    = {Sarvesh Patil and Samuel C. Alvares and Pragna Mannam and Oliver Kroemer and F. Zeynep Temel},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981257},
  pages     = {13213-13219},
  title     = {DeltaZ: An accessible compliant delta robot manipulator for research and education},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BOBCAT: Behaviors, objectives and binary states for
coordinated autonomous tasks. <em>IROS</em>, 13189–13196. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present our framework Behaviors, Objectives and Binary states for Coordinated Autonomous Tasks (BOBCAT), a multi-agent decision making and task management system for autonomous robots. BOBCAT builds on behavior-based systems and the Belief-Desire-Intention model for decision-making, by using mission objectives as the basis for selecting tasks, or behaviors. We describe how BOBCAT is formulated and present the results of a real-world implementation in the context of exploring austere underground environments in the DARPA Subterranean Challenge.},
  archive   = {C_IROS},
  author    = {Danny G. Riley and Eric W. Frew},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981135},
  pages     = {13189-13196},
  title     = {BOBCAT: Behaviors, objectives and binary states for coordinated autonomous tasks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stochastic games with stopping states and their application
to adversarial motion planning problems. <em>IROS</em>, 13181–13188. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We model a finite horizon decision making process between an ego and a non-ego vehicle, where the non-ego vehicle has a certain probability of moving adversarially over each planning stage. The adversarial intent of the non-ego vehicle is inferred only when a particular set of actions are performed by both vehicles, thereby creating a stopping state. We term such a decision-making process as a multi-stage stochastic zero-sum game (SSG) with stopping states, i.e., once adversarial intent is ascertained, the non-ego vehicle continues to choose its actions adversarially for the remaining stages of the interaction. We analytically characterize the Nash equilibria of this game for the case of two actions per player. We then demonstrate this approach via two autonomous motion planning applications. The first involves maintaining a safe distance from a non-ego vehicle ahead, modeled using fixed stage costs. The second involves safe lane-changing with costs that are stage dependent. In both scenarios, we provide a comparison between the analytic/simulated and experimental results using ground robots.},
  archive   = {C_IROS},
  author    = {Sandeep Banik and Shaunak D. Bopardikar},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982011},
  pages     = {13181-13188},
  title     = {Stochastic games with stopping states and their application to adversarial motion planning problems},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Low-drift LiDAR-only odometry and mapping for UGVs in
environments with non-level roads. <em>IROS</em>, 13174–13180. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study focuses on localization and mapping for UGVs when they are deployed in environments with non-level roads. In these scenarios, the vehicles need to travel through flat but not necessarily level grounds, i.e., ascent or descent, which may cause drifts of the robot pose and distortion of the map. We develop a low-drift LiDAR odometry and mapping approach for the UGV with LiDAR as the only exteroceptive sensor. A factor-graph based pose optimization method is developed with a specifically designed factor named slope factor. This factor includes the slope information that is estimated from a real-time LiDAR data stream. The slope information is also used to enhance the loop-closure detection procedure. Moreover, an incremental pitch estimation mechanism is designed to achieve further pose estimation refinement. We demonstrate the effectiveness of the developed framework in real-world environments. The odometry drift is lower and the map is more precise than experiments with the state-of-the-arts. Notably, on the Kitti dataset, our method also exhibits convincing performance, demonstrating its strength in more general application scenarios.},
  archive   = {C_IROS},
  author    = {Xiangyu Chen and Yinchuan Wang and Chaoqun Wang and Rui Song and Yibin Li},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982264},
  pages     = {13174-13180},
  title     = {Low-drift LiDAR-only odometry and mapping for UGVs in environments with non-level roads},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Handling non-convex constraints in MPC-based humanoid gait
generation. <em>IROS</em>, 13167–13173. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In most MPC-based schemes used for humanoid gait generation, simple Quadratic Programming (QP) problems are considered for real-time implementation. Since these only allow for convex constraints, the generated gait may be conservative. In this paper we focus on the non-convex reachable region of the swinging foot, also known as Kinematic Admissible Region (KAR), and the corresponding constraint. We represent an approximation of such non-convex region as the union of multiple non-overlapping convex sub-regions. By leveraging the concept of feasibility region, i.e., the subset of the state space for which a QP problem is feasible, and introducing a proper selection criterion, we are able to maintain linearity of the constraints and thus use our Intrinsically Stable Model Predictive Control (IS-MPC) scheme with a negligible additional computational load. This approach allows for a wider range of possible generated motions and is very effective when reacting to a push or avoiding an obstacle, as illustrated in dynamically simulated scenarios.},
  archive   = {C_IROS},
  author    = {Andrew S. Habib and Filippo M. Smaldone and Nicola Scianca and Leonardo Lanari and Giuseppe Oriolo},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981419},
  pages     = {13167-13173},
  title     = {Handling non-convex constraints in MPC-based humanoid gait generation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Factorization of dynamic games over spatio-temporal
resources. <em>IROS</em>, 13159–13166. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dynamic games feature a state-space complexity that scales superlinearly with the number of players. This makes this class of games often intractable even for a handful of players. We introduce the factorization process of dynamic games as a transformation leveraging the independence of players at equilibrium to build a leaner game graph. When applicable, it yields fewer nodes, fewer players per game node, hence much faster solutions. While for the general case checking for independence of players requires to solve the game itself, we observe that for dynamic games in the robotic domain there exist exact heuristics based on the spatio-temporal occupancy of the individual players. We validate our findings in realistic autonomous driving scenarios showing that already for a 4-player intersection we have a reduction of game nodes and solving time close to 99\%.},
  archive   = {C_IROS},
  author    = {Alessandro Zanardi and Saverio Bolognani and Andrea Censi and Florian Dorfler and Emilio Frazzoli},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981416},
  pages     = {13159-13166},
  title     = {Factorization of dynamic games over spatio-temporal resources},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visibility-inspired models of touch sensors for navigation.
<em>IROS</em>, 13151–13158. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces mathematical models of touch sensors for mobile robots based on visibility. Serving a purpose similar to the pinhole camera model for computer vision, the introduced models are expected to provide a useful, idealized characterization of task-relevant information that can be inferred from their outputs or observations. Possible tasks include navigation, localization and mapping when a mobile robot is deployed in an unknown environment. These models allow direct comparisons to be made between traditional depth sensors, highlighting cases in which touch sensing may be interchangeable with time of flight or vision sensors, and char-acterizing unique advantages provided by touch sensing. The models include contact detection, compression, load bearing, and deflection. The results could serve as a basic building block for innovative touch sensor designs for mobile robot sensor fusion systems.},
  archive   = {C_IROS},
  author    = {Kshitij Tiwari and Basak Sakcak and Prasanna Routray and M. Manivannan and Steven M. LaValle},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981084},
  pages     = {13151-13158},
  title     = {Visibility-inspired models of touch sensors for navigation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cola-HRL: Continuous-lattice hierarchical reinforcement
learning for autonomous driving. <em>IROS</em>, 13143–13150. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning (RL) has shown promising performance in autonomous driving applications in recent years. The early end-to-end RL method is usually unexplainable and fails to generate stable actions, while the hierarchical RL (HRL) method can tackle the above issues by dividing complex problems into multiple sub-tasks. Prior HRL works either select discrete driving behaviors with continuous control commands, or generate expected goals for the low-level controller. However, they typically have strong scenario dependence or fail to generate goals with good quality. To address the above challenges, we propose a Continuous-Lattice Hierarchical RL (Cola-HRL) method for autonomous driving tasks to make high-quality decisions in various scenarios. We utilize the continuous-lattice module to generate reasonable goals, ensuring temporal and spatial reachability. Then, we train and evaluate our method under different traffic scenarios based on real-world High Definition maps. Experimental results show our method can handle multiple scenarios. In addition, our method also demonstrates better performance and driving behaviors compared to existing RL methods.},
  archive   = {C_IROS},
  author    = {Lingping Gao and Ziqing Gu and Cong Qiu and Lanxin Lei and Shengbo Eben Li and Sifa Zheng and Wei Jing and Junbo Chen},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982041},
  pages     = {13143-13150},
  title     = {Cola-HRL: Continuous-lattice hierarchical reinforcement learning for autonomous driving},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A biologically-inspired simultaneous localization and
mapping system based on LiDAR sensor. <em>IROS</em>, 13136–13142. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simultaneous localization and mapping (SLAM) is one of the essential techniques and functionalities used by robots to perform autonomous navigation tasks. Inspired by the rodent hippocampus, this paper presents a biologically inspired SLAM system based on a LiDAR sensor using a hippocampal model to build a cognitive map and estimate the robot pose in indoor environments. Based on the biologically inspired models mimicking boundary cells, place cells, and head direction cells, the SLAM system using LiDAR point cloud data is capable of leveraging the self-motion cues from the LiDAR odometry and the boundary cues from the LiDAR boundary cells to build a cognitive map and estimate the robot pose. Experiment results show that with the LiDAR boundary cells the proposed SLAM system greatly outperforms the camera-based brain-inspired method in both simulation and indoor environments, and is competitive with the conventional LiDAR-based SLAM methods.},
  archive   = {C_IROS},
  author    = {Genghang Zhuang and Zhenshan Bing and Yuhong Huang and Kai Huang and Alois Knoll},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981362},
  pages     = {13136-13142},
  title     = {A biologically-inspired simultaneous localization and mapping system based on LiDAR sensor},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IndoLayout: Leveraging attention for extended indoor layout
estimation from an RGB image. <em>IROS</em>, 13128–13135. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we propose IndoLayout, a novel real-time approach for generating high-quality occupancy maps from an RGB image for indoor scenes. Such occupancy maps are often crucial for path-planning and mapping in indoor environments but are often built using only information contained in the ego view. In contrast, our approach also predicts occupancy values beyond immediately visible regions from just a monocular image, leveraging learnt priors from indoor scenes. Hence, our proposed network can produce a hallucinated, amodal scene layout that includes areas occluded in the RGB image, such as a navigable floor behind a desk. Specifically, we propose a novel architecture that uses self-attention and adversarial learning to vastly improve the quality of the predicted layout. We evaluate our model on several photorealistic indoor datasets and outperform previous relevant work on all metrics that measure layout quality, including newly adopted ones. Finally, we demonstrate the effectiveness of our method by showing significant improvements on the PointNav task over similar approaches using IndoLayout. For more details, please refer to the project page: https://indolayout.github.io/.},
  archive   = {C_IROS},
  author    = {Shantanu Singh and Jaidev Shriram and Shaantanu Kulkarni and Brojeshwar Bhowmick and K. Madhava Krishna},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982106},
  pages     = {13128-13135},
  title     = {IndoLayout: Leveraging attention for extended indoor layout estimation from an RGB image},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sequential thermal image-based adult and baby detection
robust to thermal residual heat marks. <em>IROS</em>, 13120–13127. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The awareness for preserving privacy in in-home monitoring robots is increasing. Although several studies have proposed privacy-preserved in-home monitoring robot systems for adults, only a limited amount of attention has been paid attention to research on privacy-preserved in-home monitoring of babies. Like previous studies, thermal infrared image-based methods could ensure a privacy-preserved monitoring of babies, yet when existing detection methods were applied to thermal images to detect babies and adults, we discovered a frequent occurrence of misdetection due to the presence of thermal residual heat marks. In this research, we propose a sequential thermal image-based detection that conjugated the characteristics of thermal residual heat marks. The proposed detection reduced misdetection caused by thermal residual heat marks by 98.7\% when compared to RetinaNet. In addition, we open-source our collected thermal image-based baby and adult dataset via: https://github.com/donkeymouse/ThermalAdultandBaby.},
  archive   = {C_IROS},
  author    = {Dong-Guw Lee and Kyu-Seob Song and Young-Hoon Nho and Ayoung Kim and Dong-Soo Kwon},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982115},
  pages     = {13120-13127},
  title     = {Sequential thermal image-based adult and baby detection robust to thermal residual heat marks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Grasp pre-shape selection by synthetic training: Eye-in-hand
shared control on the hannes prosthesis. <em>IROS</em>, 13112–13119. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the task of object grasping with a prosthetic hand capable of multiple grasp types. In this setting, communicating the intended grasp type often requires a high user cognitive load which can be reduced adopting shared autonomy frameworks. Among these, so-called eye-in-hand systems automatically control the hand pre-shaping before the grasp, based on visual input coming from a camera on the wrist. In this paper, we present an eye-in-hand learning-based approach for hand pre-shape classification from RGB sequences. Differently from previous work, we design the system to support the possibility to grasp each considered object part with a different grasp type. In order to overcome the lack of data of this kind and reduce the need for tedious data collection sessions for training the system, we devise a pipeline for rendering synthetic visual sequences of hand trajectories. We develop a sensorized setup to acquire real human grasping sequences for benchmarking and show that, compared on practical use cases, models trained with our synthetic dataset achieve better generalization performance than models trained on real data. We finally integrate our model on the Hannes prosthetic hand and show its practical effectiveness. We make publicly available the code and dataset to reproduce the presented results 1 1 https://github.com/hsp-iit/prosthetic-grasping-simulation.},
  archive   = {C_IROS},
  author    = {Federico Vasile and Elisa Maiettini and Giulia Pasquale and Astrid Florio and Nicolò Boccardo and Lorenzo Natale},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981035},
  pages     = {13112-13119},
  title     = {Grasp pre-shape selection by synthetic training: Eye-in-hand shared control on the hannes prosthesis},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Object pose estimation using mid-level visual
representations. <em>IROS</em>, 13105–13111. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work proposes a novel pose estimation model for object categories that can be effectively transferred to pre-viously unseen environments. The deep convolutional network models (CNN) for pose estimation are typically trained and evaluated on datasets specifically curated for object detection, pose estimation, or 3D reconstruction, which requires large amounts of training data. In this work, we propose a model for pose estimation that can be trained with small amount of data and is built on the top of generic mid-level represen-tations [33] (e.g. surface normal estimation and re-shading). These representations are trained on a large dataset without requiring pose and object annotations. Later on, the predictions are refined with a small CNN neural network that exploits object masks and silhouette retrieval. The presented approach achieves superior performance on the Pix3D dataset [26] and shows nearly 35\% improvement over the existing models when only 25\% of the training data is available. We show that the approach is favorable when it comes to generalization and transfer to novel environments. Towards this end, we introduce a new pose estimation benchmark for commonly encountered furniture categories on challenging Active Vision Dataset [1] and evaluated the models trained on the Pix3D dataset.},
  archive   = {C_IROS},
  author    = {Negar Nejatishahidin and Pooya Fayyazsanavi and Jana Košecka},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981452},
  pages     = {13105-13111},
  title     = {Object pose estimation using mid-level visual representations},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Realism assessment for synthetic images in robot vision
through performance characterization. <em>IROS</em>, 13089–13096. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Synthetic image generation plays a crucial role in the development of robot vision algorithms, circumventing manual data collection. However, the realism of synthetic images could affect the performance of the algorithms when applied in real-world settings. In this study, we propose a framework to quantitatively assess the realism of synthetic images using a set of realism metrics as a means of performance characterization. We use a commercial rendering engine as a test-bed for generating synthetic images and ascertain that a set of rendering parameters affect specific image metrics through statistical hypothesis testing. We demonstrate that this framework can be used to optimize rendering parameter values and generate synthetic datasets with improved performance on downstream robot vision tasks such as instance segmentation.},
  archive   = {C_IROS},
  author    = {Arturo E. Ceron-Lopez and Rahul Ranjan and Nishanth Koganti},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982192},
  pages     = {13089-13096},
  title     = {Realism assessment for synthetic images in robot vision through performance characterization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 6-DoF pose estimation of household objects for robotic
manipulation: An accessible dataset and benchmark. <em>IROS</em>,
13081–13088. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a new dataset for 6-DoF pose estimation of known objects, with a focus on robotic manipulation research. We propose a set of toy grocery objects, whose physical instantiations are readily available for purchase and are appropriately sized for robotic grasping and manipulation. We provide 3D scanned textured models of these objects, suitable for generating synthetic training data, as well as RGBD images of the objects in challenging, cluttered scenes exhibiting partial occlusion, extreme lighting variations, multiple instances per image, and a large variety of poses. Using semi-automated RGBD-to-model texture correspondences, the images are annotated with ground truth poses accurate within a few millimeters. We also propose a new pose evaluation metric called ADD-H based on the Hungarian assignment algorithm that is robust to symmetries in object geometry without requiring their explicit enumeration. We share pre-trained pose estimators for all the toy grocery objects, along with their baseline performance on both validation and test sets. We offer this dataset to the community to help connect the efforts of computer vision researchers with the needs of roboticists. 1 1 https://github.com/swtyree/hope-dataset},
  archive   = {C_IROS},
  author    = {Stephen Tyree and Jonathan Tremblay and Thang To and Jia Cheng and Terry Mosier and Jeffrey Smith and Stan Birchfield},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981838},
  pages     = {13081-13088},
  title     = {6-DoF pose estimation of household objects for robotic manipulation: An accessible dataset and benchmark},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EVOPS benchmark: Evaluation of plane segmentation from RGBD
and LiDAR data. <em>IROS</em>, 13074–13080. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper provides the EVOPS dataset for plane segmentation from 3D data, both from RGBD images and LiDAR point clouds. We have designed two annotation methodologies (RGBD and LiDAR) running on well-known and widely-used datasets for SLAM evaluation and we have provided a complete set of benchmarking tools including point, planes and segmentation metrics. The data includes a total number of 10k RGBD and 7K LiDAR frames over different selected scenes which consist of high quality segmented planes. The experiments report quality of SOTA methods for RGBD plane segmentation on our annotated data. We also have provided learnable baseline for plane segmentation in LiDAR point clouds. All labeled data and benchmark tools used have been made publicly available https://evops.netlify.app/.},
  archive   = {C_IROS},
  author    = {Anastasiia Kornilova and Dmitrii Iarosh and Denis Kukushkin and Nikolai Goncharov and Pavel Mokeev and Arthur Saliou and Gonzalo Ferrer},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981470},
  pages     = {13074-13080},
  title     = {EVOPS benchmark: Evaluation of plane segmentation from RGBD and LiDAR data},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ProgressLabeller: Visual data stream annotation for training
object-centric 3D perception. <em>IROS</em>, 13066–13073. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual perception tasks often require vast amounts of labelled data, including 3D poses and image space segmen-tation masks. The process of creating such training data sets can prove difficult or time-intensive to scale up to efficacy for general use. Consider the task of pose estimation for rigid objects. Deep neural network based approaches have shown good performance when trained on large, public datasets. However, adapting these networks for other novel objects, or fine-tuning existing models for different environments, requires significant time investment to generate newly labelled instances. Towards this end, we propose ProgressLabeller as a method for more efficiently generating large amounts of 6D pose training data from color images sequences for custom scenes in a scalable manner. ProgressLabeller is intended to also support transparent or translucent objects, for which the previous methods based on depth dense reconstruction will fail. We demonstrate the effectiveness of ProgressLabeller by rapidly create a dataset of over 1M samples with which we fine-tune a state-of-the-art pose estimation network in order to markedly improve the downstream robotic grasp success rates. Progresslabeller is open-source at https://github.com/huijieZH/ProgressLabeller},
  archive   = {C_IROS},
  author    = {Xiaotong Chen and Huijie Zhang and Zeren Yu and Stanley Lewis and Odest Chadwicke Jenkins},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982076},
  pages     = {13066-13073},
  title     = {ProgressLabeller: Visual data stream annotation for training object-centric 3D perception},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detecting adversarial perturbations in multi-task
perception. <em>IROS</em>, 13050–13057. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While deep neural networks (DNNs) achieve impressive performance on environment perception tasks, their sensitivity to adversarial perturbations limits their use in practical applications. In this paper, we (i) propose a novel adversarial perturbation detection scheme based on multi-task perception of complex vision tasks (i.e., depth estimation and semantic segmentation). Specifically, adversarial perturbations are detected by inconsistencies between extracted edges of the input image, the depth output, and the segmentation output. To further improve this technique, we (ii) develop a novel edge consistency loss between all three modalities, thereby improving their initial consistency which in turn supports our detection scheme. We verify our detection scheme&#39;s effectiveness by employing various known attacks and image noises. In addition, we (iii) develop a multi-task adversarial attack, aiming at fooling both tasks as well as our detection scheme. Experimental evaluation on the Cityscapes and KITTI datasets shows that under an assumption of a 5\% false positive rate up to 100\% of images are correctly detected as adversarially perturbed, depending on the strength of the perturbation. Code is available at https://github.com/ifnspaml/AdvAttackDet. A short video at https://youtu.be/KKa6gOyWmH4 provides qualitative results.},
  archive   = {C_IROS},
  author    = {Marvin Klingner and Varun Ravi Kumar and Senthil Yogamani and Andreas Bär and Tim Fingscheidt},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981559},
  pages     = {13050-13057},
  title     = {Detecting adversarial perturbations in multi-task perception},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Domain knowledge driven pseudo labels for interpretable
goal-conditioned interactive trajectory prediction. <em>IROS</em>,
13034–13041. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion forecasting in highly interactive scenarios is a challenging problem in autonomous driving. In such scenarios, we need to accurately predict the joint behavior of interacting agents to ensure the safe and efficient navigation of autonomous vehicles. Recently, goal-conditioned methods have gained increasing attention due to their advantage in performance and their ability to capture the multimodality in trajec-tory distribution. In this work, we study the joint trajectory prediction problem with the goal-conditioned framework. In particular, we introduce a conditional-variational-autoencoder-based (CVAE) model to explicitly encode different interaction modes into the latent space. However, we discover that the vanilla model suffers from posterior collapse and cannot induce an informative latent space as desired. To address these issues, we propose a novel approach to avoid KL vanishing and induce an interpretable interactive latent space with pseudo labels. The proposed pseudo labels allow us to incorporate domain knowledge on interaction in a flexible manner. We motivate the proposed method using an illustrative toy example. In addition, we validate our framework on the Waymo Open Motion Dataset with both quantitative and qualitative evaluations.},
  archive   = {C_IROS},
  author    = {Lingfeng Sur and Chen Tang and Yaru Niu and Enna Sachdeva and Chiho Choi and Teruhisa Misu and Masayoshi Tomizuka and Wei Zhan},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982147},
  pages     = {13034-13041},
  title     = {Domain knowledge driven pseudo labels for interpretable goal-conditioned interactive trajectory prediction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Contextual driving scene perception from anonymous vehicle
bus data for automotive applications. <em>IROS</em>, 13010–13017. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, driving context perception has emerged as one of the key aspects to design driving assistance algorithms and user interfaces that are effective in adapting to different traffic situations or environments. To this aim, we introduce the Anonymous Driving Scene Perception (ADSP) Model, a novel deep neural network designed to classify anony-mous Controller Area Network (CAN)-bus data into multiple driving context domains. ADSP extends the idea of driving scene classification to time series signals, as previous works relied heavily on visual features. Our model achieved a multi -domain classification accuracy of 84.9\% on our custom-built naturalistic data set, as a combination of 92.7\% on road type classification and 90.1\% on binary traffic detection, performing 2.0\% and 1.6\% better than the state-of-the-art model for multivariate time series classification. Our work demonstrates the feasibility of driving scene classification from anonymous CAN-bus data, without collecting sensitive data from users (images or GPS).},
  archive   = {C_IROS},
  author    = {Marco Wiedner and Francesco Branca and Enrico Mion and Andrea Censi and Emilio Frazzoli},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981063},
  pages     = {13010-13017},
  title     = {Contextual driving scene perception from anonymous vehicle bus data for automotive applications},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Active mapping via gradient ascent optimization of shannon
mutual information over continuous SE(3) trajectories. <em>IROS</em>,
12994–13001. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The problem of active mapping aims to plan an informative sequence of sensing views given a limited budget such as distance traveled. This paper considers active occupancy grid mapping using a range sensor, such as LiDAR or depth camera. State-of-the-art methods optimize information-theoretic measures relating the occupancy grid probabilities with the range sensor measurements. The non-smooth nature of ray-tracing within a grid representation makes the objective function non-differentiable, forcing existing methods to search over a discrete space of candidate trajectories. This work proposes a differentiable approximation of the Shannon mutual information between a grid map and ray-based observations that enables gradient ascent optimization in the continuous space of SE(3) sensor poses. Our gradient-based formulation leads to more informative sensing trajectories, while avoiding occlusions and collisions. The proposed method is demonstrated in simulated and real-world experiments in 2-D and 3-D environments. Materials supplementing this paper are available at: https://arashasgharivaskasi-bc.github.io/grad_active_mapping/},
  archive   = {C_IROS},
  author    = {Arash Asgharivaskasi and Shumon Koga and Nikolay Atanasov},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981875},
  pages     = {12994-13001},
  title     = {Active mapping via gradient ascent optimization of shannon mutual information over continuous SE(3) trajectories},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning agile hybrid whole-body motor skills for
thruster-aided humanoid robots. <em>IROS</em>, 12986–12993. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humanoid robots are versatile platforms with the potential for multiple locomotion skills. However, this contact-switched system with only two contact feet is fragile to keep balance in many scenarios. Inspired by birds combining legs and wings, we propose the novel hybrid locomotion behavior for the humanoid robots with the aid of a thruster suit. To fully leverage their agility while guaranteeing efficient computation, we combine the neural controller based on reinforcement learning to handle the complexity of the highly non-linear system and the optimization-based controller to explicitly handle the constraint conditions of the safety-critical thruster module. Our learning framework is demonstrated on several thruster-aided humanoid platforms with hybrid walking and even dynamic locomotion skills. To our best knowledge, it is the first work that, 1. demonstrates agile hybrid whole-body locomotion skills on the thruster-aided humanoid robot; 2. achieves hybrid locomotion under the reinforcement learning settings.},
  archive   = {C_IROS},
  author    = {Fan Shi and Tomoki Anzai and Yuta Kojio and Kei Okada and Masayuki Inaba},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981974},
  pages     = {12986-12993},
  title     = {Learning agile hybrid whole-body motor skills for thruster-aided humanoid robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Integration of variable-height and hopping strategies for
humanoid push recovery. <em>IROS</em>, 12979–12985. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this study, we present a framework to en-sure seamless transition in humanoid push recovery involving hopping strategy. We propose a method to adaptively change the time constant that integrated the ankle strategy and variable height strategy. This framework excites a hopping motion against a large disturbance, which provides a seamless transition from the variable height to the hopping strategies. We analyze the applicable region of each strategy based on the simplified model. Moreover, we show that the hopping strategy prevents falling through whole-body dynamic simulations.},
  archive   = {C_IROS},
  author    = {Ko Yamamoto and Naoki Kobayashi and Taiki Ishigaki and Yuichi Sakemi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981804},
  pages     = {12979-12985},
  title     = {Integration of variable-height and hopping strategies for humanoid push recovery},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Minor change, major gains II: Are maximal coordinates the
fastest choice for trajectory optimization? <em>IROS</em>, 12963–12970.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It has been shown that changing the coordinates describing a multi-body system to use absolute rather than relative angles produces a significant improvement in the tractability of trajectory optimization problems. This simplifies the equations of motion when modelling long kinematic chains. In this paper, we extend this idea by investigating whether a maximal coordinate system, which also describes the translational position of bodies using absolute coordinates, might lead to further performance improvements. We compare it to the relative translation, absolute orientation (RTAO) coordinate scheme using a batch of trajectory optimization trials selected with contact-implicit legged locomotion applications in mind. We find that maximal coordinates tend to shorten solving times for spatial problems, while the RTAO formulation still performs best in the case of planar motion.},
  archive   = {C_IROS},
  author    = {Stacey Shield and Amir Patel},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981885},
  pages     = {12963-12970},
  title     = {Minor change, major gains II: Are maximal coordinates the fastest choice for trajectory optimization?},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling, analysis and activation of planar
viscoelastically-combined rimless wheels. <em>IROS</em>, 12957–12962.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes novel passive-dynamic walk-ers formed by two cross-shaped frames and eight viscoelastic elements. Since it is a combination of two four-legged rimless wheels via viscoelastic elements, we call it viscoelastically-combined rimless wheel (VCRW). Two types of VCRWs consisting of different cross-shaped frames are introduced; one is formed by combining two Greek-cross-shaped frames (VCRW1), and the other is formed by combining two-link cross-shaped frames that can rotate freely around the central axis (VCRW2). First, we describe the model assumptions and equations of motion and collision. Second, we numerically analyze the basic gait properties of passive dynamic walking. Furthermore, we consider an activation of VCRW2 for gen-erating a stable level gait, and discuss the significance of the study as a novel walking support device.},
  archive   = {C_IROS},
  author    = {Fumihiko Asano and Yuxuan Xiang and Yanqiu Zheng and Cong Yan},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981411},
  pages     = {12957-12962},
  title     = {Modeling, analysis and activation of planar viscoelastically-combined rimless wheels},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time footstep planning and control of the solo
quadruped robot in 3D environments. <em>IROS</em>, 12950–12956. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Quadruped robots have proved their robustness to cross complex terrain despite little environment knowledge. Yet advanced locomotion controllers are expected to take advantage of exteroceptive information. This paper presents a complete method to plan and control the locomotion of quadruped robots when 3D information about the surrounding obstacles is available, based on several stages of decision. We first propose a contact planner formulated as a mixed-integer program, optimized on-line at each new robot step. It selects a surface from a set of convex surfaces describing the environment for the next footsteps while ensuring kinematic constraints. We then propose to optimize the exact contact location and the feet trajectories at control frequency to avoid obstacles, thanks to an efficient formulation of quadratic programs optimizing Bezier curves. By relying on the locomotion controller of our quadruped robot Solo, we finally implement the complete method, provided as an open-source package. Its efficiency is asserted by statistical evaluation of the importance of each component in simulation. We have a 100\% success rate for our framework, and we show that the deactivation of the contact planning, footstep adaptation and collision avoidance, respectively induced a drop to 70\%, 62\% and 83\% success rate in the worst case, justifying the complete architecture.},
  archive   = {C_IROS},
  author    = {Fanny Risbourg and Thomas Corbères and Pierre-Alexandre Léziart and Thomas Flayols and Nicolas Mansard and Steve Tonneau},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981539},
  pages     = {12950-12956},
  title     = {Real-time footstep planning and control of the solo quadruped robot in 3D environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to guide online multi-contact receding horizon
planning. <em>IROS</em>, 12942–12949. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In Receding Horizon Planning (RHP), it is critical that the motion being executed facilitates the completion of the task, e.g. building momentum to overcome large obstacles. This requires a value function to inform the desirability of robot states. However, given the complex dynamics, value functions are often approximated by expensive computation of trajectories in an extended planning horizon. In this work, to achieve online multi-contact Receding Horizon Planning (RHP), we propose to learn an oracle that can predict local objectives (intermediate goals) for a given task based on the current robot state and the environment. Then, we use these local objectives to construct local value functions to guide a short-horizon RHP. To obtain the oracle, we take a supervised learning approach, and we present an incremental training scheme that can improve the prediction accuracy by adding demonstrations on how to recover from failures. We compare our approach against the baseline (long-horizon RHP) for planning centroidal trajectories of humanoid walking on moderate slopes as well as large slopes where static stability cannot be achieved. We validate these trajectories by tracking them via a whole-body inverse dynamics controller in simulation. We show that our approach can achieve online RHP for 95\%-98.6\% cycles, outperforming the baseline (8\%-51.2\%).},
  archive   = {C_IROS},
  author    = {Jiayi Wang and Teguh Santoso Lembono and Sanghyun Kim and Sylvain Calinon and Sethu Vijayakumar and Steve Tonneau},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981234},
  pages     = {12942-12949},
  title     = {Learning to guide online multi-contact receding horizon planning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic tuning and selection of whole-body controllers.
<em>IROS</em>, 12935–12941. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Designing controllers for complex robots such as humanoids is not an easy task. Often, researchers hand-tune controllers, but this is a time-consuming approach that yields a single controller which cannot generalize well to varied tasks. This work presents a method which uses the NSGA-II multi-objective optimization algorithm with various training trajectories to output a diverse Pareto set of well-functioning controller weights and gains. The best of these are shown to also work well on the real Talos robot. The learned Pareto front is then used in a Bayesian optimization (BO) algorithm both as a search space and as a source of prior information in the initial mean estimate. This combined learning approach, leveraging the two optimization methods together, finds a suitable parameter set for a new trajectory within 20 trials and outperforms both BO in the continuous parameter search space and random search along the precomputed Pareto front. The few trials required for this formulation of BO suggest that it could feasibly be applied on the physical robot using a Pareto front generated in simulation.},
  archive   = {C_IROS},
  author    = {Evelyn D&#39;Elia and Jean-Baptiste Mouret and Jens Kober and Serena Ivaldi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981058},
  pages     = {12935-12941},
  title     = {Automatic tuning and selection of whole-body controllers},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scalable safety-critical policy evaluation with accelerated
rare event sampling. <em>IROS</em>, 12919–12926. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Evaluating rare but high-stakes events is one of the main challenges in obtaining reliable reinforcement learning policies, especially in large or infinite state/action spaces where limited scalability dictates a prohibitively large number of testing iterations. On the other hand, a biased or inaccurate policy evaluation in a safety-critical system could potentially cause unexpected catastrophic failures during deployment. This paper proposes the Accelerated Policy Evaluation (APE) method, which simultaneously uncovers rare events and estimates the rare event probability in Markov decision processes. The APE method treats the environment nature as an adversarial agent and learns towards, through adaptive importance sampling, the zero-variance sampling distribution for the policy evaluation. Moreover, APE is scalable to large discrete or continuous spaces by incorporating function approximators. We investigate the convergence property of APE in the tabular setting. Our empirical studies show that APE can estimate the rare event probability with a smaller bias while only using orders of magnitude fewer samples than baselines in multi-agent and single-agent environments.},
  archive   = {C_IROS},
  author    = {Mengdi Xu and Peide Huang and Fengpei Li and Jiacheng Zhu and Xuewei Qi and Kentaro Oguchi and Zhiyuan Huang and Henry Lam and Ding Zhao},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981867},
  pages     = {12919-12926},
  title     = {Scalable safety-critical policy evaluation with accelerated rare event sampling},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quantifying safety of learning-based self-driving control
using almost-barrier functions. <em>IROS</em>, 12903–12910. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Path-tracking control of self-driving vehicles can benefit from deep learning for tackling longstanding challenges such as nonlinearity and uncertainty. However, deep neural controllers lack safety guarantees, restricting their practical use. We propose a new approach of learning almost-barrier functions, which approximately characterizes the forward invariant set for the system under neural controllers, to quantitatively analyze the safety of deep neural controllers for path-tracking. We design sampling-based learning procedures for constructing candidate neural barrier functions, and certification procedures that utilize robustness analysis for neural networks to identify regions where the barrier conditions are fully satisfied. We use an adversarial training loop between learning and certification to optimize the almost-barrier functions. The learned barrier can also be used to construct online safety monitors through reachability analysis. We demonstrate effectiveness of our methods in quantifying safety of neural controllers in various simulation environments, ranging from simple kinematic models to the TORCS simulator with high-fidelity vehicle dynamics simulation.},
  archive   = {C_IROS},
  author    = {Zhizhen Qin and Tsui-Wei Weng and Sicun Gao},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982058},
  pages     = {12903-12910},
  title     = {Quantifying safety of learning-based self-driving control using almost-barrier functions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data-driven kalman filter with kernel-based koopman
operators for nonlinear robot systems. <em>IROS</em>, 12872–12878. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Designing the Kalman filter for nonlinear robot systems with theoretical guarantees is challenging, especially when the dynamics model is unavailable. This paper proposes a data-driven Kalman filter algorithm using kernel-based Koop-man operators for unknown nonlinear robot systems. First, the Koopman operator using sparse kernel-based extended dynamic decomposition (EDMD) is presented to learn the unknown dynamics with input-output datasets. Unlike classic EDMD, which requires manual selection of kernel functions, our approach automatically constructs kernel functions using an approximate linear dependency analysis method. The resulting Koopman model is a linear dynamic evolution in the kernel space, enabling us to address the nonlinear filtering problem using the standard linear Kalman filter design process. Despite this, our approach generates a nonlinear filtering law thanks to the adopted nonlinear kernel functions. Finally, the effectiveness of the proposed approach is validated by simulated experiments.},
  archive   = {C_IROS},
  author    = {Wei Jiang and Xing long Zhang and Zhen Zuo and Meiping Shi and Shaojing Su},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981408},
  pages     = {12872-12878},
  title     = {Data-driven kalman filter with kernel-based koopman operators for nonlinear robot systems},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CROON: Automatic multi-LiDAR calibration and refinement
method in road scene. <em>IROS</em>, 12857–12863. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sensor-based environmental perception is a crucial part of the autonomous driving system. In order to get an excellent perception of the surrounding environment, an intelligent system would configure multiple LiDARs (3D Light Detection and Ranging) to cover the distant and near space of the car. The precision of perception relies on the quality of sensor calibration. This research aims at developing an accurate, automatic, and robust calibration strategy for multiple LiDAR systems in the general road scene. We thus propose CROON (automatic multi-LiDAR Calibration and Refinement methOd in rOad sceNe), a two-stage method including rough and refinement calibration. The first stage can calibrate the sensor from an arbitrary initial pose, and the second stage is able to precisely calibrate the sensor iteratively. Specifically, CROON utilize the nature characteristics of road scene so that it is independent and easy to apply in large-scale conditions. Experimental results on real-world and simulated data sets demonstrate the reliability and accuracy of our method. All the related data sets and codes are open-sourced on the Github website https://github.com/OpenCalib/LiDAR2LiDAR.},
  archive   = {C_IROS},
  author    = {Pengjin Wei and Guohang Yan and Yikang Li and Kun Fang and Xinyu Cai and Jie Yang and Wei Liu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981558},
  pages     = {12857-12863},
  title     = {CROON: Automatic multi-LiDAR calibration and refinement method in road scene},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An optimal dynamic control method for robots with virtual
links. <em>IROS</em>, 12843–12848. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Virtual links and virtual joints can be appended to the kinematic chain of a robot arm to assist in modelling and control of certain tasks. Activities such as spray painting, sand blasting, or scanning with a laser or camera can be enhanced by modelling the fluid stream, light beam, or field of view using a virtual link. Virtual joints can be used to allow movement in semi-redundant degrees of freedom of the task space. This can can be exploited to optimize the control of the real robot. A prudent choice is to minimize the effort required by the manipulator to execute the task. This often requires the inversion of the inertia matrix. However, virtual links have no inertia so the inverse does not exist. This paper first explores methods of adding virtual mass or modifying the inertia matrix to allow inversion and the consequences. Then an optimal control problem is proposed that minimizes kinetic energy in the real manipulator and maximizes use of the virtual joints. In doing so, we only need the real inertia matrix which is always invertible. The method is validated in a case study for high pressure water blasting. It is shown to reduce the dynamic torque norm compared to a minimum velocity controller.},
  archive   = {C_IROS},
  author    = {Jon Woolfrey and Dikai Liu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982273},
  pages     = {12843-12848},
  title     = {An optimal dynamic control method for robots with virtual links},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An adaptive approach to whole-body balance control of
wheel-bipedal robot ollie. <em>IROS</em>, 12835–12842. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The wheel-bipedal robot has the advantages of both wheeled robots and legged robots, but as a cost, it is more challenging to perform flexible movements in various surroundings while keeping it balanced. The inaccurate dynamics of the robot makes the balance problem even more intractable. To solve this problem, the robot Ollie is used as a testbed. The whole-body control (WBC) framework is adopted to enhance the dexterity of the robot with multiple degrees of freedom in the task space. Moreover, a learning-based adaptive technique is applied to assist the WBC such that the balance controller can be designed in the absence of the accurate dynamics. Physical experiments demonstrate that the robot can manage various actions, with the help of the combination of the WBC and the learning-based adaptive technique.},
  archive   = {C_IROS},
  author    = {Jingfan Zhang and Shuai Wang and Haitao Wang and Jie Lai and Zhenshan Bing and Yu Jiang and Yu Zheng and Zhengyou Zhang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981985},
  pages     = {12835-12842},
  title     = {An adaptive approach to whole-body balance control of wheel-bipedal robot ollie},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Plane-to-plane positioning by proximity-based control.
<em>IROS</em>, 12795–12802. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we consider a multi-sensor arrangement of proximity sensors that forms a proximity array. A general modeling methodology is considered within the framework of Sensor-based Control. It incorporates multiple sensor signals from the proximity array by giving primary emphasis on the interaction screw. To prove its effectiveness, modeling approach is applied to the task of plane-to-plane positioning. We discuss the development of two sensor-based task functions for the specific task considered. The validity of the methodology is provided using relevant experimental results.},
  archive   = {C_IROS},
  author    = {John Thomas and François Pasteau and François Chaumette},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981781},
  pages     = {12795-12802},
  title     = {Plane-to-plane positioning by proximity-based control},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unbiased active inference for classical control.
<em>IROS</em>, 12787–12794. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Active inference is a mathematical framework that originated in computational neuroscience. Recently, it has been demonstrated as a promising approach for constructing goal-driven behavior in robotics. Specifically, the active inference controller (AIC) has been successful on several continuous control and state-estimation tasks. Despite its relative success, some established design choices lead to a number of practical limitations for robot control. These include having a biased estimate of the state, and only an implicit model of control actions. In this paper, we highlight these limitations and propose an extended version of the unbiased active inference controller (u-AIC). The u-AIC maintains all the compelling benefits of the AIC and removes its limitations. Simulation results on a 2-DOF arm and experiments on a real 7-DOF manipulator show the improved performance of the u-AIC with respect to the standard AIC. The code can be found at https://github.com/cpezzato/unbiasedaic.},
  archive   = {C_IROS},
  author    = {Mohamed Baioumy and Corrado Pezzato and Riccardo Ferrari and Nick Hawes},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981095},
  pages     = {12787-12794},
  title     = {Unbiased active inference for classical control},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simultaneous gesture classification and speed control for
myoelectric prosthetic hand using joint-loss neural network.
<em>IROS</em>, 12780–12786. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Gesture classification and motion speed regression are always two major issues in myoelectrical prosthetic hand research. However, there is little research considering these two issues in conjunction. Some shared EMG feature information in these two processing tasks is promising to improve the performance of prosthetic hand control. In this study, a joint-loss (JL) neural network architecture is proposed to implement gesture classification and motion speed regression problems in parallel by sharing the hidden neural units in the training process and optimizing the joint loss function. We evaluated the proposed control system through motion experiments performed on six participants. The experiment result shows that the classification and regression models can successfully reproduce smooth movement based on EMG measurement with high accuracy. Furthermore, the possibility of clinical application is demonstrated through the online movement of the real prosthetic hand.},
  archive   = {C_IROS},
  author    = {Hashimoto Naoki and Zhenzhi Ying and Nakashima Koki and Liming Shu and Naohiko Sugita},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981666},
  pages     = {12780-12786},
  title     = {Simultaneous gesture classification and speed control for myoelectric prosthetic hand using joint-loss neural network},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Application of piece-wise constant strain model to flexible
deformation calculation of sports prosthesis and stiffness estimation.
<em>IROS</em>, 12758–12763. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this study, we present an application of the Piece-wise Constant Strain (PCS) model to a flexible deformation analysis of a sports prosthesis leg. Dynamic motion analysis of an athlete wearing a sports prosthesis is important to clarify a relationship between the prosthesis characteristics and the performance of an athlete, which contributes to training of an athlete or design of the prosthesis. However, there are few studies on modeling of the three-dimensional deformation of the sports prosthesis. In soft robotics, the PCS model was proposed for calculating a flexible deformation of a beam or rod structure with a low computational cost. We employ the PCS model to calculate the flexible deformation of the prosthesis, assuming that its structure can be discretized into a finite number of segments. Moreover, we propose an estimation method of the prosthesis stiffness using optical motion capture data and calculating the semi-definite programming.},
  archive   = {C_IROS},
  author    = {Yuta Shimane and Taiki Ishigaki and Sunghee Kim and Yosuke Ikegami and Ko Yamamoto},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981207},
  pages     = {12758-12763},
  title     = {Application of piece-wise constant strain model to flexible deformation calculation of sports prosthesis and stiffness estimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Biomechanical design optimization of passive exoskeletons
through surrogate modeling on industrial activity data. <em>IROS</em>,
12752–12757. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Passive exoskeletons are unpowered wearable robotic devices aimed at providing biomechanical assistance. They can be applied in industries such as manufacturing, construction and logistics to reduce repetitive stress injuries among workers. Their design process typically considers a static user, with muscle outputs computed later during dynamic tasks to evaluate performance. Attempting to reduce human muscle effort at this stage requires manual redesign. Instead, we propose a parameter optimization approach that minimizes muscle effort rates during realistic dynamic tasks in the design stage itself. We extract human kinematics in assembly tasks from an industry-oriented motion capture dataset, and compute the induced joint torques. Using a passive exoskeleton for shoulder joint gravity compensation from the literature as a baseline, we optimize its design parameters through a multi-objective Pareto Local Search, minimizing the muscle effort rates during these tasks. As the estimation of muscle outputs through biomechanical simulation techniques is computation-ally expensive, we train ensemble regression models for each muscle of interest during the task motions. These models serve as surrogates for the objective function in the design optimization procedure, speeding up search in the parameter space. The resulting exoskeleton with optimized design param-eters reduces estimated muscle effort rates by an average of 5.73\% and peak of 35.1\% compared to default parameters, and an average of 14.5\% and peak of 32.2\% compared to not wearing an exoskeleton in overhead assembly tasks. A larger peak reduction compared to default parameters may be due to hindrance in motion caused by device. This approach may be adapted to other exoskeletons and applications, improving biomechanical assistance by design.},
  archive   = {C_IROS},
  author    = {Vighnesh Vatsal and Balamuralidhar Purushothaman},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981403},
  pages     = {12752-12757},
  title     = {Biomechanical design optimization of passive exoskeletons through surrogate modeling on industrial activity data},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Continuous locomotion mode recognition and gait phase
estimation based on a shank-mounted IMU with artificial neural networks.
<em>IROS</em>, 12744–12751. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To improve the control of wearable robotics for gait assistance, we present an approach for continuous locomotion mode recognition as well as gait phase and stair slope estimation based on artificial neural networks that include time history information. The input features consist exclusively of processed variables that can be measured with a single shank-mounted inertial measurement unit. We introduce a wearable device to acquire real-world environment test data to demonstrate the performance and the robustness of the approach. Mean absolute error (gait phase, stair slope) and accuracy (locomotion mode) were determined for steady level walking and steady stair ambulation. Robustness was assessed using test data from different sensor hardware, sensor fixations, ambulation environments and subjects. The mean absolute error from the steady gait test data for the gait phase was 2.0-3.5\% for gait phase estimation and 3.3-3.8° for stair slope estimation. The accuracy of classifying the correct locomotion mode on the test data with the utilization of time history information was in between 98.51\% and 99.67\%. Results show high performance and robustness for continuously predicting gait phase, stair slope and locomotion mode during steady gait. As hypothesized, time history information improves the locomotion mode recognition. However, while the gait phase estimation performed well for untrained transitions between locomotion modes, our qualitative analysis revealed that it may be beneficial to include transition data into the training of the neural network to improve the prediction of the slope and the locomotion mode. Our results suggest that artificial neural networks could be used for high level control of wearable lower limb robotics.},
  archive   = {C_IROS},
  author    = {Florian Weigand and Andreas Höhl and Julian Zeiss and Ulrich Konigorski and Martin Grimmer},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981154},
  pages     = {12744-12751},
  title     = {Continuous locomotion mode recognition and gait phase estimation based on a shank-mounted IMU with artificial neural networks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Coordinated multi-agent exploration, rendezvous, &amp; task
allocation in unknown environments with limited connectivity.
<em>IROS</em>, 12706–12712. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The lack of communication between agents in a multi-robot system is often regarded as a limiting factor that can affect and delay cooperative exploration and exploitation of cluttered and uncertain environments. On the contrary, this paper proposes a complete planning framework to enable cooperative behavior without the need for constant communication between robots, demonstrating drastic improvements in task completion and coverage time as compared to both fully connected robotic networks and widely used frontier-based exploration methods. Specifically, the proposed scheme considers three behaviors: i) exploration, promoting separation and disconnection, ii) rendezvous to reconnect and share information gained during exploration, and iii) task allocation for prioritized objectives. Exploration is achieved via a Sobel edge detection frontier algorithm that enables navigation of unknown complex (both convex and non-convex) environments. Once a task is discovered, a multi-objective weighted sum optimization method is proposed for allocating tasks based on prioritization and expectation estimation. The utility, generality, and scalability of the proposed approach is demonstrated using extensive simulations and experiments with unmanned ground vehicles in various cluttered environments.},
  archive   = {C_IROS},
  author    = {Lauren Bramblett and Rahul Peddi and Nicola Bezzo},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981898},
  pages     = {12706-12712},
  title     = {Coordinated multi-agent exploration, rendezvous, &amp; task allocation in unknown environments with limited connectivity},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modular robot networking: A novel schema and its performance
assessment. <em>IROS</em>, 12698–12705. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modular robots (MRs) consist of unique robots which interconnect and work as a collective to perform objectives. Coordinating these robots rely on robust communication, as modules moving independently can lead to damaging behaviour. We present a robust structure for modular robot communication, implemented and tested on a new MR. The structure has different communication protocols depending on the importance and bandwidth of the exchanged information, has fast error responses, and considerations which allow for two modules to actuate the same joint. We evaluate the wireless protocols, novel error response, and coordinated actuation empirically, validating the system on a new modular robot, the Mori3. We find two wireless protocols can be used to balance speed and reliability; transmitting errors through both wireless and serial is more consistent and faster; and sharing motor targets, control variables, and measurements allow for motors to operate a shared joint with equal efforts.},
  archive   = {C_IROS},
  author    = {Kevin Holdcroft and Anastasia Bolotnikova and Christoph Belke and Jamie Paik},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981688},
  pages     = {12698-12705},
  title     = {Modular robot networking: A novel schema and its performance assessment},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design, modeling and control of a composable and extensible
drone with tilting rotors. <em>IROS</em>, 12682–12689. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce a composable and extensible drone with tilting rotors (CEDTR). We aimed for a function that could optimally match the load capacity, degree of freedom (DOF), speed and endurance with diverse mission requirements by changing the quantity and form of combinations. First, we propose a decentralized modular controller to allow a team of physically connected modules to fly cooperatively. Second, we divide all the combinations into three categories according to the different control methods. Three generalized control strategies are proposed to control both position and attitude independently by tilting the directions of the propellers. We carried out experiments to demonstrate the feasibility of this mechanical design and control method. The experiment video is available at https://youtu.be/7RvxiV4FPq4.},
  archive   = {C_IROS},
  author    = {Zegui Wu and Ruqing Zhao and Mengfan Yu and Yanchun Zhao and Wanqi Yang and Weiye Zhang and Fusheng Li},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982090},
  pages     = {12682-12689},
  title     = {Design, modeling and control of a composable and extensible drone with tilting rotors},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-modal multi-agent optimization for LIMMS, a modular
robotics approach to delivery automation. <em>IROS</em>, 12674–12681.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we present a motion planner for LIMMS, a modular multi-agent, multi-modal package delivery platform. A single LIMMS unit is a robot that can operate as an arm or leg depending on how and what it is attached to, e.g., a manipulator when it is anchored to walls within a delivery vehicle or a quadruped robot when 4 are attached to a box. Coordinating amongst multiple LIMMS, when each one can take on vastly different roles, can quickly become complex. For such a planning problem we first compose the necessary logic and constraints. The formulation is then solved for skill exploration and can be implemented on hardware after refinement. To solve this optimization problem we use alternating direction method of multipliers (ADMM). The proposed planner is experimented under various scenarios which shows the capability of LIMMS to enter into different modes or combinations of them to achieve their goal of moving shipping boxes.},
  archive   = {C_IROS},
  author    = {Xuan Lin and Gabriel I. Fernandez and Yeting Liu and Taoyuanmin Zhu and Yuki Shirai and Dennis Hong},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981582},
  pages     = {12674-12681},
  title     = {Multi-modal multi-agent optimization for LIMMS, a modular robotics approach to delivery automation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Physical neural cellular automata for 2D shape
classification. <em>IROS</em>, 12667–12673. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Materials with the ability to self-classify their own shape have the potential to advance a wide range of engineering applications and industries. Biological systems possess the ability not only to self-reconfigure but also to self-classify themselves to determine a general shape and function. Previous work into modular robotics systems has only enabled self-recognition and self-reconfiguration into a specific target shape, missing the inherent robustness present in nature to self-classify. In this paper we therefore take advantage of recent advances in deep learning and neural cellular automata, and present a simple modular 2D robotic system that can infer its own class of shape through the local communication of its components. Furthermore, we show that our system can be successfully transferred to hardware which thus opens op-portunities for future self-classifying machines. Code available at https://github.com/kattwalker/projectcube. Video available at https://youtu.be/0TCOkE4keyc.},
  archive   = {C_IROS},
  author    = {Kathryn Walker and Rasmus Berg Palm and Rodrigo Moreno and Andres Faina and Kasper Stoy and Sebastian Risi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981214},
  pages     = {12667-12673},
  title     = {Physical neural cellular automata for 2D shape classification},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Selective self-assembly using re-programmable magnetic
pixels. <em>IROS</em>, 12659–12666. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a method to generate highly selective encodings that can be magnetically “programmed” onto physical modules to enable them to self-assemble in chosen configurations. We generate these encodings based on Hadamard matrices, and show how to design the faces of modules to be maximally attractive to their intended mate, while remaining maximally agnostic to other faces. We derive guarantees on these bounds, and verify their attraction and agnosticism experimentally. Using cubic modules whose faces have been covered in soft magnetic material, we show how inexpensive, passive modules with planar faces can be used to selectively self-assemble into target shapes without geometric guides. We show that these modules can be easily re-programmed for new target shapes using a CNC-based magnetic plotter, and demonstrate self-assembly of 8 cubes in a water tank.},
  archive   = {C_IROS},
  author    = {Martin Nisser and Yashaswini Makaram and Faraz Faruqi and Ryo Suzuki and Stefanie Mueller},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981879},
  pages     = {12659-12666},
  title     = {Selective self-assembly using re-programmable magnetic pixels},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RePoSt: Distributed self-reconfiguration algorithm for
modular robots based on porous structure. <em>IROS</em>, 12651–12658.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a new self-reconfiguration scheme for modular robots based on a metamodule design that allows to form a 3D porous structure. The porous structure enables a parallel flow of modules inside it without blocking. The metamodule can also be used to fill its internal volume with an additional number of modules allowing the structure to be compressible and expandable. Hence, it is a potential for improving the self-reconfiguration process. We first present the metamodule model and the porous structure built using it. Then, we describe an algorithm to self-reconfigure the structure from an initial shape to a given goal shape. We evaluated the algorithm in simulation on structures composed of up to 2,700 modules. We studied the performance in term of parallelism, showed that the number of communications is proportional to the number of motions and the execution time varies linearly with the diameter of the configuration.},
  archive   = {C_IROS},
  author    = {Jad Bassil and Benoît Piranda and Abdallah Makhoul and Julien Bourgeois},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981212},
  pages     = {12651-12658},
  title     = {RePoSt: Distributed self-reconfiguration algorithm for modular robots based on porous structure},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Prismatic soft actuator augments the workspace of soft
continuum robots. <em>IROS</em>, 12630–12636. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft robots are promising for manipulation tasks thanks to their compliance, safety, and high degree of freedom. However, the commonly used bidirectional continuum segment design means soft robotic manipulators only function in a limited hemispherical workspace. This work increases a soft robotic arm&#39;s workspace by designing, fabricating, and controlling an additional soft prismatic actuator at the base of the soft arm. This actuator consists of pneumatic artificial muscles and a piston, making the actuator back-driveable. We increase the task space volume by 116\%, and we are now able to perform manipulation tasks that were previously impossible for soft robots, such as picking and placing objects at different positions on a surface and grabbing an object out of a container. By combining a soft robotic arm with a prismatic joint, we greatly increase the usability of soft robots for object manipulation. This work promotes the use of integrated and modular soft robotic systems for practical manipulation applications in human-centered environments.},
  archive   = {C_IROS},
  author    = {Philipp Wand and Oliver Fischer and Robert K. Katzschmann},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981473},
  pages     = {12630-12636},
  title     = {Prismatic soft actuator augments the workspace of soft continuum robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Collision-aware fast simulation for soft robots by
optimization-based geometric computing. <em>IROS</em>, 12614–12621. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft robots can safely interact with environments because of their mechanical compliance. Self-collision is also employed in the modern design of soft robots to enhance their performance during different tasks. However, developing an efficient and reliable simulator that can handle the collision response well, is still a challenging task in the research of soft robotics. This paper presents a collision-aware simulator based on geometric optimization, in which we develop a highly efficient and realistic collision checking / response model incorporating a hyperelastic material property. Both actuated deformation and collision response for soft robots are formulated as geometry-based objectives. The collision-free body of a soft robot can be obtained by minimizing the geometry-based objective function. Unlike the FEA-based physical simulation, the proposed pipeline performs a much lower computational cost. Moreover, adaptive remeshing is applied to achieve the improvement of the convergence when dealing with soft robots that have large volume variations. Experimental tests are conducted on different soft robots to verify the performance of our approach.},
  archive   = {C_IROS},
  author    = {Guoxin Fang and Yingjun Tian and Andrew Weightman and Charlie C.L. Wang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981870},
  pages     = {12614-12621},
  title     = {Collision-aware fast simulation for soft robots by optimization-based geometric computing},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sim2Real for soft robotic fish via differentiable
simulation. <em>IROS</em>, 12598–12605. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate simulation of soft mechanisms under dynamic actuation is critical for the design of soft robots. We address this gap with our differentiable simulation tool by learning the material parameters of our soft robotic fish. On the example of a soft robotic fish, we demonstrate an experimentally-verified, fast optimization pipeline for learning the material parameters from quasi-static data via differentiable simulation and apply it to the prediction of dynamic performance. Our method identifies physically plausible Young&#39;s moduli for various soft silicone elastomers and stiff acetal copolymers used in creation of our three different robotic fish tail designs. We show that our method is compatible with varying internal geometry of the actuators, such as the number of hollow cavities. Our framework allows high fidelity prediction of dynamic behavior for composite bi-morph bending structures in real hardware to millimeter-accuracy and within 3\% error normalized to actuator length. We provide a differentiable and robust estimate of the thrust force using a neural network thrust predictor; this estimate allows for accurate modeling of our experimental setup measuring bollard pull. This work presents a prototypical hardware and simulation problem solved using our differentiable framework; the framework can be applied to higher dimensional parameter inference, learning control policies, and computational design due to its differentiable character.},
  archive   = {C_IROS},
  author    = {John Z. Zhang and Yu Zhang and Pingchuan Ma and Elvis Nava and Tao Du and Philip Arm and Wojciech Matusik and Robert K. Katzschmann},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981338},
  pages     = {12598-12605},
  title     = {Sim2Real for soft robotic fish via differentiable simulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep residual reinforcement learning based autonomous blimp
control. <em>IROS</em>, 12566–12573. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Blimps are well suited to perform long-duration aerial tasks as they are energy efficient, relatively silent and safe. To address the blimp navigation and control task, in previous work we developed a hardware and software-in-the-loop framework and a PID-based controller for large blimps in the presence of wind disturbance. However, blimps have a deformable structure and their dynamics are inherently non-linear and time-delayed, making PID controllers difficult to tune. Thus, often resulting in large tracking errors. Moreover, the buoyancy of a blimp is constantly changing due to variations in ambient temperature and pressure. To address these issues, in this paper we present a learning-based framework based on deep residual reinforcement learning (DRRL), for the blimp control task. Within this framework, we first employ a PID controller to provide baseline performance. Subsequently, the DRRL agent learns to modify the PID decisions by interaction with the environment. We demonstrate in simulation that DRRL agent consistently improves the PID performance. Through rigorous simulation experiments, we show that the agent is robust to changes in wind speed and buoyancy. In real-world experiments, we demonstrate that the agent, trained only in simulation, is sufficiently robust to control an actual blimp in windy conditions. We openly provide the source code of our approach at https://github.com/robot-perception-group/AutonomousBlimpDRL. Video demonstration is provided at https://youtu.be/EMC4KnlH0yI.},
  archive   = {C_IROS},
  author    = {Yu Tang Liu and Eric Price and Michael J. Black and Aamir Ahmad},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981182},
  pages     = {12566-12573},
  title     = {Deep residual reinforcement learning based autonomous blimp control},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning object-based state estimators for household robots.
<em>IROS</em>, 12558–12565. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A robot operating in a household makes observations of multiple objects as it moves around over the course of days or weeks. The objects may be moved by inhabitants, but not completely at random. The robot may be called upon later to retrieve objects and will need a long-term object-based memory in order to know how to find them. Existing work in semantic SLAM does not attempt to capture the dynamics of object movement. In this paper, we combine some aspects of classic techniques for data-association filtering with modern attention-based neural networks to construct object-based memory systems that operate on high-dimensional observations and hypotheses. We perform end-to-end learning on labeled observation trajectories to learn both the transition and observation models. We demonstrate the system&#39;s effectiveness in maintaining memory of dynamically changing objects in both simulated environment and real images, and demonstrate improvements over classical structured approaches as well as unstructured neural approaches. Additional information available at project website: https://yilundu.github.io/obm/.},
  archive   = {C_IROS},
  author    = {Yilun Du and Tomas Lozano-Perez and Leslie Pack Kaelbling},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981287},
  pages     = {12558-12565},
  title     = {Learning object-based state estimators for household robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Comparing reconstruction- and contrastive-based models for
visual task planning. <em>IROS</em>, 12550–12557. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning state representations enables robotic planning directly from raw observations such as images. Several methods learn state representations by utilizing losses based on the reconstruction of the raw observations from a lower-dimensional latent space. The similarity between observations in the space of images is often assumed and used as a proxy for estimating similarity between the underlying states of the system. However, observations commonly contain task-irrelevant factors of variation which are nonetheless important for reconstruction, such as varying lighting and different camera viewpoints. In this work, we define relevant evaluation metrics and perform a thorough study of different loss functions for state representation learning. We show that models exploiting task priors, such as Siamese networks with a simple contrastive loss, outperform reconstruction-based representations in visual task planning in case of task-irrelevant factors of variations.},
  archive   = {C_IROS},
  author    = {Constantinos Chamzas and Martina Lippi and Michael C. Welle and Anastasia Varava and Lydia E. Kavraki and Danica Kragic},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981533},
  pages     = {12550-12557},
  title     = {Comparing reconstruction- and contrastive-based models for visual task planning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual-tactile sensing for real-time liquid volume
estimation in grasping. <em>IROS</em>, 12542–12549. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a deep visuo-tactile model for real-time estimation of the liquid inside a deformable container in a proprioceptive way. We fuse two sensory modalities, i.e., the raw visual inputs from the RGB camera and the tactile cues from our specific tactile sensor without any extra sensor calibrations. The robotic system is well controlled and adjusted based on the estimation model in real time. The main contributions and novelties of our work are listed as follows: 1) Explore a proprioceptive way for liquid volume estimation by developing an end-to-end predictive model with multi-modal convolutional networks, which achieve a high precision with an error of ~ 2 ml in the experimental validation. 2) Propose a multi-task learning architecture which comprehensively considers the losses from both classification and regression tasks, and comparatively evaluate the performance of each variant on the collected data and actual robotic platform. 3) Utilize the proprioceptive robotic system to accurately serve and control the requested volume of liquid, which is continuously flowing into a deformable container in real time. 4) Adaptively adjust the grasping plan to achieve more stable grasping and manipulation according to the real-time liquid volume prediction.},
  archive   = {C_IROS},
  author    = {Fan Zhu and Ruixing Jia and Lei Yang and Youcan Yan and Zheng Wang and Jia Pan and Wenping Wang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981153},
  pages     = {12542-12549},
  title     = {Visual-tactile sensing for real-time liquid volume estimation in grasping},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TAE: A semi-supervised controllable behavior-aware
trajectory generator and predictor. <em>IROS</em>, 12534–12541. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trajectory generation and prediction are two in-terwoven tasks that play important roles in planner evaluation and decision making for intelligent vehicles. Most existing methods focus on one of the two and are optimized to directly output the final generated/predicted trajectories, which only contain limited information for critical scenario augmentation and safe planning. In this work, we propose a novel behavior-aware Trajectory Autoencoder (TAE) that explicitly models drivers&#39; behavior such as aggressiveness and intention in the latent space, using semi-supervised adversarial autoencoder and domain knowledge in transportation. Our model addresses trajectory generation and prediction in a unified architecture and benefits both tasks: the model can generate diverse, controllable and realistic trajectories to enhance planner op-timization in safety-critical and long-tailed scenarios, and it can provide prediction of critical behavior in addition to the final trajectories for decision making. Experimental results demonstrate that our method achieves promising performance on both trajectory generation and prediction.},
  archive   = {C_IROS},
  author    = {Ruochen Jiao and Xiangguo Liu and Bowen Zheng and Dave Liang and Qi Zhu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981029},
  pages     = {12534-12541},
  title     = {TAE: A semi-supervised controllable behavior-aware trajectory generator and predictor},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards defensive autonomous driving: Collecting and probing
driving demonstrations of mixed qualities. <em>IROS</em>, 12528–12533.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Designing or learning an autonomous driving policy is undoubtedly a challenging task as the policy has to maintain its safety in all corner cases. In order to secure safety in autonomous driving, the ability to detect hazardous situations, which can be seen as an out-of-distribution (OOD) detection problem, becomes crucial. However, conventional datasets often only contain expert driving demonstrations, although some non-expert or uncommon driving behavior data are needed to implement a safety guaranteed autonomous driving platform. To this end, we present a dataset called the R3 Driving Dataset, composed of driving data with different qualities. The dataset categorizes abnormal driving behaviors into eight categories and 369 different detailed situations. The situations include dangerous lane changes and near-collision situations. To further enlighten how these abnormal driving behaviors can be detected, we utilize different uncertainty estimation and anomaly detection methods for the proposed dataset. From the results of the proposed experiment, it can be inferred that by using both uncertainty estimation and anomaly detection, most of the abnormal cases in the proposed dataset can be discriminated. https://rllab-snu.github.io/projects/R3-Driving-Dataset/doc.html},
  archive   = {C_IROS},
  author    = {Jeongwoo Oh and Gunmin Lee and Jeongeun Park and Wooseok Oh and Jaeseok Heo and Hojun Chung and Do Hyung Kim and Byungkyu Park and Chang-Gun Lee and Sungjoon Choi and Songhwai Oh},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981110},
  pages     = {12528-12533},
  title     = {Towards defensive autonomous driving: Collecting and probing driving demonstrations of mixed qualities},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Torque control of hydraulic pressure servo valve driven
actuator with deep neural network. <em>IROS</em>, 12512–12519. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Non-linear dynamics, model uncertainties due to hydraulic fluid, and disturbances in hydraulic systems make it difficult to obtain accurate torque tracking performance. In this study, a learning-based torque-tracking method is proposed, which does not require approximating the torque dynamics. The proposed method can capture disturbances and model uncertainties of system. The applied neural network comprises a nonlinear autoregressive model with exogenous inputs (NARX) and a long shortterm neural network (LSTM). NARX is employed due to its ability to predict time series control input from the states of system, and LSTM is used to overcome the vanishing and exploding gradient, which causes long-term memory loss in NARX, leading to inaccurate torque tracking performance. LSTM with NARX achieved a better prediction performance with a mean square error and standard deviation of $0.0015 \pm 0.4\times 103$ compared to only NARX with a mean square error of $0.004 \pm 1.0\times 103$ at 10 K training data size.},
  archive   = {C_IROS},
  author    = {Kamgang Blaise Tcheumchoua and Seokho Nam and Wan Kyun Chung},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981211},
  pages     = {12512-12519},
  title     = {Torque control of hydraulic pressure servo valve driven actuator with deep neural network},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Don’t share my face: Privacy preserving inpainting for
visual localization. <em>IROS</em>, 12506–12511. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual localization is an important task for many robotic and augmented reality applications. As localizing within large scale maps can be memory and computationally de-manding, cloud-based localization services are appealing for developers. However, such services raise important privacy concerns for both passive and active users. In particular, some sensitive information might be revealed by an attacker who intercepts data during the data-sharing process. Therefore, the sensitive data in the image should be concealed before it is shared. As a motivating case, we demonstrated the exposure generated by the common feature descriptor SIFT when attempting to recover private content. In this paper, we propose a pipeline to effectively conceal privacy-sensitive image regions from possible attacks to the transmission or localization services, by making use of learning-based image inpainting techniques while preserving, and even boosting, the localization performance. We tested our pipeline with two off-the-shelf localization services based on deep neural networks on the publicly available Oxford Robotcar dataset, showing that the localization performance on our generated private concealed images is on par with the non-private baseline. 1},
  archive   = {C_IROS},
  author    = {Saad Himmi and Oguzhan Ilter and François Pailleau and Roland Siegwart and Berta Bescos and Cesar Cadena},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982235},
  pages     = {12506-12511},
  title     = {Don&#39;t share my face: Privacy preserving inpainting for visual localization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adversarial attacks on monocular pose estimation.
<em>IROS</em>, 12500–12505. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Advances in deep learning have resulted in steady progress in computer vision with improved accuracy on tasks such as object detection and semantic segmentation. Nevertheless, deep neural networks are vulnerable to adversarial attacks, thus presenting a challenge in reliable deployment. Two of the prominent tasks in 3D scene-understanding for robotics and advanced driver assistance systems are monocular depth and pose estimation, often learned together in an unsupervised manner. While studies evaluating the impact of adversarial attacks on monocular depth estimation exist, a systematic demonstration and analysis of adversarial perturbations against pose estimation are lacking. We show how additive imperceptible perturbations can not only change predictions to increase the trajectory drift but also catastrophically alter its geometry. We also study the relation between adversarial perturbations targeting monocular depth and pose estimation networks, as well as the transferability of perturbations to other networks with different architectures and losses. Our experiments show how the generated perturbations lead to notable errors in relative rotation and translation predictions and elucidate vulnerabilities of the networks. 1 1 Code can be found at https://github.com/NeurAI-Lab/mono-pose-attack.},
  archive   = {C_IROS},
  author    = {Hemang Chawla and Arnav Varma and Elahe Arani and Bahram Zonooz},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982154},
  pages     = {12500-12505},
  title     = {Adversarial attacks on monocular pose estimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robot motion planning as video prediction: A spatio-temporal
neural network-based motion planner. <em>IROS</em>, 12492–12499. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Neural network (NN)-based methods have emerged as an attractive approach for robot motion planning due to strong learning capabilities of NN models and their inherently high parallelism. Despite the current development in this direction, the efficient capture and processing of important sequential and spatial information, in a direct and simultaneous way, is still relatively under-explored. To overcome the challenge and unlock the potentials of neural networks for motion planning tasks, in this paper, we propose STP-Net, an end-to-end learning framework that can fully extract and leverage important spatio-temporal information to form an efficient neural motion planner. By interpreting the movement of the robot as a video clip, robot motion planning is transformed to a video prediction task that can be performed by STP-Net in both spatially and temporally efficient ways. Empirical evaluations across different seen and unseen environments show that, with nearly 100\% accuracy (aka, success rate), STP-Net demonstrates very promising performance with respect to both planning speed and path cost. Compared with existing NN-based motion planners, STP-Net achieves at least 5×, 2.6× and 1.8× faster speed with lower path cost on 2D Random Forest, 2D Maze and 3D Random Forest environments, respectively. Furthermore, STP-Net can quickly and simultaneously compute multiple near-optimal paths in multi-robot motion planning tasks.},
  archive   = {C_IROS},
  author    = {Xiao Zang and Miao Yin and Lingyi Huang and Jingjin Yu and Saman Zonouz and Bo Yuan},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981769},
  pages     = {12492-12499},
  title     = {Robot motion planning as video prediction: A spatio-temporal neural network-based motion planner},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FloorGenT: Generative vector graphic model of floor plans
for robotics. <em>IROS</em>, 12485–12491. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Floor plans are the basis of reasoning in and communicating about indoor environments. In this paper, we show that by modelling floor plans as sequences of line segments seen from a particular point of view, recent advances in autoregressive sequence modelling can be leveraged to model and predict floor plans. The line segments are canonicalized and translated to sequence of tokens and an attention-based neural network is used to fit a one-step distribution over next tokens. We fit the network to sequences derived from a set of large-scale floor plans, and demonstrate the capabilities of the model in four scenarios: novel floor plan generation, completion of partially observed floor plans, generation of floor plans from simulated sensor data, and finally, the applicability of a floor plan model in predicting the shortest distance with partial knowledge of the environment.},
  archive   = {C_IROS},
  author    = {Ludvig Ericson and Patric Jensfelt},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982144},
  pages     = {12485-12491},
  title     = {FloorGenT: Generative vector graphic model of floor plans for robotics},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). OpenDR: An open toolkit for enabling high performance, low
footprint deep learning for robotics. <em>IROS</em>, 12479–12484. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing Deep Learning (DL) frameworks typically do not provide ready-to-use solutions for robotics, where very specific learning, reasoning, and embodiment problems exist. Their relatively steep learning curve and the different methodologies employed by DL compared to traditional approaches, along with the high complexity of DL models, which often leads to the need of employing specialized hardware accelerators, further increase the effort and cost needed to employ DL models in robotics. Also, most of the existing DL methods follow a static inference paradigm, as inherited by the traditional computer vision pipelines, ignoring active perception, which can be employed to actively interact with the environment in order to increase perception accuracy. In this paper, we present the Open Deep Learning Toolkit for Robotics (OpenDR). OpenDR aims at developing an open, non-proprietary, efficient, and modular toolkit that can be easily used by robotics companies and research institutions to efficiently develop and deploy AI and cognition technologies to robotics applications, providing a solid step towards addressing the aforementioned challenges. We also detail the design choices, along with an abstract interface that was created to overcome these challenges. This interface can describe various robotic tasks, spanning beyond traditional DL cognition and inference, as known by existing frameworks, incorporating openness, homogeneity and robotics-oriented perception e.g., through active perception, as its core design principles.},
  archive   = {C_IROS},
  author    = {N. Passalis and S. Pedrazzi and R. Babuska and W. Burgard and D. Dias and F. Ferro and M. Gabbouj and O. Green and A. Iosifidis and E. Kayacan and J. Kober and O. Michel and N. Nikolaidis and P. Nousi and R. Pieters and M. Tzelepi and A. Valada and A. Tefas},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981703},
  pages     = {12479-12484},
  title     = {OpenDR: An open toolkit for enabling high performance, low footprint deep learning for robotics},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural-guided runtime prediction of planners for improved
motion and task planning with graph neural networks. <em>IROS</em>,
12471–12478. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The past decade has amply demonstrated the remarkable functionality that can be realized by learning complex input/output relationships. Algorithmically, one of the most important and opaque relationships is that between a problem&#39;s structure and an effective solution method. Here, we quantitatively connect the structure of a planning problem to the performance of a given sampling-based motion planning (SBMP) algorithm. We demonstrate that the geometric relationships of motion planning problems can be well captured by graph neural networks (GNNs) to predict SBMP runtime. By using an algorithm portfolio we show that GNN predictions of runtime on particular problems can be leveraged to accelerate online motion planning in both navigation and manipulation tasks. Moreover, the problem-to-runtime map can be inverted to identify subproblems easier to solve by particular SBMPs. We provide a motivating example of how this knowledge may be used to improve integrated task and motion planning on simulated examples. These successes rely on the relational structure of GNNs to capture scalable generalization from low-dimensional navigation tasks to high degree-of-freedom manipulation tasks in 3d environments.},
  archive   = {C_IROS},
  author    = {Simon Odense and Kamal Gupta and William G. Macready},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981823},
  pages     = {12471-12478},
  title     = {Neural-guided runtime prediction of planners for improved motion and task planning with graph neural networks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Grounding commands for autonomous vehicles via layer fusion
with region-specific dynamic layer attention. <em>IROS</em>,
12464–12470. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Grounding a command to the visual environment is an essential ingredient for interactions between autonomous vehicles and humans. In this work, we study the problem of language grounding for autonomous vehicles, which aims to localize a region in a visual scene according to a natural language command from a passenger. Prior work only employs the top layer representations of a vision-and-language pretrained model to predict the region referred to by the command. However, such a method omits the useful features encoded in other layers, and thus results in inadequate understanding of the input scene and command. To tackle this limitation, we present the first layer fusion approach for this task. Since different visual regions may require distinct types of features to disambiguate them from each other, we further propose the region-specific dynamic (RSD) layer attention to adaptively fuse the multimodal information across layers for each region. Extensive experiments on the Talk2Car benchmark demonstrate that our approach helps predict more accurate regions and outperforms state-of-the-art methods.},
  archive   = {C_IROS},
  author    = {Hou Pong Chan and Mingxi Guo and Cheng-Zhong Xu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981515},
  pages     = {12464-12470},
  title     = {Grounding commands for autonomous vehicles via layer fusion with region-specific dynamic layer attention},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PUA-MOS: End-to-end point-wise uncertainty weighted
aggregation for moving object segmentation. <em>IROS</em>, 12456–12463.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Segmenting moving objects in the 3D LiDAR point cloud can provide important guidance to localization, mapping and decision-making for self-driving vehicles. As for the conventional approaches to point cloud segmentation, they rely on semantic-level information, which makes it inevitable for long-tail problems to arise as there are always unseen types of objects on the road. To achieve moving segmentation while avoiding the reliance on the object category, the point motion is identified in this paper by fully exploring and aggregating the point-level geometric consistency in sequential point clouds. More specifically, an end-to-end point-wise uncertainty weighted aggregation approach known as PUA-MOS is proposed to segment the moving points in 3D LiDAR Data. Our method is applicable to estimate point-wise moving mask, scene flow and rigid-body transformation simultaneously in a coarse- to-fine network, where the relations between each prediction are implicitly learned. To explicitly model the inner and inter relations across these predictions among all points, the point- wise estimation and the average value of the same motion points are aggregated according to a predicted uncertainty. Then, the aggregated estimation is fed again into the next-level fusion, where the points will be re-segmented using the aggregated mask from the last level. Through iterative joint aggregation, our PUA-MOS outperforms the previous methods significantly on both KITTI [4] and Waymo [26] datasets. The code will be provided to generate the moving segmentation labels on both datasets for reproduction.},
  archive   = {C_IROS},
  author    = {Cheng Chi and Peiliang Li and Xiaozhi Chen and Xin Yang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981500},
  pages     = {12456-12463},
  title     = {PUA-MOS: End-to-end point-wise uncertainty weighted aggregation for moving object segmentation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Safe adaptation in multiagent competition. <em>IROS</em>,
12441–12447. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Achieving the capability of adapting to ever-changing environments is a critical step towards building fully autonomous robots that operate safely in complicated scenarios. In multiagent competitive scenarios, agents may have to adapt to new opponents with previously unseen behaviors by learning from the interaction experiences between the ego-agent and the opponent. However, this adaptation is susceptible to opponent exploitation. As the ego-agent updates its own behavior to exploit the opponent, its own behavior could become more exploitable as a result of overfitting to this specific opponent&#39;s behavior. To overcome this difficulty, we developed a safe adaptation approach in which the ego-agent is trained against a regularized opponent model, which effectively avoids overfitting and consequently improves the robustness of the ego-agent&#39;s policy. We evaluated our approach in the Mujoco domain with two competing agents. The experiment results suggest that our approach effectively achieves both adaptation to the specific opponent that the ego-agent is interacting with and maintaining low exploitability to other possible opponent exploitation.},
  archive   = {C_IROS},
  author    = {Macheng Shen and Jonathan P. How},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981763},
  pages     = {12441-12447},
  title     = {Safe adaptation in multiagent competition},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-supervised traversability prediction by learning to
reconstruct safe terrain. <em>IROS</em>, 12419–12425. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Navigating off-road with a fast autonomous vehicle depends on a robust perception system that differentiates traversable from non-traversable terrain. Typically, this depends on a semantic understanding which is based on supervised learning from images annotated by a human expert. This requires a significant investment in human time, assumes correct expert classification, and small details can lead to misclassification. To address these challenges, we propose a method for predicting high- and low-risk terrains from only past vehicle experience in a self-supervised fashion. First, we develop a tool that projects the vehicle trajectory into the front camera image. Second, occlusions in the 3D representation of the terrain are filtered out. Third, an autoencoder trained on masked vehicle trajectory regions identifies low- and high-risk terrains based on the reconstruction error. We evaluated our approach with two models and different bottleneck sizes with two different training and testing sites with a four-wheeled off-road vehicle. Comparison with two independent test sets of semantic labels from similar terrain as training sites demonstrates the ability to separate the ground as low-risk and the vegetation as high-risk with 81.1\% and 85.1\% accuracy.},
  archive   = {C_IROS},
  author    = {Robin Schmid and Deegan Atha and Frederik Schöller and Sharmita Dey and Seyed Fakoorian and Kyohei Otsu and Barry Ridge and Marko Bjelonic and Lorenz Wellhausen and Marco Hutter and Ali-akbar Agha-mohammadi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981368},
  pages     = {12419-12425},
  title     = {Self-supervised traversability prediction by learning to reconstruct safe terrain},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Autonomous cycle time reduction of robotic tasks using
iterative learning control. <em>IROS</em>, 12405–12411. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When robots are used to automate repetitive production tasks, the productivity of the manufacturing system crucially depends on the robot&#39;s task execution speed. An out-of-the-box solution is typically slow, whereas achieving shorter cycle times typically requires large efforts with respect to controller design and tuning. This dilemma can be resolved by learning control algorithms that autonomously improve performance without requiring any system-specific tuning. In the present work, we propose a novel learning control scheme that autonomously reduces the execution times of robotic systems that perform repetitive manufacturing tasks. To this end, we combine an Iterative Learning Control (ILC) approach with a trial-varying reference adaptation. The reference trajectory is slowly adapted to ensure that the given task is performed successfully on every single iteration without constraint violations. Therefore, the learning process can be carried out during operation. We validate the practical applicability of the method by real-world experiments on a 6-axis robot that performs a linear motion and a contact-force task. Despite the fundamentally different characteristics of these two tasks, the proposed algorithm achieves a remarkable reduction of cycle times, namely, by a factor of 4 in the linear motion task and a factor of 10 in the contact-force task. These results provide an important step toward robotic manufacturing systems that autonomously optimize their own performance during operation.},
  archive   = {C_IROS},
  author    = {Lorenz Halt and Michael Meindl and Victor Bayer and Werner Kraus and Thomas Seel},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981042},
  pages     = {12405-12411},
  title     = {Autonomous cycle time reduction of robotic tasks using iterative learning control},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Autonomous quadrotor landing on inclined surfaces in high
particle environments using radar sensor perception. <em>IROS</em>,
12352–12358. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an autonomous approach for landing a quadrotor on inclined surfaces up to 40 degrees using radar perception in a high particle environment, such as dust, rain, or fog. This system uses five radar sensors to determine the direction, angle, and smoothness of a slope through eigenvalue decomposition of a point cloud covariance matrix. The point cloud itself is generated using a FIFO queue with the radar sensors after their points are transformed to a common frame. Then, two asymmetric landing skids of different lengths actively conform to a slope in order to maintain level body attitude upon landing. For perception error tolerance, a study to understand the distance between the propeller and slope surface with respect to slope angles was developed. We evaluate the accuracy and consistency of radar sensors in accomplishing these tasks, to include a comparison of the results with a depth camera while in a high particle environment. Finally, the experimental result shows that the detected slope angle and direction were within 2.2 and 2.4 degrees of ground, and the proposed system is viable and robust for use in real-world applications.},
  archive   = {C_IROS},
  author    = {Mark C. Lesak and Dylan Taylor and Jinho Kim and Christopher Korpela},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981929},
  pages     = {12352-12358},
  title     = {Autonomous quadrotor landing on inclined surfaces in high particle environments using radar sensor perception},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accurate vision-based flight with fixed-wing drones.
<em>IROS</em>, 12344–12351. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fixed-wing drones must navigate to the desired location accurately for maneuvers such as picking up objects and perching. However, current GNSS receivers limit their navigation accuracy to several meters in outdoor environments, making such maneuvers impossible. RTK GNSS can improve flight accuracy, but it requires ground stations at the target location and additional communication modules on the drone. Here, we describe a fixed-wing platform with onboard computation that uses positional information from a GNSS receiver and vision from an onboard camera. The drone relies on a GNSS signal for flying towards a point of interest and switches to vision-based information to accurately reach the target. We conducted outdoor experiments to compare the flight accuracy of three navigation methods: GNSS, RTK GNSS, and the proposed GNSS-vision method. We also systematically assessed the robustness of vision-based control to compensate for GNSS errors and quantify the accuracy of the proposed method. Our results show that the accuracy of the proposed GNSS-vision system is on par with RTK GNSS. GNSS-vision reduces the average error of GNSS by over an order of magnitude, from 3.033 m to 0.283 m, and reduces the variance across repeated flights from 2.095 m to 0.309 m. We open-source the software-hardware architecture used in this paper to enable the research community to build on these results and expand the capabilities of fixed-wing drones.},
  archive   = {C_IROS},
  author    = {Valentin Wüest and Enrico Ajanic and Matthias Müller and Dario Floreano},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981921},
  pages     = {12344-12351},
  title     = {Accurate vision-based flight with fixed-wing drones},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tightly-coupled EKF-based radar-inertial odometry.
<em>IROS</em>, 12336–12343. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multicopter Unmanned Aerial Vehicles (UAV) are small and agile robots with the potential to become prominent in performing autonomous tasks in various Global Navigation Satellite System (GNSS)-denied environments. These environments can potentially be rendered even more challenging due to external factors impairing the robot&#39;s perception, such as low or too bright light, permeation with aerosols or smoke. A precondition of autonomous operation, though, is the ability of a robot to accurately localize itself in the surrounding environment. Millimeter-wave Frequency Modulated Continuous Wave (FMCW) radar sensors are resilient to the aforementioned factors while being lightweight, inexpensive and highly accurate. In this paper, we present a Radar-Inertial Odometry (RIO) method for estimating the full 6DoF pose and 3D velocity of a UAV. In an Extended Kalman Filter (EKF) framework, we fuse range measurements and velocity measurements of 3D points detected by an FMCW radar sensor together with Inertial Measurement Unit (IMU) readings. In real experiments we show that our approach enables accurate state estimation of a UAV and that it exhibits improvements over similar existing state-of-the-art method.},
  archive   = {C_IROS},
  author    = {Jan Michalczyk and Roland Jung and Stephan Weiss},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981396},
  pages     = {12336-12343},
  title     = {Tightly-coupled EKF-based radar-inertial odometry},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Geometric MPC techniques for reduced attitude control on
quadrotors with bidirectional thrust. <em>IROS</em>, 12330–12335. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present two novel nonlinear MPC formulations for reduced attitude tracking on quadrotors with bidirectional thrust capabilities. Reduced attitude tracking is relevant to recovery from partial thrust loss, which can occur due to the failure of one or more motors. The first formulation builds on a linearization of the quadrotor attitude dynamics on $S(2)$ to achieve simultaneous tracking of reduced attitude and total thrust targets. The second formulation, meanwhile, accomplishes the same goal using a linearization of the dynamics on the Lie algebra of $SO(3)$ and a proposed method for projecting Lie algebra errors onto reduced attitude errors. Both methods achieve global tracking on $S(2)$ without requiring the use of computationally expensive sequential quadratic program solvers. Through simulations, we show that the second approach generally tracks aggressive attitude references better, while the first controller offers more reliable regulation.},
  archive   = {C_IROS},
  author    = {Jad Wehbeh and Inna Sharf},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982250},
  pages     = {12330-12335},
  title     = {Geometric MPC techniques for reduced attitude control on quadrotors with bidirectional thrust},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unilateral stiffness modulation with a robotic hip
exoskeleton elicits adaptation during gait. <em>IROS</em>, 12275–12281.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Wearable robotic exoskeletons show promise in their ability to provide gait assistance and rehabilitation in real-world contexts. However, a better understanding is needed of how exoskeletons contribute to neural adaptation in locomotion, a critical component of neurological gait rehabilitation. We tested whether unilateral perturbations elicit neural adaptation in healthy participants using a novel robotic hip exoskeleton, taking inspiration from asymmetry augmentation strategies used in split-belt treadmill training. We found that applying a virtual stiffness parallel to the hip joint on one side elicited changes in hip range of motion and step length, and that these changes were time varying, indicating an adaptation response. However, participants converged on asymmetric hip ranges of motion and step lengths both with and without applied stiffness from the exoskeleton. These results suggest that while adaptation appears to have occurred, it was not solely driven by the nervous system reducing gait asymmetry. Our findings indicate that applying mechanical impedance asymmetrically to the joints may be an effective gait training and rehabilitation approach, as well as a method to elicit a novel adaptation response to further study neuromotor control of locomotion.},
  archive   = {C_IROS},
  author    = {Mark Price and Banu Abdikadirova and Dominic Locurto and Jonaz Moreno Jaramillo and Nicholas Cline and Wouter Hoogkamer and Meghan E. Huber},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981067},
  pages     = {12275-12281},
  title     = {Unilateral stiffness modulation with a robotic hip exoskeleton elicits adaptation during gait},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual environment perception for obstacle detection and
crossing of lower-limb exoskeletons. <em>IROS</em>, 12267–12274. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Lower limb exoskeletons offer support for patients suffering from mobility disorders due to injury, stroke, etc. But these devices are not used in day-to-day life and environments due to their limited human-computer interface to perceive and handle different terrains and tasks. In this paper, we introduce a simple vision-based environment perception pipeline for lower- limb exoskeletons for obstacle crossing tasks. The proposed pipeline consists of three stages, namely, ground plane and obstacle detection, estimating obstacle location and dimensions, and obstacle tracking. To reduce noisy artifacts and reliably detect obstacles, we propose a similarity metric based on color, gradient orientation, and 2D surface normal. Depth map of the detected obstacle region is utilized for estimating the obstacle location and dimensions. Also, we consider two obstacle tracking modes for obstacle crossing, visual tracking using a RGB-D camera and positional tracking using a SLAM camera. The proposed vision-based perception pipeline is integrated with an exoskeleton, where we propose a control scheme that can vary step length adaptively to successfully cross detected obstacles. We conduct offline and online experiments to validate the proposed perception pipeline and provide insights on the same. Our experiments show that the proposed pipeline allows exoskeletons to understand their environment and successfully cross obstacles.},
  archive   = {C_IROS},
  author    = {Manoj Ramanathan and Lincong Luo and Jie Kai Er and Ming Jeat Foo and Chye Hsia Chiam and Lei Li and Wei Yun Yau and Wei Tech Ang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981412},
  pages     = {12267-12274},
  title     = {Visual environment perception for obstacle detection and crossing of lower-limb exoskeletons},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design of EMG-driven musculoskeletal model for volitional
control of a robotic ankle prosthesis. <em>IROS</em>, 12261–12266. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing robotic lower-limb prostheses use autonomous control to address cyclic, locomotive tasks, but are inadequate in adapting to variations in non-cyclic and unpredictable tasks. This study aims to address this challenge by designing a novel electromyography (EMG)-driven musculoskeletal model for volitional control of a robotic ankle-foot prosthesis. The proposed controller ensures continuous control of the device, allowing users to freely manipulate the prosthesis behavior. A Hill-type muscle model was implemented to model a dorsiflexor and a plantarflexor to function around a virtual ankle joint. The model parameters for a subject specific model was determined by fitting the model to the experimental data collected from an able-bodied subject. EMG signals recorded from antagonist muscle pairs were used to activate the virtual muscle models. This model-based approach was then validated via offline simulations and real-time prosthesis control. Additionally, the feasibility of the proposed prosthesis control on assisting the user&#39;s functional tasks was demonstrated. The present control may further improve the function of robotic prosthesis for supporting versatile activities in individuals with lower-limb amputations.},
  archive   = {C_IROS},
  author    = {Chinmay Shah and Aaron Fleming and Varun Nalam and Ming Liu and He Helen Huang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981305},
  pages     = {12261-12266},
  title     = {Design of EMG-driven musculoskeletal model for volitional control of a robotic ankle prosthesis},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Experimental assessment of a control strategy for locomotion
assistance relying on simplified motor primitives. <em>IROS</em>,
12254–12260. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Lower-limb exoskeletons are robotic devices that can provide assistance to human locomotion. Since they are expected to be used in ecological environments, their control strategy should handle different kinds of daily-life situations. Taking inspiration from the human neuromuscular system - and particularly from the socalled motor primitives - may help in adapting the type of delivered assistance to different locomotion tasks. In this work, we validated the combination of simplified primitives and a musculoskeletal model for assisting healthy subjects with a hip exoskeleton. This framework showed adaptation to the user&#39;s gait for different slope inclinations, although its effects on the subject&#39;s speed and their perceived effort showed no significant improvement compared to wearing the device in transparent mode.},
  archive   = {C_IROS},
  author    = {Henri Laloyaux and Clara Beatriz Sanz-Morère and Chiara Livolsi and Andrea Pergolini and Simona Crea and Nicola Vitiello and Renaud Ronsse},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982128},
  pages     = {12254-12260},
  title     = {Experimental assessment of a control strategy for locomotion assistance relying on simplified motor primitives},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). InterFusion: Interaction-based 4D radar and LiDAR fusion for
3D object detection. <em>IROS</em>, 12247–12253. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many recent works detect 3D objects by several sensor modalities for autonomous driving, where high-resolution cameras and high-line LiDARs are mostly used but relatively expensive. To achieve a balance between overall cost and detection accuracy, many multi-modal fusion techniques have been suggested. In recent years, the fusion of LiDAR and Radar has gained ever-increasing attention, especially 4D Radar, which can adapt to bad weather conditions due to its penetrability. Although features have been fused from multiple sensing modalities, most methods cannot learn interactions from different modalities, which does not make for their best use. Inspired by the self-attention mechanism, we present InterFusion, an interaction-based fusion framework, to fuse 16-line LiDAR with 4D Radar. It aggregates features from two modalities and identifies cross-modal relations between Radar and LiDAR features. In experimental evaluations on the Astyx HiRes 2019 dataset, our method outperformed the baseline by 4.20\% mAP in 3D and 10.76\% BEV mAP for the car class at the moderate level.},
  archive   = {C_IROS},
  author    = {Li Wang and Xinyu Zhang and Baowei Xv and Jinzhao Zhang and Rong Fu and Xiaoyu Wang and Lei Zhu and Haibing Ren and Pingping Lu and Jun Li and Huaping Liu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982123},
  pages     = {12247-12253},
  title     = {InterFusion: Interaction-based 4D radar and LiDAR fusion for 3D object detection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-modal fusion-based prior correction for road detection
in off-road environments. <em>IROS</em>, 12239–12246. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Road detection plays a fundamental role in the visual navigation system of autonomous vehicles. However, it&#39;s still challenging to achieve robust road detection in off-road scenarios due to their complicated road appearances and ambiguous road structures. Therefore, existing image-based road detection approaches usually fail to extract the right routes due to the lack of the effective fusion of the image and prior reference paths(road guidances generated via map annotations and GPS localization). Besides, the reference paths are not always reliable because of GPS localization errors and mapping errors. To achieve robust road detection in off-road scenarios, we propose a prior-correction-based road detection network named PR-ROAD via fusing the cross-model information provided by both the reference path and the input image. These two heterogeneous data, prior and image, are deeply fused by a cross-attention module and formulate contextual inter-dependencies. We conduct experiments in our collected rural, off-road and urban datasets. The experimental results demonstrate the effectiveness of the proposed method both on unstructured and structured roads.},
  archive   = {C_IROS},
  author    = {Yuru Wang and Yi Sun and Jian Li and Meiping Shi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981350},
  pages     = {12239-12246},
  title     = {Cross-modal fusion-based prior correction for road detection in off-road environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Vehicle type specific waypoint generation. <em>IROS</em>,
12225–12230. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We develop a generic mechanism for generating vehicle-type specific sequences of waypoints from a probabilistic foundation model of driving behavior. Many foundation behavior models are trained on data that does not include vehicle information, which limits their utility in downstream applications such as planning. Our novel methodology conditionally specializes such a behavior predictive model to a vehicle-type by utilizing byproducts of the reinforcement learning algorithms used to produce vehicle specific controllers. We show how to compose a vehicle specific value function estimate with a generic probabilistic behavior model to generate vehicle-type specific waypoint sequences that are more likely to be physically plausible then their vehicle-agnostic counterparts.},
  archive   = {C_IROS},
  author    = {Yunpeng Liu and Jonathan Wilder Lavington and Adam Scibior and Frank Wood},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981421},
  pages     = {12225-12230},
  title     = {Vehicle type specific waypoint generation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). State dropout-based curriculum reinforcement learning for
self-driving at unsignalized intersections. <em>IROS</em>, 12219–12224.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traversing intersections is a challenging problem for autonomous vehicles, especially when the intersections do not have traffic control. Recently deep reinforcement learning has received massive attention due to its success in dealing with autonomous driving tasks. In this work, we address the problem of traversing unsignalized intersections using a novel curriculum for deep reinforcement learning. The proposed curriculum leads to: 1) A faster training process for the reinforcement learning agent, and 2) Better performance compared to an agent trained without curriculum. Our main contribution is two-fold: 1) Presenting a unique curriculum for training deep reinforcement learning agents, and 2) demonstrating the performance improvement using the proposed curriculum in the unsignalized intersection traversal task. The framework expects processed observations of the surroundings from the perception system of the autonomous vehicle. We test our method in the CommonRoad motion planning simulator on T-intersections and four-way intersections.},
  archive   = {C_IROS},
  author    = {Shivesh Khaitan and John M. Dolan},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981109},
  pages     = {12219-12224},
  title     = {State dropout-based curriculum reinforcement learning for self-driving at unsignalized intersections},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). D-LC-nets: Robust denoising and loop closing networks for
LiDAR SLAM in complicated circumstances with noisy point clouds.
<em>IROS</em>, 12212–12218. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The current LiDAR SLAM (Simultaneous Localization and Mapping) system suffers greatly from low accuracy and limited robustness when faced with complicated circumstances. From our experiments, we find that current LiDAR SLAM systems have limited performance when the noise level in the obtained point clouds is large. Therefore, in this work, we propose a general framework to tackle the problem of denoising and loop closure for LiDAR SLAM in complex environments with many noises and outliers caused by reflective materials. Current approaches for point clouds denoising are mainly designed for small-scale point clouds and can not be extended to large-scale point clouds scenes. In this work, we firstly proposed a lightweight network for large-scale point clouds denoising. Subsequently, we have also designed an efficient loop closure network for place recognition in global optimization to improve the localization accuracy of the whole system. Finally, we have demonstrated by extensive experiments and benchmark studies that our method can have a significant boost on the localization accuracy of the LiDAR SLAM system when faced with noisy point clouds, with a marginal increase in computational cost.},
  archive   = {C_IROS},
  author    = {Kangcheng Liu and Aoran Xiao and Jiaxing Huang and Kaiwen Cui and Yun Xing and Shijian Lu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981388},
  pages     = {12212-12218},
  title     = {D-LC-nets: Robust denoising and loop closing networks for LiDAR SLAM in complicated circumstances with noisy point clouds},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Heterogeneous-agent trajectory forecasting incorporating
class uncertainty. <em>IROS</em>, 12196–12203. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reasoning about the future behavior of other agents is critical to safe robot navigation. The multiplicity of plausible futures is further amplified by the uncertainty inherent to agent state estimation from data, including positions, velocities, and semantic class. Forecasting methods, however, typically neglect class uncertainty, conditioning instead only on the agent&#39;s most likely class, even though perception models often return full class distributions. To exploit this information, we present HAICU, a method for heterogeneous-agent trajectory forecasting that explicitly incorporates agents&#39; class probabilities. We additionally present PUP, a new challenging real-world autonomous driving dataset, to investigate the im-pact of Perceptual Uncertainty in Prediction. It contains chal-lenging crowded scenes with unfiltered agent class probabilities that reflect the long-tail of current state-of-the-art perception systems. We demonstrate that incorporating class probabilities in trajectory forecasting significantly improves performance in the face of uncertainty, and enables new forecasting capabilities such as counterfactual predictions.},
  archive   = {C_IROS},
  author    = {Boris Ivanovic and Kuan-Hui Lee and Pavel Tokmakov and Blake Wulfe and Rowan Mcllister and Adrien Gaidon and Marco Pavone},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982283},
  pages     = {12196-12203},
  title     = {Heterogeneous-agent trajectory forecasting incorporating class uncertainty},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic-GAN: Learning spatial-temporal attention for dynamic
object removal in feature dense environments. <em>IROS</em>,
12189–12195. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an attention-based, deep learning framework that converts robot camera frames with dynamic content into static frames to more easily apply simultaneous localization and mapping (SLAM) algorithms. The vast majority of SLAM methods have difficulty in the presence of dynamic objects appearing in the environment and occluding the area being captured by the camera. Despite past attempts to deal with dynamic objects, challenges remain to reconstruct large, occluded areas with complex backgrounds. Our proposed Dynamic-GAN framework employs a generative adversarial network to remove dynamic objects from a scene and inpaint a static image free of dynamic objects. The Dynamic-GAN framework utilizes spatial-temporal transformers, and a novel spatial-temporal loss function. The evaluation of Dynamic-GAN was comprehensively conducted both quantitatively and qualitatively by testing it on benchmark datasets, and on a mobile robot in indoor navigation environments. As people appeared dynamically in close proximity to the robot, results showed that large, feature-rich occluded areas can be accurately reconstructed with our attention-based deep learning framework for dynamic object removal. Through experiments we demonstrate that our proposed algorithm has up to 25\% better performance on average as compared to the standard benchmark algorithms.},
  archive   = {C_IROS},
  author    = {Christopher M Trombley and Sumit Kumar Das and Dan O Popa},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981102},
  pages     = {12189-12195},
  title     = {Dynamic-GAN: Learning spatial-temporal attention for dynamic object removal in feature dense environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Robust trajectory planning for spatial-temporal multi-drone
coordination in large scenes. <em>IROS</em>, 12182–12188. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we describe a robust multi-drone planning framework for high-speed trajectories in large scenes. It uses a free-space-oriented map to free the optimization from cumbersome environment data. A capsule-like safety constraint is designed to avoid reciprocal collisions when vehicles deviate from their nominal flight progress under disturbance. We further show the minimum-singularity differential flatness of our drone dynamics with nonlinear drag effects involved. Leveraging the flatness map, trajectory optimization is efficiently conducted on the flat outputs while still subject to physical limits considering drag forces at high speeds. The robustness and effectiveness of our framework are both validated in large-scale simulations. It can compute collision-free trajectories satisfying high-fidelity vehicle constraints for hundreds of drones within 10 minutes.},
  archive   = {C_IROS},
  author    = {Zhepei Wang and Chao Xu and Fei Gao},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982032},
  pages     = {12182-12188},
  title     = {Robust trajectory planning for spatial-temporal multi-drone coordination in large scenes},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Drone with pneumatic-tethered suction-based perching
mechanism for high payload application. <em>IROS</em>, 12154–12161. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Concrete infrastructures provide the means to connect cities and transport people and goods. They require regular inspection to assess their current conditions. Aerial work platforms and underbridge platforms or scaffolding are the common equipment used for inspection of elevated infrastructure. These methods often cost more to operate and maintain, are time-consuming, and raise risks for the inspector. One interesting field of research for UAVs that can be used for infrastructure inspection is aerial perching. A perching UAV can be loaded with an inspection apparatus foregoing the need for costly equipment and risks involved in the inspection. Many have presented aerial perching for various applications and not as much for applications related to concrete infrastructure inspection. This study investigates a perching UAV that can perform perching on both smooth and rough concrete surfaces. This paper presents an unmanned aerial system that utilizes a suction-based perching mechanism with a pneumatic supply tethered from the ground. The proposed perching mechanism provides a reliable and high payload capacity needed for non-destructive testing of the infrastructure. The paper introduces the concept, presents the design and proof of concept, and validates the idea through actual bridge experiments.},
  archive   = {C_IROS},
  author    = {Jim David Ang and Lester Librado and Carl John Salaan and Jonathan Maglasang and Kristine Sanchez and Marcelo Ang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982219},
  pages     = {12154-12161},
  title     = {Drone with pneumatic-tethered suction-based perching mechanism for high payload application},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HighlightNet: Highlighting low-light potential features for
real-time UAV tracking. <em>IROS</em>, 12146–12153. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Low-light environments have posed a formidable challenge for robust unmanned aerial vehicle (UAV) tracking even with state-of-the-art (SOTA) trackers since the poten-tial image features are hard to extract under adverse light conditions. Besides, due to the low visibility, accurate online selection of the object also becomes extremely difficult for human monitors to initialize UAV tracking in ground con-trol stations. To solve these problems, this work proposes a novel enhancer, i.e., HighlightNet, to light up potential objects for both human operators and UAV trackers. By employing Transformer, HighlightNet can adjust enhancement parameters according to global features and is thus adaptive for the illumination variation. Pixel-level range mask is introduced to make HighlightNet more focused on the enhancement of the tracking object and regions without light sources. Furthermore, a soft truncation mechanism is built to prevent background noise from being mistaken for crucial features. Evaluations on image enhancement benchmarks demonstrate HighlightNet has advantages in facilitating human perception. Experiments on the public UAVDark135 benchmark show that HightlightNet is more suitable for UAV tracking tasks than other state-of-the-art (SOTA) low-light enhancers. In addition, real-world tests on a typical UAV platform verify HightlightNet&#39;s practicability and efficiency in nighttime aerial tracking-related applications. The code and demo videos are available at https://github.com/vision4robotics/HighlightNet.},
  archive   = {C_IROS},
  author    = {Changhong Fu and Haolin Dong and Junjie Ye and Guangze Zheng and Sihang Li and Jilin Zhao},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981070},
  pages     = {12146-12153},
  title     = {HighlightNet: Highlighting low-light potential features for real-time UAV tracking},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). End-to-end feature decontaminated network for UAV tracking.
<em>IROS</em>, 12130–12137. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Object feature pollution is one of the burning issues in vision-based UAV tracking, commonly caused by occlusion, fast motion, and illumination variation. Due to the contaminated information in the polluted object features, most trackers fail to precisely estimate the object location and scale. To address the above disturbing issue, this work proposes a novel end-to-end feature decontaminated network for efficient and effective UAV tracking, i.e., FDNT. FDNT mainly includes two modules: a decontaminated downsampling network and a decontaminated upsampling network. The former reduces the interference information of the feature pollution and enhanced the expression of the object location information with two asymmetric convolution branches. The latter restores the object scale information with the super-resolution technology-based low-to-high encoder, achieving a further decontamination effect. Moreover, a novel pooling distance loss is carefully developed to assist the decontaminated downsampling network in concentrating on the critical regions with the object information. Exhaustive experiments on three well-known benchmarks validate the effectiveness of FDNT, especially on the sequences with feature pollution. In addition, real-world tests show the efficiency of FDNT with 31.4 frames per second. The code and demo videos are available at https://github.com/vision4robotics/FDNT.},
  archive   = {C_IROS},
  author    = {Haobo Zuo and Changhong Fu and Sihang Li and Junjie Ye and Guangze Zheng},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981882},
  pages     = {12130-12137},
  title     = {End-to-end feature decontaminated network for UAV tracking},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Local perception-aware transformer for aerial tracking.
<em>IROS</em>, 12122–12129. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Transformer-based visual object tracking has been utilized extensively. However, the Transformer structure is lack of enough inductive bias. In addition, only focusing on encoding the global feature does harm to modeling local details, which restricts the capability of tracking in aerial robots. Specifically, with local-modeling to global-search mechanism, the proposed tracker replaces the global encoder by a novel local-recognition encoder. In the employed encoder, a local-recognition attention and a local element correction network are carefully designed for reducing the global redundant information interference and increasing local inductive bias. Meanwhile, the latter can model local object details precisely under aerial view through detail-inquiry net. The proposed method achieves competitive accuracy and robustness in several authoritative aerial benchmarks with 316 sequences in total. The proposed tracker&#39;s practicability and efficiency have been validated by the real-world tests. The source code is available at https://github.com/vision4robotics/LPAT.},
  archive   = {C_IROS},
  author    = {Changhong Fu and Weiyu Peng and Sihang Li and Junjie Ye and Ziang Cao},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981248},
  pages     = {12122-12129},
  title     = {Local perception-aware transformer for aerial tracking},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Linear and nonlinear model predictive control strategies for
trajectory tracking micro aerial vehicles: A comparative study.
<em>IROS</em>, 12106–12113. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a comparison of linear and nonlinear Model Predictive Control (MPC) strategies for trajectory tracking Micro Aerial Vehicles (MAVs). In this comparative study, we paid particular attention to establish quantitatively fair metrics and testing conditions for both strategies. In particular, we chose the most suitable numerical algorithms to bridge the gap between linear and nonlinear MPC, leveraged the very same underlying solver and estimation algorithm with identical parameters, and allow both strategies to operate with a similar computational budget. In order to obtain a well-tuned performance from the controllers, we employed the parameter identification results determined in a previous study for the same robotic platform and added a reliable disturbance observer to compensate for model uncertainties. We carried out a thorough experimental campaign involving multiple representative trajectories. Our approach included three different stages for tuning the algorithmic parameters, evaluating the predictive control feasibility, and validating the performances of both MPC-based strategies. As a result, we were able to propose a decisional recipe for selecting a linear or nonlinear MPC scheme that considers the predictive control feasibility for a peculiar trajectory, characterized by specific speed and acceleration requirements, as a function of the available on-board resources.},
  archive   = {C_IROS},
  author    = {I.K. Erunsal and J. Zheng and R. Ventura and A. Martinoli},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981880},
  pages     = {12106-12113},
  title     = {Linear and nonlinear model predictive control strategies for trajectory tracking micro aerial vehicles: A comparative study},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Can we reach human expert programming performance? A tactile
manipulation case study in learning time and task performance.
<em>IROS</em>, 12081–12088. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reaching human-level performance in tactile manipulation is one of the grand challenges in nowadays robotics research. Over the past decade significant progress in both skill control and learning was made. However, the achievable execution speed still falls behind the human ability, without clearly understanding whether the specific shortcomings are mainly in the control, skill learning, or motion planning layer. For gaining a better understanding of this complex problem, we draw an experimental side-by-side comparative case study. First, given a task program for a challenging benchmarking task, the goal is to objectify the achievable task performance from a human expert programmer against autonomously learning these assembly behaviors with a state-of-the-art skill learning framework. Second, we compare the manually tuned and learned robot skills to the performance of an adult human solving the task manually. For the former, it could be shown that despite longer learning duration, the task execution speed of the machine learning-based solution is equivalent to the one programmed by the human expert. For the latter, the identified performance gap remained significantly larger, where only for some specific isolated skills the system was able to reach comparable or even faster than human execution speeds. The overall analysis gave also useful hints where in particular manipulation policies and arm-hand coordination still need significant improvements in the future.},
  archive   = {C_IROS},
  author    = {Lars Johannsmeier and Sami Haddadin},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982025},
  pages     = {12081-12088},
  title     = {Can we reach human expert programming performance? a tactile manipulation case study in learning time and task performance},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Behavior-tree embeddings for robot task-level knowledge.
<em>IROS</em>, 12074–12080. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, the behavior tree is gaining popularity as a robotic task-level knowledge representation. Manual design of behavior trees from scratch is tedious and cumbersome. Motivated by the need for an efficient way to reuse or transfer robot task-level knowledge, we propose a vector-space embedding approach that encodes a symbolic task into a numerical form. This approach, called behavior-tree embedding, takes a behavior tree that produces a single task as input and generates a corresponding vector. By exploiting the pretrained language-embedding model and the node-aggregation mechanism, the produced embedding is capable of preserving both semantic information of task description and structural information of the hierarchical task organization. We evaluated the effectiveness and versatility of our proposed vector-space embedding approach in three different tasks.},
  archive   = {C_IROS},
  author    = {Yue Cao and C.S. George Lee},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981774},
  pages     = {12074-12080},
  title     = {Behavior-tree embeddings for robot task-level knowledge},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient task/motion planning for a dual-arm robot from
language instructions and cooking images. <em>IROS</em>, 12058–12065.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When generating robot motions based on instructions such as cooking recipes, ambiguity of the instructions and lack of necessary information are problematic for the robot. To solve this problem, we propose an efficient motion planning approach for a dual-arm robot by constructing a graph repre-senting a motion sequence based on a recipe consisting of verbal instructions and cooking images. A functional unit is generated based on the linguistic instructions in the recipe. Since most recipes lack the necessary information for executing the motion, we first consider extracting the information about the cooking motion like cutting from the food images of the recipe and supplementing it. In addition, to supplement the actions that humans perform unconsciously, we generate functional units for actions not explicitly mentioned in the recipe based on the current situation of the cooking process, and then connect them to the functional units generated from the recipe. Moreover, during the connection we consider the motion of the robot&#39;s arms in parallel for an efficient execution of the recipe, similar to those of a human. Through experiments, we demonstrate that for a given recipe, the proposed method can be used to generate a cooking sequence with the supplementary information needed, and executed by a dual-arm robot. The results show that the proposed method is effective and can simplify robot teaching in cooking tasks.},
  archive   = {C_IROS},
  author    = {Kota Takata and Takuya Kiyokawa and Ixchel G. Ramirez-Alpizar and Natsuki Yamanobe and Weiwei Wan and Kensuke Harada},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981280},
  pages     = {12058-12065},
  title     = {Efficient Task/Motion planning for a dual-arm robot from language instructions and cooking images},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Probabilistic planning for AUV data harvesting from smart
underwater sensor networks. <em>IROS</em>, 12051–12057. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Harvesting valuable ocean data, ranging from climate and marine life analysis to industrial equipment monitoring, is an extremely challenging real-world problem. Sparse underwater sensor networks are a promising approach to scale to larger and deeper environments, but these have difficulty offloading their data without external assistance. Traditionally, offloading data has been achieved by costly, fixed communication infrastructure. In this paper, we propose a planning under uncertainty method that enables an autonomous underwater vehicle (AUV) to adaptively collect data from smart sensor networks in underwater environments. Our novel solution exploits the ability of sensor nodes to provide the AUV with time-of-flight acoustic localisation, and is able to prioritise nodes with the most valuable data. In both simulated experiments and a real-world field trial, we demonstrate that our method outperforms the type of hand-designed behaviours that has previously been used in the context of underwater data harvesting.},
  archive   = {C_IROS},
  author    = {Matthew Budd and Georgios Salavasidis and Izzat Karnarudzaman and Catherine A. Harris and Alexander B. Phillips and Paul Duckworth and Nick Hawes and Bruno Lacerda},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981460},
  pages     = {12051-12057},
  title     = {Probabilistic planning for AUV data harvesting from smart underwater sensor networks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-objective task allocation for multi-agent systems
using hierarchical cost function. <em>IROS</em>, 12045–12050. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent systems are deployed to accomplish tasks that take a long time with a single agent. The task allocation problem becomes particularly difficult when the objectives are conflicting with one another (e.g. minimizing the mission time while respecting the task priorities, while simultaneously maximizing agent&#39;s fitness for the task). This paper presents an algorithm to create task assignments for a group of autonomous agents with competing objectives. We consider a variety of constraints including agent capabilities to perform tasks, priorities set by a human supervisor, as well as temporal constraints such as arrival time or coalition formation. We propose a multi-objective Particle Swarm Optimization (PSO) that uses a hierarchical cost function by leveraging the paradigm of lexicographic optimization. The particles are driven by higher ranked objectives with lower ranked objectives used to break ties. We demonstrate the effectiveness of this algorithm in a battlefield scenario where sub-teams of aerial vehicles are assigned to perform area reconnaissance, target strikes, and intelligence gathering.},
  archive   = {C_IROS},
  author    = {Navid Dadkhah Tehrani and Andrew Krzywosz and Igor Cherepinsky and Sean Carlson},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981071},
  pages     = {12045-12050},
  title     = {Multi-objective task allocation for multi-agent systems using hierarchical cost function},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Extended time dependent vehicle routing problem for joint
task allocation and path planning in shared space. <em>IROS</em>,
12037–12044. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the joint task allocation and path planning problem whereby an operator with a fleet of vehicles must assign multiple tasks to each vehicle, while ensuring collision-free paths for them such that the total travel cost is minimized. Instead of sequentially solving the task allocation problem first, and then resolving all predicted collisions, i.e. conflicts between vehicles, we propose a novel method that solves in a simultaneous way task allocation and multi-agent path planning. Specifically, we introduce an extension of the Time Dependent Vehicle Routing Problem (TDVRP) whereby we propose to integrate conflicts information into a time dependent cost function used in the task allocation resolution. We compare our approach to two baseline approaches that both use a standard Capacitated VRP (CVRP) solver, a “one-shot” method and a “multi-shot” method. We perform simulations on benchmark realistic warehouse scenarios and the obtained results show that our proposed approach is able to generate improvements in the solutions costs compared to the baseline approaches.},
  archive   = {C_IROS},
  author    = {Aayush Aggarwal and Florence Ho and Shinji Nakadai},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981570},
  pages     = {12037-12044},
  title     = {Extended time dependent vehicle routing problem for joint task allocation and path planning in shared space},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal constrained task planning as mixed integer
programming. <em>IROS</em>, 12029–12036. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For robots to successfully execute tasks as-signed to them, they must be capable of planning the right sequence of actions. These actions must be both optimal with respect to a specified objective and satisfy whatever constraints exist in their world. We propose an approach for robot task planning that is capable of planning the optimal sequence of grounded actions to accomplish a task given a specific objective function while satisfying all specified numerical constraints. Our approach accomplishes this by encoding the entire task planning problem as a single mixed integer convex program, which it then solves using an off-the-shelf Mixed Integer Program-ming solver. We evaluate our approach on several mobile manipulation tasks in both simulation and on a physical humanoid robot. Our approach is able to consistently produce optimal plans while accounting for all specified numerical constraints in the mobile manipulation tasks. Open-source implementations of the components of our approach as well as videos of robots executing planned grounded actions in both simulation and the physical world can be found at this url: https://adubredu.github.io/gtpmip},
  archive   = {C_IROS},
  author    = {Alphonsus Adu-Bredu and Nikhil Devraj and Odest Chadwicke Jenkins},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981237},
  pages     = {12029-12036},
  title     = {Optimal constrained task planning as mixed integer programming},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Contact-timing and trajectory optimization for 3D jumping on
quadruped robots. <em>IROS</em>, 11994–11999. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Performing highly agile acrobatic motions with a long flight phase requires perfect timing, high accuracy, and coordination of the full-body motion. To address these challenges, we present a novel approach on timings and trajectory optimization framework for legged robots performing aggressive 3D jumping. In our method, we firstly utilize an effective optimization framework using simplified rigid body dynamics to solve for contact timings and a reference trajectory of the robot body. The solution of this module is then used to formulate a full-body trajectory optimization based on the full nonlinear dynamics of the robot. This combination allows us to effectively optimize for contact timings while ensuring that the jumping trajectory can be effectively realized in the robot hardware. We first validate the efficiency of the proposed framework on the A1 robot model for various 3D jumping tasks such as double-backflips off the high altitude of 2m. Experimental validation was then successfully conducted for various aggressive 3D jumping motions such as diagonal jumps, barrel roll, and double barrel roll from a box of heights 0.4m and 0.9m, respectively.},
  archive   = {C_IROS},
  author    = {Chuong Nguyen and Quan Nguyen},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981284},
  pages     = {11994-11999},
  title     = {Contact-timing and trajectory optimization for 3D jumping on quadruped robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Contact-implicit differential dynamic programming for model
predictive control with relaxed complementarity constraints.
<em>IROS</em>, 11978–11985. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we propose a novel differential dynamic programming (DDP) framework for systems involving contact with the ground. The approach converts a general constrained differential dynamic programming into contact-implicit one by incorporating contact dynamics in a linear complementarity problem (LCP) formulation. Analytical gradients of the contact dynamics are obtained through a relaxed complementarity condition in the LCP formulation that helps the search directions of optimization avoid stalling in bad local minima or saddle points. Incorporation of contact dynamics and its analytical gradients into DDP enables an online discovery of not only dynamically-feasible trajectories of states, control inputs, and contact forces but also contact mode sequences. We demonstrate that our Contact-Implicit Differential Dynamic Programming framework successfully finds totally new dynamic motions with contact mode sequences in a variety of robotic systems including an one-legged hopping robot and planar quadrupedal robot in simulation environment.},
  archive   = {C_IROS},
  author    = {Gijeong Kim and Dongyun Kang and Joon-Ha Kim and Hae-Won Park},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981476},
  pages     = {11978-11985},
  title     = {Contact-implicit differential dynamic programming for model predictive control with relaxed complementarity constraints},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Zero-shot retargeting of learned quadruped locomotion
policies using hybrid kinodynamic model predictive control.
<em>IROS</em>, 11971–11977. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement Learning (RL) has witnessed great strides for quadruped locomotion, with continued progress in the reliable sim-to-real transfer of policies. However, it remains a challenge to reuse a policy on another robot, which could save time for retraining. In this work, we present a framework for zero-shot policy retargeting wherein diverse motor skills can be transferred between robots of different shapes and sizes. The new framework centers on a planning-and-control pipeline that systematically integrates RL and Model Predictive Control (MPC). The planning stage employs RL to generate a dynamically plausible trajectory as well as the contact schedule, avoiding the combinatorial complexity of contact sequence optimization. This information is then used to seed the MPC to stabilize and robustify the policy roll-out via a new Hybrid Kinodynamic (HKD) model that implicitly optimizes the foothold locations. Hardware results show an ability to transfer policies from both the A1 and Laikago robots to the MIT Mini Cheetah robot without requiring any policy re-tuning.},
  archive   = {C_IROS},
  author    = {He Li and Wenhao Yu and Tingnan Zhang and Patrick M. Wensing},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981967},
  pages     = {11971-11977},
  title     = {Zero-shot retargeting of learned quadruped locomotion policies using hybrid kinodynamic model predictive control},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved performance of CPG parameter inference for
path-following control of legged robots. <em>IROS</em>, 11963–11970. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The difficulty associated with the coordinated locomotion of legged robots grows quickly as the number of joints increases. Although prior approaches have addressed this problem through sampling-based planners, learning-based techniques have recently been explored as a means to handle such complexity. Among these recent approaches are systems that utilize probabilistic graphical models in order to infer parameters for central pattern generators (CPGs) which enable the path-following locomotion of highly-articulated legged robots through unstructured terrain. This paper presents a novel formulation of a CPG parameter inference-based path-following controller. The new inference process and accompanying CPG formulation enforce oscillator convergence to the limit-cycle specified by the inferred parameters in addition to biasing towards parameters that quickly reach stable-state. This formulation is shown to improve the performance of CPG parameter inference-based path-following control for legged robots across a number of simulated and physical experiments.},
  archive   = {C_IROS},
  author    = {Nathan D. Kent and David Neiman and Matthew Travers and Thomas M. Howard},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981859},
  pages     = {11963-11970},
  title     = {Improved performance of CPG parameter inference for path-following control of legged robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Animal motions on legged robots using nonlinear model
predictive control. <em>IROS</em>, 11955–11962. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents a motion capture-driven locomotion controller for quadrupedal robots that replicates the non-periodic footsteps and subtle body movement of animal motions. We adopt a nonlinear model predictive control (NMPC) formulation that generates optimal base trajectories and stepping locations. By optimizing both footholds and base trajectories, our controller effectively tracks retargeted animal motions with natural body movements and highly irregular strides. We demonstrate our approach with prerecorded animal motion capture data. In simulation and hardware experiments, our motion controller enables quadrupedal robots to robustly reproduce fundamental characteristics of a target animal motion regardless of the significant morphological disparity.},
  archive   = {C_IROS},
  author    = {Dongho Kang and Flavio De Vincenti and Naomi C. Adami and Stelian Coros},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981945},
  pages     = {11955-11962},
  title     = {Animal motions on legged robots using nonlinear model predictive control},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Vastus and gastrocnemius improve hopping efficiency and
joints synchronicity at different frequencies: A robotic study.
<em>IROS</em>, 11947–11954. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The lower limb morphology of biological locomotors is abundant in muscle-tendon units. Yet, not much is known about how these actuation units contribute to the output performance and energy economy of movements. In this work, we investigate the functionality of four of the important lower limb muscles - Vastus, Popliteus, Soleus, and Gastrocnemius - in a hopping task at different frequencies (1.5-3.5 Hz). These muscles are implemented as pneumatic artificial muscles (PAMs) on the EPA-Hopper-II robot, which is a human-sized 3-segmented leg co-actuated by electrical motors and PAMs. A bioinspired reflex-based Force Modulated Control (FMC) is also implemented on the robot to achieve hopping at different frequencies. The results show that the Vastus contributes the most to energy-efficient hopping at low to mid frequencies. The biarticular Gastrocnemius also helps increase efficiency at low frequencies. Further, it is found that the Gastrocnemius synchronizes the knee-ankle motion and mitigates lateral knee motion. The outcomes of this work add further evidence to hypotheses regarding human lower-limb actuation and proper recruitment of muscles for building more efficient robots.},
  archive   = {C_IROS},
  author    = {Omid Mohseni and Patrick Schmidt and Andre Seyfarth and Maziar A. Sharbafi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981685},
  pages     = {11947-11954},
  title     = {Vastus and gastrocnemius improve hopping efficiency and joints synchronicity at different frequencies: A robotic study},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards autonomous grading in the real world. <em>IROS</em>,
11940–11946. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Surface grading is an integral part of the construction pipeline. Here, a bulldozer, which is a key machinery tool at any construction site, is required to level an uneven area containing pre-dumped sand piles. In this work, we aim to tackle the problem of autonomous surface grading on real-world scenarios. We design both a realistic physical simulation and a scaled real-world prototype environment mimicking real bulldozer dynamics and sensory information. In addition, we establish heuristics and learning strategies in order to solve the problem. Through extensive experiments, we show that although heuristics are capable of tackling the problem in a clean and noise-free simulated environment, they fail catastrophically when facing real-world scenarios. However, we show that the simulation can be leveraged to guide a learning agent, which can generalize and solve the task both in simulation and in a scaled prototype environment.},
  archive   = {C_IROS},
  author    = {Yakov Miron and Chana Ross and Yuval Goldfracht and Chen Tessler and Dotan Di Castro},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982114},
  pages     = {11940-11946},
  title     = {Towards autonomous grading in the real world},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Informative path planning for active learning in aerial
semantic mapping. <em>IROS</em>, 11932–11939. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Semantic segmentation of aerial imagery is an important tool for mapping and earth observation. However, supervised deep learning models for segmentation rely on large amounts of high-quality labelled data, which is labour-intensive and time-consuming to generate. To address this, we propose a new approach for using unmanned aerial vehicles (UAVs) to autonomously collect useful data for model training. We exploit a Bayesian approach to estimate model uncertainty in semantic segmentation. During a mission, the semantic predictions and model uncertainty are used as input for terrain mapping. A key aspect of our pipeline is to link the mapped model uncertainty to a robotic planning objective based on active learning. This enables us to adaptively guide a UAV to gather the most informative terrain images to be labelled by a human for model training. Our experimental evaluation on real-world data shows the benefit of using our informative planning approach in comparison to static coverage paths in terms of maximising model performance and reducing labelling efforts.},
  archive   = {C_IROS},
  author    = {Julius Rückin and Liren Jin and Federico Magistri and Cyrill Stachniss and Marija Popović},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981738},
  pages     = {11932-11939},
  title     = {Informative path planning for active learning in aerial semantic mapping},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive coverage path planning for efficient exploration of
unknown environments. <em>IROS</em>, 11916–11923. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a method for solving the coverage problem with the objective of autonomously exploring an unknown environment under mission time constraints. Here, the robot is tasked with planning a path over a horizon such that the accumulated area swept out by its sensor footprint is maximized. Because this problem exhibits a diminishing returns property known as submodularity, we choose to formulate it as a tree-based sequential decision making process. This formulation allows us to evaluate the effects of the robot&#39;s actions on future world coverage states, while simultaneously accounting for traversability risk and the dynamic constraints of the robot. To quickly find near-optimal solutions, we propose an effective approximation to the coverage sensor model which adapts to the local environment. Our method was extensively tested across various complex environments and served as the local exploration algorithm for a competing entry in the DARPA Subterranean Challenge.},
  archive   = {C_IROS},
  author    = {Amanda Bouman and Joshua Ott and Sung-Kyun Kim and Kenny Chen and Mykel J. Kochenderfer and Brett Lopez and Ali-akbar Agha-mohammadi and Joel Burdick},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982287},
  pages     = {11916-11923},
  title     = {Adaptive coverage path planning for efficient exploration of unknown environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic compressed sensing of unsteady flows with a mobile
robot. <em>IROS</em>, 11910–11915. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large-scale environmental sensing with a finite number of mobile sensors is a challenging task that requires tremendous resources and time. This is especially true when the environmental features of interest are spatiotemporally changing with unknown or partially known dynamics. Fortunately, these dynamic features often evolve in a low-dimensional space, making it possible to capture their dynamics sufficiently well with a finite number of sensor measurements. This paper investigates the problem of dynamic compressed sensing of an unsteady, periodic flow field with a mobile sensor. We take advantage of the inherently low dimensionality of the under-lying flow dynamics to reduce number of critical waypoints for sensor trajectory. The optimal set of sensing waypoints are identified by an iterative compressed sensing algorithm that optimizes the flow reconstruction based on the proper orthogonal decomposition modes. An optimal sampling trajectory is then found to traverse these waypoints while minimizing the energy consumption, time, and flow reconstruction error. Simulation results in a double-gyre flow field is presented to demonstrate the efficacy of the proposed algorithms. Experimental results with an indoor quadcopter are presented to show the tracking feasibility of the resulting trajectory.},
  archive   = {C_IROS},
  author    = {Sachin Shriwastav and Gregory Snyder and Zhuoyuan Song},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981737},
  pages     = {11910-11915},
  title     = {Dynamic compressed sensing of unsteady flows with a mobile robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sampling-based view planning for MAVs in active
visual-inertial state estimation. <em>IROS</em>, 11893–11899. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Micro aerial vehicles usually have strap-down sensors on the vehicle body, leading to the severe coupling effect between perception and trajectory planning. As a result, visual-inertial simultaneous localization and mapping (VI-SLAM) technologies implemented on MAVs suffer from tracking failure problems, especially in featureless environments. To overcome these challenges, based on MAVs with movable camera mechanisms (e.g., gimbal stabilizer, pan-tilt, or bionic neck-eye system), we proposed two sampling-based algorithms for known and unknown environments respectively. The first active perception planning algorithm based on a scene richness model is developed with a built feature map for the environment. Differ from the first algorithm, the second one is modified for active localization in unknown 3D space. It is basically a time-based sampling-based approach that uses the same scene richness model. In addition, it also achieved a balance between exploitation and exploration. With the above solutions, the robustness of visual perception is improved while avoiding over-exploitation of known information. Simulation and real-world experiments are performed to verify the feasibility of our algorithms.},
  archive   = {C_IROS},
  author    = {Zhengyu Hua and Fengyu Quan and Haoyao Chen and Jiabi Sun and Jianheng Liu and Yunhui Liu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981941},
  pages     = {11893-11899},
  title     = {Sampling-based view planning for MAVs in active visual-inertial state estimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generating safe corridors roadmap for urban air mobility.
<em>IROS</em>, 11866–11871. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Personal air transportation on short distances, so-called Urban Air Mobility (UAM), is a trend in modern aviation that raises new challenges as flying in urban areas at low altitudes induces an additional risk to people and properties on the ground. Risk-aware trajectory planning can mitigate the risk by detouring and flying over less populated and thus less risky areas. Existing risk-aware trajectory planning approaches are computationally demanding single-query methods that are impractical for online usage. Moreover, coordinated planning for multiple aircraft is prohibitively expensive. Therefore, we propose to reduce computational demands by determining low-risk areas called safe corridors and creating a roadmap of safe corridors based on multiple least risky trajectories. The created roadmap can be used in graph-based multi-agent planning methods for coordinated trajectory planning. The proposed method has been evaluated in a realistic urban scenario, suggesting a significant computational burden reduction and less risky trajectories than the current state-of-the-art methods.},
  archive   = {C_IROS},
  author    = {Jakub Sláma and Petr Váňa and Jan Faigl},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981326},
  pages     = {11866-11871},
  title     = {Generating safe corridors roadmap for urban air mobility},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). E-TRoll: Tactile sensing and classification via a simple
robotic gripper for extended rolling manipulations. <em>IROS</em>,
11826–11833. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic tactile sensing provides a method of recognizing objects and their properties where vision fails. Prior work on tactile perception in robotic manipulation has frequently focused on exploratory procedures (EPs). However, the also-human-inspired technique of in-hand-manipulation can glean rich data in a fraction of the time of EPs. We propose a simple 3-DOF robotic hand design, optimized for object rolling tasks via a variable-width palm and associated control system. This system dynamically adjusts the distance between the finger bases in response to object behavior. Compared to fixed finger bases, this technique significantly increases the area of the object that is exposed to finger-mounted tactile arrays during a single rolling motion (an increase of over 60\% was observed for a cylinder with a 30-millimeter diameter). In addition, this paper presents a feature extraction algorithm for the collected spatiotemporal dataset, which focuses on object corner identification, analysis, and compact representation. This technique drastically reduces the dimensionality of each data sample from $\boldsymbol{10\times 1500}$ time series data to 80 features, which was further reduced by Principal Component Analysis (PCA) to 22 components. An ensemble subspace k-nearest neighbors (KNN) classification model was trained with 90 observations on rolling three different geometric objects, resulting in a three-fold cross-validation accuracy of 95.6\% for object shape recognition.},
  archive   = {C_IROS},
  author    = {Xin Zhou and Adam J. Spiers},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982191},
  pages     = {11826-11833},
  title     = {E-TRoll: Tactile sensing and classification via a simple robotic gripper for extended rolling manipulations},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optical proximity sensing for pose estimation during in-hand
manipulation. <em>IROS</em>, 11818–11825. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {During in-hand manipulation, robots must be able to continuously estimate the pose of the object in order to generate appropriate control actions. The performance of algorithms for pose estimation hinges on the robot&#39;s sensors being able to detect discriminative geometric object features, but previous sensing modalities are unable to make such measurements robustly. The robot&#39;s fingers can occlude the view of environment- or robot-mounted image sensors, and tactile sensors can only measure at the local areas of contact. Motivated by fingertip-embedded proximity sensors&#39; robustness to occlusion and ability to measure beyond the local areas of contact, we present the first evaluation of proximity sensor based pose estimation for in-hand manipulation. We develop a novel two-fingered hand with fingertip-embedded optical time-of-flight proximity sensors as a testbed for pose estimation during planar in-hand manipulation. Here, the in-hand manipulation task consists of the robot moving a cylindrical object from one end of its workspace to the other. We demonstrate, with statistical significance, that proximity-sensor based pose estimation via particle filtering during in-hand manipulation: a) exhibits 50\% lower average pose error than a tactile-sensor based baseline; b) empowers a model predictive controller to achieve 30\% lower final positioning error compared to when using tactile-sensor based pose estimates.},
  archive   = {C_IROS},
  author    = {Patrick Lancaster and Pratik Gyawali and Christoforos Mavrogiannis and Siddhartha S. Srinivasa and Joshua R. Smith},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981692},
  pages     = {11818-11825},
  title     = {Optical proximity sensing for pose estimation during in-hand manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A system for imitation learning of contact-rich bimanual
manipulation policies. <em>IROS</em>, 11810–11817. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we discuss a framework for teaching bimanual manipulation tasks by imitation. To this end, we present a system and algorithms for learning compliant and contact-rich robot behavior from human demonstrations. The presented system combines insights from admittance control and machine learning to extract control policies that can (a) recover from and adapt to a variety of disturbances in time and space, while also (b) effectively leveraging physical contact with the environment. We demonstrate the effectiveness of our approach using a real-world insertion task involving multiple simultaneous contacts between a manipulated object and insertion pegs. We also investigate efficient means of collecting training data for such bimanual settings. To this end, we conduct a human-subject study and analyze the effort and mental demand as reported by the users. Our experiments show that, while harder to provide, the additional force/torque information available in teleoperated demonstrations is crucial for phase estimation and task success. Ultimately, force/torque data substantially improves manipulation robustness, resulting in a 90\% success rate in a multipoint insertion task. Code and videos can be found at https://bimanualmanipulation.com/},
  archive   = {C_IROS},
  author    = {Simon Stepputtis and Maryam Bandari and Stefan Schaal and Heni Ben Amor},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981802},
  pages     = {11810-11817},
  title     = {A system for imitation learning of contact-rich bimanual manipulation policies},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Transferring dexterous manipulation from GPU simulation to a
remote real-world TriFinger. <em>IROS</em>, 11802–11809. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In-hand manipulation of objects is an important capability to enable robots to carry-out tasks which demand high levels of dexterity. This work presents a robot systems approach to learning dexterous manipulation tasks involving moving objects to arbitrary 6-DoF poses. We show empirical benefits, both in simulation and sim - to- real transfer, of using keypoint-based representations for object pose in policy observations and reward calculation to train a model-free reinforcement learning agent. By utilizing domain randomization strategies and large-scale training, we achieve a high success rate of 83\% on a real TriFinger system, with a single policy able to perform grasping, ungrasping, and finger gaiting in order to achieve arbitrary poses within the workspace. We demonstrate that our policy can generalise to unseen objects, and success rates can be further improved through finetuning. With the aim of assisting further research in learning in-hand manipulation, we provide a detailed exposition of our system and make the codebase of our system available, along with checkpoints trained on billions of steps of experience, at https://s2r2-ig.github.io},
  archive   = {C_IROS},
  author    = {Arthur Allshire and Mayank MittaI and Varun Lodaya and Viktor Makoviychuk and Denys Makoviichuk and Felix Widmaier and Manuel Wüthrich and Stefan Bauer and Ankur Handa and Animesh Garg},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981458},
  pages     = {11802-11809},
  title     = {Transferring dexterous manipulation from GPU simulation to a remote real-world TriFinger},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Kinesthetic teaching of bi-manual tasks with known relative
constraints. <em>IROS</em>, 11796–11801. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Kinesthetic teaching allows the direct skill transfer from the human to the robot and has been widely used to teach single arm tasks intuitively. In the bi-manual case, simultaneously moving both end-effectors is challenging due to the high physical and cognitive load imposed to the user. Thus, previous works on bi-manual task teaching resort to less intuitive methods by teaching each arm separately. This in turn requires motion synthesis and synchronization before execution. In this work, we leverage knowledge from the relative task space to facilitate a kinesthetic demonstration by guiding both end-effectors which is more human-like and intuitive way for performing bi-manual tasks. Our method utilizes the notion of virtual fixtures and inertia minimization in the null space of the task. The controller is experimentally validated in a bi-manual task which involves the drawing of a preset line on a workpiece utilizing two KUKA IIWA7 R800 robots. Results from ten participants were compared with a gravity compensation scheme demonstrating improved performance.},
  archive   = {C_IROS},
  author    = {Sotiris Stavridis and Dimitrios Papageorgiou and Zoe Doulgeri},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981196},
  pages     = {11796-11801},
  title     = {Kinesthetic teaching of bi-manual tasks with known relative constraints},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High-speed accurate robot control using learned forward
kinodynamics and non-linear least squares optimization. <em>IROS</em>,
11789–11795. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate control of robots at high speeds requires a control system that can take into account the kinodynamic interactions of the robot with the environment. Prior works on learning inverse kinodynamic (IKD) models of robots have shown success in capturing the complex kinodynamic effects. However, the types of control problems these approaches can be applied to are limited only to that of following pre-computed kinodynamically feasible trajectories. In this paper we present Optim-FKD, a new formulation for accurate, high-speed robot control that makes use of a learned forward kinodynamic (FKD) model and non-linear least squares optimization. Optim-FKD can be used for accurate, high speed control on any control task specifiable by a non-linear least squares objective. Optim-FKD can solve for control objectives such as path following and time-optimal control in real time, without needing access to pre-computed kinodynamically feasible trajectories. We empirically demonstrate these abilities of our approach through experiments on a scale one-tenth autonomous car. Our results show that Optim-FKD can follow desired trajectories more accurately and can find better solutions to optimal control problems than baseline approaches.},
  archive   = {C_IROS},
  author    = {Pranav Atreya and Haresh Karnan and Kavan Singh Sikand and Xuesu Xiao and Sadegh Rabiee and Joydeep Biswas},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981259},
  pages     = {11789-11795},
  title     = {High-speed accurate robot control using learned forward kinodynamics and non-linear least squares optimization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analytical second-order partial derivatives of rigid-body
inverse dynamics. <em>IROS</em>, 11781–11788. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Optimization-based robot control strategies often rely on first-order dynamics approximation methods, as in iLQR. Using second-order approximations of the dynamics is expensive due to the costly second-order partial derivatives of the dynamics with respect to the state and control. Current approaches for calculating these derivatives typically use automatic differentiation (AD) and chain-rule accumulation or finite-difference. In this paper, for the first time, we present analytical expressions for the second-order partial derivatives of inverse dynamics for open-chain rigid-body systems with floating base and multi-DoF joints. A new extension of spatial vector algebra is proposed that enables the analysis. A recursive algorithm with complexity of $\mathcal{O}(Nd^{2})$ is also provided where N is the number of bodies and d is the depth of the kinematic tree. A comparison with AD in CasADi shows speedups of 1.5-3 x for serial kinematic trees with N &gt; 5, and a C++ implementation shows runtimes of $\approx \mathbf{5 1} \mu \mathrm{s}$ for a quadruped.},
  archive   = {C_IROS},
  author    = {Shubham Singh and Ryan P. Russell and Patrick M. Wensing},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981356},
  pages     = {11781-11788},
  title     = {Analytical second-order partial derivatives of rigid-body inverse dynamics},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic generation of optimization model using process
mining and petri nets for optimal motion planning of 6-DOF manipulators.
<em>IROS</em>, 11767–11772. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose an optimization system for motion planning of robot arms using Petri Nets. The proposed optimization system consists of four sub-systems consisting of automatic generation of Petri Nets from event log data, optimization system of firing sequence of derived Petri Net model, verification system using Petri Net simulation, and an automatic program generation system. The model generation system automatically generates the Petri Net model from the event logs using process mining. The Petri Net verification system is used to check the consistency of the generated Petri Nets to obtain the optimal firing sequence for robot motion. The motion planning algorithm generates motion programs for robots based on optimal firing sequences. The proposed optimization model is applied to a 6-DOF (Degree of Freedom) robot manipulator (Niryo Ned). Experimental results show that the proposed method achieves motion plan optimization for the pick-and-place operation with different robot configurations.},
  archive   = {C_IROS},
  author    = {Takuma Bando and Tatsushi Nishi and Md Moktadir Alam and Ziang Liu and Tomofumi Fujiwara},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982201},
  pages     = {11767-11772},
  title     = {Automatic generation of optimization model using process mining and petri nets for optimal motion planning of 6-DOF manipulators},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time predictive kinematics control of redundancy: A
benchmark of optimal control approaches. <em>IROS</em>, 11759–11766. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern collaborative manipulators operate in unknown environments and share the work space with human coworkers. To ensure flexibility, their kinematic design is redundant which increases the solution space of the inverse kinematics (IK). We propose a real-time capable Predictive Kinematics Controller (PKC) that tracks task space trajectories as a first priority and computes optimal joint trajectories w.r.t. secondary objectives based on model predictive control (MPC). Therefor, the PKC solves a MPC problem in the nullspace of the task space trajectory. We benchmark a direct shooting, a direct collocation and an indirect gradient method in simulation and we identify the direct shooting method as the most efficient. We demonstrate the superior performance of the PKC compared to state-of-the-art local redundancy resolution approaches. In experiments, we show the real-time capability of our implementation.},
  archive   = {C_IROS},
  author    = {Jonas Wittmann and Arian Kist and Daniel J. Rixen},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981675},
  pages     = {11759-11766},
  title     = {Real-time predictive kinematics control of redundancy: A benchmark of optimal control approaches},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Planning under periodic observations: Bounds and
bounding-based solutions. <em>IROS</em>, 11751–11758. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study planning problems faced by robots operating in uncertain environments with incomplete knowledge of state, and actions that are noisy and/or imprecise. This paper identifies a new problem sub-class that models settings in which information is revealed only intermittently through some exogenous process that provides state information periodically. Several practical domains fit this model, including the specific scenario that motivates our research: autonomous navigation of a planetary exploration rover augmented by remote imaging. With an eye to efficient specialized solution methods, we examine the structure of instances of this sub-class. They lead to Markov Decision Processes with exponentially large action-spaces but for which, as those actions comprise sequences of more atomic elements, one may establish performance bounds by comparing policies under different information assumptions. This provides a way in which to construct performance bounds systematically. Such bounds are useful because, in conjunction with the insights they confer, they can be employed in bounding-based methods to obtain high-quality solutions efficiently; the empirical results we present demonstrate their effectiveness for the considered problems. The foregoing has also alluded to the distinctive role that time plays for these problems -more specifically: time until information is revealed- and we uncover and discuss several interesting subtleties in this regard.},
  archive   = {C_IROS},
  author    = {Federico Rossi and Dylan A. Shell},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982056},
  pages     = {11751-11758},
  title     = {Planning under periodic observations: Bounds and bounding-based solutions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reference acceleration model predictive control (RA-MPC) for
cable-driven robots. <em>IROS</em>, 11735–11742. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, a computationally efficient model predictive control (MPC) is proposed for the trajectory tracking of cable-driven robots subject to state/input constraints. While MPC has been an effective tool in dealing with various constraints, the primary drawback is the high computational load caused by the non-convexity of the corresponding optimization problem. In order to avoid the non-convexity, the prediction model in the proposed reference acceleration MPC (RA-MPC) is simplified into a linear one by assuming the reference accelerations being taken in the future horizon steps. As a result, RA-MPC only optimizes for the instantaneous joint accelerations and the corresponding actuator commands for the current step, resulting in a convex quadratic program that can be efficiently solved. It is further shown that by properly selecting parameters, RA-MPC can be interpreted as ‘soft-CTC’ and ‘soft-LQR’, where the joint acceleration is allowed to deviate from the corresponding desired value, computed from a PD gain or an LQR gain. The effectiveness of the proposed RA-MPC are demonstrated in both simulation and hardware experiment using cable-driven robots.},
  archive   = {C_IROS},
  author    = {Chen Song and Darwin Lau},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982081},
  pages     = {11735-11742},
  title     = {Reference acceleration model predictive control (RA-MPC) for cable-driven robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DC-MRTA: Decentralized multi-robot task allocation and
navigation in complex environments. <em>IROS</em>, 11711–11718. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel reinforcement learning (RL) based task allocation and decentralized navigation algorithm for mobile robots in warehouse environments. Our approach is designed for scenarios in which multiple robots are used to perform various pick up and delivery tasks. We consider the problem of joint decentralized task allocation and navigation and present a two level approach to solve it. At the higher level, we solve the task allocation by formulating it in terms of Markov Decision Processes and choosing the appropriate rewards to minimize the Total Travel Delay (TTD). At the lower level, we use a decentralized navigation scheme based on ORCA that enables each robot to perform these tasks in an independent manner, and avoid collisions with other robots and dynamic obstacles. We combine these lower and upper levels by defining rewards for the higher level as the feedback from the lower level navigation algorithm. We perform extensive evaluation in complex warehouse layouts with large number of agents and highlight the benefits over state-of-the-art algorithms based on myopic pickup distance minimization and regret-based task selection. We observe improvement up to 14\% in terms of task completion time and up-to 40\% improvement in terms of computing collision-free trajectories for the robots.},
  archive   = {C_IROS},
  author    = {Aakriti Agrawal and Senthil Hariharan and Amrit Singh Bedi and Dinesh Manocha},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981353},
  pages     = {11711-11718},
  title     = {DC-MRTA: Decentralized multi-robot task allocation and navigation in complex environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A polynomial time approximation scheme for the scheduling
problem in the AGV system. <em>IROS</em>, 11695–11702. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Logistics warehouses face the challenge of fulfilling large bulk pick orders limit in a given time, as the information of logistics orders is different and timeliness. Therefore, in automated warehouses, it is imperative to improve the efficiency and intelligence of order picking by robotic systems. However, the existing automated guided vehicle (AGV) system has only a few fixed functions (such as order sorting and transportation, etc.), which cannot be changed in time according to actual needs. Meanwhile, the scheduling algorithm has only mass heuristics results and a few approximate algorithm results. In this paper, we build a new AGV system, including two kinds of shelves and four kinds of stations, where the system can add new station types according to the actual situation. We establish the equivalent relationship between the order group picking task in this system and the multi-stage hybrid flow shop scheduling problem, without considering the order group transfer process between stations. Furthermore, we propose a polynomial time approximation scheme (PTAS) for the scheduling problem in this system which has been proved to be strongly NP-hard [13].},
  archive   = {C_IROS},
  author    = {Xinrui Li and Chaoyang Wang and Hao Hu and Yanxue Liang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981205},
  pages     = {11695-11702},
  title     = {A polynomial time approximation scheme for the scheduling problem in the AGV system},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep reinforcement learning based robot navigation in
dynamic environments using occupancy values of motion primitives.
<em>IROS</em>, 11687–11694. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a Deep Reinforcement Learning based navigation approach in which we define the occu-pancy observations as heuristic evaluations of motion primitives, rather than using raw sensor data. Our method enables fast mapping of the occupancy data, generated by multi-sensor fusion, into trajectory values in 3D workspace. The computationally efficient trajectory evaluation allows dense sampling of the action space. We utilize our occupancy observations in different data structures to analyze their effects on both training process and navigation performance. We train and test our methodology on two different robots within challenging physics-based simulation environments including static and dy-namic obstacles. We benchmark our occupancy representations with other conventional data structures from state-of-the-art methods. The trained navigation policies are also validated successfully with physical robots in dynamic environments. The results show that our method not only decreases the required training time but also improves the navigation performance as compared to other occupancy representations. The open-source implementation of our work and all related info are available at https://github.com/RIVeR-Lab/tentabot.},
  archive   = {C_IROS},
  author    = {Neşet Ünver Akmandor and Hongyu Li and Gary Lvov and Eric Dusel and Taşkin Padir},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982133},
  pages     = {11687-11694},
  title     = {Deep reinforcement learning based robot navigation in dynamic environments using occupancy values of motion primitives},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning coordination policies over heterogeneous graphs for
human-robot teams via recurrent neural schedule propagation.
<em>IROS</em>, 11679–11686. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As human-robot collaboration increases in the workforce, it becomes essential for human-robot teams to coordinate efficiently and intuitively. Traditional approaches for human-robot scheduling either utilize exact methods that are intractable for large-scale problems and struggle to account for stochastic, time varying human task performance, or application-specific heuristics that require expert domain knowledge to develop. We propose a deep learning-based framework, called HybridNet, combining a heterogeneous graph-based encoder with a recurrent schedule propagator for scheduling stochastic human-robot teams under upper- and lower-bound temporal constraints. The HybridNet&#39;s encoder leverages Heterogeneous Graph Attention Networks to model the initial environment and team dynamics while accounting for the constraints. By formulating task scheduling as a sequential decision-making process, the HybridNet&#39;s recurrent neural schedule propagator leverages Long Short-Term Memory (LSTM) models to propagate forward consequences of actions to carry out fast schedule generation, removing the need to interact with the environment between every taskagent pair selection. The resulting scheduling policy network provides a computationally lightweight yet highly expressive model that is end-to-end trainable via Reinforcement Learning algorithms. We develop a virtual task scheduling environment for mixed human-robot teams in a multi-round setting, capable of modeling the stochastic learning behaviors of human workers. Experimental results showed that HybridNet outperformed other human-robot scheduling solutions across problem sizes for both deterministic and stochastic human performance, with faster runtime compared to pure-GNN-based schedulers.},
  archive   = {C_IROS},
  author    = {Batuhan Altundas and Zheyuan Wang and Joshua Bishop and Matthew Gombolay},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981748},
  pages     = {11679-11686},
  title     = {Learning coordination policies over heterogeneous graphs for human-robot teams via recurrent neural schedule propagation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive online sampling of periodic processes with
application to coral reef acoustic abundance monitoring. <em>IROS</em>,
11671–11678. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present an approach that enables long-term monitoring of biological activity on coral reefs by extending mission time and adaptively focusing sensing resources on high-value periods. Coral reefs are one of the most biodiverse ecosystems on the planet; yet they are also among the most imperiled: facing bleaching, ecological community collapses due to global climate change, and degradation from human activities. Our proposed method improves the ability of scientists to monitor biological activity and abundance using passive acoustic sensors. We accomplish this by extracting periodicities from the observed abundance, and using them to predict future abundance. This predictive model is then used with a Monte Carlo Tree Search planning algorithm to schedule sampling at periods of high biological activity, and power down the sensor during periods of low activity. In simulated experiments using long-term acoustic datasets collected in the US Virgin Islands, our adaptive Online Sensor Scheduling algorithm is able to double the lifetime of a sensor while simultaneously increasing the average observed acoustic activity by 21\%.},
  archive   = {C_IROS},
  author    = {Seth McCammon and Nadège Aoki and T. Aran Mooney and Yogesh Girdhar},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982217},
  pages     = {11671-11678},
  title     = {Adaptive online sampling of periodic processes with application to coral reef acoustic abundance monitoring},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Selecting the partial state abstractions of MDPs: A
metareasoning approach with deep reinforcement learning. <em>IROS</em>,
11665–11670. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Markov decision processes (MDPs) are a common general-purpose model used in robotics for representing sequential decision-making problems. Given the complexity of robotics applications, a popular approach for approximately solving MDPs relies on state aggregation to reduce the size of the state space but at the expense of policy fidelity-offering a trade-off between policy quality and computation time. Naturally, this poses a challenging metareasoning problem: how can an autonomous system dynamically select different state abstractions that optimize this trade-off as it operates online? In this paper, we formalize this metareasoning problem with a notion of time-dependent utility and solve it using deep reinforcement learning. To do this, we develop several general, cheap heuristics that summarize the reward structure and transition topology of the MDP at hand to serve as effective features. Empirically, we demonstrate that our metareasoning approach outperforms several baseline approaches and a strong heuristic approach on a standard benchmark domain.},
  archive   = {C_IROS},
  author    = {Samer B. Nashed and Justin Svegliato and Abhinav Bhatia and Stuart Russell and Shlomo Zilberstein},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981612},
  pages     = {11665-11670},
  title     = {Selecting the partial state abstractions of MDPs: A metareasoning approach with deep reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Planning with intermittent state observability: Knowing when
to act blind. <em>IROS</em>, 11657–11664. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Contemporary planning models and methods often rely on constant availability of free state information at each step of execution. However, autonomous systems are increasingly deployed in the open world where state information may be costly or simply unavailable in certain situations. Failing to account for sensor limitations may lead to costly behavior or even catastrophic failure. While the partially observable Markov decision process (POMDP) can be used to model this problem, solving POMDPs is often intractable. We introduce a planning model called a semi-observable Markov decision process (SOMDP) specifically designed for MDPs where state observability may be intermittent. We propose an approach for solving SOMDPs that uses memory states to proactively plan for the potential loss of sensor information while exploiting the unique structure of SOMDPs. Our theoretical analysis and empirical evaluation demonstrate the advantages of SOMDPs relative to existing planning models.},
  archive   = {C_IROS},
  author    = {Connor Basich and John Peterson and Shlomo Zilberstein},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981883},
  pages     = {11657-11664},
  title     = {Planning with intermittent state observability: Knowing when to act blind},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Electroadhesive clutches for programmable shape morphing of
soft actuators. <em>IROS</em>, 11594–11599. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft robotic actuators are safe and adaptable devices with inherent compliance, which makes them attractive for manipulating delicate and complex objects. Researchers have integrated stiff materials into soft actuators to increase their force capacity and direct their deformation. However, these embedded materials have largely been pre-prescribed and static, which constrains the actuators to a predetermined range of motion. In this work, electroadhesive (EA) clutches integrated on a single-chamber soft pneumatic actuator (SPA) provide local programmable stiffness modulation to control the actuator deformation. We show that activating different clutch patterns inflates a silicone membrane into pyramidal, round, and plateau shapes. Curvatures from these shapes are combined during actuation to apply forces on both a 3.7 g and 820 g object along five different degrees of freedom (DoF). The actuator workspace is up to 12 mm for light objects. Clutch deactivation, which results in local elastomeric expansion, rapidly applies forces up to 3.2 N to an object resting on the surface and launches a 3.7 g object in controlled directions. The actuator also rotates a heavier, 820 g, object by 5 degrees and rapidly restores it to horizontal alignment after clutch deactivation. This actuator is fully powered by a 5 V battery, AA battery, DC-DC transformer, and 4.5 V (63 g) DC air pump. These results demonstrate a first step towards realizing a soft actuator with high DoF shape change that preserves the inherent benefits of pneumatic actuation while gaining the electrical controllability and strength of EA clutches. We envision such a system supplying human contact forces in the form of a low-profile sit-to-stand assistance device, bed-ridden patient manipulator, or other ergonomic mechanism.},
  archive   = {C_IROS},
  author    = {Gregory M. Campbell and Jessica Yin and Yuyang Song and Umesh Gandhi and Mark Yim and James Pikul},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982131},
  pages     = {11594-11599},
  title     = {Electroadhesive clutches for programmable shape morphing of soft actuators},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mechanically programmable jamming based on articulated mesh
structures for variable stiffness robots. <em>IROS</em>, 11586–11593.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft robots are capable of effortlessly adapting to their environment using elastic materials that impart structural compliance into their designs, allowing them to execute complex tasks with minimal sensing and control. However, soft robots cannot exert high forces and can only handle low deformation forces. These characteristics typically limit their applicabil-ity to tasks that require delicate interactions. In this work, we present a mechanically programmable, variable stiffness, jamming actuator based on an articulated mesh structure. The proposed actuator can elastically bend when it is not activated but compresses to attain a pre-programmed shape that is determined by the mesh geometry of the multi-layer jamming architecture when pressure is applied to the silicone pouch containing it. Unlike traditional jamming structures the utilisation of the articulated mesh structure facilitates elastic deformations past the yield point when jammed. The actuator can become &gt;27 times stiffer than its relaxed configuration when exposed to only 90 kPa pressure. We demonstrate the efficiency of this actuator by developing variable stiffness joints that can be used to create: i) underactuated, tendon driven robotic grippers and soft, disposable robotic grippers that exhibit increased dexterity and ii) wearable, affordable, lightweight elbow exoskeleton systems that can assist humans in holding heavy objects with minimal effort.},
  archive   = {C_IROS},
  author    = {Geng Gao and Junbang Liang and Minas Liarokapis},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981272},
  pages     = {11586-11593},
  title     = {Mechanically programmable jamming based on articulated mesh structures for variable stiffness robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Decentralized multi-robot velocity estimation for UAVs
enhancing onboard camera-based velocity measurements. <em>IROS</em>,
11570–11577. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Within the field of multi-robot systems, developing systems that rely only on onboard sensing without the use of external infrastructure (e.g. GNSS) has many potential applications. However, relying only on visual-based modalities for localization presents challenges in terms of accuracy and reliability. We introduce a decentralized multi-robot lateral velocity estimation method for Unmanned Aerial Vehicles (UAVs) to improve onboard measurements in case GNSS infrastructure is not available. This method relies on sharing the onboard measurements of neighbors, as well as the estimation of the relative motion of a focal UAV within the swarm, based on observation of coworking robots. The proposed velocity estimation method does not rely on centralized communication to achieve high reliability and scalability within the swarm system. The performance of the state estimation approach has been verified in simulations and real-world experiments. The results have shown that a swarm of UAVs using the proposed velocity estimator can stabilize individual robots when their primary onboard localization source is not reliable enough.},
  archive   = {C_IROS},
  author    = {Jiri Horyna and Vit Kratky and Eliseo Ferrante and Martin Saska},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981894},
  pages     = {11570-11577},
  title     = {Decentralized multi-robot velocity estimation for UAVs enhancing onboard camera-based velocity measurements},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Impressionist algorithms for autonomous multi-robot systems:
Flocking as a case study. <em>IROS</em>, 11562–11569. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot swarms have the potential to revolutionize areas ranging from warehouse management and agriculture to underwater and space exploration. However, there remains a substantial gap between theory and robot implementation. While algorithms might assume reliable communication, perfect sensing, and instantaneous cognition, most robots have lossy or even no communication, imperfect sensing, and limited cognition speed. In our previous work on implicit vision-based coordination, we demonstrated autonomous three-dimensional behaviors underwater by removing the need for radio communication between robots. Here we explore impressionist algorithms, capable of working with even more minimal information where traditional algorithms are prone to fail. Our case study focuses on classic flocking behaviors, where a robot swarm must coordinate group motion. We demonstrate that reliable alignment, dispersion, and milling can be achieved with only infrequent and imperfect sensory impressions. In simulation studies and theoretical analyses, we investigate the effect of systematically reducing spatial and temporal fidelity of individual information on the success metrics for the group; we also demonstrate physical experiments with Blueswarm robots using simple color detection. Our results show the potential of impressionist algorithms that operate on simpler neighborhood-awareness metrics and still achieve desired global goals.},
  archive   = {C_IROS},
  author    = {Florian Berlinger and Julia T. Ebert and Radhika Nagpal},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981448},
  pages     = {11562-11569},
  title     = {Impressionist algorithms for autonomous multi-robot systems: Flocking as a case study},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interactive multi-robot aerial cinematography through
hemispherical manifold coverage. <em>IROS</em>, 11528–11534. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a distributed interactive framework to provide high-level position instructions for multi-robot aerial cinematography based on coverage over a hemisphere. The control strategy based on optimization of the coverage functional and geometric relationships over a hemisphere is presented. It enables multiple Unmanned Aerial Vehicles (UAVs) to coordinate their motion while tracking a dynamic (real or virtual) target, and can accommodate high-level human inputs to influence UAV concentration. In this framework, each UAV uses local information combined with exogenous inputs to determine its motion. The two inputs to the system, i.e., the predicted trajectory of the target and user-defined aesthetic preferences, are agnostic to the size of the multi-robot system (MRS). The proposed framework is validated using the PX4 SITL Autopilot simulator in Gazebo, and the scalability of the framework is verified via simulations.},
  archive   = {C_IROS},
  author    = {Xiaotian Xu and Guangyao Shi and Pratap Tokekar and Yancy Diaz-Mercado},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981181},
  pages     = {11528-11534},
  title     = {Interactive multi-robot aerial cinematography through hemispherical manifold coverage},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hybrid PSO algorithm for multi-robot target search and
decision awareness. <em>IROS</em>, 11520–11527. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Groups of robots can be tasked with identifying a location in an environment where a feature cue is past a threshold, then disseminating this information throughout the group – such as identifying a high-enough elevation location to place a communications tower. This is a continuous-cue target search, where multi-robot search algorithms like particle swarm optimization (PSO) can improve search time through parallelization. However, many robots lack global communication in large spaces, and PSO-based algorithms often fail to consider how robots disseminate target knowledge after a single robot locates it. We present a two-stage hybrid algorithm to solve this task: (1) locating a target with a variation of PSO, and (2) moving to maximize target knowledge across the group. We conducted parameter sweep simulations of up to 32 robots in a grid-based grayscale environment. Pre-decision, we find that PSO with a variable velocity update interval improves target localization. In the post-decision phase, we show that dispersion is the fastest strategy to communicate with all other robots. Our algorithm is also competitive with a coverage sweep benchmark, while requiring significantly less inter-individual coordination.},
  archive   = {C_IROS},
  author    = {Julia T. Ebert and Florian Berlinger and Bahar Haghighat and Radhika Nagpal},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982022},
  pages     = {11520-11527},
  title     = {A hybrid PSO algorithm for multi-robot target search and decision awareness},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FedDrive: Generalizing federated learning to semantic
segmentation in autonomous driving. <em>IROS</em>, 11504–11511. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Semantic Segmentation is essential to make self-driving vehicles autonomous, enabling them to understand their surroundings by assigning individual pixels to known categories. However, it operates on sensible data collected from the users&#39; cars; thus, protecting the clients&#39; privacy becomes a primary concern. For similar reasons, Federated Learning has been recently introduced as a new machine learning paradigm aiming to learn a global model while preserving privacy and leveraging data on millions of remote devices. Despite several efforts on this topic, no work has explicitly addressed the challenges of federated learning in semantic segmentation for driving so far. To fill this gap, we propose FedDrive, a new benchmark consisting of three settings and two datasets, incorporating the real-world challenges of statistical heterogeneity and domain generalization. We benchmark state-of-the-art algorithms from the federated learning literature through an in-depth analysis, combining them with style transfer methods to improve their generalization ability. We demonstrate that correctly handling normalization statistics is crucial to deal with the aforementioned challenges. Furthermore, style transfer improves performance when dealing with significant appearance shifts. Official website: https://feddrive.github.io.},
  archive   = {C_IROS},
  author    = {Lidia Fantauzzo and Eros Fanì and Debora Caldarola and Antonio Tavera and Fabio Cermelli and Marco Ciccone and Barbara Caputo},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981098},
  pages     = {11504-11511},
  title     = {FedDrive: Generalizing federated learning to semantic segmentation in autonomous driving},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient spatial-temporal information fusion for
LiDAR-based 3D moving object segmentation. <em>IROS</em>, 11456–11463.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate moving object segmentation is an es-sential task for autonomous driving. It can provide effective information for many downstream tasks, such as collision avoidance, path planning, and static map construction. How to effectively exploit the spatial-temporal information is a critical question for 3D LiDAR moving object segmentation (LiDAR-MOS). In this work, we propose a novel deep neural network exploiting both spatial-temporal information and different representation modalities of LiDAR scans to improve LiDAR-MOS performance. Specifically, we first use a range image-based dual-branch structure to separately deal with spatial and temporal information that can be obtained from sequential LiDAR scans, and later combine them using motion-guided attention modules. We also use a point refinement module via 3D sparse convolution to fuse the information from both LiDAR range image and point cloud representations and reduce the artifacts on the borders of the objects. We verify the effectiveness of our proposed approach on the LiDAR-MOS benchmark of SemanticKITTI. Our method outperforms the state-of-the-art methods significantly in terms of LiDAR-MOS IoU. Benefiting from the devised coarse-to-fine architecture, our method operates online at sensor frame rate. Code is available at: https://github.com/haomo-ai/MotionSeg3D.},
  archive   = {C_IROS},
  author    = {Jiadai Sun and Yuchao Dai and Xianjing Zhang and Jintao Xu and Rui Ai and Weihao Gu and Xieyuanli Chen},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981210},
  pages     = {11456-11463},
  title     = {Efficient spatial-temporal information fusion for LiDAR-based 3D moving object segmentation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hybrid belief pruning with guarantees for
viewpoint-dependent semantic SLAM. <em>IROS</em>, 11440–11447. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Semantic simultaneous localization and mapping is a subject of increasing interest in robotics and AI that directly influences the autonomous vehicles industry, the army industries, and more. One of the challenges in this field is to obtain object classification jointly with robot trajectory estimation. Considering view-dependent semantic measurements, there is a coupling between different classes, resulting in a combinatorial number of hypotheses. A common solution is to prune hypotheses that have a sufficiently low probability and to retain only a limited number of hypotheses. However, after pruning and renormalization, the updated probability is overconfident with respect to the original probability. This is especially problematic for systems that require high accuracy. If the prior probability of the classes is independent, the original normalization factor can be computed efficiently without pruning hypotheses. To the best of our knowledge, this is the first work to present these results. If the prior probability of the classes is dependent, we propose a lower bound on the normalization factor that ensures cautious results. The bound is calculated incrementally and with similar efficiency as in the independent case. After pruning and updating based on the bound, this belief is shown empirically to be close to the original belief.},
  archive   = {C_IROS},
  author    = {Tuvy Lemberg and Vadim Indelman},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981534},
  pages     = {11440-11447},
  title     = {Hybrid belief pruning with guarantees for viewpoint-dependent semantic SLAM},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TIP: Task-informed motion prediction for intelligent
vehicles. <em>IROS</em>, 11432–11439. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When predicting trajectories of road agents, motion predictors often approximate the future distribution by a limited number of samples. This constraint requires the predictors to generate samples that best support the task given task specifications. However, existing predictors are often optimized and evaluated via task-agnostic measures without accounting for the use of predictions in downstream tasks, and thus could result in sub-optimal task performance. In this paper, we propose a task-informed motion prediction model that better supports the tasks through its predictions by jointly reasoning about prediction accuracy and the utility of the downstream tasks during training. The task utility function is commonly used to evaluate task performance. It does not require the full task information, but rather a specification of the utility of the task, resulting in predictors that are tailored to different downstream tasks. We demonstrate our approach on two use cases of common decision making tasks and their utility functions, in the context of autonomous driving and parallel autonomy. Experiment results show that our predictor produces accurate predictions that improve the task performance by a large margin in both tasks when compared to task-agnostic baselines on the Waymo Open Motion dataset.},
  archive   = {C_IROS},
  author    = {Xin Huang and Guy Rosman and Ashkan Jasour and Stephen G. McGill and John J. Leonard and Brian C. Williams},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982100},
  pages     = {11432-11439},
  title     = {TIP: Task-informed motion prediction for intelligent vehicles},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Driving anomaly detection using contrastive multiview coding
to interpret cause of anomaly. <em>IROS</em>, 11424–11431. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern advanced driver assistant systems (ADAS) rely on various types of sensors to monitor the vehicle status, driver&#39;s behaviors and road condition. The multimodal systems in the vehicle include sensors, such as accelerometers, pressure sensors, cameras, lidar and radars. When looking at a given scene with multiple modalities, there should be congruent in-formation among different modalities. Exploring the congruent information across modalities can lead to appealing solutions to create robust multimodal representations. This work proposes an unsupervised approach based on contrastive multiview coding (CMC) to capture the correlations in representations extracted from different modalities, learning a more discriminative rep-resentation space for unsupervised anomaly driving detection. We use CMC to train our model to extract view-invariant factors by maximizing the mutual information between mul-tiple representations from a given view, and increasing the distance of views from unrelated segments. We consider the vehicle driving data, driver&#39;s physiological data, and external environment data consisting of distances to nearby pedestrians, bicycles, and vehicles. The experimental results on the driving anomaly dataset (DAD) indicate that the CMC representation is effective for driving anomaly detection. The approach is efficient, scalable and interpretable, where the distances in the contrastive embedding for each view can be used to understand potential causes of the detected anomalies.},
  archive   = {C_IROS},
  author    = {Yuning Qiu and Teruhisa Misu and Carlos Busso},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981815},
  pages     = {11424-11431},
  title     = {Driving anomaly detection using contrastive multiview coding to interpret cause of anomaly},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). InterSim: Interactive traffic simulation via explicit
relation modeling. <em>IROS</em>, 11416–11423. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Interactive traffic simulation is crucial to autonomous driving systems by enabling testing for planners in a more scalable and safe way compared to real-world road testing. Existing approaches learn an agent model from large-scale driving data to simulate realistic traffic scenarios, yet it remains an open question to produce consistent and diverse multi-agent interactive behaviors in crowded scenes. In this work, we present InterSim, an interactive traffic simulator for testing autonomous driving planners. Given a test plan trajectory from the ego agent, InterSim reasons about the interaction relations between the agents in the scene and generates realistic trajectories for each environment agent that are consistent with the relations. We train and validate our model on a large-scale interactive driving dataset. Experiment results show that InterSim achieves better simulation realism and reactivity in two simulation tasks compared to a state-of-the-art learning-based traffic simulator.},
  archive   = {C_IROS},
  author    = {Qiao Sun and Xin Huang and Brian C. Williams and Hang Zhao},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982008},
  pages     = {11416-11423},
  title     = {InterSim: Interactive traffic simulation via explicit relation modeling},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interventional behavior prediction: Avoiding overly
confident anticipation in interactive prediction. <em>IROS</em>,
11409–11415. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Conditional behavior prediction (CBP) builds up the foundation for a coherent interactive prediction and plan-ning framework that can enable more efficient and less conser-vative maneuvers in interactive scenarios. In CBP task, we train a prediction model approximating the posterior distribution of target agents&#39; future trajectories conditioned on the future trajectory of an assigned ego agent. However, we argue that CBP may provide overly confident anticipation on how the autonomous agent may influence the target agents&#39; behavior. Consequently, it is risky for the planner to query a CBP model. Instead, we should treat the planned trajectory as an intervention and let the model learn the trajectory distribution under intervention. We refer to it as the interventional behavior prediction (IBP) task. Moreover, to properly evaluate an IBP model with offline datasets, we propose a Shapley-value-based metric to verify if the prediction model satisfies the inherent temporal independence of an interventional distribution. We show that the proposed metric can effectively identify a CBP model violating the temporal independence, which plays an important role when establishing IBP benchmarks.},
  archive   = {C_IROS},
  author    = {Chen Tang and Wei Zhan and Masayoshi Tomizuka},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981524},
  pages     = {11409-11415},
  title     = {Interventional behavior prediction: Avoiding overly confident anticipation in interactive prediction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Development of a research testbed for cooperative driving in
mixed traffic of human-driven and autonomous vehicles. <em>IROS</em>,
11403–11408. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a cooperative driving testbed based on vehicle-to-vehicle (V2V) communication, which can be used for research in intelligent transportation systems, such as collision avoidance in mixed traffic of both human-driven vehicles and autonomous vehicles. To achieve the goal, an intelligent copilot is developed. The copilot can share the data regarding vehicle status, intention, etc, with other nearby vehicles through V2V communication. Several case studies are conducted to validate the proposed testbed and evaluate the performances of cooperative driving. When dangerous situations occur, the copilot solves the collision avoidance problem using Mixed Integer Programming (MIP), which either provides control commands to the autonomous vehicle, or advises the human driver to take action. Experimental results show that the safety and stability of the involved vehicles have been significantly enhanced. This cooperative driving testbed can be used by researchers to develop and test cooperative driving algorithms before they are deployed in real vehicles.},
  archive   = {C_IROS},
  author    = {Jiaxing Lu and Ryan Stracener and Weihua Sheng and He Bai and Sanzida Hossain},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981436},
  pages     = {11403-11408},
  title     = {Development of a research testbed for cooperative driving in mixed traffic of human-driven and autonomous vehicles},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep kernel learning for uncertainty estimation in multiple
trajectory prediction networks. <em>IROS</em>, 11396–11402. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Predicting future paths of vehicles or pedestrians is an essential task for automated vehicles to allow for planning the own trajectory. Using predicted paths, a planning algorithm can, e.g., react to anticipated manoeuvres of other traffic participants. For calculating risks of planned manoeuvres, it is essential that the predicted paths are generated with information about their uncertainty. Since today&#39;s state of the art trajectory prediction algorithms are based on deep neural networks (DNNs), the estimation of uncertainty is left to the neural networks as well, which usually provide no means of assessing how the uncertainty estimation works. In this paper, we present a combination of DNNs with Gaussian processes via Deep Kernel Learning (DKL), which combines the ability of DNNs to perform the prediction task with the advantage of Gaussian processes of having more interpretable probabilistic outputs. We propose and evaluate two different variants for the task of multimodal trajectory prediction using Stochastic Variational Gaussian Processes (SVGPs) and the recently proposed regression method Deep Sigma Point Processes (DSPPs), respectively. We evaluate the predictive distributions of both approaches on the publicly available Argoverse Motion Forecasting dataset and compare them to other, purely neural network based methods for uncertainty estimation.},
  archive   = {C_IROS},
  author    = {Jan Strohbeck and Johannes Müller and Martin Herrmann and Michael Buchholz},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982167},
  pages     = {11396-11402},
  title     = {Deep kernel learning for uncertainty estimation in multiple trajectory prediction networks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A value-based dynamic learning approach for vehicle dispatch
in ride-sharing. <em>IROS</em>, 11388–11395. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To ensure real-time response to passengers, existing solutions to the vehicle dispatch problem typically optimize dispatch policies using small batch windows and ignore the spatial-temporal dynamics over the long-term horizon. In this paper, we focus on improving the long-term performance of ride-sharing services and propose a deep reinforcement learning based approach for the ride-sharing dispatch problem. In particular, this work includes: (1) an offline policy evaluation (OPE) based method to learn a value function that indicates the expected reward of a vehicle reaching a particular state; (2) an online learning procedure to update the offline trained value function to capture the real-time dynamics during the operation; (3) an efficient online dispatch method that optimizes the matching policy by considering both past and future influences. Extensive simulations are conducted based on New York City taxi data, and show that the proposed solution further increases the service rate compared to the state-of-the-art farsighted ride-sharing dispatch approach.},
  archive   = {C_IROS},
  author    = {Cheng Li and David Parker and Qi Hao},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981216},
  pages     = {11388-11395},
  title     = {A value-based dynamic learning approach for vehicle dispatch in ride-sharing},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IMU dead-reckoning localization with RNN-IEKF algorithm.
<em>IROS</em>, 11382–11387. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In complex urban environments, the Inertial Navigation System (INS) is important for navigating unmanned ground vehicles (UAVs) for its environment-independency and reliability of real-time localization. It is usually employed as the baseline in the case of other sensors failures, such as the GPS, Lidar, or Cameras. However, one problem for the INS is that its estimation error of localization accumulates over time, and thus the estimated trajectories of the UAVs continue to drift away from their ground truths. To solve this problem, this paper proposes an improved algorithm based on the Invariant Extended Kalman Filter (IEKF) for dead-reckoning of autonomous vehicles, which dynamically adjusts the process noise and the observation noise covariance matrixes through Attention mechanism and Recurrent Neural Network (RNN). The algorithm achieves more robust and accurate dead-reckoning localization in the experiments conducted on the KITTI dataset, reducing the translational error by about 45\%compared to the baseline.},
  archive   = {C_IROS},
  author    = {Hang Zhou and Yibo Zhao and Xiaogang Xiong and Yunjiang Lou and Shyam Kamal},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982087},
  pages     = {11382-11387},
  title     = {IMU dead-reckoning localization with RNN-IEKF algorithm},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Trajectory prediction with graph-based dual-scale context
fusion. <em>IROS</em>, 11374–11381. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion prediction for traffic participants is essential for a safe and robust automated driving system, especially in cluttered urban environments. However, it is highly challenging due to the complex road topology as well as the uncertain intentions of the other agents. In this paper, we present a graph-based trajectory prediction network named the Dual Scale Predictor (DSP), which encodes both the static and dynamical driving context in a hierarchical manner. Different from methods based on a rasterized map or sparse lane graph, we consider the driving context as a graph with two layers, focusing on both geometrical and topological features. Graph neural networks (GNNs) are applied to extract features with different levels of granularity, and features are subsequently aggregated with attention-based inter-layer networks, realizing better local-global feature fusion. Following the recent goal-driven trajectory prediction pipeline, goal candidates with high likelihood for the target agent are extracted, and predicted trajectories are generated conditioned on these goals. Thanks to the proposed dual-scale context fusion network, our DSP is able to generate accurate and human-like multi-modal trajectories. We evaluate the proposed method on the large-scale Argoverse motion forecasting benchmark, and it achieves promising results, outperforming the recent state-of-the-art methods. We release the code on our project website. 1 1 https://github.com/HKUST-Aerial-Robotics/DSP},
  archive   = {C_IROS},
  author    = {Lu Zhang and Peiliang Li and Jing Chen and Shaojie Shen},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981923},
  pages     = {11374-11381},
  title     = {Trajectory prediction with graph-based dual-scale context fusion},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An optimal motion planning framework for quadruped jumping.
<em>IROS</em>, 11366–11373. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an optimal motion planning framework to generate versatile energy-optimal quadrupedal jumping motions automatically (e.g., flips, spin). The jumping motions via the centroidal dynamics are formulated as a 12-dimensional black-box optimization problem subject to the robot kino-dynamic constraints. Gradient-based approaches offer great success in addressing trajectory optimization (TO), yet, prior knowledge (e.g., reference motion, contact schedule) is required and results in sub-optimal solutions. The new proposed framework first employed a heuristics-based optimization method to avoid these problems. Moreover, a prioritization fitness function is created for heuristics-based algorithms in robot ground reaction force (GRF) planning, enhancing convergence and searching performance considerably. Since heuristics-based algorithms often require significant time, motions are planned offline and stored as a pre-motion library. A selector is designed to automatically choose motions with user-specified or perception information as input. The proposed framework has been successfully validated only with a simple continuously tracking PD controller in an open-source Mini-Cheetah by several challenging jumping motions, including jumping over a window-shaped obstacle with 30 cm height and left-flipping over a rectangle obstacle with 27 cm height. (Video*)},
  archive   = {C_IROS},
  author    = {Zhitao Song and Linzhu Yue and Guangli Sun and Yihu Ling and Hongshuo Wei and Linhai Gui and Yun-Hui Liu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981642},
  pages     = {11366-11373},
  title     = {An optimal motion planning framework for quadruped jumping},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discover life skills for planning as bandits via observing
and learning how the world works. <em>IROS</em>, 11360–11365. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel approach for planning agents to compose abstract skills via observing and learning from historical interactions with the world. Our framework operates in a Markov state-space model via a set of actions under unknown pre-conditions. We formulate skills as high-level abstract policies that propose action plans based on the current state. Each policy learns new plans by observing the states&#39; transitions while the agent interacts with the world. Such an approach automatically learns new plans to achieve specific intended effects, but the success of such plans is often dependent on the states in which they are applicable. Therefore, we formulate the evaluation of such plans as infinitely many multi-armed bandit problems, where we balance the allocation of resources on evaluating the success probability of existing arms and exploring new options. The result is a planner capable of automatically learning robust high-level skills under a noisy environment; such skills implicitly learn the action pre-condition without explicit knowledge. We show that this planning approach is experimentally very competitive in high-dimensional state space domains.},
  archive   = {C_IROS},
  author    = {Tin Lai},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981158},
  pages     = {11360-11365},
  title     = {Discover life skills for planning as bandits via observing and learning how the world works},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Watch out! There may be a human. Addressing invisible humans
in social navigation. <em>IROS</em>, 11344–11351. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current approaches in human-aware or social robot navigation address the humans that are visible to the robot. However, it is also important to address the possible emergences of humans to avoid shocks or surprises to humans and erratic behavior of the robot planner. In this paper, we propose a novel approach to detect and address these human emergences called ‘invisible humans’. We determine the places from which a human, currently not visible to the robot, can appear suddenly and then adapt the path and speed of the robot with the anticipation of potential collisions. This is done while still considering and adapting humans present in the robot&#39;s field of view. We also show how this detection can be exploited to identify and address the doorways or narrow passages. Finally, the effectiveness of the proposed methodology is shown through several simulated and real-world experiments.},
  archive   = {C_IROS},
  author    = {Phani Teja Singamaneni and Anthony Favier and Rachid Alami},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982186},
  pages     = {11344-11351},
  title     = {Watch out! there may be a human. addressing invisible humans in social navigation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feedback-efficient active preference learning for socially
aware robot navigation. <em>IROS</em>, 11336–11343. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Socially aware robot navigation, where a robot is required to optimize its trajectory to maintain comfortable and compliant spatial interactions with humans in addition to reaching its goal without collisions, is a fundamental yet challenging task in the context of human-robot interaction. While existing learning-based methods have achieved better performance than the preceding model-based ones, they still have drawbacks: reinforcement learning depends on the handcrafted reward that is unlikely to effectively quantify broad social compliance, and can lead to reward exploitation problems; meanwhile, inverse rein-forcement learning suffers from the need for expensive human demonstrations. In this paper, we propose a feedback-efficient active preference learning approach, FAPL, that distills human comfort and expectation into a reward model to guide the robot agent to explore latent aspects of social compliance. We further introduce hybrid experience learning to improve the efficiency of human feedback and samples, and evaluate benefits of robot behaviors learned from FAPL through extensive simulation experiments and a user study (N=10) employing a physical robot to navigate with human subjects in real-world scenarios. Source code and experiment videos for this work are available at: https://sites.google.com/view/san-fapl.},
  archive   = {C_IROS},
  author    = {Ruiqi Wang and Weizheng Wang and Byung-Cheol Min},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981616},
  pages     = {11336-11343},
  title     = {Feedback-efficient active preference learning for socially aware robot navigation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning a group-aware policy for robot navigation.
<em>IROS</em>, 11328–11335. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human-aware robot navigation promises a range of applications in which mobile robots bring versatile assistance to people in common human environments. While prior research has mostly focused on modeling pedestrians as independent, intentional individuals, people move in groups; consequently, it is imperative for mobile robots to respect human groups when navigating around people. This paper explores learning group-aware navigation policies based on dynamic group formation using deep reinforcement learning. Through simulation experiments, we show that group-aware policies, compared to baseline policies that neglect human groups, achieve greater robot navigation performance (e.g., fewer collisions), minimize violation of social norms and discomfort, and reduce the robot&#39;s movement impact on pedestrians. Our results contribute to the development of social navigation and the integration of mobile robots into human environments.},
  archive   = {C_IROS},
  author    = {Kapil Katyal and Yuxiang Gao and Jared Markowitz and Sara Pohland and Corban Rivera and I-Jeng Wang and Chien-Ming Huang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981183},
  pages     = {11328-11335},
  title     = {Learning a group-aware policy for robot navigation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multimodal generation of novel action appearances for
synthetic-to-real recognition of activities of daily living.
<em>IROS</em>, 11320–11327. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Domain shifts, such as appearance changes, are a key challenge in real-world applications of activity recognition models, which range from assistive robotics and smart homes to driver observation in intelligent vehicles. For example, while simulations are an excellent way of economical data collection, a Synthetic→Real domain shift leads to &gt; 60\% drop in accuracy when recognizing Activities of Daily Living (ADLs). We tackle this challenge and introduce an activity domain generation framework which creates novel ADL appearances (novel domains) from different existing activity modalities (source domains) inferred from video training data. Our frame-work computes human poses, heatmaps of body joints, and optical flow maps and uses them alongside the original RGB videos to learn the essence of source domains in order to generate completely new ADL domains. The model is optimized by maximizing the distance between the existing source appearances and the generated novel appearances while ensuring that the semantics of an activity is preserved through an additional classification loss. While source data multimodality is an important concept in this design, our setup does not rely on multi-sensor setups, (i.e., all source modalities are inferred from a single video only.) The newly created activity domains are then integrated in the training of the ADL classification networks, resulting in models far less susceptible to changes in data distributions. Extensive experiments on the Synthetic→Real benchmark Sims4Action demonstrate the potential of the domain generation paradigm for cross-domain ADL recognition, setting new state-of-the-art results. Our code is publicly available at https://github.com/Zrrr1997/syn2real_DG.},
  archive   = {C_IROS},
  author    = {Zdravko Marinov and David Schneider and Alina Roitberg and Rainer Stiefelhagen},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981946},
  pages     = {11320-11327},
  title     = {Multimodal generation of novel action appearances for synthetic-to-real recognition of activities of daily living},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Recursive hierarchical projection for whole-body control
with task priority transition. <em>IROS</em>, 11312–11319. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Whole-body control (WBC) with task priority transition is an important technology for robots to switch multiple behaviors, change different objectives, and adapt to various environments. Many methods have solved the problem of control continuity in the priority transition process. However, they either increased the computation consumption or sacrificed the accuracy of tasks in practical application. In this work, we propose a Recursive Hierarchical Projection (RHP) matrix and introduce it in Hierarchical Quadratic Programming (HQP). This RHP-HQP scheme can form continuously changing hierarchical projection and regard the WBC problem with task priority transition as a unified formulation. This unified formulation can be smoothly transitioned without increasing computation consumption and solved without losing task accuracy. The comparative simulations of the reactive collision avoidance verify that this priority transition scheme can guarantee high computational efficiency and task accuracy.},
  archive   = {C_IROS},
  author    = {Gang Han and Jiajun Wang and Xiaozhu Ju and Mingguo Zhao},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981328},
  pages     = {11312-11319},
  title     = {Recursive hierarchical projection for whole-body control with task priority transition},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Whole-body control with motion/force transmissibility for
parallel-legged robot. <em>IROS</em>, 11304–11311. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For achieving kinematically suitable configurations and highly dynamic task execution, an efficient way is to consider robot performance indices in the whole-body control (WBC) of robots. However, current WBC methods have not considered the intrinsic features of parallel robots, especially motion/force transmissibility (MFT). This paper proposes an MFT-enhanced WBC scheme for parallel-legged robots. Introducing the performance indices of MFT into a WBC is challenging due to the nonlinear relationship between MFT indices and the robot configuration. To overcome this challenge, we establish the MFT preferable space of the robot offline and formulate it as a polyhedron in the joint space at the acceleration level. Then, the WBC employs the polyhedron as a soft constraint. As a result, the robot possesses high-speed and high-acceleration capabilities by satisfying this constraint. The offline preprocessing relieves the online computation burden and helps the WBC achieve a 1kHz servo rate. Finally, we validate the performance and robustness of the proposed method via simulations and experiments on a parallel-legged bipedal robot.},
  archive   = {C_IROS},
  author    = {Jiajun Wang and Gang Han and Xiaozhu Ju and Mingguo Zhao},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981199},
  pages     = {11304-11311},
  title     = {Whole-body control with Motion/Force transmissibility for parallel-legged robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On-device CPU scheduling for robot systems. <em>IROS</em>,
11296–11303. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots have to take highly responsive real-time actions, driven by complex decisions involving a pipeline of sensing, perception, planning, and reaction tasks. These tasks must be scheduled on resource-constrained devices such that the performance goals and the requirements of the application are met. This is a difficult problem that requires handling multiple scheduling dimensions, and variations in computational resource usage and availability. In practice, system designers manually tune parameters for their specific hardware and application, which results in poor generalization and increases the development burden. In this work, we highlight the emerging need for scheduling CPU resources at runtime in robot systems. We use robot navigation as a case-study to understand the key scheduling requirements for such systems. Armed with this understanding, we develop a CPU scheduling framework, Catan, that dynamically schedules compute resources across different components of an app so as to meet the specified application requirements. Through experiments with a prototype implemented on ROS, we show the impact of system scheduling on meeting the application&#39;s performance goals, and how Catan dynamically adapts to runtime variations.},
  archive   = {C_IROS},
  author    = {Aditi Partap and Samuel Grayson and Muhammad Huzaifa and Sarita Adve and Brighten Godfrey and Saurabh Gupta and Kris Hauser and Radhika Mittal},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982085},
  pages     = {11296-11303},
  title     = {On-device CPU scheduling for robot systems},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic co-design of aerial robots using a graph grammar.
<em>IROS</em>, 11260–11267. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unmanned aerial vehicles (UAVs) have broad applications including disaster response, transportation, photography, and mapping. A significant bottleneck in the development of UAVs is the limited availability of automatic tools for task-specific co-design of a UAV&#39;s shape and controller. The development of such tools is particularly challenging as UAVs can take many forms, including fixed-wing planes, radial copters, and hybrid topologies, with each class of topology showing different advantages. In this work, we present a computational design pipeline for UAVs based on a graph grammar that can search across a wide range of topologies. Graphs generated by the grammar encode different topologies and component selections, while continuous parameters encode the dimensions and properties of each component. We further augment the shape representation with deformation cages, which allow expressing a variety of wing shapes. Each UAV design is associated with an LQR controller with tunable continuous parameters. To search over this complex discrete and continuous design space, we develop a hybrid algorithm that combines discrete graph search strategies and gradient-based continuous optimization methods using a differentiable UAV simulator. We evaluate our pipeline on a set of simulated flight tasks requiring dynamic motions, showing that it discovers novel UAV designs that outperform canonical UAVs typically made by engineers.},
  archive   = {C_IROS},
  author    = {Allan Zhao and Tao Du and Jie Xu and Josie Hughes and Juan Salazar and Pingchuan Ma and Wei Wang and Daniela Rus and Wojciech Matusik},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982013},
  pages     = {11260-11267},
  title     = {Automatic co-design of aerial robots using a graph grammar},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SROS2: Usable cyber security tools for ROS 2. <em>IROS</em>,
11253–11259. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {ROS 2 is rapidly becoming a standard in the robotics industry. Built upon DDS as its default communication middleware and used in safety-critical scenarios, adding secu-rity to robots and ROS computational graphs is increasingly becoming a concern. The present work introduces SROS2, a series of developer tools and libraries that facilitate adding security to ROS 2 graphs. Focusing on a usability-centric approach in SROS2, we present a methodology for securing graphs systematically while following the DevSecOps model. We also demonstrate the use of our security tools by presenting an application case study that considers securing a graph using the popular Navigation2 and SLAM Toolbox stacks applied in a TurtieBot3 robot. We analyse the current capabilities of SROS2 and discuss the shortcomings, which provides insights for future contributions and extensions. Ultimately, we present SROS2 as usable security tools for ROS 2 and argue that without usability, security in robotics will be greatly impaired.},
  archive   = {C_IROS},
  author    = {Victor Mayoral-Vilches and Ruffin White and Gianluca Caiazza and Mikael Arguedas},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982129},
  pages     = {11253-11259},
  title     = {SROS2: Usable cyber security tools for ROS 2},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SocialGym: A framework for benchmarking social robot
navigation. <em>IROS</em>, 11246–11252. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots moving safely and in a socially compliant manner in dynamic human environments is an essential benchmark for long-term robot autonomy. However, it is not feasible to learn and benchmark social navigation behaviors entirely in the real world, as learning is data-intensive, and it is challenging to make safety guarantees during training. Therefore, simulation-based benchmarks that provide abstractions for social navigation are required. A framework for these benchmarks would need to support a wide variety of learning approaches, be extensible to the broad range of social navigation scenarios, and abstract away the perception problem to focus on social navigation explicitly. While there have been many proposed solutions, including high fidelity 3D simulators and grid world approximations, no existing solution satisfies all of the aforementioned properties for learning and evaluating social navigation behaviors. In this work, we propose SocialGym, a lightweight 2D simulation environment for robot social navigation designed with extensibility in mind, and a benchmark scenario built on SocialGym. Further, we present benchmark results that compare and contrast human-engineered and model-based learning approaches to a suite of off-the-shelf Learning from Demonstration (LfD) and Reinforcement Learning (RL) approaches applied to social robot navigation. These results demonstrate the data efficiency, task performance, social compliance, and environment transfer capabilities for each of the policies evaluated to provide a solid grounding for future social navigation research.},
  archive   = {C_IROS},
  author    = {Jarrett Holtz and Joydeep Biswas},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982021},
  pages     = {11246-11252},
  title     = {SocialGym: A framework for benchmarking social robot navigation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gazebo fluids: SPH-based simulation of fluid interaction
with articulated rigid body dynamics. <em>IROS</em>, 11238–11245. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Physical simulation is an indispensable component of robotics simulation platforms that serves as the basis for a plethora of research directions. Looking strictly at robotics, the common characteristic of the most popular physics engines, such as ODE, DART, MuJoCo, bullet, SimBody, PhysX or RaiSim, is that they focus on the solution of articulated rigid bodies with collisions and contacts problems, while paying less attention to other physical phenomena. This restriction limits the range of addressable simulation problems, rendering applications such as soft robotics, cloth simulation, simulation of viscoelastic materials, and fluid dynamics, especially surface swimming, infeasible. In this work, we present Gazebo Fluids, an open-source extension of the popular Gazebo robotics simulator that enables the interaction of articulated rigid body dynamics with particle-based fluid and deformable solid simulation. We implement fluid dynamics and highly viscous and elastic material simulation capabilities based on the Smoothed Particle Hydrodynamics method. We demonstrate the practical impact of this extension for previously infeasible application scenarios in a series of experiments, showcasing one of the first self-propelled robot swimming simulations with SPH in a robotics simulator.},
  archive   = {C_IROS},
  author    = {Emmanouil Angelidis and Jan Bender and Jonathan Arreguit and Lars Gleim and Wei Wang and Cristian Axenie and Alois Knoll and Auke Ijspeert},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982036},
  pages     = {11238-11245},
  title     = {Gazebo fluids: SPH-based simulation of fluid interaction with articulated rigid body dynamics},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bio-inspired grasping controller for sensorized 2-DoF
grippers. <em>IROS</em>, 11231–11237. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a holistic grasping controller, combining free-space position control and in-contact force-control for reliable grasping given uncertain object pose estimates. Employing tactile fingertip sensors, undesired object displacement during grasping is minimized by pausing the finger closing motion for individual joints on first contact until force-closure is established. While holding an object, the controller is compliant with external forces to avoid high internal object forces and prevent object damage. Gravity as an external force is explicitly considered and compensated for, thus preventing gravity-induced object drift. We evaluate the controller in two experiments on the TIAGo robot and its parallel-jaw gripper proving the effectiveness of the approach for robust grasping and minimizing object displacement. In a series of ablation studies, we demonstrate the utility of the individual controller components.},
  archive   = {C_IROS},
  author    = {Luca Lach and Séverin Lemaignan and Francesco Ferro and Helge Ritter and Robert Haschke},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981819},
  pages     = {11231-11237},
  title     = {Bio-inspired grasping controller for sensorized 2-DoF grippers},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A comparative study of force observers for accurate force
control of multisensor-based force controlled motion systems.
<em>IROS</em>, 11223–11230. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a comprehensive comparative study of the multisensor-based force observers for accurate force control. A force controlled system which contains a force sensor for measuring force transmitted to the load by the motor and an encoder for measuring motor position is considered as the general multisensor-based motion system in this study. Even though these multisensor-based motion systems are emerging as potential motion systems as the demands for collaborative robots increase, there has been few studies that investigate their advantages and limitations. to address this issue, three types of observer-based force controllers that utilize the multisensors are designed and implemented. These controllers exploit the availability of force sensor, motor encoder, and motor torque information from the multisensor-based motion system to estimate accurate force which is later utilized to close the feedback loop. Mathematical and quantitative analyses are conducted to compare performances of the proposed observer-based force control and through this, their advantages and limitations are pointed out. Finally, simulation and an experimental case study with an actual robot are conducted to validate the force tracking performance of the designed force control systems.},
  archive   = {C_IROS},
  author    = {Kangwagye Samuel and Sehoon Oh},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981321},
  pages     = {11223-11230},
  title     = {A comparative study of force observers for accurate force control of multisensor-based force controlled motion systems},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feel the tension: Manipulation of deformable linear objects
in environments with fixtures using force information. <em>IROS</em>,
11216–11222. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans are able to manipulate Deformable Linear Objects (DLOs) such as cables and wires, with little or no visual information, relying mostly on force sensing. In this work, we propose a reduced DLO model which enables such blind manipulation by keeping the object under tension. Further, an online model estimation procedure is also proposed. A set of elementary sliding and clipping manipulation primitives are defined based on our model. The combination of these primitives allows for more complex motions such as winding of a DLO. The model estimation and manipulation primitives are tested individually but also together in a real-world cable harness production task, using a dual-arm YuMi, thus demonstrating that force-based perception can be sufficient even for such a complex scenario.},
  archive   = {C_IROS},
  author    = {Finn Süberkrüb and Rita Laezza and Yiannis Karayiannidis},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982065},
  pages     = {11216-11222},
  title     = {Feel the tension: Manipulation of deformable linear objects in environments with fixtures using force information},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the performance and passivity of admittance control with
feed-forward input. <em>IROS</em>, 11209–11215. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper analyzes the effect of control param-eters of feed-forward and inner loop velocity controller in an admittance control scheme on the performance and passivity. The interaction force, inertia, and damping compensation were considered as the feed-forward input. Sufficient conditions and guidelines for each parameter were provided to enable the implementation of a wide range of desired admittance satis-fying passivity. The proposed guidelines were verified through experiments.},
  archive   = {C_IROS},
  author    = {Dongwoo Ko and Donghyeon Lee and Wan Kyun Chung and Keehoon Kim},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981227},
  pages     = {11209-11215},
  title     = {On the performance and passivity of admittance control with feed-forward input},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient learning of inverse dynamics models for adaptive
computed torque control. <em>IROS</em>, 11203–11208. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modelling robot dynamics accurately is essential for control, motion optimisation and safe human-robot collaboration. Given the complexity of modern robotic systems, dynamics modelling remains non-trivial, mostly in the presence of compliant actuators, mechanical inaccuracies, friction and sensor noise. Recent efforts have focused on utilising datadriven methods such as Gaussian processes and neural networks to overcome these challenges, as they are capable of capturing these dynamics without requiring extensive knowledge beforehand. While Gaussian processes have shown to be an effective method for learning robotic dynamics with the ability to also represent the uncertainty in the learned model through its variance, they come at a cost of cubic time complexity rather than linear, as is the case for deep neural networks. In this work, we leverage the use of deep kernel models, which combine the computational efficiency of deep learning with the nonparametric flexibility of kernel methods (Gaussian processes), with the overarching goal of realising an accurate probabilistic framework for uncertainty quantification. Through using the predicted variance, we adapt the feedback gains as more accurate models are learned, leading to low-gain control without compromising tracking accuracy. Using simulated and real data recorded from a seven degree-of-freedom robotic manipulator, we illustrate how using stochastic variational inference with deep kernel models increases compliance in the computed torque controller, and retains tracking accuracy. We empirically show how our model outperforms current state-of-the-art methods with prediction uncertainty for online inverse dynamics model learning, and solidify its adaptation and generalisation capabilities across different setups.},
  archive   = {C_IROS},
  author    = {David Jorge and Gabriella Pizzuto and Michael Mistry},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981744},
  pages     = {11203-11208},
  title     = {Efficient learning of inverse dynamics models for adaptive computed torque control},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Probabilistic approach to online stiffness estimation for
robotic tasks. <em>IROS</em>, 11196–11202. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Information about environmental stiffness is useful for robotic tasks involving interactions with unstructured and unknown environments. However, online estimation remains a challenge. Owing to the nature of its calculation algorithm, a large amount of noise may be generated, depending on the response value of the force and position. In this study, we propose a variable gain filter that predicts the degree of such noise using a probabilistic approach and reflects only reliable data in the estimation. We show experimentally that the proposed method improves the accuracy of the stiffness estimation without degrading the estimation time constant.},
  archive   = {C_IROS},
  author    = {Toshiaki Tsuji and Tsukasa Kusakabe},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981146},
  pages     = {11196-11202},
  title     = {Probabilistic approach to online stiffness estimation for robotic tasks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Integrating impedance control and nonlinear disturbance
observer for robot-assisted arthroscope control in elbow arthroscopic
surgery. <em>IROS</em>, 11172–11179. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot-assisted arthroscopic surgery is transforming the tradition in orthopaedic surgery. Compliance and stability are essential features that a surgical robot must have for safe physical human-robot interaction ( P HRI). Surgical tools attached at the robot end-effector and human-robot interaction will affect the robot dynamics inevitably. This could undermine the utility and stability of the robotic system if the varying robot dynamics are not identified and updated in the robot control law. In this paper, an integrated frame-work for robot impedance control and nonlinear disturbance observer (NDOB)-based compensation of uncertain dynamics is proposed, where the former ensures compliant robot behavior and the latter compensates for dynamic uncertainties when necessary. The combination of impedance controller and NDOB is analyzed theoretically in three scenarios. A complete simulation and experimental studies involving three common conditions are then conducted to evaluate the theoretical analyses. A preliminary $p$ HRI application on arthroscopic surgery is designed to implement the proposed framework on a robotic surgeonassist system and evaluate its effectiveness experimentally. By integrating impedance controller with NDOB, the proposed framework allows an accurate impedance control when dynamic model inaccuracy and external disturbance exist.},
  archive   = {C_IROS},
  author    = {Teng Li and Armin Badre and Hamid D. Taghirad and Mahdi Tavakoli},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981208},
  pages     = {11172-11179},
  title     = {Integrating impedance control and nonlinear disturbance observer for robot-assisted arthroscope control in elbow arthroscopic surgery},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A wearable system with harmonic oscillations to assess
finger biomechanics. <em>IROS</em>, 11150–11157. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a wearable device for finger assessment that can identify finger joint impedance parameters through harmonic oscillation perturbations. This device is designed to help assess motor impairments related to hypertonic soft-tissue changes, that can arise from a number of conditions such as stroke. By measuring the ratio of the applied torque and resulting velocities, the impedance values for any bending direction of a metacarpophalangeal (MCP) joint can be estimated. The ability of this device to effectively estimate finger parameters was tested in experiments with six participants. The experimental result was validated through comparison to prior works on finger impedance estimation. The user experience of the presented system was also analysed, indicating that the device design is comfortable and acceptable for participants.},
  archive   = {C_IROS},
  author    = {Hao Yu and Aran Sena and Etienne Burdet},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982042},
  pages     = {11150-11157},
  title     = {A wearable system with harmonic oscillations to assess finger biomechanics},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel wheelchair-exoskeleton hybrid robot to assist
movement and aid rehabilitation. <em>IROS</em>, 11127–11133. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As a traditional movement assist equipment for people with lower-limb dysfunction, the wheelchair can support and carry users to perform a long-distance movement indoor and outdoor, however, prolonged inactivity can lead to muscle atrophy and deteriorate motion functions. As a promising solution, the lower limb exoskeleton provides people the ability of standing and walking to avoid these problems. However, the exoskeleton has inevitable shortcomings in long-distance movement and balance, which do not exist in a wheelchair. To integrate the advantages of both devices, in this paper, we proposed a wheelchair-exoskeleton hybrid robot (WeHR) that can not only provide users long-time support and long-distance movement but also provide walking training and keep self-balance. Moreover, motion transitions such as sit-to-stand and stand-to-sit can also be implemented by the newly proposed device without help from caregivers. We have developed the prototype to implement the above functions. In this paper, we emphasize the strategy of motion transition including two trajectory planning methods for the Sit-To-Stand (STS) process as well as the mechanism design to implement it. Furthermore, the preliminary experiments of motion transition and walking test are also conducted and the results prove that our device can support users sitting, standing, and walking and the motion transition.},
  archive   = {C_IROS},
  author    = {Zhibin Song and Wenjie Ju and Dechao Chen and Hexi Gong and Rongjie Kang and Paolo Dario},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981240},
  pages     = {11127-11133},
  title     = {A novel wheelchair-exoskeleton hybrid robot to assist movement and aid rehabilitation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Development of a novel low-profile robotic exoskeleton glove
for patients with brachial plexus injuries. <em>IROS</em>, 11121–11126.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents the design and development of a novel, low-profile, exoskeleton robotic glove aimed for people who suffer from brachial plexus injuries to restore their lost grasping functionality. The key idea of this new glove lies in its new finger mechanism that takes advantage of the rigid coupling hybrid mechanism (RCHM) concept. This mechanism concept couples the motions of the adjacent human finger links using rigid coupling mechanisms so that the overall mechanism motion (e.g., bending, extension, etc.) could be achieved using fewer actuators. The finger mechanism utilizes the single degree of freedom case of the RCHM that uses a rack-and-pinion mechanism as the rigid coupling mechanism. This special arrangement enables to design each finger mechanism of the glove as thin as possible while maintaining mechanical robustness simultaneously. Based on this novel finger mechanism, a two-finger low-profile robotic glove was developed. Remote center of motion mechanisms were used for the metacarpophalangeal (MCP) joints. Kinematic analysis and optimization-based kinematic synthesis were conducted to determine the design parameters of the new glove. Passive abduction/adduction joints were considered to improve the grasping flexibility. A proof-of-concept prototype was built and pinch grasping experiments of various objects were conducted. The results validated the mechanism and the mechanical design of the new robotic glove and demonstrated its functionalities and capabilities in grasping objects with various shapes and weights that are used in activities of daily living (ADLs).},
  archive   = {C_IROS},
  author    = {Wenda Xu and Yujiong Liu and Pinhas Ben-Tzvi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981124},
  pages     = {11121-11126},
  title     = {Development of a novel low-profile robotic exoskeleton glove for patients with brachial plexus injuries},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Development and experimental evaluation of a novel portable
haptic robotic exoskeleton glove system for patients with brachial
plexus injuries. <em>IROS</em>, 11115–11120. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents the development and experimental evaluation of a portable haptic exoskeleton glove system designed for people who suffer from brachial plexus injuries to restore their lost grasping functionality. The proposed glove system involves force perception, linkage-driven finger mechanism, and personalized voice control to achieve various grasping functionality requirements. The fully integrated system provides our wearable device with lightweight, portable, and comfortable characterization for grasping objects used in daily activities. Rigid articulated linkages powered by Series Elastic Actuators (SEAs) with slip detection on the fingertips provide stable and robust grasp for multiple objects. The passive abduction-adduction motion of each finger is also considered to provide better grasping flexibility for the user. The continuous voice control with bio-authentication also provides a hands-free user interface. The experiments with different objects verify the functionalities and capabilities of the proposed exoskeleton glove system in grasping objects with various shapes and weights used in activities of daily living (ADLs).},
  archive   = {C_IROS},
  author    = {Wenda Xu and Yunfei Guo and Cesar Bravo and Pinhas Ben-Tzvi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981468},
  pages     = {11115-11120},
  title     = {Development and experimental evaluation of a novel portable haptic robotic exoskeleton glove system for patients with brachial plexus injuries},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Soft actuators for facial reanimation. <em>IROS</em>,
11109–11114. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Facial paralysis is a challenging condition that alters a patient&#39;s ability to express emotion and communicate. Restoring facial movements thus has crucial implications for the patients&#39; quality of life. This publication introduces an approach for artificial muscles implementation targeting facial reanimation, as well as the challenges and limitations of the proposed strategy. The aim is to develop a Dielectric Elastomer Actuator (DEA) prosthesis for patients suffering from facial paralysis. DEAs are chosen as they are soft, have large strain (up to 200\%) and high dynamic behaviour (up to 20 kHz), making them a promising actuator for the application. Myo-electric signals are extracted using electromyography sensors from the Zygomaticus Major muscle, and they are processed in order to emphasize the activation phases. The resulting actuating signal is used to control a high voltage power supply to operate the DEA in an open loop. The resulting induced movement qualitatively matches the myoelectric signal, showing great potential of the proposed approach for facial paralysis reanimation.},
  archive   = {C_IROS},
  author    = {Stefania Konstantinidi and Thomas Martinez and Amine Benouhiba and Yoan Civet and Yves Perriard},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982089},
  pages     = {11109-11114},
  title     = {Soft actuators for facial reanimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). City-wide street-to-satellite image geolocalization of a
mobile ground agent. <em>IROS</em>, 11102–11108. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cross-view image geolocalization provides an estimate of an agent&#39;s global position by matching a local ground image to an overhead satellite image without the need for GPS. It is challenging to reliably match a ground image to the correct satellite image since the images have significant viewpoint differences. Existing works have demonstrated localization in constrained scenarios over small areas but have not demonstrated wider-scale localization. Our approach, called Wide-Area Geolocalization (WAG), combines a neural network with a particle filter to achieve global position estimates for agents moving in GPS-denied environments, scaling efficiently to city-scale regions. WAG introduces a trinomial loss function for a Siamese network to robustly match non-centered image pairs and thus enables the generation of a smaller satellite image database by coarsely discretizing the search area. A modified particle filter weighting scheme is also presented to improve localization accuracy and convergence. Taken together, WAG&#39;s network training and particle filter weighting approach achieves city-scale position estimation accuracies on the order of 20 meters, a 98\% reduction compared to a baseline training and weighting approach. Applied to a smaller-scale testing area, WAG reduces the final position estimation error by 64\% compared to a state-of-the-art baseline from the literature. WAG&#39;s search space discretization additionally significantly reduces storage and processing requirements. We include in our submission a video demonstrating particle filter convergence results for WAG compared to the baseline for the Chicago test area.},
  archive   = {C_IROS},
  author    = {Lena M. Downes and Dong-Ki Kim and Ted J. Steiner and Jonathan P. How},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981996},
  pages     = {11102-11108},
  title     = {City-wide street-to-satellite image geolocalization of a mobile ground agent},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PFilter: Building persistent maps through feature filtering
for fast and accurate LiDAR-based SLAM. <em>IROS</em>, 11087–11093. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simultaneous localization and mapping (SLAM) based on laser sensors has been widely adopted by mobile robots and autonomous vehicles. These SLAM systems are required to support accurate localization with limited computational resources. In particular, point cloud registration, i.e., the process of matching and aligning multiple LiDAR scans collected at multiple locations in a global coordinate framework, has been deemed as the bottleneck step in SLAM. In this paper, we propose a feature filtering algorithm, PFilter, that can filter out invalid features and can thus greatly alleviate this bottleneck. Meanwhile, the overall registration accuracy is also improved due to the carefully curated feature points. We integrate PFilter into the well-established scan-to-map LiDAR odometry framework, F-LOAM, and evaluate its performance on the KITTI dataset. The experimental results show that PFilter can remove about 48.4\% of the points in the local feature map and reduce feature points in scan by 19.3\% on average, which save 20.9\% processing time per frame. In the mean time, we improve the accuracy by 9.4\%.},
  archive   = {C_IROS},
  author    = {Yifan Duan and Jie Peng and Yu Zhang and Jianmin Ji and Yanyong Zhang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981566},
  pages     = {11087-11093},
  title     = {PFilter: Building persistent maps through feature filtering for fast and accurate LiDAR-based SLAM},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ACEFusion - accelerated and energy-efficient semantic 3D
reconstruction of dynamic scenes. <em>IROS</em>, 11063–11070. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {ACEFusion is the first 3D reconstruction system able to capture the geometry and semantics of dynamic scenes using an RGB-D camera in real-time on a robotic computing platform. Harnessing the hardware accelerators of an Nvidia Jetson AGX Xavier, the system uses heterogeneous computing to achieve 30 FPS under a 30W power budget. Using a data-parallel design, we perform most image computation on the dedicated hardware accelerators, freeing the general purpose cores and GPU to process 3D geometry. To further increase efficiency, we employ a hybrid geometry representation with octrees for static-semantic reconstruction and surfels for dynamic reconstruction. ACEFusion achieves competitive results on standard benchmarks while efficiently performing a more complex overall task than existing SLAM techniques. Figure. 1 shows the output of our system on a dynamic sequence.},
  archive   = {C_IROS},
  author    = {Mihai Bujanca and Barry Lennox and Mikel Luján},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981591},
  pages     = {11063-11070},
  title     = {ACEFusion - accelerated and energy-efficient semantic 3D reconstruction of dynamic scenes},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual-inertial multi-instance dynamic SLAM with
object-level relocalisation. <em>IROS</em>, 11055–11062. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a tightly-coupled visual-inertial object-level multi-instance dynamic SLAM system. Even in extremely dynamic scenes, it can robustly optimise for the camera pose, velocity, IMU biases and build a dense 3D reconstruction object-level map of the environment. Our system can robustly track and reconstruct the geometries of arbitrary objects, their semantics and motion by incrementally fusing associated colour, depth, semantic, and foreground object probabilities into each object model thanks to its robust sensor and object tracking. In addition, when an object is lost or moved outside the camera field of view, our system can reliably recover its pose upon re-observation. We demonstrate the robustness and accuracy of our method by quantitatively and qualitatively testing it in real-world data sequences.},
  archive   = {C_IROS},
  author    = {Yifei Ren and Binbin Xu and Christopher L. Choi and Stefan Leutenegger},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981795},
  pages     = {11055-11062},
  title     = {Visual-inertial multi-instance dynamic SLAM with object-level relocalisation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MD-SLAM: Multi-cue direct SLAM. <em>IROS</em>, 11047–11054.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simultaneous Localization and Mapping (SLAM) systems are fundamental building blocks for any autonomous robot navigating in unknown environments. The SLAM implementation heavily depends on the sensor modality employed on the mobile platform. For this reason, assumptions on the scene&#39;s structure are often made to maximize estimation accuracy. This paper presents a novel direct 3D SLAM pipeline that works independently for RGB-D and LiDAR sensors. Building upon prior work on multi-cue photometric frame-to-frame alignment [4], our proposed approach provides an easy-to-extend and generic SLAM system. Our pipeline requires only minor adaptations within the projection model to handle different sensor modalities. We couple a position tracking system with an appearance-based relocalization mechanism that handles large loop closures. Loop closures are validated by the same direct registration algorithm used for odometry estimation. We present comparative experiments with state-of-the-art approaches on publicly available benchmarks using RGB-D cameras and 3D LiDARs. Our system performs well in heterogeneous datasets compared to other sensor-specific methods while making no assumptions about the environment. Finally, we release an open-source C++ implementation of our system.},
  archive   = {C_IROS},
  author    = {Luca Di Giammarino and Leonardo Brizi and Tiziano Guadagnino and Cyrill Stachniss and Giorgio Grisetti},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981147},
  pages     = {11047-11054},
  title     = {MD-SLAM: Multi-cue direct SLAM},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detecting invalid map merges in lifelong SLAM.
<em>IROS</em>, 11039–11046. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For Lifelong SLAM, one has to deal with temporary localization failures, e.g., induced by kidnapping. We achieve this by starting a new map and merging it with the previous map as soon as relocalization succeeds. Since relocalization methods are fallible, it can happen that such a merge is invalid, e.g., due to perceptual aliasing. To address this issue, we propose methods to detect and undo invalid merges. These methods compare incoming scans with scans that were previously merged into the current map and consider how well they agree with each other. Evaluation of our methods takes place using a dataset that consists of multiple flat and office environments, as well as the public MIT Stata Center dataset. We show that methods based on a change detection algorithm and on comparison of gridmaps perform well in both environments and can be run in real-time with a reasonable computational cost.},
  archive   = {C_IROS},
  author    = {Matthias Holoch and Gerhard Kurz and Peter Biber},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981564},
  pages     = {11039-11046},
  title     = {Detecting invalid map merges in lifelong SLAM},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning physics-informed simulation models for soft robotic
manipulation: A case study with dielectric elastomer actuators.
<em>IROS</em>, 11031–11038. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft actuators offer a safe, adaptable approach to tasks like gentle grasping and dexterous manipulation. Creating accurate models to control such systems however is challenging due to the complex physics of deformable materials. Accurate Finite Element Method (FEM) models incur prohibitive computational complexity for closed-loop use. Using a differentiable simulator is an attractive alternative, but their applicability to soft actuators and deformable materials remains under-explored. This paper presents a framework that combines the advantages of both. We learn a differentiable model consisting of a material properties neural network and an analytical dynamics model of the remainder of the manipulation task. This physics-informed model is trained using data generated from FEM, and can be used for closed-loop control and inference. We evaluate our framework on a dielectric elastomer actuator (DEA) coin-pulling task. We simulate the task of using DEA to pull a coin along a surface with frictional contact, using FEM, and evaluate the physics-informed model for simulation, control, and inference. Our model attains ≤ 5\% simulation error compared to FEM, and we use it as the basis for an MPC controller that requires fewer iterations to converge than model-free actor-critic, PD, and heuristic policies.},
  archive   = {C_IROS},
  author    = {Manu Lahariya and Craig Innes and Chris Develder and Subramanian Ramamoorthy},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981373},
  pages     = {11031-11038},
  title     = {Learning physics-informed simulation models for soft robotic manipulation: A case study with dielectric elastomer actuators},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Omnidirectional walking of a quadruped robot enabled by
compressible tendon-driven soft actuators. <em>IROS</em>, 11015–11022.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Using soft actuators as legs, soft quadruped robots have shown great potential in traversing unstructured and complex terrains and environments. However, unlike rigid robots whose gaits can be generated using foot pattern design and kinematic model of the rigid legs, the gait generation of soft quadruped robots remains challenging due to the high DoFs of the soft actuators and the uncertain deformations during their contact with the ground. This study is based on a quadruped robot using four Compressible Tendon-driven Soft Actuators (CTSAs) as the legs, with the actuator&#39;s compression motion being utilized to improve the walking performance of the robot. For the gait design, an inverse kinematics model considering the compression of the CTSA is developed and validated in simulation. Based on this model, walking gaits realizing different motion speeds and directions are generated. Closed loop direction and speed controllers are developed for increasing the robustness and precision of the robot walking. Simulation and experimental results show that omnidirectional locomotion and complex walking tasks can be realized by tuning the gait parameters and the motions are resistant to external disturbances.},
  archive   = {C_IROS},
  author    = {Qinglei Ji and Shuo Fu and Lei Feng and George Andrikopoulos and Xi Vincent Wang and Lihui Wang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981314},
  pages     = {11015-11022},
  title     = {Omnidirectional walking of a quadruped robot enabled by compressible tendon-driven soft actuators},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Task-space control of continuum robots using underactuated
discrete rod models. <em>IROS</em>, 10967–10974. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Underactuation is a core challenge associated with controlling soft and continuum robots, which possess theoreti-cally infinite degrees of freedom, but few actuators. However, $m$ actuators may still be used to control a dynamic soft robot in an m-dimensional output task space. In this paper we develop a task-space control approach for planar continuum robots that is robust to modeling error and requires very little sensor information. The controller is based on a highly underactuated discrete rod mechanics model in maximal coordinates and does not require conversion to a classical robot dynamics model form. This promotes straightforward control design, implementation and efficiency. We perform input-output feedback linearization on this model, apply sliding mode control to increase robustness, and formulate an observer to estimate the full state from sparse output measurements. Simulation results show exact task-space reference tracking behavior can be achieved even in the presence of significant modeling error, inaccurate initial conditions, and output-only sensing.},
  archive   = {C_IROS},
  author    = {Caleb Rucker and Eric J. Barth and Joshua Gaston and James C. Gallentine},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982271},
  pages     = {10967-10974},
  title     = {Task-space control of continuum robots using underactuated discrete rod models},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online adaptive compensation for model uncertainty using
extreme learning machine-based control barrier functions. <em>IROS</em>,
10959–10966. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A control barrier functions-based quadratic programming (CBF-QP) method has emerged as a controller synthesis tool to assure safety of autonomous systems owing to the appealing safe forward invariant set. However, the provable safety relies on a precisely described dynamic model, which is not always available in practice. Recent works leverage learning to compensate model uncertainty for a CBF controller. However, these approaches based on reinforcement learning or episodic learning are limited to dealing with time-invariant uncertainty. Also, the reinforcement learning approach learns the uncertainty offline, while episodic learning only updates the controller after a batch of data is available by the end of an episode. Instead, we propose a novel tuning extreme learning machine (tELM)-based CBF controller that can compensate time-variant and time-invariant model uncertainty adaptively in an online manner. We validate our approach&#39;s effectiveness in a simulation of an Adaptive Cruise Control (ACC) system.},
  archive   = {C_IROS},
  author    = {Emanuel Munoz and Dvij Kalaria and Qin Lin and John M. Dolan},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981680},
  pages     = {10959-10966},
  title     = {Online adaptive compensation for model uncertainty using extreme learning machine-based control barrier functions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sex parity in cognitive fatigue model development for
effective human-robot collaboration. <em>IROS</em>, 10951–10958. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, robots have become vital to achieving manufacturing competitiveness. Especially in industrial environments, a strong level of interaction is reached when humans and robots form a dynamic system that works together towards achieving a common goal or accomplishing a task. However, the human-robot collaboration can be cognitively demanding, potentially contributing to cognitive fatigue. Therefore, the consideration of cognitive fatigue becomes particularly important to ensure the efficiency and safety in the overall human-robot collaboration. Additionally, sex is an inevitable human factor that needs further investigation for machine learning model development given the perceptual and physiological differences between the sexes in responding to fatigue. As such, this study explored sex differences and labeling strategies in the development of machine learning models for cognitive fatigue detection. Sixteen participants, balanced by sex, recruited to perform a surface finishing task with a UR10 collaborative robot under fatigued and non-fatigued states. Fatigue perception and heart rate activity data collected throughout to create a dataset for cognitive fatigue detection. Equitable machine learning models developed based on perception (survey responses) and condition (fatigue manipulation). The labeling approach had a significant impact on the accuracy and F1-score, where perception-based labels lead to lower accuracy and F1-score for females likely due to sex differences in reporting of fatigue. Additionally, we observed a relationship between heart rate, algorithm type, and labeling approach, where heart rate was the most significant predictor for the two labeling approaches and for all the algorithms utilized. Understanding the implications of label type, algorithm type, and sex on the design of fatigue detection algorithms is essential to designing equitable fatigue-adaptive human-robot collaborations across the sexes.},
  archive   = {C_IROS},
  author    = {Apostolos Kalatzis and Sarah Hopko and Ranjana K. Mehta and Laura Stanley and Mike P. Wittie},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981097},
  pages     = {10951-10958},
  title     = {Sex parity in cognitive fatigue model development for effective human-robot collaboration},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robot skill learning with identification of preconditions
and postconditions via level set estimation. <em>IROS</em>, 10943–10950.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hierarchical algorithms have often been used to plan and execute complicated robotic sequential manipulation tasks, where an abstract planner searches for a skill sequence in an abstract space, and each skill generates actual motions on the basis of the planned skill sequences. To generate executable plans, the abstract planner should know the pre-/postconditions of each skill and appropriately choose skills so that the generated plan satisfies their pre-/postconditions. For such hierarchical planning, this paper presents a novel method for robot skill learning that learns not only a control policy but also the learned skill&#39;s pre-/postconditions to complete a given task. Our method combines an optimal control method and an active learning approach called level set estimation (LSE) to effectively collect training data for learning control policies and pre-/postconditions. Although there exists a LSE-based policy learning algorithm that identifies preconditions, its performance is limited to cases where the dimension of the search space for pre-/postconditions is low. The main contribution of this paper is the proposal of a new learning method that can handle tasks having a high-dimensional search space for pre-/postconditions. We demonstrate our proposed method in two robotic tasks. The results show that our method can more effectively learn a control policy and its pre-/postconditions compared with the existing LSE-based method.},
  archive   = {C_IROS},
  author    = {Rin Takano and Hiroyuki Oyama and Yuki Taya},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981933},
  pages     = {10943-10950},
  title     = {Robot skill learning with identification of preconditions and postconditions via level set estimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Non-blocking asynchronous training for reinforcement
learning in real-world environments. <em>IROS</em>, 10927–10934. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep Reinforcement Learning (DRL) faces challenges bridging the sim-to-real gap to enable real-world applications. In contrast to the simulated environments used in conventional DRL training, real-world systems are non-linear and evolve in an asynchronous fashion; sensors and actuators have limited precision; communication channels are noisy; and many components introduce variable delays. While these issues are known to many researchers, published methods for systematically tackling the problem of DRL training under these conditions without using simulation are sparse in the field. To this end, this paper proposes a non-blocking and asynchronous DRL training architecture for non-linear, real-time dynamical systems tailored to handling variable delays. Compared to conventional DRL training, we: (i) decouple the RL loop into separate processes run independently at their own frequencies, (ii) further decouple collection of transition tuples $(s_{t}, at_{t}, s_{t+1})$ via asynchronous and independent streaming of both actions and observations, and (iii) mitigate the effects of delays and increase sample efficiency by providing delay-length measurements to the training loop and regular retraining of the DRL network. This allows the action step time to be tuned to find an optimal control frequency for a given system, and handles streamed observations that arrive with random delays and independently of action timing. We demonstrate the efficacy of this architecture with a physical implementations of a commodity-grade swing-up pendulum and a quadrupedal robot. Our architecture achieves the best results balancing the pendulum for almost entire length of the episode, compared to conventional blocking approaches which fail to learn effective policies. Our results show that these techniques scale to more complex tasks such as quadrupedal locomotion.},
  archive   = {C_IROS},
  author    = {Peter Böhm and Pauline Pounds and Archie C. Chapman},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981333},
  pages     = {10927-10934},
  title     = {Non-blocking asynchronous training for reinforcement learning in real-world environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Grasp planning for occluded objects in a confined space with
lateral view using monte carlo tree search. <em>IROS</em>, 10921–10926.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the lateral access environment, the robot be-havior should be planned considering surrounding objects and obstacles because object observation directions and approach angles are limited. To safely retrieve a partially occluded target object in these environments, we have to relocate objects using prehensile actions to create a collision-free path for the target. We propose a learning-based method for object rearrangement planning applicable to objects of various types and sizes in the lateral environment. We plan the optimal rearrangement sequence by considering both collisions and approach angles at which objects can be grasped. The proposed method finds the grasping order through Monte Carlo tree search, significantly reducing the tree search cost using point cloud states. In the experiment, the proposed method shows the best and most stable performance in various scenarios compared to the existing TAMP methods. In addition, we confirm that the proposed method trained in simulation can be easily applied to a real robot without additional fine-tuning, showing the robustness of the proposed method.},
  archive   = {C_IROS},
  author    = {Minjae Kang and Hogun Kee and Junseok Kim and Songhwai Oh},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981069},
  pages     = {10921-10926},
  title     = {Grasp planning for occluded objects in a confined space with lateral view using monte carlo tree search},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic inference on graphs using structured transition
models. <em>IROS</em>, 10913–10920. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Enabling robots to perform complex dynamic tasks such as picking up an object in one sweeping motion or pushing off a wall to quickly turn a corner is a challenging problem. The dynamic interactions implicit in these tasks are critical towards the successful execution of such tasks. Graph neural networks (GNNs) provide a principled way of learning the dynamics of interactive systems but can suffer from scaling issues as the number of interactions increases. Furthermore, the problem of using learned GNN-based models for optimal control is insufficiently explored. In this work, we present a method for efficiently learning the dynamics of interacting systems by simultaneously learning a dynamic graph structure and a stable and locally linear forward model of the system. The dynamic graph structure encodes evolving contact modes along a trajectory by making probabilistic predictions over the edges of the graph. Additionally, we introduce a temporal dependence in the learned graph structure which allows us to incorporate contact measurement updates during execution thus enabling more accurate forward predictions. The learned stable and locally linear dynamics enable the use of optimal control algorithms such as iLQR for long-horizon planning and control for complex interactive tasks. Through experiments in simulation and in the real world, we evaluate the performance of our method by using the learned interaction dynamics for control and demonstrate generalization to more objects and interactions not seen during training. We introduce a control scheme that takes advantage of contact measurement updates and hence is robust to prediction inaccuracies during execution.},
  archive   = {C_IROS},
  author    = {Saumya Saxena and Oliver Kroemer},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981449},
  pages     = {10913-10920},
  title     = {Dynamic inference on graphs using structured transition models},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SMS-MPC: Adversarial learning-based simultaneous prediction
control with single model for mobile robots. <em>IROS</em>, 10905–10912.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Model predictive control is a promising method in robot control tasks. How to design an effective model structure and efficient prediction framework for model predictive control is still an open challenge. To reduce the time consumption and avoid compounding-error of the multi-step prediction process in model predictive control, we propose a single-model simultaneous framework, which uses single dynamics model to predict the entire prediction horizon simultaneously by taking all control actions with the current state as inputs. Based on this framework, we further propose an adversarial dynamics model that contains two parts. The generator provides a dynamics model for the prediction process, while the discriminator provides constraints that are hard to describe by manually defined loss. This adversarial dynamics model can accelerate training and improve model accuracy in unstructured environments. Experiments conducted in Gazebo simulator and on a real mobile robot demonstrate the efficiency and accuracy of the single-model simultaneous framework with an adversarial dynamics model.},
  archive   = {C_IROS},
  author    = {Andong Yang and Wei Li and Yu Hu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981289},
  pages     = {10905-10912},
  title     = {SMS-MPC: Adversarial learning-based simultaneous prediction control with single model for mobile robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiscale sensor fusion and continuous control with neural
CDEs. <em>IROS</em>, 10897–10904. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Though robot learning is often formulated in terms of discrete-time Markov decision processes (MDPs), physical robots require near-continuous multiscale feedback control. Machines operate on multiple asynchronous sensing modalities, each with different frequencies, e.g., video frames at 30Hz, proprioceptive state at 100Hz, force-torque data at 500Hz, etc. While the classic approach is to batch observations into fixed-time windows then pass them through feed-forward encoders (e.g., with deep networks), we show that there exists a more elegant approach - one that treats policy learning as modeling latent state dynamics in continuous-time. Specifically, we present InFuser, a unified architecture that trains continuous time-policies with Neural Controlled Differential Equations (CDEs). InFuser evolves a single latent state representation over time by (In)tegrating and (Fus)ing multi-sensory observations (arriving at different frequencies), and inferring actions in continuous-time. This enables policies that can react to multi-frequency multi-sensory feedback for truly end-to-end visuomotor control, without discrete-time assumptions. Behavior cloning experiments demonstrate that InFuser learns robust policies for dynamic tasks (e.g., swinging a ball into a cup) notably outperforming several baselines in settings where observations from one sensing modality can arrive at much sparser intervals than others.},
  archive   = {C_IROS},
  author    = {Sumeet Singh and Francis McCann Ramirez and Jacob Varley and Andy Zeng and Vikas Sindhwani},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982210},
  pages     = {10897-10904},
  title     = {Multiscale sensor fusion and continuous control with neural CDEs},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An unsupervised domain adaptive approach for multimodal 2D
object detection in adverse weather conditions. <em>IROS</em>,
10865–10872. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Integrating different representations from complementary sensing modalities is crucial for robust scene interpretation in autonomous driving. While deep learning architectures that fuse vision and range data for 2D object detection have thrived in recent years, the corresponding modalities can degrade in adverse weather or lighting conditions, ultimately leading to a drop in performance. Although domain adaptation methods attempt to bridge the domain gap between source and target domains, they do not readily extend to heterogeneous data distributions. In this work, we propose an unsupervised domain adaptation framework, which adapts a 2D object detector for RGB and LiDAR sensors to one or more target domains featuring adverse weather conditions. Our proposed approach consists of three components. First, a data augmentation scheme that simulates weather distortions is devised to add domain confusion and prevent overfitting on the source data. Second, to promote cross-domain foreground object alignment, we leverage the complementary features of multiple modalities through a multi-scale entropy-weighted domain discriminator. Finally, we use carefully designed pretext tasks to learn a more robust representation of the target domain data. Experiments performed on the DENSE dataset show that our method can substantially alleviate the domain gap under the single-target domain adaptation setting and the less explored yet more general multi-target domain adaptation setting.},
  archive   = {C_IROS},
  author    = {George Eskandar and Robert A. Marsden and Pavithran Pandiyan and Mario Döbler and Karim Guirguis and Bin Yang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982109},
  pages     = {10865-10872},
  title     = {An unsupervised domain adaptive approach for multimodal 2D object detection in adverse weather conditions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pseudo-label guided cross-video pixel contrast for robotic
surgical scene segmentation with limited annotations. <em>IROS</em>,
10857–10864. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Surgical scene segmentation is fundamentally crucial for prompting cognitive assistance in robotic surgery. However, pixel-wise annotating surgical video in a frame-by-frame manner is expensive and time consuming. To greatly reduce the labeling burden, in this work, we study semi-supervised scene segmentation from robotic surgical video, which is practically essential yet rarely explored before. We consider a clinically suitable annotation situation under the equidistant sampling. We then propose PGV-CL, a novel pseudo-label guided cross-video contrast learning method to boost scene segmentation. It effectively leverages unlabeled data for a trusty and global model regularization that produces more discriminative feature representation. Concretely, for trusty representation learning, we propose to incorporate pseudo labels to instruct the pair selection, obtaining more reliable representation pairs for pixel contrast. Moreover, we expand the representation learning space from previous image-level to cross-video, which can capture the global semantics to benefit the learning process. We extensively evaluate our method on a public robotic surgery dataset EndoVis18 and a public cataract dataset CaDIS. Experimental results demonstrate the effectiveness of our method, consistently outperforming the state-of-the-art semi-supervised methods under different labeling ratios, and even surpassing fully supervised training on EndoVis18 with 10.1\% labeling. Our code is available at https://github.com/yangyu-cuhk/PGV-CL.},
  archive   = {C_IROS},
  author    = {Yang Yu and Zixu Zhao and Yueming Jin and Guangyong Chen and Qi Dou and Pheng-Ann Heng},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981798},
  pages     = {10857-10864},
  title     = {Pseudo-label guided cross-video pixel contrast for robotic surgical scene segmentation with limited annotations},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamics-aware spatiotemporal occupancy prediction in urban
environments. <em>IROS</em>, 10836–10841. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Detection and segmentation of moving obstacles, along with prediction of the future occupancy states of the local environment, are essential for autonomous vehicles to proactively make safe and informed decisions. In this paper, we propose a framework that integrates the two capabilities together using deep neural network architectures. Our method first detects and segments moving objects in the scene, and uses this information to predict the spatiotemporal evolution of the environment around autonomous vehicles. to address the problem of direct integration of both static-dynamic object segmentation and environment prediction models, we propose using occupancy-based environment representations across the whole framework. Our method is validated on the real-world Waymo Open Dataset and demonstrates higher prediction accuracy than baseline methods.},
  archive   = {C_IROS},
  author    = {Maneekwan Toyungyernsub and Esen Yel and Jiachen Li and Mykel J. Kochenderfer},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981323},
  pages     = {10836-10841},
  title     = {Dynamics-aware spatiotemporal occupancy prediction in urban environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DiffCloud: Real-to-sim from point clouds with differentiable
simulation and rendering of deformable objects. <em>IROS</em>,
10828–10835. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Research in manipulation of deformable objects is typically conducted on a limited range of scenarios, because handling each scenario on hardware takes significant effort. Realistic simulators with support for various types of deformations and interactions have the potential to speed up experimentation with novel tasks and algorithms. However, for highly deformable objects it is challenging to align the output of a simulator with the behavior of real objects. Manual tuning is not intuitive, hence automated methods are needed. We view this alignment problem as a joint perception-inference challenge and demonstrate how to use recent neural network architectures to successfully perform simulation parameter inference from real point clouds. We analyze the performance of various architectures, comparing their data and training requirements. Furthermore, we propose to leverage differentiable point cloud sampling and differentiable simulation to significantly reduce the time to achieve the alignment. We employ an efficient way to propagate gradients from point clouds to simulated meshes and further through to the physical simulation parameters, such as mass and stiffness. Experiments with highly deformable objects show that our method can achieve comparable or better alignment with real object behavior, while reducing the time needed to achieve this by more than an order of magnitude. Videos and supplementary material are available at https://tinyurl.com/diffcloud.},
  archive   = {C_IROS},
  author    = {Priya Sundaresan and Rika Antonova and Jeannette Bohgl},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981101},
  pages     = {10828-10835},
  title     = {DiffCloud: Real-to-sim from point clouds with differentiable simulation and rendering of deformable objects},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian active learning for sim-to-real robotic perception.
<em>IROS</em>, 10820–10827. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While learning from synthetic training data has recently gained an increased attention, in real-world robotic applications, there are still performance deficiencies due to the so-called Sim-to-Real gap. In practice, this gap is hard to resolve with only synthetic data. Therefore, we focus on an efficient acquisition of real data within a Sim-to-Real learning pipeline. Concretely, we employ deep Bayesian active learning to minimize manual annotation efforts and devise an autonomous learning paradigm to select the data that is considered useful for the human expert to annotate. To achieve this, a Bayesian Neural Network (BNN) object detector providing reliable un-certainty estimates is adapted to infer the informativeness of the unlabeled data. Furthermore, to cope with misalignments of the label distribution in uncertainty-based sampling, we develop an effective randomized sampling strategy that performs favorably compared to other complex alternatives. In our experiments on object classification and detection, we show benefits of our approach and provide evidence that labeling efforts can be reduced significantly. Finally, we demonstrate the practical effectiveness of this idea in a grasping task on an assistive robot.},
  archive   = {C_IROS},
  author    = {Jianxiang Feng and Jongseok Lee and Maximilian Durner and Rudolph Triebel},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982175},
  pages     = {10820-10827},
  title     = {Bayesian active learning for sim-to-real robotic perception},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning from demonstration using a curvature regularized
variational auto-encoder (CurvVAE). <em>IROS</em>, 10795–10800. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning intricate manipulation skills from human demonstrations requires good sample efficiency. We introduce a novel learning algorithm, the Curvature-regularized Variational Auto-Encoder (CurvVAE), to achieve this goal. The CurvVAE is able to model the natural variations in human-demonstrated trajectory data without overfitting. It does so by regularizing the curvature of the learned manifold. To showcase our algorithm, our robot learns an interpretable model of the variation in how humans acquire soft, slippery banana slices with a fork. We evaluate our learned trajectories on a physical robot system, resulting in banana slice acquisition performance better than current state-of-the-art.},
  archive   = {C_IROS},
  author    = {Travers Rhodes and Tapomayukh Bhattacharjee and Daniel D. Lee},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981930},
  pages     = {10795-10800},
  title     = {Learning from demonstration using a curvature regularized variational auto-encoder (CurvVAE)},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Behaviour learning with adaptive motif discovery and
interacting multiple model. <em>IROS</em>, 10788–10794. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose an approach that enables simultaneous interpretable learning of a high-level discrete behaviour and its low-level rhythmic sub-behaviour. We do this though a unified reward function, where a reward function that only describes low-level behaviour, with less impact on learning of other behaviours is recovered from few-shot motion demonstrations. To this end, we first extract local behaviour motifs from state-only human demonstrations and random driving samples using an adaptive motif discovery approach derived from the Matrix Profile algorithm. We then optimize parameters for motif discovery by maximizing the sum and entropy over motif sizes. Interacting Multiple Model (IMM) estimators are constructed on top of linear-Gaussian dynamics of discovered motifs, the cumulative distributions over motifs estimated by IMMs serve as the basis of the reward function. By combining the recovered reward with the terrain type signal gathered from the environment, we are able to train a dual-objective off-road vehicle controller that demonstrates both terrain selection and human-like driving behaviours. Compared with related approaches across 10 people, our rhythmic behaviour reward recovery approach enables the controller to produce higher preference over human driving demonstrations. In addition to performing more stable across different people with 87\% less variance than the best baseline in rhythmic behaviour indicator, our method reduces the negative effects on higher-level behaviour learning while maintaining high interpretability at all stages of the algorithm.},
  archive   = {C_IROS},
  author    = {Hanging Zhao and Travis Manderson and Hao Zhang and Xue Liu and Gregory Dudek},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981588},
  pages     = {10788-10794},
  title     = {Behaviour learning with adaptive motif discovery and interacting multiple model},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning high speed precision table tennis on a physical
robot. <em>IROS</em>, 10780–10787. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning goal conditioned control in the real world is a challenging open problem in robotics. Reinforcement learning systems have the potential to learn autonomously via trial-and-error, but in practice the costs of manual reward design, ensuring safe exploration, and hyperparameter tuning are often enough to preclude real world deployment. Imitation learning approaches, on the other hand, offer a simple way to learn control in the real world, but typically require costly cu-rated demonstration data and lack a mechanism for continuous improvement. Recently, iterative imitation methods have been shown to be effective at relaxing both these constraints, learning goal directed control from undirected demonstration data, and improving continuously via self-supervised goal reaching. These approaches, however, have not yet been shown to scale beyond simple simulated environments. In this work, we present the first evidence that simple iterative imitation learning can scale to goal-directed behavior on a real robot in a dynamic setting: high speed, precision table tennis (e.g. “land the ball on this particular target”). We find that this approach offers a straightforward way to do continuous on-robot learning, without complexities such as reward design, value function learning, or sim-to-real transfer. We also find that this approach is scalable-sample efficient enough to train on a physical robot in just a few hours. In real world evaluations, we find that that the resulting policy can perform on par or better than amateur humans (with players sampled randomly from a robotics lab) at the task of returning the ball to specific targets on the table. Finally, we analyze the effect of an initial undirected bootstrap dataset size on performance, finding that a modest amount of unstructured demonstration data provided up-front drastically speeds up the convergence of a general purpose goal-reaching policy. See supplementary video for examples of the policy on a physical robot.},
  archive   = {C_IROS},
  author    = {Tianli Ding and Laura Graesser and Saminda Abeyruwan and David B. D&#39;Ambrosio and Anish Shankar and Pierre Sermanet and Pannag R. Sanketi and Corey Lynch},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982205},
  pages     = {10780-10787},
  title     = {Learning high speed precision table tennis on a physical robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Extending extrapolation capabilities of probabilistic motion
models learned from human demonstrations using shape-preserving virtual
demonstrations. <em>IROS</em>, 10772–10779. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning from Demonstration (LfD) requires methodologies able to generalize tasks in new situations. This paper studies the use of virtual demonstrations to extend the extrapolation capabilities of probabilistic motion models such as the traPPCA method. Similarly to other LfD methods, traPPCA is able to calculate new trajectories very fast, but does not generalize well outside the area covered by the demonstrations. Another approach, the invariants method, shows outstanding generalization capabilities thanks to its shape-preserving prop-erties, while being limited by long computation times. The pro-posed methodology combines the advantages of the two methods by learning traPPCA models using virtual demonstrations generated by the invariants method. The proposed approach is analyzed in three case studies. Furthermore, a comparison is made between learning with virtual demonstrations and learning with only real demonstrations. The results encourage the use of virtual demonstrations to extend the extrapolation capabilities of probabilistic motion models and hence reduce the required number of real demonstrations. The latter has the potential of reducing the cost of commissioning robot tasks.},
  archive   = {C_IROS},
  author    = {Riccardo Burlizzi and Maxim Vochten and Joris De Schutter and Erwin Aertbeliën},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982222},
  pages     = {10772-10779},
  title     = {Extending extrapolation capabilities of probabilistic motion models learned from human demonstrations using shape-preserving virtual demonstrations},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Transporters with visual foresight for solving unseen
rearrangement tasks. <em>IROS</em>, 10756–10763. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Rearrangement tasks have been identified as a crucial challenge for intelligent robotic manipulation, but few methods allow for precise construction of unseen structures. We propose a visual foresight model for pick-and-place rearrangement manipulation which is able to learn efficiently. In addition, we develop a multi-modal action proposal module which builds on the Goal-Conditioned Transporter Network, a state-of-the-art imitation learning method. Our image-based task planning method, Transporters with Visual Foresight, is able to learn from only a handful of data and generalize to multiple unseen tasks in a zero-shot manner. TVF is able to improve the performance of a state-of-the-art imitation learning method on unseen tasks in simulation and real robot experiments. In particular, the average success rate on unseen tasks improves from 55.4\% to 78.5\% in simulation experiments and from 30\% to 63.3\% in real robot experiments when given only tens of expert demonstrations. Video and code are available on our project website: https://chirikjianlab.github.io/tvf/},
  archive   = {C_IROS},
  author    = {Hongtao Wu and Jikai Ye and Xin Meng and Chris Paxton and Gregory S. Chirikjian},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981832},
  pages     = {10756-10763},
  title     = {Transporters with visual foresight for solving unseen rearrangement tasks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quantifying changes in kinematic behavior of a
human-exoskeleton interactive system. <em>IROS</em>, 10734–10739. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While human-robot interaction studies are becoming more common, quantification of the effects of repeated interaction with an exoskeleton remains unexplored. We draw upon existing literature in human skill assessment and present extrinsic and intrinsic performance metrics that quantify how the human-exoskeleton system&#39;s behavior changes over time. Specifically, in this paper, we present a new performance metric that provides insight into the system&#39;s kinematics associated with ‘successful’ movements resulting in a richer characterization of changes in the system&#39;s behavior. A human subject study is carried out wherein participants learn to play a challenging and dynamic reaching game over multiple attempts, while donning an upper-body exoskeleton. The results demonstrate that repeated practice results in learning over time as identified through the improvement of extrinsic performance. Changes in the newly developed kinematics-based measure further illumi-nate how the participant&#39;s intrinsic behavior is altered over the training period. Thus, we are able to quantify the changes in the human-exoskeleton system&#39;s behavior observed in relation with learning.},
  archive   = {C_IROS},
  author    = {Keya Ghonasgi and Reuth Mirsky and Adrian M. Haith and Peter Stone and Ashish D. Deshpande},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981032},
  pages     = {10734-10739},
  title     = {Quantifying changes in kinematic behavior of a human-exoskeleton interactive system},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bounded rational game-theoretical modeling of human joint
actions with incomplete information. <em>IROS</em>, 10720–10725. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As humans and robots start to collaborate in close proximity, robots are tasked to perceive, comprehend, and anticipate human partners&#39; actions, which demands a predictive model to describe how humans collaborate with each other in joint actions. Previous studies either simplify the collaborative task as an optimal control problem between two agents or do not consider the learning process of humans during repeated interaction. This idyllic representation is thus not able to model human rationality and the learning process. In this paper, a bounded-rational and game-theoretical human cooperative model is developed to describe the cooperative behaviors of the human dyad. An experiment of a joint object pushing collaborative task was conducted with 30 human subjects using haptic interfaces in a virtual environment. The proposed model uses inverse optimal control (IOC) to model the reward parameters in the collaborative task. The collected data verified the accuracy of the predicted human trajectory generated from the bounded rational model excels the one with a fully rational model. We further provide insight from the conducted experiments about the effects of leadership on the performance of human collaboration.},
  archive   = {C_IROS},
  author    = {Yiwei Wang and Pallavi Shintre and Sunny Amatya and Wenlong Zhang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982108},
  pages     = {10720-10725},
  title     = {Bounded rational game-theoretical modeling of human joint actions with incomplete information},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sociable and ergonomic human-robot collaboration through
action recognition and augmented hierarchical quadratic programming.
<em>IROS</em>, 10712–10719. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The recognition of actions performed by humans and the anticipation of their intentions are important enablers to yield sociable and successful collaboration in human-robot teams. Meanwhile, robots should have the capacity to deal with multiple objectives and constraints, arising from the collaborative task or the human. In this regard, we propose vision techniques to perform human action recognition and image classification, which are integrated into an Augmented Hierarchical Quadratic Programming (AHQP) scheme to hierarchically optimize the robot&#39;s reactive behavior and human ergonomics. The proposed framework allows one to intuitively command the robot in space while a task is being executed. The experiments confirm increased human ergonomics and usability, which are fundamental parameters for reducing musculoskeletal diseases and increasing trust in automation.},
  archive   = {C_IROS},
  author    = {Francesco Tassi and Francesco Iodice and Elena De Momi and Arash Ajoudani},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982160},
  pages     = {10712-10719},
  title     = {Sociable and ergonomic human-robot collaboration through action recognition and augmented hierarchical quadratic programming},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The predictive kinematic control tree: Enhancing
teleoperation of redundant robots through probabilistic user models.
<em>IROS</em>, 10704–10711. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When teleoperating complex robotic manipula-tors, operators often find it most natural to issue commands that dictate end effector movements in task space. If the robot has redundant degrees of freedom, the translation of this com-mand from task space into configuration space can affect the robot&#39;s maneuverability, smoothness of motion, and the general precision of the teleoperated system. In this paper, we propose a novel method for performing this translation that predicts future operator commands in order to choose joint motions that maintain maneuverability in future timesteps. We introduce a Predictive Kinematic Control Tree (PrediKCT) that optimizes joint movement in the nullspace of the Jacobian over multiple future timesteps by reasoning over probabilistic models of the human operator. In essence, PrediKCT builds out and evaluates a tree of possible future commands. We implement this system on two simulated and one physical 7 -degree-of-freedom robotic arms and characterize performance by analyzing robot motions produced through multiple command trajectories with differing user model accuracies and tree parameters, demonstrating benefits to path accuracy over both a minimum-norm joint velocity solution and local optimization of joint movement.},
  archive   = {C_IROS},
  author    = {Connor Brooks and Daniel Szafir},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982150},
  pages     = {10704-10711},
  title     = {The predictive kinematic control tree: Enhancing teleoperation of redundant robots through probabilistic user models},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Human-robot collaborative carrying of objects with unknown
deformation characteristics. <em>IROS</em>, 10681–10687. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we introduce an adaptive control framework for human-robot collaborative transportation of objects with unknown deformation behaviour. The proposed framework takes as input the haptic information transmitted through the object, and the kinematic information of the human body obtained from a motion capture system to create reactive whole-body motions on a mobile collaborative robot. In order to validate our framework experimentally, we compared its performance with an admittance controller during a co-transportation task of a partially deformable object. We additionally demonstrate the potential of the framework while co-transporting rigid (aluminum rod) and highly deformable (rope) objects. A mobile manipulator which consists of an Omni-directional mobile base, a collaborative robotic arm, and a robotic hand is used as the robotic partner in the experiments. Quantitative and qualitative results of a 12-subjects experiment show that the proposed framework can effectively deal with objects of unknown deformability and provides intuitive assistance to human partners.},
  archive   = {C_IROS},
  author    = {Doganay Sirintuna and Alberto Giammarino and Arash Ajoudani},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981948},
  pages     = {10681-10687},
  title     = {Human-robot collaborative carrying of objects with unknown deformation characteristics},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust human motion forecasting using transformer-based
model. <em>IROS</em>, 10674–10680. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Comprehending human motion is a fundamental challenge for developing Human-Robot Collaborative applications. Computer vision researchers have addressed this field by only focusing on reducing error in predictions, but not taking into account the requirements to facilitate its implementation in robots. In this paper, we propose a new model based on Transformer that simultaneously deals with the real time 3D human motion forecasting in the short and long term. Our 2-Channel Transformer (2CH-TR) is able to efficiently exploit the spatio-temporal information of a shortly observed sequence (400ms) and generates a competitive accuracy against the current state-of-the-art. 2CH-TR stands out for the efficient performance of the Transformer, being lighter and faster than its competitors. In addition, our model is tested in conditions where the human motion is severely occluded, demonstrating its robustness in reconstructing and predicting 3D human motion in a highly noisy environment. Our experiment results show that the proposed 2CH-TR outperforms the ST-Transformer, which is another state-of-the-art model based on the Transformer, in terms of reconstruction and prediction under the same conditions of input prefix. Our model reduces in 8.89\% the mean squared error of ST-Transformer in short-term prediction, and 2.57\% in long-term prediction in Human3.6M dataset with 400ms input prefix.},
  archive   = {C_IROS},
  author    = {Esteve Valls Mascaro and Shuo Ma and Hyemin Ahn and Dongheui Lee},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981877},
  pages     = {10674-10680},
  title     = {Robust human motion forecasting using transformer-based model},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attention-guided RGB-d fusion network for category-level 6D
object pose estimation. <em>IROS</em>, 10651–10658. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work focuses on estimating 6D poses and sizes of category-level objects from a single RGB-D image. How to exploit the complementary RGB and depth features plays an important role in this task yet remains an open question. Due to the large intra-category texture and shape variations, an object instance in test may have different RGB and depth features from those of the object instances in training, which poses challenges to previous RGB-D fusion methods. To deal with such problem, an Attention-guided RGB-D Fusion Network (ARF-Net) is proposed in this work. Our key design is an ARF module that learns to adaptively fuse RGB and depth features with guidance from both structure-aware attention and relation-aware attention. Specifically, the structure-aware attention captures spatial relationship among object parts and the relation-aware attention captures the RGB-to-depth correlations between the appearance and geometric features. Our ARF -Net directly establishes canonical correspondences with a compact decoder based on the multi-modal features from our ARF module. Extensive experiments show that our method can effectively fuse RGB features to various popular point cloud encoders and provide consistent performance improvement. In particular, without reconstructing instance 3D models, our method with its relatively compact architecture outperforms all state-of-the-art models on CAMERA25 and REAL275 benchmarks by a large margin.},
  archive   = {C_IROS},
  author    = {Hao Wang and Weiming Li and Jiyeon Kim and Qiang Wang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981242},
  pages     = {10651-10658},
  title     = {Attention-guided RGB-D fusion network for category-level 6D object pose estimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DeepMLE: A robust deep maximum likelihood estimator for
two-view structure from motion. <em>IROS</em>, 10643–10650. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Two-view structure from motion (SfM) is the cornerstone of 3D reconstruction and visual SLAM (vSLAM). Many existing end-to-end learning-based methods usually formulate it as a brute regression problem. However, the inadequate utilization of traditional geometry model makes the model not robust in unseen environments. To improve the generalization capability and robustness of end-to-end two-view SfM network, we formulate the two-view SfM problem as a maximum likelihood estimation (MLE) and solve it with the proposed framework, denoted as DeepMLE. First, we propose to take the deep multi-scale correlation maps to depict the visual similarities of 2D image matches decided by ego-motion. In addition, in order to increase the robustness of our framework, we formulate the likelihood function of the correlations of 2D image matches as a Gaussian and Uniform mixture distribution which takes the uncertainty caused by illumination changes, image noise and moving objects into account. Meanwhile, an uncertainty prediction module is presented to predict the pixel-wise distribution parameters. Finally, we iteratively refine the depth and relative camera pose using the gradient-like information to maximize the likelihood function of the correlations. Extensive experimental results on several datasets prove that our method significantly outperforms the state-of-the-art end-to-end two-view SfM approaches in accuracy and generalization capability.},
  archive   = {C_IROS},
  author    = {Yuxi Xiao and Li Li and Xiaodi Li and Jian Yao},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981975},
  pages     = {10643-10650},
  title     = {DeepMLE: A robust deep maximum likelihood estimator for two-view structure from motion},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 3D object aided self-supervised monocular depth estimation.
<em>IROS</em>, 10635–10642. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Monocular depth estimation has been actively studied in fields such as robot vision, autonomous driving, and 3D scene understanding. Given a sequence of color images, unsupervised learning methods based on the framework of Structure-From-Motion (SfM) simultaneously predict depth and camera relative pose. However, dynamically moving objects in the scene violate the static world assumption, resulting in inaccurate depths of dynamic objects. In this work, we propose a new method to address such dynamic object movements through monocular 3D object detection. Specifically, we first detect 3D objects in the images and build the per-pixel correspondence of the dynamic pixels with the detected object pose while leaving the static pixels corresponding to the rigid background to be modeled with camera motion. In this way, the depth of every pixel can be learned via a meaningful geometry model. Besides, objects are detected as cuboids with absolute scale, which is used to eliminate the scale ambiguity problem inherent in monocular vision. Experiments on the KITTI depth dataset show that our method achieves State-of-The-Art performance for depth estimation. Furthermore, joint training of depth, camera motion and object pose also improves monocular 3D object detection performance. To the best of our knowledge, this is the first work that allows a monocular 3D object detection network to be fine-tuned in a self-supervised manner.},
  archive   = {C_IROS},
  author    = {Songlin Wei and Guodong Chen and Wenzheng Chi and Zhenhua Wang and Lining Sun},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981590},
  pages     = {10635-10642},
  title     = {3D object aided self-supervised monocular depth estimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CA-SpaceNet: Counterfactual analysis for 6D pose estimation
in space. <em>IROS</em>, 10627–10634. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reliable and stable 6D pose estimation of un-cooperative space objects plays an essential role in on-orbit servicing and debris removal missions. Considering that the pose estimator is sensitive to background interference, this paper proposes a counterfactual analysis framework named CA-SpaceNet to complete robust 6D pose estimation of the space-borne targets under complicated background. Specifically, conventional methods are adopted to extract the features of the whole image in the factual case. In the counterfactual case, a non-existent image without the target but only the background is imagined. Side effect caused by background interference is reduced by counterfactual analysis, which leads to unbiased prediction in final results. In addition, we also carry out low-bit-width quantization for CA-SpaceNet and deploy part of the framework to a Processing-In-Memory (PIM) accelerator on FPGA. Qualitative and quantitative results demonstrate the effectiveness and efficiency of our proposed method. To our best knowledge, this paper applies causal inference and network quantization to the 6D pose estimation of space-borne targets for the first time. The code is available at https://github.com/Shunli-Wang/CA-SpaceNet.},
  archive   = {C_IROS},
  author    = {Shunli Wang and Shuaibing Wang and Bo Jiao and Dingkang Yang and Liuzhen Su and Peng Zhai and Chixiao Chen and Lihua Zhang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981172},
  pages     = {10627-10634},
  title     = {CA-SpaceNet: Counterfactual analysis for 6D pose estimation in space},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Timestamp-supervised action segmentation with graph
convolutional networks. <em>IROS</em>, 10619–10626. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a novel approach for temporal activity segmentation with timestamp supervision. Our main contribution is a graph convolutional network, which is learned in an end-to-end manner to exploit both frame features and connections between neighboring frames to generate dense framewise labels from sparse timestamp labels. The gener-ated dense framewise labels can then be used to train the segmentation model. In addition, we propose a framework for alternating learning of both the segmentation model and the graph convolutional model, which first initializes and then iteratively refines the learned models. Detailed experiments on four public datasets, including 50 Salads, GTEA, Breakfast, and Desktop Assembly, show that our method is superior to the multi-layer perceptron baseline, while performing on par with or better than the state of the art in temporal activity segmentation with timestamp supervision.},
  archive   = {C_IROS},
  author    = {Hamza Khan and Sanjay Haresh and Awais Ahmed and Shakeeb Siddiqui and Andrey Konin and M. Zeeshan Zia and Quoc-Huy Tran},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981351},
  pages     = {10619-10626},
  title     = {Timestamp-supervised action segmentation with graph convolutional networks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MPT-net: Mask point transformer network for large scale
point cloud semantic segmentation. <em>IROS</em>, 10611–10618. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Point cloud semantic segmentation is important for road scene perception, a task for driverless vehicles to achieve full fledged autonomy. In this work, we introduce Mask Point Transformer Network (MPT-Net), a novel architecture for point cloud segmentation that is simple to implement. MPT-Net consists of a local and global feature encoder and a transformer based decoder; a 3D Point-Voxel Convolution encoder backbone with voxel self attention to encode features and a Mask Point Transformer module to decode point features and segment the point cloud. Firstly, we introduce the novel MPT designed to specifically handle point cloud segmentation. MPT offers two benefits. It attends to every point in the point cloud using mask tokens to extract class specific features globally with cross attention, and provide inter-class feature information exchange using self attention on the learned mask tokens. Secondly, we design a backbone to use sparse point voxel convolutional blocks and a self attention block using transformers to learn local and global contextual features. We evaluate MPT-Net on large scale outdoor driving scene point cloud datasets, SemanticKITTI and nuScenes. Our experiments show that by replacing the standard segmentation head with MPT, MPT-Net achieves a state-of-the-art performance over our baseline approach by 3.8\% in SemanticKITTI and is highly effective in detecting &#39;stuffs&#39; in point cloud.},
  archive   = {C_IROS},
  author    = {Zhe Jun Tang and Tat-Jen Cham},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981809},
  pages     = {10611-10618},
  title     = {MPT-net: Mask point transformer network for large scale point cloud semantic segmentation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards specialized hardware for learning-based visual
odometry on the edge. <em>IROS</em>, 10603–10610. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning-based visual odometry (VO) has gained increasing popularity in autonomous navigation of small robots. However, most methods in the category require computation resources not normally available on edge systems. We contend that specialized hardware accelerators are ideal solutions to this problem because of their superior energy efficiency. In this paper, we first propose a model to derive compute specifications for VO from physical characteristics of unmanned aerial vehicles (UAVs). These specifications serve as the basis to guide our accelerator design process. Based on the specifications derived from the DJI Mavic Air 2 and Crazyflie 2.0 UAVs, we explore the speed/flight-time design spaces for three target VO algorithms on two NVIDIA Jetson systems. Then, we propose a hardware accelerator architecture and present prototype implementations based on FPGAs. Additionally, we illustrate the algorithm/hardware co-design approach with a series of hardware-aware algorithmic redesigns targeting the FPGA prototypes, and quantify the throughput-accuracy tradeoff of them. Our FPGA implementation of DFVO is 2.7x more energy efficient compared to off-the-shelf embedded computers.},
  archive   = {C_IROS},
  author    = {Siyuan Chen and Ken Mai},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982046},
  pages     = {10603-10610},
  title     = {Towards specialized hardware for learning-based visual odometry on the edge},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Planning for negotiations in autonomous driving using
reinforcement learning. <em>IROS</em>, 10595–10602. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Planning autonomous driving behaviors in dense traffic is challenging. Human drivers are able to influence their road environment to achieve (otherwise unachievable) goals, by communicating their intents to other drivers. An autonomous system that is required to drive in the presence of human traffic must thus possess this fundamental negotiation capability. This work presents a novel benchmark that includes a stochastic driver negotiation model and a framework for training policies to drive and negotiate based on reinforcement learning. It is shown that driving policies trained in this framework lead to greater safety, higher mission accomplishment rates and more driving comfort, and can generalize across scenarios.},
  archive   = {C_IROS},
  author    = {Roi Reshef},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981988},
  pages     = {10595-10602},
  title     = {Planning for negotiations in autonomous driving using reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DSOL: A fast direct sparse odometry scheme. <em>IROS</em>,
10587–10594. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we describe Direct Sparse Odometry Lite (DSOL), an improved version of Direct Sparse Odometry (DSO) [1]. We propose several algorithmic and implementation enhancements which speed up computation by a significant factor (on average 5x) even on resource-constrained platforms. The increase in speed allows us to process images at higher frame rates, which in turn provides better results on rapid motions. Our open-source implementation is available at https://github.com/versatran01/dso1.},
  archive   = {C_IROS},
  author    = {Chao Qu and Shreyas S. Shivakumar and Ian D. Miller and Camillo J. Taylor},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981491},
  pages     = {10587-10594},
  title     = {DSOL: A fast direct sparse odometry scheme},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Depth-CUPRL: Depth-imaged contrastive unsupervised
prioritized representations in reinforcement learning for mapless
navigation of unmanned aerial vehicles. <em>IROS</em>, 10579–10586. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement Learning (RL) has presented an impressive performance in video games through raw pixel imaging and continuous control tasks. However, RL performs poorly with high-dimensional observations such as raw pixel images. It is generally accepted that physical state-based RL policies such as laser sensor measurements give a more sample-efficient result than learning by pixels. This work presents a new approach that extracts information from a depth map estimation to teach an RL agent to perform the mapless navigation of Unmanned Aerial Vehicle (UAV). We propose the Depth-Imaged Contrastive Unsupervised Prioritized Representations in Reinforcement Learning (Depth-CUPRL) that estimates the depth of images with a prioritized replay memory. We used a combination of RL and Contrastive Learning to lead with the problem of RL based on images. From the analysis of the results with Unmanned Aerial Vehicles (UAVs), it is possible to conclude that our Depth-CUPRL approach is effective for the decision-making and outperforms state-of-the-art pixel-based approaches in the mapless navigation capability.},
  archive   = {C_IROS},
  author    = {Junior C. de Jesus and Victor A. Kich and Alisson H. Kolling and Ricardo B. Grando and Rodrigo S. Guerra and Paulo L. J. Drews},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982161},
  pages     = {10579-10586},
  title     = {Depth-CUPRL: Depth-imaged contrastive unsupervised prioritized representations in reinforcement learning for mapless navigation of unmanned aerial vehicles},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Navigating to objects in unseen environments by distance
prediction. <em>IROS</em>, 10571–10578. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Object Goal Navigation (ObjectNav) task is to navigate an agent to an object category in unseen environments without a pre-built map. In this paper, we solve this task by predicting the distance to the target using semantically-related objects as cues. Based on the estimated distance to the target object, our method directly choose optimal midterm goals that are more likely to have a shorter path to the target. Specifically, based on the learned knowledge, our model takes a bird&#39;s-eye view semantic map as input, and estimates the path length from the frontier map cells to the target object. With the estimated distance map, the agent could simultaneously explore the environment and navigate to the target objects based on a simple human-designed strategy. Empirical results in visually realistic simulation environments show that the proposed method outperforms a wide range of baselines on success rate and efficiency. Real-robot experiment also demonstrates that our method generalizes well to the real world.},
  archive   = {C_IROS},
  author    = {Minzhao Zhu and Binglei Zhao and Tao Kong},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981766},
  pages     = {10571-10578},
  title     = {Navigating to objects in unseen environments by distance prediction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Temporal logic path planning under localization uncertainty.
<em>IROS</em>, 10563–10570. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a method to find the optimal control strategy for a robot using prior information of localization that maximizes the probability of satisfaction of a temporal logic specification while considering the uncertainty in both motion and sensing, two major causes for localization uncertainty. The specifications are given in the probabilistic computation tree logic (PCTL) formulae over a set of propositions, which capture the presence of the robot in some key locations in the environment. A computation model that can deal with the uncertainty in both motion and sensing is the Partially Observable Markov Decision Process (POMDP), which is computationally expensive. We approximate the underlying POMDP using Augmented Markov Decision Process (AMDP) and present a control synthesis algorithm for AMDP. We carry out numerous experiments on workspaces with sizes up to 100 × 100 and three different PCTL specifications to evaluate the efficacy of our technique. Experimental results show that our technique for computing robot control policy using localization prior can deal with localization uncertainty effectively and scale to large environments.},
  archive   = {C_IROS},
  author    = {Amit Dhyani and Indranil Saha},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981624},
  pages     = {10563-10570},
  title     = {Temporal logic path planning under localization uncertainty},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Contextual tuning of model predictive control for autonomous
racing. <em>IROS</em>, 10555–10562. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning-based model predictive control has been widely applied in autonomous racing to improve the closed-loop behaviour of vehicles in a data-driven manner. When environmental conditions change, e.g., due to rain, often only the predictive model is adapted, but the controller parameters are kept constant. However, this can lead to suboptimal behaviour. In this paper, we address the problem of data-efficient controller tuning, adapting both the model and objective simultaneously. The key novelty of the proposed approach is that we leverage a learned dynamics model to encode the environmental condition as a so-called context. This insight allows us to employ contextual Bayesian optimization to efficiently transfer knowledge across different environmental conditions. Consequently, we require fewer data to find the optimal controller configuration for each context. The proposed framework is extensively evaluated with more than 3&#39;000 laps driven on an experimental platform with 1:28 scale RC race cars. The results show that our approach successfully optimizes the lap time across different contexts requiring fewer data compared to other approaches based on standard Bayesian optimization.},
  archive   = {C_IROS},
  author    = {Lukas P. Fröhlich and Christian Küttel and Elena Arcari and Lukas Hewing and Melanie N. Zeilinger and Andrea Carron},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981780},
  pages     = {10555-10562},
  title     = {Contextual tuning of model predictive control for autonomous racing},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MPNP: Multi-policy neural planner for urban driving.
<em>IROS</em>, 10549–10554. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Our goal is to train a neural planner that can capture diverse driving behaviors in complex urban scenarios. We observe that even state-of-the-art neural planners are struggling to perform common maneuvers such as lane change, which is rather natural for human drivers. We propose to explore the multi-modalities in the planning problem and force the neural planner to explicitly consider different policies. This is achieved by generating the future trajectories conditioned on every possible reference line, which could simply be the centerline of the surrounding lanes. We find this simple strategy yet enables the planner to perform rich and complex behaviors. We train our model using real-world driving data and demonstrate the effectiveness of our method through both open-loop and closed-loop evaluations. Project website https://jchengai.github.io/mpnp.},
  archive   = {C_IROS},
  author    = {Jie Cheng and Ren Xin and Sheng Wang and Ming Liu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982111},
  pages     = {10549-10554},
  title     = {MPNP: Multi-policy neural planner for urban driving},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time visual inertial odometry with a resource-efficient
harris corner detection accelerator on FPGA platform. <em>IROS</em>,
10542–10548. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual Inertial Odometry (VIO) is a widely studied localization technique in robotics. State-of-the-art VIO algorithms are composed of two parts: a frontend which performs visual perception and inertial measurement pre-processing, and a backend which fuses vision and inertial measurements to estimate the robot&#39;s pose. Both image processing in the frontend and sensor fusion in the backend are computationally expensive, making it very challenging to run the VIO algorithm, especially the optimization-based VIO algorithm in real time on embedded platforms with limited power budget. In this paper, a real-time optimization-based monocular VIO algorithm is proposed based on algorithm-and-hardware co-design and successfully implemented on an embedded platform with only 2.6W processor power consumption. In particular, the time-consuming Harris corner detection (HCD) is accelerated on Field Programmable Gate Array (FPGA), achieving an average 16 × processing time reduction compared with the ARM implementation. Compared with the state-of-the-art HCD accelerator provided by Xilinx, the hardware resource required of our accelerator is largely reduced without any compromise in speed, thanks to the proposed dedicated pruning and paral-lelization techniques. Finally, experiment on the public dataset demonstrates that the proposed real-time VIO algorithm on the FPGA-based platform has comparable accuracy with respect to the existing state-of-the-art VIO algorithm on the desktop, and 3 × faster frontend processing speed over the ARM-based implementation.},
  archive   = {C_IROS},
  author    = {Pengfei Gu and Ziyang Meng and Pengkun Zhou},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981598},
  pages     = {10542-10548},
  title     = {Real-time visual inertial odometry with a resource-efficient harris corner detection accelerator on FPGA platform},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Autoexplorer: Autonomous exploration of unknown environments
using fast frontier-region detection and parallel path planning.
<em>IROS</em>, 10536–10541. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a fully autonomous system for mobile robot exploration in unknown environments. Our system employs a novel frontier detection algorithm based on the fast front propagation (FFP) technique and uses parallel path planning to reach the detected front regions. Given an occupancy grid map in 2D, possibly updated online, our algorithm can find all the frontier points that can allow mobile robots to visit unexplored regions to maximize the exploratory coverage. Our FFP method is six~seven times faster than the state-of-the-art wavefront frontier detection algorithm in terms of finding frontier points without compromising the detection accuracy. The speedup can be further accelerated by simplifying the map without degrading the detection accuracy. To expedite locating the optimal frontier point, We also eliminate spurious points by the obstacle filter and the novel boundary filter. In addition, we parallelize the global planning phase using the branch-and-bound A*, where the search space of each thread is confined by its best knowledge discovered during the parallel search. As a result, our parallel path-planning algorithm operating on 20 threads is about 30 times faster than the vanilla exploration system that operates on a single thread. Our method is validated through extensive experiments, including autonomous robot exploration in both synthetic and real-world scenarios. In the real-world experiment, we show that an autonomous navigation system using a human-sized mobile manipulator robot equipped with a low-end embedded processor that fully integrates our FFP and parallel path-planning algorithms.},
  archive   = {C_IROS},
  author    = {Kyung Min Han and Young J. Kim},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981263},
  pages     = {10536-10541},
  title     = {Autoexplorer: Autonomous exploration of unknown environments using fast frontier-region detection and parallel path planning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Obstacle avoidance of resilient UAV swarm formation with
active sensing system in the dense environment. <em>IROS</em>,
10529–10535. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a perception-shared and swarm trajectory global optimal (STGO) algorithm fused UAVs formation motion planning framework aided by an active sensing system. First, the point cloud received by each UAV is fit by the gaussian mixture model (GMM) and transmitted in the swarm. Resampling from the received GMM contributes to a global map, which is used as the foundation for consensus. Second, to improve flight safety, an active sensing system is designed to plan the observation angle of each UAV considering the unknown field, overlap of the field of view (FOV), velocity direction and smoothness of yaw rotation, and this planning problem is solved by the distributed particle swarm optimization (DPSO) algorithm. Last, for the formation motion planning, to ensure obstacle avoidance, the formation structure is allowed for affine transformation and is treated as the soft constraint on the control points of the B-spline. Besides, the STGO is introduced to avoid local minima. The combination of GMM communication and STGO guarantees a safe and strict consensus between UAVs. Tests on different formations in the simulation show that our algorithm can contribute to a strict consensus and has a success rate of at least 80\% for obstacle avoidance in a dense environment. Besides, the active sensing system can increase the success rate of obstacle avoidance from 50\% to 100\% in some scenarios.},
  archive   = {C_IROS},
  author    = {Peng Peng and Wei Dong and Gang Chen and Xiangyang Zhu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981858},
  pages     = {10529-10535},
  title     = {Obstacle avoidance of resilient UAV swarm formation with active sensing system in the dense environment},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic free-space roadmap for safe quadrotor motion
planning. <em>IROS</em>, 10523–10528. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Free-space-oriented roadmaps typically generate a series of convex geometric primitives, which constitute the safe region for motion planning. However, a static environment is assumed for this kind of roadmap. This assumption makes it unable to deal with dynamic obstacles and limits its applications. In this paper, we present a dynamic free-space roadmap, which provides feasible spaces and a navigation graph for safe quadrotor motion planning. Our roadmap is constructed by continuously seeding and extracting free regions in the environment. In order to adapt our map to environments with dynamic obstacles, we incrementally decompose the polyhedra intersecting with obstacles into obstacle-free regions, while the graph is also updated by our well-designed mechanism. Extensive simulations and real-world experiments demonstrate that our method is practically applicable and efficient.},
  archive   = {C_IROS},
  author    = {Junlong Guo and Zhiren Xun and Shuang Geng and Yi Lin and Chao Xu and Fei Gao},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981447},
  pages     = {10523-10528},
  title     = {Dynamic free-space roadmap for safe quadrotor motion planning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time trajectory planning for aerial perching.
<em>IROS</em>, 10516–10522. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel trajectory planning method for aerial perching. Compared with the existing work, the terminal states and the trajectory durations can be adjusted adaptively, instead of being determined in advance. Further-more, our planner is able to minimize the tangential relative speed on the premise of safety and dynamic feasibility. This feature is especially notable on micro aerial robots with low maneuverability or scenarios where the space is not enough. Moreover, we design a flexible transformation strategy to eliminate terminal constraints along with reducing optimization variables. Besides, we take precise SE(3) motion planning into account to ensure that the drone would not touch the landing platform until the last moment. The proposed method is validated onboard by a palm-sized micro aerial robot with quite limited thrust and moment (thrust-to-weight ratio 1.7) perching on a mobile inclined surface. Sufficient experimental results show that our planner generates an optimal trajectory within 20ms, and replans with warm start in 2ms.},
  archive   = {C_IROS},
  author    = {Jialin Ji and Tiankai Yang and Chao Xu and Fei Gao},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981489},
  pages     = {10516-10522},
  title     = {Real-time trajectory planning for aerial perching},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design and analysis of truss aerial transportation system
(TATS): The lightweight bar spherical joint mechanism. <em>IROS</em>,
10501–10507. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In aerial cooperative transportation missions, it has been recognized that for small-sized but heavy payloads, the cable-suspended framework is a preferred manner. However, to maintain proper safe flight distances, cables always stay inclined, which implies that horizontal force components have to be generated by UAVs, and only partial thrust forces are used for gravity compensation. To overcome this drawback, in this paper, a new cooperative transportation system named Truss Aerial Transportation System (TATS) is proposed, where those horizontal forces can be internally compensated by the bar spherical joint structure. In the TATS, rigid bars can powerfully sustain the desired distances among UAVs for safe flight, resulting in a more compact and effective transportation system. Thanks to the structural advantage of the truss, the rigid bars can be made lightweight so as to minimize their induced gravity burden. The construction method of the proposed TATS is presented. The improvement in energy efficiency is analyzed and compared with the cable-suspended framework. Furthermore, the robustness property of a TATS configuration is evaluated by computing the margin capacity. Finally, a load test experiment is conducted on our made prototype, the results of which show the effectiveness and feasibility of the proposed TATS.},
  archive   = {C_IROS},
  author    = {Xiaozhen Zhang and Qingkai Yang and Rui Yu and Delong Wu and Shaozhun Wei and Jinqiang Cui and Hao Fang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981191},
  pages     = {10501-10507},
  title     = {Design and analysis of truss aerial transportation system (TATS): The lightweight bar spherical joint mechanism},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsteady aerodynamic modeling of aerobat using lifting line
theory and wagner’s function. <em>IROS</em>, 10493–10500. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Flying animals possess highly complex physical characteristics and are capable of performing agile maneuvers using their wings. The flapping wings generate complex wake structures that influence the aerodynamic forces, which can be difficult to model. While it is possible to model these forces using fluidstructure interaction, it is very computationally expensive and difficult to formulate. In this paper, we follow a simpler approach by deriving the aerodynamic forces using a relatively small number of states and presenting them in a simple state-space form. The formulation utilizes Prandtl&#39;s lifting line theory and Wagner&#39;s function to determine the unsteady aerodynamic forces acting on the wing in a simulation, which then are compared to experimental data of the bat-inspired robot called the Aerobat. The simulated trailingedge vortex shedding can be evaluated from this model, which then can be analyzed for a wake-based gait design approach to improve the aerodynamic performance of the robot.},
  archive   = {C_IROS},
  author    = {Eric Sihite and Paul Ghanem and Adarsh Salagame and Alireza Ramezani},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982125},
  pages     = {10493-10500},
  title     = {Unsteady aerodynamic modeling of aerobat using lifting line theory and wagner&#39;s function},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Siamese object tracking for vision-based UAM approaching
with pairwise scale-channel attention. <em>IROS</em>, 10486–10492. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although the manipulating of the unmanned aerial manipulator (UAM) has been widely studied, vision-based UAM approaching, which is crucial to the subsequent manipulating, generally lacks effective design. The key to the visual UAM approaching lies in object tracking, while current UAM tracking typically relies on costly model-based methods. Besides, UAM approaching often confronts more severe object scale variation issues, which makes it inappro-priate to directly employ state-of-the-art model-free Siamese-based methods from the object tracking field. To address the above problems, this work proposes a novel Siamese network with pairwise scale-channel attention (SiamSA) for vision-based UAM approaching. Specifically, SiamSA consists of a pairwise scale-channel attention network (PSAN) and a scale-aware anchor proposal network (SA-APN). PSAN acquires valuable scale information for feature processing, while SA-APN mainly attaches scale awareness to anchor proposing. Moreover, a new tracking benchmark for UAM approaching, namely UAMT100, is recorded with 35K frames on a flying UAM platform for evaluation. Exhaustive experiments on the benchmarks and real-world tests validate the efficiency and practicality of SiamSA with a promising speed. Both the code and UAMT100 benchmark are now available at https://github.com/vision4robotics/SiamSA.},
  archive   = {C_IROS},
  author    = {Guangze Zheng and Changhong Fu and Junjie Ye and Bowen Li and Geng Lu and Jia Pan},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982189},
  pages     = {10486-10492},
  title     = {Siamese object tracking for vision-based UAM approaching with pairwise scale-channel attention},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Downwash-aware control allocation for over-actuated UAV
platforms. <em>IROS</em>, 10478–10485. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tracking position and orientation independently affords more agile maneuver for over-actuated multirotor Unmanned Aerial Vehicles (UAVs) while introducing undesired downwash effects; downwash flows generated by thrust generators may counteract others due to close proximity, which significantly threatens the stability of the platform. The complexity of modeling aerodynamic airflow challenges control algorithms from properly compensating for such a side effect. Leveraging the input redundancies in over-actuated UAVs, we tackle this issue with a novel control allocation framework that considers downwash effects and explores the entire allocation space for an optimal solution. This optimal solution avoids downwash effects while providing high thrust efficiency within the hardware constraints. To the best of our knowledge, ours is the first formal derivation to investigate the downwash effects on over-actuated UAVs. We verify our framework on different hardware configurations in both simulation and experiment.},
  archive   = {C_IROS},
  author    = {Yao Su and Chi Chu and Meng Wang and Jiarui Li and Liu Yang and Yixin Zhu and Hangxin Liu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981140},
  pages     = {10478-10485},
  title     = {Downwash-aware control allocation for over-actuated UAV platforms},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DUQIM-net: Probabilistic object hierarchy representation for
multi-view manipulation. <em>IROS</em>, 10470–10477. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Object manipulation in cluttered scenes is a difficult and important problem in robotics. To efficiently manipulate objects, it is crucial to understand their surroundings, especially in cases where multiple objects are stacked one on top of the other, preventing effective grasping. We here present DUQIM-Net, a decision-making approach for object manipulation in a setting of stacked objects. In DUQIM-Net, the hierarchical stacking relationship is assessed using Adj-Net, a model that leverages existing Transformer Encoder-Decoder object detectors by adding an adjacency head. The output of this head probabilistically infers the underlying hierarchical structure of the objects in the scene. We utilize the properties of the adjacency matrix in DUQIM-Net to perform decision making and assist with object-grasping tasks. Our experimental results show that Adj-Net surpasses the state-of-the-art in object-relationship inference on the Visual Manipulation Relationship Dataset (VMRD), and that DUQIM-Net outperforms comparable approaches in bin clearing tasks.},
  archive   = {C_IROS},
  author    = {Vladimir Tchuiev and Yakov Miron and Dotan Di Castro},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981406},
  pages     = {10470-10477},
  title     = {DUQIM-net: Probabilistic object hierarchy representation for multi-view manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simultaneous depth estimation and localization for cell
manipulation based on deep learning. <em>IROS</em>, 10432–10438. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual localization, which is a key technology to realize the automation of cell manipulation, has been widely studied. Since the depth of field of the microscope is narrow, the planar localization and depth estimation are usually coupled together. At present, most methods adopt the serial working mode of focusing first and then planar localization, but they usually do not have good real-time performance and stability. In this paper, a simultaneous depth estimation and localization network was developed for cell manipulation. The network takes a focused image and a defocus-offset image as inputs, and outputs the defocus in the depth direction and the offset in the plane at the same time after going through defocus-offset information extraction, defocus classification mapping and offset regression mapping. To train and test our network, we also create two datasets: An Adherent Cell dataset and an Injection Micropipette dataset. The experimental results demonstrated that the proposed method achieves the detection of all test samples with a frame rate of more than 40Hz, and the maximum errors of depth estimation and localization are $\boldsymbol{2.44\mu m}$ and $\boldsymbol{0.49\mu m}$ , respectively. The proposed method has good stability, which is mainly reflected in its strong generalization ability and anti-noise ability.},
  archive   = {C_IROS},
  author    = {Zengshuo Wang and Huiying Gong and Ke Li and Bin Yang and Yue Du and Yaowei Liu and Xin Zhao and Mingzhu Sun},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982228},
  pages     = {10432-10438},
  title     = {Simultaneous depth estimation and localization for cell manipulation based on deep learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward efficient task planning for dual-arm tabletop object
rearrangement. <em>IROS</em>, 10425–10431. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate the problem of coordinating two robot arms to solve non-monotone tabletop multi-object re- arrangement tasks. In a non-monotone rearrangement task, complex object-object dependencies exist that require moving some objects multiple times to solve an instance. In working with two arms in a large workspace, some objects must be handed off between the robots, which further complicates the planning process. For the challenging dual-arm tabletop rearrangement problem, we develop effective task planning algorithms for scheduling the pick-n-place sequence that can be properly distributed between the two arms. We show that, even without using a sophisticated motion planner, our method achieves significant time savings in comparison to greedy approaches and naive parallelization of single-robot plans.},
  archive   = {C_IROS},
  author    = {Kai Gao and Jingjin Yu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981715},
  pages     = {10425-10431},
  title     = {Toward efficient task planning for dual-arm tabletop object rearrangement},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Consensus-based normalizing-flow control: A case study in
learning dual-arm coordination. <em>IROS</em>, 10417–10424. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We develop two consensus-based learning algorithms for multi-robot systems applied on complex tasks involving collision constraints and force interactions, such as the cooperative peg-in-hole placement. The proposed algorithms integrate multi-robot distributed consensus and normalizing-flow-based reinforcement learning. The algorithms guarantee the stability and the consensus of the multi-robot system&#39;s generalized variables in a transformed space. This transformed space is obtained via a diffeomorphic transformation parameterized by normalizing-flow models that the algorithms use to train the underlying task, learning hence skillful, dexterous trajectories required for the task accomplishment. We validate the proposed algorithms by parameterizing reinforcement learning policies, demonstrating efficient cooperative learning, and strong generalization of dual-arm assembly skills in a dynamics-engine simulator.},
  archive   = {C_IROS},
  author    = {Hang Yin and Christos K. Verginis and Danica Kragic},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981827},
  pages     = {10417-10424},
  title     = {Consensus-based normalizing-flow control: A case study in learning dual-arm coordination},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards learning to play piano with dexterous hands and
touch. <em>IROS</em>, 10410–10416. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As Liszt once said “(a virtuoso) must call up scent and blossom, and breathe the breath of life”, a virtuoso plays the piano with passion, poetry, and extraordinary technical ability. Hence, piano playing, being a task that is quintessentially human, becomes a hallmark for roboticians and artificial intelligence researchers to pursue. In this paper, we advocate an end-to-end reinforcement learning (RL) paradigm to demonstrate how an agent can learn directly from machine-readable music score to play the piano with touch-augmented dexterous hands on a simulated piano. To achieve the desired tasks, we design useful touch- and audio-based reward functions and a series of tasks. Empirical results show that the RL agent can not only find the correct key position but also deal with the various rhythmic, volume, and fingering requirements. As a result, the agent demonstrates its effectiveness in playing simple pieces that have different musical requirements which show the potential of leveraging reinforcement learning approach for the piano playing tasks.},
  archive   = {C_IROS},
  author    = {Huazhe Xu and Yuping Luo and Shaoxiong Wang and Trevor Darrell and Roberto Calandra},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981221},
  pages     = {10410-10416},
  title     = {Towards learning to play piano with dexterous hands and touch},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time digital double framework to predict collapsible
terrains for legged robots. <em>IROS</em>, 10387–10394. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inspired by the digital twinning systems, a novel real-time digital double framework is developed to enhance robot perception of the terrain conditions. Based on the very same physical model and motion control, this work exploits the use of such simulated digital double synchronized with a real robot to capture and extract discrepancy information between the two systems, which provides high dimensional cues in multiple physical quantities to represent differences between the modelled and the real world. Soft, non-rigid terrains cause common failures in legged locomotion, whereby visual perception solely is insufficient in estimating such physical properties of terrains. We used digital double to develop the estimation of the collapsibility, which addressed this issue through physical interactions during dynamic walking. The discrepancy in sensory measurements between the real robot and its digital double are used as input of a learning-based algorithm for terrain collapsibility analysis. Although trained only in simulation, the learned model can perform collapsibility estimation successfully in both simulation and real world. Our evaluation of results showed the generalization to different scenarios and the advantages of the digital double to reliably detect nuances in ground conditions.},
  archive   = {C_IROS},
  author    = {Garen Haddeler and Hari P. Palanivelu and Yung Chuen Ng and Fabien Colonnier and Albertus H. Adiwahono and Zhibin Li and Chee-Meng Chew and Meng Yee Chuah},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981613},
  pages     = {10387-10394},
  title     = {Real-time digital double framework to predict collapsible terrains for legged robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust high-speed running for quadruped robots via deep
reinforcement learning. <em>IROS</em>, 10364–10370. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep reinforcement learning has emerged as a popular and powerful way to develop locomotion controllers for quadruped robots. Common approaches have largely focused on learning actions directly in joint space, or learning to modify and offset foot positions produced by trajectory generators. Both approaches typically require careful reward shaping and training for millions of time steps, and with trajectory generators introduce human bias into the resulting control policies. In this paper, we present a learning framework that leads to the natural emergence of fast and robust bounding policies for quadruped robots. The agent both selects and controls actions directly in task space to track desired velocity commands subject to environmental noise including model uncertainty and rough terrain. We observe that this framework improves sample efficiency, necessitates little reward shaping, leads to the emergence of natural gaits such as galloping and bounding, and eases the sim-to-real transfer at running speeds. Policies can be learned in only a few million time steps, even for challenging tasks of running over rough terrain with loads of over 100\% of the nominal quadruped mass. Training occurs in PyBullet, and we perform a sim-to-sim transfer to Gazebo and sim-to-real transfer to the Unitree A1 hardware. For sim-to-sim, our results show the quadruped is able to run at over 4 m/s without a load, and 3.5 m/s with a 10 kg load, which is over 83\% of the nominal quadruped mass. For sim-to-real, the Unitree A1 is able to bound at 2 m/s with a 5 kg load, representing 42\% of the nominal quadruped mass.},
  archive   = {C_IROS},
  author    = {Guillaume Bellegarda and Yiyu Chen and Zhuochen Liu and Quan Nguyen},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982132},
  pages     = {10364-10370},
  title     = {Robust high-speed running for quadruped robots via deep reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Motion planning for agile legged locomotion using failure
margin constraints. <em>IROS</em>, 10350–10355. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The complex dynamics of agile robotic legged locomotion requires motion planning to intelligently adjust footstep locations. Often, bipedal footstep and motion planning use mathematically simple models such as the linear inverted pendulum, instead of dynamically-rich models that do not have closed-form solutions. We propose a real-time optimization method to plan for dynamical models that do not have closed form solutions and experience irrecoverable failure. Our method uses a data-driven approximation of the step-to-step dynamics and of a failure margin function. This failure margin function is an oriented distance function in state-action space where it describes the signed distance to success or failure. The motion planning problem is formed as a nonlinear program with constraints that enforce the approximated forward dynamics and the validity of state-action pairs. For illustration, this method is applied to create a planner for an actuated spring-loaded inverted pendulum model. In an ablation study, the failure margin constraints decreased the number of invalid solutions by between 24 and 47 percentage points across different objectives and horizon lengths. While we demonstrate the method on a canonical model of locomotion, we also discuss how this can be applied to data-driven models and full-order robot models.},
  archive   = {C_IROS},
  author    = {Kevin Green and John Warila and Ross L. Hatton and Jonathan Hurst},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981903},
  pages     = {10350-10355},
  title     = {Motion planning for agile legged locomotion using failure margin constraints},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A versatile co-design approach for dynamic legged robots.
<em>IROS</em>, 10343–10349. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a versatile framework for the computational co-design of legged robots and dynamic maneuvers. Current state-of-the-art approaches are typically based on random sampling or concurrent optimization. We propose a novel bilevel optimization approach that exploits the derivatives of the motion planning sub-problem (i.e., the lower level). These motion-planning derivatives allow us to incorporate arbitrary design constraints and costs in an general-purpose nonlinear program (i.e., the upper level). Our approach allows for the use of any differentiable motion planner in the lower level and also allows for an upper level that captures arbitrary design constraints and costs. It efficiently optimizes the robot&#39;s morphology, payload distribution and actuator parameters while considering its full dynamics, joint limits and physical constraints such as friction cones. We demonstrate these capabilities by designing quadruped robots that jump and trot. We show that our method is able to design a more energy-efficient Solo robot for these tasks.},
  archive   = {C_IROS},
  author    = {Traiko Dinev and Carlos Mastalli and Vladimir Ivan and Steve Tonneau and Sethu Vijayakumar},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981378},
  pages     = {10343-10349},
  title     = {A versatile co-design approach for dynamic legged robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning coordinated terrain-adaptive locomotion by
imitating a centroidal dynamics planner. <em>IROS</em>, 10335–10342. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a simple imitation learning procedure for learning locomotion controllers that can walk over very challenging terrains. We use trajectory optimization (TO) to produce a large dataset of trajectories over procedurally generated terrains and use Reinforcement Learning (RL) to imitate these trajectories. We demonstrate with a realistic model of the ANYmal robot that the learned controllers transfer to unseen terrains and provide an effective initialization for fine-tuning on challenging terrains that require exteroception and precise foot placements. Our setup combines TO and RL in a simple fashion that overcomes the computational limitations and need for a robust tracking controller of the former and the exploration and reward-tuning difficulties of the latter.},
  archive   = {C_IROS},
  author    = {Philémon Brakel and Steven Bohez and Leonard Hasenclever and Nicolas Heess and Konstantinos Bousmalis},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981648},
  pages     = {10335-10342},
  title     = {Learning coordinated terrain-adaptive locomotion by imitating a centroidal dynamics planner},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). T-PRM: Temporal probabilistic roadmap for path planning in
dynamic environments. <em>IROS</em>, 10320–10327. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sampling-based motion planners are widely used in robotics due to their simplicity, flexibility and computational efficiency. However, in their most basic form, these algorithms operate under the assumption of static scenes and lack the ability to avoid collisions with dynamic (i.e. moving) obstacles. This raises safety concerns, limiting the range of possible applications of mobile robots in the real world. Motivated by these challenges, in this work we present Temporal-PRM, a novel sampling-based path-planning algorithm that performs obstacle avoidance in dynamic environments. The proposed approach extends the original Probabilistic Roadmap (PRM) with the notion of time, generating an augmented graph-like structure that can be efficiently queried using a time-aware variant of the A* search algorithm, also introduced in this paper. Our design maintains all the properties of PRM, such as the ability to perform multiple queries and to find smooth paths, while circumventing its downside by enabling collision avoidance in highly dynamic scenes with a minor increase in the computational cost. Through a series of challenging experiments in highly cluttered and dynamic environments, we demonstrate that the proposed path planner outperforms other state-of-the-art sampling-based solvers. Moreover, we show that our algorithm can run onboard a flying robot, performing obstacle avoidance in real time.},
  archive   = {C_IROS},
  author    = {Matthias Hüppi and Luca Bartolomei and Ruben Mascaro and Margarita Chli},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981739},
  pages     = {10320-10327},
  title     = {T-PRM: Temporal probabilistic roadmap for path planning in dynamic environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NMPC-LBF: Nonlinear MPC with learned barrier function for
decentralized safe navigation of multiple robots in unknown
environments. <em>IROS</em>, 10297–10303. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a decentralized control approach based on a Nonlinear Model Predictive Control (NMPC) method that employs barrier certificates for safe navigation of multiple nonholonomic wheeled mobile robots in unknown environments with static and/or dynamic obstacles. This method incorporates a Learned Barrier Function (LBF) into the NMPC design in order to guarantee safe robot navigation, i.e., prevent robot collisions with other robots and the obstacles. We refer to our proposed control approach as NMPC-LBF. Since each robot does not have a priori knowledge about the obstacles and other robots, we use a Deep Neural Network (DeepNN) running in real-time on each robot to learn the Barrier Function (BF) only from the robot&#39;s LiDAR and odometry measurements. The DeepNN is trained to learn the BF that separates safe and unsafe regions. We implemented our proposed method on simulated and actual Turtlebot3 Burger robot(s) in different scenarios. The implementation results show the effectiveness of the NMPC-LBF method at ensuring safe navigation of the robots.},
  archive   = {C_IROS},
  author    = {Amir Salimi Lafmejani and Spring Berman and Georgios Fainekos},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981177},
  pages     = {10297-10303},
  title     = {NMPC-LBF: Nonlinear MPC with learned barrier function for decentralized safe navigation of multiple robots in unknown environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning enabled fast planning and control in dynamic
environments with intermittent information. <em>IROS</em>, 10290–10296.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses a safe planning and control problem for mobile robots operating in communication- and sensor-limited dynamic environments. In this case the robots cannot sense the objects around them and must instead rely on intermittent, external information about the environment, as e.g., in underwater applications. The challenge in this case is that the robots must plan using only this stale data, while accounting for any noise in the data or uncertainty in the environment. To address this challenge we propose a compositional technique which leverages neural networks to quickly plan and control a robot through crowded and dynamic environments using only intermittent information. Specifically, our tool uses reachability analysis and potential fields to train a neural network that is capable of generating safe control actions. We demonstrate our technique both in simulation with an underwater vehicle crossing a crowded shipping channel and with real experiments with ground vehicles in communication-and sensor-limited environments.},
  archive   = {C_IROS},
  author    = {Matthew Cleaveland and Esen Yel and Yiannis Kantaros and Insup Lee and Nicola Bezzo},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981508},
  pages     = {10290-10296},
  title     = {Learning enabled fast planning and control in dynamic environments with intermittent information},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast 3D sparse topological skeleton graph generation for
mobile robot global planning. <em>IROS</em>, 10283–10289. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, mobile robots are becoming ambitious and deployed in large-scale scenarios. Serving as a high-level understanding of environments, a sparse skeleton graph is beneficial for more efficient global planning. Currently, existing solutions for skeleton graph generation suffer from several major limitations, including poor adaptiveness to different map representations, dependency on robot inspection trajectories and high computational overhead. In this paper, we propose an efficient and flexible algorithm generating a trajectory-independent 3D sparse topological skeleton graph capturing the spatial structure of the free space. In our method, an efficient ray sampling and validating mechanism are adopted to find distinctive free space regions, which contributes to skeleton graph vertices, with traversability between adjacent vertices as edges. A cycle formation scheme is also utilized to maintain skeleton graph compactness. Benchmark comparison with state-of-the-art works demonstrates that our approach generates sparse graphs in a substantially shorter time, giving high-quality global planning paths. Experiments conducted in real-world maps further validate the capability of our method in real-world scenarios. Our method will be made open source to benefit the community.},
  archive   = {C_IROS},
  author    = {Xinyi Chen and Boyu Zhou and Jiarong Lin and Yichen Zhang and Fu Zhang and Shaojie Shen},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981397},
  pages     = {10283-10289},
  title     = {Fast 3D sparse topological skeleton graph generation for mobile robot global planning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Collision and rollover-free g2 path planning for mobile
manipulation. <em>IROS</em>, 10275–10282. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a path planning refinement technique that allows the efficient collision and rollover-free motion planning for mobile manipulator robots working on rough terrain. First, the necessary theoretical background on a mobile manipulator&#39;s kinematics and dynamic stability measure is introduced. Then, after the brief introduction of the sampling-based path planning problem, the additional refinement stage and its problem formulation will be introduced. Within the refinement stage, the novel Bézier control point addition method is introduced to allow for fast, collision and rollover-free path smoothing using curvature-continuous parametrized curves. Analytical proofs and simulated comparisons are provided in the paper to show effectiveness. The beneficial effect of the refined path on trajectory planning will also be demonstrated through simulation.},
  archive   = {C_IROS},
  author    = {Jiazhi Song and Inna Sharf},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981151},
  pages     = {10275-10282},
  title     = {Collision and rollover-free g2 path planning for mobile manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast-replanning motion control for non-holonomic vehicles
with aborting a*. <em>IROS</em>, 10267–10274. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomously driving vehicles must be able to navigate in dynamic and unpredictable environments in a collision-free manner. So far, this has only been partially achieved in driverless cars and warehouse installations where marked structures such as roads, lanes, and traffic signs simplify the motion planning and collision avoidance problem. We are presenting a new control approach for car-like vehicles that is based on an unprecedentedly fast-paced A* implementation that allows the control cycle to run at a frequency of 30 Hz. This frequency enables us to place our A* algorithm as a low-level replanning controller that is well suited for navigation and collision avoidance in virtually any dynamic environment. Due to an efficient heuristic consisting of rotate-translate-rotate motions laid out along the shortest path to the target, our Short-Term Aborting A* (STAA*) converges fast and can be aborted early in order to guarantee a high and steady control rate. While our STAA* expands states along the shortest path, it takes care of collision checking with the environment including predicted states of moving obstacles, and returns the best solution found when the computation time runs out. Despite the bounded computation time, our STAA* does not get trapped in corners due to the following of the shortest path. In simulated and real-robot experiments, we demonstrate that our control approach eliminates collisions almost entirely and is superior to an improved version of the Dynamic Window Approach with predictive collision avoidance capabilities [1].},
  archive   = {C_IROS},
  author    = {Marcell Missura and Arindam Roychoudhury and Maren Bennewitz},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981663},
  pages     = {10267-10274},
  title     = {Fast-replanning motion control for non-holonomic vehicles with aborting a*},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A general method for autonomous assembly of arbitrary parts
in the presence of uncertainty. <em>IROS</em>, 10259–10266. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel and general method for autonomous robotic assembly of arbitrary and complex-shaped parts in the presence of 6-dimensional uncertainty. When a nominal assembly motion of the robot holding a part is stopped by contact due to uncertainty, our method finds the best estimate for the uncertainty and the contact configuration of the part based on sensed force/torque and uses that information to find a more accurate estimate of the goal configuration to guide a recovery motion of the part. It is based on a general, surface-based sphere tree representation of parts, a constrained optimization strategy to find the best estimate of the contact configuration under an uncertainty estimate, and a learned force/torque calibration model to relate computed force/torque and the sensed real force/torque. The method is applied and evaluated on different complex-shaped multi-peg-in-hole tasks. The results show that our method can achieve successful assembly with the presence of realistic 6-D uncertainties more than 10 times of the tight task clearances in terms of orientation clearance $(&amp;lt; 0.015rad)$ and position clearance $(&amp;lt; 1.5mm)$ , in all the test cases.},
  archive   = {C_IROS},
  author    = {Shichen Cao and Jing Xiao},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981276},
  pages     = {10259-10266},
  title     = {A general method for autonomous assembly of arbitrary parts in the presence of uncertainty},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Flexible and precision snap-fit peg-in-hole assembly based
on multiple sensations and damping identification. <em>IROS</em>,
10252–10258. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Snap-fit peg-in-hole assembly widely exists in both industry and daily life, especially for consumer electronics. The buckle mechanism leads to a damping zone inside the port where insertion force needs to be increased. It is much difficult to automate this process by robots, for size and clearance of the components are always small, and the damping buckle should be perceived and distinguished from solid inner walls of the port. End-effector position control might be invalid, since grasping error will make it difficult to locate the plug accurately. In this article, we undertake this assembly challenge by taking advantage of fingertip tactile perception combined with visual images and force feedback. Raw sensor data is collected, processed, and fused together to be state input of a reinforcement learning network, generating continuous action vectors. We also propose a novel damping zone predictor through feature extraction and multimodal fusion, which is able to identify whether the plug has touched the buckle mechanism, so as to adjust the insertion force. The whole framework is implemented through a common USB Type-C insertion experiment on Franka Panda robot platform, reaching a success rate of 88\%. Furthermore, system robustness is verified, and comparisons of different modalities are also conducted.},
  archive   = {C_IROS},
  author    = {Ruikai Liu and Xiansheng Yang and Ajian Li and Yunjiang Lou},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981639},
  pages     = {10252-10258},
  title     = {Flexible and precision snap-fit peg-in-hole assembly based on multiple sensations and damping identification},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hierarchical finite-state machine-based task allocation
framework for human-robot collaborative assembly tasks. <em>IROS</em>,
10238–10244. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Work-related musculoskeletal disorders (MSD) are one of the major cause of injuries and absenteeism at work. These lead to important cost in the manufacturing industry. Human-robot collaboration can help decreasing this issue by appropriately distributing the tasks and decreasing the workload of the factory worker. This paper proposes a novel generic task allocation approach based on hierarchical finite-state machines for human-robot assembly tasks. The developed framework decomposes first the main task into sub-tasks modelled as state machines. Based on capabilities considerations, workload, and performance estimations, the task allocator assigns the sub-task to human or robot agent. The algorithm was validated on the assembly of a crusher unit of a smoothie machine using the collaborative Franka Emika Panda robot and showed promising results in terms of productivity thanks to task parallelization, with improvement of more than 30\% of the total assembly time with respect to a collaborative scenario, where the agents perform the tasks sequentially.},
  archive   = {C_IROS},
  author    = {Ilias El Makrini and Mohsen Omidi and Fabio Fusaro and Edoardo Lamon and Arash Ajoudani and Bram Vandcrborght},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981618},
  pages     = {10238-10244},
  title     = {A hierarchical finite-state machine-based task allocation framework for human-robot collaborative assembly tasks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Coordinated toolpath planning for multi-extruder additive
manufacturing. <em>IROS</em>, 10230–10237. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a new algorithm for coordinating the motion of multiple extruders to increase throughput in fused filament fabrication (FFF)/fused deposition modeling (FDM) additive manufacturing. Platforms based on FFF are commonly available and advantageous to several industries, but are limited by slow fabrication time and could be could be significantly improved through efficient use of multiple extruders. We propose the coordinated toolpath planning problem for systems of extruders mounted as end-effectors on robot arms with the objective of maximizing utilization and avoiding collisions. Building on the idea of dependency graphs introduced in our earlier work, we develop a planning and control framework that precomputes a set of multi-layer toolpath segments from the input model and efficiently assigns them to individual extruders such that executed toolpaths are collision-free. Our method overcomes key limitations of existing methods, including utilization loss from workspace partitioning, precomputed toolpaths subject to collisions with the partially fabricated object, and wasted motion resulting from strict layer-by-layer fabrication. We report simulation results that show a major increase in utilization compared to single and multi-extruder methods, and favorable fabrication results using commodity hardware that demonstrate the feasibility of our method in practice.},
  archive   = {C_IROS},
  author    = {Jayant Khatkar and Chanyeol Yool and Robert Fitch and Lee Clemon and Ramgopal Mettu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981543},
  pages     = {10230-10237},
  title     = {Coordinated toolpath planning for multi-extruder additive manufacturing},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Assembly planning from observations under physical
constraints. <em>IROS</em>, 10223–10229. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the problem of copying an unknown assembly of primitives with known shape and appearance using information extracted from a single photograph by an off-the-shelf procedure for object detection and pose estimation. The proposed algorithm uses a simple combination of physical stability constraints, convex optimization and Monte Carlo tree search to plan assemblies as sequences of pick-and-place operations represented by STRIPS operators. It is efficient and, most importantly, robust to the errors in object detection and pose estimation unavoidable in any real robotic system. The proposed approach is demonstrated with thorough experiments on a UR5 manipulator.},
  archive   = {C_IROS},
  author    = {Thomas Chabal and Robin Strudel and Etienne Arlaud and Jean Ponce and Cordelia Schmid},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981623},
  pages     = {10223-10229},
  title     = {Assembly planning from observations under physical constraints},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph-based reinforcement learning meets mixed integer
programs: An application to 3D robot assembly discovery. <em>IROS</em>,
10215–10222. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot assembly discovery (RAD) is a challenging problem that lives at the intersection of resource allocation and motion planning. The goal is to combine a predefined set of objects to form something new while considering task execution with the robot-in-the-loop. In this work, we tackle the problem of building arbitrary, predefined target structures entirely from scratch using a set of Tetris-like building blocks and a robotic manipulator. Our novel hierarchical approach aims at efficiently decomposing the overall task into three feasible levels that benefit mutually from each other. On the high level, we run a classical mixed-integer program for global optimization of block-type selection and the blocks&#39; final poses to recreate the desired shape. Its output is then exploited to efficiently guide the exploration of an underlying reinforcement learning (RL) policy. This RL policy draws its generalization properties from a flexible graph-based representation that is learned through Q-learning and can be refined with search. Moreover, it accounts for the necessary conditions of structural stability and robotic feasibility that cannot be effectively reflected in the previous layer. Lastly, a grasp and motion planner transforms the desired assembly commands into robot joint movements. We demonstrate our proposed method&#39;s performance on a set of competitive simulated RAD environments, showcase real-world transfer, and report performance and robustness gains compared to an unstructured end-to-end approach.},
  archive   = {C_IROS},
  author    = {Niklas Funk and Svenja Menzenbach and Georgia Chalvatzaki and Jan Peters},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981784},
  pages     = {10215-10222},
  title     = {Graph-based reinforcement learning meets mixed integer programs: An application to 3D robot assembly discovery},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On CAD informed adaptive robotic assembly. <em>IROS</em>,
10207–10214. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a robotic assembly system that streamlines the design-to-make workflow for going from a CAD model of a product assembly to a fully programmed and adaptive assembly process. Our system captures (in the CAD tool) the intent of the assembly process for a specific robotic workcell and generates a recipe of task-level instructions. By integrating visual sensing with deep-learned perception models, the robots infer the necessary actions to assemble the design from the generated recipe. The perception models are trained directly from simulation, allowing the system to identify various parts based on CAD information. We demonstrate the system with a workcell of two robots to assemble interlocking 3D part designs. We first build and tune the assembly process in simulation, verifying the generated recipe. Finally, the real robotic workcell assembles the design using the same behavior.},
  archive   = {C_IROS},
  author    = {Yotto Koga and Heather Kerrick and Sachin Chitta},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982242},
  pages     = {10207-10214},
  title     = {On CAD informed adaptive robotic assembly},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Additive manufacturing for tissue engineering applications
in a temperature-controlled environment. <em>IROS</em>, 10201–10206. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, with the combination of tissue engineering and additive manufacturing technologies, the possibility of fabricating scaffolds with porosity and complex structure has been improved. Since the properties of most biomaterial inks are influenced by temperature and thereby affect the quality of the scaffolds, a controlled printing environment is very important. This study focuses on temperature monitoring from the nozzle to the working platform. A compact heating jacket is developed to heat the needle and sense its temperature inside the nozzle. It makes it very different from common cartridge heating mechanisms. Moreover, a semi-closed printing environment composed of an air curtain and temperature circulation device is developed to create a stable cooling environment. It improves the uniformity of the work platform and increases by 50\% the cooling time efficiency. To demonstrate the robustness for a wide range of temperatures, this study presents two experiments of printing two biomaterial inks at body and low temperatures, respectively.},
  archive   = {C_IROS},
  author    = {Wei-Chih Tseng and Chao-Yaug Liao and Bo-Ren Chen and Luc Chassagne and Barthélemy Cagneau},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981836},
  pages     = {10201-10206},
  title     = {Additive manufacturing for tissue engineering applications in a temperature-controlled environment},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analysis of randomization effects on Sim2Real transfer in
reinforcement learning for robotic manipulation tasks. <em>IROS</em>,
10193–10200. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Randomization is currently a widely used approach in Sim2Real transfer for data-driven learning algorithms in robotics. Still, most Sim2Real studies report results for a specific randomization technique and often on a highly customized robotic system, making it difficult to evaluate different randomization approaches systematically. To address this problem, we define an easy-to-reproduce experimental setup for a robotic reach-and-balance manipulator task, which can serve as a benchmark for comparison. We compare four randomization strategies with three randomized parameters both in simulation and on a real robot. Our results show that more randomization helps in Sim2Real transfer, yet it can also harm the ability of the algorithm to find a good policy in simulation. Fully randomized simulations and fine-tuning show differentiated results and translate better to the real robot than the other approaches tested.},
  archive   = {C_IROS},
  author    = {Josip Josifovski and Mohammadhossein Malmir and Noah Klarmann and Bare Luka Žagar and Nicolás Navarro-Guerrero and Alois Knoll},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981951},
  pages     = {10193-10200},
  title     = {Analysis of randomization effects on Sim2Real transfer in reinforcement learning for robotic manipulation tasks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-supervised noisy label learning for source-free
unsupervised domain adaptation. <em>IROS</em>, 10185–10192. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Domain adaptation is an important property in robot vision, which enables the neural networks pre-trained on source domains to adapt target domains automatically without any annotation efforts. During this process, source data is not always accessible due to the constraints of expensive storage overhead and data privacy protection. Therefore, the source domain pre-trained model is expected to optimize with only unlabeled target data, termed as source-free unsupervised domain adaptation. In this paper, we view this problem as a special case of noisy label learning, since the given pre-trained model can generate noisy labels for unlabeled target data via network inference. The potential semantic cues for unsupervised domain adaptation exactly lie on these noisy labels. Inspired by this problem modeling, we propose a simple yet effective Self-Supervised Noisy Label Learning method, which injects self-supervised learning to impose the intrinsic data structure and facilitate label-denoising. Extensive experiments have been conducted on diverse benchmarks to validate the effectiveness. Our method achieves state-of-the-art performance.},
  archive   = {C_IROS},
  author    = {Weijie Chen and Luojun Lin and Shicai Yang and Di Xie and Shiliang Pu and Yueting Zhuang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981099},
  pages     = {10185-10192},
  title     = {Self-supervised noisy label learning for source-free unsupervised domain adaptation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bilateral knowledge distillation for unsupervised domain
adaptation of semantic segmentation. <em>IROS</em>, 10177–10184. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unsupervised domain adaptation (UDA) aims to learn domain-invariant representations between the labeled source domain and the unlabeled target domain. Existing self- training-based UDA methods use ground truth and pseudo- labels to supervise source data and target data respectively. However, strong supervision in the source domain and pseudo- label noise in the target domain lead to some problems, such as biased predictions and over-fitting. To tackle these issues, we propose a novel Bilateral Knowledge Distillation (BKD) framework for UDA in semantic segmentation, which adopts different knowledge distillation strategies depending on the domain. Specifically, we first introduce a Source-Flow Distillation (SD) to smooth the labels of source images, which weakens the supervision in the source domain. Meanwhile, a Target-Flow Distillation (TD) is designed to extract the inter- class knowledge in the probability map output from the teacher model, which alleviates the influence of pseudo-label noise in the target domain. Considering the class imbalance in semantic segmentation, we further propose an Image-Wise Hard Pixel Mining (HPM) to address this issue without estimating class frequency in the unlabeled target domain. The effectiveness of our framework against existing state-of-the-art methods is demonstrated by extensive experiments on two benchmarks: GTA5-to-Cityscapes and SYNTHIA-to-Cityscapes.},
  archive   = {C_IROS},
  author    = {Yunnan Wang and Jianxun Li},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981567},
  pages     = {10177-10184},
  title     = {Bilateral knowledge distillation for unsupervised domain adaptation of semantic segmentation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Using simulation optimization to improve zero-shot policy
transfer of quadrotors. <em>IROS</em>, 10170–10176. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we propose a data-driven approach to optimize the parameters of a simulation such that control policies can be directly transferred from simulation to a real-world quadrotor. Our neural network-based policies take only onboard sensor data as input and run entirely on the embed-ded hardware. In real-world experiments, we compare low-level Pulse-Width Modulated control with higher-level control structures such as Attitude Rate and Attitude, which utilize Proportional-Integral-Derivative controllers to output motor commands. Our experiments show that low-level controllers trained with Reinforcement Learning require a more accurate simulation than higher-level control policies at the expense of being less robust towards parameter uncertainties.},
  archive   = {C_IROS},
  author    = {Sven Gronauer and Matthias Kissel and Luca Sacchetto and Mathias Korte and Klaus Diepold},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981229},
  pages     = {10170-10176},
  title     = {Using simulation optimization to improve zero-shot policy transfer of quadrotors},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Subspace-based feature alignment for unsupervised domain
adaptation. <em>IROS</em>, 10163–10169. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous agents need to perceive the world in a robust way, such that the shift in data distribution does not lead to faulty perception results. When agents cannot be trained with abundant data, agents may need to operate on real world environments while trained on simulated data, and suffer from domain shift. This paper proposes an effective and robust unsupervised domain adaptation (UDA) method that can resolve these situations. In the UDA setup, we are given a labeled source domain and an unlabeled target domain that share the same set of classes but are sampled from different distributions. This domain shift prevents agents which employ deep neural networks from generalizing well on the target domain. Recent methods adopt the strategy of self-training the networks with pseudo labeled target samples. However, falsely labeled samples cause negative transfer and deteriorate generalization of a network. to reduce negative transfer we propose an algorithm that can filter the pseudo labels, and use the filtered labels to align the domains in the feature space. The samples whose labels have not passed the filtering process can be used as an index to tune the hyperparameters of our method. Across various benchmarks, we validate the performance of our method. Especially, our method achieves strong performance on the synthetic-to-real adaptation scenario.},
  archive   = {C_IROS},
  author    = {Eojindl Yi and Junmo Kim},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981324},
  pages     = {10163-10169},
  title     = {Subspace-based feature alignment for unsupervised domain adaptation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient multi-task learning via iterated single-task
transfer. <em>IROS</em>, 10141–10146. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In order to be effective general purpose machines in real world environments, robots not only will need to adapt their existing manipulation skills to new circumstances, they will need to acquire entirely new skills on-the-fly. One approach to achieving this capability is via Multi-task Reinforcement Learning (MTRL). Most recent work in MTRL trains a single policy to solve all tasks at once. In this work, we investigate the feasibility of instead training separate policies for each task, and only transferring from a task once the policy for it has finished training. We describe a method of finding near optimal sequences of transfers to perform in this setting, and use it to show that performing the optimal sequence of transfer is competitive with other MTRL methods on the Meta World MT10 benchmark. Lastly, we describe a method for finding nearly optimal transfer sequences during training that is able to improve on training each task from scratch.},
  archive   = {C_IROS},
  author    = {K.R. Zentner and Ujjwal Puri and Yulun Zhang and Ryan Julian and Gaurav S. Sukhatme},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981244},
  pages     = {10141-10146},
  title     = {Efficient multi-task learning via iterated single-task transfer},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Contrastive learning for cross-domain open world
recognition. <em>IROS</em>, 10133–10140. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to evolve is fundamental for any valuable autonomous agent whose knowledge cannot remain limited to that injected by the manufacturer. Consider for example a home assistant robot: it should be able to incrementally learn new object categories when requested, but also to recognize the same objects in different environments (rooms) and poses (hand-held/on the floor/above furniture), while rejecting unknown ones. Despite its importance, this scenario has started to raise interest in the robotic community only recently and the related research is still in its infancy, with existing experimental testbeds but no tailored methods. With this work, we propose the first learning approach that deals with all the previously mentioned challenges at once by exploiting a single contrastive objective. We show how it learns a feature space perfectly suitable to incrementally include new classes and is able to capture knowledge which generalizes across a variety of visual domains. Our method is endowed with a tailored effective stopping criterion for each learning episode and exploits a self-paced thresholding strategy that provides the classifier with a reliable rejection option. Both these novel contributions are based on the observation of the data statistics and do not need manual tuning. An extensive experimental analysis confirms the effectiveness of the proposed approach in establishing the new state-of-the-art. The code is available at https://github.com/FrancescoCappio/Contrastive_Open_World.},
  archive   = {C_IROS},
  author    = {Francesco Cappio Borlino and Silvia Bucci and Tatiana Tommasi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981592},
  pages     = {10133-10140},
  title     = {Contrastive learning for cross-domain open world recognition},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gathering physical particles with a global magnetic field
using reinforcement learning. <em>IROS</em>, 10126–10132. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For biomedical applications in targeted therapy delivery and interventions, a large swarm of micro-scale particles (“agents”) has to be moved through a maze-like environment (“vascular system”) to a target region (“tumor”). Due to limited on-board capabilities, these agents cannot move autonomously; instead, they are controlled by an external global force that acts uniformly on all particles. In this work, we demonstrate how to use a time-varying magnetic field to gather particles to a desired location. We use reinforcement learning to train networks to efficiently gather particles. Methods to overcome the simulation-to-reality gap are explained, and the trained networks are deployed on a set of mazes and goal locations. The hardware experiments demonstrate fast convergence, and robustness to both sensor and actuation noise. To encourage extensions and to serve as a benchmark for the reinforcement learning community, the code is available at Github.},
  archive   = {C_IROS},
  author    = {Matthias Konitzny and Yitong Lu and Julien Leclerc and Sándor P. Fekete and Aaron T. Becker},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982256},
  pages     = {10126-10132},
  title     = {Gathering physical particles with a global magnetic field using reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Non-submodular maximization via the greedy algorithm and the
effects of limited information in multi-agent execution. <em>IROS</em>,
10118–10125. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We provide theoretical bounds on the worst case performance of the greedy algorithm in seeking to maximize a normalized, monotone, but not necessarily submodular ob-jective function under a simple partition matroid constraint. We also provide worst case bounds on the performance of the greedy algorithm in the case that limited information is available at each planning step. We specifically consider limited information as a result of unreliable communications during distributed execution of the greedy algorithm. We utilize notions of curvature for normalized, monotone set functions to develop the bounds provided in this work. To demonstrate the value of the bounds provided in this work, we analyze a variant of the benefit of search objective function and show, using real-world data collected by an autonomous underwater vehicle, that theoretical approximation guarantees are achieved despite non-submodularity of the objective function.},
  archive   = {C_IROS},
  author    = {Benjamin Biggs and James McMahon and Philip Baldoni and Daniel J. Stilwell},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982070},
  pages     = {10118-10125},
  title     = {Non-submodular maximization via the greedy algorithm and the effects of limited information in multi-agent execution},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DiMOpt: A distributed multi-robot trajectory optimization
algorithm. <em>IROS</em>, 10110–10117. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper deals with Multi-robot Trajectory Planning, that is, the problem of computing trajectories for multiple robots navigating in a shared space while minimizing for control energy. Approaches based on trajectory optimization can solve this problem optimally. However, such methods are hampered by complex robot dynamics and collision constraints that couple robot&#39;s decision variables. We propose a distributed multi-robot optimization algorithm (DiMOpt) that addresses these issues by exploiting (1) consensus optimization strategies to tackle coupling collision constraints, and (2) a single-robot sequential convex programming method for efficiently handling non-convexities introduced by dynamics. We compare DiMOpt with a baseline centralized multi-robot sequential convex programming algorithm (SCP). We empirically demonstrate that DiMOpt scales well for large fleets of robots while computing solutions faster and with lower costs. Finally, DiMOpt is an iterative algorithm that finds feasible trajectories before converging to a locally optimal solution, and results suggest the quality of such fast initial solutions is comparable to a converged solution computed via SCP.},
  archive   = {C_IROS},
  author    = {João Salvado and Masoumeh Mansouri and Federico Pecora},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981345},
  pages     = {10110-10117},
  title     = {DiMOpt: A distributed multi-robot trajectory optimization algorithm},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scalable online coverage path planning for multi-robot
systems. <em>IROS</em>, 10102–10109. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Online coverage path planning to explore an unknown workspace with multiple homogeneous robots could be either centralized or distributed. While distributed planners are computationally faster, centralized planners can produce more efficient paths, reducing the duration of completing a coverage mission significantly. To exploit the power of a centralized framework, we propose a receding horizon centralized online multi-robot planner. In each planning horizon, it generates collision-free paths that guide the robots to visit some obstacle-free locations (aka goals) not visited so far, which in turn help them explore some new regions with their laser rangefinders. We formally prove that, under reasonable conditions, it enables the robots to cover a workspace completely and subsequently analyze its time complexity. We evaluate our planner for ground and aerial robots by performing experiments with up to 128 robots on six 2D grid-based benchmark obstacle maps, establishing scalability. We also perform Gazebo simulations with 10 quadcopters and real experiments with 2 four-wheel ground robots, demonstrating its practical feasibility. Further-more, a comparison with a state-of-the-art distributed planner establishes its superiority in coverage completion time.},
  archive   = {C_IROS},
  author    = {Ratijit Mitra and Indranil Saha},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981213},
  pages     = {10102-10109},
  title     = {Scalable online coverage path planning for multi-robot systems},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MAPFASTER: A faster and simpler take on multi-agent path
finding algorithm selection. <em>IROS</em>, 10088–10093. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Portfolio-based algorithm selection can help in choosing the best suited algorithm for a given task while leveraging the complementary strengths of the candidates. Solving the Multi-Agent Path Finding (MAPF) problem optimally has been proven to be NP-Hard. Furthermore, no single optimal algorithm has been shown to have the fastest runtime for all MAPF problem instances, and there are no proven approaches for when to use each algorithm. To address these challenges, we develop MAPFASTER, a smaller and more accurate deep learning based architecture aiming to be deployed in fleet management systems to select the fastest MAPF solver in a multi-robot setting. MAPF problem instances are encoded as images and passed to the model for classification into one of the portfolio&#39;s candidates. We evaluate our model against state-of-the-art Optimal-MAPF-Algorithm selectors, showing +5.42\% improvement in accuracy while being 7.1× faster to train. The dataset, code and analysis used in this research can be found at https://github.com/jeanmarcalkazzi/mapfaster.},
  archive   = {C_IROS},
  author    = {Jean-Marc Alkazzi and Anthony Rizk and Michel Salomon and Abdallah Makhoul},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981981},
  pages     = {10088-10093},
  title     = {MAPFASTER: A faster and simpler take on multi-agent path finding algorithm selection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Energy-efficient orienteering problem in the presence of
ocean currents. <em>IROS</em>, 10081–10087. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many environmental monitoring applications robots are often tasked to visit various distinct locations to make observations and/or collect specific measurements. The problem of scheduling and assigning robots to the various tasks and planning feasible paths for the robots can be posed as an Orienteering Problem (OP). In the standard OP, routing and scheduling is achieved by maximizing an objective function by visiting the most rewarding locations while respecting a limited travel budget. However, traditional formulations for such problems usually neglect some environmental features that can greatly impact the tour, e.g., flows, such as wind or ocean currents. This is of particular importance for applications in marine and atmospheric environments where vehicle motions can be significantly impacted by the environmental dynamics and the environment exerts a non-negligible force on the vehicles. In this paper, we tackle the OP in fluid environments where robots must operate in the presence of ocean and/or atmospheric currents. We introduce a novel multi-objective formulation that combines both task and path planning problems, and whose goals are to (i) maximize the collected reward, while (ii) minimizing the energy expenditure by leveraging the environmental dynamics wherever possible. We validate our strategy using simulated ocean model data to show that our approach can generate a diverse set of solutions that have an adequate compromise between both objectives.},
  archive   = {C_IROS},
  author    = {Ariella Mansfield and Douglas G. Macharet and M. Ani Hsieh},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981818},
  pages     = {10081-10087},
  title     = {Energy-efficient orienteering problem in the presence of ocean currents},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Polynomial time near-time-optimal multi-robot path planning
in three dimensions with applications to large-scale UAV coordination.
<em>IROS</em>, 10074–10080. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For enabling efficient, large-scale coordination of unmanned aerial vehicles (UAV s) under the labeled setting, in this work, we develop the first polynomial time algorithm for the reconfiguration of many moving bodies in three-dimensional spaces, with provable 1. $x$ asymptotic makespan optimality guarantee under high robot density. More precisely, on an $m_{1} \times m_{2} \times m_{3}$ grid, $m_{1}\geq m_{2}\geq m_{3}$ , our method computes solutions for routing up to $\displaystyle \frac{m_{1}m_{2}m_{3}}{3}$ uniquely labeled robots with uniformly randomly distributed start and goal configurations within a makespan of $m_{1}+2m_{2}+2m_{3}+o(m_{1})$ , with high probability. Because the makespan lower bound for such instances is $m_{1}+m_{2}+m_{3}-o(m_{1})$ , also with high probability, as $m_{1}\displaystyle \rightarrow\infty, \frac{m_{1}+2m_{2}+2m_{3}}{m_{1}+m_{2}+m_{3}}$ optimality guarantee is achieved. $\displaystyle \frac{m_{1}+2 m_{2}+2m_{3}}{m_{1}+m_{2}+m_{3}}\in\left(1, \displaystyle \frac{5}{3}\right]$ , yielding 1. $x$ optimality. In contrast, it is well-known that multi-robot path planning is NP-hard to optimally solve. In numerical evaluations, our method readily scales to support the motion planning of over 100, 000 robots in 3D while simultaneously achieving 1. $x$ optimality. We demonstrate the application of our method in coordinating many quadcopters in both simulation and hardware experiments.},
  archive   = {C_IROS},
  author    = {Teng Guo and Si Wei Feng and Jingjin Yu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982231},
  pages     = {10074-10080},
  title     = {Polynomial time near-time-optimal multi-robot path planning in three dimensions with applications to large-scale UAV coordination},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Collaborative navigation-aware coverage in feature-poor
environments. <em>IROS</em>, 10066–10073. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi agent coverage and robot navigation are two very important research fields within robotics. However, their intersection has received limited attention. In multi agent coverage, perfect navigation is often assumed, and in robot navigation, the focus is often to minimize the localization error with the aid of stationary features from the environment. The need for integration of the two becomes clear in environments with very sparse features or landmarks, for example when a group of Autonomous Underwater Vehicles (AUVs) are to search a uniform seafloor for mines or other dangerous objects. In such environments, localization systems are often deprived of detectable features to use that could increase their accuracy. In this paper we propose an algorithm for doing navigation aware multi agent coverage in areas with no landmarks. Instead of using identical lawn mower patterns, we propose to mirror every other pattern to enable the agents to meet up and make inter-agent measurements and share information regularly. This improves performance in two ways, global drift in relation to the area to be covered is reduced, and local coverage gaps between adjacent patterns are reduced. Further, we show that this can be accomplished within the constraints of very limited sensing, computing and communication resources that most AUVs have available. The effectiveness of our method is shown through statistically significant simulated experiments.},
  archive   = {C_IROS},
  author    = {Özer Özkahraman and Petter Ögren},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981547},
  pages     = {10066-10073},
  title     = {Collaborative navigation-aware coverage in feature-poor environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Soft-skin actuator capable of seawater propulsion based on
MagnetoHydroDynamics. <em>IROS</em>, 10059–10065. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Underwater robots have a variety of potential uses, including marine resource research, ecological research, and disaster relief. Most of the underwater robots currently in practical use have screw propulsion systems, which have several noises, collision, and entrainment problems. There is a lot of research on underwater robots using soft actuators to solve these problems. However, current soft actuators have disadvantages, such as the need for special fluids, pressure sources, and high voltage circuits. Therefore, we have developed a soft-skin actuator based on magnetohydrodynamics (MHD). The soft-skin MHD actuator is made of soft material and the structure is prepared as thin, which allows it to attach to the surface of an object, including curved surfaces, to provide the object with a propulsive function in the sea. Since it has no moving parts, it does not generate mechanical noise, and there is no danger of entrapment. Because it can pump seawater directly, it does not require a special working fluid, and its structure is simple and easy to miniaturize. This paper investigates the thrust and power consumption of the developed soft-skin MHD actuator when attached to a flat surface. As a result, we obtained a thrust of 1.37 mN from a single soft-skin MHD actuator with a maximum power of about 140 W. We also measured the thrust force by attaching it to a curved surface. We obtained a higher thrust on a curved surface by adjusting the crossing of the magnetic field and the current than when using a flat surface. We developed an untethered robot that can remove oil from the sea using soft-skin MHD actuators. We demonstrated the adaptability of the soft-skin MHD actuator by attaching it to a commercial underwater camera weighing about 253.5 g and providing propulsion.},
  archive   = {C_IROS},
  author    = {Mutsuki Matsumoto and Yu Kuwajima and Hiroki Shigemune},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981745},
  pages     = {10059-10065},
  title     = {Soft-skin actuator capable of seawater propulsion based on MagnetoHydroDynamics},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A virtual 2D tactile array for soft actuators using acoustic
sensing. <em>IROS</em>, 10029–10034. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We create a virtual 2D tactile array for soft pneumatic actuators using embedded audio components. We detect contact-specific changes in sound modulation to infer tactile information. We evaluate different sound representations and learning methods to detect even small contact variations. We demonstrate the acoustic tactile sensor array by the example of a PneuFlex actuator and use a Braille display to individually control the contact of 29 x 4 pins with the actuator&#39;s 90 x 10 mm palmar surface. Evaluating the spatial resolution, the acoustic sensor localizes edges in x- and y-direction with a root-mean-square regression error of 1.67 mm and 0.0 mm, respectively. Even light contacts of a single Braille pin with a lifting force of 0.17 N are measured with high accuracy. Finally, we demonstrate the sensor&#39;s sensitivity to complex contact shapes by successfully reading the 26 letters of the Braille alphabet from a single display cell with a classification rate of 88\%.},
  archive   = {C_IROS},
  author    = {Vincent Wall and Oliver Brock},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981225},
  pages     = {10029-10034},
  title     = {A virtual 2D tactile array for soft actuators using acoustic sensing},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design and characterisation of a soft barometric sensing
skin for robotic manipulation. <em>IROS</em>, 10021–10028. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft sensorised skins are essential for improving robotic manipulation capabilities towards that of humans. Integration of sensors into existing robotic hands is challenging due to rigidity of components, low packing density or poor sensor response. We propose a sensorised skin, based-on barometric sensing, which can be molded over a skeletal robot hand. The sensors connect air chambers embedded in the soft skin to wrist-mounted pressure sensors, allowing sensor spacing 2–4 mm, force ranges from 23 mN to 5700 mN and bandwidth of 20 Hz. Integrating this with a skeletal hand allows us to showcase the potential of these sensors to aid robotic manipulation. We demonstrate 3-axis contact modelling, useful for in-hand manipulation and exploration. In addition, by grasping a chopstick and sensing forces transmitted from the environment, the system can remotely detect small environmental features, e.g., hole finding using tools.},
  archive   = {C_IROS},
  author    = {Kieran Gilday and Louis Relandeau and Fumiya Iida},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981855},
  pages     = {10021-10028},
  title     = {Design and characterisation of a soft barometric sensing skin for robotic manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Kirigami skin based flexible whisker sensor. <em>IROS</em>,
10015–10020. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Whiskers are widely used by animals for sensing physical interactions with their environments. By combining the Kirigami skin pop-up feature and flexible conducting layer, we designed a deployable Kirigami whisker sensor. The sensor can deploy from a flat state to a sensing state while whisker stiffness and initial pop-up angle can be tuned by adjusting the pre-stretch strain. Preliminary results show that the sensor works well both in air and underwater. The sensor is capable of measuring both externally applied forces and water flow.},
  archive   = {C_IROS},
  author    = {Bangyuan Liu and Robert Herbert and Woon-Hong Yeo and Frank L. Hammond},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981556},
  pages     = {10015-10020},
  title     = {Kirigami skin based flexible whisker sensor},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimation of soft robotic bladder compression for smart
helmets using IR range finding and hall effect magnetic sensing.
<em>IROS</em>, 10009–10014. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This research focuses on soft robotic bladders that are used to monitor and control the interaction between a user&#39;s head and the shell of a Smart Helmet. Compression of these bladders determines impact dissipation; hence the focus of this paper is sensing and estimation of bladder compression. An IR rangefinder-based solution is evaluated using regression techniques as well as a Neural Network to estimate bladder compression. A Hall-Effect (HE) magnetic sensing system is also examined where HE sensors embedded in the base of the bladder sense the position of a magnet in the top of the bladder. The paper presents the HE sensor array, signal processing of HE voltage data, and then a Neural Network (NN) for predicting bladder compression. Efficacy of different training data sets on NN performance is studied. Different NN configurations are examined to determine a configuration that provides accurate estimates with as few nodes as possible. Different bladder compression profiles are evaluated to characterize IR range finding and HE based techniques in application scenarios.},
  archive   = {C_IROS},
  author    = {Colin Pollard and Jon Aston and Mark A. Minor},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981160},
  pages     = {10009-10014},
  title     = {Estimation of soft robotic bladder compression for smart helmets using IR range finding and hall effect magnetic sensing},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Slip anticipation for grasping deformable objects using a
soft force sensor. <em>IROS</em>, 10003–10008. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots using classical control have revolutionised assembly lines where the environment and manipulated objects are restricted and predictable. However, they have proven less effective when the manipulated objects are deformable due to their complex and unpredictable behaviour. The use of tactile sensors and continuous monitoring of tactile feedback is there-fore particularly important for pick-and-place tasks using these materials. This is in part due to the need to use multiple points of contact for the manipulation of deformable objects which can result in slippage with inadequate coordination between manipulators. In this paper, continuous monitoring of tactile feedback, using a liquid metal soft force sensor, for grasping deformable objects is presented. The trained data-driven model distinguishes between successful grasps, slippage and failure during a manipulation task for multiple deformable objects. Slippage could be anticipated before failure occurred using data acquired over a 30 ms period with a greater than 95\% accuracy using a random forest classifier. The results were achieved using a single sensor that can be mounted on the fingertips of existing grippers and contributes to the development of an automated pick-and-place process for deformable objects.},
  archive   = {C_IROS},
  author    = {Euan Judd and Bekir Aksoy and Krishna Manaswi Digumarti and Herbert Shea and Dario Floreano},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981174},
  pages     = {10003-10008},
  title     = {Slip anticipation for grasping deformable objects using a soft force sensor},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-modal user interface for multi-robot control in
underground environments. <em>IROS</em>, 9995–10002. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Leveraging both the autonomy of robots and the expert knowledge of humans can enable a multi-robot system to complete missions in challenging environments with a high degree of adaptivity and robustness. This paper proposes a multi-modal task-based graphical user interface for controlling a heterogeneous multi-robot team. The core of the interface is an integrated multi-robot task allocation system to allow the user to encode his/her intents to guide the heterogeneous multi-robot team. The design of the interface aims to provide the human operator continuous situational awareness and effective control for rapid decision-making in time-critical missions. Team CSIRO Data61 came in second place utilizing this interface for the DARPA Subterranean (SubT) Challenge. The ideas used for this user interface can apply to other multi-robot applications.},
  archive   = {C_IROS},
  author    = {Shengkang Chen and Matthew J. O&#39;Brien and Fletcher Talbot and Jason Williams and Brendan Tidd and Alex Pitt and Ronald C. Arkin},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981165},
  pages     = {9995-10002},
  title     = {Multi-modal user interface for multi-robot control in underground environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-robot path planning using medial-axis-based
pebble-graph embedding. <em>IROS</em>, 9987–9994. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a centralized algorithm for labeled, disk-shaped Multi-Robot Path Planning (MPP) in a continuous planar workspace with polygonal boundaries. Our method automatically transform the continuous problem into a discrete, graph-based variant termed the pebble motion problem, which can be solved efficiently. To construct the underlying pebble graph, we identify inscribed circles in the workspace via a medial axis transform and organize robots into layers within each inscribed circle. We show that our layered pebble-graph enables collision-free motions, allowing all graph-restricted MPP instances to be feasible. MPP instances with continuous start and goal positions can then be solved via local navigations that route robots from and to graph vertices. We tested our method on several environments with high robot-packing densities (up to 61.6\% of the workspace). For environments with narrow passages, such density violates the well-separated assumptions made by state-of-the-art MPP planners, while our method achieves an average success rate of 83\%.},
  archive   = {C_IROS},
  author    = {Liang He and Zherong Pan and Kiril Solovey and Biao Jia and Dinesh Manocha},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981807},
  pages     = {9987-9994},
  title     = {Multi-robot path planning using medial-axis-based pebble-graph embedding},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Decentralized learning with limited communications for
multi-robot coverage of unknown spatial fields. <em>IROS</em>,
9980–9986. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an algorithm for a team of mobile robots to simultaneously learn a spatial field over a domain and spatially distribute themselves to optimally cover it. Drawing from previous approaches that estimate the spatial field through a centralized Gaussian process, this work leverages the spatial structure of the coverage problem and presents a decentralized strategy where samples are aggregated locally by establishing communications through the boundaries of a Voronoi partition. We present an algorithm whereby each robot runs a local Gaussian process calculated from its own measurements and those provided by its Voronoi neighbors, which are incorporated into the individual robot&#39;s Gaussian process only if they provide sufficiently novel information. The performance of the algorithm is evaluated in simulation and compared with centralized approaches.},
  archive   = {C_IROS},
  author    = {Kensuke Nakamura and María Santos and Naomi Ehrich Leonard},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981665},
  pages     = {9980-9986},
  title     = {Decentralized learning with limited communications for multi-robot coverage of unknown spatial fields},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Asynchronous real-time decentralized multi-robot trajectory
planning. <em>IROS</em>, 9972–9979. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel overconstraining and constraint-discarding method for asynchronous, real-time, decentralized, multi-robot trajectory planning that ensures collision avoidance. Our approach utilizes communication between robots. The communication medium is best-effort: messages may be dropped, re-ordered or delayed. Robots conservatively constrain themselves against others assuming they may be working with outdated information, and discard constraints when they receive update messages from others. Our method can augment existing synchronized decentralized receding horizon planning algorithms that utilize separating hyperplanes for collision avoidance thereby making them applicable to asynchronous setups. As an example, we extend an existing model predictive control based, synchronized, decentralized multi-robot planner using our method. We show our method&#39;s effectiveness under asynchronous planning and imperfect communication by comparing our extension to the base version. Our extension does not result in any collisions or synchronization-induced deadlocks to which the base version is prone.},
  archive   = {C_IROS},
  author    = {Baskin Şenbaşlar and Gaurav S. Sukhatme},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981760},
  pages     = {9972-9979},
  title     = {Asynchronous real-time decentralized multi-robot trajectory planning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-goal multi-agent pickup and delivery. <em>IROS</em>,
9964–9971. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we consider the Multi-Agent Pickup-and-Delivery (MAPD) problem, where agents constantly engage with new tasks and need to plan collision-free paths to execute them. To execute a task, an agent needs to visit a pair of goal locations, consisting of a pickup location and a delivery location. We propose two variants of an algorithm that assigns a sequence of tasks to each agent using the anytime algorithm Large Neighborhood Search (LNS) and plans paths using the Multi-Agent Path Finding (MAPF) algorithm Priority-Based Search (PBS). LNS-PBS is complete for well-formed MAPD instances, a realistic subclass of MAPD instances, and empirically more effective than the existing complete MAPD algorithm CENTRAL. LNS-wPBS provides no completeness guarantee but is empirically more efficient and stable than LNS-PBS. It scales to thousands of agents and thousands of tasks in a large warehouse and is empirically more effective than the existing scalable MAPD algorithm HBH+MLA*. LNS-PBS and LNS-wPBS also apply to a more general variant of MAPD, namely the Multi-Goal MAPD (MG-MAPD) problem, where tasks can have different numbers of goal locations.},
  archive   = {C_IROS},
  author    = {Qinghong Xu and Jiaoyang Li and Sven Koenig and Hang Ma},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981785},
  pages     = {9964-9971},
  title     = {Multi-goal multi-agent pickup and delivery},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On coverage control for limited range multi-robot systems.
<em>IROS</em>, 9957–9963. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a coverage based control algorithm to coordinate a group of autonomous robots. Most of the solutions presented in the literature rely on an exact Voronoi partitioning, whose computation requires complete knowledge of the environment to be covered. This can be achieved only by robots with unlimited sensing capabilities, or through communication among robots in a limited sensing scenario. To overcome these limitations, we present a distributed control strategy to cover an unknown environment with a group of robots with limited sensing capabilities and in the absence of reliable communication. The control law is based on a limited Voronoi partitioning of the sensing area, and we demonstrate that the group of robots can optimally cover the environment using only information that is locally detected (without communication). The proposed method is validated by means of simulations and experiments carried out on a group of mobile robots.},
  archive   = {C_IROS},
  author    = {Federico Pratissoli and Beatrice Capelli and Lorenzo Sabattini},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982002},
  pages     = {9957-9963},
  title     = {On coverage control for limited range multi-robot systems},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient range-constraint manifold optimization with
application to cooperative navigation. <em>IROS</em>, 9950–9956. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a manifold optimization approach to solve inference and planning problems with range constraints. The core of our approach is the definition of a manifold that represents points or poses with range constraints. We discover that the manifold of range-constrained points is homogeneous under the rigid transformation group action, and utilize the group action to derive the tangent space, retraction and topology of the manifold. We evaluate the performance of manifold optimization approach on solving range-constrained inference problems over state-of-the-art constrained optimization methods. The results show that manifold optimization with the range-constraint manifold achieves both faster speed and better constraint satisfaction. We further study the conditions of inference problems that we can treat range measurements as constraints in practice.},
  archive   = {C_IROS},
  author    = {Yetong Zhang and Gerry Chen and Adam Rutkowski and Frank Dellaert},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982188},
  pages     = {9950-9956},
  title     = {Efficient range-constraint manifold optimization with application to cooperative navigation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-robot unknown area exploration using frontier trees.
<em>IROS</em>, 9934–9941. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel approach for multi-robot unknown area exploration. Recently, the frontier tree data structure was used in single robot exploration to memorize frontiers, their positions, exploration state, and the map. This tree could be queried to decide on further exploration steps. In this paper, we take the concept further for multi-robot exploration by proposing a new abstraction called the ‘group,’ meant to share information through a common frontier tree, requisite operations at the group level, and a method to assign goals to multiple robots. A group is a set of robots, the union of whose explored regions forms a contiguous region (a single connected region in a topological sense). As a group has precisely one tree, the robots share a common state of the exploration task. We propose techniques to merge groups and their frontier trees once their maps overlap. Finally, we suggest a method to designate and assign exploration goals to the individual robots by choosing nodes from the frontier tree. The proposed approach outperforms seven state-of-the-art research works in simulation.},
  archive   = {C_IROS},
  author    = {Ankit Soni and Chirag Dasannacharya and Avinash Gautam and Virendra Singh Shekhawat and Sudeept Mohan},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981914},
  pages     = {9934-9941},
  title     = {Multi-robot unknown area exploration using frontier trees},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Relationship oriented semantic scene understanding for daily
manipulation tasks. <em>IROS</em>, 9926–9933. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Assistive robot systems have been developed to help people accomplish daily manipulation tasks especially for those with disabilities, where scene understanding plays a crucial role in enabling robots to interpret the surroundings and behave accordingly. Most of the current systems approach scene understanding without considering the functional dependencies between objects. However, it is only valuable to interact with some objects when their function-relevant counterparts are considered. In this paper, we augment an assistive robotic arm system with an end-to-end semantic relationship reasoning model. It incorporates functional relationships between pairs of objects for semantic scene understanding. To ensure good generalization to unseen objects and relationships, the model works in a category-agnostic manner. We evaluate our design and three baseline methods on a self-collected benchmark with two levels of difficulty. To further demonstrate the effectiveness, the model is integrated with a symbolic planner for goal-oriented, multi-step manipulation task on a real-world assistive robotic arm platform.},
  archive   = {C_IROS},
  author    = {Chao Tang and Jingwen Yu and Weinan Chen and Bingyi Xia and Hong Zhang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981960},
  pages     = {9926-9933},
  title     = {Relationship oriented semantic scene understanding for daily manipulation tasks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time semantic 3D reconstruction for high- touch surface
recognition for robotic disinfection. <em>IROS</em>, 9919–9925. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Disinfection robots have applications in promoting public health and reducing hospital acquired infections and have drawn considerable interest due to the COVID-19 pan-demic. To disinfect a room quickly, motion planning can be used to plan robot disinfection trajectories on a reconstructed 3D map of the room&#39;s surfaces. However, existing approaches discard semantic information of the room and, thus, take a long time to perform thorough disinfection. Human cleaners, on the other hand, disinfect rooms more efficiently by prioritizing the cleaning of high-touch surfaces. To address this gap, we present a novel GPU-based volumetric semantic TSDF (Truncated Signed Distance Function) integration system for semantic 3D reconstruction. Our system produces 3D reconstructions that distinguish high-touch surfaces from non-high-touch surfaces at approximately 50 frames per second on a consumer-grade GPU, which is approximately 5 times faster than existing CPU-based TSDF semantic reconstruction methods. In addition, we extend a UV disinfection motion planning algorithm to incorporate semantic awareness for optimizing coverage of disinfection tra-jectories. Experiments show that our semantic-aware planning outperforms geometry-only planning by disinfecting up to 20\% more high-touch surfaces under the same time budget. Further, the real-time nature of our semantic reconstruction pipeline enables future work on simultaneous disinfection and mapping. Code is available at: https://github.com/uiuc-iml/RA-SLAM},
  archive   = {C_IROS},
  author    = {Ri-Zhao Qiu and Yixiao Sun and Joao Marcos Correia Marques and Kris Hauser},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981300},
  pages     = {9919-9925},
  title     = {Real-time semantic 3D reconstruction for high- touch surface recognition for robotic disinfection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fisheye object detection based on standard image datasets
with 24-points regression strategy. <em>IROS</em>, 9911–9918. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fisheye object detection is a difficult task in robotics and autonomous driving. One of the reasons is that the fisheye datasets are inferior to standard image datasets in scale and quantity, which inspires the idea of using standard image datasets for fisheye object detection. However, the models trained on standard image datasets do not perform well with fisheye data. In this work, we explore the effect of fisheye images on different stages of the YOLOX with published weights generated by standard image datasets. We also propose a new regression strategy for 24-points object representation method, which is insensitive to image distortion. The experiments show that the feature extraction part is robust to fisheye image features, while the regression part of location and category performs poorly. The strategy can achieve the position of discrete points without calculating the IOU of irregular-shaped boxes. Theoretically, the strategy can be widely adopted to regress the irregular bounding boxes composed of discrete points. Source code is at https://github.com/IN2-ViAUn/Exploration-of-Potential.},
  archive   = {C_IROS},
  author    = {Xi Xu and Yu Gao and Hao Liang and Yi Yang and Mengyin Fu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981891},
  pages     = {9911-9918},
  title     = {Fisheye object detection based on standard image datasets with 24-points regression strategy},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SectionKey: 3-d semantic point cloud descriptor for place
recognition. <em>IROS</em>, 9905–9910. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Place recognition is seen as a crucial factor to correct cumulative errors in Simultaneous Localization and Mapping (SLAM) applications. Most existing studies focus on visual place recognition, which is inherently sensitive to environmental changes such as illumination, weather and seasons. Considering these facts, more recent attention has been attracted to use 3-D Light Detection and Ranging (LiDAR) scans for place recognition, which demonstrates more credibility by exerting accurate geometric information. Different from pure geometric-based studies, this paper proposes a novel global descriptor, named SectionKey, which leverages both semantic and geometric information to tackle the problem of place recognition in large-scale urban environments. The proposed descriptor is robust and invariant to viewpoint changes. Specifically, the encoded three-layers key serves as a pre-selection step and a ‘candidate center’ selection strategy is deployed before calculating the similarity score, thus improving the accuracy and efficiency significantly. Then, a two-step semantic iterative closest point (ICP) algorithm is applied to acquire the 3-D pose (x, y, θ) that is used to align the candidate point clouds with the query frame and calculate the similarity score. Extensive experiments have been conducted on public Semantic KITTI dataset to demonstrate the superior performance of our proposed system over state-of-the-art baselines.},
  archive   = {C_IROS},
  author    = {Shutong Jin and Zhenyu Wu and Chunyang Zhao and Jun Zhang and Guohao Peng and Danwei Wang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981605},
  pages     = {9905-9910},
  title     = {SectionKey: 3-D semantic point cloud descriptor for place recognition},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised domain adaptation for point cloud semantic
segmentation via graph matching. <em>IROS</em>, 9899–9904. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unsupervised domain adaptation for point cloud semantic segmentation has attracted great attention due to its effectiveness in learning with unlabeled data. Most of existing methods use global-level feature alignment to transfer the knowledge from the source domain to the target domain, which may cause the semantic ambiguity of the feature space. In this paper, we propose a graph-based framework to explore the local-level feature alignment between the two domains, which can reserve semantic discrimination during adaptation. Specifically, in order to extract local-level features, we first dynamically construct local feature graphs on both domains and build a memory bank with the graphs from the source domain. In particular, we use optimal transport to generate the graph matching pairs. Then, based on the assignment matrix, we can align the feature distributions between the two domains with the graph-based local feature loss. Furthermore, we consider the correlation between the features of different categories and formulate a category-guided contrastive loss to guide the segmentation model to learn discriminative features on the target domain. Extensive experiments on different synthetic-to-real and real-to-real domain adaptation scenarios demonstrate that our method can achieve state-of-the-art performance. Our code is available at https://github.com/BianYikai/PointUDA.},
  archive   = {C_IROS},
  author    = {Yikai Bian and Le Hui and Jianjun Qian and Jin Xie},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981603},
  pages     = {9899-9904},
  title     = {Unsupervised domain adaptation for point cloud semantic segmentation via graph matching},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Implicit-part based context aggregation for point cloud
instance segmentation. <em>IROS</em>, 9892–9898. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Context information is important for instance segmentation on point clouds. Existing methods either only use local surroundings by stacking multiple convolution layers or use non-local methods to model long-range interactions. However, they usually directly operate on points which is an unstructured and low-level representation and is highly dependent on context. To address this issue, we propose an effective framework named Implicit-Part Context Aggregation (IPCA), which adopts implicit parts as an intermediate representation and achieves context aggregation through message passing along the implicit part graph. Specifically, we first organize unstructured points into geometrically consistent implicit parts and construct the implicit part graph according to the geometric adjacency. Then, an initial part embedding is extracted using the proposed Implicit Part Network (IPN) which can aggregate point features and capture the intrinsic geometric shape of the part. We further refine the part embedding by a graph reasoning module named Context Aggregation Network (CAN), which helps to make a more precise prediction by well exploiting the context information. Instance proposals are then generated by grouping implicit parts. Finally, we propose an additional step to attribute the entire instance proposal to a Semantic Criterion Net (SCN) to infer the semantics of the instance. The purpose is to correct the semantic prediction errors caused by not knowing the boundary and overall shape of the object in the previous steps. Extensive experiments on two large datasets, ScanNet and 3RScan, demonstrate the effectiveness of our method. To our knowledge, it yields the highest performance on the ScanNet test benchmark and its AP@50 is 9.5 points higher than the baseline. The code is available at https://github.com/xiaodongww/IPCA},
  archive   = {C_IROS},
  author    = {Xiaodong Wu and Ruiping Wang and Xilin Chen},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981772},
  pages     = {9892-9898},
  title     = {Implicit-part based context aggregation for point cloud instance segmentation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Low-latency LiDAR semantic segmentation. <em>IROS</em>,
9886–9891. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Several methods of semantic segmentation using light detection and ranging (LiDAR) sensors have been proposed for the recognition of surrounding objects by autonomous driving cars. LiDAR is a sensor that compensates for the weaknesses of other sensors, such as cameras or radar systems, and semantic segmentation assigns a class label to each point in the LiDAR point cloud. Recently, real-time semantic segmentation methods that are capable of processing LiDAR point clouds at frame rates have been proposed. Real-time semantic segmentation is essential for the autonomous driving system because it can output class labels for LiDAR point clouds at high speeds. However, this segmentation method suffers from a delay equal to processing time. To address this challenge, we propose a novel method that combines SalsaNext [1], a method of real-time LiDAR semantic segmentation, and semantic forecasting, which predicts the results of future semantic segmentation. We quantitatively evaluate our method using the Semantic-KITTI dataset, which comprises point cloud data acquired from the LiDAR sensor in the real world, and compare the latency and accuracy of our method with other semantic segmentation methods. Consequently, our method is found to be capable of operating in real-time and with low-latency, and it can achieve a performance similar to that of previously reported real-time semantic segmentation methods.},
  archive   = {C_IROS},
  author    = {Takahiro Hori and Takehisa Yairi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982099},
  pages     = {9886-9891},
  title     = {Low-latency LiDAR semantic segmentation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accurate instance-level CAD model retrieval in a large-scale
database. <em>IROS</em>, 9879–9885. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a new solution to the fine-grained retrieval of clean CAD models from a large-scale database in order to recover detailed object shape geometries for RGBD scans. Unlike previous work simply indexing into a moderately small database using an object shape descriptor and accepting the top retrieval result, we argue that in the case of a large-scale database a more accurate model may be found within a neighborhood of the descriptor. More importantly, we propose that the distinctiveness deficiency of shape descriptors at the instance level can be compensated by a geometry-based re-ranking of its neighborhood. Our approach first leverages the discriminative power of learned representations to distinguish between different categories of models and then uses a novel robust point set distance metric to re-rank the CAD neighbor-hood, enabling fine-grained retrieval in a large shape database. Evaluation on a real-world dataset shows that our geometry-based re-ranking is a conceptually simple but highly effective method that can lead to a significant improvement in retrieval accuracy compared to the state-of-the-art.},
  archive   = {C_IROS},
  author    = {Jiaxin Wei and Lan Hu and Chenyu Wang and Laurent Kneip},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981296},
  pages     = {9879-9885},
  title     = {Accurate instance-level CAD model retrieval in a large-scale database},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hierarchical deliberative architecture framework based on
goal decomposition. <em>IROS</em>, 9865–9870. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Performing a complex autonomous mission with a multi-robot system requires to integrate several deliberative approaches to perform task allocation, optimization, and execution control. Implementing such a deliberative architecture is a complex task: it requires the developer to master the decision algorithms themselves (e.g., automated planning models), to have a good knowledge of the involved robotic platforms, and to think about how these elements will be assembled as a system architecture. We propose a framework to help designing such deliberative architectures. The framework relies on the concept of a hierarchical structure of actors, each actor managing goals with specific planning or optimization approaches, and delegating sub-goals to other actors.},
  archive   = {C_IROS},
  author    = {Charles Lesire and Rafael Bailon-Ruiz and Magali Barbier and Christophe Grand},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981488},
  pages     = {9865-9870},
  title     = {A hierarchical deliberative architecture framework based on goal decomposition},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Social-PatteRNN: Socially-aware trajectory prediction guided
by motion patterns. <em>IROS</em>, 9859–9864. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As robots across domains start collaborating with humans in shared environments, algorithms that enable them to reason over human intent are important to achieve safe inter-play. In our work, we study human intent through the problem of predicting trajectories in dynamic environments. We explore domains where navigation guidelines are relatively strictly defined but not clearly marked in their physical environments. We hypothesize that within these domains, agents tend to exhibit short-term motion patterns that reveal context information related to the agent&#39;s general direction, intermediate goals and rules of motion, e.g., social behavior. From this intuition, we propose Social-PatteRNN, an algorithm for recurrent, multi-modal trajectory prediction that exploits motion patterns to encode the aforesaid contexts. Our approach guides long-term trajectory prediction by learning to predict short-term motion patterns. It then extracts sub-goal information from the patterns and aggregates it as social context. We assess our approach across three domains: humans crowds, humans in sports and manned aircraft in terminal airspace, achieving state-of-the-art performance.},
  archive   = {C_IROS},
  author    = {Ingrid Navarro and Jean Oh},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981486},
  pages     = {9859-9864},
  title     = {Social-PatteRNN: Socially-aware trajectory prediction guided by motion patterns},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pedestrian intention prediction based on traffic-aware scene
graph model. <em>IROS</em>, 9851–9858. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Anticipating the future behavior of pedestrians is a crucial part of deploying Automated Driving Systems (ADS) in urban traffic scenarios. Most recent works utilize a convolutional neural network (CNN) to extract visual information, which is then input to a recurrent neural network (RNN) along with pedestrian-specific features like location and speed to obtain temporal features. However, the majority of these approaches lack the ability to parse the relationships of the related objects in the specific traffic scene, which leads to omitting the interactions between the pedestrians and the interactions between the pedestrians and the traffic. For this purpose, we propose a graph-structured model which can dig out pedestrians&#39; dynamic constraints by constructing a traffic-aware scene graph within each frame. In addition, to capture pedestrian movement more effectively, we also introduce a temporal feature representation model, which first uses inter-frame and intra-frame GRU (II-GRU) to mine inter-frame information and intra-frame information together, and then employs a novel attention mechanism to adaptively generate attention weights. Extensive experiments on the JAAD and PIE datasets prove that our proposed model is effective in reaching and enhancing the state-of-the-art performance.},
  archive   = {C_IROS},
  author    = {Xingchen Song and Miao Kang and Sanping Zhou and Jianji Wang and Yishu Mao and Nanning Zheng},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981690},
  pages     = {9851-9858},
  title     = {Pedestrian intention prediction based on traffic-aware scene graph model},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimization of forcemyography sensor placement for arm
movement recognition. <em>IROS</em>, 9845–9850. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {How to design an optimal wearable device for human movement recognition is vital to reliable and accurate human-machine collaboration. Previous works mainly fabricate wearable devices heuristically. Instead, this paper raises an academic question: can we design an optimization algorithm to optimize the fabrication of wearable devices such as figuring out the best sensor arrangement automatically? Specifically, this work focuses on optimizing the placement of Forcemyography (FMG) sensors for FMG armbands in the application of arm movement recognition. Firstly, based on graph theory, the arm-band is modeled considering sensors&#39; signals and connectivity. Then, a Graph-based Armband Modeling Network (GAM-Net) is introduced for arm movement recognition. Afterward, the sensor placement optimization for FMG armbands is formu-lated and an optimization algorithm with greedy local search is proposed. To study the effectiveness of our optimization algorithm, a dataset for mechanical maintenance tasks using FMG armbands with 16 sensors is collected. Our experiments show that using only 4 sensors optimized with our algorithm can help maintain a comparable recognition accuracy to using all sensors. Finally, the optimized sensor placement result is verified from a physiological view. This work would like to shed light on the automatic fabrication of wearable devices considering downstream tasks, such as human biological signal collection and movement recognition.},
  archive   = {C_IROS},
  author    = {Xiaohao Xu and Zihao Du and Huaxin Zhang and Ruichao Zhang and Zihan Hong and Qin Huang and Bin Han},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981236},
  pages     = {9845-9850},
  title     = {Optimization of forcemyography sensor placement for arm movement recognition},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MPC-PF: Social interaction aware trajectory prediction of
dynamic objects for autonomous driving using potential fields.
<em>IROS</em>, 9837–9844. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Predicting object motion behaviour is a challenging but crucial task for safe decision making and path planning for an autonomous vehicle. It is challenging in large part due to the uncertain, multi-modal, and practically intractable set of possible agent-agent and agent-space interactions, especially in urban driving settings. Models solely based on constant velocity or social force have an inherent bias and may lead to inaccurate predictions across the prediction horizon whereas purely data driven approaches suffer from a lack of a holistic set of rules governing predictions. We tackle this problem by introducing MPC-PF: a novel potential field-based trajectory predictor that incorporates social interaction and is able to tradeoff between inherent model biases across the prediction horizon. Through evaluation on a variety of common urban driving scenarios, we show that our model is capable of producing accurate predictions for both short and long term timesteps. We also demonstrate the significance of our model architecture through an ablation study.},
  archive   = {C_IROS},
  author    = {Neel P. Bhatt and Amir Khajepour and Ehsan Hashemi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981046},
  pages     = {9837-9844},
  title     = {MPC-PF: Social interaction aware trajectory prediction of dynamic objects for autonomous driving using potential fields},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Flash: Fast and light motion prediction for autonomous
driving with bayesian inverse planning and learned motion profiles.
<em>IROS</em>, 9829–9836. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion prediction of road users in traffic scenes is critical for autonomous driving systems that must take safe and robust decisions in complex dynamic environments. We present a novel motion prediction system for autonomous driving. Our system is based on the Bayesian inverse planning framework, which efficiently orchestrates map-based goal extraction, a classical control-based trajectory generator and a mixture of experts collection of light-weight neural networks specialised in motion profile prediction. In contrast to many alternative methods, this modularity helps isolate performance factors and better interpret results, without compromising performance. This system addresses multiple aspects of interest, namely multi-modality, motion profile uncertainty and trajectory physical feasibility. We report on several experiments with the popular highway dataset NGSIM, demonstrating state-of-the-art performance in terms of trajectory error. We also perform a detailed analysis of our system&#39;s components, along with experiments that stratify the data based on behaviours, such as change-lane versus follow-lane, to provide insights into the challenges in this domain. Finally, we present a qualitative analysis to show other benefits of our approach, such as the ability to interpret the outputs.},
  archive   = {C_IROS},
  author    = {Morris Antonello and Mihai Dobre and Stefano V. Albrecht and John Redford and Subramanian Ramamoorthy},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981347},
  pages     = {9829-9836},
  title     = {Flash: Fast and light motion prediction for autonomous driving with bayesian inverse planning and learned motion profiles},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Disentangled sequence clustering for human intention
inference. <em>IROS</em>, 9814–9820. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Equipping robots with the ability to infer human intent is a vital precondition for effective collaboration. Most computational approaches towards this objective derive a probability distribution of “intent” conditioned on the robot&#39;s perceived state. However, these approaches typically assume task-specific labels of human intent are known a priori. To overcome this constraint, we propose the Disentangled Sequence Clustering Variational Autoencoder (DiSCVAE), a clustering framework capable of learning such a distribution of intent in an unsupervised manner. The proposed framework leverages recent advances in unsupervised learning to disentangle latent representations of sequence data, separating time-varying local features from time-invariant global attributes. As a novel extension, the DiSCVAE also infers a discrete variable to form a latent mixture model and thus enable clustering over these global sequence concepts, e.g. high-level intentions. We evaluate the DiSCVAE on a real-world human-robot interaction dataset collected using a robotic wheelchair. Our findings reveal that the inferred discrete variable coincides with human intent, holding promise for collaborative settings, such as shared control.},
  archive   = {C_IROS},
  author    = {Mark Zolotas and Yiannis Demiris},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981116},
  pages     = {9814-9820},
  title     = {Disentangled sequence clustering for human intention inference},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intention estimation from gaze and motion features for
human-robot shared-control object manipulation. <em>IROS</em>,
9806–9813. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Shared control can help in teleoperated object manipulation by assisting with the execution of the user&#39;s intention. To this end, robust and prompt intention estimation is needed, which relies on behavioral observations. Here, an intention estimation framework is presented, which uses natural gaze and motion features to predict the current action and the target object. The system is trained and tested in a simulated environment with pick and place sequences produced in a relatively cluttered scene and with both hands, with possible hand-over to the other hand. Validation is conducted across different users and hands, achieving good accuracy and earliness of prediction. An analysis of the predictive power of single features shows the predominance of the grasping trigger and the gaze features in the early identification of the current action. In the current framework, the same probabilistic model can be used for the two hands working in parallel and independently, while a rule-based model is proposed to identify the resulting bimanual action. Finally, limitations and perspectives of this approach to more complex, full-bimanual manipulations are discussed.},
  archive   = {C_IROS},
  author    = {Anna Belardinelli and Anirudh Reddy Kondapally and Dirk Ruiken and Daniel Tanneberg and Tomoki Watabe},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982249},
  pages     = {9806-9813},
  title     = {Intention estimation from gaze and motion features for human-robot shared-control object manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A soft fabric-based shrink-to-fit pneumatic sleeve for
comfortable limb assistance. <em>IROS</em>, 9766–9773. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Upper limb impairments and weakness are com-mon post-stroke and with advanced aging. Rigid exoskeletons have been developed as a potential solution, but have had limited impact. In addition to user concerns about safety, their weight and appearance, the rigid attachment and typical anchoring methods can result in skin damage. In this paper, we present a soft, fabric-based pneumatic sleeve, which can shrink from a loose fit to a tight fit in order to anchor to the limbs temporarily, thereby enabling the application of mechanical assistance only when needed. The sleeve is comfortable, ergonomic and can be embedded unobtrusively with clothing. A mathematical model is built to simulate and design sleeves with different geometric parameters. The best sleeve was capable of generating a friction force of 98 N on the limb when inflated to 25 kPa. This sleeve was used to create a wearable assistive device, integrated with a cable-driven actuator. This device was able to lift a 1.44 kg forearm rig up to 95 degree at low pressure of 20 kPa. The device was tested with six healthy participants, in terms of fit, comfort and assistive functionality. The average acceptable sleeve pressure was found to be 33±4.7 kPa. All participants liked the appearance of the sleeve, with a high average perceived assistance score of 7.33±1.6 (out of 10). The shrink-to-fit sleeve is expected to significantly increase the development and adoption of soft robotic assistive devices and emerging powered clothing.},
  archive   = {C_IROS},
  author    = {Richard Suphapol Diteesawat and Sam Hoh and Emanuele Pulvirenti and Nahian Rahman and Leah Morris and Ailie Turton and Mary Cramp and Jonathan Rossiter},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981265},
  pages     = {9766-9773},
  title     = {A soft fabric-based shrink-to-fit pneumatic sleeve for comfortable limb assistance},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RANK - robotic ankle: Design and testing on irregular
terrains. <em>IROS</em>, 9752–9757. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite the large amount of available exoskeletons, their use in daily life is still limited due to the absence of testing in real-life environments. Thus, the present work aims to test on a series of uneven terrains a wearable ankle exoskeleton, named RANK, designed for walking assistance and drop-foot prevention. RANK consists of a 3D-printed brace attached to the user and a piezoresistive insole, to be incorporated into the user&#39;s shoe. Real-time analysis of the insole&#39;s sensor outputs enables the control system to provide torque assistance to the ankle joint through a four-bar linkage mechanism. Two healthy male subjects were enrollee, asking them to walk on three different terrain conditions (flat, soft, and irregular) with and without exoskeleton. Human kinematics was gathered via inertial measurements units (IMUs). The effects of ankle exoskeleton on lower limb joint angles were assessed in terms of range of motion (ROM), whereas statistical parametric map method was applied to compare joint angle curves. As expected, a reduction of the ankle ROM approximatively of 10° was found in all terrain conditions between the trails performed with and without exoskeleton. No effects induced on the hip and knee joint were observed. Moreover, no significant differences were found over the almost totality of the gait cycle regardless the terrain conditions. Results demonstrate the capability of the exoskeleton to work properly regardless the type of walking surface.},
  archive   = {C_IROS},
  author    = {J. Taborri and I. Mileti and G. Mariani and L. Mattioli and L. Liguori and S. Salvatori and E. Palermo and F. Patanè and S. Rossi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981580},
  pages     = {9752-9757},
  title     = {RANK - robotic ankle: Design and testing on irregular terrains},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Human-exoskeleton cooperative balance strategy for a
human-powered augmentation lower exoskeleton. <em>IROS</em>, 9744–9751.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Lower Limb Exoskeletons (LLE) have received considerable interest in strength augmentation, rehabilitation, and walking assistance scenarios. For strength augmentation, LLE is expected to have the capability of reducing metabolic energy. However, the energy for adjusting Center of Gravity (CoG) is a main part of the total energy consumed during walking. This paper proposes a novel Human-exoskeleton Cooperative Balance (HCB) strategy which gives assistive torques balance ability and combine with the direction selected by the pilot to achieve balance walking of human-exoskeleton systems. In which, a Dynamic Torque Primitive Model (DTPM) is designed to plan a bionic assistive torque, and the balance parameters obtained by an Inverted Pendulum Model (IPM) is superimposed on it. Finally, the performance improved by the HCB strategy can break the limitation of traditional strategies and substantially increase the efficiency of assistance. We demonstrated the effectiveness of the proposed HCB strategy on the HUman-powered Augmentation Lower EXoskeleton (HUALEX) system. Experimental results indicate that the proposed HCB is more efficient than traditional strategies.},
  archive   = {C_IROS},
  author    = {Guangkui Song and Rui Huang and Zhinan Peng and Kecheng Shi and Long Zhang and Rong He and Jing Qiu and Huayi Zhan and Hong Cheng},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981568},
  pages     = {9744-9751},
  title     = {Human-exoskeleton cooperative balance strategy for a human-powered augmentation lower exoskeleton},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An impedance-controlled testbed for simulating variations in
the mechanical fit of wearable devices. <em>IROS</em>, 9736–9743. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The fit of a wearable device, such as a prosthesis, can be quantitatively characterized by the mechanical coupling at the user-device interface. It is thought that the mechanical impedance, specifically the stiffness and damping, of wearable device interfaces can significantly impact human performance while using them. To test this theory, we develop a forearm-mounted testbed with a motorized, two degree of freedom (2-DOF) gimbal to simulate variations in the mechanical fit of an upper-extremity wearable device during pointing and target tracking tasks. The two gimbal motors are impedance-controlled to vary the mechanical stiffness and damping between the user and the device&#39;s laser pointer end-effector. In this paper, experiments are conducted to determine the torque constants of the motors before implementation in the testbed, and to validate the accuracy of the joint impedance controller. The completed impedance-controlled wearable interface testbed is validated further by comparing the gimbal joint displacements and torques, recorded during 2-DOF base excitation experiments, to MATLAB Simulink simulation data.},
  archive   = {C_IROS},
  author    = {Alexander B. Ambrose and Chelse VanAtter and Frank L. Hammond},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981391},
  pages     = {9736-9743},
  title     = {An impedance-controlled testbed for simulating variations in the mechanical fit of wearable devices},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Transactional transform library for ROS. <em>IROS</em>,
9722–9727. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the Robot Operating System (ROS), a major middleware for robots, the Transform Library (TF) is a mandatory package that manages transformation information between coordinate systems by using a single-rooted directed tree and providing methods for registering and computing the information. However, the tree has two fundamental problems. The first is its poor scalability: since it accepts only a single thread at a time due to using a single giant lock for mutual exclusion, the access to the tree is sequential. Second, there is a lack of data freshness: it retrieves non-latest synthetic data when computing coordinate transformations because it prioritizes temporal consistency over data freshness. In this paper, we propose methods to solve these problems. First, we decentralize the giant lock to provide performance scalability and show that this results in a throughput 243 times higher than conventional TF on a read-only workload. Second, we design transactional methods based on serializable protocols that prevent anomalies, thus retrieving the freshest data. These transactional methods show a freshness up to 1276 times higher than the conventional one on a read-write combined workload.},
  archive   = {C_IROS},
  author    = {Yushi Ogiwara and Ayanori Yorozu and Akihisa Ohya and Hideyuki Kawashima},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981089},
  pages     = {9722-9727},
  title     = {Transactional transform library for ROS},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Containerization and orchestration of software for
autonomous mobile robots: A case study of mixed-criticality tasks across
edge-cloud computing platforms. <em>IROS</em>, 9708–9713. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Containerization promises to strengthen platform-independent development, better resource utilization, and secure deployment of software. As these benefits come with negligible overhead in CPU and memory utilization, containerization is increasingly being adopted in mobile robotic applications. An open challenge is supporting software tasks that have mixed-criticality requirements. Even more challenging is the combination of real-time containers with orchestration, which is an emerging paradigm to automate the deployment, networking, scaling, and availability of containerized workloads and services. This paper addresses this challenge by presenting a framework that extends the de-facto reference standard for container orchestration, Kubernetes, to schedule tasks with mixed-criticality requirements. Quantitative experimental results on the software implementing the mission of a Robotnik RB-Kairos mobile robot demonstrate the effectiveness of the proposed approach. The source code is publicly available on GitHub.},
  archive   = {C_IROS},
  author    = {Francesco Lumpp and Franco Fummi and Hiren D. Patel and Nicola Bombieri},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981581},
  pages     = {9708-9713},
  title     = {Containerization and orchestration of software for autonomous mobile robots: A case study of mixed-criticality tasks across edge-cloud computing platforms},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tasho: A python toolbox for rapid prototyping and deployment
of optimal control problem-based complex robot motion skills.
<em>IROS</em>, 9700–9707. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present Tasho (Task specification for receding horizon control), an open-source Python toolbox that facilitates systematic programming of optimal control problem (OCP)-based robot motion skills. Separation-of-concerns is followed while designing the components of a motion skill, which promotes their modularity and reusability. This allows us to program complex motion tasks by configuring and composing simpler tasks. We provide templates for several basic tasks like point-to-point and end-effector path-following tasks to speed up prototyping. Internally, the task&#39;s symbolic expressions are computed using CasADi and the resulting OCP is transcribed using Rockit. A wide and growing range of mature open-source optimization solvers are supported for solving the OCP. Monitor functions can be easily specified and are automatically deployed with the motion skill, so that the generated motion skills can be easily embedded in a larger control architecture involving higher-level discrete controllers. The motion skills thus programmed can be directly deployed on robot platforms using the C-code generation capabilities of CasADi. The toolbox has been validated through several experiments both in simulation and on physical robot systems. The open-source toolbox can be accessed at: https://gitlab.kuleuven.be/meco-software/tasho},
  archive   = {C_IROS},
  author    = {Ajay Suresha Sathya and Alejandro Astudillo and Joris Gillis and Wilm Decré and Goele Pipeleers and Jan Swevers},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981681},
  pages     = {9700-9707},
  title     = {Tasho: A python toolbox for rapid prototyping and deployment of optimal control problem-based complex robot motion skills},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RobotCore: An open architecture for hardware acceleration in
ROS 2. <em>IROS</em>, 9692–9699. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hardware acceleration can revolutionize robotics, enabling new applications by speeding up robot response times while remaining power-efficient. However, the diversity of acceleration options makes it difficult for roboticists to easily deploy accelerated systems without expertise in each specific hardware platform. In this work, we address this challenge with RobotCore, an architecture to integrate hardware acceleration in the widely-used ROS 2 robotics software framework. This architecture is target-agnostic (supports edge, workstation, data center, or cloud targets) and accelerator-agnostic (supports both FPGAs and GPUs). It builds on top of the common ROS 2 build system and tools and is easily portable across different research and commercial solutions through a new firmware layer. We also leverage the Linux Tracing Toolkit next generation (LTTng) to enable low-overhead real-time tracing and benchmarking of accelerated ROS 2 systems. To demonstrate the acceleration enabled by this architecture, we use it to deploy a ROS 2 perception computational graph on a CPU and FPGA. We also employ our integrated tracing and benchmarking to analyze bottlenecks, uncovering insights that guide us to improve FPGA communication efficiency. In particular, we design an intra-FPGA ROS 2 node communication queue template and use it in conjunction with FPGA-accelerated nodes to achieve a 24.42\% speedup over a CPU.},
  archive   = {C_IROS},
  author    = {Víctor Mayoral-Vilches and Sabrina M. Neuman and Brian Plancher and Vijay Janapa Reddi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982082},
  pages     = {9692-9699},
  title     = {RobotCore: An open architecture for hardware acceleration in ROS 2},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data-driven variable impedance control of a powered
knee-ankle prosthesis for sit, stand, and walk with minimal tuning.
<em>IROS</em>, 9660–9667. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although the average healthy adult transitions from sit to stand over 60 times per day, most research on powered prosthesis control has only focused on walking. In this paper, we present a data-driven controller that enables sitting, standing, and walking with minimal tuning. Our controller comprises two high level modes of sit/stand and walking, and we develop heuristic biomechanical rules to control transitions. We use a phase variable based on the user&#39;s thigh angle to parameterize both walking and sit/stand motions, and use variable impedance control during ground contact and position control during swing. We extend previous work on data-driven optimization of continuous impedance parameter functions to design the sit/stand control mode using able-bodied data. Experiments with a powered knee-ankle prosthesis used by a participant with above-knee amputation demonstrate promise in clinical outcomes, as well as trade-offs between our minimal-tuning approach and accommodation of user preferences. Specifically, our controller enabled the participant to complete the sit/stand task 20\% faster and reduced average asymmetry by half compared to his everyday passive prosthesis. The controller also facilitated a timed up and go test involving sitting, standing, walking, and turning, with only a mild (10\%) decrease in speed compared to the everyday prosthesis. Our sit/stand/walk controller enables multiple activities of daily life with minimal tuning and mode switching.},
  archive   = {C_IROS},
  author    = {Cara Gonzalez Welker and T. Kevin Best and Robert D. Gregg},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982037},
  pages     = {9660-9667},
  title     = {Data-driven variable impedance control of a powered knee-ankle prosthesis for sit, stand, and walk with minimal tuning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Development of low-inertia backdrivable arm focusing on
learning-based control. <em>IROS</em>, 9642–9649. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A robot designed to coexist and work with humans in the same workspace should be able to work at the same speed as humans and have safe contact with humans and with the environment. However, when a robot arm has been given flexibility through mechanisms and controls for the purpose of coexistence, it is difficult for it to perform tasks at the speed and accuracy desired by humans if it is moved simply by using conventional position-based controls. With such an arm, we consider that the use of learning-based control is necessary to achieve both safety and speed. Therefore, we prototyped a low-inertia, high-backdrivability arm as a platform for studying learning-based control and tested two types of learning-based control. This paper describes our design process, in which hardware suitable for learning-based control was developed according to the requirements of the specific task. It also presents the results of our evaluation experiments, in which tasks involving quick movements and motion requiring physical contact with an object were performed using learning-based control.},
  archive   = {C_IROS},
  author    = {Manabu Nishiura and Akira Hatano and Kazutoshi Nishii and Yoshihiro Okumatsu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981337},
  pages     = {9642-9649},
  title     = {Development of low-inertia backdrivable arm focusing on learning-based control},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An observer-based responsive variable impedance control for
dual-user haptic training system. <em>IROS</em>, 9635–9641. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a variable impedance control architecture to facilitate eye surgery training in a dual-user haptic system. In this system, an expert surgeon (the trainer) and a novice surgeon (the trainee) collaborate on a surgical procedure using their own haptic devices. The mechanical impedance parameters of the trainer&#39;s haptic device remain constant during the operation, whereas those of the trainee vary with his/her proficiency level. The trainee&#39;s relative proficiency might be objectively quantified in real-time based on position error between the trainer and the trainee. The proposed architecture enables the trainer to intervene in the training process as needed to ensure the trainee is following the right course of action and to avoid the trainee&#39;s from potential tissue injuries. The stability of the overall nonlinear closed-loop system has been investigated using the input-to-state stability (ISS) criterion. High-gain observer with unknown inputs is considered in this work to estimate the interaction forces. Simulation and experimental results under different scenarios confirm the effectiveness of the proposed control methods.},
  archive   = {C_IROS},
  author    = {A. Rashvand and R. Heidari and M. Motaharifar and A. Hassani and M.R. Dindarloo and M. J. Ahmadi and K. Hashtrudi-Zaad and M. Tavakoli and H. D. Taghirad},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981977},
  pages     = {9635-9641},
  title     = {An observer-based responsive variable impedance control for dual-user haptic training system},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Electro-adhesive tubular clutch for variable-stiffness
robots. <em>IROS</em>, 9628–9634. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Electro-adhesive clutches have become effective tools for variable stiffness functions in many robotic systems due to their light weight, high speed and strong brake force. In this paper, we present a novel, tubular design of an electro-adhesive clutch. Our clutch consists of flexible electrode sheets rolled into a tubular structure. This design allows encapsulating large electrode areas in a compact size for strong brake force. Additionally, the tubular structure acts as a guide for directional sliding without external guides. The structure also ensures that the electrode surfaces are encapsulated, preventing the accumulation of dust and thus leading to reliable performance. This structure is therefore an improvement over the commonly used planar designs. The characterization of the electro-adhesive tubular clutch shows that the frictional force increases with the increase of the electrode contact area, the decrease of the roll diameter and the dielectric layer thickness. A retractable tubular clutch is made by fixing an elastic cable along the clutch axis and achieves a stiffness change factor up to 260. Applications of this retractable clutch in robotics to achieve variable stiffness are demonstrated in two systems: a tensegrity structure and a wing skeleton. Changes in stiffness by 13.2 and 30.2 times are achieved for the two systems, respectively. The proposed tubular clutch is an effective means of achieving variable stiffness, particularly in the case of robotic systems that transmit forces through tensioned cables.},
  archive   = {C_IROS},
  author    = {Yi Sun and Krishna Manaswi Digumarti and Hoang-Vu Phan and Omar Aloui and Dario Floreano},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982098},
  pages     = {9628-9634},
  title     = {Electro-adhesive tubular clutch for variable-stiffness robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A whole-body controller based on a simplified template for
rendering impedances in quadruped manipulators. <em>IROS</em>,
9620–9627. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Quadrupedal manipulators require to be compliant when dealing with external forces during autonomous manipulation, tele-operation or physical human-robot interaction. This paper presents a whole-body controller that allows for the implementation of a Cartesian impedance control to coordinate tracking performance and desired compliance for the robot base and manipulator arm. The controller is formulated through an optimization problem using Quadratic Programming (QP) to impose a desired behavior for the system while satisfying friction cone constraints, unilateral force constraints, joint and torque limits. The presented strategy decouples the arm and the base of the platform, enforcing the behavior of a linear double-mass spring damper system, and allows to independently tune their inertia, stiffness and damping properties. The control architecture is validated through an extensive simulation study using the 90kg HyQ robot equipped with a 7-DoF manipulator arm. Simulation results show the impedance rendering performance when external forces are applied at the arm&#39;s end-effector. The paper presents results for full stance condition (all legs on the ground) and, for the first time, also shows how the impedance rendering is affected by the contact conditions during a dynamic gait.},
  archive   = {C_IROS},
  author    = {Mattia Risiglione and Victor Barasuol and Darwin G. Caldwell and Claudio Semini},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981895},
  pages     = {9620-9627},
  title     = {A whole-body controller based on a simplified template for rendering impedances in quadruped manipulators},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Passivity-based skill motion learning in stiffness-adaptive
unified force-impedance control. <em>IROS</em>, 9604–9611. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tactile robots shall be deployed for dynamic task execution in production lines with small batch sizes. Therefore, these robots should have the ability to respond to changing conditions and be easy to (re-)program. Operating under uncertain environments requires unifying subsystems such as robot motion and force policy into one framework, referred to as tactile skills. In this paper, we propose the enhancement of these skills for passivity-based skill motion learning in stiffness-adaptive unified force-impedance control. To achieve the increased level of adaptability, we represent all tactile skills by three basic primitives: contact initiation, manipulation, and contact termination. To ensure passivity and stability, we develop an energy-based approach for unified force-impedance control that allows humans to teach the robot motion through physical interaction during the execution of a tactile task. We incorporate our proposed framework into a tactile robot to experimentally validate the motion adaptation by interaction performance and stability of the control. While the polishing task is presented as our use case through the paper, the experiments can also be carried out with various tactile skills. Finally, the results show the novel controller&#39;s stability and passivity to contact-loss and stiffness adaptation, leading to successful programming by interaction.},
  archive   = {C_IROS},
  author    = {Kübra Karacan and Hamid Sadeghian and Robin Kirschner and Sami Haddadin},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981728},
  pages     = {9604-9611},
  title     = {Passivity-based skill motion learning in stiffness-adaptive unified force-impedance control},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Shape memory polymer variable stiffness magnetic catheters
with hybrid stiffness control. <em>IROS</em>, 9589–9595. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Variable stiffness catheters typically rely on thermally induced stiffness transitions with a transition temperature above body temperature. This imposes considerable safety limitations for medical applications. In this work, we present a variable stiffness catheter using a hybrid control strategy capable of actively heating and actively cooling the catheter material. The proposed catheter is made of a single biocompatible shape memory polymer, which significantly increases its manufacturability and scalability compared to existing designs. Potentially increased safety is obtained by ensuring a lower-risk compliant state at body temperature while maintaining higher stiffness ranges in actively controlled states. Additionally, the combined use of variable stiffness and magnetic actuation increases the dexterity and steerability of the device compared to existing robotic tools.},
  archive   = {C_IROS},
  author    = {Michael Mattmann and Quentin Boehler and Xiang-Zhong Chen and Salvador Pané and Bradley J. Nelson},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981935},
  pages     = {9589-9595},
  title     = {Shape memory polymer variable stiffness magnetic catheters with hybrid stiffness control},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Colonoscopy navigation using end-to-end deep visuomotor
control: A user study. <em>IROS</em>, 9582–9588. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Flexible Endoscopes (FEs) for colonoscopy present several limitations due to their inherent complexity, resulting in patient discomfort and lack of intuitiveness for clinicians. Robotic FEs with autonomous control represent a viable solution to reduce the workload of endoscopists and the training time while improving the procedure outcome. Prior works on autonomous endoscope FE control use heuristic policies that limit their generalisation to the unstructured and highly deformable colon environment and require frequent human intervention. This work proposes an image-based FE control using Deep Reinforcement Learning, called Deep Visuomotor Control (DVC), to exhibit adaptive behaviour in convoluted sections of the colon. DVC learns a mapping between the images and the FE control signal. A first user study of 20 expert gastrointestinal endoscopists was carried out to compare their navigation performance with DVC using a realistic virtual simulator. The results indicate that DVC shows equivalent performance on several assessment parameters, being more safer. Moreover, a second user study with 20 novice users was performed to demonstrate easier human supervision compared to a state-of-the-art heuristic control policy. Seamless supervision of colonoscopy procedures would enable endoscopists to focus on the medical decision rather than on the control of FE.},
  archive   = {C_IROS},
  author    = {Ameya Pore and Martina Finocchiaro and Diego Dall&#39;Alba and Albert Hernansanz and Gastone Ciuti and Alberto Arezzo and Arianna Menciassi and Alicia Casals and Paolo Fiorini},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981480},
  pages     = {9582-9588},
  title     = {Colonoscopy navigation using end-to-end deep visuomotor control: A user study},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A dataset and benchmark for learning the kinematics of
concentric tube continuum robots. <em>IROS</em>, 9550–9557. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Establishing a physics-based model capturing the kinetostatic behavior of concentric tube continuum robots is challenging as elastic interactions between the flexible tubes constituting the robot result in a highly non-linear problem. The Goldstandard physics-based model using the Cosserat theory of elastic rods achieves reasonable approximations with 1.5 - 3\% with respect to the robot&#39;s length, if well-calibrated. Learning-based models of concentric tube continuum robots have been shown to outperform the Goldstandard model with approximation errors below 1\%. Yet, the merits of learning-based models remain largely unexplored as no common dataset and benchmark exist. In this paper, we present a dataset captured from a three-tube concentric tube continuum robot for use in learning-based kinematics research. The dataset consists of 100 000 joint configurations and the corresponding four 6 dof sensors in SE(3) measured with an electromagnetic tracking system (github.com/ContinuumRoboticsLab/CRL-Dataset-CTCR-Pose). With our dataset, we empower the continuum robotics and machine learning community to advance the field. We share our insights and lessons learned on joint space representation, shape representation in task space, and sampling strategies. Furthermore, we provide benchmark results for learning the forward kinematics using a simple, shallow feedforward neural network. The benchmark results for the tip error are 0.74 mm w.r.t. position (0.4\% of total robot length) and 6.49° w.r.t. orientation.},
  archive   = {C_IROS},
  author    = {Reinhard M. Grassmann and Ryan Zeyuan Chen and Nan Liang and Jessica Burgner-Kahrs},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981719},
  pages     = {9550-9557},
  title     = {A dataset and benchmark for learning the kinematics of concentric tube continuum robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GESRsim: Gastrointestinal endoscopic surgical robot
simulator. <em>IROS</em>, 9542–9549. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot-assisted gastrointestinal endoscopic surgery (GES) as a kind of natural orifice transluminal endoscopic surgery (NOTES) is the next-generation minimally invasive surgery (MIS). Besides, rendering certain autonomy to a Gas-trointestinal Endoscopic Surgical Robot (GESR) is promising but highly challenging. Therefore, to accelerate the development and augment the autonomy of GESR, we use CoppeliaSim to develop the first robotic simulator for the GESR system (GESRsim) based on our previous design. The GESRsim provides several 3D models and kinematics of our designed manipulators and endoscopic snake bone. Additionally, we build several scenes for robotic GES training and then utilize different programming interfaces to perform teleoperation. Furthermore, several advanced control algorithms, including visual servoing (VS) and deep reinforcement learning (DRL), are implemented to verify the performance of the GESRsim.},
  archive   = {C_IROS},
  author    = {Huxin Gao and Zedong Zhang and Changsheng Li and Xiao Xiao and Liang Qiu and Xiaoxiao Yang and Ruoyi Hao and Xiuli Zuo and Yanqing Li and Hongliang Ren},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982138},
  pages     = {9542-9549},
  title     = {GESRsim: Gastrointestinal endoscopic surgical robot simulator},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design and development of a lorentz force-based MRI-driven
neuroendoscope. <em>IROS</em>, 9534–9541. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The introduction of neuroendoscopy, microneu- rosurgery, neuronavigation, and intraoperative imaging for surgical operations has made significant improvements over other traditionally invasive surgical techniques. The integration of magnetic resonance imaging (MRI)-driven surgical devices with intraoperative imaging and endoscopy can enable further advancements in surgical treatments and outcomes. This work proposes the design and development of an MRI-driven endo- scope leveraging the high (3–7 T), external magnetic field of an MR scanner for heat-mitigated steering within the ventricular system of the brain. It also demonstrates the effectiveness of a Lorentz force-based grasper for diseased tissue manipulation and ablation. Feasibility studies show the neuroendoscope can be steered precisely within the lateral ventricle to locate a tumor using both MRI and endoscopic guidance. Results also indicate grasping forces as high as 31 mN are possible and power inputs as low as 0.69 mW can cause cancerous tissue ablation. These findings enable further developments of steerable devices using MR imaging integrated with endoscopic guidance for improved outcomes.},
  archive   = {C_IROS},
  author    = {Martin Francis Phelan and Nihal Olcay Dogan and Jelena Lazovic and Metin Sitti},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981526},
  pages     = {9534-9541},
  title     = {Design and development of a lorentz force-based MRI-driven neuroendoscope},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A metric for finding robust start positions for medical
steerable needle automation. <em>IROS</em>, 9526–9533. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Steerable needles are medical devices with the ability to follow curvilinear paths to reach targets while circumventing obstacles. In the deployment process, a human operator typically places the steerable needle at its start position on a tissue surface and then hands off control to the automation that steers the needle to the target. Due to uncertainty in the placement of the needle by the human operator, choosing a start position that is robust to deviations is crucial since some start positions may make it impossible for the steerable needle to safely reach the target. We introduce a method to efficiently evaluate steerable needle motion plans such that they are safe to variation in the start position. This method can be applied to many steerable needle planners and requires that the needle&#39;s orientation angle at insertion can be robotically controlled. Specifically, we introduce a method that builds a funnel around a given plan to determine a safe insertion surface corresponding to insertion points from which it is guaranteed that a collision-free motion plan to the goal can be computed. We use this technique to evaluate multiple feasible plans and select the one that maximizes the size of the safe insertion surface. We evaluate our method through simulation in a lung biopsy scenario and show that the method is able to quickly find needle plans with a large safe insertion surface.},
  archive   = {C_IROS},
  author    = {Janine Hoelscher and Inbar Fried and Mengyu Fu and Mihir Patwardhan and Max Christman and Jason Akulian and Robert J. Webster and Ron Alterovitz},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982227},
  pages     = {9526-9533},
  title     = {A metric for finding robust start positions for medical steerable needle automation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A tightly-coupled event-inertial odometry using exponential
decay and linear preintegrated measurements. <em>IROS</em>, 9475–9482.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce an event-based visual odometry and mapping framework that relies on decaying event-based corners. Event cameras, unlike conventional cam-eras, can provide sensor data during high-speed motions or in scenes with high dynamic ranges. Rather than providing intensity information at a global shutter rate, events are trig-gered asynchronously depending on whether there is a change in brightness at the pixel location. This novel sensing paradigm calls for unconventional ego-motion estimation techniques to address these new challenges. The key aspect of our framework is the use of a continuous representation of inertial measurements to characterise the system&#39;s motion which accommodates the asynchronous nature of the event data while estimating a discrete state in an optimisation-based approach. The proposed method relies on corners extracted from events-only data and associates them with a spatio-temporal locality scheme based on exponential decay. Event tracks are then tightly coupled with temporally accurate preintegrated inertial measurements, allowing for the estimation of ego-motion and a sparse map. The proposed method is evaluated on the Event Camera Dataset showing performance against the state-of-art in event-based visual-inertial odometry.},
  archive   = {C_IROS},
  author    = {Benny Dai and Cedric Le Gentil and Teresa Vidal-Calleja},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981249},
  pages     = {9475-9482},
  title     = {A tightly-coupled event-inertial odometry using exponential decay and linear preintegrated measurements},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). WiSARD: A labeled visual and thermal image dataset for
wilderness search and rescue. <em>IROS</em>, 9467–9474. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sensor-equipped unoccupied aerial vehicles (UAVs) have the potential to help reduce search times and alleviate safety risks for first responders carrying out Wilderness Search and Rescue (WiSAR) operations, the process of finding and rescuing person(s) lost in wilderness areas. Unfortunately, visual sensors alone do not address the need for robustness across all the possible terrains, weather, and lighting conditions that WiSAR operations can be conducted in. The use of multi-modal sensors, specifically visual-thermal cameras, is critical in enabling WiSAR UAVs to perform in diverse operating conditions. However, due to the unique challenges posed by the wilderness context, existing dataset benchmarks are inadequate for developing vision-based algorithms for autonomous WiSAR UAVs. To this end, we present WiSARD, a dataset with roughly 56,000 labeled visual and thermal images collected from UAV flights in various terrains, seasons, weather, and lighting conditions. To the best of our knowledge, WiSARD is the first large-scale dataset collected with multi-modal sensors for autonomous WiSAR operations. We envision that our dataset will provide researchers with a diverse and challenging benchmark that can test the robustness of their algorithms when applied to real-world (life-saving) applications. Link to dataset: https://sites.google.com/uw.edu/wisard/},
  archive   = {C_IROS},
  author    = {Daniel Broyles and Christopher R. Hayner and Karen Leung},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981298},
  pages     = {9467-9474},
  title     = {WiSARD: A labeled visual and thermal image dataset for wilderness search and rescue},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scalable probabilistic gas distribution mapping using
gaussian belief propagation. <em>IROS</em>, 9459–9466. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper advocates the Gaussian belief propagation solver for factor graphs in the case of gas distribution mapping to support an olfactory sensing robot. The local message passing of belief propagation moves away from the standard Cholesky decomposition technique, which avoids solving the entire factor graph at once and allows for only areas of interest to be updated more effectively. Implementing a local solver means that iterative updates to the distribution map can be achieved orders of magnitude quicker than conventional direct solvers which scale computationally to the size of the map. After defining the belief propagation algorithm for gas mapping, several state of the art message scheduling algorithms are tested in simulation against the standard Cholesky solver for their ability to converge to the exact solution. Testing shows that under the wildfire scheduling method for a large urban scenario, that distribution maps can be iterated at least 10 times faster whilst still maintaining exact solutions. This move to an efficient local framework allows future works to consider 3D mapping, predictive utility and multi-robot distributed mapping.},
  archive   = {C_IROS},
  author    = {Callum Rhodes and Cunjia Liu and Wen-Hua Chen},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981901},
  pages     = {9459-9466},
  title     = {Scalable probabilistic gas distribution mapping using gaussian belief propagation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards robust visual-inertial odometry with multiple
non-overlapping monocular cameras. <em>IROS</em>, 9452–9458. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a Visual-Inertial Odometry (VIO) algorithm with multiple non-overlapping monocular cameras aiming at improving the robustness of the VIO algorithm. An initialization scheme and tightly-coupled bundle adjustment for multiple non-overlapping monocular cameras are proposed. With more stable features captured by multiple cameras, VIO can maintain stable state estimation, especially when one of the cameras tracked unstable or limited features. We also address the high CPU usage rate brought by multiple cameras by proposing a GPU-accelerated frontend. Finally, we use our pedestrian carried system to evaluate the robustness of the VIO algorithm in several challenging environments. The results show that the multi-camera setup yields significantly higher estimation robustness than a monocular system while not increasing the CPU usage rate (reducing the CPU resource usage rate and computational latency by 40.4\% and 50.6\% on each camera). A demo video can be found at https://youtu.be/r7QvPth1m10.},
  archive   = {C_IROS},
  author    = {Yao He and Huai Yu and Wen Yang and Sebastian Scherer},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981664},
  pages     = {9452-9458},
  title     = {Towards robust visual-inertial odometry with multiple non-overlapping monocular cameras},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast and safe exploration via adaptive semantic perception
in outdoor environments. <em>IROS</em>, 9445–9451. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous exploration in unknown environments is a fundamental task for robots. Existing approaches mostly were concentrated on the efficiency of the exploration with the assumption of perfect state estimation, but the drift of pose estimation in visual SLAM occurs frequently and is detrimental to robot&#39;s localization and exploration performance. In this paper, a perception-aware exploration(PAE) method is proposed for rapidly and safely autonomous exploration in outdoor environments. The adaptive semantic information is proposed to improve the robustness of perception. Based on the perception module, both the selection of exploration goal on a novel weighted information gain and path planning can avoid the areas with high localization uncertainty. In addition, thanks to the proposed pipeline, including scan-based frontier detection, kd-tree based map prediction and suboptimal frontier buffer strategy, the PAE planner can explore the environment with high success rate and high efficiency. Several simulations are performed to verify the effectiveness of our methods.},
  archive   = {C_IROS},
  author    = {Zhihao Wang and Lingxu Chen and Hongjin Chen and Haoyao Chen and Xin Jiang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981640},
  pages     = {9445-9451},
  title     = {Fast and safe exploration via adaptive semantic perception in outdoor environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Variable stiffness object recognition with bayesian
convolutional neural network on a soft gripper. <em>IROS</em>,
9431–9436. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {From a medical standpoint, detecting the size and shape of hard inclusions hidden in soft three-dimensional objects is of great significance for early detection of cancer through palpation. Soft robots, especially soft grippers, substantially broaden robots&#39; palpation capabilities from soft to hard materials without the assistance of a camera. We have recently introduced a CNN-Bayes approach which added a Naïve Bayes classifier to a convolutional neural network (CNN) architecture called SoftTactNet for variable stiffness object recognition on a three-finger FinRay soft gripper. SoftTactNet itself lacks uncertainty estimations though it can reach a certain level of recognition accuracy. In this paper, we further improve the framework by merging Bayes method directly into CNN architectures and build a new Bayes-SoftTactNet for object recognition. The new approach, using a prior distribution instead of point estimation, allows the network to present results with uncertainty estimates. We conduct new experiments using the same soft gripper with tactile sensor arrays to grasp different variable stiffness objects surrounded by non-different soft material and generate tactile images as dataset. The results show that our new algorithm is more efficient than the previous approach and still able to achieve higher recognition accuracy than general deterministic CNNs.},
  archive   = {C_IROS},
  author    = {Jinyue Cao and Jingyi Huang and Andre Rosendo},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982051},
  pages     = {9431-9436},
  title     = {Variable stiffness object recognition with bayesian convolutional neural network on a soft gripper},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Model-based disturbance estimation for a fiber-reinforced
soft manipulator using orientation sensing. <em>IROS</em>, 9424–9430.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To aid in real-world situations, soft robots need to be able to estimate their state and external interactions based on proprioceptive sensors. Estimating disturbances allows a soft robot to perform desirable force control. However, even in the case of rigid manipulators, force estimation at the end-effector is seen as a non-trivial problem. And indeed, current approaches to address this challenge have shortcomings that prevent their general application. They are often based on simplified soft dynamic models, such as the ones relying on a piece-wise constant curvature approximation or matched rigid-body models that do not represent enough details of the problem. This severely limits applications in complex human-robot interaction. Finite element method (FEM) based modeling allows for predictions of soft robot dynamics in a more generic fashion. Here, using the soft robot modeling capabilities of the frame-work SOFA, we built a detailed FEM model of a multi-segment soft continuum robotic arm composed of compliant deformable materials and fiber-reinforced pressurized actuation chambers. In addition, a model for sensors that provide orientation output is presented. This model is used to establish a state observer for the manipulator. The sensor model is adequate for representing the output of flexible bend sensors as well as orientations provided by IMUs or coming from tracking systems, all of which are popular choices in soft robotics. Model parameters were calibrated to match imperfections of the manual fabrication process using physical experiments. We then solve a quadratic programming inverse statics problem to compute the components of external force that explain the pose mismatch. Our experiments show an average force estimation error of around 1.2\%. As the methods proposed are generic, these results are encouraging for the task of building soft robots exhibiting complex, reactive, sensor-based behavior that can be deployed in human-centered environments.},
  archive   = {C_IROS},
  author    = {Barnabas Gavin Cangan and Stefan Escaida Navarro and Bai Yang and Yu Zhang and Christian Duriez and Robert K. Katzschmann},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981637},
  pages     = {9424-9430},
  title     = {Model-based disturbance estimation for a fiber-reinforced soft manipulator using orientation sensing},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Planar modeling and sim-to-real of a tethered multimaterial
soft swimmer driven by peano-HASELs. <em>IROS</em>, 9417–9423. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft robotics has the potential to revolutionize robotic locomotion, in particular, soft robotic swimmers offer a minimally invasive and adaptive solution to explore and preserve our oceans. Unfortunately, current soft robotic swimmers are vastly inferior to evolved biological swimmers, especially in terms of controllability, efficiency, maneuverability, and longevity. Additionally, the tedious iterative fabrication and empirical testing required to design soft robots has hindered their optimization. In this work, we tackle this challenge by providing an efficient and straightforward pipeline for designing and fabricating soft robotic swimmers equipped with electrostatic actuation. We streamline the process to allow for rapid additive manufacturing, and show how a differentiable simulation can be used to match a simplified model to the real deformation of a robotic swimmer. We perform several experiments with the fabricated swimmer by varying the voltage and actuation frequency of the swimmer&#39;s antagonistic muscles. We show how the voltage and frequency vary the locomotion speed of the swimmer while moving in liquid oil and observe a clear optimum in forward swimming speed. The differentiable simulation model we propose has various downstream applications, such as control and shape optimization of the swimmer; optimization results can be directly mapped back to the real robot through our sim-to-real matching.},
  archive   = {C_IROS},
  author    = {Stephan-Daniel Gravert and Mike Y. Michelis and Simon Rogler and Dario Tscholl and Thomas Buchner and Robert K. Katzschmann},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981192},
  pages     = {9417-9423},
  title     = {Planar modeling and sim-to-real of a tethered multimaterial soft swimmer driven by peano-HASELs},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Contact-implicit trajectory and grasp planning for soft
continuum manipulators. <em>IROS</em>, 9401–9408. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As robots begin to move from structured industrial environments to the real world, they must be equipped to not only safely interact with the environment, but also reason about how to leverage contact to perform tasks. In this work, we develop a modeling and motion planning framework for continuum robots that accounts for contact anywhere along the robot. We first present an analytical model for continuum manipulators under contact and discuss the ideal choice of generalized coordinates given properties of the manipulator and task specifications. We then demonstrate the utility of our model by developing a motion planning framework that can solve a diverse set of tasks. We apply our framework to end effector path planning for a soft arm in an obstacle-rich environment, and grasp planning for soft robotic grippers, where contact can happen anywhere on the arm or gripper. Finally, we verify the utility of our model and planning framework by planning a grasp with a desired contact force for a soft antipodal gripper and testing this grasp in a hardware demonstration. Overall, our model and planning approach further enhance soft and continuum robots where they already excel: utilizing contact with the world to achieve their goals with a gentle touch.},
  archive   = {C_IROS},
  author    = {Moritz A. Graule and Clark B. Teeple and Robert J. Wood},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981044},
  pages     = {9401-9408},
  title     = {Contact-implicit trajectory and grasp planning for soft continuum manipulators},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A unified and modular model predictive control framework for
soft continuum manipulators under internal and external constraints.
<em>IROS</em>, 9393–9400. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fluidically actuated soft robots have promising capabilities such as inherent compliance and user safety. The control of soft robots needs to properly handle nonlinear actuation dynamics, motion constraints, workspace limitations, and variable shape stiffness, so having a unique algorithm for all these issues would be extremely beneficial. In this work, we adapt Model Predictive Control (MPC), popular for rigid robots, to a soft robotic arm called SoPrA. We address the challenges that current control methods are facing, by proposing a frame-work that handles these in a modular manner. While previous work focused on Joint-Space formulations, we show through simulation and experimental results that Task-Space MPC can be successfully implemented for dynamic soft robotic control. We provide a way to couple the Piece-wise Constant Curvature and Augmented Rigid Body Model assumptions with internal and external constraints and actuation dynamics, delivering an algorithm that unites these aspects and optimizes over them. We believe that a MPC implementation based on our approach could be the way to address most of model-based soft robotics control issues within a unified and modular framework, while allowing to include improvements that usually belong to other control domains such as machine learning techniques.},
  archive   = {C_IROS},
  author    = {Filippo A. Spinelli and Robert K. Katzschmann},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981702},
  pages     = {9393-9400},
  title     = {A unified and modular model predictive control framework for soft continuum manipulators under internal and external constraints},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A proprioceptive method for soft robots using inertial
measurement units. <em>IROS</em>, 9379–9384. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Proprioception, or the perception of the configuration of one&#39;s body, is challenging to achieve with soft robots due to their infinite degrees of freedom and incompatibility with most off-the-shelf sensors. This work explores the use of inertial measurement units (IMUs), sensors that output orientation with respect to the direction of gravity, to achieve soft robot proprioception. A simple method for estimating the shape of a soft continuum robot arm from IMUs mounted along the arm is presented. The approach approximates a soft arm as a serial chain of rigid links, where the orientation of each link is given by the output of an IMU or by spherical linear interpolation of the output of adjacent IMUs. In experiments conducted on a 660mm long real-world soft arm, this approach provided estimates of its end effector position with a median error of less than 10\% of the arm&#39;s length. This demonstrates the potential of IMUs to serve as inexpensive off-the-shelf sensors for soft robot proprioception.},
  archive   = {C_IROS},
  author    = {Yves J. Martin and Daniel Bruder and Robert J. Wood},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982185},
  pages     = {9379-9384},
  title     = {A proprioceptive method for soft robots using inertial measurement units},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards accurate modeling of modular soft pneumatic robots:
From volume FEM to cosserat rod. <em>IROS</em>, 9371–9378. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Compared to their rigid counterparts, soft material robotic systems offer great advantages when it comes to flexibility and adaptability. Despite their advantages, modeling of soft systems is still a challenging task, due to the continuous and often highly nonlinear nature of deformation these systems exhibit. Tasks like motion planning or design optimization of soft robots require computationally cheap models of the system&#39;s behavior. In this paper we address this need by deriving operational point dependent Cosserat rod models from detailed volume finite element models (FEM). While the latter offer detailed simulations, they generally come with high computational burden that hinders them from being used in time critical model-based methods like motion planning or control. Basic Cosserat rod models promise to provide computationally efficient mechanical models of soft continuum robots. By using a detailed FE model in an offline stage to identify operational point dependent Cosserat rod models, we bring together the accuracy of volumetric FEM with the efficiency of Cosserat rod models. We apply the approach to a fiber reinforced soft pneumatic bending actuator module (SPA module) and evaluate the model&#39;s predictive capabilities for a single module as well as a two-module robot.},
  archive   = {C_IROS},
  author    = {Mats Wiese and Benjamin-Hieu Cao and Annika Raatz},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981628},
  pages     = {9371-9378},
  title     = {Towards accurate modeling of modular soft pneumatic robots: From volume FEM to cosserat rod},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cloud-edge training architecture for sim-to-real deep
reinforcement learning. <em>IROS</em>, 9363–9370. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep reinforcement learning (DRL) is a promising approach to solve complex control tasks by learning policies through interactions with the environment. However, the training of DRL policies requires large amounts of training experiences, making it impractical to learn the policy directly on physical systems. Sim-to-real approaches leverage simulations to pretrain DRL policies and then deploy them in the real world. Unfortunately, the direct real-world deployment of pretrained policies usually suffers from performance deterioration due to the different dynamics, known as the reality gap. Recent sim-to-real methods, such as domain randomization and domain adaptation, focus on improving the robustness of the pretrained agents. Nevertheless, the simulation-trained policies often need to be tuned with real-world data to reach optimal performance, which is challenging due to the high cost of real-world samples. This work proposes a distributed cloud-edge architecture to train DRL agents in the real world in real-time. In the architecture, the inference and training are assigned to the edge and cloud, separating the real-time control loop from the computationally expensive training loop. To overcome the reality gap, our architecture exploits sim-to-real transfer strategies to continue the training of simulation-pretrained agents on a physical system. We demonstrate its applicability on a physical inverted-pendulum control system, analyzing critical parameters. The real-world experiments show that our architecture can adapt the pretrained DRL agents to unseen dynamics consistently and efficiently. 1 1 A video showing a real-world training process under the proposed method can be found from https://youtu.be/hMY9-c0SST0.},
  archive   = {C_IROS},
  author    = {Hongpeng Cao and Mirco Theile and Federico G. Wyrwal and Marco Caccamo},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981565},
  pages     = {9363-9370},
  title     = {Cloud-edge training architecture for sim-to-real deep reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Active exploration for robotic manipulation. <em>IROS</em>,
9355–9362. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic manipulation stands as a largely unsolved problem despite significant advances in robotics and machine learning in recent years. One of the key challenges in manipulation is the exploration of the dynamics of the environment when there is continuous contact between the objects being manipulated. This paper proposes a model-based active exploration approach that enables efficient learning in sparse-reward robotic manipulation tasks. The proposed method estimates an information gain objective using an ensemble of probabilistic models and deploys model predictive control (MPC) to plan actions online that maximize the expected reward while also performing directed exploration. We evaluate our proposed algorithm in simulation and on a real robot, trained from scratch with our method, on a challenging ball pushing task on tilted tables, where the target ball position is not known to the agent a-priori. Our real-world robot experiment serves as a fundamental application of active exploration in model-based reinforcement learning of complex robotic manipulation tasks. Project page https://sites.google.com/view/aerm.},
  archive   = {C_IROS},
  author    = {Tim Schneider and Boris Belousov and Georgia Chalvatzaki and Diego Romeres and Devesh K. Jha and Jan Peters},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982061},
  pages     = {9355-9362},
  title     = {Active exploration for robotic manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hybrid LMC: Hybrid learning and model-based control for
wheeled humanoid robot via ensemble deep reinforcement learning.
<em>IROS</em>, 9347–9354. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Control of wheeled humanoid locomotion is a challenging problem due to the nonlinear dynamics and under-actuated characteristics of these robots. Traditionally, feedback controllers have been utilized for stabilization and locomotion. However, these methods are often limited by the fidelity of the underlying model used, choice of controller, and environmental variables considered (surface type, ground inclination, etc). Recent advances in reinforcement learning (RL) offer promising methods to tackle some of these conventional feedback controller issues, but require large amounts of interaction data to learn. Here, we propose a hybrid learning and model-based controller Hybrid LMC that combines the strengths of a classical linear quadratic regulator (LQR) and ensemble deep reinforcement learning. Ensemble deep reinforcement learning is composed of multiple Soft Actor-Critic (SAC) and is utilized in reducing the variance of RL networks. By using a feedback controller in tandem the network exhibits stable performance in the early stages of training. As a preliminary step, we explore the viability of Hybrid LMC in controlling wheeled locomotion of a humanoid robot over a set of different physical parameters in MuJoCo simulator. Our results show that Hybrid LMC achieves better performance compared to other existing techniques and has increased sample efficiency.},
  archive   = {C_IROS},
  author    = {Donghoon Baek and Amartya Purushottam and Joao Ramos},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981913},
  pages     = {9347-9354},
  title     = {Hybrid LMC: Hybrid learning and model-based control for wheeled humanoid robot via ensemble deep reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simulation-based learning of the peg-in-hole process using
robot-skills. <em>IROS</em>, 9340–9346. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Increasingly volatile markets challenge companies and demand flexible production systems that can be quickly adapted to new conditions. Machine Learning has proven to show significant potential in supporting the human operator during the time-consuming and complex task of robot pro-gramming by identifying relevant parameters of the underlying robot control program. We present a solution to learn these parameters for contact-rich, force-controlled assembly tasks from a simulation using hardware-independent robot skills. We show that successful learning and real-world execution are possible even under process deviation and tolerances utilizing the designed learning system. We present learning skill param-eters as high-level robot control, evaluation and comparison of extensive simulations, and preliminary experiments on a physical robot test-bed. The developed solution approach is evaluated and discussed using the Peg-in-Hole process, a typical benchmark process in force-controlled assembly.},
  archive   = {C_IROS},
  author    = {Arik Lämmle and Philipp Tenbrock and Balázs Bálint and Frank Nägele and Werner Kraus and József Váncza and Marco F. Huber},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982212},
  pages     = {9340-9346},
  title     = {Simulation-based learning of the peg-in-hole process using robot-skills},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PourNet: Robust robotic pouring through curriculum and
curiosity-based reinforcement learning. <em>IROS</em>, 9332–9339. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pouring liquids accurately into containers is one of the most challenging tasks for robots as they are unaware of the complex fluid dynamics and the behavior of liquids when pouring. Therefore, it is not possible to formulate a generic pouring policy for real-time applications. In this paper, we propose PourNet, as a generalized solution to pouring different liquids into containers. PourNet is a hybrid planner that uses deep reinforcement learning, for end-effector planning, and Nonlinear Model Predictive Control, for joint planning. In this work, we introduce a novel simulation environment using Unity3D and NVIDIA-Flex to train our agents. By effective choice of the state space, action space and the reward functions, we allow for a direct sim-to-real transfer of the learned skills without additional training. In the simulation, PourNet outperforms state-of-the-art by an average of 4.9g deviation for water-like, and 9.2g deviation for honey-like liquids. In the real-world scenario using Kinova Movo Platform, PourNet achieves an average pouring deviation of 2.3g for dish soap when using a novel pouring container. The average pouring deviation measured for water was 5.5g. All comprehensive experiments and the simulation environment is available at: http://cxdcxd.github.io/RRS/.},
  archive   = {C_IROS},
  author    = {Edwin Babaians and Tapan Sharma and Mojtaba Karimi and Sahand Sharifzadeh and Eckehard Steinbach},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981195},
  pages     = {9332-9339},
  title     = {PourNet: Robust robotic pouring through curriculum and curiosity-based reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Federated learning from demonstration for active assistance
to smart wheelchair users. <em>IROS</em>, 9326–9331. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning from Demonstration (LfD) is a very appealing approach to empower robots with autonomy. Given some demonstrations provided by a human teacher, the robot can learn a policy to solve the task without explicit programming. A promising use case is to endow smart robotic wheelchairs with active assistance to navigation. By using LfD, it is possible to learn to infer short-term destinations anywhere, without the need of building a map of the environment beforehand. Nevertheless, it is difficult to generalize robot behaviors to environments other than those used for training. We believe that one possible solution is learning from crowds, involving a broad number of teachers (the end users themselves) who perform demonstrations in diverse and real environments. To this end, in this work we consider Federated Learning from Demonstration (FLfD), a distributed approach based on a Federated Learning architecture. Our proposal allows the training of a global deep neural network using sensitive local data (images and laser readings) with privacy guarantees. In our experiments we pose a scenario involving different clients working in heterogeneous domains. We show that the federated model is able to generalize and deal with non Independent and Identically Distributed (non-IID) data.},
  archive   = {C_IROS},
  author    = {Fernando E. Casado and Yiannis Demiris},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981998},
  pages     = {9326-9331},
  title     = {Federated learning from demonstration for active assistance to smart wheelchair users},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Leveraging multi-level modelling to automatically design
behavioral arbitrators in robotic controllers. <em>IROS</em>, 9318–9325.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automatic control design for robotic systems is becoming more and more popular. However, this usually involves a significant computational cost, due to the expensive and noisy evaluation of candidate solutions through high-fidelity simulation or even real hardware. This work aims at reducing the computational cost of automatic design of behavioral arbitrators through the introduction of a two-step approach. In the first step, the structure of the finite state machine governing the behavioral arbitrator is optimized. To this purpose, a more abstracted model of the robotic system is leveraged in order to significantly reduce the computational cost. In the second step, the close-to-hardware, behavioral parameters are fine-tuned using a high-fidelity model. We show that, for a scenario involving a single robot and multiple tasks to be solved sequentially, using the proposed method results in a significant decrease of the computational cost while reaching the same controller performance both in simulation and reality.},
  archive   = {C_IROS},
  author    = {Cyrill Baumann and Hugo Birch and Alcherio Martinoli},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981817},
  pages     = {9318-9325},
  title     = {Leveraging multi-level modelling to automatically design behavioral arbitrators in robotic controllers},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hybrid approach for stabilizing large time delays in
cooperative adaptive cruise control with reduced performance penalties.
<em>IROS</em>, 9310–9317. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cooperative adaptive cruise control (CACC) is a smart transportation solution that can mitigate traffic jams and improve road safety. CACC performance is heavily impacted by communication time delay; moreover, control theory solutions generally compromise control performance by tuning control gains in order to maintain plant stability. We propose a control-machine learning hybrid approach called deep time delay filter (DTDF). DTDF predicts the present (un-delayed) car states given time delayed versions. We successfully train a neural network for the DTDF method and use a physical testbed to show that DTDF can mitigate the effects of constant time delays as large as 5s while maintaining superior control performance compared to that of a baseline control algorithm.},
  archive   = {C_IROS},
  author    = {Kuei-Fang Hsueh and Ayleen Farnood and Mohammad Al Janaideh and Deepa Kundur},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981245},
  pages     = {9310-9317},
  title     = {Hybrid approach for stabilizing large time delays in cooperative adaptive cruise control with reduced performance penalties},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reactive stepping for humanoid robots using reinforcement
learning: Application to standing push recovery on the exoskeleton
atalante. <em>IROS</em>, 9302–9309. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {State-of-the-art reinforcement learning is now able to learn versatile locomotion, balancing and push-recovery capabilities for bipedal robots in simulation. Yet, the reality gap has mostly been overlooked and the simulated results hardly transfer to real hardware. Either it is unsuccessful in practice because the physics is over-simplified and hardware limitations are ignored, or regularity is not guaranteed, and unexpected hazardous motions can occur. This paper presents a reinforcement learning framework capable of learning ro-bust standing push recovery for bipedal robots that smoothly transfer to reality, providing only instantaneous proprioceptive observations. By combining original termination conditions and policy smoothness conditioning, we achieve stable learning, sim-to-real transfer and safety using a policy without memory nor explicit history. Reward engineering is then used to give insights into how to keep balance. We demonstrate its performance in reality on the lower-limb medical exoskeleton Atalante.},
  archive   = {C_IROS},
  author    = {Alexis Duburcq and Fabian Schramm and Guilhem Boéris and Nicolas Bredeche and Yann Chevaleyre},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982234},
  pages     = {9302-9309},
  title     = {Reactive stepping for humanoid robots using reinforcement learning: Application to standing push recovery on the exoskeleton atalante},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Outdoor evaluation of sound source localization for drone
groups using microphone arrays. <em>IROS</em>, 9296–9301. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For robot and drone auditions, microphone arrays have been used for estimating sound source directions and sound source locations. By using sound source localization techniques, for example, drones can detect people calling for help even if the target person is not visible. Most sound source localization methods are based on estimated sound source directions and triangulation. However, when it comes to situations using drones, severe drone noise distorts direction estimation results which could worsen the localization results badly due to the discreteness of direction estimation. In this perspective, the authors have proposed a sound source localization method that can omit outlying triangulation points, which could improve its localization performance. In this paper, an outdoor experiment has been held, and the proposed method is evaluated whether it can localize a sound source even if real drone noise is added to the recordings. Experiment results show that the proposed method can localize with 4.15 m of estimation error for a sound source up to 50 m away, suppress the impact of outliers, and use only plausible triangulation points.},
  archive   = {C_IROS},
  author    = {Taiki Yamada and Katsutoshi Itoyama and Kenji Nishida and Kazuhiro Nakadai},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982039},
  pages     = {9296-9301},
  title     = {Outdoor evaluation of sound source localization for drone groups using microphone arrays},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Controlling the impression of robots via GAN-based gesture
generation. <em>IROS</em>, 9288–9295. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As a type of body language, gestures can largely affect the impressions of human-like robots perceived by users. Recent data-driven approaches to the generation of co-speech gestures have successfully promoted the naturalness of produced gestures. These approaches also possess greater generalizability to work under various contexts than rule-based methods. However, most have no direct control over the human impressions of robots. The main obstacle is that creating a dataset that covers various impression labels is not trivial. In this study, based on previous findings in cognitive science on robot impressions, we present a heuristic method to control them without manual labeling, and demonstrate its effectiveness on a virtual agent and partially on a humanoid robot through subjective experiments with 50 participants.},
  archive   = {C_IROS},
  author    = {Bowen Wu and Jiaqi Shi and Chaoran Liu and Carlos T. Ishi and Hiroshi Ishiguro},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981535},
  pages     = {9288-9295},
  title     = {Controlling the impression of robots via GAN-based gesture generation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Recognizing object surface material from impact sounds for
robot manipulation. <em>IROS</em>, 9280–9287. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigated the use of impact sounds generated during exploratory behaviors in a robotic manipulation setup as cues for predicting object surface material and for recognizing individual objects. We collected and make available the YCB-impact sounds dataset which includes over 3,000 impact sounds for the YCB set of everyday objects lying on a table. Impact sounds were generated in three modes: (i) human holding a gripper and hitting, scratching, or dropping the object; (ii) gripper attached to a teleoperated robot hitting the object from the top; (iii) autonomously operated robot hitting the objects from the side with two different speeds. A convolutional neural network is trained from scratch to recognize the object material (steel, aluminium, hard plastic, soft plastic, other plastic, ceramic, wood, paper/cardboard, foam, glass, rubber) from a single impact sound. On the manually collected dataset with more variability in the speed of the action, nearly 60\% accuracy for the test set (not presented objects) was achieved. On a robot setup and a stereotypical poking action from top, accuracy of 85\% was achieved. This performance drops to 79\% if multiple exploratory actions are combined. Individual objects from the set of 75 objects can be recognized with a 79\% accuracy. This work demonstrates promising results regarding the possibility of using impact sound for recognition in tasks like single-stream recycling where objects have to be sorted based on their material composition.},
  archive   = {C_IROS},
  author    = {Mariella Dimiccoli and Shubhan Patni and Matej Hoffmann and Francesc Moreno-Noguer},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981578},
  pages     = {9280-9287},
  title     = {Recognizing object surface material from impact sounds for robot manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Object surface recognition using microphone array by
acoustic standing wave. <em>IROS</em>, 9274–9279. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a microphone array with a speaker to recognize the shape of the surface of the target object by using the standing wave between the transmitted and the reflected acoustic signals. Because the profile of the distance spectrum encodes both the distance to the target and the distance to the edges of the target&#39;s surface, this paper proposes to fuse distance spectra using a microphone array to estimate the three-dimensional structure of the target surface. The proposed approach was verified through numerical simulations and outdoor field experiments. Results showed the effectiveness of the method as it could extract the shape of the board located 2m in front of the microphone array by using a chirp tone with 20kHz bandwidth.},
  archive   = {C_IROS},
  author    = {Tomoya Manabe and Rikuto Fukunaga and Kei Nakatsuma and Makoto Kumon},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981386},
  pages     = {9274-9279},
  title     = {Object surface recognition using microphone array by acoustic standing wave},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Direction-aware adaptive online neural speech enhancement
with an augmented reality headset in real noisy conversational
environments. <em>IROS</em>, 9266–9273. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper describes the practical response- and performance-aware development of online speech enhancement for an augmented reality (AR) headset that helps a user understand conversations made in real noisy echoic environments (e.g., cocktail party). One may use a state-of-the-art blind source separation method called fast multichannel nonnegative matrix factorization (FastMNMF) that works well in various environments thanks to its unsupervised nature. Its heavy computational cost, however, prevents its application to real-time processing. In contrast, a supervised beamforming method that uses a deep neural network (DNN) for estimating spatial information of speech and noise readily fits real-time processing, but suffers from drastic performance degradation in mismatched conditions. Given such complementary characteristics, we propose a dual-process robust online speech enhancement method based on DNN-based beamforming with FastMNMF-guided adaptation. FastMNMF (back end) is performed in a mini-batch style and the noisy and enhanced speech pairs are used together with the original parallel training data for updating the direction-aware DNN (front end) with backpropagation at a computationally-allowable interval. This method is used with a blind dereverberation method called weighted prediction error (WPE) for transcribing the noisy reverberant speech of a speaker, which can be detected from video or selected by a user&#39;s hand gesture or eye gaze, in a streaming manner and spatially showing the transcriptions with an AR technique. Our experiment showed that the word error rate was improved by more than 10 points with the run-time adaptation using only twelve minutes observation.},
  archive   = {C_IROS},
  author    = {Kouhei Sekiguchi and Aditya Arie Nugraha and Yicheng Du and Yoshiaki Bando and Mathieu Fontaine and Kazuyoshi Yoshii},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981659},
  pages     = {9266-9273},
  title     = {Direction-aware adaptive online neural speech enhancement with an augmented reality headset in real noisy conversational environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Noisy agents: Self-supervised exploration by predicting
auditory events. <em>IROS</em>, 9259–9265. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans integrate multiple sensory modalities (e.g., visual and audio) to build a causal understanding of the physical world. In this work, we propose a novel type of intrinsic motivation for Reinforcement Learning (RL) that encourages the agent to understand the causal effect of its actions through auditory event prediction. First, we allow the agent to collect a small amount of acoustic data and use K-means to discover underlying auditory event clusters. We then train a neural network to predict the auditory events and use the prediction errors as intrinsic rewards to guide RL exploration. We first conduct proof-of-concept experiments using a set of Atari games for an in-depth analysis of our module. We then apply our model to embodied audio-visual exploration using the Habitat simulator and active exploration with a rolling robot using the ThreeDWorld (TDW) simulator. Experimental results demonstrate the advantages of using audio signals over vision-based models as intrinsic rewards to guide RL explorations.},
  archive   = {C_IROS},
  author    = {Chuang Gan and Xiaoyu Chen and Phillip Isola and Antonio Torralba and Joshua B. Tenenbaum},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981614},
  pages     = {9259-9265},
  title     = {Noisy agents: Self-supervised exploration by predicting auditory events},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spotforming by NMF using multiple microphone arrays.
<em>IROS</em>, 9253–9258. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sound source separation is a method to extract a target sound source from a mixture of various sound sources and noises. One of the typical sound source separation methods is beamforming, which can separate sound sources by direction based on the phase difference between channels from the recorded signal of a microphone array, a multi-channel recording system. However, beamforming is a direction-based method and cannot separate multiple sources in the same direction. In this paper, we propose a method for separating sources in the same direction using multiple microphone arrays. The proposed method performs beamforming using multiple microphone arrays and extracts only the target sound source from the separated sound by the Non-negative Matrix Factorization (NMF), thus reducing the influence of other sources in the same direction. In this paper, to investigate the effectiveness of the proposed method, experiments were conducted assuming the presence of another sound source in the same direction from an arbitrary microphone array. The results show that the proposed method outperforms the delay-sum method in a simulation environment. In addition, experiments were conducted in a real environment to verify the effect of reverberation.},
  archive   = {C_IROS},
  author    = {Yasuhiro Kagimoto and Katsutoshi Itoyama and Kenji Nishida and Kazuhiro Nakadai},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981808},
  pages     = {9253-9258},
  title     = {Spotforming by NMF using multiple microphone arrays},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design of a low-cost passive acoustic monitoring system for
animal localisation from calls. <em>IROS</em>, 9247–9252. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The field of bioacoustics is concerned with monitoring wild animals based on their vocalisations. Passive acoustic recorders are now commonly used to collect data of the soundscapes of our wild places. While the data they collect is extremely useful, the majority of the recorders use a single omnidirectional microphone, and thus cannot independently perform localisation of a calling animal. Localisation can be useful to differentiate between multiple calling animals, to improve statistical estimates of abundance, and to locate calling posts, which may be close to nests. In this paper, we consider the design of a low-cost, practical, passive directional acoustic recorder that will facilitate animal localisation, and present and evaluate a prototype system for this purpose.},
  archive   = {C_IROS},
  author    = {Benjamin Yen and Jemima Prins and Gian Schmid and Yusuke Hioka and Susan Ellis and Stephen Marsland},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981127},
  pages     = {9247-9252},
  title     = {Design of a low-cost passive acoustic monitoring system for animal localisation from calls},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Audio-visual depth and material estimation for robot
navigation. <em>IROS</em>, 9239–9246. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reflective and textureless surfaces such as windows, mirrors, and walls can be a challenge for scene reconstruction, due to depth discontinuities and holes. We propose an audio-visual method that uses the reflections of sound to aid in depth estimation and material classification for 3D scene reconstruction in robot navigation and AR/VR applications. The mobile phone prototype emits pulsed audio, while recording video for audio-visual classification for 3D scene reconstruction. Reflected sound and images from the video are input into our audio (EchoCNN-A) and audio-visual (EchoCNN-AV) convolutional neural networks for surface and sound source detection, depth estimation, and material classification. The inferences from these classifications enhance 3D scene reconstructions containing open spaces and reflective surfaces by depth filtering, inpainting, and placement of unmixed sound sources in the scene. Our prototype, demos, and experimental results from real-world with challenging surfaces and sound, also validated with virtual scenes, indicate high success rates on classification of material, depth estimation, and closed/open surfaces, leading to considerable improvement in 3D scene reconstruction for robot navigation.},
  archive   = {C_IROS},
  author    = {Justin Wilson and Nicholas Rewkowski and Ming C. Lin},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981549},
  pages     = {9239-9246},
  title     = {Audio-visual depth and material estimation for robot navigation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Beyond mAP: Towards practical object detection for weed
spraying in precision agriculture. <em>IROS</em>, 9232–9238. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The evolution of smaller and more powerful GPUs over the last 2 decades has vastly increased the opportunity to apply robust deep learning-based machine vision approaches to real-time use cases in practical environments. One exciting application domain for such technologies is precision agriculture, where the ability to integrate on-board machine vision with data-driven actuation means that farmers can make decisions about crop care and harvesting at the level of the individual plant rather than the whole field. This makes sense both economically and environmentally. This paper assesses the feasibility of precision spraying weeds via a comprehensive evaluation of weed detection accuracy and speed using two separate datasets, two types of GPU, and several state-of-the-art object detection algorithms. A simplified model of precision spraying is used to determine whether the weed detection accuracy achieved could result in a sufficiently high weed hit rate combined with a significant reduction in herbicide usage. The paper introduces two metrics to capture these aspects of the real-world deployment of precision weeding and demonstrates their utility through experimental results.},
  archive   = {C_IROS},
  author    = {Adrian Salazar-Gomez and Madeleine Darbyshire and Junfeng Gao and Elizabeth I Sklar and Simon Parsons},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982139},
  pages     = {9232-9238},
  title     = {Beyond mAP: Towards practical object detection for weed spraying in precision agriculture},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predicting fruit-pick success using a grasp classifier
trained on a physical proxy. <em>IROS</em>, 9225–9231. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Apple picking is a challenging manipulation task, but it is difficult to test solutions due to the limited window of time that apples are in season. Previous methods have built simulations of apple trees, but simulations rarely capture soft contact and deformation well, both of which are common in fruit picking. In this paper we present and validate a physical proxy that replicates the mechanics of a real world apple pick. This proxy, in conjunction with a novel hand with multiple sensors, enables large-scale capture of sensor data for data collection and testing. To validate our approach, we train a Long Short-Term Memory network to classify a pick as successful or failed based on sensor feedback from the robot hand. We show that a network trained on the proxy performs as well, or even better, than a network trained solely on real apple trees, with accuracies up to 90\%. We determine which sensors are most important for pick classification and also demonstrate that our proxy preserves the most important sensor feature data for pick classification. For our hand, the most informative sensor was the finger&#39;s servomotor effort.},
  archive   = {C_IROS},
  author    = {Alejandro Velasquez and Nigel Swenson and Miranda Cravetz and Cindy Grimm and Joseph R. Davidson},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981716},
  pages     = {9225-9231},
  title     = {Predicting fruit-pick success using a grasp classifier trained on a physical proxy},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Algorithm design and integration for a robotic apple
harvesting system. <em>IROS</em>, 9217–9224. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Due to labor shortage and rising labor cost for the apple industry, there is an urgent need for the development of robotic systems to efficiently and autonomously harvest apples. In this paper, we present a system overview and algorithm design of our recently developed robotic apple harvester prototype. Our robotic system is enabled by the close integration of several core modules, including visual perception, planning, and control. This paper covers the main methods and advancements in deep learning-based multi-view fruit detection and localization, unified picking and dropping planning, and dexterous manipulation control. Indoor and field experiments were conducted to evaluate the performance of the developed system, which achieved an average picking rate of 3.6 seconds per apple. This is a significant improvement over other reported apple harvesting robots with a picking rate in the range of 7–10 seconds per apple. The current prototype shows promising performance towards further development of efficient and automated apple harvesting technology. Finally, limitations of the current system and future work are discussed.},
  archive   = {C_IROS},
  author    = {Kaixiang Zhang and Kyle Lammers and Pengyu Chu and Nathan Dickinson and Zhaojian Li and Renfu Lu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981417},
  pages     = {9217-9224},
  title     = {Algorithm design and integration for a robotic apple harvesting system},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An integrated actuation-perception framework for robotic
leaf retrieval: Detection, localization, and cutting. <em>IROS</em>,
9210–9216. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Contemporary robots in precision agriculture focus primarily on automated harvesting or remote sensing to monitor crop health. Comparatively less work has been performed with respect to collecting physical leaf samples in the field and retaining them for further analysis. Typically, orchard growers manually collect sample leaves and utilize them for stem water potential measurements to analyze tree health and determine irrigation routines. While this technique benefits orchard management, the process of collecting, assessing, and interpreting measurements requires significant human labor and often leads to infrequent sampling. Automated sampling can provide highly accurate and timely information to growers. The first step in such automated in-situ leaf analysis is identifying and cutting a leaf from a tree. This retrieval process requires new methods for actuation and perception. We present a technique for detecting and localizing candidate leaves using point cloud data from a depth camera. This technique is tested on both indoor and outdoor point clouds from avocado trees. We then use a custom-built leaf-cutting end-effector on a 6-DOF robotic arm to test the proposed detection and localization technique by cutting leaves from an avocado tree. Experimental testing with a real avocado tree demonstrates our proposed approach can enable our mobile manipulator and custom end-effector system to successfully detect, localize, and cut leaves.},
  archive   = {C_IROS},
  author    = {Merrick Campbell and Amel Dechemi and Konstantinos Karydis},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981118},
  pages     = {9210-9216},
  title     = {An integrated actuation-perception framework for robotic leaf retrieval: Detection, localization, and cutting},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). BonnBot-i: A precise weed management and crop monitoring
platform. <em>IROS</em>, 9202–9209. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cultivation and weeding are two of the primary tasks performed by farmers today. A recent challenge for weeding is the desire to reduce herbicide and pesticide treatments while maintaining crop quality and quantity. In this paper we introduce BonnBot-I a precise weed management platform which can also performs field monitoring. Driven by crop monitoring approaches which can accurately locate and classify plants (weed and crop) we further improve their performance by fusing the platform available GNSS and wheel odometry. This improves tracking accuracy of our crop monitoring approach from a normalized average error of 8.3\% to 3.5\%, evaluated on a new publicly available corn dataset. We also present a novel arrangement of weeding tools mounted on linear actuators evaluated in simulated environments. We replicate weed distributions from a real field, using the results from our monitoring approach, and show the validity of our work-space division techniques which require significantly less movement (a 50\% reduction) to achieve similar results. Overall, BonnBot-I is a significant step forward in precise weed management with a novel method of selectively spraying and controlling weeds in an arable field.},
  archive   = {C_IROS},
  author    = {Alireza Ahmadi and Michael Halstead and Chris McCool},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981304},
  pages     = {9202-9209},
  title     = {BonnBot-I: A precise weed management and crop monitoring platform},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). View planning using discrete optimization for 3D
reconstruction of row crops. <em>IROS</em>, 9195–9201. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In view planning, the position and orientation of the cameras have been a major contributing factor to the quality of the resulting 3D model. In applications such as precision agriculture, a dense and accurate reconstruction must be obtained quickly while the data is still actionable. Instead of using an arbitrarily large number of images taken from every possible position and orientation in order to cover the desired area of study, a more optimal approach is required. We present an efficient and realistic pipeline, which aims to optimize the positioning of cameras and hence the quality of the 3D reconstruction of a field of row crops. This is achieved with four steps; an initial flight to obtain a sparse point cloud, the fitting of a simple mesh model, the planning of images via a discrete optimization process, and a second flight to obtain the final reconstruction. We demonstrate the effectiveness of our method by comparing it with baseline methods commonly used for agricultural data collection and processing.},
  archive   = {C_IROS},
  author    = {Athanasios Bacharis and Henry J. Nelson and Nikolaos Papanikolopoulos},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981209},
  pages     = {9195-9201},
  title     = {View planning using discrete optimization for 3D reconstruction of row crops},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Near real-time vineyard downy mildew detection and severity
estimation. <em>IROS</em>, 9187–9194. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The global grape and wine industry has been considerably impacted by diseases such as downy mildew (DM). Agricultural robots have demonstrated great potential to accurately and rapidly map DM infection for precision applications. Although the robots can autonomously acquire high-resolution images in the vineyard, data processing is mostly performed offline because of network infrastructure and onboard computing power constraints, limiting the use of agricultural robots for field operations. To address this issue, we developed a semantic segmentation model based on the modified DeepLabv3 network for near real time DM segmentation in high resolution images. Compared with state-of-the-art real time semantic segmentation models, the developed one achieved the best efficiency-accuracy balance on the DM dataset using embedded computing devices that can be easily integrated with commercial robotic platforms. DM severity estimation pipeline based on the model also showed a comparable measurement accuracy and statistical power in differentiation of fungicide treatments as the one based on offline semantic segmentation models. This enables the use of robotic perception systems for field operations.},
  archive   = {C_IROS},
  author    = {Ertai Liu and Kaitlin Gold and Lance Cadle-Davidson and David Combs and Yu Jiang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981404},
  pages     = {9187-9194},
  title     = {Near real-time vineyard downy mildew detection and severity estimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optical flow-based branch segmentation for complex orchard
environments. <em>IROS</em>, 9180–9186. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Machine vision is a critical subsystem for enabling robots to be able to perform a variety of tasks in orchard environments. However, orchards are highly visually complex environments, and computer vision algorithms operating in them must be able to contend with variable lighting conditions and background noise. Past work on enabling deep learning algorithms to operate in these environments has typically required large amounts of hand-labeled data to train a deep neural network or physically controlling the conditions under which the environment is perceived. In this paper, we train a neural network system in simulation only using simulated RGB data and optical flow. This resulting neural network is able to perform foreground segmentation of branches in a busy orchard environment without additional real-world training or using any special setup or equipment beyond a standard camera. Our results show that our system is highly accurate and, when compared to a network using manually labeled RGBD data, achieves significantly more consistent and robust performance across environments that differ from the training set.},
  archive   = {C_IROS},
  author    = {Alexander You and Cindy Grimm and Joseph R. Davidson},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982017},
  pages     = {9180-9186},
  title     = {Optical flow-based branch segmentation for complex orchard environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Understanding acoustic patterns of human teachers
demonstrating manipulation tasks to robots. <em>IROS</em>, 9172–9179.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans use audio signals in the form of spoken language or verbal reactions effectively when teaching new skills or tasks to other humans. While demonstrations allow humans to teach robots in a natural way, learning from trajectories alone does not leverage other available modalities including audio from human teachers. To effectively utilize audio cues accompanying human demonstrations, first it is important to understand what kind of information is present and conveyed by such cues. This work characterizes audio from human teachers demonstrating multi-step manipulation tasks to a situated Sawyer robot along three dimensions: (1) duration of speech used, (2) expressiveness in speech or prosody, and (3) semantic content of speech. We analyze these features for four different independent variables and find that teachers convey similar semantic content via spoken words for different conditions of (1) demonstration types, (2) audio usage instructions, (3) subtasks, and (4) errors during demonstrations. However, differentiating properties of speech in terms of duration and expressiveness are present for the four independent variables, highlighting that human audio carries rich information, potentially beneficial for technological advancement of robot learning from demonstration methods.},
  archive   = {C_IROS},
  author    = {Akanksha Saran and Kush Desai and Mai Lee Chang and Rudolf Lioutikov and Andrea Thomaz and Scott Niekum},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981053},
  pages     = {9172-9179},
  title     = {Understanding acoustic patterns of human teachers demonstrating manipulation tasks to robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SESNO: Sample efficient social navigation from observation.
<em>IROS</em>, 9164–9171. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present the Sample Efficient Social Navigation from Observation (SESNO) algorithm that efficiently learns socially-compliant navigation policies from observations of human trajectories. SESNO is an inverse reinforcement learning (IRL)-based algorithm that learns from human trajectory observations without knowledge of their actions. We improve the sample-efficiency over previous IRL-based methods by introducing a shared experience replay buffer that allows reuse of past trajectory experiences to estimate the policy and the reward. We evaluate SESNO using publicly available pedestrian motion data sets and compare its performance to related baseline methods in the literature. We show that SESNO yields performance superior to existing baselines while dramatically improving the sample complexity by using as few as a hundredth of the samples required by existing baselines.},
  archive   = {C_IROS},
  author    = {Bobak H. Baghi and Abhisek Konar and Francois Hogan and Michael Jenkin and Gregory Dudek},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981645},
  pages     = {9164-9171},
  title     = {SESNO: Sample efficient social navigation from observation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel perceptive robotic cane with haptic navigation for
enabling vision-independent participation in the social dynamics of seat
choice. <em>IROS</em>, 9156–9163. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Goal-based navigation in public places is critical for independent mobility and for breaking barriers that exist for blind or visually impaired (BVI) people in a sight-centric society. Through this work we present a proof-of-concept system that autonomously leverages goal-based navigation assistance and perception to identify socially preferred seats and safely guide its user towards them in unknown indoor environments. The robotic system includes a camera, an IMU, vibrational motors, and a white cane, powered via a backpack-mounted laptop. The system combines techniques from computer vision, robotics, and motion planning with insights from psychology to perform 1) SLAM and object localization, 2) goal disambiguation and scoring, and 3) path planning and guidance. We introduce a novel 2-motor haptic feedback system on the cane&#39;s grip for navigation assistance. Through a pilot user study we show that the system is successful in classifying and providing haptic navigation guidance to socially preferred seats, while optimizing for users&#39; convenience, privacy, and intimacy in addition to increasing their confidence in independent navigation. The implications are encouraging as this technology, with careful design guided by the BVI community, can be adopted and further developed to be used with medical devices enabling the BVI population to better independently engage in socially dynamic situations like seat choice.},
  archive   = {C_IROS},
  author    = {Shivendra Agrawal and Mary Etta West and Bradley Hayes},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981219},
  pages     = {9156-9163},
  title     = {A novel perceptive robotic cane with haptic navigation for enabling vision-independent participation in the social dynamics of seat choice},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Proactive robotic assistance via theory of mind.
<em>IROS</em>, 9148–9155. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Advanced social cognitive skills enhance the effectiveness of human-robot interactions. Research shows that an important precursor to the development of these abilities in humans is Theory of Mind (ToM) - the ability to attribute mental states to oneself and to others. In this work, we endow robots with ToM abilities and propose a ToM-based approach to proactive robotic assistance by appealing to epistemic planning techniques. Our evaluation shows that robots implementing our approach and demonstrating ToM are measurably more helpful and perceived by humans as more socially intelligent compared to robots with a deficit in ToM.},
  archive   = {C_IROS},
  author    = {Maayan Shvo and Ruthrash Hari and Ziggy O&#39;Reilly and Sophia Abolore and Sze-Yuh Nina Wang and Sheila A. McIlraith},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981627},
  pages     = {9148-9155},
  title     = {Proactive robotic assistance via theory of mind},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reasoning about counterfactuals to improve human inverse
reinforcement learning. <em>IROS</em>, 9140–9147. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To collaborate well with robots, we must be able to understand their decision making. Humans naturally infer other agents&#39; beliefs and desires by reasoning about their observable behavior in a way that resembles inverse reinforcement learning (IRL). Thus, robots can convey their beliefs and desires by providing demonstrations that are informative for a human learner&#39;s IRL. An informative demonstration is one that differs strongly from the learner&#39;s expectations of what the robot will do given their current understanding of the robot&#39;s decision making. However, standard IRL does not model the learner&#39;s existing expectations, and thus cannot do this counterfactual reasoning. We propose to incorporate the learner&#39;s current understanding of the robot&#39;s decision making into our model of human IRL, so that a robot can select demonstrations that maximize the human&#39;s understanding. We also propose a novel measure for estimating the difficulty for a human to predict instances of a robot&#39;s behavior in unseen environments. A user study finds that our test difficulty measure correlates well with human performance and confidence. Interestingly, considering human beliefs and counterfactuals when selecting demonstrations decreases human performance on easy tests, but increases performance on difficult tests, providing insight on how to best utilize such models.},
  archive   = {C_IROS},
  author    = {Michael S. Lee and Henny Admoni and Reid Simmons},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982062},
  pages     = {9140-9147},
  title     = {Reasoning about counterfactuals to improve human inverse reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards inclusive HRI: Using Sim2Real to address
underrepresentation in emotion expression recognition. <em>IROS</em>,
9132–9139. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots and artificial agents that interact with humans should be able to do so without bias and inequity, but facial perception systems have notoriously been found to work more poorly for certain groups of people than others. In our work, we aim to build a system that can perceive humans in a more transparent and inclusive manner. Specifically, we focus on dynamic expressions on the human face, which are difficult to collect for a broad set of people due to privacy concerns and the fact that faces are inherently identifiable. Furthermore, datasets collected from the Internet are not necessarily representative of the general population. We address this problem by offering a Sim2Real approach in which we use a suite of 3D simulated human models that enables us to create an auditable synthetic dataset covering 1) underrepresented facial expressions, outside of the six basic emotions, such as confusion; 2) ethnic or gender minority groups; and 3) a wide range of viewing angles that a robot may encounter a human in the real world. By augmenting a small dynamic emotional expression dataset containing 123 samples with a synthetic dataset containing 4536 samples, we achieved an improvement in accuracy of 15\% on our own dataset and 11\% on an external benchmark dataset, compared to the performance of the same model architecture without synthetic training data. We also show that this additional step improves accuracy specifically for racial minorities when the architecture&#39;s feature extraction weights are trained from scratch.},
  archive   = {C_IROS},
  author    = {Saba Akhyani and Mehryar Abbasi and Mo Chen and Angelica Lim},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982252},
  pages     = {9132-9139},
  title     = {Towards inclusive HRI: Using Sim2Real to address underrepresentation in emotion expression recognition},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DULA and DEBA: Differentiable ergonomic risk models for
postural assessment and optimization in ergonomically intelligent pHRI.
<em>IROS</em>, 9124–9131. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ergonomics and human comfort are essential concerns in physical human-robot interaction applications. Defining an accurate and easy-to-use ergonomic assessment model stands as an important step in providing feedback for postural correction to improve operator health and comfort. Common practical methods in the area suffer from inaccurate ergonomics models in performing postural optimization. In order to retain assessment quality, while improving computational considerations, we propose a novel framework for postural assessment and optimization for ergonomically intelligent physical human-robot interaction. We introduce DULA and DEBA, differentiable and continuous ergonomics models learned to replicate the popular and scientifically validated RULA and REBA assessments with more than 99\% accuracy. We show that DULA and DEBA provide assessment comparable to RULA and REBA while providing computational benefits when being used in postural optimization. We evaluate our framework through human and simulation experiments. We highlight DULA and DEBA&#39;s strength in a demonstration of postural optimization for a simulated pHRI task.},
  archive   = {C_IROS},
  author    = {Amir Yazdani and Roya Sabbagh Novin and Andrew Merryweather and Tucker Hermans},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981528},
  pages     = {9124-9131},
  title     = {DULA and DEBA: Differentiable ergonomic risk models for postural assessment and optimization in ergonomically intelligent pHRI},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Coordination with humans via strategy matching.
<em>IROS</em>, 9116–9123. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human and robot partners increasingly need to work together to perform tasks as a team. Robots designed for such collaboration must reason about how their task-completion strategies interplay with the behavior and skills of their human team members as they coordinate on achieving joint goals. Our goal in this work is to develop a computational framework for robot adaptation to human partners in human-robot team collaborations. We first present an algorithm for autonomously recognizing available task-completion strategies by observing human-human teams performing a collaborative task. By transforming team actions into low dimensional representations using hidden Markov models, we can identify strategies without prior knowledge. Robot policies are learned on each of the identified strategies to construct a Mixture-of-Experts model that adapts to the task strategies of unseen human partners. We evaluate our model on a collaborative cooking task using an Overcooked simulator. Results of an online user study with 125 participants demonstrate that our framework improves the task performance and collaborative fluency of human-agent teams, as compared to state of the art reinforcement learning methods.},
  archive   = {C_IROS},
  author    = {Michelle Zhao and Reid Simmons and Henny Admoni},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982277},
  pages     = {9116-9123},
  title     = {Coordination with humans via strategy matching},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BIMRL: Brain inspired meta reinforcement learning.
<em>IROS</em>, 9048–9053. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sample efficiency has been a key issue in reinforcement learning (RL). An efficient agent must be able to leverage its prior experiences to quickly adapt to similar, but new tasks and situations. Meta-RL is one attempt at formalizing and ad-dressing this issue. Inspired by recent progress in meta-RL, we introduce BIMRL, a novel multi-layer architecture along with a novel brain-inspired memory module that will help agents quickly adapt to new tasks within a few episodes. We also utilize this memory module to design a novel intrinsic reward that will guide the agent&#39;s exploration. Our architecture is inspired by findings in cognitive neuroscience and is compatible with the knowledge on connectivity and functionality of different regions in the brain. We empirically validate the effectiveness of our proposed method by competing with or surpassing the performance of some strong baselines on multiple MiniGrid environments.},
  archive   = {C_IROS},
  author    = {Seyed Roozbeh Razavi Rohani and Saeed Hedayatian and Mahdieh Soleymani Baghshah},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981250},
  pages     = {9048-9053},
  title     = {BIMRL: Brain inspired meta reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Backward imitation and forward reinforcement learning via
bi-directional model rollouts. <em>IROS</em>, 9040–9047. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traditional model-based reinforcement learning (RL) methods generate forward rollout traces using the learnt dynamics model to reduce interactions with the real environment. The recent model-based RL method considers the way to learn a backward model that specifies the conditional probability of the previous state given the previous action and the current state to additionally generate backward rollout trajectories. However, in this type of model-based method, the samples derived from backward rollouts and those from forward rollouts are simply aggregated together to optimize the policy via the model-free RL algorithm, which may decrease both the sample efficiency and the convergence rate. This is because such an approach ignores the fact that backward rollout traces are often generated starting from some high-value states and are certainly more instructive for the agent to improve the behavior. In this paper, we propose the backward imitation and forward reinforcement learning (BIFRL) framework where the agent treats backward rollout traces as expert demonstrations for the imitation of excellent behaviors, and then collects forward rollout transitions for policy reinforcement. Consequently, BIFRL empowers the agent to both reach to and explore from high-value states in a more efficient manner, and further reduces the real interactions, making it potentially more suitable for real-robot learning. Moreover, a value-regularized generative adversarial network is introduced to augment the valuable states which are infrequently received by the agent. Theoretically, we provide the condition where BIFRL is superior to the baseline methods. Experimentally, we demonstrate that BIFRL acquires the better sample efficiency and produces the competitive asymptotic performance on various MuJoCo locomotion tasks compared against state-of-the-art model-based methods.},
  archive   = {C_IROS},
  author    = {Yuxin Pan and Fangzhen Lin},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981869},
  pages     = {9040-9047},
  title     = {Backward imitation and forward reinforcement learning via bi-directional model rollouts},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-objective policy gradients with topological
constraints. <em>IROS</em>, 9034–9039. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-objective optimization models that encode ordered sequential constraints provide a solution to model various challenging problems including encoding preferences, modeling a curriculum, and enforcing measures of safety. A recently developed theory of topological Markov decision processes (TMDPs) captures this range of problems for the case of discrete states and actions. In this work, we extend TMDPs towards continuous spaces and unknown transition dynamics by formulating, proving, and implementing the policy gradient theorem for TMDPs. This theoretical result enables the creation of TMDP learning algorithms that use function approximators, and can generalize existing deep reinforcement learning (DRL) approaches. Specifically, we present a new algorithm for a policy gradient in TMDPs by a simple extension of the proximal policy optimization (PPO) algorithm. We demonstrate this on a real-world multiple-objective navigation problem with an arbitrary ordering of objectives both in simulation and on a real robot.},
  archive   = {C_IROS},
  author    = {Kyle Hollins Wray and Stas Tiomkin and Mykel J. Kochenderfer and Pieter Abbeel},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982278},
  pages     = {9034-9039},
  title     = {Multi-objective policy gradients with topological constraints},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Safety correction from baseline: Towards the risk-aware
policy in robotics via dual-agent reinforcement learning. <em>IROS</em>,
9027–9033. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning a risk-aware policy is essential but rather challenging in unstructured robotic tasks. Safe reinforcement learning methods open up new possibilities to tackle this problem. However, the conservative policy updates make it intractable to achieve sufficient exploration and desirable performance in complex, sample-expensive environments. In this paper, we propose a dual-agent safe reinforcement learning strategy consisting of a baseline and a safe agent. Such a decoupled framework enables high flexibility, data efficiency and risk-awareness for RL-based control. Concretely, the baseline agent is responsible for maximizing rewards under standard RL settings. Thus, it is compatible with off-the-shelf training techniques of unconstrained optimization, exploration and exploitation. On the other hand, the safe agent mimics the baseline agent for policy improvement and learns to fulfill safety constraints via off-policy RL tuning. In contrast to training from scratch, safe policy correction requires significantly fewer interactions to obtain a near-optimal policy. The dual policies can be optimized synchronously via a shared replay buffer, or leveraging the pre-trained model or the non-learning-based controller as a fixed baseline agent. Experimental results show that our approach can learn feasible skills without prior knowledge as well as deriving risk-averse counterparts from pre-trained unsafe policies. The proposed method outperforms the state-of-the-art safe RL algorithms on difficult robot locomotion and manipulation tasks with respect to both safety constraint satisfaction and sample efficiency.},
  archive   = {C_IROS},
  author    = {Linrui Zhang and Zichen Yan and Li Shen and Shoujie Li and Xueqian Wang and Dacheng Tao},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981831},
  pages     = {9027-9033},
  title     = {Safety correction from baseline: Towards the risk-aware policy in robotics via dual-agent reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scalable model-based policy optimization for decentralized
networked systems. <em>IROS</em>, 9019–9026. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning algorithms require a large amount of samples; this often limits their real-world applications on even simple tasks. Such a challenge is more outstanding in multi-agent tasks, as each step of operation is more costly, requiring communications or shifting or resources. This work aims to improve data efficiency of multi-agent control by model-based learning. We consider networked systems where agents are cooperative and communicate only locally with their neighbors, and propose the decentralized model-based policy optimization framework (DMPO). In our method, each agent learns a dynamic model to predict future states and broadcast their predictions by communication, and then the policies are trained under the model rollouts. To alleviate the bias of model-generated data, we restrain the model usage for generating myopic rollouts, thus reducing the compounding error of model generation. To pertain the independence of policy update, we introduce extended value function and theoretically prove that the resulting policy gradient is a close approximation to true policy gradients. We evaluate our algorithm on several benchmarks for intelligent transportation systems, which are connected autonomous vehicle control tasks (Flow and CACC) and adaptive traffic signal control (ATSC). Empirical results show that our method achieves superior data efficiency and matches the performance of model-free methods using true models. The source code of our algorithm and baselines can be found at https://github.com/PKU-MARL/Model-Based-MARL.},
  archive   = {C_IROS},
  author    = {Yali Du and Chengdong Ma and Yuchen Liu and Runji Lin and Hao Dong and Jun Wang and Yaodong Yang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982253},
  pages     = {9019-9026},
  title     = {Scalable model-based policy optimization for decentralized networked systems},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive environment modeling based reinforcement learning
for collision avoidance in complex scenes. <em>IROS</em>, 9011–9018. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The major challenges of collision avoidance for robot navigation in crowded scenes lie in accurate environment modeling, fast perceptions, and trustworthy motion planning policies. This paper presents a novel adaptive environment model based collision avoidance reinforcement learning (i.e., AEMCARL) framework for an unmanned robot to achieve collision-free motions in challenging navigation scenarios. The novelty of this work is threefold: (1) developing a hierarchical network of gated-recurrent-unit (GRU) for environment modeling; (2) developing an adaptive perception mechanism with an attention module; (3) developing an adaptive reward function for the reinforcement learning (RL) framework to jointly train the environment model, perception function and motion planning policy. The proposed method is tested with the Gym-Gazebo simulator and a group of robots (Husky and Turtlebot) under various crowded scenes. Both simulation and experimental results have demonstrated the superior performance of the proposed method over baseline methods.},
  archive   = {C_IROS},
  author    = {Shuaijun Wang and Rui Gao and Ruihua Han and Shengduo Chen and Chengyang Li and Qi Hao},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982107},
  pages     = {9011-9018},
  title     = {Adaptive environment modeling based reinforcement learning for collision avoidance in complex scenes},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DARL1N: Distributed multi-agent reinforcement learning with
one-hop neighbors. <em>IROS</em>, 9003–9010. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent reinforcement learning (MARL) meth-ods face a curse of dimensionality in the policy and value function representations as the number of agents increases. The development of distributed or parallel training techniques is also hindered by the global coupling among the agent dynamics, requiring simultaneous state transitions. This paper introduces Distributed multi-Agent Reinforcement Learning with One-hop Neighbors (DARLIN). DARLIN is an off-policy actor-critic MARL method that breaks the curse of dimensionality and achieves distributed training by restricting the agent interactions to one-hop neighborhoods. Each agent optimizes its value and policy functions over a one-hop neighborhood, reducing the representation complexity, yet maintaining expressiveness by training with varying numbers and states of neighbors. This structure enables the key contribution of DARLIN: a distributed training procedure in which each compute node simulates the state transitions of only a small subset of the agents, greatly accelerating the training of large-scale MARL policies. Comparisons with state-of-the-art MARL methods show that DARLIN significantly reduces training time without sacrificing policy quality as the number of agents increases.},
  archive   = {C_IROS},
  author    = {Baoqian Wang and Junfei Xie and Nikolay Atanasov},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981441},
  pages     = {9003-9010},
  title     = {DARL1N: Distributed multi-agent reinforcement learning with one-hop neighbors},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Active tactile exploration using shape-dependent
reinforcement learning. <em>IROS</em>, 8995–9002. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tactile signals provide rich information about objects via touch and are essential for a robot to perform dex-terous manipulation. Exploring actively via tactile perception collects important information about the workspace. However, designing an effective tactile exploration policy is challenging in unstructured environments. Typically, the geometric information is incomplete, and need to be completed by actively and repeatedly interacting with the environment. In this paper, we address the tactile exploration problem by proposing a shape-information-dependent exploration strategy, which consists of two components: (1) a Shape-Belief Encoder that encodes the explored area by learning effective 3-D reconstruction and predicts the complete object shape; (2) a shape-dependent exploration policy which incorporates the encoding in (1) to plan an exploration trajectory. The policy actively acquires new information about object surface by executing exploration actions. The Shape-Belief Encoder leverages the newly collected contact points to update the surface model and guides future exploration. We validate the proposed algorithm on simulated and real robots.},
  archive   = {C_IROS},
  author    = {Shuo Jiang and Lawson L.S. Wong},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982266},
  pages     = {8995-9002},
  title     = {Active tactile exploration using shape-dependent reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved robustness and safety for pre-adaptation of meta
reinforcement learning with prior regularization. <em>IROS</em>,
8987–8994. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Meta Reinforcement Learning (Meta-RL) has seen substantial advancements recently. In particular, off-policy methods were developed to improve the data efficiency of Meta-RL techniques. Probabilistic embeddings for actor-critic $\boldsymbol{RL}$ (PEARL) is a leading approach for multi-MDP adaptation problems. A major drawback of many existing Meta-RL methods, including PEARL, is that they do not explicitly consider the safety of the prior policy when it is exposed to a new task for the first time. Safety is essential for many real world applications, including field robots and Autonomous Vehicles (AVs), In this paper, we develop the PEARL PLUS (PEARL + ) algorithm, which optimizes the policy for both prior (pre-adaptation) safety and posterior (after-adaptation) performance. Building on top of PEARL, our proposed PEARL + algorithm introduces a prior regularization term in the reward function and a new Q-network for recovering the state-action value under prior context assumptions, to improve the robustness to task distribution shift and safety of the trained network exposed to a new task for the first time. The performance of PEARL + is validated by solving three safety-critical problems related to robots and AVs, including two MuJoCo benchmark problems. From the simulation experiments, we show that safety of the prior policy is significantly improved and more robust to task distribution shift compared to PEARL.},
  archive   = {C_IROS},
  author    = {Lu Wen and Songan Zhang and H. Eric Tseng and Baljeet Singh and Dimitar Filev and Huei Peng},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981621},
  pages     = {8987-8994},
  title     = {Improved robustness and safety for pre-adaptation of meta reinforcement learning with prior regularization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RECCraft system: Towards reliable and efficient collective
robotic construction. <em>IROS</em>, 8979–8986. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This research presents a novel Collective Robotic Construction (CRC) system named RECCraft. The RECCraft hardware system is composed of the mobile manipulation vehicles, the cubic blocks, and the folding ramp blocks. Solid connection and easy removal of the blocks are achieved by an electropermanent magnet and silicon steel sheets. With one degree of freedom (DOF) lifting manipulator, the robot can carry a block 3.7 times its volume. An active folding ramp block can provide a robust passage to the upper level for the robot. Our study focuses on systemic improvement of the construction speed and reliability of the robotic construction system. Visual perception system realized by Apritag is adopted, featured by convenient deployment and high precision, to provide a reliable guarantee for robotic construction. RL-based planner provides end-to-end solution for planning tasks of building multi-layer constructions, which is validated by simulation platform and real prototype. Compared with construction speed of existing robotic construction systems, our proposed RECCraft system achieves state-of-the-art level. The robot builds a 2-layer construction by RL-based planner in 4 minutes and 16 seconds, which achieves construction volumetric throughput of 6.7×10 5 mm 3 /s.},
  archive   = {C_IROS},
  author    = {Qiwei Xu and Yizheng Zhang and Shenghao Zhang and Rui Zhao and Zhuoxing Wu and Dongsheng Zhang and Cheng Zhou and Xiong Li and Jiahong Chen and Zengjun Zhao and Luyang Tang and Zhengyou Zhang and Lei Han},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982068},
  pages     = {8979-8986},
  title     = {RECCraft system: Towards reliable and efficient collective robotic construction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hybrid discrete-continuous path planning for lattice
traversal. <em>IROS</em>, 8971–8978. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Lattice structures allow robotic systems to operate in complex and hazardous environments, e.g. construction, mining and nuclear plants, reliably and effectively. However, current navigation systems for these structures are neither realistic, as they assume simplistic motion primitives and obstacle-free workspaces, nor efficient as they rely solely on global discrete search in an attempt to leverage the modularity of lattices. This paper tackles this gap and studies how robots can navigate lattice structures efficiently. We present a realistic application environment where robots have to avoid obstacles and the structure itself to reach target locations. Our solution couples discrete optimal search, using a domain-dependent heuristic, and sampling-based motion planning to find feasible trajectories in the discrete search space and in the continuous joint space at the same time. We provide two search graph formulations and a path planning approach. Simulation experiments, based on structures and robots created for the Innovate UK Connect-R project, examine scalability to large grid spaces while maintaining performances close to optimal.},
  archive   = {C_IROS},
  author    = {Santiago Franco and Julius Sustarevas and Sara Bernardini},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981801},
  pages     = {8971-8978},
  title     = {Hybrid discrete-continuous path planning for lattice traversal},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reconstructing a spatial field with an autonomous robot
under a budget constraint. <em>IROS</em>, 8963–8970. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we consider the information path-planning problem for a single robot in a stochastic environment with static obstacles subject to a preassigned constraint on the distance it can travel. Given a set of candidate sampling locations, the objective is to determine a path for the robot that allows to visit as many sampling locations as possible to accurately reconstruct an unknown underlying scalar field while not exceeding the assigned travel budget. Starting from the assumption that the phenomenon being measured can be modeled by a Gaussian Process, our algorithm balances exploration and exploitation to determine a sequence of locations ensuring that a preassigned final site is reached before the budget is consumed. Using mutual information as a reward criterion, as well as a generative model to predict consumed energy, the algorithm iteratively determines where to sample next, and when to end the mission. Our findings are validated in simulation in various scenarios and lead to a better reconstruction with less failures when compared with other methods.},
  archive   = {C_IROS},
  author    = {Azin Shamshirgaran and Stefano Carpin},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981128},
  pages     = {8963-8970},
  title     = {Reconstructing a spatial field with an autonomous robot under a budget constraint},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Compact strawberry harvesting tube employing laser cutter.
<em>IROS</em>, 8956–8962. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, a novel prototype for hanging produce harvesting is presented, that is productive, versatile, and robust. In our methodology, the robot-mounted tube approaches, and eventually surrounds the produce of interest at the entry side, that can be as small as the produce diameter, plus a small margin. The stem is then cut by a laser beam, with the optics set up for a distant focal point. Such arrangement allows for minimal hardware at the produce-entry side and in turn, the interaction, and possible disturbance to the local environment. This is essential for fruit reachability and avoiding it dislocation. Experiments has been conducted to drive the laser power to time of cut relation, as well as successful demonstration to sample harvest strawberry.},
  archive   = {C_IROS},
  author    = {Mohamed Sorour and PåL Johan From and Khaled Elgeneidy and Stratis Kanarachos and Mohamed Sallam},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981720},
  pages     = {8956-8962},
  title     = {Compact strawberry harvesting tube employing laser cutter},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EPAR: An efficient and privacy-aware augmented reality
framework for indoor location-based services. <em>IROS</em>, 8948–8955.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Augmented reality (AR) defines a new information-delivery paradigm by overlaying computer-generated information on the perception of the real world. AR-integrated robot has become an appealing concept in terms of enhanced human-robot interaction. Despite intensive research on AR, existing indoor location-based AR systems are vulnerable to attacks and can hardly meet the security and privacy requirements in practice. The problem of designing a secure AR framework to ensure the efficiency and privacy of location-based AR has not been sufficiently studied. In this paper, we holistically study this problem and propose EPAR, an efficient and privacy-aware AR framework for indoor location-based services. EPAR distinguishes itself from the existing work by being the first to address the issues of AR delivery in terms of system scalability, accuracy, privacy, and efficiency. First, an effective indoor location cloaking scheme is presented to safeguard user&#39;s privacy while improving system scalability and accuracy. Then, a novel privacy-aware localization scheme is proposed to hierarchically localize the user with privacy concerns. Finally, for the AR content delivery, a new authenticated data structure is tailored to save the data transmission cost and improve system efficiency. We implement EPAR and conduct extensive experiments in real-world scenarios. Evaluation results demonstrate the effectiveness of our EPAR system.},
  archive   = {C_IROS},
  author    = {Zhe Peng and Songlin Hou and Yixuan Yuan},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981149},
  pages     = {8948-8955},
  title     = {EPAR: An efficient and privacy-aware augmented reality framework for indoor location-based services},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Development and control of robot hand with finger camera for
garment handling tasks. <em>IROS</em>, 8940–8947. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic automation is steadily growing in different industries around the world. However, in some industries, such as garment manufacturing, most tasks are still predominantly manual, due to the flexible nature of clothes. Garment and clothes are easily deformed when some force is applied, so it is difficult for robots to handle them while predicting their deformation. Our general research goal is to realize flexible cloth handling using robots to automate different tasks in the garment manufacturing industry. We draw inspiration from the actions that humans perform when manipulating clothes and emulate them using a robotic system. In this paper, we developed a robot hand with a camera at the finger, to obtain local information of the contact between the garment and the robot hand, in order to achieve garment handling tasks. Specifically, we focus on the pinch and slide motion that humans perform when straightening a piece of cloth. We selected a specific task to be automated and proposed three manipulation strategies to approach the garment using visual information from the finger camera that enabled the system to perform the task consistently. We carried out two validation experiments to demonstrate the effectiveness of the proposed methods, and an application experiment where we evaluate their applicability to a specific task.},
  archive   = {C_IROS},
  author    = {Hirokazu Kondo and Jose Victorio Salazar Luces and Yasuhisa Hirata},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982134},
  pages     = {8940-8947},
  title     = {Development and control of robot hand with finger camera for garment handling tasks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Metal wire manipulation planning for 3D curving - a low
payload robot that uses a bending machine to bend high-stiffness wire.
<em>IROS</em>, 8927–8932. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a combined task and motion planner for a robot arm to carry out 3D metal wire curving tasks by collaborating with a bending machine. We assume a collaborative robot that is safe to work in a human environment but has a weak payload to bend objects with large stiffness, and developed a combined planner for the robot to use a bending machine. Our method converts a 3D curve to a bending set and generates the feasible bending sequence, machine usage, robotic grasp poses, and pick-and-place arm motion considering the combined task and motion level constraints. Compared with previous deformable linear object shaping work that relied on forces provided by robotic arms, the proposed method is suitable for the material with high stiffness. We evaluate the system using different tasks. The results show that the proposed system is flexible and robust to generate robotic motion to corporate with the designed bending machine.},
  archive   = {C_IROS},
  author    = {Ruishuang Liu and Weiwei Wan and Emiko Isomura and Kensuke Harada},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981672},
  pages     = {8927-8932},
  title     = {Metal wire manipulation planning for 3D curving - a low payload robot that uses a bending machine to bend high-stiffness wire},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robot companion, an intelligent interactive robot coworker
for the industry 5.0. <em>IROS</em>, 8918–8925. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To overcome the limitations of the so-called Industry 4.0 focusing on mass production and full automation, a novel paradigm was recently introduced, namely Industry 5.0, which aims at an increased collaboration between humans and machines, and particularly robots, instead of replacing the former with the latter. This challenge requires novel interactive intelligent robots able to perform complex tasks easily and efficiently and to collaborate on the fly with humans whenever required, be it for training or working. In this work, the Robot Companion, a novel demonstrator of this paradigm, is introduced. It combines robotics, Artificial Intelligence, software engineering and embedded systems technologies, and targets industrial assembly tasks. First tests show that this robot can efficiently assemble a representative gear system autonomously or in collaboration with human operators.},
  archive   = {C_IROS},
  author    = {F. Gosselin and S. Kchir and G. Acher and F. Keith and O. Lebec and C. Louison and B. Luvison and F. Mayran de Chamisso and B. Meden and M. Morelli and B. Perochon and J. Rabarisoa and C. Vienne and G. Ameyugo},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982004},
  pages     = {8918-8925},
  title     = {Robot companion, an intelligent interactive robot coworker for the industry 5.0},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GoferBot: A visual guided human-robot collaborative assembly
system. <em>IROS</em>, 8910–8917. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The current transformation towards smart manufacturing has led to a growing demand for human-robot collaboration (HRC) in the manufacturing process. Perceiving and understanding the human co-worker&#39;s behaviour introduces challenges for collaborative robots to efficiently and effectively perform tasks in unstructured and dynamic environments. Integrating recent data-driven machine vision capabilities into HRC systems is a logical next step in addressing these challenges. However, in these cases, off-the-shelf components struggle due to generalisation limitations. Real-world evaluation is required in order to fully appreciate the maturity and robustness of these approaches. Furthermore, understanding the pure-vision aspects is a crucial first step before combining multiple modalities in order to understand the limitations. In this paper, we propose GoferBot, a novel vision-based semantic HRC system for a real-world assembly task. It is composed of a visual servoing module that reaches and grasps assembly parts in an unstructured multi-instance and dynamic environment, an action recognition module that performs human action prediction for implicit communication, and a visual handover module that uses the perceptual understanding of human behaviour to produce an intuitive and efficient collaborative assembly experience. GoferBot is a novel assembly system that seamlessly integrates all sub-modules by utilising implicit semantic information purely from visual perception.},
  archive   = {C_IROS},
  author    = {Zheyu Zhuang and Yizhak Ben-Shabat and Jiahao Zhang and Stephen Gould and Robert Mahony},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981122},
  pages     = {8910-8917},
  title     = {GoferBot: A visual guided human-robot collaborative assembly system},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online 3D bin packing reinforcement learning solution with
buffer. <em>IROS</em>, 8902–8909. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The 3D Bin Packing Problem (3D-BPP) is one of the most demanded yet challenging problems in industry, where an agent must pack variable size items delivered in sequence into a finite bin with the aim to maximize the space utilization. It represents a strongly NP-Hard optimization problem such that no solution has been offered to date with high performance in space utilization. In this paper, we present a new reinforcement learning (RL) framework for a 3D-BPP solution for improving performance. First, a buffer is introduced to allow multi-item action selection. By increasing the degree of freedom in action selection, a more complex policy that results in better packing performance can be derived. Second, we propose an agnostic data augmentation strategy that exploits both bin item symmetries for improving sample efficiency. Third, we implement a model-based RL method adapted from the popular algorithm AlphaGo, which has shown superhuman performance in zero-sum games. Our adaptation is capable of working in single-player and score based environments. In spite of the fact that AlphaGo versions are known to be computationally heavy, we manage to train the proposed framework with a single thread and GPU, while obtaining a solution that outperforms the state-of-the-art results in space utilization.},
  archive   = {C_IROS},
  author    = {Aaron Valero Puche and Sukhan Lee},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982095},
  pages     = {8902-8909},
  title     = {Online 3D bin packing reinforcement learning solution with buffer},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A composable framework for policy design, learning, and
transfer toward safe and efficient industrial insertion. <em>IROS</em>,
8894–8901. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Delicate industrial insertion tasks (e.g., PC board assembly) remain challenging for industrial robots. The chal-lenges include low error tolerance, delicacy of the components, and large task variations with respect to the components to be inserted. To deliver a feasible robotic solution for these insertion tasks, we also need to account for hardware limits of existing robotic systems and minimize the integration effort. This paper proposes a composable framework for efficient integration of a safe insertion policy on existing robotic platforms to accomplish these insertion tasks. The policy has an interpretable modularized design and can be learned efficiently on hardware and transferred to new tasks easily. In particular, the policy includes a safe insertion agent as a baseline policy for insertion, an optimal configurable Cartesian tracker as an interface to robot hardware, a probabilistic inference module to handle component variety and insertion errors, and a safe learning module to optimize the parameters in the aforementioned modules to achieve the best performance on designated hard-ware. The experiment results on a URIO robot show that the proposed framework achieves safety (for the delicacy of components), accuracy (for low tolerance), robustness (against perception error and component defection), adaptability and transferability (for task variations), as well as task efficiency during execution plus data and time efficiency during learning.},
  archive   = {C_IROS},
  author    = {Rui Chen and Chenxi Wang and Tianhao Wei and Changliu Liu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981467},
  pages     = {8894-8901},
  title     = {A composable framework for policy design, learning, and transfer toward safe and efficient industrial insertion},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Heuristic-free optimization of force-controlled robot search
strategies in stochastic environments. <em>IROS</em>, 8887–8893. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In both industrial and service domains, a central benefit of the use of robots is their ability to quickly and reliably execute repetitive tasks. However, even relatively simple peg-in-hole tasks are typically subject to stochastic variations, requiring search motions to find relevant features such as holes. While search improves robustness, it comes at the cost of increased runtime: More exhaustive search will maximize the probability of successfully executing a given task, but will significantly delay any downstream tasks. This trade-off is typically resolved by human experts according to simple heuristics, which are rarely optimal. This paper introduces an automatic, data-driven and heuristic-free approach to optimize robot search strategies. By training a neural model of the search strategy on a large set of simulated stochastic environments, conditioning it on few real-world examples and inverting the model, we can infer search strategies which adapt to the time-variant characteristics of the underlying probability distributions, while requiring very few real-world measurements. We evaluate our approach on two different industrial robots in the context of spiral and probe search for THT electronics assembly.**See github.com/benjaminalt/dpse for code and data.},
  archive   = {C_IROS},
  author    = {Benjamin Alt and Darko Katic and Rainer Jäkel and Michael Beetz},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982093},
  pages     = {8887-8893},
  title     = {Heuristic-free optimization of force-controlled robot search strategies in stochastic environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Lifted contact dynamics for efficient optimal control of
rigid body systems with contacts. <em>IROS</em>, 8879–8886. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel and efficient lifting approach for the optimal control of rigid-body systems with contacts to improve the convergence properties of Newton-type methods. To relax the high nonlinearity, we consider the state, acceleration, contact forces, and control input torques, as optimization variables and the inverse dynamics and acceleration constraints on the contact frames as equality constraints. We eliminate the update of the acceleration, contact forces, and their dual variables from the linear equation to be solved in each Newton-type iteration in an efficient manner. As a result, the computational cost per Newton-type iteration is almost identical to that of the conventional non-lifted Newton-type iteration that embeds contact dynamics in the state equation. We conducted numerical experiments on the whole-body optimal control of various quadrupedal gaits subject to the friction cone constraints considered in interior-point methods and demonstrated that the proposed method can significantly increase the convergence speed to more than twice that of the conventional non-lifted approach.},
  archive   = {C_IROS},
  author    = {Sotaro Katayama and Toshiyuki Ohtsuka},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982048},
  pages     = {8879-8886},
  title     = {Lifted contact dynamics for efficient optimal control of rigid body systems with contacts},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal gait families using lagrange multiplier method.
<em>IROS</em>, 8873–8878. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Robotic locomotion community is interested in optimal gaits for control. Based on the optimization criterion, however, there could be a number of possible optimal gaits. For example, the optimal gait for maximizing displacement with respect to cost is quite different from the maximum displacement optimal gait. Beyond these two general optimal gaits, we believe that the optimal gait should deal with various situations for high-resolution of motion planning, e.g., steering the robot or moving in “baby steps.” As the step size or steering ratio increases or decreases, the optimal gaits will slightly vary by the geometric relationship and they will form the families of gaits. In this paper, we explored the geometrical framework across these optimal gaits having different step sizes in the family via the Lagrange multiplier method. Based on the structure, we suggest an optimal locus generator that solves all related optimal gaits in the family instead of optimizing each gait respectively. By applying the optimal locus generator to two simplified swimmers in drag-dominated environments, we verify the behavior of the optimal locus generator.},
  archive   = {C_IROS},
  author    = {Jinwoo Choi and Capprin Bass and Ross L. Hatton},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981871},
  pages     = {8873-8878},
  title     = {Optimal gait families using lagrange multiplier method},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generating families of optimally actuated gaits from a
legged system’s energetically conservative dynamics. <em>IROS</em>,
8866–8872. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a homotopic approach to generating energetically optimal gaits for legged robots that maps passive (i.e., unactuated) gaits of an energetically conservative model of the robot to a model with user-defined target dynamics with dissipation and actuation (i.e., the more “realistic” legged model). Our core contribution is advancing the state-of-the-art towards a turn-key approach where the seed values are known by design and do not rely on domain-specific knowledge to generate or randomly guess across a range of energetic cost functions and desired gait properties (e.g., walking speed, hopping height, etc.), which can limit the usefulness of the typical optimization-based approach. We demonstrate this methodology on a parallel elastic actuated planar monoped with five degrees of freedom. Our work also demonstrates an explicit connection between passive gaits and optimally actuated motions, which has long been an area of interest in the fields of robotics and biome-chanics.},
  archive   = {C_IROS},
  author    = {Maximilian Raff and Nelson Rosa and C. David Remy},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981693},
  pages     = {8866-8872},
  title     = {Generating families of optimally actuated gaits from a legged system&#39;s energetically conservative dynamics},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Whole-body model predictive control with rigid contacts via
online switching time optimization. <em>IROS</em>, 8858–8865. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study presents a whole-body model predictive control (MPC) of robotic systems with rigid contacts, under a given contact sequence using online switching time optimization (STO). We treat robot dynamics with rigid contacts as a switched system and formulate an optimal control problem of switched systems to implement the MPC. We utilize an efficient solution algorithm for the MPC problem that optimizes the switching times and trajectory simultaneously. The present efficient algorithm, unlike inefficient existing methods, enables online optimization as well as switching times. The proposed MPC with online STO is compared over the conventional MPC with fixed switching times, through numerical simulations of dynamic jumping motions of a quadruped robot. In the simulation comparison, the proposed MPC successfully controls the dynamic jumping motions in twice as many cases as the conventional MPC, which indicates that the proposed method extends the ability of the whole-body MPC. We further conduct hardware experiments on the quadrupedal robot Unitree A1 and prove that the proposed method achieves dynamic motions on the real robot.},
  archive   = {C_IROS},
  author    = {Sotaro Katayama and Toshiyuki Ohtsuka},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981790},
  pages     = {8858-8865},
  title     = {Whole-body model predictive control with rigid contacts via online switching time optimization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An error-state model predictive control on connected matrix
lie groups for legged robot control. <em>IROS</em>, 8850–8857. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper reports on a new error-state Model Predictive Control (MPC) approach to connected matrix Lie groups for robot control. The linearized tracking error dynamics and the linearized equations of motion are derived in the Lie algebra. Moreover, given an initial condition, the linearized tracking error dynamics and equations of motion are globally valid and evolve independently of the system trajectory. By exploiting the symmetry of the problem, the proposed approach shows faster convergence of rotation and position simultaneously than the state-of-the-art geometric variational MPC based on variational-based linearization. Numerical simulation on tracking control of a fully-actuated 3D rigid body dynamics confirms the benefits of the proposed approach compared to the baselines. Furthermore, the proposed MPC is also verified in pose control and locomotion experiments on a quadrupedal robot MIT Mini Cheetah.},
  archive   = {C_IROS},
  author    = {Sangli Teng and Dianhao Chen and William Clark and Maani Ghaffari},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981282},
  pages     = {8850-8857},
  title     = {An error-state model predictive control on connected matrix lie groups for legged robot control},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Large-scale ADMM-based co-design of legged robots.
<em>IROS</em>, 8842–8849. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper considers the problem of designing legged robots for traversing uneven terrain, wherein terrain characteristics represent uncertainty for the design process. When this process encompasses a wider variety of terrains, the likelihood of the designed robot falling in the real world should decrease. However, computational scalability limits the number of terrains that can be taken into account during design. The proposed framework uses the Alternating Direction Method of Multipliers (ADMM) to solve large-scale concurrent design (co-design) problems. The ADMM coordinates the solution of small-size sub-problems and enforces constraints to reach a consensus on the best design. The framework uses stochastic programming (SP) to account for terrain uncertainty and trajectory optimization (TO) to co-optimize a nominal trajectory alongside hardware parameters and a feedback controller. Case studies demonstrate application for a monopod and a quadruped. For the monopod, ADMM facilitated an increase in the number of terrains considered within co-design by 400\% compared to SP alone, which contributed to robustifying the design and decreasing its failure probability to under 1\% in an anticipated operating space. A multi-scenario co-design implementation for the quadruped had previously been intractable due to scalability limitations. The ADMM framework, by contrast, shows tractability running with 30 terrain types, opening the horizon for designing more complex systems.},
  archive   = {C_IROS},
  author    = {Gabriel Bravo-Palacios and Patrick M. Wensing},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981641},
  pages     = {8842-8849},
  title     = {Large-scale ADMM-based co-design of legged robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Balancing control and pose optimization for wheel-legged
robots navigating high obstacles. <em>IROS</em>, 8835–8841. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a novel approach to controlling wheel-legged quadrupedal robots using pose optimization and force-based control via quadratic programming (QP). Our method allows the robot to leverage the whole-body motion and the wheel actuation to roll over high obstacles while keeping wheel traction with the terrain. In detail, we first present linear rigid body dynamics with wheels that can be used for real-time balancing control of wheel-legged robots. We then introduce an effective pose optimization method for wheel-legged robot&#39;s locomotion over steep ramp and stair terrains. The pose optimization solves for optimal poses to enhance stability and enforce collision-free constraints at critical pose locations for rolling over high obstacles. Experimental validation of the real robot demonstrated the capability of rolling up on a 0.36 m obstacle. The robot can also successfully roll up and down multiple stairs without lifting its legs or colliding with the terrain.},
  archive   = {C_IROS},
  author    = {Junheng Li and Junchao Ma and Quan Nguyen},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981432},
  pages     = {8835-8841},
  title     = {Balancing control and pose optimization for wheel-legged robots navigating high obstacles},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Contact-implicit trajectory optimization with hydroelastic
contact and iLQR. <em>IROS</em>, 8829–8834. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Contact-implicit trajectory optimization offers an appealing method of automatically generating complex and contact-rich behaviors for robot manipulation and locomotion. The scalability of such techniques has been limited, however, by the challenge of ensuring both numerical reliability and physical realism. In this paper, we present preliminary results suggesting that the Iterative Linear Quadratic Regulator (iLQR) algorithm together with the recently proposed pressure-field-based hydroelastic contact model enables reliable and physically realistic trajectory optimization through contact. We use this approach to synthesize contact-rich behaviors like quadruped locomotion and whole-arm manipulation. Furthermore, open-loop playback on a Kinova Gen3 robot arm demonstrates the physical accuracy of the whole-arm manipulation trajectories. Code is available at https://bit.ly/ilqr_hc and videos can be found at https://youtu.be/IqxJKbM8_ms.},
  archive   = {C_IROS},
  author    = {Vince Kurtz and Hai Lin},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981686},
  pages     = {8829-8834},
  title     = {Contact-implicit trajectory optimization with hydroelastic contact and iLQR},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The uncertainty aware salted kalman filter: State estimation
for hybrid systems with uncertain guards. <em>IROS</em>, 8821–8828. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a method for updating robotic state belief through contact with uncertain surfaces and apply this update to a Kalman filter for more accurate state estimation. Examining how guard surface uncertainty affects the time spent in each mode, we derive a novel guard saltation matrix- which maps perturbations prior to hybrid events to perturbations after - accounting for additional variation in the resulting state. Additionally, we propose the use of parame-terized reset functions - capturing how unknown parameters change how states are mapped from one mode to the next - the Jacobian of which accounts for additional uncertainty in the resulting state. The accuracy of these mappings is shown by simulating sampled distributions through uncertain transition events and comparing the resulting covariances. Finally, we integrate these additional terms into the “uncertainty aware Salted Kalman Filter”, uaSKF, and show a peak reduction in average estimation error by 24–60\% on a variety of test conditions and systems.},
  archive   = {C_IROS},
  author    = {J. Joe Payne and Nathan J. Kong and Aaron M. Johnson},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981218},
  pages     = {8821-8828},
  title     = {The uncertainty aware salted kalman filter: State estimation for hybrid systems with uncertain guards},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Transferring multi-agent reinforcement learning policies for
autonomous driving using sim-to-real. <em>IROS</em>, 8814–8820. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous Driving requires high levels of coordination and collaboration between agents. Achieving effective coordination in multi-agent systems is a difficult task that remains largely unresolved. Multi-Agent Reinforcement Learning has arisen as a powerful method to accomplish this task because it considers the interaction between agents and also allows for decentralized training—which makes it highly scalable. However, transferring policies from simulation to the real world is a big challenge, even for single-agent applications. Multi-agent systems add additional complexities to the Sim-to-Real gap due to agent collaboration and environment synchronization. In this paper, we propose a method to transfer multi-agent autonomous driving policies to the real world. For this, we create a multi-agent environment that imitates the dynamics of the Duckietown multi-robot testbed, and train multi-agent policies using the MAPPO algorithm with different levels of domain randomization. We then transfer the trained policies to the Duckietown testbed and show that when using our method, domain randomization can reduce the reality gap by 90\%. Moreover, we show that different levels of parameter randomization have a substantial impact on the Sim-to-Real gap. Finally, our approach achieves significantly better results than a rule-based benchmark.},
  archive   = {C_IROS},
  author    = {Eduardo Candela and Leandro Parada and Luis Marques and Tiberiu-Andrei Georgescu and Yiannis Demiris and Panagiotis Angeloudis},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981319},
  pages     = {8814-8820},
  title     = {Transferring multi-agent reinforcement learning policies for autonomous driving using sim-to-real},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FlowBot: Flow-based modeling for robot navigation.
<em>IROS</em>, 8799–8805. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous navigation among people is a com-plex problem that also exhibits considerable variation depending on the type of environment and people involved. Here we consider navigation among crowds that exhibit flow-like behavior like people moving through a train station. We propose a novel pseudo-fluid model of crowd flow for such problems. These have an intuitive physical interpretation and do not require much tuning. We further formalize an observation model to infer flow properties from discrete sensor observations, including support for partial observability, and pair it with a flow-aware planner. We demonstrate the potential of the approach in simulated navigation scenarios. We achieve state of the art results on the CrowdBot navigation benchmark, and also compare favorably against a standard ROS planner on a partially observable environment, demonstrating that the flow-aware planner successfully estimates and plans around counter-flows in the crowd in real time. We conclude that flow-based planning shows great promise for crowded environments that may exhibit such flow-like behavior.},
  archive   = {C_IROS},
  author    = {Daniel Dugas and Kuanqi Cai and Olov Andersson and Nicholas Lawrance and Roland Siegwart and Jen Jen Chung},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981407},
  pages     = {8799-8805},
  title     = {FlowBot: Flow-based modeling for robot navigation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LTR*: Rapid replanning in executing consecutive tasks with
lazy experience graph. <em>IROS</em>, 8784–8790. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In an environment where a manipulator needs to execute multiple consecutive tasks, the act of object manoeuvre will change the underlying configuration space, affecting all subsequent tasks. Previously free configurations might now be occupied by the manoeuvred objects, and previously occupied space might now open up new paths. We propose Lazy Tree-based Replanner (LTR *)-a novel hybrid planner that inherits the rapid planning nature of existing anytime incremental sampling-based planners. At the same time, it allows subsequent tasks to leverage prior experience via a lazy experience graph. Previous experience is summarised in a lazy graph structure, and LTR * is formulated to be robust and beneficial regard-less of the extent of changes in the workspace. Our hybrid approach attains a faster speed in obtaining an initial solution than existing roadmap-based planners and often with a lower cost in trajectory length. Subsequent tasks can utilise the lazy experience graph to speed up finding a solution and take advant-age of the optimised graph to minimise the cost objective. We provide proofs of probabilistic completeness and almost-surely asymptotic optimal guarantees. Experimentally, we show that in repeated pick-and-place tasks, L T R * attains a high gain in performance when planning for subsequent tasks.},
  archive   = {C_IROS},
  author    = {Tin Lai and Fabio Ramos},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982237},
  pages     = {8784-8790},
  title     = {LTR*: Rapid replanning in executing consecutive tasks with lazy experience graph},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lazy lifelong planning for efficient replanning in graphs
with expensive edge evaluation. <em>IROS</em>, 8778–8783. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an incremental search algorithm, called Lifelong-GLS, which combines the vertex efficiency of Lifelong Planning A* (LPA*) and the edge efficiency of Generalized Lazy Search (GLS) for efficient replanning on dynamic graphs where edge evaluation is expensive. We use a lazily evaluated LPA* to repair the cost-to-come inconsistencies of the relevant region of the current search tree based on the previous search results, and then we restrict the expensive edge evaluations only to the current shortest subpath as in the GLS framework. The proposed algorithm is complete and correct in finding the optimal solution in the current graph, if one exists. We also show the efficiency of the proposed algorithm compared to the standard LPA* and the GLS algorithms over consecutive search episodes in a dynamic environment.},
  archive   = {C_IROS},
  author    = {Jaein Lim and Siddhartha Srinivasa and Panagiotis Tsiotras},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981389},
  pages     = {8778-8783},
  title     = {Lazy lifelong planning for efficient replanning in graphs with expensive edge evaluation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online planning for interactive-POMDPs using nested monte
carlo tree search. <em>IROS</em>, 8770–8777. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to make good decisions in partially observed non-cooperative multi-agent scenarios is important for robots to interact effectively in human environments. A robust framework for such decision-making problems is the Interactive Partially Observable Markov Decision Processes (I-POMDPs), which explicitly models the other agents&#39; beliefs up to a finite reasoning level in order to more accurately predict their actions. This paper proposes a new online approximate solver for I-POMDPs, called Interactive Nested Tree Monte-Carlo Planning (I-NTMCP), that combines Monte Carlo Tree Search with the finite nested-reasoning construction of I-POMDPs. Unlike existing full-width I-POMDP planners, I-NTMCP focuses planning on the set of beliefs at each nesting level which are reachable under an optimal policy and uses sampling to construct and update policies at each nesting level, online. This strategy enables I-NTMCP to plan effectively in significantly larger I-POMDP problems and to deeper reasoning levels than has previously been possible. We demonstrate I-NTMCP&#39;s effectiveness on two competitive environments. The results indicate that I-NTMCP can generate substantially better policies up to more than 50× faster than I-POMDP Lite - one of the fastest I-POMDP solvers today. In the pursuit-evasion domain, we show I-NTMCP can plan effectively in a complex problem with over 88K states, which is two orders of magnitude larger than existing I-POMDP planning benchmark problems.},
  archive   = {C_IROS},
  author    = {Jonathon Schwartz and Ruijia Zhou and Hanna Kurniawati},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981713},
  pages     = {8770-8777},
  title     = {Online planning for interactive-POMDPs using nested monte carlo tree search},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive sampling of latent phenomena using heterogeneous
robot teams (ASLaP-HR). <em>IROS</em>, 8762–8769. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present an online adaptive planning strategy for a team of robots with heterogeneous sensors to sample from a latent spatial field using a learned model for decision making. Current robotic sampling methods seek to gather information about an observable spatial field. However, many applications, such as environmental monitoring and precision agriculture, involve phenomena that are not directly observable or are costly to measure, called latent phenomena. In our approach, we seek to reason about the latent phenomenon in real-time by effectively sampling the observable spatial fields using a team of robots with heterogeneous sensors, where each robot has a distinct sensor to measure a different observable field. The information gain is estimated using a learned model that maps from the observable spatial fields to the latent phenomenon. This model captures aleatoric uncertainty in the relationship to allow for information theoretic measures. Additionally, we explicitly consider the correlations among the observable spatial fields, capturing the relationship between sensor types whose observations are not independent. We show it is possible to learn these correlations, and investigate the impact of the learned correlation models on the performance of our sampling approach. Through our qualitative and quantitative results, we illustrate that empirically learned correlations improve the overall sampling efficiency of the team. We simulate our approach using a data set of sensor measurements collected on Lac Hertel, in Quebec, which we make publicly available.},
  archive   = {C_IROS},
  author    = {Matthew Malencia and Sandeep Manjanna and M. Ani Hsieh and George Pappas and Vijay Kumar},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982270},
  pages     = {8762-8769},
  title     = {Adaptive sampling of latent phenomena using heterogeneous robot teams (ASLaP-HR)},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FIG-OP: Exploring large-scale unknown environments on a
fixed time budget. <em>IROS</em>, 8754–8761. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a method for autonomous exploration of large-scale unknown environments under mission time con-straints. We start by proposing the Frontloaded Information Gain Orienteering Problem (FIG-OP) - a generalization of the traditional orienteering problem where the assumption of a reliable environmental model no longer holds. The FIG-OP ad-dresses model uncertainty by frontloading expected information gain through the addition of a greedy incentive, effectively expe-diting the moment in which new area is uncovered. In order to reason across multi-kilometer environments, we solve FIG-OP over an information-efficient world representation, constructed through the aggregation of information from a topological and metric map. Our method was extensively tested and field-hardened across various complex environments, ranging from subway systems to mines. In comparative simulations, we observe that the FIG-OP solution exhibits improved coverage efficiency over solutions generated by greedy and traditional orienteering-based approaches (i.e. severe and minimal model uncertainty assumptions, respectively).},
  archive   = {C_IROS},
  author    = {Oriana Peltzer and Amanda Bouman and Sung-Kyun Kim and Ransalu Senanayake and Joshua Ott and Harrison Delecki and Mamoru Sobue and Mykel J. Kochenderfer and Mac Schwager and Joel Burdick and Ali-akbar Agha-mohammadi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981271},
  pages     = {8754-8761},
  title     = {FIG-OP: Exploring large-scale unknown environments on a fixed time budget},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning pseudo front depth for 2D forward-looking
sonar-based multi-view stereo. <em>IROS</em>, 8730–8737. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Retrieving the missing dimension information in acoustic images from 2D forward-looking sonar is a well-known problem in the field of underwater robotics. There are works attempting to retrieve 3D information from a single image which allows the robot to generate 3D maps with fly-through motion. However, owing to the unique image formulation principle, estimating 3D information from a single image faces severe ambiguity problems. Classical methods of multi-view stereo can avoid the ambiguity problems, but may require a large number of viewpoints to generate an accurate model. In this work, we propose a novel learning-based multi-view stereo method to estimate 3D information. To better utilize the information from multiple frames, an elevation plane sweeping method is proposed to generate the depth-azimuth-elevation cost volume. The volume after regularization can be considered as a probabilistic volumetric representation of the target. Instead of performing regression on the elevation angles, we use pseudo front depth from the cost volume to represent the 3D information which can avoid the 2D-3D problem in acoustic imaging. High-accuracy results can be generated with only two or three images. Synthetic datasets were generated to simulate various underwater targets. We also built the first real dataset with accurate ground truth in a large scale water tank. Experimental results demonstrate the superiority of our method, compared to other state-of-the-art methods.},
  archive   = {C_IROS},
  author    = {Yusheng Wang and Yonghoon Ji and Hiroshi Tsuchiya and Hajime Asama and Atsushi Yamashita},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982049},
  pages     = {8730-8737},
  title     = {Learning pseudo front depth for 2D forward-looking sonar-based multi-view stereo},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Forest traversability mapping (FTM): Traversability
estimation using 3D voxel-based normal distributed transform to enable
forest navigation. <em>IROS</em>, 8714–8721. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous navigation in dense vegetation remains an open challenge and is an area of major interest for the research community. In this paper we propose a novel traversability estimation method, the Forest Traversability Map, that gives autonomous ground vehicles the ability to navigate in harsh forests or densely vegetated environments. The method estimates travers ability in unstructured environments dominated by vegetation, void of any dominant human structures, gravel or dirt roads, with higher accuracy than the state of the art: we demonstrate an improvement of over 20\% F1 score (from 0.71 to 0.91) on challenging real-world data. Our method is based on 3D voxel representation and introduces a robust colour fusion method to overcome occlusion and frequent changes of lighting conditions in these environments. We also introduce and fuse multi-return lidar measurements into our probabilistic map representation in a recursive manner. Finally, we include information of neighboring voxels to increase our ability to assess the terrain travers ability correctly. These measures improve the state-of-the-art results and allow for effective traversability estimation in very challenging, densely vegetated environments.},
  archive   = {C_IROS},
  author    = {Fabio Ruetz and Paulo Borges and Niko Suenderhauf and Emili Hernández and Thierry Peynot},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981401},
  pages     = {8714-8721},
  title     = {Forest traversability mapping (FTM): Traversability estimation using 3D voxel-based normal distributed transform to enable forest navigation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Speed up of wave-driven unmanned surface vehicle using
passively transformable two-segment foils. <em>IROS</em>, 8698–8705. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For wave-driven unmanned surface vehicles (WUSVs), utilizing oscillating foils is the most straightforward and common wave energy conversion mechanism. Improving the thrust of the oscillating foil to increase its speed can help WUSVs improve their maneuverability and shorten the completion of ocean missions. This paper proposes a novel transformable two-segment foil, improving the wave energy-converting efficiency to provide more average thrust in every wave cycle. We estimate their working effectiveness numerically with a simple model to verify that the design enhances foils&#39; thrust force. The thrust enhancement was further confirmed by computational fluid dynamic (CFD) simulations, and we estimated the suitable values of parameters of the foils in several different common sea conditions in coastal waters by CFD simulations. We design and make two wave gliders with traditional and transformable two-segment foils and finish the speed enhancement experiments. The speed enhancement is verified, and transformable two-segment foils can increase the speed of WUSVs by 10\% in similar sea conditions in experiments.},
  archive   = {C_IROS},
  author    = {Lyucheng Xie and Hongzheng Cui and Tin Lun Lam},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982255},
  pages     = {8698-8705},
  title     = {Speed up of wave-driven unmanned surface vehicle using passively transformable two-segment foils},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online localisation and colored mesh reconstruction
architecture for 3D visual feedback in robotic exploration missions.
<em>IROS</em>, 8690–8697. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces an Online Localisation and Colored Mesh Reconstruction (OLCMR) ROS perception architecture for ground exploration robots aiming to perform robust Simultaneous Localisation And Mapping (SLAM) in challenging unknown environments and provide an associated colored 3D mesh representation in real time. It is intended to be used by a remote human operator to easily visualise the mapped environment during or after the mission or as a development base for further researches in the field of exploration robotics. The architecture is mainly composed of carefully-selected open-source ROS implementations of a LiDAR-based SLAM algorithm alongside a colored surface reconstruction procedure using a point cloud and RGB camera images projected into the 3D space. The overall performances are evaluated on the Newer College handheld LiDAR-Vision reference dataset and on two experimental trajectories gathered on board of representative wheeled robots in respectively urban and countryside outdoor environments.},
  archive   = {C_IROS},
  author    = {Quentin Serdel and Christophe Grand and Julien Marzat and Julien Moras},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981137},
  pages     = {8690-8697},
  title     = {Online localisation and colored mesh reconstruction architecture for 3D visual feedback in robotic exploration missions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Polytopic planar region characterization of rough terrains
for legged locomotion. <em>IROS</em>, 8682–8689. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies the problem of constructing polytopic representations for planar regions from depth camera readings. This problem is of great importance for terrain mapping in complicated environment and has great potentials in legged locomotion applications. To address the polytopic planar region characterization problem, we propose a two-stage solution scheme. At the first stage, the planar regions embedded within a sequence of depth images are extracted individually and then merged to establish a terrain map containing only planar regions in a selected frame. To simplify the representations of the planar regions that are applicable to foothold planning for legged robots, we further approximate the extracted planar regions via convex polytopes at the second stage. With the polytopic representation, the proposed approach achieves a great balance between accuracy and simplicity. Experimental validations with RGB-D cameras are conducted to demonstrate the performance of the proposed scheme. The proposed scheme successfully characterizes the planar regions via polytopes with acceptable accuracy. More importantly, the run time of the overall scheme is less than 10ms (i.e., &gt; 100Hz) throughout the tests, which strongly illustrates the advantages of our approach developed in this paper.},
  archive   = {C_IROS},
  author    = {Zhi Xu and Hongbo Zhu and Hua Chen and Wei Zhang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981553},
  pages     = {8682-8689},
  title     = {Polytopic planar region characterization of rough terrains for legged locomotion},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DPMP-deep probabilistic motion planning: A use case in
strawberry picking robot. <em>IROS</em>, 8675–8681. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel probabilistic approach to deep robot learning from demonstrations (LfD). Deep move-ment primitives (DMPs) are deterministic LfD model that maps visual information directly into a robot trajectory. This paper extends DMPs and presents a deep probabilistic model that maps the visual information into a distribution of effective robot trajectories. The architecture that leads to the highest level of trajectory accuracy is presented and compared with the existing methods. Moreover, this paper introduces a novel training method for learning domain-specific latent features. We show the superiority of the proposed probabilistic approach and novel latent space learning in the real-robot task of strawberry harvesting in the lab. The experimental results demonstrate that latent space learning can significantly improve model prediction performances. The proposed approach allows to sample trajectories from distribution and optimises the robot trajectory to meet a secondary objective, e.g. collision avoidance.},
  archive   = {C_IROS},
  author    = {Alessandra Tafuro and Bappaditya Debnath and Andrea M. Zanchettin and E. Amir Ghalamzan},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982187},
  pages     = {8675-8681},
  title     = {DPMP-deep probabilistic motion planning: A use case in strawberry picking robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Back to the manifold: Recovering from out-of-distribution
states. <em>IROS</em>, 8660–8666. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning from previously collected datasets of expert data offers the promise of acquiring robotic policies without unsafe and costly online explorations. However, a major challenge is a distributional shift between the states in the training dataset and the ones visited by the learned policy at the test time. While prior works mainly studied the distribution shift caused by the policy during the offline training, the problem of recovering from out-of-distribution states at the deployment time is not very well studied yet. We alleviate the distributional shift at the deployment time by introducing a recovery policy that brings the agent back to the training manifold whenever it steps out of the in-distribution states, e.g., due to an external perturbation. The recovery policy relies on an approximation of the training data density and a learned equivariant mapping that maps visual observations into a latent space in which translations correspond to the robot actions. We demonstrate the effectiveness of the proposed method through several manipulation experiments on a real robotic platform. Our results show that the recovery policy enables the agent to complete tasks while the behavioral cloning alone fails because of the distributional shift problem.},
  archive   = {C_IROS},
  author    = {Alfredo Reichlin and Giovanni Luca Marchetti and Hang Yin and Ali Ghadirzadeh and Danica Kragic},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981315},
  pages     = {8660-8666},
  title     = {Back to the manifold: Recovering from out-of-distribution states},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical model-based imitation learning for planning in
autonomous driving. <em>IROS</em>, 8652–8659. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We demonstrate the first large-scale application of model-based generative adversarial imitation learning (MGAIL) to the task of dense urban self-driving. We augment standard MGAIL using a hierarchical model to enable generalization to arbitrary goal routes, and measure performance using a closed-loop evaluation framework with simulated interactive agents. We train policies from expert trajectories collected from real vehicles driving over 100,000 miles in San Francisco, and demonstrate a steerable policy that can navigate robustly even in a zero-shot setting, generalizing to synthetic scenarios with novel goals that never occurred in real-world driving. We also demonstrate the importance of mixing closed-loop MGAIL losses with open-loop behavior cloning losses, and show our best policy approaches the performance of the expert. We evaluate our imitative model in both average and challenging scenarios, and show how it can serve as a useful prior to plan successful trajectories.},
  archive   = {C_IROS},
  author    = {Eli Bronstein and Mark Palatucci and Dominik Notz and Brandyn White and Alex Kuefler and Yiren Lu and Supratik Paul and Payam Nikdel and Paul Mougin and Hongge Chen and Justin Fu and Austin Abrams and Punit Shah and Evan Racah and Benjamin Frenkel and Shimon Whiteson and Dragomir Anguelov},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981695},
  pages     = {8652-8659},
  title     = {Hierarchical model-based imitation learning for planning in autonomous driving},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Output feedback tube MPC-guided data augmentation for
robust, efficient sensorimotor policy learning. <em>IROS</em>,
8644–8651. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Imitation learning (IL) can generate computationally efficient sensorimotor policies from demonstrations provided by computationally expensive model-based sensing and control algorithms. However, commonly employed IL methods are often data-inefficient, requiring the collection of a large number of demonstrations and producing policies with limited robustness to uncertainties. In this work, we combine IL with an output feedback robust tube model predictive controller (RTMPC) to co-generate demonstrations and a data augmentation strategy to efficiently learn neural network-based sensorimotor policies. Thanks to the augmented data, we reduce the computation time and the number of demonstrations needed by IL, while providing robustness to sensing and process uncertainty. We tailor our approach to the task of learning a trajectory tracking visuomotor policy for an aerial robot, leveraging a 3D mesh of the environment as part of the data augmentation process. We numerically demonstrate that our method can learn a robust visuomotor policy from a single demonstration—a two-orders of magnitude improvement in demonstration efficiency compared to existing IL methods.},
  archive   = {C_IROS},
  author    = {Andrea Tagliabue and Jonathan P. How},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981888},
  pages     = {8644-8651},
  title     = {Output feedback tube MPC-guided data augmentation for robust, efficient sensorimotor policy learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MMFN: Multi-modal-fusion-net for end-to-end driving.
<em>IROS</em>, 8638–8643. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inspired by the fact that humans use diverse sensory organs to perceive the world, sensors with different modalities are deployed in end-to-end driving to obtain the global context of the 3D scene. In previous works, camera and LiDAR inputs are fused through transformers for better driving performance. These inputs are normally further interpreted as high-level map information to assist navigation tasks. Nevertheless, extracting useful information from the complex map input is challenging, for redundant information may mislead the agent and negatively affect driving performance. We propose a novel approach to efficiently extract features from vectorized High-Definition (HD) maps and utilize them in end-to-end driving tasks. In addition, we design a new expert to enhance the model performance by considering multi-road rules. Experimental results prove that both proposed improvements enable our agent to achieve superior performance compared with other methods.},
  archive   = {C_IROS},
  author    = {Qingwen Zhang and Mingkai Tang and Ruoyu Geng and Feiyi Chen and Ren Xin and Lujia Wang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981775},
  pages     = {8638-8643},
  title     = {MMFN: Multi-modal-fusion-net for end-to-end driving},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Divide &amp; conquer imitation learning. <em>IROS</em>,
8630–8637. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When cast into the Deep Reinforcement Learning framework, many robotics tasks require solving a long horizon and sparse reward problem, where learning algorithms struggle. In such context, Imitation Learning (IL) can be a powerful approach to bootstrap the learning process. However, most IL methods require several expert demonstrations which can be prohibitively difficult to acquire. Only a handful of IL algorithms have shown efficiency in the context of an extreme low expert data regime where a single expert demonstration is available. In this paper, we present a novel algorithm designed to imitate complex robotic tasks from the states of an expert trajectory. Based on a sequential inductive bias, our method divides the complex task into smaller skills. The skills are learned into a goal-conditioned policy that is able to solve each skill individually and chain skills to solve the entire task. We show that our method imitates a non-holonomic navigation task and scales to a complex simulated robotic manipulation task with very high sample efficiency.},
  archive   = {C_IROS},
  author    = {Alexandre Chenu and Nicolas Perrin-Gilbert and Olivier Sigaud},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982020},
  pages     = {8630-8637},
  title     = {Divide &amp; conquer imitation learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Using human gaze in few-shot imitation learning for robot
manipulation. <em>IROS</em>, 8622–8629. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Imitation learning has attracted attention as a method for realizing complex robot control without programmed robot behavior. Meta-imitation learning has been proposed to solve the high cost of data collection and low generalizability to new tasks that imitation learning suffers from. Meta-imitation can learn new tasks involving unknown objects from a small amount of data by learning multiple tasks during training. However, meta-imitation learning, especially using images, is still vulnerable to changes in the background, which occupies a large portion of the input image. This study introduces a human gaze into meta-imitation learning-based robot control. We created a model with model-agnostic meta-learning to predict the gaze position from the image by measuring the gaze with an eye tracker in the head-mounted display. Using images around the predicted gaze position as an input makes the model robust to changes in visual information. We experimentally verified the performance of the proposed method through picking tasks using a simulated robot. The results indicate that our proposed method has a greater ability than the conventional method to learn a new task from only 9 demonstrations even if the object&#39;s color or the background pattern changes between the training and test.},
  archive   = {C_IROS},
  author    = {Shogo Hamano and Heecheol Kim and Yoshiyuki Ohmura and Yasuo Kuniyoshi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981706},
  pages     = {8622-8629},
  title     = {Using human gaze in few-shot imitation learning for robot manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Demonstrate once, imitate immediately (DOME): Learning
visual servoing for one-shot imitation learning. <em>IROS</em>,
8614–8621. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present DOME, a novel method for one-shot imitation learning, where a task can be learned from just a single demonstration and then be deployed immediately, without any further data collection or training. DOME does not require prior task or object knowledge, and can perform the task in novel object configurations and with distractors. At its core, DOME uses an image-conditioned object segmentation network followed by a learned visual servoing network, to move the robot&#39;s end-effector to the same relative pose to the object as during the demonstration, after which the task can be completed by replaying the demonstration&#39;s end-effector velocities. We show that DOME achieves near 100\% success rate on 7 real-world everyday tasks, and we perform several studies to thoroughly understand each individual component of DOME. Videos and supplementary material are available at: https://www.robot-learning.uk/dome.},
  archive   = {C_IROS},
  author    = {Eugene Valassakis and Georgios Papagiannis and Norman Di Palo and Edward Johns},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981982},
  pages     = {8614-8621},
  title     = {Demonstrate once, imitate immediately (DOME): Learning visual servoing for one-shot imitation learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LNC assisted localization and mapping in pipe environment.
<em>IROS</em>, 8567–8572. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Regular maintenance of pipelines is an important task to ensure oil transportation and other operation (sewers, nature gas). Precise localization of pipeline damage can greatly improve the efficiency of maintenance work. Since the texture similarity and illumination change of pipe, traditional local descriptors for image matching like SIFT, SURF and ORB are easy to suffer from false correspondences. As to remove the false matches, the local neighborhood constraints (LNC) that contain spatial constructs around feature points are proposed. Good correspondences are essential for the high-accuracy localization and mapping solution given the limited textures and illumination in the pipes. The LNC method is also integrated into the state-of-the-art visual SLAM system. The proposed LNC image matching method and the SLAM system are evaluated on datasets gathered from the pipe environment. Compared with other state-of-the-art methods, our LNC image matching method achieves similar or better performance in precision, recall and runtime. The SLAM system provides state estimation and map reconstruction of the pipe in real-time, and the localization error is within 1\%.},
  archive   = {C_IROS},
  author    = {Hongkai Zhang and Jianjun Yuan and Shijie Guo and Hesheng Wang and Shugen Ma and Sheng Bao and Liang Du},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981563},
  pages     = {8567-8572},
  title     = {LNC assisted localization and mapping in pipe environment},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). InCloud: Incremental learning for point cloud place
recognition. <em>IROS</em>, 8559–8566. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Place recognition is a fundamental component of robotics, and has seen tremendous improvements through the use of deep learning models in recent years. Networks can experience significant drops in performance when deployed in unseen or highly dynamic environments, and require additional training on the collected data. However naively fine-tuning on new training distributions can cause severe degradation of performance on previously visited domains, a phenomenon known as catastrophic forgetting. In this paper we address the problem of incremental learning for point cloud place recognition and introduce InCloud, a structure-aware distillation-based approach which preserves the higher-order structure of the network&#39;s embedding space. We introduce several challenging new benchmarks on four popular and large-scale LiDAR datasets (Oxford, MulRan, In-house and KITTI) showing broad improvements in point cloud place recognition performance over a variety of network architectures. To the best of our knowledge, this work is the first to effectively apply incremental learning for point cloud place recognition. Data pre-processing, training and evaluation code for this paper can be found at https://github.com/csiro-robotics/InCloud.},
  archive   = {C_IROS},
  author    = {Joshua Knights and Peyman Moghadam and Milad Ramezani and Sridha Sridharan and Clinton Fookes},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981252},
  pages     = {8559-8566},
  title     = {InCloud: Incremental learning for point cloud place recognition},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Centralized-equivalent pairwise estimation with asynchronous
communication constraints for two robots. <em>IROS</em>, 8544–8551. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collaboratively estimating the state of two robots under communication constraints is challenging regarding computational complexity and statistical optimality. Previous work only achieves practical solutions by either disregarding parts of the measurements or imposing a communication overhead, being non-optimal or not entirely distributed, respectively. In this work, we present a centralized-equivalent but dis-tributed approach for pairwise state estimation where two agents only communicate when they meet. Our approach utilizes elements from wave scattering theory to efficiently and consistently summarize (pre-compute) past estimator information (i.e., state evolution and uncertainty) between encounters of two agents. This summarized information is then used in a joint correction step taking into account all past information of each agent in a statistically correct way. This novel approach enables us to distribute the pre-computations of both state evolution and uncertainties on the agents and reconstruct the centralized-equivalent system estimate with very few computations once the agents meet again while still applying all measurements from both agents on both estimates upon encounter. We compare our approach on a real-world dataset against a state of the art collaborative state estimation approach.},
  archive   = {C_IROS},
  author    = {Eren Allak and Axel Barrau and Roland Jung and Jan Steinbrener and Stephan Weiss},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982117},
  pages     = {8544-8551},
  title     = {Centralized-equivalent pairwise estimation with asynchronous communication constraints for two robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Grasping state analysis of soft manipulator based on
flexible tactile sensor array. <em>IROS</em>, 8515–8520. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although the grasping state analysis is vital in the study of manipulators, the grasping state analysis of soft manipulators as an independent research topic is not much so far. This paper proposes a novel pneumatic soft manipulator with a flexible tactile sensor array (SM-FTSA). The flexible tactile sensor array comprises piezoresistive materials with a porous structure. An equal potential approach is adopted to realize the collection of tactile signals of the SM-FTSA. Inspired by the grasping analysis of rigid manipulators, we propose 4 grasping states for the SM-FTSA, including inflating, shaking, stable, and slipping. Based on the experimental data, we conduct grasping experiments on 12 objects with SM-FTSA, and we propose 10 features that reflect the grasping state. Several machine learning methods are utilized to classify the grasping state. Among them, the Random Forest method presents the best performance, and the average classification accuracy reaches 99\%.},
  archive   = {C_IROS},
  author    = {Haoyuan Wang and Hongge Ru and Hongliang Lei and Chi Zhang and Cheng Han and Hao Wu and Jian Huang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981972},
  pages     = {8515-8520},
  title     = {Grasping state analysis of soft manipulator based on flexible tactile sensor array},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Jammkle: Fibre jamming 3D printed multi-material tendons and
their application in a robotic ankle. <em>IROS</em>, 8507–8514. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fibre jamming is a new and understudied soft robotic mechanism that has previously found success in stiffness-tunable arms and fingers. However, to date researchers have not fully taken advantage of the freedom offered by contemporary fabrication techniques including multi-material 3D printing in the creation of fibre jamming structures. In this research, we present a novel, modular, multi-material, 3D printed, fibre jamming tendon unit for use in a stiffness-tunable compliant robotic ankle, or Jammkle. Its multimaterial printed design offers unparalleled design freedom, enabling application specific tendon design. We develop analytical and finite element models of the tendon unit, showing good agreement with experimental data and numerically explore the design space. Finally, we demonstrate a practical application by integrating multiple tendon units into a robotic ankle and perform extensive testing and characterisation. We show that the Jammkle outperforms comparative leg structures in terms of compliance, damping, and slip prevention.},
  archive   = {C_IROS},
  author    = {Joshua Pinskier and James Brett and Lauren Hanson and Katrina Lo Surdo and David Howard},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982171},
  pages     = {8507-8514},
  title     = {Jammkle: Fibre jamming 3D printed multi-material tendons and their application in a robotic ankle},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automated fruit quality testing using an electrical
impedance tomography-enabled soft robotic gripper. <em>IROS</em>,
8500–8506. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft robotic grippers are becoming increasingly popular for agricultural and logistics automation. Their passive conformability enables them to adapt to varying product shapes and sizes, providing stable large-area grasps. This work presents a novel methodology for combining soft robotic grippers with electrical impedance tomography-based sensors to infer intrinsic properties of grasped fruits. We use a Fin Ray soft robotic finger with embedded microspines to grab and obtain rich multi-direction electrical properties of the object. Learning-based techniques are then used to infer the desired fruit properties. The framework is extensively tested and validated on multiple fruit groups. Our results show that ripeness parameters and even weight of the grasped fruit can be estimated with reasonable accuracy autonomously using the proposed system.},
  archive   = {C_IROS},
  author    = {Elijah Almanzor and Thomas George Thuruthel and Fumiya Iida},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981987},
  pages     = {8500-8506},
  title     = {Automated fruit quality testing using an electrical impedance tomography-enabled soft robotic gripper},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-DoF soft robotic actuators based on spring reinforce
and particle jamming. <em>IROS</em>, 8494–8499. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft robots have a wide rang of applications due to their compliance, flexibility and low fabrication cost. Compare to rigid robots, soft robots are more safe for human. In this work, we design various multi-DoF actuators with spring reinforce and particle jamming, and two fabrication methods are proposed to make them. Each type of actuator is tested to evaluate the mechanical properties by experiments. Experimental results show that both spring and particle jamming have effect on the stiffness. Besides, Spring reinforced actuator (SRA) increases the maximum allowable inflation pressure, output bending force and gives a good linear relationship of bending. We also integrate spring and particles to fabricate a hybrid actuator whose behaviors are explored by experiments. Finally, we create a snake-like robotic manipulator assembled with two actuators and show its bending motion. Results show that the manipulator is able to achieve several bending shapes steadily and large range of motion.},
  archive   = {C_IROS},
  author    = {Weiwang Fan and He Xu and Haihang Wang and Siqing Chen and Qiandiao Wei and Chaochao You},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981444},
  pages     = {8494-8499},
  title     = {Multi-DoF soft robotic actuators based on spring reinforce and particle jamming},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design of a soft wearable passive fitness device for upper
limb resistance exercise. <em>IROS</em>, 8488–8493. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An increase in health awareness has fueled the development of fitness equipment or devices nowadays. Most conventional fitness devices have had some issues in space limitation and the high cost of equipment. With the advance in wearable robotics, we proposed a soft passive fitness wearable device for upper limb resistance exercises such as chest press, frontal raise, and chest fly. Users can customize the exercise intensity by adjusting the length of the elastic bands embedded in the wearable device. Moreover, the exer-tainment (Exercise-entertainment) user display interface was designed to motivate the user. Movements of users were estimated using inertial measurement units (IMUs), and haptic feedback was provided through the vibro-stimulation. Furthermore, the effectiveness of the proposed device was evaluated with the Borg scales representing the rating of perceived exertion (RPE) and measuring the surface electromyography (sEMG) of the three muscles located one on the shoulder and two on the chest. Both the Borg 6–20 and CR 10 scales were increased, and the normalized sEMG activities of the upper limb muscles with the activated device had more than double in magnitude compared to that with a bare condition; therefore, the proposed device has a potential effectiveness as for resistance exercise. Overall, this research devotes preliminary evidence on the benefits of the device in promoting the user to work out and contributing to the exercise effects.},
  archive   = {C_IROS},
  author    = {Junghoon Park and Jaehong Kim and Dong Hyun Kim and Jungsik Hwang and Youngtae G. Kim and SeungYong Hyung and Soon-Heum Ko and Minhyung Lee},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981189},
  pages     = {8488-8493},
  title     = {Design of a soft wearable passive fitness device for upper limb resistance exercise},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Confined water body coverage under resource constraints.
<em>IROS</em>, 8465–8471. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel algorithm for monitoring marine environments utilizing a resource-constrained robot. Collecting water quality data from large bodies of water is paramount for monitoring the ecosystem&#39;s health, particularly for predicting harmful cyanobacteria blooms. The large spatial dimensions of such bodies of water and the slow varying of water quality parameters make exhaustive, complete coverage impractical and unnecessary. This work explores a new strategy for efficiently measuring water quality quantities with an autonomous surface vehicle (ASV). The method utilizes the medial axis of the water body producing a guideline for the ASV trajectory that visits representative areas of the environment. The proposed method ensures data collection in the narrower parts of the lake, where researchers have historically observed harmful blooms while also visiting open water areas. It also presents an analysis of the Spatio-temporal sensitivity of the target sensor. A comparison with the traditional lawnmower algorithm demonstrates that the conventional BCD-based complete coverage method cannot sample the small coves of a lake. As such, we show that the proposed method captures more diverse regions of the area with a partial coverage technique. Offline analysis of several lakes and reservoirs and results from field deployments at Lake Murray, SC, USA, demonstrate the proposed method&#39;s effectiveness.},
  archive   = {C_IROS},
  author    = {Ibrahim Salman and Jason Raiti and Nare Karapetyan and Archana Venkatachari and Annie Bourbonnais and Jason M. O&#39;Kane and Ioannis Rekleitis},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981764},
  pages     = {8465-8471},
  title     = {Confined water body coverage under resource constraints},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DRACo-SLAM: Distributed robust acoustic
communication-efficient SLAM for imaging sonar equipped underwater robot
teams. <em>IROS</em>, 8457–8464. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An essential task for a multi-robot system is generating a common understanding of the environment and relative poses between robots. Cooperative tasks can be executed only when a vehicle has knowledge of its own state and the states of the team members. However, this has primarily been achieved with direct rendezvous between underwater robots, via inter-robot ranging. We propose a novel distributed multi-robot simultaneous localization and mapping (SLAM) framework for underwater robots using imaging sonar-based perception. By passing only scene descriptors between robots, we do not need to pass raw sensor data unless there is a likelihood of inter-robot loop closure. We utilize pairwise consistent measurement set maximization (PCM), making our system robust to erroneous loop closures. The functionality of our system is demonstrated using two real-world datasets, one with three robots and another with two robots. We show that our system effectively estimates the trajectories of the multi-robot system and keeps the bandwidth requirements of inter-robot communication low. To our knowledge, this paper describes the first instance of multi-robot SLAM using real imaging sonar data (which we implement offline, using simulated communication). Code link: https://github.com/jake3991/DRACo-SLAM.},
  archive   = {C_IROS},
  author    = {John McConnell and Yewei Huang and Paul Szenher and Ivana Collado-Gonzalez and Brendan Englot},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981822},
  pages     = {8457-8464},
  title     = {DRACo-SLAM: Distributed robust acoustic communication-efficient SLAM for imaging sonar equipped underwater robot teams},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HoloOcean: Realistic sonar simulation. <em>IROS</em>,
8450–8456. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sonar sensors play an integral part in underwater robotic perception by providing imagery at long distances where standard optical cameras cannot. They have proven to be an important part in various robotic algorithms including localization, mapping, and structure from motion. Unfortunately, generating realistic sonar imagery for algorithm development is difficult due to the high cost of field trials and lack of simulation methods. To remove these obstacles, we present various upgrades to the sonar simulation method in HoloOcean, our open-source marine robotics simulator. In particular, we improve the noise modeling using a novel cluster-based multipath ray-tracing algorithm, various probabilistic noise models, and material dependence. We also develop and integrate simulated models for side-scan, single-beam, and multibeam profiling sonars.},
  archive   = {C_IROS},
  author    = {Easton Potokar and Kalliyan Lay and Kalin Norman and Derek Benham and Tracianne B. Neilsen and Michael Kaess and Joshua G. Mangelson},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981119},
  pages     = {8450-8456},
  title     = {HoloOcean: Realistic sonar simulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). UWRange: An open ROS framework for simulating acoustic
ranging and localization for underwater robots under realistic
conditions. <em>IROS</em>, 8442–8449. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Considering realistic characteristics of acoustic localization methods is crucial for roboticists when developing guidance and control algorithms for small and agile underwater robots. Current simulators either rely purely on geometric distancing, i.e. do not consider dynamic effects such as robot motion during acoustic signal propagation, or they are too complex for usage by non-communication experts and, thus, vulnerable to misconfiguration. We propose an open ROS-based framework that extends existing robot simulators (e. g. Gazebo) by simulating the effects of realistic acoustic ranging for underwater robot localization. Thus, our simulator enables realistic real-time analysis and evaluation of guidance, navigation, and control algorithms in software in-the-loop systems. For this purpose, we incorporate and encapsulate the non-trivial characteristics of acoustic communication and ranging such as robot motion during signal propagation, packet reception failure, and modem timings. This ensures the applicability of the tool by roboticists who are typically non-experts in acoustic communication and guarantees accurate and realistic simulation results. We demonstrate the functionality and performance of our framework and validate it on real-world experimental data on the example of a two-way ranging method. Our open-source release includes well-defined interfaces and parameters as well as a tutorial. This targets other roboticists who can either use our framework directly or easily adapt it to their individual setup, e. g., by adding further acoustic-ranging protocols.},
  archive   = {C_IROS},
  author    = {Fabian Steinmetz and Daniel A Duecker and Nils Sichert and Christian Busse and Edwin Kreuzer and Bernd-Christian Renner},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981364},
  pages     = {8442-8449},
  title     = {UWRange: An open ROS framework for simulating acoustic ranging and localization for underwater robots under realistic conditions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving marine radar odometry by modeling radar resolution
and exploiting additional temporal information. <em>IROS</em>,
8436–8441. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Radar odometry may provide valuable input for surface vessels in several marine applications. The vulnerability of global positioning satellite systems to jamming and spoofing motivates the search for alternatives. In this work, we investigate the feasibility of W-band frequency modulated continuous wave radars in marine settings for odometry. A method to model radar resolution is presented and is further extend to include multiple radar frames. Numerical implementation relies on concepts from Lie theory. The proposed methods were evaluated on datasets collected from a ferry in a harbour area, where the average relative translation error was reduced to 3.07\% compared to 14.2\% of a baseline method.},
  archive   = {C_IROS},
  author    = {Carl H. Schiller and Bruno Arsenali and Deran Maas and Stefano Maranó},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981293},
  pages     = {8436-8441},
  title     = {Improving marine radar odometry by modeling radar resolution and exploiting additional temporal information},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analysis of hybrid cable-thruster actuated ROV in heavy
lifting interventions. <em>IROS</em>, 8430–8435. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many operations performed by work class Remotely Operated Vehicles (ROVs) require the manipulation of heavy loads. An example is the manipulation and grouting of armour stones. A way to increase the working capabilities of the ROV is to introduce cables among the set of actuators. The cable lengths and tensions are controlled by winches placed on the vehicle. Being similar to a cable-driven parallel robot (CDPR), the resultant system inherits some advantages such as the possibility to generate large forces over a large workspace and the possibility to use CDPR techniques to estimate the pose of the ROV. This paper proposes a complete control architecture for the Hybrid Cable-Thruster actuated ROV (HCT-ROV) and analyzes, in computer simulations, the performances of such a system while it performs real world operations, such as heavy lifting and hovering in presence of water current.},
  archive   = {C_IROS},
  author    = {Nikolas Sacchi and Enrico Simetti and Gianluca Antonelli and Giovanni Indiveri and Vincent Creuze and Marc Gouttefarde},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981861},
  pages     = {8430-8435},
  title     = {Analysis of hybrid cable-thruster actuated ROV in heavy lifting interventions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluating the benefit of using multiple low-cost
forward-looking sonar beams for collision avoidance in small AUVs.
<em>IROS</em>, 8423–8429. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We seek to rigorously evaluate the benefit of using a few beams rather than a single beam for a low-cost obstacle avoidance sonar for small AUVs. For a small low-cost AUV, the complexity, cost, and volume required for a multi-beam forward looking sonar are prohibitive. In contrast, a single-beam system is relatively easy to integrate into a small AUV, but does not provide the performance of a multi-beam solution. To better understand this trade-off, we seek to rigorously quantify the improvement with respect to obstacle avoidance performance of adding just a few beams to a single-beam forward looking sonar relative to the performance of the single-beam system. Our work fundamentally supports the goal of using small low-cost AUV systems in cluttered and unstructured environments. Specifically, we investigate the benefit of incorporating a port and starboard beam to a single-beam sonar system for collision avoidance. A methodology for collision avoidance is developed to obtain a fair comparison between a single-beam and multi-beam system, explicitly incorporating the geometry of the beam patterns from forward-looking sonars with large beam angles, and simulated using a high-fidelity representation of acoustic signal propagation.},
  archive   = {C_IROS},
  author    = {Christopher Morency and Daniel J. Stilwell},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982072},
  pages     = {8423-8429},
  title     = {Evaluating the benefit of using multiple low-cost forward-looking sonar beams for collision avoidance in small AUVs},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semantic scene completion through multi-level feature
fusion. <em>IROS</em>, 8399–8406. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Partial observation of indoor scenes (single-viewed RGB-D) carries insufficient spatial information for complex tasks such as autonomous navigation and virtual reality, thus many learning-based methods are proposed to realize semantic scene completion (SSC) from single-viewed input. However, most of them only extract scene-level features of input to generate output, which might lose details. In this paper, a new method that fully utilizes both instance-level and scene-level features is proposed. Firstly, an object detection module is pre-trained to localize indoor objects. Secondly, coarse completion result is obtained from scene-level feature using an encoder-decoder structure. Finally, based on the pre-trained bounding boxes, coarse completion result is refined using a geometric refinement module. Our network&#39;s performance is evaluated on both real and synthetic datasets. The results demonstrate that our network is able to reconstruct indoor scenes with more geometric details, get clearer boundaries between instances and outperform most existing SSC methods both intuitively and quantitatively.},
  archive   = {C_IROS},
  author    = {Ruochong Fu and Hang Wu and Mengxiang Hao and Yubin Miao},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981517},
  pages     = {8399-8406},
  title     = {Semantic scene completion through multi-level feature fusion},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-view guided multi-view stereo. <em>IROS</em>,
8391–8398. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a novel deep framework for dense 3D reconstruction from multiple image frames, leveraging a sparse set of depth measurements gathered jointly with image acquisition. Given a deep multi-view stereo network, our framework uses sparse depth hints to guide the neural network by modulating the plane-sweep cost volume built during the forward step, enabling us to infer constantly much more accurate depth maps. Moreover, since multiple viewpoints can provide additional depth measurements, we propose a multi-view guidance strategy that increases the density of the sparse points used to guide the network, thus leading to even more accurate results. We evaluate our Multi-View Guided framework within a variety of state-of-the-art deep multi-view stereo networks, demonstrating its effectiveness at improving the results achieved by each of them on BlendedMVG and DTU datasets.},
  archive   = {C_IROS},
  author    = {Matteo Poggi and Andrea Conti and Stefano Mattoccia},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982010},
  pages     = {8391-8398},
  title     = {Multi-view guided multi-view stereo},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inferring articulated rigid body dynamics from RGBD video.
<em>IROS</em>, 8383–8390. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Being able to reproduce physical phenomena ranging from light interaction to contact mechanics, simulators are becoming increasingly useful in more and more application domains where real-world interaction or labeled data are difficult to obtain. Despite recent progress, significant human effort is needed to configure simulators to accurately reproduce real-world behavior. We introduce a pipeline that combines inverse rendering with differentiable simulation to create digital twins of real-world articulated mechanisms from depth or RGB videos. Our approach automatically discovers joint types and estimates their kinematic parameters, while the dynamic properties of the overall mechanism are tuned to attain physically accurate simulations. Control policies optimized in our derived simulation transfer successfully back to the original system, as we demonstrate on a simulated system. Further, our approach accurately reconstructs the kinematic tree of an articulated mechanism being manipulated by a robot, and highly nonlinear dynamics of a real-world coupled pendulum mechanism. Website: https://eric-heiden.github.io/video2sim},
  archive   = {C_IROS},
  author    = {Eric Heiden and Ziang Liu and Vibhav Vineet and Erwin Coumans and Gaurav S. Sukhatme},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981687},
  pages     = {8383-8390},
  title     = {Inferring articulated rigid body dynamics from RGBD video},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning feature decomposition for domain adaptive monocular
depth estimation. <em>IROS</em>, 8376–8382. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Monocular depth estimation (MDE) has attracted intense study due to its low cost and critical functions for robotic tasks such as localization, mapping and obstacle detection. Supervised approaches have led to great success with the advance of deep learning, but they rely on large quantities of ground-truth depth annotations that are expensive to acquire. Unsupervised domain adaptation (UDA) transfers knowledge from labeled source data to unlabeled target data, so as to relax the constraint of supervised learning. However, existing UDA approaches may not completely align the domain gap across different datasets because of the domain shift problem. We believe better domain alignment can be achieved via well-designed feature decomposition. In this paper, we propose a novel UDA method for MDE, referred to as Learning Feature Decomposition for Adaptation (LFDA), which learns to decompose the feature space into content and style components. LFDA only attempts to align the content component since it has a smaller domain gap. Meanwhile, it excludes the style component which is specific to the source domain from training the primary task. Furthermore, LFDA uses separate feature distribution estimations to further bridge the domain gap. Extensive experiments on three domain adaptative MDE scenarios show that the proposed method achieves superior accuracy and lower computational cost compared to the state-of-the-art approaches.},
  archive   = {C_IROS},
  author    = {Shao-Yuan Lo and Wei Wang and Jim Thomas and Jingjing Zheng and Vishal M. Patel and Cheng-Hao Kuo},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981342},
  pages     = {8376-8382},
  title     = {Learning feature decomposition for domain adaptive monocular depth estimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). From local to holistic: Self-supervised single image 3D face
reconstruction via multi-level constraints. <em>IROS</em>, 8368–8375.
(<a href="https://doi.org/10.1109/IROS47612.2022.9982284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Single image 3D face reconstruction with accurate geometric details is a critical and challenging task due to the similar appearance on the face surface and fine details in organs. In this work, we introduce a self-supervised 3D face reconstruction approach from a single image that can recover detailed textures under different camera settings. The proposed network learns high-quality disparity maps from stereo face images during the training stage, while just a single face image is required to generate the 3D model in real applications. To recover fine details of each organ and facial surface, the framework introduces facial landmark spatial consistency to constrain the face recovering learning process in local point level and segmentation scheme on facial organs to constrain the correspondences at the organ level. The face shape and textures will further be refined by establishing holistic constraints based on the varying light illumination and shading information. The proposed learning framework can recover more accurate 3D facial details both quantitatively and qualitatively compared with state-of-the-art 3DMM and geometry-based reconstruction algorithms based on a single image.},
  archive   = {C_IROS},
  author    = {Yawen Lu and Michel Sarkis and Ning Bi and Guoyu Lu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982284},
  pages     = {8368-8375},
  title     = {From local to holistic: Self-supervised single image 3D face reconstruction via multi-level constraints},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-scaled and densely connected locally convolutional
layers for depth completion. <em>IROS</em>, 8360–8367. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The depth completion task aims to predict a dense depth map from a sparse LiDAR point cloud and an RGB image. This task is critical because an accurate depth map can be used as prior information to solve many computer vision tasks, such as downstream tasks in autonomous vehicles and robot vision. Previous deep learning methods which focus on the local affinity have achieved impressive results. However, an architecture that is directly designed to extract local affinity has not been proposed yet. In this paper, we propose multi-scaled and densely connected locally convolutional layers to learn the affinity of the neighborhood. We set a different grid factor for each step of this module, and each step consists of several convolutional layers applied only to the local area assigned from the grid factor. In addition, each step is densely connected, sequentially, to take advantage of the multi-scale receptive fields. The proposed module effectively learns the neighbor-hood&#39;s affinity in a local area with multiple scales, while keeping the network size small. As a result, our architecture achieves state-of-the-art performance compared to published works on the KITTI depth completion benchmark. On the NYU Depth V2 completion benchmark our method achieves performance comparable to state-of-the-art approaches.},
  archive   = {C_IROS},
  author    = {Sihaeng Lee and Eojindl Yi and Janghyeon Lee and Junmo Kim},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982179},
  pages     = {8360-8367},
  title     = {Multi-scaled and densely connected locally convolutional layers for depth completion},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised confidence for LiDAR depth maps and
applications. <em>IROS</em>, 8352–8359. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Depth perception is pivotal in many fields, such as robotics and autonomous driving, to name a few. Consequently, depth sensors such as LiDARs rapidly spread in many applications. The 3D point clouds generated by these sensors must often be coupled with an RGB camera to understand the framed scene semantically. Usually, the former is projected over the camera image plane, leading to a sparse depth map. Unfortunately, this process, coupled with the intrinsic issues affecting all the depth sensors, yields noise and gross outliers in the final output. Purposely, in this paper, we propose an effective unsupervised framework aimed at explicitly addressing this issue by learning to estimate the confidence of the LiDAR sparse depth map and thus allowing for filtering out the outliers. Experimental results on the KITTI dataset highlight that our framework excels for this purpose. Moreover, we demonstrate how this achievement can improve a wide range of tasks.},
  archive   = {C_IROS},
  author    = {Andrea Conti and Matteo Poggi and Filippo Aleotti and Stefano Mattoccia},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981654},
  pages     = {8352-8359},
  title     = {Unsupervised confidence for LiDAR depth maps and applications},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Watch me calibrate my force-sensing shoes! <em>IROS</em>,
8344–8351. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel method for smaller-sized humanoid robots to self-calibrate their foot force sensors. The method consists of two steps: 1. The robot is commanded to move along planned whole-body trajectories in different double support configurations. 2. The sensor parameters are determined by minimizing the error between the measured and modeled center of pressure (CoP) and ground reaction force (GRF) during the robot&#39;s movement using optimization. This is the first proposed autonomous calibration method for foot force-sensing devices in smaller humanoid robots. Furthermore, we introduce a high-accuracy manual calibration method to establish CoP ground truth, which is used to validate the measured CoP using self-calibration. The results show that the self-calibration can accurately estimate CoP and GRF without any manual intervention. Our method is demonstrated using a NAO humanoid platform and our previously presented force-sensing shoes.},
  archive   = {C_IROS},
  author    = {Yuanfeng Han and Boren Jiang and Gregory S. Chirikjian},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982221},
  pages     = {8344-8351},
  title     = {Watch me calibrate my force-sensing shoes!},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Walking control framework on uneven terrain using variable
stiffness sole. <em>IROS</em>, 8328–8335. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although many walking control frameworks have been developed to enable biped robots to walk stably on uneven terrain, the foot sole of the robot is also important. Inspired by that study, we have developed a Variable Stiffness Sole (VSS), which is able to adapt to the shape of the obstacles on the ground in Compliant Mode and provide robust support in Stiff Mode. Furthermore, we proposed a walking control framework on uneven terrain for a biped robot equipped with the developed VSS. The proposed walking control framework comprises a posture balance controller to stabilize the zero moment point from disturbances applied to the robot, an ankle torque/foot force controller for walking on uneven terrain, and a VSS controller to change the mode of the VSS. Finally, the proposed framework was verified through walking experiments of RoK-3 equipped with the VSS module on a single obstacle and uneven terrain with various obstacles.},
  archive   = {C_IROS},
  author    = {Yun-Ho Han and Junyeon Namgung and Baek-Kyu Cho},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981889},
  pages     = {8328-8335},
  title     = {Walking control framework on uneven terrain using variable stiffness sole},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved zero step push recovery with a unified reduced
order model of standing balance. <em>IROS</em>, 8321–8327. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Standing balance for legged robots can be achieved through regulating the center of pressure (ankle strategy), the angular momentum about the center of mass (hip strategy), and the magnitude of ground reaction force (variable height strategy). Prevalent reduced order models used to model legged robots at most only capture two of these strategies, and the contribution of the three available strategies is unclear. We propose a unified reduced order model that includes all three standing balance strategies and compared push recovery simulations of the unified model against existing balancing models using a nonlinear model predictive controller. We also developed a full body controller for a simple one legged balancing robot that tracked control from the reduced order models. For both the reduced order model and robot simulations, we found that the unified model could recover successfully from the largest pushes and yielded the smallest center of mass excursions. Between the hip and variable height strategies, the hip had the greatest effect on improving performance. Our results suggest that successful implementation of a unified reduced order model on physical robots would enable a simplified controller that takes advantage of available balancing strategies as needed to recover from larger push disturbances than feasible before.},
  archive   = {C_IROS},
  author    = {Thomas Huckell and Amy R. Wu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981193},
  pages     = {8321-8327},
  title     = {Improved zero step push recovery with a unified reduced order model of standing balance},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hands-free telelocomotion of a wheeled humanoid.
<em>IROS</em>, 8313–8320. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic systems capable of Dynamic Mobile Manipulation (DMM) tasks combine dynamic manipulation and locomotion and could facilitate dangerous or physically demanding labor. For instance, firefighter humanoid robots could leverage their body by leaning against collapsed building rubble to push it aside. Here we introduce a teleoperation system that targets the realization of these tasks using human&#39;s whole-body motor skills. We describe a new wheeled humanoid platform, SATYRR, and a novel hands-free teleoperation architecture using a whole-body Human Machine Interface (HMI). This system enables telelocomotion of the humanoid robot using the operator&#39;s body motion, freeing their arms for manipulation tasks. In this study we evaluate the efficacy of the proposed system on hardware, and explore the control of SATYRR using two teleoperation mappings that map the operators body pitch and yaw to the robot&#39;s velocity or acceleration. Through experiments and user feedback we showcase our preliminary findings of the pilot-system response. Results suggest that the HMI is capable of effectively telelocomoting SATYRR, that pilot preferences should dictate the appropriate motion mapping and gains, and finally that the pilot can better learn to control the system over time. This study represents a fundamental step towards the realization of combined manipulation and locomotion via teleoperation.},
  archive   = {C_IROS},
  author    = {Amartya Purushottam and Yeongtae Jung and Kevin Murphy and Donghoon Baek and Joao Ramos},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981629},
  pages     = {8313-8320},
  title     = {Hands-free telelocomotion of a wheeled humanoid},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Human-humanoid robot cooperative load transportation:
Model-based control approach. <em>IROS</em>, 8306–8312. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In order to properly integrate humanoid robots in real-life situations, they must be able to collaborate with humans in completing tasks. One of these tasks is the cooperative transportation of a heavy object, which has been widely studied in the humanoids literature. However, the proposed methods rely heavily on six-axis force/torque (F/T) sensors at the wrists, which medium-sized or even some full-sized humanoid robots do not have. This paper proposes an observer to overcome the lack of F/T sensors. The observer is then coupled with a simplified dynamic model of the transportation task allowing the humanoid robot to carry out the task in a stable way. The method is tested in simulation using a humanoid robot that does not have F/T sensors, a NAO robot, to demonstrate its performance. These tests pointed out that the proposed method successfully estimated the interaction forces while generating stable walking patterns.},
  archive   = {C_IROS},
  author    = {Rémy Rahem and Christopher Yee Wong and Wael Suleiman},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981487},
  pages     = {8306-8312},
  title     = {Human-humanoid robot cooperative load transportation: Model-based control approach},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust humanoid walking system considering recognized
terrain and robots’ balance. <em>IROS</em>, 8298–8305. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When robots walk on uneven terrain, trajectory planning should take into account both the whole-body dy-namics and the ground geometry simultaneously. In uneven terrain environments, there are only a limited number of places where the robot is able to make stable contact with the ground without its feet wobbling or slipping because of the intricate round geometry. In such environments, the optional landing position and time to maintain the robot&#39;s balance and stable foot contact are not obvious and computationally expensive. In this study, we propose a robust walking system that integrates environment recognition using steppable regions and walking control for a humanoid robot to walk on uneven terrain. In this paper, a steppable region is defined as a two-dimensional convex hull that represents a region where a robot is capable of landing. We propose a method to compute the steppable region quickly by 2.SD projection of the environment points and spatial filtering. In this system, the walking controller integrates the steppable region with the Capture Region to modify the landing position from a two-dimensional geometric calculation. In addition, to cope with the environment recognition error, we have introduced a trajectory generation that allows the feet to penetrate the ground and hybrid control of position and torque. We verified the effectiveness of the proposed system through experiments in which a life-size humanoid robot walked on uneven terrain and recovered when pushed.},
  archive   = {C_IROS},
  author    = {Shimpei Sato and Yuta Kojio and Yohei Kakiuchi and Kunio Kojima and Kei Okada and Masayuki Inaba},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981410},
  pages     = {8298-8305},
  title     = {Robust humanoid walking system considering recognized terrain and robots&#39; balance},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Experimental demonstration of a general balancing controller
on an untethered planar inverted double pendulum. <em>IROS</em>,
8292–8297. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper demonstrates the practical performance of a new theory of balance control that has been shown in simulation to out-perform earlier balance control theories in the sense of allowing the robot to make larger and faster movements while still maintaining its balance. The case studied here is that of a general planar double inverted pendulum, which resembles a legged robot&#39;s behaviour when the polygon of support shrinks to a line. The results show the speed and accuracy of the controller, as well as its robustness to external disturbances and slipping during fast movements.},
  archive   = {C_IROS},
  author    = {Federico Allione and Antonios E. Gkikakis and Roy Featherstone},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981380},
  pages     = {8292-8297},
  title     = {Experimental demonstration of a general balancing controller on an untethered planar inverted double pendulum},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep gesture generation for social robots using
type-specific libraries. <em>IROS</em>, 8286–8291. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Body language such as conversational gesture is a powerful way to ease communication. Conversational gestures do not only make a speech more lively but also contain semantic meaning that helps to stress important information in the discussion. In the field of robotics, giving conversational agents (humanoid robots or virtual avatars) the ability to properly use gestures is critical, yet remain a task of extraordinary difficulty. This is because given only a text as input, there are many possibilities and ambiguities to generate an appropriate gesture. Different to previous works we propose a new method that explicitly takes into account the gesture types to reduce these ambiguities and generate human-like conversational gestures. Key to our proposed system is a new gesture database built on the TED dataset that allows us to map a word to one of three types of gestures: “Imagistic” gestures, which express the content of the speech, “Beat” gestures, which emphasize words, and “No gestures.” We propose a system that first maps the words in the input text to their corresponding gesture type, generate type-specific gestures and combine the generated gestures into one final smooth gesture. In our comparative experiments, the effectiveness of the proposed method was confirmed in user studies for both avatar and humanoid robot.},
  archive   = {C_IROS},
  author    = {Hitoshi Teshima and Naoki Wake and Diego Thomas and Yuta Nakashima and Hiroshi Kawasaki and Katsushi Ikeuchi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981734},
  pages     = {8286-8291},
  title     = {Deep gesture generation for social robots using type-specific libraries},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SanitizerBot: How human-in-the-loop social robots can
playfully support humans. <em>IROS</em>, 8278–8285. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper evaluates a robot that distributed hand-sanitizer over an eight month period (October 2020-June 2021) in public places on the Oregon State University campus. During COVID times, many robots have been deployed in public places as social distancing enforcers, food delivery robots, UV-sanitation robots and more, but few studies have assessed the social situations of these robots. Using the context of robot distributing hand sanitizer, this work explores the benefits that social robots may provide to encouraging healthy human activities, as well as ways in which street-performance inspired approaches and a bit of humor might improve the quality and experience of functional human-robot interactions. After gaining human-in-the-loop deployment experience with a customized interface to enable both planned and improvized responses to human bystanders, we run two sub-studies. In the first, we compare the performance of the robot (moving or still) relative to a traditional hand sanitizer dispenser stick ( $\mathrm{N}=2048$ , 3 week data collection period). In the second, we evaluate how varied utterance strategies further impact the interaction results ( $\mathrm{N}=185$ , 2 week data collection period). The robot dramatically outperforms the stick dispenser across all tracked behavioral variables, cuing high levels of positive social engagement. This work finds the utterance design is more complex socially, and offer insights to future robot designers about how to integrate helpful and playful speech into service robot interactions. Finally, across both sub-studies, the work shows that people in groups are more likely to engage with the robot and each other, as well as sanitize their hands.},
  archive   = {C_IROS},
  author    = {Yao-Lin Tsai and Parthasarathy Reddy Bana and Sierra Loiselle and Heather Knight},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981917},
  pages     = {8278-8285},
  title     = {SanitizerBot: How human-in-the-loop social robots can playfully support humans},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel wire-driven 3D eyebrow design for communication with
humanoid robot iCub. <em>IROS</em>, 8248–8254. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The purpose of this research is to contribute to social communication between humans and robots in scenes that have been considered difficult due to the limited facial expression capabilities of robots. In order to provide more detailed facial expressions, we designed a novel wire-driven 3D eyebrow using a soft material with a bending structure. We then demonstrated the mechanical properties of the eyebrow design and developed a prototype that could be implemented on the humanoid robot iCub to verify its operation. Lastly, we confirmed that the new design enables the production of more slight changes in facial expressions by allowing the eyebrows to change their shape at more delicate angles and continuity than the LED eyebrows of the conventional iCub.},
  archive   = {C_IROS},
  author    = {Motonobu Aoki and Karthikeyan Kalyanasundaram Balasubramanian and Diego Torazza and Francesco Rea and Doreen Jirak and Giulio Sandini and Takura Yanagi and Atsushi Takamatsu and Stephane Bouet and Tomohiro Yamamura},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981954},
  pages     = {8248-8254},
  title     = {A novel wire-driven 3D eyebrow design for communication with humanoid robot iCub},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Should a robot follow social norms? Human-robot interaction
design for social relations in mixed age group. <em>IROS</em>,
8234–8239. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Social relations within a group are one of the factors that build the social context. Less attention has been paid to social relations within a group for human-robot interaction design. In this study, we designed different types of service behaviors for social relations in a mixed age group and conducted an experiment to investigate the effect of service behavior types (serving the elderly first versus serving the young first versus serving without priority) on a user&#39;s evaluation of a service robot. We only found that service evaluation and appropriateness of the robot were rated less positively under the condition of serving the young first than under other conditions. In addition, we found that the participants evaluated the robot serving the young first as impolite. The effect of the robot&#39;s behaviors on service evaluation and appropriateness was mediated by politeness. The current study provides an initial basis for human-robot interaction design for social norms in group-robot interaction.},
  archive   = {C_IROS},
  author    = {Sangmin Kim and Jongsuk Choi and Yoonseob Lim and Sonya S. Kwak},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981755},
  pages     = {8234-8239},
  title     = {Should a robot follow social norms? human-robot interaction design for social relations in mixed age group},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HRI framework for continual learning in face recognition.
<em>IROS</em>, 8226–8233. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recognizing human partners is an essential social skill for building personalized and long-term human-robot interactions. However, robots deployed in complex, real-world environments have to face several challenges, such as managing unstructured interactions with multiple users, limited computational resources, and intrinsic and continuous variability of their sensory evidence. To cope with these challenges, we propose a framework to perform autonomous incremental learning for open-set face recognition suitable for unconstrained HRI scenarios. We validated the proposed framework in a real-world experiment, demonstrating its suitability to let the robot autonomously interact with multiple people while creating a labeled database of their faces across various encounters. Furthermore, we evaluated how an off-the-shelf model performed with data gathered from the HRI setting and proposed a fine-tuned model obtained with a transfer learning technique. Analyses about automatic threshold determination and rehearsal methods for memory sampling were also proposed. Our preliminary results suggest that exploiting the first-hand robot&#39;s experience could be crucial to ensure better models&#39; performance and, therefore, could be advantageous for the acceptance and effectiveness of social robots in the long run. With this work, we aim to provide insights on continual learning approaches in the HRI field to promote autonomous and personalized solutions meaningful for real-world applications.},
  archive   = {C_IROS},
  author    = {Giulia Belgiovine and Jonas Gonzlez-Billandon and Alessandra Sciutti and Giulio Sandini and Francesco Rea},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981671},
  pages     = {8226-8233},
  title     = {HRI framework for continual learning in face recognition},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Competency assessment for autonomous agents using deep
generative models. <em>IROS</em>, 8211–8218. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For autonomous agents to act as trustworthy partners to human users, they must be able to reliably communicate their competency for the tasks they are asked to perform. Towards this objective, we develop probabilistic world models based on deep generative modelling that allow for the simulation of agent trajectories and accurate calculation of tasking outcome probabilities. By combining the strengths of conditional variational autoencoders with recurrent neural networks, the deep generative world model can probabilistically forecast trajectories over long horizons to task completion. We show how these forecasted trajectories can be used to calculate outcome probability distributions, which enable the precise assessment of agent competency for specific tasks and initial settings.},
  archive   = {C_IROS},
  author    = {Aastha Acharya and Rebecca Russell and Nisar R. Ahmed},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981991},
  pages     = {8211-8218},
  title     = {Competency assessment for autonomous agents using deep generative models},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sequential manipulation planning on scene graph.
<em>IROS</em>, 8203–8210. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We devise a 3D scene graph representation, contact graph + (cg + ), for efficient sequential manipulation planning. Augmented with predicate-like attributes, this contact graph-based representation abstracts scene layouts with succinct geometric information and valid robot-scene interactions. Goal configurations, naturally specified on contact graphs, can be produced by a genetic algorithm with a stochastic optimization method. A task plan is then initialized by computing the Graph Editing Distance (GED) between the initial contact graph and the goal configuration, which generates graph edit operations corresponding to possible robot actions. We finalize the task plan by imposing constraints to regulate the temporal feasibility of graph edit operations, ensuring valid task and motion correspondences. In a series of simulated and real experiments, robots successfully complete complex sequential object rearrangement tasks that are difficult to specify using conventional planning language like Planning Domain Definition Language (PDDL), demonstrating high potential of planning sequential manipulation tasks on cg + .},
  archive   = {C_IROS},
  author    = {Ziyuan Jiao and Yida Niu and Zeyu Zhang and Song-Chun Zhu and Yixin Zhu and Hangxin Liu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981735},
  pages     = {8203-8210},
  title     = {Sequential manipulation planning on scene graph},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Realistic real-time simulation of RGB and depth sensors for
dynamic scenarios using augmented image based rendering. <em>IROS</em>,
8181–8188. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simulation remains one of the key methods for testing and validation of robotic perception systems and it also becomes increasingly important for training visuomotor policies for autonomous driving or manipulation. Further, as perception pipelines tend to leverage increasing amounts of modalities, it appears vital to simulate additional cues such as depth maps aside from RGB images. To align simulation with real-world observations, it is key to achieve realistic renderings of these maps, which includes the capability of rendering other dynamic objects in the scene. In this work, we propose an approach to real-time simulation of photo-realistic RGB images and sensor-realistic depth maps, that can contain dynamic objects at user-defined locations. Our method employs a selection of static samples of a pre-recorded database and multimodal cues from CAD models that are fused and warped to synthesize new imagery for a target camera pose. We show the efficacy of our method on newly proposed datasets recorded in a variety of different setups.},
  archive   = {C_IROS},
  author    = {Johan Vertens and Wolfram Burgard},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982014},
  pages     = {8181-8188},
  title     = {Realistic real-time simulation of RGB and depth sensors for dynamic scenarios using augmented image based rendering},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to simulate realistic LiDARs. <em>IROS</em>,
8173–8180. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simulating realistic sensors is a challenging part in data generation for autonomous systems, often involving carefully handcrafted sensor design, scene properties, and physics modeling. To alleviate this, we introduce a pipeline for data-driven simulation of a realistic LiDAR sensor. We propose a model that learns a mapping between RGB images and corresponding LiDAR features such as raydrop or perpoint intensities directly from real datasets. We show that our model can learn to encode realistic effects such as dropped points on transparent surfaces or high intensity returns on reflective materials. When applied to naively raycasted point clouds provided by off-the-shelf simulator software, our model enhances the data by predicting intensities and removing points based on the scene&#39;s appearance to match a real LiDAR sensor. We use our technique to learn models of two distinct LiDAR sensors and use them to improve simulated LiDAR data accordingly. Through a sample task of vehicle segmentation, we show that enhancing simulated point clouds with our technique improves downstream task performance.},
  archive   = {C_IROS},
  author    = {Benoît Guillard and Sai Vemprala and Jayesh K. Gupta and Ondrej Miksik and Vibhav Vineet and Pascal Fua and Ashish Kapoor},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981120},
  pages     = {8173-8180},
  title     = {Learning to simulate realistic LiDARs},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fidelity evaluation of virtual traffic based on anomalous
trajectory detection. <em>IROS</em>, 8157–8164. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Measuring the fidelity of synthesized virtual traffic has become an important and fundamental concern for evaluating the performance of different traffic simulation techniques and applications of autonomous vehicle testing. In this work, we propose a novel method to evaluate the fidelity of any trajectory data from the perspective of anomalous trajectory detection. First, given the trajectory data to be evaluated as input, the method learns spatio-temporal traffic features and reconstructs the input trajectory through a Long Short-Term Memory (LSTM)-based autoencoder architecture. Then, the anomalous trajectories are detected by comparing the reconstructed trajectories and the input ones using the reconstruction error as the benchmark. Our method can detect eight different kinds of anomalous trajectory in terms of changes in velocity and moving direction. In order to evaluate the fidelity of the input trajectory, we design a perceptual evaluation on virtual traffic fidelity and derive a mapping from the reconstruction error to the evaluation score. We demonstrated the effectiveness and robustness of our metric through many experiments on real-world and synthetic trajectory data containing different types of motion anomalies.},
  archive   = {C_IROS},
  author    = {Chaoneng Li and Qianwen Chao and Guanwen Feng and Qiongyan Wang and Pengfei Liu and Yunan Li and Qiguang Miao},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981800},
  pages     = {8157-8164},
  title     = {Fidelity evaluation of virtual traffic based on anomalous trajectory detection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Collaborative teleoperation with haptic feedback for
collision-free navigation of ground robots. <em>IROS</em>, 8141–8148.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a collaborative teleoperation algorithm which utilizes haptic force feedback to guide users around oncoming obstacles while accounting for non-holonomic constraints. The proposed algorithm predicts the user&#39;s goal, plans a path using a modified RRT * algorithm to the predicted goal, and provides haptic guidance to the path and away from obstacles when the user is in an unsafe pose. We show that the vehicle cannot collide with obstacles under the proposed algorithm following the haptic commands. We assess the per-formance of our algorithm with a virtual pilot in simulations and hardware experiments, demonstrating its ability to prevent collisions while reaching the goal location. Additionally, we demonstrate human-in-the-loop navigation with a Geomagic Touch haptic device providing force feedback to the user. These simulations and experiments show that the proposed haptic guidance system is a useful and effective tool for co-navigation of non-holonomic vehicles via teleoperation.},
  archive   = {C_IROS},
  author    = {Mela Coffey and Alyssa Pierson},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981426},
  pages     = {8141-8148},
  title     = {Collaborative teleoperation with haptic feedback for collision-free navigation of ground robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A framework for the systematic evaluation of obstacle
avoidance and object-aware controllers. <em>IROS</em>, 8117–8124. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-time control is an essential aspect of safe robot operation in the real world with dynamic objects. We present a framework for the analysis of object-aware con-trollers, methods for altering a robot&#39;s motion to anticipate and avoid possible collisions. This framework is focused on three design considerations: kinematics, motion profiles, and virtual constraints. Additionally, the analysis in this work relies on verification of robot behaviors using fundamental robot-obstacle experimental scenarios. To showcase the effectiveness of our method we compare three representative object-aware controllers. The comparison uses metrics originating from the design considerations. From the analysis, we find that the design of object-aware controllers often lacks kinematic considerations, continuity of control points, and stability in movement profiles. We conclude that this framework can be used in the future to design, compare, and benchmark obstacle avoidance methods.},
  archive   = {C_IROS},
  author    = {Caleb Escobedo and Nataliya Nechyporenko and Shreyas Kadekodi and Alessandro Roncone},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982198},
  pages     = {8117-8124},
  title     = {A framework for the systematic evaluation of obstacle avoidance and object-aware controllers},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HIRO: Heuristics informed robot online path planning using
pre-computed deterministic roadmaps. <em>IROS</em>, 8109–8116. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the goal of efficiently computing collisionfree robot motion trajectories in dynamically changing environments, we present results of a novel method for Heuristics Informed Robot Online Path Planning (HIRO). Dividing robot environments into static and dynamic elements, we use the static part for initializing a deterministic roadmap, which provides a lower bound of the final path cost as informed heuristics for fast path-finding. These heuristics guide a search tree to explore the roadmap during runtime. The search tree examines the edges using a fuzzy collision checking concerning the dynamic environment. Finally, the heuristics tree exploits knowledge fed back from the fuzzy collision checking module and updates the lower bound for the path cost. As we demonstrate in real-world experiments, the closed-loop formed by these three components significantly accelerates the planning procedure. An additional backtracking step ensures the feasibility of the resulting paths. Experiments in simulation and the real world show that HIRO can find collisionfree paths considerably faster than baseline methods with and without prior knowledge of the environment.},
  archive   = {C_IROS},
  author    = {Xi Huang and Gergely Sóti and Hongyi Zhou and Christoph Ledermann and Björn Hein and Torsten Kröger},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981740},
  pages     = {8109-8116},
  title     = {HIRO: Heuristics informed robot online path planning using pre-computed deterministic roadmaps},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reactive neural path planning with dynamic obstacle
avoidance in a condensed configuration space. <em>IROS</em>, 8101–8108.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a biologically inspired approach for path planning with dynamic obstacle avoidance. Path plan-ning is performed in a condensed configuration space of a robot generated by self-organizing neural networks (SONN). The robot itself and static as well as dynamic obstacles are mapped from the Cartesian task to the configuration space by precomputed kinematics. The condensed space represents a cognitive map of the environment, which is inspired by place cells and the concept of cognitive maps in mammalian brains. Generation of training data as well as the evaluation are performed on a real industrial robot accompanied by simulations. To evaluate reactive collision-free online planning within a changing environment, a demonstrator was realized. Then, a comparative study regarding sample-based planners was carried out. The robot is able to operate in dynamically changing environments and re-plan its motion trajectories within impressing 0.02 seconds, which proofs the real-time capability of our concept.},
  archive   = {C_IROS},
  author    = {Lea Steffen and Tobias Weyer and Stefan Ulbrich and Arne Roennau and Rüdiger Dillmann},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981453},
  pages     = {8101-8108},
  title     = {Reactive neural path planning with dynamic obstacle avoidance in a condensed configuration space},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Informed sampling-based collision avoidance with least
deviation from the nominal path. <em>IROS</em>, 8094–8100. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses local path re-planning for n-dimensional systems by introducing an informed sampling scheme and cost function to achieve collision avoidance with minimum deviation from an (optimal) nominal path. The proposed informed subset consists of the union of ellipsoids along the specified nominal path, such that the subset efficiently encapsulates all points along the nominal path. The cost function penalizes large deviations from the nominal path, thereby ensuring current safety in the face of potential collisions while retaining most of the overall efficiency of the nominal path. The proposed method is demonstrated on scenarios related to the navigation of autonomous marine crafts.},
  archive   = {C_IROS},
  author    = {Thomas T. Enevoldsen and Roberto Galeazzi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982202},
  pages     = {8094-8100},
  title     = {Informed sampling-based collision avoidance with least deviation from the nominal path},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Differentiable collision avoidance using collision
primitives. <em>IROS</em>, 8086–8093. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A central aspect of robotic motion planning is collision avoidance, where a multitude of different approaches are currently in use. Optimization-based motion planning is one method, that often heavily relies on distance computations between robots and obstacles. These computations can easily become a bottleneck, as they do not scale well with the complexity of the robots or the environment. To improve performance, many different methods suggested to use collision primitives, i.e. simple shapes that approximate the more complex rigid bodies, and that are simpler to compute distances to and from. However, each pair of primitives requires its own specialized code, and certain pairs are known to suffer from numerical issues. In this paper, we propose an easy-to-use, unified treatment of a wide variety of primitives. We formulate distance computation as a minimization problem, which we solve iteratively. We show how to take derivatives of this minimization problem, allowing it to be seamlessly integrated into a trajectory optimization method. We demonstrate that the resulting method can be used to plan smooth and collision-free paths based on a variety of single- and multi-robot scenarios with different obstacles.},
  archive   = {C_IROS},
  author    = {Simon Zimmermann and Matthias Busenhart and Simon Huber and Roi Poranne and Stelian Coros},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981093},
  pages     = {8086-8093},
  title     = {Differentiable collision avoidance using collision primitives},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distilled visual and robot kinematics embeddings for metric
depth estimation in monocular scene reconstruction. <em>IROS</em>,
8072–8077. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Estimating precise metric depth and scene reconstruction from monocular endoscopy is a fundamental task for surgical navigation in robotic surgery. However, traditional stereo matching adopts binocular images to perceive the depth information, which is difficult to transfer to the soft robotics-based surgical systems due to the use of monocular endoscopy. In this paper, we present a novel framework that combines robot kinematics and monocular endoscope images with deep unsupervised learning into a single network for metric depth estimation and then achieve 3D reconstruction of complex anatomy. Specifically, we first obtain the relative depth maps of surgical scenes by leveraging a brightness-aware monocular depth estimation method. Then, the corresponding endoscope poses are computed based on non-linear optimization of geo-metric and photometric reprojection residuals. Afterwards, we develop a Depth-driven Sliding Optimization (DDSO) algorithm to extract the scaling coefficient from kinematics and calculated poses offline. By coupling the metric scale and relative depth data, we form a robust ensemble that represents the metric and consistent depth. Next, we treat the ensemble as supervisory labels to train a metric depth estimation network for surgeries (i.e., MetricDepthS-Net) that distills the embeddings from the robot kinematics, endoscopic videos, and poses. With accurate metric depth estimation, we utilize a dense visual reconstruction method to recover the 3D structure of the whole surgical site. We have extensively evaluated the proposed framework on public SCARED and achieved comparable performance with stereo-based depth estimation methods. Our results demon-strate the feasibility of the proposed approach to recover the metric depth and 3D structure with monocular inputs.},
  archive   = {C_IROS},
  author    = {Ruofeng Wei and Bin Li and Hangjie Mo and Fangxun Zhong and Yonghao Long and Qi Dou and Yun-Hui Liu and Dong Sun},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982145},
  pages     = {8072-8077},
  title     = {Distilled visual and robot kinematics embeddings for metric depth estimation in monocular scene reconstruction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accurate pose estimation for comanipulation robotic surgery.
<em>IROS</em>, 8064–8071. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic comanipulation provides a cost-effective solution to telesurgery when remote operation is not strictly necessary. Within the field of laparoscopic surgery, the comanip-ulation scenario is only recently being exploited commercially in the form of lightweight backdrivable systems. A passive wrist backdrivable robot does not require preoperative alignment with the incision that acts as a fulcrum around which the laparoscopic instrument pivots. Moreover, backdrivable systems can be comanipulated by the user without the need for expensive force sensors. Unfortunately, most backdrivable systems only provide limited accuracy when measuring the end effector pose from their joint encoders. Accurate knowledge of the end effector pose is required to estimate the the instrument tip and fulcrum position. This work presents a robust method to improve localisation of the pose of the end effector of a backdrivable robot. The method fuses optical tracking with robot proprioception by means of an unscented Kalman filter and is robust against intermittent occlusions of the line of sight. The algorithm is experimentally validated by analyzing its initialization behavior and accuracy when estimating the instrument tip and fulcrum position. An accuracy of $1.58\pm 0.157$ mm and $0.699\pm 0.389$ mm is achieved when estimating the instrument tip and fulcrum position respectively, which makes the algorithm suitable for advanced guidance schemes in comanipulation robotic surgery.},
  archive   = {C_IROS},
  author    = {Jef De Smet and Gianni Borghesan and Emmanuel Vander Poorten},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981944},
  pages     = {8064-8071},
  title     = {Accurate pose estimation for comanipulation robotic surgery},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Localization of interaction using fibre-optic shape sensing
in soft-robotic surgery tools. <em>IROS</em>, 8057–8063. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Minimally invasive surgery requires real-time tool tracking to guide the surgeon where depth perception and visual occlusion present navigational challenges. Although vision-based and external sensor-based tracking methods exist, fibre-optic sensing can overcome their limitations as they can be integrated directly into the device, are biocompatible, small, robust and geometrically versatile. In this paper, we integrate a fibre Bragg grating-based shape sensor into a soft robotic device. The soft robot is the pneumatically attachable flexible (PAF) rail designed to act as a soft interface between manipulation tools and intra-operative imaging devices. We demonstrate that the shape sensing fibre can detect the location of the tools paired with the PAF rail, by exploiting the change in curvature sensed by the fibre when a strain is applied to it. We then validate this with a series of grasping tasks and continuous US swipes, using the system to detect in real-time the location of the tools interacting with the PAF rail. The overall location-sensing accuracy of the system is 64.6\%, with a margin of error between predicted location and actual location of 3.75 mm.},
  archive   = {C_IROS},
  author    = {Solène Dietsch and Aoife McDonald–Bowyer and Emmanouil Dimitrakakis and Joanna M. Coote and Lukas Lindenroth and Agostino Stilli and Danail Stoyanov},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981765},
  pages     = {8057-8063},
  title     = {Localization of interaction using fibre-optic shape sensing in soft-robotic surgery tools},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic keyframe detection for critical actions from the
experience of expert surgeons. <em>IROS</em>, 8049–8056. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot-Assisted Minimally Invasive Surgery (RAMIS), which introduced robot-actuated invasive tools to increase the dexterity and efficiency of traditional MIS, has become popular. Investigations on how to achieve autonomy in RAMIS have drawn vast intention recently, which urges further insights into the process of the surgical procedures. In this paper, the definition of critical actions, which discriminates the essential stages from regular surgical actions, is proposed to help decompose the complicated surgical processes. A critical intra-operative moment of the surgical workflow, which is called the keyframe, is introduced to indicate the beginning or ending moments of the critical actions. A keyframe detection method is proposed for critical action identification based on a new in-vivo dataset labeled by expert surgeons. Surgeons&#39; criteria for critical actions are captured by the explainable features, which can be extracted from the raw laparoscopic images with a two-stage network. Motivated by the surgeon&#39;s decision process of keyframes, a hierarchical structure is designed for keyframe identification by checking the spatial-temporal characteristics of the explainable features. Experimental results show that the reliability of the proposed method for keyframe detection achieves unanimous agreement by expert surgeons.},
  archive   = {C_IROS},
  author    = {Jie Zhang and Shenchao Shi and Yiwei Wang and Chidan Wan and Huan Zhao and Xiong Cai and Han Ding},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981454},
  pages     = {8049-8056},
  title     = {Automatic keyframe detection for critical actions from the experience of expert surgeons},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Recognition and prediction of surgical gestures and
trajectories using transformer models in robot-assisted surgery.
<em>IROS</em>, 8017–8024. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Surgical activity recognition and prediction can help provide important context in many Robot-Assisted Surgery (RAS) applications, for example, surgical progress monitoring and estimation, surgical skill evaluation, and shared control strategies during teleoperation. Transformer models were first developed for Natural Language Processing (NLP) to model word sequences and soon the method gained popularity for general sequence modeling tasks. In this paper, we propose the novel use of a Transformer model for three tasks: gesture recognition, gesture prediction, and trajectory prediction during RAS. We modify the original Transformer architecture to be able to generate the current gesture sequence, future gesture sequence, and future trajectory sequence estimations using only the current kinematic data of the surgical robot end-effectors. We evaluate our proposed models on the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS) and use Leave-One-User-Out (LOUO) cross validation to ensure generalizability of our results. Our models achieve up to 89.3\% gesture recognition accuracy, 84.6\% gesture prediction accuracy (1 second ahead) and 2.71mm trajectory prediction error (1 second ahead). Our models are comparable to and able to outperform state-of-the-art methods while using only the kinematic data channel. This approach can enable near-real time surgical activity recognition and prediction.},
  archive   = {C_IROS},
  author    = {Chang Shi and Yi Zheng and Ann Majewicz Fey},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981611},
  pages     = {8017-8024},
  title     = {Recognition and prediction of surgical gestures and trajectories using transformer models in robot-assisted surgery},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Active SLAM in 3D deformable environments. <em>IROS</em>,
7952–7958. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper considers active SLAM problem for 3D deformable environments where the trajectory of the robot is planned to optimize the SLAM results. A planning strategy combining an efficient global planner with an accurate local planner is proposed to solve the problem. Simulation results under different scenarios have shown that the proposed active SLAM algorithm provides a good balance between accuracy and efficiency as compared to the local planner and the global planner. The MATLAB code of this first active SLAM algorithm for 3D deformable environments is made publicly available 4 .},
  archive   = {C_IROS},
  author    = {Mengya Xu and Liang Zhao and Shoudong Huang and Qi Hao},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982224},
  pages     = {7952-7958},
  title     = {Active SLAM in 3D deformable environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online extrinsic correction of multi-camera systems by
low-dimensional parameterization of physical deformation. <em>IROS</em>,
7944–7951. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose the online extrinsic correction method that effectively optimizes the extrinsic parameters of multi-camera systems used in visual SLAM. In the typical visual SLAM systems that use multi-camera settings, the intrinsic and extrinsic parameters of the cameras are calculated through offline calibration, which is used as the fixed constraints in online execution. However, the camera rig can be physically deformed by shock or vibration, and the deviation from the offline calibration parameters can adversely affect the accuracy of triangulation and pose estimation. Therefore, it is crucial to maintain the accurate calibration of the camera rigs continuously throughout the execution. The previous online calibration methods optimize the extrinsic camera parameters in a full degree of freedom(DoF) by minimizing the reprojection error, but the limited visual information available online may bias the resulting camera poses. From the observation that the cameras are mounted on a physical body and the patterns that the body can be deformed is restricted and not completely free, we propose to model the pattern of physical rig deformation by external forces in advance, and then use the pre-trained low-dimensional deformation model to robustly and accurately estimate the changed camera poses in real-time. The proposed method consists of two steps. First, the physical model of the camera system is constructed in a simulator and the actual deformations by various external disturbances are recorded, and the deformation patterns are modeled by a PCA algorithm to build a low-dimensional model. In online execution, the camera poses are updated by minimizing the reprojection errors of visual features within the pre-trained low-dimensional parameterization, instead of optimizing all camera poses independently. Through the experiments in synthetic environments, the proposed online extrinsic correction method shows that it produces more accurate and robust camera pose estimation results than the existing method even when inaccurate 3D-2D correspondences exist or 2D feature positions are noisy.},
  archive   = {C_IROS},
  author    = {Sangheon Yang and Jongwoo Lim},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981254},
  pages     = {7944-7951},
  title     = {Online extrinsic correction of multi-camera systems by low-dimensional parameterization of physical deformation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Keeping less is more: Point sparsification for visual SLAM.
<em>IROS</em>, 7936–7943. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When adapting Simultaneous Mapping and Localization (SLAM) to real-world applications, such as autonomous vehicles, drones, and augmented reality devices, its memory footprint and computing cost are the two main factors limiting the performance and the range of applications. In sparse feature based SLAM algorithms, one efficient way for this problem is to limit the map point size by selecting the points potentially useful for local and global bundle adjustment (BA). This study proposes an efficient graph optimization for sparsifying map points in such SLAM systems. Specifically, we formulate a maximum pose-visibility and maximum spatial diversity problem as a minimum-cost maximum-flow graph optimization problem. The proposed method works as an additional step in existing SLAM systems, so it can be used in both conventional or learning based SLAM systems. By extensive experimental evaluations we demonstrate the proposed method achieves even more accurate camera poses with approximately 1/3 of the map points and 1/2 of the computation.},
  archive   = {C_IROS},
  author    = {Yeonsoo Park and Soohyun Bae},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981694},
  pages     = {7936-7943},
  title     = {Keeping less is more: Point sparsification for visual SLAM},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hybrid primitive-based navigation planner for the
wheeled-legged robot CENTAURO. <em>IROS</em>, 7904–7911. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Wheeled-legged robots have the potential to navigate in cluttered and irregular scenarios by altering the locomotion modes to adapt to the terrain challenges and effectively reach targeted locations in unstructured spaces. To achieve this functionality, a hybrid locomotion planner is necessary. In this work we present a search-based planner, which explores a set of motion primitives and a 2.5D traversability map extracted from the environment to generate navigation plans for the hybrid mobility robot CENTAURO. The planner explores the map from the current robot position to the goal location requested by the user, considering the most appropriate composition and tuning of locomotion primitives to build up a feasible plan, which is then executed by the robot. The available primitives are prioritized and can be easily modified, added or removed through a configuration file. Our approach was evaluated both in simulation and on the real wheeled-legged robot CENTAURO, demonstrating traversing capabilities in cluttered environments with various obstacles.},
  archive   = {C_IROS},
  author    = {Alessio De Luca and Luca Muratore and Nikos G. Tsagarakis},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981188},
  pages     = {7904-7911},
  title     = {A hybrid primitive-based navigation planner for the wheeled-legged robot CENTAURO},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ω2: Optimal hierarchical planner for object search in large
environments via mobile manipulation. <em>IROS</em>, 7888–7895. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a hierarchical planning algorithm that efficiently computes an optimal plan for finding a target object in large environments where a robot must simultaneously consider both navigation and manipulation. One key challenge that arises from large domains is the substantial increase in search space complexity that stems from considering mobile manipulation actions and the increase in number of objects. We offer a hierarchical planning solution that effectively handles such large problems by decomposing the problem into a set of low-level intra-container planning problems and a high-level key place planning problem that utilizes the low-level plans. To plan optimally, we propose a novel admissible heuristic function that, unlike previous methods, accounts for both navigation and manipulation costs. We propose two algorithms: one based on standard A* that returns the optimal solution, and the other based on Anytime Repairing A* (ARA*) which can trade-off computation time and solution quality, and prove they are optimal even when we use hierarchy. We show our method outperforms existing algorithms in simulated domains involving up to 6 times more number of objects than previously handled.},
  archive   = {C_IROS},
  author    = {Yoonyoung Cho and Donghoon Shin and Beomjoon Kim},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981194},
  pages     = {7888-7895},
  title     = {Ω2: Optimal hierarchical planner for object search in large environments via mobile manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RCA: Ride comfort-aware visual navigation via
self-supervised learning. <em>IROS</em>, 7847–7852. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Under shared autonomy, wheelchair users expect vehicles to provide safe and comfortable rides while following users&#39; high-level navigation plans. To find such a path, vehicles negotiate with different terrains and assess their traversal difficulty. Most prior works model surroundings either through geometric representations or semantic classifications, which do not reflect perceived motion intensity and ride comfort in downstream navigation tasks. We propose to model ride comfort explicitly in traversability analysis using proprioceptive sensing. We develop a self-supervised learning framework to predict traversability costmap from first-person-view images by leveraging vehicle states as training signals. Our approach estimates how the vehicle would “feel” if traversing over based on terrain appearances. We then show our navigation system provides human-preferred ride comfort through robot experiments together with a human evaluation study. The project could be found at https://sites.google.com/view/rca-navigation.},
  archive   = {C_IROS},
  author    = {Xinjie Yao and Ji Zhang and Jean Oh},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981494},
  pages     = {7847-7852},
  title     = {RCA: Ride comfort-aware visual navigation via self-supervised learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A robust sidewalk navigation method for mobile robots based
on sparse semantic point cloud. <em>IROS</em>, 7841–7846. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Last-mile delivery robots are usually required to navigate on the sidewalk through a fixed route. The current solutions heavily rely on the image-based perception and GPS localization to successfully complete delivery tasks. However, it is prone to fail and become unreliable when the robot runs in challenging conditions, such as operating in different illuminations, or under canopies of trees or buildings. To address these issues, this paper proposes a novel robust sidewalk navigation method for the last-mile delivery robots with an affordable sparse LiDAR, which consists of two main modules: Semantic Point Cloud Network (SegPCn) and Reactive Nav-igation Network (RNn), as shown in Fig. 1. More specifically, SegPCn takes the raw 3D point cloud as input and predicts the point-wise segmentation labels, presenting a robust perception capability even in the night. Then, the semantic point clouds are fed to RNn to generate an angular velocity to navigate the robot along the sidewalk, where the localization of the robot is not required. Moreover, an autolabeling mechanism is developed to reduce the labor involved in data preparation as well. And the LSTM neural network is explored to effectively leverage the historical context and derive correct decisions. Extensive experiments have been carried out to verify the efficacy of this method, and the results show that this method enables the robot to navigate on the sidewalk robustly during day and night. We open source the code and the data set on https://github.com/lukewenMX/Robust-Navigation-Method.},
  archive   = {C_IROS},
  author    = {Mingxing Wen and Yunxiang Dai and Tairan Chen and Chunyang Zhao and Jun Zhang and Danwei Wang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981281},
  pages     = {7841-7846},
  title     = {A robust sidewalk navigation method for mobile robots based on sparse semantic point cloud},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SwitchHit: A probabilistic, complementarity-based switching
system for improved visual place recognition in changing environments.
<em>IROS</em>, 7833–7840. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual place recognition (VPR) - a fundamental task in computer vision and robotics - is the problem of identifying a place mainly based on visual information. View-point and appearance changes, such as due to weather and seasonal variations, make this task challenging. Currently, there is no universal VPR technique that can work in all types of environments, on a variety of robotic platforms, and under a wide range of viewpoint and appearance changes. Recent work has shown the potential of combining different VPR methods intelligently by evaluating complementarity for some specific VPR datasets to achieve better performance. This, however, requires ground truth information (correct matches) which is not available when a robot is deployed in a real-world scenario. Moreover, running multiple VPR techniques in parallel may be prohibitive for resource-constrained embedded platforms. To overcome these limitations, this paper presents a probabilistic complementarity-based switching VPR system, SwitchHit. Our proposed system consists of multiple VPR techniques, however, it does not simply run all techniques at once, rather predicts the probability of correct match for an incoming query image and dynamically switches to another complementary technique if the probability of correctly matching the query is below a certain threshold. This innovative use of multiple VPR techniques allow our system to be more efficient and robust than other combined VPR approaches employing brute force and running multiple VPR techniques at once. Thus making it more suitable for resource constrained embedded systems and achieving an overall superior performance from what any individual VPR method in the system could have by achieved running independently.},
  archive   = {C_IROS},
  author    = {Maria Waheed and Michael Milford and Klaus McDonald-Maier and Shoaib Ehsan},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981722},
  pages     = {7833-7840},
  title     = {SwitchHit: A probabilistic, complementarity-based switching system for improved visual place recognition in changing environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Comparing human haptic perception and robotic force/torque
sensing in a simulated surgical palpation task. <em>IROS</em>,
7825–7832. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In minimally invasive surgery (MIS), the reliable detection of hard inclusions in soft tissue is crucial for the success of the intervention. In robot-assisted surgery (RAS) however, limited technologies are available for intracorporeal tissue stiffness assessment due to the lack of force and tactile feedback from the robot tool tip. This paper investigates both, human haptic perception and robotic F/T sensing in similar experimental setups to draw conclusions about the usage of a haptic sensor for teleoperation in RAS. We use a novel 6-axis F/T sensor compact enough to be moved through trocars during RAS interventions and experimentally analyze its performance in a simulated robotic palpation task. Furthermore, we carry out a comprehensive user study $(n=30)$ and collect fingertip interaction data to investigate human haptic perception. Results show, that both approaches detect larger bead diameters of 19 mm and 15 mm with high precision and show similar accuracy rates. With regards to interaction forces, subjects on average apply more than 10 times the amount of normal force $(F_{z}=28.8\pm 4.9\mathrm{N})$ , which leads to higher accuracy particularly for smaller embedded nodules. The robotic sensing technique, on the contrary, offers distinct advantages by providing more gentle treatments and reducing the risk of tissue damage.},
  archive   = {C_IROS},
  author    = {Timo Markert and Sebastian Matich and Elias Hoerner and Jonas Pfannes and Andreas Theissler and Martin Atzmueller},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981495},
  pages     = {7825-7832},
  title     = {Comparing human haptic perception and robotic Force/Torque sensing in a simulated surgical palpation task},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Whisker-inspired tactile sensing for contact localization on
robot manipulators. <em>IROS</em>, 7817–7824. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Perceiving the environment through touch is important for robots to reach in cluttered environments, but devising a way to sense without disturbing objects is challenging. This work presents the design and modelling of whisker-inspired sensors that attach to the surface of a robot manipulator to sense its surrounding through light contacts. We obtain a sensor model using a calibration process that applies to straight and curved whiskers. We then propose a sensing algorithm using Bayesian filtering to localize contact points. The algorithm combines the accurate proprioceptive sensing of the robot and sensor readings from the deflections of the whiskers. Our results show that our algorithm is able to track contact points with sub-millimeter accuracy, outperforming a baseline method. Finally, we demonstrate our sensor and perception method in a real-world system where a robot moves in between free-standing objects and uses the whisker sensors to track contacts tracing object contours.},
  archive   = {C_IROS},
  author    = {Michael A. Lin and Emilio Reyes and Jeannette Bohg and Mark R. Cutkosky},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982122},
  pages     = {7817-7824},
  title     = {Whisker-inspired tactile sensing for contact localization on robot manipulators},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Grasp stability prediction with sim-to-real transfer from
tactile sensing. <em>IROS</em>, 7809–7816. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot simulation has been an essential tool for data-driven manipulation tasks. However, most existing simulation frameworks lack either efficient and accurate models of physical interactions with tactile sensors or realistic tactile simulation. This makes the sim-to-real transfer for tactile-based manipulation tasks still challenging. In this work, we integrate simulation of robot dynamics and vision-based tactile sensors by modeling the physics of contact. This contact model uses simulated contact forces at the robot&#39;s end-effector to inform the generation of realistic tactile outputs. To eliminate the sim-to-real transfer gap, we calibrate our physics simulator of robot dynamics, contact model, and tactile optical simulator with real-world data, and then we demonstrate the effectiveness of our system on a zero-shot sim-to-real grasp stability prediction task where we achieve an average accuracy of 90.7\% on various objects. Experiments reveal the potential of applying our simulation framework to more complicated manipulation tasks. We open-source our simulation framework at https://github.com/CMURoboTouch/Taxim/tree/taxim-robot.},
  archive   = {C_IROS},
  author    = {Zilin Si and Zirui Zhu and Arpit Agarwal and Stuart Anderson and Wenzhen Yuan},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981863},
  pages     = {7809-7816},
  title     = {Grasp stability prediction with sim-to-real transfer from tactile sensing},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A deep-learning-based system for indoor active cleaning.
<em>IROS</em>, 7803–7808. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cleaning public areas like commercial complexes is challenging due to their sophisticated surroundings and the vast kinds of real-life dirt. Robots are required to distinguish dirts and apply corresponding cleaning strategies. In this work, we proposed an active-cleaning framework by utilizing deep-learning methods for both solid wastes detection and liquid stains segmentation. Our system consists of 4 components: a Perception module integrated with deep-learning models, a Post-processing module for projection, a Tracking module for map localization, and a Planning and Control module for cleaning strategies. Compared with classic approaches, our vision-based system significantly improves cleaning efficiency. Besides, we released the largest real-world indoor hybrid dirt cleaning dataset (HD10K) containing 10K labeled images, together with a track-level evaluation metric for better cleaning performance measurement. The proposed deep-learning based system is verified with extensive experiments on our dataset, and deployed to Gaussian Robotics&#39;s robots operating globally. Dataset is available at: https://gaussianopensource.github.io/projects/active_cleaning.},
  archive   = {C_IROS},
  author    = {Yike Yun and Linjie Hou and Zijian Feng and Wei Jin and Yang Liu and Heng Wang and Ruonan He and Weitao Guo and Bo Han and Baoxing Qin and Jiaxin Li},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982137},
  pages     = {7803-7808},
  title     = {A deep-learning-based system for indoor active cleaning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Soft tactile contour following for robot-assisted wiping and
bathing. <em>IROS</em>, 7797–7802. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The automated cleaning of surfaces such as furniture, bathroom sinks, and even human bodies is challenging due to the three-dimensional nature of their geometries. Yet, enabling robots to effectively and safely perform these tasks would not only reduce user efforts spent on household cleaning chores, but would also alleviate the strenuous workload of caretakers as the elderly population continues to grow at an unprecedented rate. In this work, we unify the applications of wiping objects and bathing humans as a general contour-following problem. To this end, we utilize a depth camera-based soft tactile sensor to extract the contact geometries and force-correlated measures during interaction between the robot and the target object or body part, and design a general contour-following controller that not only maintains contact with the target throughout the cleaning process, but also regulates the amount of force applied. Our system enables successful cleaning of pipes, shelving, and even human limbs and torsos without the need for data-driven methods such as deep learning, upon which the majority of existing works have relied.},
  archive   = {C_IROS},
  author    = {Isabella Huang and Dylan Chow and Ruzena Bajcsy},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982071},
  pages     = {7797-7802},
  title     = {Soft tactile contour following for robot-assisted wiping and bathing},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-fingered tactile servoing for grasping adjustment
under partial observation. <em>IROS</em>, 7781–7788. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Grasping of objects using multi-fingered robotic hands often fails due to small uncertainties in the hand motion control and the object&#39;s pose estimation. To tackle this problem, we propose a grasping adjustment strategy based on tactile seroving. Our technique employs feedback from a sensorized multi-fingered robotic hand to collaboratively servo the fingers and palm to achieve the desired grasp. We demonstrate the performance of our method through simulation and physical experiments by having a robot grasp different objects under conditions of variable uncertainty. The results show that our approach achieved a higher success rate and tolerated greater uncertainty than an open-looped grasp.},
  archive   = {C_IROS},
  author    = {Hanzhong Liu and Bidan Huang and Qiang Li and Yu Zheng and Yonggen Ling and Wangwei Lee and Yi Liu and Ya-Yen Tsai and Chenguang Yang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981464},
  pages     = {7781-7788},
  title     = {Multi-fingered tactile servoing for grasping adjustment under partial observation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to singulate layers of cloth using tactile
feedback. <em>IROS</em>, 7773–7780. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic manipulation of cloth has applications ranging from fabrics manufacturing to handling blankets and laundry. Cloth manipulation is challenging for robots largely due to their high degrees of freedom, complex dynamics, and severe self-occlusions when in folded or crumpled configurations. Prior work on robotic manipulation of cloth relies primarily on vision sensors alone, which may pose challenges for fine-grained manipulation tasks such as grasping a desired number of cloth layers from a stack of cloth. In this paper, we propose to use tactile sensing for cloth manipulation; we attach a tactile sensor (ReSkin) to one of the two fingertips of a Franka robot and train a classifier to determine whether the robot is grasping a specific number of cloth layers. During test-time experiments, the robot uses this classifier as part of its policy to grasp one or two cloth layers using tactile feedback to determine suitable grasping points. Experimental results over 180 physical trials suggest that the proposed method outperforms baselines that do not use tactile feedback and has better generalization to unseen cloth compared to methods that use image classifiers. Code, data, and videos are available at https://sites.google.com/view/reskin-cloth.},
  archive   = {C_IROS},
  author    = {Sashank Tirumala and Thomas Weng and Daniel Seita and Oliver Kroemer and Zeynep Temel and David Held},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981341},
  pages     = {7773-7780},
  title     = {Learning to singulate layers of cloth using tactile feedback},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The role of tactile sensing in learning and deploying grasp
refinement algorithms. <em>IROS</em>, 7766–7772. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A long-standing question in robot hand design is how accurate tactile sensing must be. This paper uses simulated tactile signals and the reinforcement learning (RL) framework to study the sensing needs in grasping systems. Our first experiment investigates the need for rich tactile sensing in the rewards of RL-based grasp refinement algorithms for multi-fingered robotic hands. We systematically integrate different levels of tactile data into the rewards using analytic grasp stability metrics. We find that combining information on contact positions, normals, and forces in the reward yields the highest average success rates of 95.4\% for cuboids, 93.1\% for cylinders, and 62.3\% for spheres across wrist position errors between 0 and 7 centimeters and rotational errors between 0 and 14 degrees. This contact-based reward outperforms a non-tactile binary-reward baseline by 42.9\%. Our follow-up experiment shows that when training with tactile-enabled rewards, the use of tactile information in the control policy&#39;s state vector is drastically reducible at only a slight performance decrease of at most 6.6\% for no tactile sensing in the state. Since policies do not require access to the reward signal at test time, our work implies that models trained on tactile-enabled hands are deployable to robotic hands with a smaller sensor suite, potentially reducing cost dramatically.},
  archive   = {C_IROS},
  author    = {Alexander Koenig and Zixi Liu and Lucas Janson and Robert Howe},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981915},
  pages     = {7766-7772},
  title     = {The role of tactile sensing in learning and deploying grasp refinement algorithms},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal joint TDPA formulation for kinematically redundant
robot manipulators. <em>IROS</em>, 7742–7749. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The accomplishment of a successful teleoperation task requires guaranteeing system stability and transparency. Communication delay (in particular variable time delay), quantization and discretization negatively affect system stability and might be overcome with Time Domain Passivity Approach (TDPA), a model-free and robust way to cope with energy injection due to communication delay. However, this method degrades the transparency of the teleoperation system and worsens tracking performance, introducing in particular position drift error at the slave side and high frequency vibration (jittering) at the master side. In this work, we propose a new joint passivity controller formulation for kinematically redundant manipulators. Our approach stabilizes the system guaranteeing minimal performance loss by privileging the dissipation of the observed energy in the Jacobian null-space. The residual energy (if any) is dissipated in an orthogonal subspace. This is achieved by the solution of an optimization problem with appropriately defined cost functions and constrained to dissipate the energy observed by the passivity observer, guaranteeing the stability of the system. The effectiveness of our algorithm is tested in simulation with both constant and variable time delays.},
  archive   = {C_IROS},
  author    = {Francesco Porcini and Massimiliano Solazzi and Antonio Frisoli},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981371},
  pages     = {7742-7749},
  title     = {Optimal joint TDPA formulation for kinematically redundant robot manipulators},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-phase multi-modal haptic teleoperation. <em>IROS</em>,
7734–7741. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Virtual Fixtures facilitate teleoperation, for in-stance by guiding the human operator. Developing these Virtual Fixtures in tasks with tight tolerances remains challenging. Fixtures with a high stiffness allow for more precise guidance, whereas a lower stiffness is required to allow for corrections. We observed that many assembly operations can be split into different phases - approaching, positioning, in-contact manipulation - each with different accuracy requirements. Therefore, we propose to use multi-modal fixtures, satisfying the different requirements of these phases: i.e. a position-based Trajectory Fixture for approaching and a more accurate Visual Servoing Fixture for the positioning phase. A state estimation and arbitration component ensures smooth transitions between the fixtures to provide optimal support for the operator and to achieve global availability paired with local precision at the same time. It also allows a high stiffness to be used throughout, thus achieving good guidance for all phases. The approach is validated in an application from a space scenario, consisting of the assembly of a CubeSat subsystem. The empirical results from a pilot study on this task show that our approach is faster and requires less interaction force from the operator than the baseline method.},
  archive   = {C_IROS},
  author    = {Maximilian Mühlbauer and Franz Steinmetz and Freek Stulp and Thomas Hulin and Alin Albu-Schäffer},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981525},
  pages     = {7734-7741},
  title     = {Multi-phase multi-modal haptic teleoperation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards robot avatars: Systems and methods for
teleinteraction at avatar XPRIZE semi-finals. <em>IROS</em>, 7726–7733.
(<a href="https://doi.org/10.1109/IROS47612.2022.9982258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There has been a drastic shift to remote interaction for professional, industrial and personal interactions. Improving the overall quality of these interactions by removing any sense of distance between the users is the ultimate goal. Video conferencing has been widely adopted as an improvement to audio-only interactions. Having added visuals to audio communication, the next frontier is to add physical interaction to this remote communication. In this paper, we present an avatar system with the aim of tackling these necessities. The proposed system includes both hardware and software designs to ensure a real-time telemanipulation experience with tactile force feedback. We present a coupled hydrostatic actuated gripper and glove with high system bandwidth to reduce the inherent latency of the mechanical system. To account for latency over the network, the wave variable based method is adopted to maintain the stability of the closed-loop gripper control even under hundreds of milliseconds of delay. A bidirectional audiovisual communication system comprised of off-the-shelf hardware and software is incorporated to allow realtime conversation between the operator and the recipient for collaborative tasks. the proposed system has been validated in lab experiments and the global ana avatar xprize challenge semifinal.},
  archive   = {C_IROS},
  author    = {Rui Luo and Chunpeng Wang and Eric Schwarm and Colin Keil and Evelyn Mendoza and Pushyami Kaveti and Stephen Alt and Hanumant Singh and TaŞkin Padir and John Peter Whitney},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982258},
  pages     = {7726-7733},
  title     = {Towards robot avatars: Systems and methods for teleinteraction at avatar XPRIZE semi-finals},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On stabilizing communication law for bilateral
force-reflecting teleoperation systems. <em>IROS</em>, 7720–7725. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This brief proposes a new stabilizing communication law to allow the wave transformation-based teleoperation architecture to accommodate direct environmental contact force feedback, potentially increasing the human operator&#39;s experience of telepresence. Simulation results are provided.},
  archive   = {C_IROS},
  author    = {Ho Duc Tho and Takanori Miyoshi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981557},
  pages     = {7720-7725},
  title     = {On stabilizing communication law for bilateral force-reflecting teleoperation systems},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the communication channel in bilateral teleoperation: An
experimental study for ethernet, WiFi, LTE and 5G. <em>IROS</em>,
7712–7719. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Teleoperated robots are believed to play an important role for future applications in industry, medicine and other domains. Examples for this are remote assembly and maintenance, surgery, diagnosis or deep-sea and space exploration. Such applications are made possible by state-of-the-art tactile manipulators, well-researched control schemes and novel communication technologies such as the fifth generation of mobile communication (5G). The achievable performance is highly dependent on the communication delay and thus on the distance between leader and follower station, as well as the potentially used wireless protocol. Specially in this regard, 5G is a promising technology compared to the other communication protocols for transferring tactile information. In this paper, we introduce our telepresence reference platform, which can be used for empirical evaluation of different algorithms and communications. Comparative analysis are conducted to capture the influence of wireless communication protocols on telepresence systems consisting of complex robotic arms. The experiment compares the influence of 5G, LTE and WiFi communication protocols with regard to the motion and force tracking performance of the system.},
  archive   = {C_IROS},
  author    = {Xiao Chen and Lars Johannsmeier and Hamid Sadeghian and Erfan Shahriari and Martin Danneberg and Anselm Nicklas and Fan Wu and Gerhard Fettweis and Sami Haddadin},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981833},
  pages     = {7712-7719},
  title     = {On the communication channel in bilateral teleoperation: An experimental study for ethernet, WiFi, LTE and 5G},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A method for automated drone viewpoints to support remote
robot manipulation. <em>IROS</em>, 7704–7711. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Drones can provide a minimally-constrained adapting camera view to support robot telemanipulation. Furthermore, the drone view can be automated to reduce the burden on the operator during teleoperation. However, existing approaches do not focus on two important aspects of using a drone as an automated view provider. The first is how the drone should select from a range of quality viewpoints within the workspace (e.g., opposite sides of an object). The second is how to compensate for unavoidable drone pose uncertainty in determining the viewpoint. In this paper, we provide a nonlinear optimization method that yields effective and adaptive drone viewpoints for telemanipulation with an articulated manipulator. Our first key idea is to use sparse human-in-the-loop input to toggle between multiple automatically-generated drone viewpoints. Our second key idea is to introduce optimization objectives that maintain a view of the manipulator while considering drone uncertainty and the impact on viewpoint occlusion and environment collisions. We provide an instantiation of our drone viewpoint method within a drone-manipulator remote teleoperation system. Finally, we provide an initial validation of our method in tasks where we complete common household and industrial manipulations.},
  archive   = {C_IROS},
  author    = {Emmanuel Senft and Michael Hagenow and Pragathi Praveena and Robert Radwin and Michael Zinn and Michael Gleicher and Bilge Mutlu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982063},
  pages     = {7704-7711},
  title     = {A method for automated drone viewpoints to support remote robot manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HI-DWA: Human-influenced dynamic window approach for shared
control of a telepresence robot. <em>IROS</em>, 7696–7703. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper considers the problem of enabling the user to modify the path of a telepresence robot. The robot is capable of autonomously navigating to a goal predefined by the user, but the user might still want to modify the path, for example, to go further away from other people, or to go closer to landmarks she wants to see on the way. We propose Human-Influenced Dynamic Window Approach (HI-DWA), a shared control method aimed for telepresence robots based on Dynamic Window Approach (DWA) that allows the user to influence the control input given to the robot. To verify the proposed method, we performed a user study (N=32) in Virtual Reality (VR) to compare HI-DWA with switching between autonomous navigation and manual control for controlling a simulated telepresence robot moving in a virtual environment. Results showed that users reached their goal faster using HI-DWA controller and found it easier to use. Preference between the two methods was split equally. Qualitative analysis revealed that a major reason for the participants that preferred switching between two modes was the feeling of control. We also analyzed the effect of different input methods, joystick and gesture, on the preference and perceived workload.},
  archive   = {C_IROS},
  author    = {Juho Kalliokoski and Basak Sakcak and Markku Suomalainen and Katherine J. Mimnaugh and Alexis P. Chambers and Timo Ojala and Steven M. LaValle},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981367},
  pages     = {7696-7703},
  title     = {HI-DWA: Human-influenced dynamic window approach for shared control of a telepresence robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-level task learning based on intention and constraint
inference for autonomous robotic manipulation. <em>IROS</em>, 7688–7695.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To perform tasks in unstructured environments, robots need to be able to apply learned skills to different contexts and to autonomously make decisions online. We, therefore, developed a novel data-driven task learning approach that segments a task demonstration into simpler skills and structures them in a high-level task graph. In contrast to other state-of-the-art methods, the presented approach can not only infer the low-level skills and their respective subgoals but also multimodal feature constraints fitted individually to each skill. The inferred feature constraints allow to detect anomalies during autonomous task execution, which can be automatically resolved by a recovery behavior of the task graph. The subgoals encode each skill&#39;s intention and thereby enable to flexibly transition between skills and to generalize the behavior to new setups. By separating the subgoal and constraint inference, we achieve a reduced computational complexity and an increased performance compared to state-of-the-art task learning approaches. In a real-world manipulation task, we demonstrate the reusability of skills as well as the autonomous decision-making of our approach.},
  archive   = {C_IROS},
  author    = {Christoph Willibald and Dongheui Lee},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981288},
  pages     = {7688-7695},
  title     = {Multi-level task learning based on intention and constraint inference for autonomous robotic manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning implicit priors for motion optimization.
<em>IROS</em>, 7672–7679. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion optimization is an effective framework for generating smooth and safe trajectories for robotic manipulation tasks. However, it suffers from local optima that hinder its applicability, especially for multi-objective tasks. In this paper, we study this problem in light of the integration of Energy-Based Models (EBM) as guiding priors in motion optimization. EBMs are probabilistic models with unnormalized energy functions that represent expressive multimodal distributions. Due to their implicit nature, EBMs can easily be integrated as data-driven factors or initial sampling distributions in the motion optimization problem. This work presents a set of necessary modeling and algorithmic choices to effectively learn and integrate EBMs into motion optimization. We present a set of EBM architectures for learning generalizable distributions over trajectories that are important for the subsequent deployment of EBMs. Moreover, we investigate the benefit of including smoothness regularization in the learning process to improve motion optimization. In addition to gradient-based solvers, we also propose a stochastic method for trajectory optimization with learned EBMs. We provide extensive empirical results in a set of representative tasks against competitive baselines that demonstrate the superiority of EBMs as priors in motion optimization scaling up to 7 -dof robot pouring that can be easily transferred to the real robotic system. Videos and additional details are available at https://sites.google.com/view/implicit-priors.},
  archive   = {C_IROS},
  author    = {Julen Urain and An T. Le and Alexander Lambert and Georgia Chalvatzaki and Byron Boots and Jan Peters},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981264},
  pages     = {7672-7679},
  title     = {Learning implicit priors for motion optimization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning temporal task models from human bimanual
demonstrations. <em>IROS</em>, 7664–7671. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning temporal relations between actions in a bimanual manipulation task is important for capturing the constraints of actions required to achieve the task&#39;s goal. However, given several demonstrations of a bimanual manipulation task, the problem of identifying the true temporal dependencies between actions - if there are any - is very challenging due to contradictions. We propose a model-driven approach for learning temporal task models from multiple bimanual human demonstrations that represents temporal relations on two levels. First, temporal relations between sets of actions that exhibit a tight temporal coupling, and second, temporal relations between these sets of actions. We build on Allen&#39;s interval algebra as a representation to express relations between temporal intervals. Semantically defining these interval relations allows us to soften their formulation to deal with inaccuracies in real data obtained when observing humans demonstrating the task. Our temporal task models can be learned incrementally from multiple modalities, and allow us to reason about viable alternatives during task execution in case of unexpected events. We evaluated the approach quantitatively on two datasets and qualitatively on a humanoid robot. The evaluation shows how inherent properties of bimanual human manipulation tasks can be exploited to derive a model useful for the reproduction by humanoid robots.},
  archive   = {C_IROS},
  author    = {Christian R.G. Dreher and Tamim Asfour},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981068},
  pages     = {7664-7671},
  title     = {Learning temporal task models from human bimanual demonstrations},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A framework for transferring surface finishing skills to new
surface geometries. <em>IROS</em>, 7650–7655. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a framework for transferring surface finishing skills to new surface geometries while preserving the surface finish quality. The main idea is to estimate the contact area between the workpiece and the tool by using 3D point cloud approach and replicate a given material removal rate and the accumulated material removal, as these quantities are the main parameters for quality. The grinding motion trajectory is generated by solving a constrained optimization problem that minimizes the maximal point-wise deviation between actual and desired material removal and simultaneously minimizes the average deviation between actual and desired material removal rate. The proposed approach is verified in simulation to show the difference between direct replication of force/motion and the proposed replication of material removal. Finally, experimental results confirm that the quality of a surface finishing task can be transferred to new surface geometries with the proposed method.},
  archive   = {C_IROS},
  author    = {Yitaek Kim and Christoffer Sloth and Aljaz Kramberger},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981163},
  pages     = {7650-7655},
  title     = {A framework for transferring surface finishing skills to new surface geometries},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning turn-taking behavior from human demonstrations for
social human-robot interactions. <em>IROS</em>, 7643–7649. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Turn-taking is a fundamental behavior during human interactions and robots must be capable of turn-taking to interact with humans. Current state-of-the-art approaches in turn-taking focus on developing general models to predict the end of turn (EoT) across all contexts. This demands an all-inclusive verbal and non-verbal behavioral dataset from all possible contexts of interaction. Before robot deployment, gathering such a dataset may be infeasible and/or impractical. More importantly, a robot needs to predict the EoT and decide on the best time to take a turn (i.e, start speaking). In this research, we present a learning from demonstration (LfD) system for a robot to learn from demonstrations, after it has been deployed, to make decisions on the appropriate time for taking a turn within specific social interaction contexts. The system captures demonstrations of turn-taking during social interactions and uses these demonstrations to train a LSTM RNN based model to replicate the turn-taking behavior of the demonstrator. We evaluate the system for teaching the turn-taking behavior of an interviewer during a job interview context. Furthermore, we investigate the efficacy of verbal, prosodic, and gestural cues for deciding when to begin a turn.},
  archive   = {C_IROS},
  author    = {Pourya Shahverdi and Alexander Tyshka and Madeline Trombly and Wing-Yue Geoffrey Louie},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981243},
  pages     = {7643-7649},
  title     = {Learning turn-taking behavior from human demonstrations for social human-robot interactions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Risk-sensitive MPCs with deep distributional inverse RL for
autonomous driving. <em>IROS</em>, 7635–7642. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In robot learning from demonstration (LfD), a visual representation of a cost function inferred from Inverse Reinforcement Learning (IRL) provides an intuitive tool for humans to quickly interpret the underlying objectives of the demonstration. The inferred cost function can be used by controllers, for example, Model Predictive Controllers (MPCs). In this work, we improve the recently developed IRL-MPC framework, by enhancing it in a risk-sensitive formulation to be more applicable for safety-critical applications like autonomous driving. Our risk-sensitive MPCs together with the distributional costmap demonstrate lower collision rates in the CARLA simulator for autonomous driving tasks compared to other learning-based baseline methods.},
  archive   = {C_IROS},
  author    = {Keuntaek Lee and David Isele and Evangelos A. Theodorou and Sangjae Bae},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981223},
  pages     = {7635-7642},
  title     = {Risk-sensitive MPCs with deep distributional inverse RL for autonomous driving},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stabilization of tangent and normal contact forces for a
quadrotor subject to disturbances. <em>IROS</em>, 7613–7618. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Force exertion, object manipulation, and interaction are novel trending research topics of autonomous flying robots that can yield hoovering. Moreover, specifically with quadrotors, the vibration caused by the high natural frequency of rotating propellers exacerbates the problem of maintaining contact and exerting force against a rigidly fixed object. This contact vibration transfers back kinetic energy to the quadrotor that, in worst-case scenarios, surpasses its flying capabilities, which may lead to a crash. This paper studies the problem of aerial contact stabilization of a quadrotor equipped with a hemispherical deformable tip, which accommodates contact forces at a lower frequency. Thus two phenomena not studied in the literature arise: the rolling motion, and the deformation at contact. The contact force stabilization restores the effects of deformation while simultaneously endowing rolling by controlling a tangent constrained force. A model-free continuous attitude fractional controller to guarantee finite-time attitude stabilization is proposed. The residual coupled nonlinear dynamics yields the desired attitude corresponding to a given contact force; thus, force stabilization is achieved. Finally, experimental results are presented to assess the performance of the proposed approach.},
  archive   = {C_IROS},
  author    = {C. Izaguirre-Espinosa and A. Muñoz-Vazquez and A. Sánchez-Orta and V. Parra-Vega and R. Garcia-Rodriguez and P. Castillo and D. Arreguín-Jasso},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981890},
  pages     = {7613-7618},
  title     = {Stabilization of tangent and normal contact forces for a quadrotor subject to disturbances},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonlinear model predictive control for human-robot handover
with application to the aerial case. <em>IROS</em>, 7597–7604. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this article, we consider the problem of delivering an object to a human coworker by means of an aerial robot (AR). To this aim, we present an ergonomics-aware Nonlinear Model Predictive Control (NMPC) designed to autonomously perform the handover. The method is general enough to be applied to any multi-rotor aerial vehicle (MRAV) with a minimal adaptation of the robot model. The formulation of the optimal control problem steers the AR toward a handover location by optimizing the human coworker ergonomics, which includes the predicted arm joint torques of the human. The motion task is expressed in a frame relative to the human, whose motion model is included in the equations of the NMPC. This allows the controller to promptly adapt to the human movements by predicting her future poses over the horizon. The control framework also accounts for the problem of maintaining visibility on the human coworker, while respecting both the actuation and state limits of the robot. Additionally, a safety barrier is embedded in the controller to avoid any risk of collision with the human partner. Realistic simulations are performed to validate the feasibility of the approach and the source code of the implementation is released open-source.},
  archive   = {C_IROS},
  author    = {Gianluca Corsini and Martin Jacquet and Hemjyoti Das and Amr Afifi and Daniel Sidobre and Antonio Franchi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981045},
  pages     = {7597-7604},
  title     = {Nonlinear model predictive control for human-robot handover with application to the aerial case},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A robotic aerial platform with functionally anthropomorphic
arms designed for physical interaction. <em>IROS</em>, 7581–7588. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Frequently, ground robots are hampered by debris and objects on the ground, and safely surpassing them is not always trivial. On the contrary, a robot capable of flying is intrinsically immune to such obstacles and, therefore, greatly enhances the possibility of inspecting and intervening in adverse surroundings for humans. This work introduces a novel teleoperated aerial platform for inspection and intervention in unstructured environments. The robot is composed of an aerial base, two arms, and a two-degrees-of-freedom head that consent the access of human operators in any workplace in total safety. The arms are designed with a joint structure of tendons and are held by elastic components. This composition considerably improves the robustness by inserting softness and redistributing the weights to lessen the actions on the drone. Moreover, the aerial platform employs two soft hands capable of adapting to the shape of the objects under grasp, increasing the manipulation performance. We presented the mechanical and control design, a gazebo simulation employed to test the controllers, and a physical structure for the experimental validation of the system. The system is available as Open-Source material.},
  archive   = {C_IROS},
  author    = {Fanyi Kong and Simone Monteleone and Giorgio Grioli and Manuel G. Catalano and Antonio Bicchi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981910},
  pages     = {7581-7588},
  title     = {A robotic aerial platform with functionally anthropomorphic arms designed for physical interaction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hoverability analysis and development of a quadrotor only
with clockwise rotors. <em>IROS</em>, 7558–7564. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents novel quadrotor structures composed of only clockwise rotors. A multirotor unmanned aerial vehicle (UAV) generally has both clockwise and counterclockwise rotors to counteract the torques from the rotors. While the proposed structures have only clockwise rotors, those rotors are tilted to cancel the torques around the yaw angle of the body. This paper investigates the conditions for the proposed structures to achieve static hovering. More specifically, we provide a guideline to design the rotor tilt angles of the proposed structure. Then, this paper presents the design example of the rotor tilt angles and develops a prototype of the proposed quadrotor. The cascaded controller is also developed for the proposed structure. Finally, experimental validation is conducted with a developed prototype and controller.},
  archive   = {C_IROS},
  author    = {Shusuke Mochida and Ryotaro Onuki and Takahiro Kawagoe and Takumi Ito and Tatsuya Ibuki and Riku Funada and Mitsuji Sampei},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981431},
  pages     = {7558-7564},
  title     = {Hoverability analysis and development of a quadrotor only with clockwise rotors},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effects of design and hydrodynamic parameters on optimized
swimming for simulated, fish-inspired robots. <em>IROS</em>, 7500–7506.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we developed a mathematical model and a simulation platform for a fish-inspired robotic template, namely Magnetic, Modular, Undulatory Robot $(\mu \text{Bot})$ . Through this platform, we systematically explored the effects of robot design and fluid parameters on swimming performance via reinforcement learning. The mathematical model was composed of two interacting subsystems, the robotic dynamic model and the hydrodynamic model. The hydrodynamic model consisted of the reactive components (added-mass force and pressure forces) and the resistive components (drag and friction forces). These components were nondimensionalized for deriving key “control parameters” of the robot-fluid interaction. The $\mu\text{Bots}$ were actuated via magnetic actuators controlled with harmonic voltage signals, which were optimized via EM-based Policy Hyper Parameter Exploration (EPHE) to maximize forward swimming speed. By varying the control parameters, a total of 36 cases with different robot template variations (Number of Actuators (NoA) and stiffness) and hydrodynamic parameters were simulated and optimized via EPHE. Results showed that the wavelength of the optimized gaits (i.e., backward traveling wave along the body) was independent of template variations and hydrodynamic parameters. Higher NoA yielded higher speed but lower speed per body length, suggesting a diminishing gain from added actuators. Body and caudal-fin dynamics were dominated by the interaction among fluid added-mass, spring, and actuation torque, with negligible contribution from fluid resistive drag. In contrast, thrust was dominated by the pressure force acting on the caudal fin, as steady swimming resulted from a balance between resistive force and pressure force, with minor contributions from added-mass force and body drag forces. Therefore, added-mass force only indirectly affected the thrust generation and forward swimming speed via the caudal fin dynamics.},
  archive   = {C_IROS},
  author    = {Donghao Li and Hankun Deng and Yagiz E. Bayiz and Bo Cheng},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981478},
  pages     = {7500-7506},
  title     = {Effects of design and hydrodynamic parameters on optimized swimming for simulated, fish-inspired robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Leveraging publicly available textual object descriptions
for anthropomorphic robotic grasp predictions. <em>IROS</em>, 7476–7483.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic systems using anthropomorphic end-effectors face tremendous challenges choosing a suitable pose for grasping an object. The fact that the choice of a grasp is influenced by the physical properties of an object, the intended task, and the environment results in a considerable amount of variables. The majority of models targeted towards enabling such robots to determine a suitable grasping pose rely on computer vision techniques, sometimes complemented by textual data. This paper investigates the potential of publicly available textual descriptions to predict a suitable grasping pose for anthropomorphic end-effectors. To this end, we have retrieved textual descriptions from Wikipedia, Wiktionary, and WordNet as well as a number of well-known dictionaries for 100 everyday objects. We analyze and compare the prediction quality of multiple learning methods while showing that a support vector machine-based approach can utilize this data for achieving a prediction accuracy above 0.75. Finally, we make our collected data available to the research community.},
  archive   = {C_IROS},
  author    = {Niko Kleer and Martin Feick and Michael Feld},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981541},
  pages     = {7476-7483},
  title     = {Leveraging publicly available textual object descriptions for anthropomorphic robotic grasp predictions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VGPN: 6-DoF grasp pose detection network based on hough
voting. <em>IROS</em>, 7460–7467. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel Voting based Grasp Pose Network (VGPN) to detect 6-DoF grasps in cluttered scenes. The motivation of this paper is that local object geometry can provide useful clues about where the object can be grasped. Generated by the sampled seed points from raw point cloud, the votes allow seed points in different object regions to contribute to locations where the object can be grasped. Geometric features from various local regions are aggregated to generate grasps in a more confident and dense space, which enables grasp prediction utilizing more global context features. The search space of grasp pose detection is also greatly reduced. Experimental results on both simulation and real-world environments show that our proposed method outperforms state-of-the-art approaches in terms of both success rate and coverage of the ground truth grasps. The objects can be grasped with fewer attempts which is critical in real-world applications.},
  archive   = {C_IROS},
  author    = {Liming Zheng and Yinghao Cai and Tao Lu and Shuo Wang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981925},
  pages     = {7460-7467},
  title     = {VGPN: 6-DoF grasp pose detection network based on hough voting},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SSP-pose: Symmetry-aware shape prior deformation for direct
category-level object pose estimation. <em>IROS</em>, 7452–7459. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Category-level pose estimation is a challenging problem due to intra-class shape variations. Recent methods deform pre-computed shape priors to map the observed point cloud into the normalized object coordinate space and then retrieve the pose via post-processing, i.e., Umeyama&#39;s Algorithm. The shortcomings of this two-stage strategy lie in two aspects: 1) The surrogate supervision on the intermediate results can not directly guide the learning of pose, resulting in large pose error after post-processing. 2) The inference speed is limited by the post-processing step. In this paper, to handle these shortcomings, we propose an end-to-end trainable network SSP-Pose for category-level pose estimation, which integrates shape priors into a direct pose regression network. SSP-Pose stacks four individual branches on a shared feature extractor, where two branches are designed to deform and match the prior model with the observed instance, and the other two branches are applied for directly regressing the totally 9 degrees-of-freedom pose and performing symmetry reconstruction and point-wise inlier mask prediction respectively. Consistency loss terms are then naturally exploited to align the outputs of different branches and promote the performance. During inference, only the direct pose regression branch is needed. In this manner, SSP-Pose not only learns category-level pose-sensitive characteristics to boost performance but also keeps a real-time inference speed. Moreover, we utilize the symmetry information of each category to guide the shape prior deformation, and propose a novel symmetry-aware loss to mitigate the matching ambiguity. Extensive experiments on public datasets demon-strate that SSP-Pose produces superior performance compared with competitors with a real-time inference speed at about 25Hz. The codes will be released soon.},
  archive   = {C_IROS},
  author    = {Ruida Zhang and Yan Di and Fabian Manhardt and Federico Tombari and Xiangyang Ji},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981506},
  pages     = {7452-7459},
  title     = {SSP-pose: Symmetry-aware shape prior deformation for direct category-level object pose estimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). WFA-IRL: Inverse reinforcement learning of autonomous
behaviors encoded as weighted finite automata. <em>IROS</em>, 7429–7435.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a method for learning logical task specifications and cost functions from demonstrations. Constructing specifications by hand is challenging for complex objectives and constraints in autonomous systems. Instead, we consider demonstrated task executions, whose logic structure and transition costs need to be inferred by an autonomous agent. We employ a spectral learning approach to extract a weighted finite automaton (WFA), approximating the unknown task logic. Thereafter, we define a product between the WFA for high-level task guidance and a labeled Markov decision process for low-level control. An inverse reinforcement learning (IRL) problem is considered to learn a cost function by backpropagating the loss between agent and expert behaviors through the planning algorithm. Our proposed model, termed WFA-IRL, is capable of generalizing the execution of the inferred task specification in a suite of MiniGrid environments.},
  archive   = {C_IROS},
  author    = {Tianyu Wang and Nikolay Atanasov},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981874},
  pages     = {7429-7435},
  title     = {WFA-IRL: Inverse reinforcement learning of autonomous behaviors encoded as weighted finite automata},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inverse reinforcement learning with hybrid-weight
trust-region optimization and curriculum learning for autonomous
maneuvering. <em>IROS</em>, 7421–7428. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite significant advancements, collision-free navigation in autonomous driving is still challenging, considering the navigation module needs to balance learning and planning to achieve efficient and effective control of the vehicle. We propose a novel framework of inverse reinforcement learning with hybrid-weight trust-region optimization and curriculum learning (IRL-HC) for autonomous maneuvering. Our method can incorporate both expert demonstration (from real driving) and domain knowledge (hard constraints such as collision avoidance, goal reaching, etc. encoded in reward functions) to learn an effective control policy. The hybrid-weight trustregion optimization is used to determine the difficulty of the task curriculum for fast incremental curriculum learning and improve the efficiency of inverse reinforcement learning by hybrid weight tuning of different sets of hyperparameters. IRL-HC is also compatible with domain-dependent techniques such as learn-from-accident, which can further boost performance. Overall, IRL-HC can reduce the number of collisions up to 48\%, increase the training efficiency by 2.8x, and enable the vehicle to drive 10x further compared to other methods.},
  archive   = {C_IROS},
  author    = {Yu Shen and Weizi Li and Ming C. Lin},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981103},
  pages     = {7421-7428},
  title     = {Inverse reinforcement learning with hybrid-weight trust-region optimization and curriculum learning for autonomous maneuvering},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robot policy learning from demonstration using advantage
weighting and early termination. <em>IROS</em>, 7414–7420. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning robotic tasks in the real world is still highly challenging and effective practical solutions remain to be found. Traditional methods used in this area are imitation learning and reinforcement learning, but they both have limitations when applied to real robots. Combining reinforcement learning with pre-collected demonstrations is a promising approach that can help in learning control policies to solve robotic tasks. In this paper, we propose an algorithm that uses novel techniques to leverage offline expert data using offline and online training to obtain faster convergence and improved performance. The proposed algorithm (AWET) weights the critic losses with a novel agent advantage weight to improve over the expert data. In addition, AWET makes use of an automatic early termination technique to stop and discard policy rollouts that are not similar to expert trajectories-to prevent drifting far from the expert data. In an ablation study, AWET showed improved and promising performance when compared to state-of-the-art baselines on four standard robotic tasks.},
  archive   = {C_IROS},
  author    = {Abdalkarim Mohtasib and Gerhard Neumann and Heriberto Cuayáhuitl},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981056},
  pages     = {7414-7420},
  title     = {Robot policy learning from demonstration using advantage weighting and early termination},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robot learning from demonstration using elastic maps.
<em>IROS</em>, 7407–7413. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning from Demonstration (LfD) is a popular method of reproducing and generalizing robot skills from human-provided demonstrations. In this paper, we propose a novel optimization-based LfD method that encodes demon-strations as elastic maps. An elastic map is a graph of nodes connected through a mesh of springs. We build a skill model by fitting an elastic map to the set of demonstrations. The formulated optimization problem in our approach includes three objectives with natural and physical interpretations. The main term rewards the mean squared error in the Cartesian coordinate. The second term penalizes the non-equidistant distribution of points resulting in the optimum total length of the trajectory. The third term rewards smoothness while pe-nalizing nonlinearity. These quadratic objectives form a convex problem that can be solved efficiently with local optimizers. We examine nine methods for constructing and weighting the elastic maps and study their performance in robotic tasks. We also evaluate the proposed method in several simulated and real-world experiments using a UR5e manipulator arm, and compare it to other LfD approaches to demonstrate its benefits and flexibility across a variety of metrics.},
  archive   = {C_IROS},
  author    = {Brendan Hertel and Matthew Pelland and S. Reza Ahmadzadeh},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981241},
  pages     = {7407-7413},
  title     = {Robot learning from demonstration using elastic maps},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Imitation of manipulation skills using multiple geometries.
<em>IROS</em>, 7391–7398. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Daily manipulation tasks are characterized by geometric primitives related to actions and object shapes. Such geometric descriptors are poorly represented by only using Cartesian coordinate systems. In this paper, we propose a learning approach to extract the optimal representation from a dictionary of coordinate systems to encode an observed movement/behavior. This is achieved by using an extension of Gaussian distributions on Riemannian manifolds, which is used to analyse a set of user demonstrations statistically, by considering multiple geometries as candidate representations of the task. We formulate the reproduction problem as a general optimal control problem based on an iterative linear quadratic regulator (iLQR), where the Gaussian distribution in the extracted coordinate systems are used to define the cost function. We apply our approach to object grasping and box opening tasks in simulation and on a 7-axis Franka Emika robot. The results show that the robot can exploit several geometries to execute the manipulation task and generalize it to new situations, by maintaining the invariant characteristics of the task in the coordinate system(s) of interest.},
  archive   = {C_IROS},
  author    = {Boyang Ti and Yongsheng Gao and Jie Zhao and Sylvain Calinon},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981683},
  pages     = {7391-7398},
  title     = {Imitation of manipulation skills using multiple geometries},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning object manipulation skills from video via
approximate differentiable physics. <em>IROS</em>, 7375–7382. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We aim to teach robots to perform simple object manipulation tasks by watching a single video demonstration. Towards this goal, we propose an optimization approach that outputs a coarse and temporally evolving 3D scene to mimic the action demonstrated in the input video. Similar to previous work, a differentiable renderer ensures perceptual fidelity between the 3D scene and the 2D video. Our key novelty lies in the inclusion of a differentiable approach to solve a set of Ordinary Differential Equations (ODEs) that allows us to approximately model laws of physics such as gravity, friction, and hand-object or object-object interactions. This not only enables us to dramatically improve the quality of estimated hand and object states, but also produces physically admissible trajectories that can be directly translated to a robot without the need for costly reinforcement learning. We evaluate our approach on a 3D reconstruction task that consists of 54 video demonstrations sourced from 9 actions such as pull something from right to left or put something in front of something. Our approach improves over previous state-of-the-art by almost 30\%, demonstrating superior quality on especially challenging actions involving physical interactions of two objects such as put something onto something. Finally, we showcase the learned skills on a Franka Emika Panda robot.},
  archive   = {C_IROS},
  author    = {Vladimír Petrík and Mohammad Nomaan Qureshi and Josef Sivic and Makar Tapaswi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982084},
  pages     = {7375-7382},
  title     = {Learning object manipulation skills from video via approximate differentiable physics},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Locally optimal estimation and control of cable driven
parallel robots using time varying linear quadratic gaussian control.
<em>IROS</em>, 7367–7374. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a locally optimal tracking controller for Cable Driven Parallel Robot (CDPR) control based on a time-varying Linear Quadratic Gaussian (TV-LQG) controller. In contrast to many methods which use fixed feedback gains, our time-varying controller computes the optimal gains depending on the location in the workspace and the future trajectory. Meanwhile, we rely heavily on offline computation to reduce the burden of online implementation and feasibility checking. Following the growing popularity of probabilistic graphical models for optimal control, we use factor graphs as a tool to formulate our controller for their efficiency, intuitiveness, and modularity. The topology of a factor graph encodes the relevant structural properties of equations in a way that facilitates insight and efficient computation using sparse linear algebra solvers. We first use factor graph optimization to compute a nominal trajectory, then linearize the graph and apply variable elimination to compute the locally optimal, time varying linear feedback gains. Next, we leverage the factor graph formulation to compute the locally optimal, time-varying Kalman Filter gains, and finally combine the locally optimal linear control and estimation laws to form a TV-LQG controller. We compare the tracking accuracy of our TV-LQG controller to a state-of-the-art dual-space feed-forward controller on a 2.9m x 2.3m, 4-cable planar robot and demonstrate improved tracking accuracies of 0.8° and 11.6 mm root mean square error in rotation and translation respectively.},
  archive   = {C_IROS},
  author    = {Gerry Chen and Seth Hutchinson and Frank Dellaert},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981144},
  pages     = {7367-7374},
  title     = {Locally optimal estimation and control of cable driven parallel robots using time varying linear quadratic gaussian control},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On a balanced delta robot for precise aerial manipulation:
Implementation, testing, and lessons for future designs. <em>IROS</em>,
7359–7366. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Using a delta-manipulator for stabilisation of an end-effector to perform precise spatial positioning is a current area of interest in aerial manipulation. High speed precision movements of a manipulator can cause disturbances to the aerial platform, which hinders trajectory tracking and in some cases could be sufficient to cause a loss of control of the vehicle. In this paper, a statically balanced delta aerial manipulator is developed and evaluated. The system is balanced using three counter-masses to reduce the force imparted onto the base and thus reduce perturbations to the movement of the drone. The system is thoroughly tested following trajectories while mounted to a force sensor and while on-board an aerial vehicle. Results show that the forces transmitted to the base in all axes are reduced considerably, however improvements in overall flight accuracy are not observed in aerial settings. Design lessons to make a balanced delta-manipulator viable for practical implementation on an aerial vehicle are discussed in depth. A video summarising the flight testing results is available at https://youtu.be/fXKnosnVKCk.},
  archive   = {C_IROS},
  author    = {Angus B. Clark and Nicholas Baron and Lachlan Orr and Mirko Kovac and Nicolas Rojas},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981736},
  pages     = {7359-7366},
  title     = {On a balanced delta robot for precise aerial manipulation: Implementation, testing, and lessons for future designs},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Experimental study on impact resistance of multi-DOF
electro-hydrostatic robot systems using hydracer, a 6DOF arm.
<em>IROS</em>, 7352–7358. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Industrial robots require force controllability and impact resistance to ensure safe physical interactions. An electro-hydrostaic actuator (EHA) is expected to be suitable for such applications because it has high backdrivability which improve both force controllability at contact and impact resistance. However, EHAs had been rarely used in multi-axes robotic systems. The previous works validated the force controllability of the EHA-driven robot Hydra. However, the impact resistance of an EHA-driven robot is still unclear. In order to evaluate the impact resistance of the high-power EHA-driven robot, we developed high-pressure EHAs employing ceramics as rigid material to reduce internal leakage, and developed the EHA-driven 6-DOF robot arm Hydracer as the platform for the evaluation. This paper describes the mechanism of Hydracer especially on the base 3-DOF mechanism, and conducts the backdrivability measurement and the impact resistance evaluation.},
  archive   = {C_IROS},
  author    = {Mitsuo Komagata and Yutaro Imashiro and Ryoya Suzuki and Kento Oishi and Ko Yamamoto and Yoshihiko Nakamura},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981130},
  pages     = {7352-7358},
  title     = {Experimental study on impact resistance of multi-DOF electro-hydrostatic robot systems using hydracer, a 6DOF arm},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An all-in-one cable-driven parallel robot with flexible
workspace and its auto-calibration method. <em>IROS</em>, 7345–7351. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For traditional cable-driven parallel robots (CD-PRs), changing the workspace is relatively difficult, which needs to reconfigure the anchor points and the external frame. The main reason is that the winch is separated from the moving platform, and a series of pulleys are applied to guide the driving cables. This paper proposes a novel all-in-one suspended CDPR that integrates all components in the moving platform to realize a flexible workspace. For the rapid construction of the CDPR, the ends of the cables only need to be connected to the existing anchor point. However, due to the flexible workspace, the position values of the anchor points should be recalibrated by appropriate calibration methods, especially by using a rapid auto-calibration method in application sites. Thus, based on the kinetostatic model considering sagging cable, an auto-calibration method with an inclinometer is proposed. Simulation and experiment are conducted respectively, and the experiment results indicate that 78.56\% totally reduces the errors of the fixed anchor points. Moreover, experiments were carried out to validate the design and kinematics.},
  archive   = {C_IROS},
  author    = {Hao An and Hang Liu and Xintian Liu and Han Yuan},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982214},
  pages     = {7345-7351},
  title     = {An all-in-one cable-driven parallel robot with flexible workspace and its auto-calibration method},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Vision-based safety system for barrierless human-robot
collaboration. <em>IROS</em>, 7331–7336. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human safety has always been the main priority when working near an industrial robot. With the rise of Human-Robot Collaborative environments, physical barriers to avoiding collisions have been disappearing, increasing the risk of accidents and the need for solutions that ensure a safe Human-Robot Collaboration. This paper proposes a safety system that implements Speed and Separation Monitoring (SSM) type of operation. For this, safety zones are defined in the robot&#39;s workspace following current standards for industrial collaborative robots. A deep learning-based computer vision system detects, tracks, and estimates the 3D position of operators close to the robot. The robot control system receives the operator&#39;s 3D position and generates 3D representations of them in a simulation environment. Depending on the zone where the closest operator was detected, the robot stops or changes its operating speed. Three different operation modes in which the human and robot interact are presented. Results show that the vision-based system can correctly detect and classify in which safety zone an operator is located and that the different proposed operation modes ensure that the robot&#39;s reaction and stop time are within the required time limits to guarantee safety.},
  archive   = {C_IROS},
  author    = {Lina María Amaya-Mejía and Nicolás Duque-Suárez and Daniel Jaramillo-Ramírez and Carol Martinez},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981689},
  pages     = {7331-7336},
  title     = {Vision-based safety system for barrierless human-robot collaboration},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accurate edge detection for robotic welding through tactile
exploration. <em>IROS</em>, 7323–7330. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Programming paths for robotic welding conventionally requires precise positioning of workpieces, detailed 3D models and/or tedious teach pendant programming. A new method is introduced in this paper that enables an operator to teach the weld path to the robot through a haptic-visual interface. The operator teaches the path by guiding the tool tip to contact on the workpiece surface with force feedback through the haptic device, and drawing exploratory paths that intersect the edge to be welded as well as adjoining surfaces. Tool-tip positions in contact with the workpiece are recorded. A RANSAC-type algorithm is used to automatically estimate a piecewise parametric curve along the edge as well as geometric parameters of the adjoining surfaces. The required tool trajectory for the robot to weld along the workpiece edge is automatically generated. Experiments performed in simulation and on a physical KUKA IIWA7 robot demonstrate that the developed method can successfully detect workpiece edges within a maximum deviation of 1mm. Furthermore, the method is intuitive, and requires no knowledge of robot programming for an operator to program multi-segment weld paths quickly.},
  archive   = {C_IROS},
  author    = {Shameek Ganguly and Oussama Khatib},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981824},
  pages     = {7323-7330},
  title     = {Accurate edge detection for robotic welding through tactile exploration},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A configurable skill oriented architecture based on OPC UA.
<em>IROS</em>, 7317–7322. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Over the last years, research done in automation and industrial robotics has established the foundations for skill-oriented systems based on the OPC UA standard. Nevertheless, utilizing these advances in other areas of robotics research can be challenging and time consuming. We present a framework aiming to reduce this entry threshold. Our solution is an open source, easy to configure tool based on OPC UA, that provides with a hardware agnostic, skill-oriented, event driven interface to systems. The framework allows integrating external hardware and software by means of plugins. It also provides a mechanism for endowing skills with hardware agnostic motion control. We demonstrate how our framework can be combined with state of the art approaches to control in a simulated assembly task with multiple robots, conveyors, actuators and sensors involved.},
  archive   = {C_IROS},
  author    = {Jorge Blesa Gracia and Felix Leber and Mohamed Aburaia and Wilfried Wöber},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982164},
  pages     = {7317-7322},
  title     = {A configurable skill oriented architecture based on OPC UA},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-objective geometric optimization of a multi-link
manipulator using parameterized design method. <em>IROS</em>, 7310–7316.
(<a href="https://doi.org/10.1109/IROS47612.2022.9982094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The performance of a robot is closely related to its structure. From the initial design of link lengths to structural optimization, it is still the research hotspot in recent years. To make the manipulator lightweight and ensure its working range and flexibility, researchers have proposed many optimization methods, most of which are for specific working scenarios, requirements, and robot structures, therefore their generality is limited. The optimization of the manipulator should be a comprehensive method. That is, we should pay attention to the joint configuration and each link length at the beginning of the design. Particularly, the geometric parameters of each link, which not only affect the range of the workspace but also have a direct impact on the working space, working efficiency, and flexibility of the manipulator. In this paper, a generalized optimization framework is proposed for multi-link manipulators. Starting from the optimization of manipulator link lengths, firstly, the geometry of the manipulator and workspace is parameterized; then the performance indicators are established; lastly, the geometric size of the manipulator is optimized according to the workspace limits and task requirements. Besides, we verified its feasibility and generality by applying this method to different TBM scenarios.},
  archive   = {C_IROS},
  author    = {Xiaomeng Hu and Weiwei Wan and Liang Du and Jianjun Yuan and Shugen Ma},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982094},
  pages     = {7310-7316},
  title     = {Multi-objective geometric optimization of a multi-link manipulator using parameterized design method},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Particle swarm optimizer-based attack strategy with swarm
robots. <em>IROS</em>, 7304–7309. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An environment where a robot swarm attacks a territory protected by another one leads to an attack-defense confrontation problem. Commonly-used deep reinforcement learning-based methods rely on pre-training and become intractable due to the curse of dimensionality. To develop effective attack strategies, inspired by a particle swarm optimizer (PSO), this work proposes a PSO-based strategy for a robot swarm for the first time. During the moving of a robot swarm, each robot obtains situation information through perceiving its nearby peers and enemies and uses such information to construct its fitness function. Then, each robot uses PSO to optimize its fitness function and searches for its optimal attack position, which guides it to move in the next time slot. The experimental analyses show that the PSO-based attack strategy has more potential in solving large-scale confrontational problems than the deep reinforcement learning-based algorithms.},
  archive   = {C_IROS},
  author    = {Huan Liu and JunQi Zhang and MengChu Zhou},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981215},
  pages     = {7304-7309},
  title     = {Particle swarm optimizer-based attack strategy with swarm robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deliberative democracy with robot swarms. <em>IROS</em>,
7296–7303. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Decision-making among groups of humans can benefit from open discussion and inclusion of a diversity of opinions, promoting deliberative democracy. In this work, we test whether a swarm of robots can help facilitate decision-making by visually representing the diversity of opinions. We used a swarm of robots we built, called MOSAIX, that consists of 4-inch touchscreens-on-wheels robots called Tiles. The robots acted as physical avatars for opinions, helping them travel and mix together. We recruited 46 participants split into groups of 7 and 8 to test whether the robot movement had an impact on the decision-making process versus using the robots stationary in the participants&#39; hands akin to smartphones. Furthermore, we wanted to test whether the participants felt comfortable expressing their opinion through the robots. Results show the participants indeed felt comfortable using the robots, and user engagement increased with the movement of the robots. The difference between the participants&#39; first and last opinions also increased with the movement of the robots. We believe that robot swarms have not been used before to facilitate decision-making among a group of people. Therefore, our contribution is in testing the possibility of how and whether using a moving robot swarm helps humans reach a decision.},
  archive   = {C_IROS},
  author    = {Merihan Alhafnawi and Edmund R. Hunt and Severin Lemaignan and Paul O&#39;Dowd and Sabine Hauert},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981649},
  pages     = {7296-7303},
  title     = {Deliberative democracy with robot swarms},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Collective decision making in communication-constrained
environments. <em>IROS</em>, 7266–7271. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One of the main tasks for autonomous robot swarms is to collectively decide on the best available option. Achieving that requires a high quality communication between the agents that may not always be available in a real world environment. In this paper we introduce the communication-constrained collective decision-making problem where some areas of the environment limit the agents&#39; ability to communicate, either by reducing success rate or blocking the communication channels. We propose a decentralised algorithm for mapping environmental features for robot swarms as well as improving collective decision making in communication-limited environments without prior knowledge of the communication landscape. Our results show that making a collective aware of the communication environment can improve the speed of convergence in the presence of communication limitations, at least 3 times faster, without sacrificing accuracy.},
  archive   = {C_IROS},
  author    = {Thomas G. Kelly and Mohammad Divband Soorati and Klaus-Peter Zauner and Sarvapali D. Ramchurn and Danesh Tarapore},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981622},
  pages     = {7266-7271},
  title     = {Collective decision making in communication-constrained environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A dynamical system approach to decentralized collision-free
autonomous coordination of a mobile assistive furniture swarm.
<em>IROS</em>, 7259–7265. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In order to facilitate and assist the indoor mobility of people with special needs, the classically static objects in the environment, such as furniture, can be rendered mobile. The need for efficient and safe autonomous coordination of a mobile furniture swarm arises. We present a closed-form approach for mobile furniture obstacle avoidance and navigation within an indoor environment. The approach shows that each mobile furniture agent, defined by a polygonal surface, does not collide with any static or mobile obstacle (e.g., a person is moving around). All controllable mobile furniture converges towards a defined goal position and orientation. We showcase the application of this algorithm in simulation on mobile furniture for smart environments. Results demonstrate that the proposed method can coordinate a swarm of mobile furniture to get out of the way of a mobile agent representing a person with limited mobility passing through the room while avoiding obstacles and converging towards a predefined target pose.},
  archive   = {C_IROS},
  author    = {Federico M. Conzelmann and Lukas Huber and Diego Paez-Granados and Anastasia Bolotnikova and Auke Ijspeert and Aude Billard},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981583},
  pages     = {7259-7265},
  title     = {A dynamical system approach to decentralized collision-free autonomous coordination of a mobile assistive furniture swarm},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Decentralized control of minimalistic robotic swarms for
guaranteed target encapsulation. <em>IROS</em>, 7251–7258. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a decentralized control algorithm for a minimalistic robotic swarm with limited capabilities such that the desired global behavior emerges. We consider the problem of searching for and encapsulating various targets present in the environment while avoiding collisions with both static and dynamic obstacles. The novelty of this work is the guaranteed generation of desired complex swarm behavior with constrained individual robots which have no memory, no localization, and no knowledge of the exact relative locations of their neighbors. Moreover, we analyze how the emergent behavior changes with different parameters of the task, noise in the sensor reading, and asynchronous execution.},
  archive   = {C_IROS},
  author    = {Himani Sinhmar and Hadas Kress-Gazit},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981959},
  pages     = {7251-7258},
  title     = {Decentralized control of minimalistic robotic swarms for guaranteed target encapsulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Collective decision-making with bayesian robots in dynamic
environments. <em>IROS</em>, 7245–7250. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collective decision-making enables self-organizing robot swarms to act autonomously on a swarm level and is essential to coordinate their actions as a whole. When robots only share and communicate information locally a distributed and decentralized approach is required. In a previous paper [4], an efficient method based on a distributed Bayesian algorithm was created to distinguish a binary environment. We extended it to have the capability of dealing with dynamic environments. Therefore, it must avoid global lock-in states. In many realistic applications the robot swarm needs to adapt to (collectively) measurable changes at runtime by revising previous collective decisions. The trade-off between decision-making speed and readiness to revise previous decisions is a seemingly unavoidable challenge. We present our extension of the former approach and study how this trade-off can efficiently be balanced.},
  archive   = {C_IROS},
  author    = {Kai Pfister and Heiko Hamann},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982019},
  pages     = {7245-7250},
  title     = {Collective decision-making with bayesian robots in dynamic environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CGLR: Dense multi-agent navigation using voronoi cells and
congestion metric-based replanning. <em>IROS</em>, 7213–7220. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a decentralized path-planning algorithm for navigating multiple differential-drive robots in dense environments. In contrast to prior decentralized methods, we propose a novel congestion metric-based replanning that couples local and global planning techniques to efficiently navigate in scenarios with multiple corridors. To handle dense scenes with narrow passages, our approach computes the initial path for each agent to its assigned goal using a lattice planner. Based on neighbors&#39; information, each agent performs online replanning using a congestion metric that tends to reduce the collisions and improves the navigation performance. Furthermore, we use the Voronoi cells of each agent to plan the local motion as well as a corridor selection strategy to limit the congestion in narrow passages. We evaluate the performance of our approach in complex scenes with tens of agents and narrow passages. We show that our Coupled Global-Local approach and Replanning (CGLR) improves the performance and efficiency over prior decentralized methods. In addition, our approach results in a higher success rate in terms of collision-free navigation to the goals, showing improvement in the range of 3-70\% over prior decentralized solutions in certain scenarios.},
  archive   = {C_IROS},
  author    = {Senthil Hariharan Arul and Dinesh Manocha},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982110},
  pages     = {7213-7220},
  title     = {CGLR: Dense multi-agent navigation using voronoi cells and congestion metric-based replanning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust counterexample-guided optimization for planning from
differentiable temporal logic. <em>IROS</em>, 7205–7212. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Signal temporal logic (STL) provides a powerful, flexible framework for specifying complex autonomy tasks; however, existing methods for planning based on STL specifications have difficulty scaling to long-horizon tasks and are not robust to external disturbances. In this paper, we present an algorithm for finding robust plans that satisfy STL specifications. Our method alternates between local optimization and local falsification, using automatically differentiable temporal logic to iteratively optimize its plan in response to counterexamples found during the falsification process. We benchmark our counterexample-guided planning method against state-of-the-art planning methods on two long-horizon satellite rendezvous missions, showing that our method finds high-quality plans that satisfy STL specifications despite adversarial disturbances. We find that our method consistently finds plans that are robust to adversarial disturbances and requires less than half the time of competing methods. We provide an implementation of our planner at https://github.com/MIT-REALM/architect.},
  archive   = {C_IROS},
  author    = {Charles Dawson and Chuchu Fan},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981382},
  pages     = {7205-7212},
  title     = {Robust counterexample-guided optimization for planning from differentiable temporal logic},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TerraPN: Unstructured terrain navigation using online
self-supervised learning. <em>IROS</em>, 7197–7204. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present TerraPN, a novel method that learns the surface properties (traction, bumpiness, deformability, etc.) of complex outdoor terrains directly from robot-terrain interactions through self-supervised learning, and uses it for autonomous robot navigation. Our method uses RGB images of terrain surfaces and the robot&#39;s velocities as inputs, and the IMU vibrations and odometry errors experienced by the robot as labels for self-supervision. Our method computes a surface cost map that differentiates smooth, high-traction surfaces (low navigation costs) from bumpy, slippery, deformable surfaces (high navigation costs). We compute the cost map by non-uniformly sampling patches from the input RGB image by detecting boundaries between surfaces resulting in low inference times (47.27\% lower) compared to uniform sampling and existing segmentation methods. We present a novel navigation algorithm that accounts for a surface&#39;s cost, computes cost-based acceleration limits for the robot, and dynamically feasible, collision-free trajectories. TerraPN&#39;s surface cost prediction can be trained in ∼ 25 minutes for five different surfaces, compared to several hours for previous learning-based segmentation methods. In terms of navigation, our method outperforms previous works in terms of success rates (up to 35.84\% higher), vibration cost of the trajectories (up to 21.52\% lower), and slowing the robot on bumpy, deformable surfaces (up to 46.76\% slower) in different scenarios.},
  archive   = {C_IROS},
  author    = {Adarsh Jagan Sathyamoorthy and Kasun Weerakoon and Tianrui Guan and Jing Liang and Dinesh Manocha},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981942},
  pages     = {7197-7204},
  title     = {TerraPN: Unstructured terrain navigation using online self-supervised learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved a-search guided tree for autonomous trailer
planning. <em>IROS</em>, 7190–7196. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a motion planning strategy that utilizes the improved A -search guided tree to enable autonomous parking of a general 3-trailer with a car-like tractor. Different from the state-of-the-art state-lattice-based methods, where numerous motion primitives are necessary to ensure successful planning, our work allows quick off-lattice exploration to find a solution. Our treatment brings at least three advantages: fewer and lower design complexity of motion primitives, improved success rate, and increased path quality. Unlike on-lattice exploration, where the cost-to-go is obtained by querying a heuristic look-up table, off-lattice exploration entails the heuristic function being well-defined at off-lattice nodes. We train a neural network through reinforcement learning to model the maneuver costs of the trailer and use it as the heuristic value to better approximate the cost-to-go. Simulations demonstrate the effectiveness of the proposed method in terms of planning speed and path length.},
  archive   = {C_IROS},
  author    = {Jessica Leu and Yebin Wang and Masayoshi Tomizuka and Stefano Di Cairano},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982121},
  pages     = {7190-7196},
  title     = {Improved A-search guided tree for autonomous trailer planning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Speeding up optimization-based motion planning through deep
learning. <em>IROS</em>, 7182–7189. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Planning collision-free motions for robots with many degrees of freedom is challenging in environments with complex obstacle geometries. Recent work introduced the idea of speeding up the planning by encoding prior experience of successful motion plans in a neural network. However, this “neural motion planning” did not scale to complex robots in unseen 3D environments as needed for real-world applications. Here, we introduce “basis point set”, well-known in computer vision, to neural motion planning as a modern compact environment encoding enabling efficient supervised training networks that generalize well over diverse 3D worlds. Combined with a new elaborate training scheme, we reach a planning success rate of 100\%. We use the network to predict an educated initial guess for an optimization-based planner (OMP), which quickly converges to a feasible solution, massively outperforming random multi-starts when tested on previously unseen environments. For the DLR humanoid Agile Justin with 19 DoF and in challenging obstacle environments, optimal paths can be generated in 200 ms using only a single CPU core. We also show a first successful real-world experiment based on a high-resolution world model from an integrated 3D sensor.},
  archive   = {C_IROS},
  author    = {Johannes Tenhumberg and Darius Burschka and Berthold Bäuml},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981717},
  pages     = {7182-7189},
  title     = {Speeding up optimization-based motion planning through deep learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Speeding up POMDP planning via simplification.
<em>IROS</em>, 7174–7181. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we consider online planning in par-tially observable domains. Solving the corresponding POMDP problem is a very challenging task, particularly in an online setting. Our key contribution is a novel algorithmic approach, Simplified Information Theoretic Belief Space Planning (SITH-BSP), which aims to speed up POMDP planning considering belief-dependent rewards, without compromising the solution&#39;s accuracy. We do so by mathematically relating the simplified el-ements of the problem to the corresponding counterparts of the original problem. Specifically, we focus on belief simplification and use it to formulate bounds on the corresponding original belief-dependent rewards. These bounds in turn are used to perform branch pruning over the belief tree, in the process of extracting the optimal policy from this existing belief tree. We further introduce the notion of adaptive simplification, while re-using calculations between different simplification levels, and exploit it to prune, at each level in the belief tree, all branches but one. Therefore, our approach is guaranteed to find the optimal solution (policy) that corresponds to the given belief tree but with substantial speedup. As a second key contribution, we derive novel analytical bounds for differential entropy, considering a sampling-based belief representation, which we believe are of interest on their own. We validate our approach in simulation using these bounds and where simplification corresponds to reducing the number of samples, exhibiting a significant computational speedup while yielding the optimal solution for the given belief tree.},
  archive   = {C_IROS},
  author    = {Ori Sztyglic and Vadim Indelman},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981442},
  pages     = {7174-7181},
  title     = {Speeding up POMDP planning via simplification},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A standards-based pipeline route drawing system using a
towed sensing unit. <em>IROS</em>, 7167–7173. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a method of drawing pipeline routes using a sensing unit with a rotary encoder and IMU (Inertial Measurement Unit), which is towed by a self-propelled in-pipe inspection robot. However, the IMU information generally contains integration errors, making it difficult to draw accurate pipeline routes. In this study, we propose a method combining gradient descent using a gyroscopic sensor and an accelerometer, and the correction of the route based on the standard information of the pipe. First, the method of identifying the start point, end point, direction of straight pipes, and bending direction of curved pipes is explained. Then, an experiment is conducted using the developed robot system on a 11.6-meter-long pipeline course that includes nine curved pipes and horizontal and vertical straight pipes. Consequently, the mean absolute error of the route dimension was reduced to 2.74\%.},
  archive   = {C_IROS},
  author    = {Atsushi Kakogawa and Chihiro Hirose and Shugen Ma},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981626},
  pages     = {7167-7173},
  title     = {A standards-based pipeline route drawing system using a towed sensing unit},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PUTN: A plane-fitting based uneven terrain navigation
framework. <em>IROS</em>, 7160–7166. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous navigation of ground robots has been widely used in indoor structured 2D environments, but there are still many challenges in outdoor 3D unstructured environments, especially in rough, uneven terrains. This paper proposed a plane-fitting based uneven terrain navigation framework (PUTN) to solve this problem. The implementation of PUTN is divided into three steps. First, based on Rapidly-exploring Random Trees (RRT), an improved sample-based algorithm called Plane Fitting RRT*(PF- RRT*) is proposed to obtain a sparse trajectory. Each sampling point corresponds to a custom traversability index and a fitted plane on the point cloud. These planes are connected in series to form a traversable “strip”. Second, Gaussian Process Regression is used to generate traversability of the dense trajectory interpolated from the sparse trajectory, and the sampling tree is used as the training set. Finally, local planning is performed using nonlinear model predictive control (NMPC). By adding the traversability index and uncertainty to the cost function, and adding obstacles generated by the real-time point cloud to the constraint function, a safe motion planning algorithm with smooth speed and strong robustness is available. Experiments in real scenarios are conducted to verify the effectiveness of the method. The source code is released for the reference of the community 1 1 Source code: https://github.com/jianzhuozhuTHU/putn..},
  archive   = {C_IROS},
  author    = {Zhuozhu Jian and Zihong Lu and Xiao Zhou and Bin Lan and Anxing Xiao and Xueqian Wang and Bin Liang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981038},
  pages     = {7160-7166},
  title     = {PUTN: A plane-fitting based uneven terrain navigation framework},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MIMOSA: A multi-modal SLAM framework for resilient autonomy
against sensor degradation. <em>IROS</em>, 7153–7159. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a framework for Multi-Modal SLAM (MIMOSA) that utilizes a nonlinear factor graph as the underlying representation to provide loosely-coupled fusion of any number of sensing modalities. Tailored to the goal of enabling resilient robotic autonomy in GPS-denied and perceptually-degraded environments, MIMOSA currently contains modules for pointcloud registration, fusion of multiple odometry estimates relying on visible-light and thermal vision, as well as inertial measurement propagation. A flexible back-end utilizes the estimates from various modalities as relative transformation factors. The method is designed to be robust to degeneracy through the maintenance and tracking of modality-specific health metrics, while also being inherently tolerant to sensor failure. We detail this framework alongside our implementation for handling high-rate asynchronous sensor measurements and evaluate its performance on data from autonomous subterranean robotic exploration missions using legged and aerial robots.},
  archive   = {C_IROS},
  author    = {Nikhil Khedekar and Mihir Kulkarni and Kostas Alexis},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981108},
  pages     = {7153-7159},
  title     = {MIMOSA: A multi-modal SLAM framework for resilient autonomy against sensor degradation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Capability-aware task allocation and team formation analysis
for cooperative exploration of complex environments. <em>IROS</em>,
7145–7152. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To achieve autonomy in complex real-world exploration missions, we consider deployment strategies for a team of robots with heterogeneous capabilities. We formulate a multi-robot exploration mission and compute an operation policy to maintain robot team productivity and maximize mission success. The environment description, robot capability, and mission outcome are modeled as a Markov decision process (MDP). We also include constraints, such as sensor failures, limited communication coverage, and mobility-stressing elements. The proposed operation model is applied to the DARPA Subterranean (SubT) Challenge. The deployment policy is also compared against the human-based operation strategy in the final competition of the SubT Challenge.},
  archive   = {C_IROS},
  author    = {Muhammad Fadhil Ginting and Kyohei Otsu and Mykel J. Kochenderfer and Ali-akbar Agha-mohammadi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981631},
  pages     = {7145-7152},
  title     = {Capability-aware task allocation and team formation analysis for cooperative exploration of complex environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluation of position-keeping strategies for
symmetrically-shaped autonomous water-surface robots under disturbances.
<em>IROS</em>, 7137–7144. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Extensive research has been conducted on autonomous surface robots and underwater robots for various tasks in aquatic environments. The duration of the operation of autonomous field robots depends on the capacity of the mounted battery, as they are not typically connected to an external power supply. Therefore, smart strategies which are optimized for each task are required to extend the working time of autonomous field robots. We have developed a symmetrically-shaped au-tonomous surface robot for the long-term monitoring of water quality. In this study, we propose position-keeping strategies to prolong the duration of the symmetrically-shaped surface robot for in-situ monitoring. The proposed position-keeping strategies are evaluated in terms of the power consumption and mean error distance in both practical and simulation environments. The experimental results demonstrate that a robot placed on a water surface with disturbance determines the best course of action to maintain its position based on the environmental conditions and application.},
  archive   = {C_IROS},
  author    = {Yasuyuki Fujii and Dinh Tuan Tran and Joo-Ho Lee},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982124},
  pages     = {7137-7144},
  title     = {Evaluation of position-keeping strategies for symmetrically-shaped autonomous water-surface robots under disturbances},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Autonomous pipeline tracking using bernoulli filter for
unmanned underwater surveys. <em>IROS</em>, 7129–7136. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inspection of subsea pipelines is crucial for avoiding any hazards and minimizing the risks to infrastructure and the environment. These inspections are achieved using Autonomous Underwater Vehicles (AUVs) in favour of reduced operational costs. This work presents a vehicle agnostic approach for tracking subsea pipelines at close-range for autonomous guidance along the pipeline using an AUV. A multibeam echosounder is used as the primary tracking sensor augmented by fluxgate magnetometers that can track buried pipelines over short ranges until they are exposed again. A Bernoulli filter is proposed to efficiently track pipelines in presence of environmental clutter. Field experiments were carried out in a dock and at sea to evaluate the proposed system and the benefits of using a Bernoulli filter over a Kalman filter-based solution.},
  archive   = {C_IROS},
  author    = {Vibhav Bharti and Sen Wang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981114},
  pages     = {7129-7136},
  title     = {Autonomous pipeline tracking using bernoulli filter for unmanned underwater surveys},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel robot with rolling and climbing modes for power
transmission line inspection. <em>IROS</em>, 7122–7128. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As a hard high-altitude work, power transmission line inspection increasingly demands robots to conduct in place of the human being. A variety of robots have been developed to this end, with basic locomotion and inspection implemented on the lines. However, most current line inspection robots (LIRs) are only mobile platforms with complex structures and large weights, lacking sufficiently dexterous locomotion on lines, especially for obstacle overcoming and line transition. Also with sensors fixed on the platform, the inspection range is largely limited. For higher mobility and a larger inspection range, a novel biped robot that can roll and climb on a power transmission line for inspection, called Climbot-L, is proposed in this paper. While the rolling mode has the advantage of high locomotion efficiency, the biped climbing mode makes it possible to easily overcome obstacles on the line, transition to adjacent cables, and have multi-view detection. In this paper, the design of this novel robot is first introduced, the working principle of the wheel-gripper modules is then analyzed, and obstacle overcoming gaits are stated. The effectiveness and high maneuverability of the presented robot are verified by a series of experiments.},
  archive   = {C_IROS},
  author    = {Qiang Fu and Yisheng Guan and Haifei Zhu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981434},
  pages     = {7122-7128},
  title     = {A novel robot with rolling and climbing modes for power transmission line inspection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient sampling-based planning for subterranean
exploration. <em>IROS</em>, 7114–7121. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The paper proposes a path planning solution for autonomous robotic exploration of complex subterranean environments. The work contributes to the family of graph-based planners by bringing the following improvements. Firstly, an occupancy grid-based sample-and-project solution to terrain-assessment is proposed instead of building an explicit elevation map of the environment. Secondly, the solution-search method is formulated as a constraint-satisfaction problem to obtain a good-enough solution instead of optimizing for a single objective function parametrized by penalty gains. This method is shown to significantly improve the computational efficiency of the planner. Thirdly, a coordination solution is proposed that relies on the position histories of the robots instead of merged-maps or merged-graphs, therefore, making the planning solution resilient to inter-robot map misalignments, while also reducing the communication bandwidth required to carry out coordination. Finally, the planner also takes into account changes in the environment such as blocked passages that are initially open. The proposed planning solution is demonstrated at the DARPA Subterranean Challenge final event by team MARBLE, the third place finisher of the challenge.},
  archive   = {C_IROS},
  author    = {Shakeeb Ahmad and J. Sean Humbert},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982169},
  pages     = {7114-7121},
  title     = {Efficient sampling-based planning for subterranean exploration},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards adaptive continuous control of soft robotic
manipulator using reinforcement learning. <em>IROS</em>, 7074–7081. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although the soft robot is gaining considerable popularity in dexterous and safe manipulation, accurate motion control is still an open problem to be explored. Recent investigations suggest that reinforcement learning (RL) is a promising solution but lacks efficient adaptability for Sim2Real transfer or environment variations. In this paper, we present a deep deterministic policy gradient (DDPG)-based control system for the continuous task-space manipulation of soft robots. Domain randomization is adopted in simulation for fast control-policy initialization, while an offline retraining strategy is utilized to update the controller parameters for incremental learning. The experiments demonstrate that the proposed RL controller can track a moving target accurately (with RMSE of 1.26 mm), and accommodate to external varying load effectively (with ~30\% RMSE reduction after retraining). Comparisons among the proposed RL controller and other supervised-learning-based controllers in handling additional tip load were also conducted. The results support that our RL method is appropriate for automatic learning such that there is no need of manual interference for data processing, particularly in cases with external disturbances and actuation redundancy.},
  archive   = {C_IROS},
  author    = {Yingqi Li and Xiaomei Wang and Ka-Wai Kwok},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981335},
  pages     = {7074-7081},
  title     = {Towards adaptive continuous control of soft robotic manipulator using reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SKILL-IL: Disentangling skill and knowledge in multitask
imitation learning. <em>IROS</em>, 7060–7065. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we introduce a new perspective for learning transferable content in multi-task imitation learning. Humans are capable of transferring skills and knowledge. If we can cycle to work and drive to the store, we can also cycle to the store and drive to work. We take inspiration from this and hypothesize the latent memory of a policy network can be disentangled into two partitions. These contain either the knowledge of the environmental context for the task or the generalisable skill needed to solve the task. This allows an improved training efficiency and better generalization over previously unseen combinations of skills in the same environment, and the same task in unseen environments. We used the proposed approach to train a disentangled agent for two different multi-task IL environments. In both cases, we out-performed the SOTA by 30\% in task success rate. We also demonstrated this for navigation on a real robot.},
  archive   = {C_IROS},
  author    = {Bian Xihan and Oscar Mendez and Simon Hadfield},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981375},
  pages     = {7060-7065},
  title     = {SKILL-IL: Disentangling skill and knowledge in multitask imitation learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Renaissance robot: Optimal transport policy fusion for
learning diverse skills. <em>IROS</em>, 7052–7059. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep reinforcement learning (RL) is a promising approach to solving complex robotics problems. However, the process of learning through trial-and-error interactions is often highly time-consuming, despite recent advancements in RL algorithms. Additionally, the success of RL is critically dependent on how well the reward-shaping function suits the task, which is also time-consuming to design. As agents trained on a variety of robotics problems continue to proliferate, the ability to reuse their valuable learning for new domains becomes increasingly significant. In this paper, we propose a post-hoc technique for policy fusion using Optimal Transport theory as a robust means of consolidating the knowledge of multiple agents that have been trained on distinct scenarios. We further demonstrate that this provides an improved weights initialisation of the neural network policy for learning new tasks, requiring less time and computational resources than either retraining the parent policies or training a new policy from scratch. Ultimately, our results on diverse agents commonly used in deep RL show that specialised knowledge can be unified into a “Renaissance agent”, allowing for quicker learning of new skills.},
  archive   = {C_IROS},
  author    = {Julia Tan and Ransalu Senanayake and Fabio Ramos},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981105},
  pages     = {7052-7059},
  title     = {Renaissance robot: Optimal transport policy fusion for learning diverse skills},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DRL-ISP: Multi-objective camera ISP with deep reinforcement
learning. <em>IROS</em>, 7044–7051. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a multi-objective camera ISP framework that utilizes Deep Reinforcement Learning (DRL) and camera ISP toolbox that consist of network-based and conventional ISP tools. The proposed DRL-based camera ISP framework iteratively selects a proper tool from the toolbox and applies it to the image to maximize a given vision task-specific reward function. For this purpose, we implement total 51 ISP tools that include exposure correction, color-and-tone correction, white balance, sharpening, denoising, and the others. We also propose an efficient DRL network architecture that can extract the various aspects of an image and make a rigid mapping relationship between images and a large number of actions. Our proposed DRL-based ISP framework effectively improves the image quality according to each vision task such as RAW-to-RGB image restoration, 2D object detection, and monocular depth estimation.},
  archive   = {C_IROS},
  author    = {Ukcheol Shin and Kyunghyun Lee and In So Kweon},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981361},
  pages     = {7044-7051},
  title     = {DRL-ISP: Multi-objective camera ISP with deep reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Autonomous control of redundant hydraulic manipulator using
reinforcement learning with action feedback. <em>IROS</em>, 7036–7043.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This article presents an entirely data-driven approach for autonomous control of redundant manipulators with hydraulic actuation. The approach only requires minimal system information, which is inherited from a simulation model. The non-linear hydraulic actuation dynamics are modeled using actuator networks from the data gathered during the manual operation of the manipulator to effectively emulate the real system in a simulation environment. A neural network control policy for autonomous control, based on end-effector (EE) position tracking is then learned using Reinforcement Learning (RL) with Ornstein-Uhlenbeck process noise (OUNoise) for efficient exploration. The RL agent also receives feedback based on supervised learning of the forward kinematics which facilitates selecting the best suitable action from exploration. The control policy directly provides the joint variables as outputs based on provided target EE position while taking into account the system dynamics. The joint variables are then mapped to the hydraulic valve commands, which are then fed to the system without further modifications. The proposed approach is implemented on a scaled hydraulic forwarder crane with three revolute and one prismatic joint to track the desired position of the EE in 3-Dimensional (3D) space. With the emulated dynamics and extensive learning in simulation, the results demonstrate the feasibility of deploying the learned controller directly on the real system.},
  archive   = {C_IROS},
  author    = {Rohit Dhakate and Christian Brommer and Christoph Bohm and Harald Gietler and Stephan Weiss and Jan Steinbrener},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981425},
  pages     = {7036-7043},
  title     = {Autonomous control of redundant hydraulic manipulator using reinforcement learning with action feedback},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Continuous self-localization on aerial images using visual
and lidar sensors. <em>IROS</em>, 7028–7035. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a novel method for geo-tracking, i.e. continuous metric self-localization in outdoor environments by registering a vehicle&#39;s sensor information with aerial imagery of an unseen target region. Geo- tracking methods offer the potential to supplant noisy signals from global navigation satellite systems (GNSS) and expensive and hard to maintain prior maps that are typically used for this purpose. The proposed geo-tracking method aligns data from on-board cameras and lidar sensors with geo-registered orthophotos to continuously localize a vehicle. We train a model in a metric learning setting to extract visual features from ground and aerial images. The ground features are projected into a top-down perspective via the lidar points and are matched with the aerial features to determine the relative pose between vehicle and orthophoto. Our method is the first to utilize on-board cameras in an end-to-end differentiable model for metric self-localization on unseen orthophotos. It exhibits strong generalization, is robust to changes in the environment and requires only geo-poses as ground truth. We evaluate our approach on the KITTI-360 dataset and achieve a mean absolute position error (APE) of 0.94m. We further compare with previous approaches on the KITTI odometry dataset and achieve state-of-the-art results on the geo-tracking task. 3},
  archive   = {C_IROS},
  author    = {Florian Fervers and Sebastian Bullinger and Christoph Bodensteiner and Michael Arens and Rainer Stiefelhagen},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982195},
  pages     = {7028-7035},
  title     = {Continuous self-localization on aerial images using visual and lidar sensors},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual-inertial SLAM with tightly-coupled dropout-tolerant
GPS fusion. <em>IROS</em>, 7020–7027. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic applications are continuously striving towards higher levels of autonomy. To achieve that goal, a highly robust and accurate state estimation is indispensable. Combining visual and inertial sensor modalities has proven to yield accurate and locally consistent results in short-term applications. Unfortunately, visual-inertial state estimators suffer from the accumulation of drift for long-term trajectories. To eliminate this drift, global measurements can be fused into the state estimation pipeline. The most known and widely available source of global measurements is the Global Positioning System (GPS). In this paper, we propose a novel approach that fully combines stereo Visual-Inertial Simultaneous Localisation and Mapping (SLAM), including visual loop closures, with the fusion of global sensor modalities in a tightly-coupled and optimisation-based framework. Incorporating measurement uncertainties, we provide a robust criterion to solve the global reference frame initialisation problem. Furthermore, we propose a loop-closure-like optimisation scheme to compensate drift accumulated during outages in receiving GPS signals. Experimental validation on datasets and in a real-world experiment demonstrates the robustness of our approach to GPS dropouts as well as its capability to estimate highly accurate and globally consistent trajectories compared to existing state-of-the-art methods.},
  archive   = {C_IROS},
  author    = {Simon Boche and Xingxing Zuo and Simon Schaefer and Stefan Leutenegger},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981134},
  pages     = {7020-7027},
  title     = {Visual-inertial SLAM with tightly-coupled dropout-tolerant GPS fusion},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards holistic autonomous obstacle detection in railways
by complementing of on-board vision with UAV-based object localization.
<em>IROS</em>, 7012–7019. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents the two sub-systems of the first holistic system for autonomous obstacle detection (OD) in railways, the on-board vision system and unmanned aerial vehicle (UAV) vision system for object localization (OL) on and near the rail tracks. The main goal of such a holistic system is to enable long-range detection of obstacles on the rail tracks ahead of the train where the UAV-based OL system complements the on-board system in detecting obstacles in the areas that are not visible by the on-board system such as curves. The deep learning (DL)-based object detection and distance estimation in thermal camera unit of the on-board system is presented, as well as the OL based on UAV camera image. The experimental results achieved in a real-world railway experimental scenario that includes obstacles visible only by the on-board thermal camera, only by UAV camera as well as obstacles in the Field of View (FoV) of both systems are presented. These preliminary results show the high potential of developing holistic system where the final decision on OD would be autonomously made and consequently possible actions for the train control would be suggested based on the OD outputs of individual systems having different rail tracks sections in their FoV.},
  archive   = {C_IROS},
  author    = {Marten Franke and Chaitra Reddy and Danijela Ristić-Durrant and Jehan Jayawardana and Kai Michels and Milan Banić and Miloš Simonović},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981156},
  pages     = {7012-7019},
  title     = {Towards holistic autonomous obstacle detection in railways by complementing of on-board vision with UAV-based object localization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BoxGraph: Semantic place recognition and pose estimation
from 3D LiDAR. <em>IROS</em>, 7004–7011. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper is about extremely robust and lightweight localisation using LiDAR point clouds based on instance segmentation and graph matching. We model 3D point clouds as fully-connected graphs of semantically identified components where each vertex corresponds to an object instance and encodes its shape. Optimal vertex association across graphs allows for full 6-Degree-of-Freedom (DoF) pose estimation and place recognition by measuring similarity. This representation is very concise, condensing the size of maps by a factor of 25 against the state-of-the-art, requiring only 3 kB to represent a 1.4 MB laser scan. We verify the efficacy of our system on the SemanticKITTI dataset, where we achieve a new state-of-the-art in place recognition, with an average of 88.4\% recall at 100\% precision where the next closest competitor follows with 64.9\%. We also show accurate metric pose estimation performance - estimating 6-DoF pose with median errors of 10cm and 0.33 deg.},
  archive   = {C_IROS},
  author    = {Georgi Pramatarov and Daniele De Martini and Matthew Gadd and Paul Newman},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981266},
  pages     = {7004-7011},
  title     = {BoxGraph: Semantic place recognition and pose estimation from 3D LiDAR},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new dense hybrid stereo visual odometry approach.
<em>IROS</em>, 6998–7003. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual odometry is an important part of the perception module of autonomous robots. Recent advances in deep learning approaches have given rise to hybrid visual odometry approaches that combine both deep networks and traditional pose estimation methods. One limitation of deep learning approaches is the availability of ground truth data needed to train the neural networks. For example, it is extremely difficult, if not impossible, to obtain a ground truth dense depth map of the environment to be used for stereo visual odometry. Even if unsupervised training of networks has been investigated, supervised training remains more reliable and robust. In this paper, we propose a new hybrid dense stereo visual odometry approach in which a dense depth map is obtained with a network that is supervised using ground truth poses that can be more easily obtained than ground truth depths maps. The depth map obtained from the neural network is used to warp the current image into the reference frame and the optimal pose is obtained by minimizing a cost function that encodes the similarity between the warped image and the reference image. The experimental results show that the proposed approach, not only improves state-of-the-art depth maps estimation networks on some of the standard benchmark datasets, but also outperforms the state-of-the-art visual odometry methods.},
  archive   = {C_IROS},
  author    = {Ziming Liu and Ezio Malis and Philippe Martinet},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981814},
  pages     = {6998-7003},
  title     = {A new dense hybrid stereo visual odometry approach},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CGiS-net: Aggregating colour, geometry and implicit semantic
features for indoor place recognition. <em>IROS</em>, 6991–6997. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We describe a novel approach to indoor place recognition from RGB point clouds based on aggregating low-level colour and geometry features with high-level implicit semantic features. It uses a 2-stage deep learning framework, in which the first stage is trained for the auxiliary task of semantic segmentation and the second stage uses features from layers in the first stage to generate discriminate descriptors for place recognition. The auxiliary task encourages the features to be semantically meaningful, hence aggregating the geometry and colour in the RGB point cloud data with implicit semantic information. We use an indoor place recognition dataset derived from the ScanNet dataset for training and evaluation, with a test set comprising 3,608 point clouds generated from 100 different rooms. Comparison with a traditional feature-based method and four state-of-the-art deep learning methods demonstrate that our approach significantly outperforms all five methods, achieving, for example, a top-3 average recall rate of 75\% compared with 41\% for the closest rival method. Our code is available at: https://github.com/YuhangMing/Semantic-Indoor-Place-Recognition},
  archive   = {C_IROS},
  author    = {Yuhang Ming and Xingrui Yang and Guofeng Zhang and Andrew Calway},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981113},
  pages     = {6991-6997},
  title     = {CGiS-net: Aggregating colour, geometry and implicit semantic features for indoor place recognition},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learnable spatio-temporal map embeddings for deep inertial
localization. <em>IROS</em>, 6984–6990. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Indoor localization systems often fuse inertial odometry with map information via hand-defined methods to reduce odometry drift, but such methods are sensitive to noise and struggle to generalize across odometry sources. To address the robustness problem in map utilization, we propose a data-driven prior on possible user locations in a map by combining learned spatial map embeddings and temporal odometry embeddings. Our prior learns to encode which map regions are feasible locations for a user more accurately than previous hand-defined methods. This prior leads to a 49\% improvement in inertial-only localization accuracy when used in a particle filter. This result is significant, as it shows that our relative positioning method can match the performance of absolute positioning using bluetooth beacons. To show the gen-eralizability of our method, we also show similar improvements using wheel encoder odometry. Our code will be made publicly available † 1 project page: https://rebrand.ly/learned-map-prior.},
  archive   = {C_IROS},
  author    = {Dennis Melamed and Karnik Ram and Vivek Roy and Kris Kitani},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981092},
  pages     = {6984-6990},
  title     = {Learnable spatio-temporal map embeddings for deep inertial localization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VMVG-loc: Visual localization for autonomous driving using
vector map and voxel grid map. <em>IROS</em>, 6976–6983. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study proposes a visual localization method using a vector map and voxel grid map with a stereo camera. The two maps provide different modality advantages and are integrated using a particle filter. In contrast to other vector map-based methods, our method does not use road markings because creating and maintaining vector maps that include high-accuracy road markings is laborious. Furthermore, it limits the regions where they are available. This method uses only lane center-lines from vector maps, which are easier to create than road markings. The method performs ray casting and computes the reprojection error to evaluate the vehicle position for voxel grid maps. Although this makes the method environmentally sensitive, the constraints by lanes make the estimation stable. Experiments confirmed that the method could perform localization stably and accurately without failure even over long distances. In addition, an ablation study showed the benefits of combining both maps.},
  archive   = {C_IROS},
  author    = {Kento Yabuuchi and Shinpei Kato},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981776},
  pages     = {6976-6983},
  title     = {VMVG-loc: Visual localization for autonomous driving using vector map and voxel grid map},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FSM: Correspondenceless scan-matching of panoramic 2D range
scans. <em>IROS</em>, 6968–6975. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent years have seen the introduction of more affordable but less accurate 2D range sensors whose field of view is $2\pi$ . Scan-matching with these has been insufficiently researched, while being a challenge due to these sensors&#39; increased measurement uncertainty. This paper proposes a real-time method for matching scans extracted from panoramic 2D LIDAR sensors. The method leverages properties of the Fourier transform which arise due to the periodicity of the range signal. Matching is performed in a correspondenceless manner. The proposed method outperforms established scan-matching methods in terms of pose accuracy and robustness in tests on public domain data, and over noise levels of commercially available sensors. The source code is available for download.},
  archive   = {C_IROS},
  author    = {Alexandros Filotheou and Georgios D. Sergiadis and Antonis G. Dimitriou},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981228},
  pages     = {6968-6975},
  title     = {FSM: Correspondenceless scan-matching of panoramic 2D range scans},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Autonomous intraluminal navigation of a soft robot using
deep-learning-based visual servoing. <em>IROS</em>, 6952–6959. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Navigation inside luminal organs is an arduous task that requires non-intuitive coordination between the movement of the operator&#39;s hand and the information obtained from the endoscopic video. The development of tools to automate certain tasks could alleviate the physical and mental load of doctors during interventions allowing them to focus on diagnosis and decision-making tasks. In this paper we present a synergic solution for intraluminal navigation consisting of a 3D printed endoscopic soft robot that can move safely inside luminal structures. Visual servoing based on Convolutional Neural Networks (CNNs) is used to achieve the autonomous navigation task. The CNN is trained with phantoms and in-vivo data to segment the lumen and a model-less approach is presented to control the movement in constrained environments. The proposed robot is validated in anatomical phantoms in different path configurations. We analyze the movement of the robot using different metrics such as task completion time smoothness error in the steady-state mean and maximum error. We show that our method is suitable to navigate safely in hollow environments and conditions which are different than the ones the network was originally trained on.},
  archive   = {C_IROS},
  author    = {Jorge F. Lazo and Chun-Feng Lai and Sara Moccia and Benoit Rosa and Michele Catellani and Michel de Mathelin and Giancarlo Ferrigno and Paul Breedveld and Jenny Dankelman and Elena De Momi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982141},
  pages     = {6952-6959},
  title     = {Autonomous intraluminal navigation of a soft robot using deep-learning-based visual servoing},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Development of a 6 DOF soft robotic manipulator with
integrated sensing skin. <em>IROS</em>, 6944–6951. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a new 6 DOF soft robotic manipulator intended for colorectal surgery. The manipulator, based on a novel design that employs an inextensible tube to limit axial extension, is shown to maximize the force exerted at its tip and the bending angle, the latter being measured with a soft sensing skin. Manufacturing of the prototype is achieved with a lost-wax silicone-casting technique. The kinematic model of the manipulator, its workspace, and its manipulability are discussed. The prototype is evaluated with extensive experiments, including pressure-deflection measurement with and without tip load, and lateral force measurements with and without the soft sensing skin to assess hysteresis. The experimental results indicate that the prototype fulfils the key design requirements for colorectal surgery: (i) it can generate sufficient force to perform a range of laparoscopic tasks; (ii) the workspace is commensurate with the dimensions of the large intestine; (iii) the soft sensing skin only results in a marginal reduction of the maximum tip rotation within the range of pressures and external loads relevant for the chosen application.},
  archive   = {C_IROS},
  author    = {Shen Treratanakulchai and Enrico Franco and Arnau Garriga-Casanovas and Hu Minghao and Panagiotis Kassanos and Ferdinando Rodriguez y Baena},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981437},
  pages     = {6944-6951},
  title     = {Development of a 6 DOF soft robotic manipulator with integrated sensing skin},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sensor-based reconstruction of slender flexible beams
undergoing large-scale deflection. <em>IROS</em>, 6936–6943. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a model-based approach to reconstructing the large deformations of slender flexible beams through strain-gauge deflection sensors. Using the principal axes decomposition of structural compliance, a closed-form kinetostatics model can be obtained to characterize the non-linear force-deformation behavior of the flexible beams under-going large-scale deflection. Owing to analytical derivation of the system Jacobian, the efficient Newton-Raphson method is employed to determine the equilibrium configuration of the flexible beams, as well as the corresponding reaction force. To verify the correctness and effectiveness of the proposed method, an experimental apparatus is built up, on which a variety of experiments are conducted. The results show that for a 300 mm long beam, the tip position can be predicted with an accuracy of 1.27 mm, 4.42 mm, and 1.17°, respectively, for the x, y directions and rotation. Accordingly, the estimation errors for the planar forces and torque are 0.075 N (3.33\%), 0.155 N (14.23\%), and 0.027 Nm (26.84\%), respectively.},
  archive   = {C_IROS},
  author    = {Junjie Luo and Yuanhao Xun and Jiaji Yao and Genliang Chen and Hao Wang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981608},
  pages     = {6936-6943},
  title     = {Sensor-based reconstruction of slender flexible beams undergoing large-scale deflection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A soft fluidic sensor-actuator for active sensing of force
and displacement in biomedical applications. <em>IROS</em>, 6913–6919.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Achieving compact and biocompatible actuators with sensing capabilities is a key challenge for the safety critical and highly patient-specific biomedical field. In this study, a compact and versatile soft fluidic sensor-actuator capable of measuring both force and displacement in static and dynamic conditions is presented. Pressure and resistance are shown to be interchangeable in predicting load and sensor-actuator height, and showed good repeatability and distinction between the loaded and constrained conditions tested. Furthermore the sensor-actuator is demonstrated in a probe application and showed comparable findings to a tensile test machine when tested on three objects of varying stiffness. Overall, this sensor-actuator has the potential to be a key building block for biomedical robots that require large expansion, as well as continuous monitoring of both displacement and force.},
  archive   = {C_IROS},
  author    = {Joanna Jones and Dana D. Damian},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981059},
  pages     = {6913-6919},
  title     = {A soft fluidic sensor-actuator for active sensing of force and displacement in biomedical applications},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Embeddable coiled soft sensor-based joint angle sensing for
flexible surgical manipulator. <em>IROS</em>, 6906–6912. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tendon-driven flexible endoscopic surgical robots have been developed to access narrow curved paths without incision. Robot shape information is essential for precise control and to prevent unwanted tissue damage. In this paper, we propose a joint angle sensing method using coiled soft sensors to estimate the shape of the hyperredundant manipulator, which is commonly used in flexible endoscopic surgical robots. The soft sensors can be fabricated with small size and are highly stretchable, such that by being pre-stretched, they can be integrated between individual joints, maintain a center hollow, and sense both compression and extension. The pre-stretch length is experimentally selected by using the sensor linearity to maximize the potential sensitivity. We validated the proposed design using a two-degree of freedom (DOF) single joint manipulator by implementing two sensors; sensors at all joints could sense joint angle independently and simultaneously with a root-mean-square error (RMSE) less than 2.53°. Based on the proposed method, a two-DOF configuration of the hyperredundant manipulator that can be used in real applications was achieved, following a constant curvature model in real time with values RMSE of 2.30° and 2.63°, for pitch and yaw joint angle respectively.},
  archive   = {C_IROS},
  author    = {Yesung Yi and Jung-Hwan Youn and Ki-Uk Kyung and Dong-Soo Kwon},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981505},
  pages     = {6906-6912},
  title     = {Embeddable coiled soft sensor-based joint angle sensing for flexible surgical manipulator},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-morphing soft parallel-and-coplanar electroadhesive
grippers based on laser-scribed graphene oxide electrodes.
<em>IROS</em>, 6900–6905. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Electroadhesion is a versatile and controllable adhesion mechanism that has been used extensively in robotics. Soft electroadhesion embodies electrostatic adhesion in soft materials and is required for shape-adaptive and safe grasping of curved objects and delicate materials. In this work, we present a soft electroadhesive fabrication method based on laser scribing graphene oxide on a silicone film, which is cost-effective, facile and green. The method can be used to generate complex electroadhesive patterns without molds or stencils. We then present a 2D finite element model to demonstrate the shape-changing behavior and electric field distributions of a dual-mode parallel dielectric elastomer actuation and coplanar electroadhesion structure. The soft electroadhesive fabrication method based on laser-scribed graphene oxide electrodes and its experimental characterization results, together with its shape-morphing simulation model are expected to enable the wider adoption of soft electroadhesion in future robotics.},
  archive   = {C_IROS},
  author    = {Jianglong Guo and Djen Kuhnel and Qiukai Qi and Chaoqun Xiang and Van Anh Ho and Charl Faul and Jonathan Rossiter},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981267},
  pages     = {6900-6905},
  title     = {Self-morphing soft parallel-and-coplanar electroadhesive grippers based on laser-scribed graphene oxide electrodes},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An underwater target perception framework for underwater
operation scene. <em>IROS</em>, 6894–6899. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes an underwater target perception framework to comprehensively explore target information in underwater scenes, to improve the work efficiency and safety of underwater operations. This framework adopts a layered processing mechanism including water column imaging, constant false alarm rate detection (CFAR) detection, and local feature analysis, to accurately distinguish between false targets, static targets, and dynamic targets in the underwater scene, and obtain the motion trajectory of dynamic targets. The experiment is designed to simulate the underwater operation scene, and the results prove the effectiveness of the proposed framework.},
  archive   = {C_IROS},
  author    = {Jue Gao and Chi Zhu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981170},
  pages     = {6894-6899},
  title     = {An underwater target perception framework for underwater operation scene},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Training dynamic motion primitives using deep reinforcement
learning to control a robotic tadpole. <em>IROS</em>, 6881–6887. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Developing a good control strategy for biomimetic robots is challenging. Robust control methods require an accurate model of the robot. Nowadays, model-free methods are being extensively explored for the control and navigation of terrestrial robots. In this paper, we consider a novel deep reinforcement learning-based model-free swimming control for our bio-inspired robotic tadpole. To realize this, we utilize dynamic motion primitives, which can represent a large range of motion behaviors, and combine them with a decoupled reinforcement learning framework. The proposed architecture optimizes the motion primitives first to develop a travelling wave undulation pattern in the tail and then to navigate the robot along different predefined paths. Through this framework, effective swimming gait emerges, and the robot is able to navigate well on the surface of water. This framework combines the optimization potential of deep reinforcement learning with stability and generalization properties of dynamic motion primitives. We train and test our method on a simulated model of the robot to demonstrate the effectiveness of the method and also conduct experimental testing on the real robot to verify the results.},
  archive   = {C_IROS},
  author    = {Imran Hameed and Xu Chao and David Navarro-Alarcon and Xingjian Jing},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981112},
  pages     = {6881-6887},
  title     = {Training dynamic motion primitives using deep reinforcement learning to control a robotic tadpole},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Motion attribute-based clustering and collision avoidance of
multiple in-water obstacles by autonomous surface vehicle.
<em>IROS</em>, 6873–6880. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Navigation and obstacle avoidance in aquatic en-vironments for autonomous surface vehicles (ASVs) in high-traffic maritime scenarios is still an open challenge, as the Convention on the International Regulations for Preventing Collisions at Sea (COLREGs) is not defined for multi-encounter situations. Current state-of-the-art methods resolve single-to-single encounters with sequential actions and assume that other obstacles follow COLREGs. Our work proposes a novel real-time non-myopic obstacle avoidance method, allowing an ASV that has only partial knowledge of the surroundings within the sensor radius to navigate in high-traffic maritime scenarios. Specifically, we achieve a holistic view of the feasible ASV action space able to avoid deadlock scenarios, by proposing (1) a clustering method based on motion attributes of other obstacles, (2) a geometric framework for identifying the feasible action space, and (3) a multi-objective optimization to determine the best action. Theoretical analysis and extensive realistic exper-iments in simulation considering real-world traffic scenarios demonstrate that our proposed real-time obstacle avoidance method is able to achieve safer trajectories than other state-of-the-art methods and that is robust to uncertainty present in the current information available to the ASV.},
  archive   = {C_IROS},
  author    = {Mingi Jeong and Alberto Quattrini Li},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981226},
  pages     = {6873-6880},
  title     = {Motion attribute-based clustering and collision avoidance of multiple in-water obstacles by autonomous surface vehicle},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inertial-measurement-based catenary shape estimation of
underwater cables for tethered robots. <em>IROS</em>, 6867–6872. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper deals with the estimation of the shape of a catenary for a negatively buoyant cable, connecting a pair of underwater robots in a robot chain. The new estimation method proposed here is based on the calculation of local tangents thanks to the data acquired from inertial measurement units (IMUs), which are attached to the cable near its ends. This method is compared with a vision-based estimation method that was developed previously. Experiments are conducted, in the air and in a pool, using a motion capture system for ground truth. The results obtained show that the new method significantly improves the estimation of the catenary height. Furthermore, the identification of the cable shape is not affected by the limits of the camera&#39;s field of view and by the image projection, resulting in increased accuracy and range, without singularities.},
  archive   = {C_IROS},
  author    = {Juliette Drupt and Claire Dune and Andrew I. Comport and Sabine Seillier and Vincent Hugel},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981980},
  pages     = {6867-6872},
  title     = {Inertial-measurement-based catenary shape estimation of underwater cables for tethered robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Development and field testing of an optimal path following
ASV controller for marine surveys. <em>IROS</em>, 6861–6866. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Marine autonomous vehicles deployed to conduct marine geophysical surveys are becoming an increasingly used asset in the commercial, academic, and defense industries. However, the ability to collect high-quality data from applicable sensors is directly related to the robustness of vehicle motion caused by environmental disturbances. In this paper we designed and integrated a new path following controller on an autonomous surface vehicle (ASV) that minimizes the linear and angular accelerations on the sensor&#39;s local frame. Simulation and experimental results verify reduction of vehicle motion, improvement in path following, and improvement in preliminary sonar data quality compared to that of the existing proportional-yaw path following controller.},
  archive   = {C_IROS},
  author    = {Kleio Baxevani and Grant E. Otto and Herbert G. Tanner and Arthur C. Trembanis},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981691},
  pages     = {6861-6866},
  title     = {Development and field testing of an optimal path following ASV controller for marine surveys},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Flexible collision-free platooning method for unmanned
surface vehicle with experimental validations. <em>IROS</em>, 6854–6860.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the flexible formation problem for unmanned surface vehicles in the presence of obstacles. Building upon the leader-follower formation scheme, a hybrid line-of-sight based flexible platooning method is proposed for follower vehicle to keep tracking the leader ship. A fusion artificial potential field collision avoidance approach is tailored to generate optimal collision-free trajectories for the vehicle to track. To steer the vehicle towards and stay within the neighborhood of the generated collision-free trajectory, a nonlinear model predictive controller is designed. Experimental results are presented to validate the efficiency of proposed method, showing that the unmanned surface vehicle is able to track the leader ship without colliding with the surrounded static obstacles in the considered experiments.},
  archive   = {C_IROS},
  author    = {Bin Du and Bin Lin and Wei Xie and Weidong Zhang and Rudy R. Negenborn and Yusong Pang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981043},
  pages     = {6854-6860},
  title     = {Flexible collision-free platooning method for unmanned surface vehicle with experimental validations},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HyperPocket: Generative point cloud completion.
<em>IROS</em>, 6848–6853. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Scanning real-life scenes with modern registration devices typically give incomplete point cloud representations, mostly due to the limitations of the scanning process and 3D occlusions. Therefore, completing such partial representations remains a fundamental challenge of many computer vision applications. Most of the existing approaches aim to solve this problem by learning to reconstruct individual 3D objects in a synthetic setup of an uncluttered environment, which is far from a real-life scenario. In this work, we reformulate the problem of point cloud completion into an objects hallucination task. Thus, we introduce a novel autoencoder-based architecture called HyperPocket that disentangles latent representations and, as a result, enables the generation of multiple variants of the completed 3D point clouds. Furthermore, we split point cloud processing into two disjoint data streams and leverage a hypernetwork paradigm to fill the spaces, dubbed pockets, that are left by the missing object parts. As a result, the generated point clouds are smooth, plausible, and geometrically consistent with the scene. Moreover, our method offers competitive performances to the other state-of-the-art models, enabling a plethora of novel applications.},
  archive   = {C_IROS},
  author    = {P. Spurek and A. Kasymov and M. Mazur and D. Janik and S.K. Tadeja and Ł. Struski and J. Tabor and T. Trzciński},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981829},
  pages     = {6848-6853},
  title     = {HyperPocket: Generative point cloud completion},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised simultaneous learning for camera
re-localization and depth estimation from video. <em>IROS</em>,
6840–6847. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an unsupervised simultaneous learning framework for the task of monocular camera re-localization and depth estimation from unlabeled video sequences. Monocular camera re-localization refers to the task of estimating the absolute camera pose from an instance image in a known environment, which has been intensively studied for alternative localization in GPS-denied environments. In recent works, cam-era re-localization methods are trained via supervised learning from pairs of camera images and camera poses. In contrast to previous works, we propose a completely unsupervised learning framework for camera re-localization and depth estimation, requiring only monocular video sequences for training. In our framework, we train two networks that estimate the scene coordinates using directions and the depth map from each image which are then combined to estimate the camera pose. The networks can be trained through the minimization of loss functions based on our loop closed view synthesis. In experiments with the 7-scenes dataset, the proposed method outperformed the re-localization of the state-of-the-art visual SLAM, ORB-SLAM3. Our method also outperforms state-of-the-art monocular depth estimation in a trained environment.},
  archive   = {C_IROS},
  author    = {Shun Taguchi and Noriaki Hirose},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982213},
  pages     = {6840-6847},
  title     = {Unsupervised simultaneous learning for camera re-localization and depth estimation from video},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning important regions via attention for video streaming
on cloud robotics. <em>IROS</em>, 6833–6839. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cloud robotics, i.e., controlling robots from the cloud, make it possible to perform more complex processes, make robots smaller, and coordinate multi-robots by sharing information between robots and utilizing abundant computing resources. In cloud robotics, robots need to transmit videos to the cloud in real time to recognize their surroundings. Lowering the video quality reduces the bitrate in low bandwidth environments; however, this may lead to control errors and misrecognition due to lack of detailed image features. Even with 5G, bandwidth fluctuates widely, especially in moving robots, making it difficult to upload high quality video consistently. To reduce bitrate while preserving Quality of Control (QoC), we propose a method of learning the important regions for a pretrained autonomous agent using self-attention, and transmitting the video to the agent by controlling the image quality of each region on the basis of the estimated importance. The evaluation results demonstrate that our approach can maintain QoC while reducing the bitrate to 26\% by setting important regions to high quality and the rest to low quality.},
  archive   = {C_IROS},
  author    = {Hayato Itsumi and Florian Beye and Vitthal Charvi and Koichi Nihei},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981132},
  pages     = {6833-6839},
  title     = {Learning important regions via attention for video streaming on cloud robotics},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Application of ghost-DeblurGAN to fiducial marker detection.
<em>IROS</em>, 6827–6832. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Feature extraction or localization based on the fiducial marker could fail due to motion blur in real-world robotic applications. To solve this problem, a lightweight generative adversarial network, named Ghost-DeblurGAN, for real-time motion deblurring is developed in this paper. Furthermore, on account that there is no existing deblurring benchmark for such task, a new large-scale dataset, York-Tag, is proposed that provides pairs of sharp/blurred images containing fiducial markers. With the proposed model trained and tested on YorkTag, it is demonstrated that when applied along with fiducial marker systems to motion-blurred images, Ghost-DeblurGAN improves the marker detection significantly. The datasets and codes used in this paper are available at: https://github.com/York-SDCNLab/Ghost-DeblurGAN.},
  archive   = {C_IROS},
  author    = {Yibo Liu and Amaldev Haridevan and Hunter Schofield and Jinjun Shan},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981701},
  pages     = {6827-6832},
  title     = {Application of ghost-DeblurGAN to fiducial marker detection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LiSnowNet: Real-time snow removal for LiDAR point clouds.
<em>IROS</em>, 6820–6826. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Light Detection And Rangings (LiDARs) have been widely adopted to modern self-driving vehicles, providing 3D information of the scene and surrounding objects. However, adverser weather conditions still pose significant challenges to LiDARs since point clouds captured during snowfall can easily be corrupted. The resulting noisy point clouds degrade downstream tasks such as mapping. Existing works in de-noising point clouds corrupted by snow are based on nearest-neighbor search, and thus do not scale well with modern LiDARs which usually capture 100k or more points at 10Hz. In this paper, we introduce an unsupervised de-noising algorithm, LiSnowNet, running 52 x faster than the state-of-the-art methods while achieving superior performance in de-noising. Unlike previous methods, the proposed algorithm is based on a deep convolutional neural network and can be easily deployed to hardware accelerators such as GPUs. In addition, we demonstrate how to use the proposed method for mapping even with corrupted point clouds.},
  archive   = {C_IROS},
  author    = {Ming-Yuan Yu and Ram Vasudevan and Matthew Johnson-Roberson},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982248},
  pages     = {6820-6826},
  title     = {LiSnowNet: Real-time snow removal for LiDAR point clouds},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ParaPose: Parameter and domain randomization optimization
for pose estimation using synthetic data. <em>IROS</em>, 6788–6795. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pose estimation is the task of determining the 6D position of an object in a scene. Pose estimation aid the abilities and flexibility of robotic set-ups. However, the system must be configured towards the use case to perform adequately. This configuration is time-consuming and limits the usability of pose estimation and, thereby, robotic systems. Deep learning is a method to overcome this configuration procedure by learning parameters directly from the dataset. However, obtaining this training data can also be very time-consuming. The use of synthetic training data avoids this data collection problem, but a configuration of the training procedure is necessary to overcome the domain gap problem. Additionally, the pose estimation parameters also need to be configured. This configuration is jokingly known as grad student descent as parameters are manually adjusted until satisfactory results are obtained. This paper presents a method for automatic configuration using only synthetic data. This is accomplished by learning the domain randomization during network training, and then using the domain randomization to optimize the pose estimation parameters. The developed approach shows state-of-the-art performance of 82.0\% recall on the challenging OCCLUSION dataset, outperforming all previous methods with a large margin. These results prove the validity of automatic set-up of pose estimation using purely synthetic data.},
  archive   = {C_IROS},
  author    = {Frederik Hagelskjær and Anders Glent Buch},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981511},
  pages     = {6788-6795},
  title     = {ParaPose: Parameter and domain randomization optimization for pose estimation using synthetic data},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Comparison of EKF-based floating base estimators for
humanoid robots with flat feet. <em>IROS</em>, 6780–6787. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Extended Kalman filtering is a common approach to achieve floating base estimation of a humanoid robot. These filters rely on measurements from an Inertial Measurement Unit (IMU) and relative forward kinematics for estimating the base position-and-orientation and its linear velocity along with the augmented states of feet position-and-orientation. We refer to such filters as flat-foot filters. However, the availability of only partial measurements often poses the question of consistency in the filter design. In this paper, we perform an experimental comparison of state-of-the-art flat-foot filters based on the representation choice of state, observation, matrix Lie group error and system dynamics evaluated for filter consistency and trajectory errors. The comparison is performed over simulated and real-world experiments conducted on the iCub humanoid platform. It is observed that filters on Lie groups that exploit properties of invariant filtering tend to perform better as consistent estimators while discrete-time filters in general provide higher accuracy along observable directions.},
  archive   = {C_IROS},
  author    = {Prashanth Ramadoss and Giulio Romualdi and Stefano Dafarra and Silvio Traversaro and Daniele Pucci},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982275},
  pages     = {6780-6787},
  title     = {Comparison of EKF-based floating base estimators for humanoid robots with flat feet},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved biped walking performance around the kinematic
singularities of biomimetic four-bar knees. <em>IROS</em>, 6774–6779.
(<a href="https://doi.org/10.1109/IROS47612.2022.9982027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies the effects of replacing pin-joint knees in passive dynamic bipedal walkers with biomimetic four-bar knees. The kinetic model of the four-bar knees is presented in detail, and an analytical model of the passive walking dynamics is derived. The resulting four-bar kneed biped is compared with a pin-joint kneed walker, for their passive walking performance. The geometry of the four-bar knees used in the study is based on human anatomical data. It is found that the biomimetic four-bar knee configuration works to the advantage of the biped, especially around the extended-knee singular position. The four-bar knees are found to overperform the pin-joint ones, resulting in significant reduction of peak impact loads and energetic expenditure.},
  archive   = {C_IROS},
  author    = {Aikaterini Smyrli and Evangelos Papadopoulos},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982027},
  pages     = {6774-6779},
  title     = {Improved biped walking performance around the kinematic singularities of biomimetic four-bar knees},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Resolved motion control for 3D underactuated bipedal walking
using linear inverted pendulum dynamics and neural adaptation.
<em>IROS</em>, 6761–6767. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a framework to generate periodic trajectory references for a 3D under-actuated bipedal robot, using a linear inverted pendulum (LIP) based controller with adaptive neural regulation. We use the LIP template model to estimate the robot&#39;s center of mass (CoM) position and velocity at the end of the current step, and formulate a discrete controller that determines the next footstep location to achieve a desired walking profile. This controller is equipped on the frontal plane with a Neural-Network-based adaptive term that reduces the model mismatch between the template and physical robot that particularly affects the lateral motion. Then, the foot placement location computed for the LIP model is used to generate task space trajectories (CoM and swing foot trajectories) for the actual robot to realize stable walking. We use a fast, real-time QP-based inverse kinematics algorithm that produces joint references from the task space trajectories, which makes the formulation independent of the knowledge of the robot dynamics. Finally, we implemented and evaluated the proposed approach in simulation and hardware experiments with a Digit robot obtaining stable periodic locomotion for both cases.},
  archive   = {C_IROS},
  author    = {Victor C. Paredes and Ayonga Hereid},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982009},
  pages     = {6761-6767},
  title     = {Resolved motion control for 3D underactuated bipedal walking using linear inverted pendulum dynamics and neural adaptation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Humanoid balance control using centroidal angular momentum
based on hierarchical quadratic programming. <em>IROS</em>, 6753–6760.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Maintaining balance to external pushes is one of the most important features for a humanoid to walk in a real environment. In particular, methods for counteracting to pushes using the centroidal angular momentum (CAM) control have been actively developed. In this paper, a CAM control scheme based on hierarchical quadratic programming (HQP) is proposed. The scheme of the CAM control consists of CAM tracking control and initial pose return control, which is hierarchically operated based on HQP to ensure the priority of CAM tracking performance. The proposed method is implemented in a capture point (CP) feedback control framework. Through simulations and experiments, the proposed method demonstrated more stable balance control performance than the previous method when the humanoid is walking in the presence of external perturbation.},
  archive   = {C_IROS},
  author    = {Myeong-Ju Kim and Daegyu Lim and Gyeongjae Park and Jaeheung Park},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981036},
  pages     = {6753-6760},
  title     = {Humanoid balance control using centroidal angular momentum based on hierarchical quadratic programming},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning dynamic bipedal walking across stepping stones.
<em>IROS</em>, 6746–6752. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we propose a learning approach for 3D dynamic bipedal walking when footsteps are constrained to stepping stones. While recent work has shown progress on this problem, real-world demonstrations have been limited to relatively simple open-loop, perception-free scenarios. Our main contribution is a more advanced learning approach that enables real-world demonstrations, using the Cassie robot, of closed-loop dynamic walking over moderately difficult stepping-stone patterns. Our approach first uses reinforcement learning (RL) in simulation to train a controller that maps footstep commands onto joint actions without any reference motion information. We then learn a model of that controller&#39;s capabilities, which enables prediction of feasible footsteps given the robot&#39;s current dynamic state. The resulting controller and model are then integrated with a real-time overhead camera system for detecting stepping stone locations. For evaluation, we develop a benchmark set of stepping stone patterns, which are used to test performance in both simulation and the real world. Overall, we demonstrate that sim-to-real learning is extremely promising for enabling dynamic locomotion over stepping stones. We also identify challenges remaining that motivate important future research directions.},
  archive   = {C_IROS},
  author    = {Helei Duan and Ashish Malik and Mohitvishnu S. Gadde and Jeremy Dao and Alan Fern and Jonathan Hurst},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981884},
  pages     = {6746-6752},
  title     = {Learning dynamic bipedal walking across stepping stones},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Uniform global exponential stabilizing passivity-based
tracking controller applied to planar biped robots. <em>IROS</em>,
6739–6745. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel control approach, based on the interconnection and damping-assignment passivity-based control (IDA-PBC), to achieve stable and periodic walking for underactuated planar biped robots with one degree of underactuation. The system&#39;s physical structure is preserved by assigning a target port-Hamiltonian dynamics to the closed-loop system, which also ensures passivity. The control design ensures that the tracking error to the desired periodic gait converges exponentially to zero, and the convergence rate can be adjusted via gain tuning. Besides, through the hybrid zero dynamics, the stability of the full-order system can be retrieved from the stability of the orbit created in a lower-dimensional manifold. The proposed approach is the first example of a tracking controller based on the IDA-PBC applied to underactuated biped robots. Numerical simulations on a five-link planar biped robot with unactuated ankles validate the approach and show the performance of the closed-loop system.},
  archive   = {C_IROS},
  author    = {Pierluigi Arpenti and Alejandro Donaire and Fabio Ruggiero and Vincenzo Lippiello},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981206},
  pages     = {6739-6745},
  title     = {Uniform global exponential stabilizing passivity-based tracking controller applied to planar biped robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust contact state estimation in humanoid walking gaits.
<em>IROS</em>, 6732–6738. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this article, we propose a deep learning frame-work that provides a unified approach to the problem of leg contact detection in humanoid robot walking gaits. Our formulation accomplishes to accurately and robustly estimate the contact state probability for each leg (i.e., stable or slip/no contact). The proposed framework employs solely propriocep-tive sensing and although it relies on simulated ground-truth contact data for the classification process, we demonstrate that it generalizes across varying friction surfaces and different legged robotic platforms and, at the same time, is readily transferred from simulation to practice. The framework is quantitatively and qualitatively assessed in simulation via the use of ground-truth contact data and is contrasted against state-of-the-art methods with an ATLAS, a NAO, and a TALOS humanoid robot. Furthermore, its efficacy is demonstrated in base estimation with a real TALOS humanoid. To reinforce further research endeavors, our implementation is offered as an open-source ROS/Python package, coined Legged Contact Detection (LCD).},
  archive   = {C_IROS},
  author    = {Stylianos Piperakis and Michael Maravgakis and Dimitrios Kanoulas and Panos Trahanias},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981354},
  pages     = {6732-6738},
  title     = {Robust contact state estimation in humanoid walking gaits},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Terrain-adaptive, ALIP-based bipedal locomotion controller
via model predictive control and virtual constraints. <em>IROS</em>,
6724–6731. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a gait controller for bipedal robots to achieve highly agile walking over various terrains given local slope and friction cone information. Without these considerations, untimely impacts can cause a robot to trip and inadequate tangential reaction forces at the stance foot can cause slippages. We address these challenges by combining, in a novel manner, a model based on an Angular Momentum Linear Inverted Pendulum (ALIP) and a Model Predictive Control (MPC) foot placement planner that is executed by the method of virtual constraints. The process starts with abstracting from the full dynamics of a Cassie 3D bipedal robot, an exact low-dimensional representation of its center of mass dynamics, parameterized by angular momentum. Under a piecewise planar terrain assumption and the elimination of terms for the angular momentum about the robot&#39;s center of mass, the centroidal dynamics about the contact point become linear and have dimension four. Importantly, we include the intra-step dynamics at uniformly-spaced intervals in the MPC formulation so that realistic workspace constraints on the robot&#39;s evolution can be imposed from step-to-step. The output of the low-dimensional MPC controller is directly implemented on a high-dimensional Cassie robot through the method of virtual constraints. In experiments, we validate the performance of our control strategy for the robot on a variety of surfaces with varied inclinations and textures.},
  archive   = {C_IROS},
  author    = {Grant Gibson and Oluwami Dosunmu-Ogunbi and Yukai Gong and Jessy Grizzle},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981969},
  pages     = {6724-6731},
  title     = {Terrain-adaptive, ALIP-based bipedal locomotion controller via model predictive control and virtual constraints},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluation of on-robot capacitive proximity sensors with
collision experiments for human-robot collaboration. <em>IROS</em>,
6716–6723. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A robot must comply with very restrictive safety standards in close human-robot collaboration applications. These standards limit the robot&#39;s performance because of speed reductions to avoid potentially large forces exerted on humans during collisions. On-robot capacitive proximity sensors (CPS) can serve as a solution to allow higher speeds and thus better productivity. They allow early reactive measures before contacts occur to reduce the forces during collisions. An open question on designing the systems is the selection of an adequate activation distance to trigger safety measures for a specific robot while considering latency and detection robustness. Furthermore, the systems&#39; actual effectiveness of impact attenuation and performance gain has not been evaluated before. In this work, we define and conduct a unified test procedure based on collision experiments to determine these parameters and investigate the performance gain. Two capacitive proximity sensor systems are evaluated on this test strategy on two robots. A significant performance increase can be achieved, since a small detection distance doubles robot operation speed while maintaining the same contact force as without Capacitive Proximity Sensor (CPS). This work can serve as a reference guide for designing, configuring and implementing future on-robot CPS.},
  archive   = {C_IROS},
  author    = {Hosam Alagi and Serkan Ergun and Yitao Ding and Tom P. Huck and Ulrike Thomas and Hubert Zangl and Björn Hein},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981490},
  pages     = {6716-6723},
  title     = {Evaluation of on-robot capacitive proximity sensors with collision experiments for human-robot collaboration},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Safe and efficient exploration of human models during
human-robot interaction. <em>IROS</em>, 6708–6715. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many collaborative human-robot tasks require the robot to stay safe and work efficiently around humans. Since the robot can only stay safe with respect to its own model of the human, we want the robot to learn a good model of the human in order to act both safely and efficiently. This paper studies methods that enable a robot to safely explore the space of a human-robot system to improve the robot&#39;s model of the human, which will consequently allow the robot to access a larger state space and better work with the human. In particular, we introduce active exploration under the framework of energy-function based safe control, investigate the effect of different active exploration strategies, and finally analyze the effect of safe active exploration on both analytical and neural network human models.},
  archive   = {C_IROS},
  author    = {Ravi Pandya and Changliu Liu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981085},
  pages     = {6708-6715},
  title     = {Safe and efficient exploration of human models during human-robot interaction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Suppressing delay-induced oscillations in physical
human-robot interaction with an upper-limb exoskeleton using
rate-limiting. <em>IROS</em>, 6695–6701. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In physical human-robot interaction (pHRI) enabled by admittance control, delay-induced oscillations arising from both the neuromuscular time-delays of the human and electromechanical delays of the robot can cause unsafe instability in the system. This study presents and evaluates rate-limiting as a means to overcome such instability, and provides a new perspective on how rate-limiting can benefit pHRI. Specifically, a rate-limited and time-delayed human-in-the-loop (HITL) model is analyzed to show not only how the rate-limiter can transform an unstable equilibrium (due to time-delay) into a stable limit-cycle, but also how a desired upper-bound on the range of persistent oscillations can be achieved by appropriately setting the rate-limiter threshold. In addition, a study involving 10 subjects and the EXO-UL8 upper-limb exoskeleton, and consisting of 16 trials - 4 rate-limiter thresholds by 4 time-delays - is performed to: (1) validate the relationships between time-delays, rate-limits, and position bounds on persistent oscillations, and (2) demonstrate the effectiveness of rate-limiting for recovery from delay-induced oscillations without interfering with regular operation. Agreement of experimental results with the theoretical developments supports the feasibility of incorporating rate-limiting in admittance-controlled pHRI systems as a safety mechanism.},
  archive   = {C_IROS},
  author    = {Jianwei Sun and Peter Walker Ferguson and Jacob Rosen},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981943},
  pages     = {6695-6701},
  title     = {Suppressing delay-induced oscillations in physical human-robot interaction with an upper-limb exoskeleton using rate-limiting},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robot contact reflexes: Adaptive maneuvers in the contact
reflex space. <em>IROS</em>, 6687–6694. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In order to transform a robot into an intelligent machine it needs to be enabled to react to unforeseen events (most importantly collisions) during task execution and have a plan on how to continue the task afterwards. This requires a flexible operational framework that allows to define adaptive reactions and interactions with the motion generation and task planning stage. Within this work we first reason about the choices the robot has for reactions to unforeseen events such as collisions with respect to safety of humans in the workspace, the robot itself and the environment as well as the successful task execution. We further present a flexible reflex engine together with a concept of integration into the motion generation and control work flow. The reflex engine and it&#39;s reflex maneuvers are a combination of state machines and decision trees that take into account the state of the robot and the world. It is capable of choosing safe reactions and can differentiate between different levels of contact severity and according reaction sets. Several reflex maneuvers are evaluated towards safety performance criteria in real robot experiments using an ISO/TS 15066 conform measurement device. Some of the tested reflexes are furthermore integrated into an implementation of the proposed approach for a simple real world example task where the robot needs to pickup a container and dispose it&#39;s content into a bin.},
  archive   = {C_IROS},
  author    = {Jonathan Vorndamme and Luis Figueredo and Sami Haddadin},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981904},
  pages     = {6687-6694},
  title     = {Robot contact reflexes: Adaptive maneuvers in the contact reflex space},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Safe and ergonomic human-drone interaction in warehouses.
<em>IROS</em>, 6681–6686. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an application of human-drone interaction (HDI) for inventory management in a ware-house 4.0 that aims at improving the operators&#39; safety and well-being together with increasing efficiency and reducing production costs. In our work, the speed and separation monitoring (SSM) methodology is applied for the first time to HDI, in analogy to the human-robot interaction (HRI) ISO safety requirements as well as the rapid upper limb assessment (RULA), for evaluating the operator&#39;s ergonomic posture during the interaction with the drone. With the aim of validating the proposed approach in a realistic scenario, a quadrotor is controlled to perform a pick and place task along a desired trajectory, from the picking bay to the palletizing area where the operator is located, avoiding collisions with the warehouse shelves by implementing the artificial potential field technique (APF) for planning and the linear quadratic regulator (LQR) and iterative LQR (iLQR) algorithms for tracking. The obtained results of the HDI architecture simulations are presented and discussed in detail proving the effectiveness of the proposed method for a safe and ergonomic HDI.},
  archive   = {C_IROS},
  author    = {Silvia Proia and Graziana Cavone and Antonio Camposeo and Fabio Ceglie and Raffaele Carli and Mariagrazia Dotoli},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981469},
  pages     = {6681-6686},
  title     = {Safe and ergonomic human-drone interaction in warehouses},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Regularized deep signed distance fields for reactive motion
generation. <em>IROS</em>, 6673–6680. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous robots should operate in real-world dynamic environments and collaborate with humans in tight spaces. A key component for allowing robots to leave structured lab and manufacturing settings is their ability to evaluate online and real-time collisions with the world around them. Distance-based constraints are fundamental for enabling robots to plan their actions and act safely, protecting both humans and their hardware. However, different applications require different distance resolutions, leading to various heuristic approaches for measuring distance fields w.r.t. obstacles, which are computationally expensive and hinder their application in dynamic obstacle avoidance use-cases. We propose Regularized Deep Signed Distance Fields (ReDSDF), a single neural implicit function that can compute smooth distance fields at any scale, with fine-grained resolution over high-dimensional manifolds and articulated bodies like humans, thanks to our effective data generation and a simple inductive bias during training. We demonstrate the effectiveness of our approach in representative simulated tasks for whole-body control (WBC) and safe Human- Robot Interaction (HRI) in shared workspaces. Finally, we provide proof of concept of a real-world application in a HRI handover task with a mobile manipulator robot.},
  archive   = {C_IROS},
  author    = {Puze Liu and Kuo Zhang and Davide Tateo and Snehal Jauhri and Jan Peters and Georgia Chalvatzaki},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981456},
  pages     = {6673-6680},
  title     = {Regularized deep signed distance fields for reactive motion generation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PSM: A predictive safety model for body motion based on the
spring-damper pendulum. <em>IROS</em>, 6657–6664. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Quantifying the safety of the human body ori-entation is an important issue in human-robot interaction. Knowing the changing physical constraints on human motion can improve inspection of safe human motions and bring essential information about stability and normality of human body orientations with real-time risk assessment. Also, this information can be used in cooperative robots and monitoring systems to evaluate and interact in the environment more freely. Furthermore, the workspace area can be more deterministic with the known physical characteristics of safety. Based on this motivation, we propose a novel predictive safety model (PSM) that relies on the information of an inertial measurement unit on the human chest. The PSM encompasses a 3-Dofs spring-damper pendulum model that predicts human motion based on a safe motion dataset. The estimated safe orientation of humans is obtained by integrating a safety dataset and an elastic spring-damper model in a way that the proposed approach can realize complex motions at different safety levels. We did experiments in a real-world scenario to verify our novel proposed model. This novel approach can be used in different guidance/assistive robots and health monitoring systems to support and evaluate the human condition, particularly elders.},
  archive   = {C_IROS},
  author    = {Seyed Amir Tafrishi and Ankit A. Ravankar and Yasuhisa Hirata},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981274},
  pages     = {6657-6664},
  title     = {PSM: A predictive safety model for body motion based on the spring-damper pendulum},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mapping of spatiotemporal scalar fields by mobile robots
using gaussian process regression. <em>IROS</em>, 6651–6656. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Spatiotemporal maps are data-driven estimates of time changing phenomena. For environmental science, rather than collect data from an array of static sensors, a mobile sensor platform could reduce setup time and cost, maintain flexibility to be deployed to any area of interest, and provide active feedback during observations. While promising, mapping is challenging with mobile sensors because vehicle constraints limit not only where, but also when observations can be made. By assuming spatial and temporal correlations in the data through kernel functions, this paper uses Gaussian process regression (GPR) to generate a maximum likelihood estimate of the phenomenon while also tracking the estimate uncertainty. Spatiotemporal mapping by GPR is simulated for a single fixed-path mobile robot observing a latent spatiotemporal scalar field. The learned spatiotemporal map captures the structure of the latent scalar field with the largest uncertainties in areas the robot never visited.},
  archive   = {C_IROS},
  author    = {Thomas M. C. Sears and Joshua A. Marshall},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981548},
  pages     = {6651-6656},
  title     = {Mapping of spatiotemporal scalar fields by mobile robots using gaussian process regression},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dual-camera high magnification surveillance system with
non-delay gaze control and always-in-focus function in indoor scenes.
<em>IROS</em>, 6637–6642. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study proposes a dual-camera system for indoor high magnification surveillance which is capable of achieving always-in-focus and non-delay gaze control based on high-speed vision. The users are enabled to move the mouse freely on the wide-view screen while observing its in-focal zoom-in monitoring video in real-time. The proposed system consists of a wide-angle camera for wide-view and a Galvano mirror-enabled ultra-fast pan-tilt-zoom (PTZ) camera for zoom-in view. To achieve always-in-focus, a high-speed focus scanning system is proposed that is comprised of a high-speed camera, a parfocal zoom lens, and a gear mechanism. Through continuously reciprocating rotational motion of the focusing ring driven by the servo motor, the high-speed camera captures sets of images with varying focal distances. Moreover, we proposed a most-in-focus (MIF) frame extraction algorithm to select the sharpest images as output. The experimental results are obtained to confirm the effectiveness of our system.},
  archive   = {C_IROS},
  author    = {Tianyi Zhang and Shaopeng Hu and Kohei Shimasaki and Idaku Ishii and Akio Namiki},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981485},
  pages     = {6637-6642},
  title     = {Dual-camera high magnification surveillance system with non-delay gaze control and always-in-focus function in indoor scenes},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ReINView: Re-interpreting views for multi-view 3D object
recognition. <em>IROS</em>, 6630–6636. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-view-based 3D object recognition is important in robot-environment interaction. However, recent methods simply extract features from each view via convolutional neural networks (CNNs) and then fuse these features together to make predictions. These methods ignore the inherent ambiguities of each view caused due to 3D-2D projection. To address this problem, we propose a novel deep framework for multi-view-based 3D object recognition. Instead of fusing the multi-view features directly, we design a re-interpretation module (ReINView) to eliminate the ambiguities at each view. To achieve this, ReINView re-interprets view features patch by patch by using their context from nearby views, considering that local patches are generally co-visible at nearby viewpoints. Since contour shapes are essential for 3D object recognition as well, ReINView further performs view-level re-interpretation, in which we use all the views as context sources since the target contours to be re-interpreted are globally observable. The re-interpreted multi-view features can better reflect the 3D global and local structures of the object. Experiments on both ModelNet40 and ModelNet10 show that the proposed model outperforms state-of-the-art methods in 3D object recognition.},
  archive   = {C_IROS},
  author    = {Ruchang Xu and Wei Ma and Qing Mil and Hongbin Zha},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981777},
  pages     = {6630-6636},
  title     = {ReINView: Re-interpreting views for multi-view 3D object recognition},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). STUN: Self-teaching uncertainty estimation for place
recognition. <em>IROS</em>, 6614–6621. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Place recognition is key to Simultaneous Localization and Mapping (SLAM) and spatial perception. However, a place recognition in the wild often suffers from erroneous predictions due to image variations, e.g., changing viewpoints and street appearance. Integrating uncertainty estimation into the life cycle of place recognition is a promising method to mitigate the impact of variations on place recognition performance. However, existing uncertainty estimation approaches in this vein are either computationally inefficient (e.g., Monte Carlo dropout) or at the cost of dropped accuracy. This paper proposes STUN, a self-teaching framework that learns to simultaneously predict the place and estimate the prediction uncertainty given an input image. To this end, we first train a teacher net using a standard metric learning pipeline to produce embedding priors. Then, supervised by the pretrained teacher net, a student net with an additional variance branch is trained to finetune the embedding priors and estimate the uncertainty sample by sample. During the online inference phase, we only use the student net to generate a place prediction in conjunction with the uncertainty. When compared with place recognition systems that are ignorant of the uncertainty, our framework features the uncertainty estimation for free without sacrificing any prediction accuracy. Our experimental results on the large-scale Pittsburgh30k dataset demonstrate that STUN outperforms the state-of-the-art methods in both recognition accuracy and the quality of uncertainty estimation.},
  archive   = {C_IROS},
  author    = {Kaiwen Cai and Chris Xiaoxuan Lu and Xiaowei Huang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981546},
  pages     = {6614-6621},
  title     = {STUN: Self-teaching uncertainty estimation for place recognition},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LSDNet: A lightweight self-attentional distillation network
for visual place recognition. <em>IROS</em>, 6608–6613. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual Place Recognition (VPR) has become an indispensable capacity for mobile robots to operate in large-scale environments. Existing methods in this field mostly focus on exploring high-performance encoding strategies, while few attempts are devoted to lightweight models that balance per-formance and computational cost. In this work, we propose a Lightweight Self-attentional Distillation Network (LSDNet) aiming to obtain advantages of both performance and efficiency. (1) From a performance perspective, an attentional encoding strategy is proposed to integrate crucial information in the scene. It extends the NetVlad architecture with a self-attention module to facilitate non-local information interaction between local features. Through further visual word vector rescaling, the final image representation can benefit from both non-local spatial integration and cluster-wise weighting. (2) From an efficiency perspective, LSDNet is built upon a lightweight back-bone. To maintain comparable performance to large backbone models, a dual distillation strategy is introduced. It prompts LSDNet to learn both encoding patterns in the hidden space and feature distributions in the encoding space from the teacher model. Through distillation-augmented training, LSDNet is able to rival the teacher model and outperform SOTA global representations with the same lightweight backbone.},
  archive   = {C_IROS},
  author    = {Guohao Peng and Yifeng Huang and Heshan Li and Zhenyu Wu and Danwei Wang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982272},
  pages     = {6608-6613},
  title     = {LSDNet: A lightweight self-attentional distillation network for visual place recognition},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Smart explorer: Recognizing objects in dense clutter via
interactive exploration. <em>IROS</em>, 6600–6607. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recognizing objects in dense clutter accurately plays an important role to a wide variety of robotic manipulation tasks including grasping, packing, rearranging and many others. However, conventional visual recognition models usually miss objects because of the significant occlusion among instances and causes incorrect prediction due to the visual ambiguity with the high object crowdedness. In this paper, we propose an interactive exploration framework called Smart Explorer for recognizing all objects in dense clutters. Our Smart Explorer physically interacts with the clutter to maximize the recognition performance while minimize the number of motions, where the false positives and negatives can be alleviated effectively with the optimal accuracy-efficiency trade-offs. Specifically, we first collect the multi-view RGB-D images of the clutter and reconstruct the corresponding point cloud. By aggregating the instance segmentation of RGB images across views, we acquire the instance-wise point cloud partition of the clutter through which the existed classes and the number of objects for each class are predicted. The pushing actions for effective physical interaction are generated to sizably reduce the recognition uncertainty that consists of the instance segmentation entropy and multi-view object disagreement. Therefore, the optimal accuracy-efficiency trade-off of object recognition in dense clutter is achieved via iterative instance prediction and physical interaction. Extensive experiments demonstrate that our Smart Explorer acquires promising recognition accuracy with only a few actions, which also outperforms the random pushing by a large margin.},
  archive   = {C_IROS},
  author    = {Zhenyu Wu and Ziwei Wang and Zibu Wei and Yi Wei and Haibin Yan},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981813},
  pages     = {6600-6607},
  title     = {Smart explorer: Recognizing objects in dense clutter via interactive exploration},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). STEADY: Simultaneous state estimation and dynamics learning
from indirect observations. <em>IROS</em>, 6593–6599. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate kinodynamic models play a crucial role in many robotics applications such as off-road navigation and high-speed driving. Many state-of-the-art approaches for learning stochastic kinodynamic models, however, require precise measurements of robot states as labeled input/output examples, which can be hard to obtain in outdoor settings due to limited sensor capabilities and the absence of ground truth. In this work, we propose a new technique for learning neural stochastic kinodynamic models from noisy and indirect observations by performing simultaneous state estimation and dynamics learning. The proposed technique iteratively improves the kinodynamic model in an expectation-maximization loop, where the E Step samples posterior state trajectories using particle filtering, and the M Step updates the dynamics to be more consistent with the sampled trajectories via stochastic gradient ascent. We evaluate our approach on both simulation and real-world benchmarks and compare it with several baseline techniques. Our approach not only achieves significantly higher accuracy but is also more robust to observation noise, thereby showing promise for boosting the performance of many other robotics applications.},
  archive   = {C_IROS},
  author    = {Jiayi Wei and Jarrett Holtz and Isil Dillig and Joydeep Biswas},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981279},
  pages     = {6593-6599},
  title     = {STEADY: Simultaneous state estimation and dynamics learning from indirect observations},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Towards autonomous visual navigation in arable fields.
<em>IROS</em>, 6585–6592. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous navigation of a robot in agricultural fields is essential for every task from crop monitoring to weed management and fertilizer application. Many current approaches rely on accurate GPS, however, such technology is expensive and can be impacted by lack of coverage. As such, autonomous navigation through sensors that can interpret their environment (such as cameras) is important to achieve the goal of autonomy in agriculture. In this paper, we introduce a purely vision-based navigation scheme that is able to reliably guide the robot through row-crop fields using computer vision and signal processing techniques without manual intervention. Independent of any global localization or mapping, this approach is able to accurately follow the crop-rows and switch between the rows, only using onboard cameras. The proposed navigation scheme can be deployed in a wide range of fields with different canopy shapes in various growth stages, creating a crop agnostic navigation approach. This was completed under various illumination conditions using simulated and real fields where we achieve an average navigation accuracy of 3.82cm with minimal human intervention (hyper-parameter tuning) on BonnBot-I.},
  archive   = {C_IROS},
  author    = {Alireza Ahmadi and Michael Halstead and Chris McCool},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981299},
  pages     = {6585-6592},
  title     = {Towards autonomous visual navigation in arable fields},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Loading an autonomous large-scale dump truck: Path planning
based on motion data from human-operated construction vehicles.
<em>IROS</em>, 6577–6584. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A large-scale dump truck that automatically transports earth and sand in cooperation with a human-operated backhoe is of interest to the construction industry. A human-operated dump truck generally drives slightly past the desired loading position and then backs up to it for loading the sediment. The turning and loading positions are subjectively decided according to the working posture of the backhoe and the surrounding environment, and the safety margin of cooperative works. Backhoe operators want to perform the same maneuvers for human-operated/automated dump trucks. The movements of the autonomous vehicle should be similar to those of a human-operated one. However, it is difficult to derive a human-like path that does more than minimize costs. This study proposes a path-planning method that generates a path including a turning back, according to the changing backhoe position and surrounding conditions. We modeled the positional relationship during loading between a backhoe and dump truck, determining the loading and turning positions and related parameters from operational data collected in trials with human-operated construction vehicles. The proposed method allowed the autonomous dump truck path to resemble a human-like one. The authors have retrofitted an existing large-scale six-wheeled dump truck for automatic operation. Automatic loading in cooperation with a human-operated backhoe was realized all 17 times using the retrofitted dump. The average stopping accuracy was 0.57 m and 9.7°.},
  archive   = {C_IROS},
  author    = {Tetsu Akegawa and Kazunori Ohno and Shotaro Kojima and Naoto Miyamoto and Taro Suzuki and Tomohiro Komatsu and Takahiro Suzuki and Yukinori Shibata and Kimitaka Asano and Satoshi Tadokoro},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981828},
  pages     = {6577-6584},
  title     = {Loading an autonomous large-scale dump truck: Path planning based on motion data from human-operated construction vehicles},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Autonomous mobile 3D printing of large-scale trajectories.
<em>IROS</em>, 6561–6568. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile 3D Printing (M3DP), using printing-in-motion, is a powerful paradigm for automated construction. A mobile robot, equipped with its own power, materials and an arm-mounted extruder, simultaneously navigates and creates its environment. Such systems can be highly scalable, parallelizable and flexible. However, planning and controlling the motion of the arm and base at the same time is challenging and most deployments either avoid robot-base motion entirely or use human prescribed robot-base paths. In a previous paper, we developed a high-level planning algorithm to automate M3DP given a print task. The generated robot-base paths avoid collisions and maintain task reachability. In this paper, we extend this work to robot control. We develop and compare three different ways to integrate the long-duration planned path with a short horizon Model Predictive Controller. Experiments are carried out via a new M3DP system - Armstone. We evaluate and demonstrate our algorithm in a 250 m long multi-layer print which is about 5 times longer than any previous physical printing-in-motion system.},
  archive   = {C_IROS},
  author    = {Julius Sustarevas and Dimitrios Kanoulas and Simon Julier},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982274},
  pages     = {6561-6568},
  title     = {Autonomous mobile 3D printing of large-scale trajectories},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Connected reconfiguration of polyominoes amid obstacles
using RRT. <em>IROS</em>, 6554–6560. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper investigates using a sampling-based approach, the RRT*, to reconfigure a 2D set of connected tiles in complex environments, where multiple obstacles might be present. Since the target application is automated building of discrete, cellular structures using mobile robots, there are constraints that determine what tiles can be picked up and where they can be dropped off during reconfiguration. We compare our approach to two algorithms as global and local planners, and show that we are able to find more efficient build sequences using a reasonable amount of samples, in environments with varying degrees of obstacle space.},
  archive   = {C_IROS},
  author    = {Javier Garcia and Michael Yannuzzi and Peter Kramer and Christian Rieck and Aaron T. Becker},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981184},
  pages     = {6554-6560},
  title     = {Connected reconfiguration of polyominoes amid obstacles using RRT},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Model learning and predictive control for autonomous
obstacle reduction via bulldozing. <em>IROS</em>, 6531–6538. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate how employing model learning methods in concert with model predictive control (MPC) can be used to automate obstacle reduction to mitigate risks to Combat Engineers operating construction equipment in an active battlefield. We focus on the task of earthen berm removal using a bladed vehicle. We introduce a novel data-driven formulation for earthmoving dynamics that enables prediction of the vehicle and detailed terrain state over a one second horizon. In a simulation environment, we first record demonstrations from a human operator and then train two different earthmoving models to produce predictions of the high-dimensional state using under six minutes of data. Optimization over the learned model is performed to select an action sequence, constrained to a 2D space of template action trajectories. Simple recovery controllers are implemented to improve controller performance when the model predictions degrade. This system yields near human-level performance on a berm removal task, indicating that model learning and predictive control is a promising data-efficient approach to autonomous earthmoving.},
  archive   = {C_IROS},
  author    = {W. Jacob Wagner and Katherine Driggs-Campbell and Ahmet Soylemezoglu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981911},
  pages     = {6531-6538},
  title     = {Model learning and predictive control for autonomous obstacle reduction via bulldozing},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Excavation of fragmented rocks with multi-modal model-based
reinforcement learning. <em>IROS</em>, 6523–6530. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a multi-modal model-based reinforcement learning (MBRL) approach to the excavation of fragmented rocks, which are very challenging to model due to their highly variable sizes and geometries, and visual occlusions. A multi-modal recurrent neural network (RNN) learns the dynamics of bucket-terrain interaction from a small physical dataset, with a discrete set of motion primitives encoded with domain knowledge as the action space. Then a model predictive controller (MPC) tracks a global reference path using multi-modal feedback. We show that our RNN-based dynamics function achieves lower prediction errors compared to a feed-forward neural network baseline, and the MPC is able to significantly outperform manually designed strategies on such a challenging task.},
  archive   = {C_IROS},
  author    = {Yifan Zhu and Liyang Wang and Liangjun Zhang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981537},
  pages     = {6523-6530},
  title     = {Excavation of fragmented rocks with multi-modal model-based reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Magnetic microrobot control using an adaptive fuzzy
sliding-mode method. <em>IROS</em>, 6484–6489. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The magnetic medical microrobots are influenced by diverse factors such as the medium, the geometry of the microrobot, and the imaging procedure. It is worth noting that the size limitations make it difficult or even impossible to obtain reliable physical properties of the system. In this research, to achieve a precise microrobot control using minimum knowledge about the system, an Adaptive Fuzzy Sliding-Mode Control (AFSMC) scheme is designed for the motion control problem of the magnetically actuated microrobots in presence of input saturation constraint. The AFSMC input consists of a fuzzy system designed to approximate an unknown nonlinear dynamical system and a robust term considered for mismatch compensation. According to the designed adaptation laws, the asymptotic stability is proved based on the Lyapunov theorem and Barbalat&#39;s lemma. In order to evaluate the effectiveness of the proposed method, a comparative simulation study is conducted.},
  archive   = {C_IROS},
  author    = {Alireza Mousavi and Hesam Khaksar and Awais Ahmed and Hongsoo Choi and Ali Kafash Hoshiar},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981438},
  pages     = {6484-6489},
  title     = {Magnetic microrobot control using an adaptive fuzzy sliding-mode method},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robotic auscultation over clothes for eliminating gender
bias. <em>IROS</em>, 6468–6475. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {During auscultation, patients in difficult age often feel embarrassed and uncomfortable when exposing their chests to doctors of the different gender and being touched physically by doctors. We assume that an auscultation with robot technology can address the aforementioned gender-related issue. Toward eliminating gender bias during auscultation exam, this paper proposes a robotic platform which enables to perform the automated auscultation over clothes. Our developed system is comprised of two folds: a depth image-based estimation system of the listening positions over clothes with RGB-D camera and a contact force adjustment system for minimizing the acoustic attenuation due to the clothes with a passive-actuated end-effector. Our preliminary results demonstrated the robotic platform enables to estimate the listening locations to hear the sounds of four cardiac valves over the clothes by combining the estimated skeletal structure with statistical anatomical data and acquire the maximized acoustic quality over the clothes by adjusting the contact force. The developed robotic platform has the potential to address the gender-related issues in auscultation.},
  archive   = {C_IROS},
  author    = {Ryosuke Tsumura and Akihiro Umezawa and Yuko Morishima and Hiroyasu Iwata and Yoshihiko Koseki and Naotaka Nitta and Kiyoshi Yoshinaka},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981054},
  pages     = {6468-6475},
  title     = {Robotic auscultation over clothes for eliminating gender bias},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BOEM-SLAM: A block online EM algorithm for the
visual-inertial SLAM backend. <em>IROS</em>, 6420–6427. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we present BOEM-SLAM, a backend for visual-inertial SLAM systems capable of creating a globally consistent trajectory and map without retaining the entire history of data. By leveraging the hidden Markov model structure, BOEM-SLAM can summarize historical data into sufficient statistics and then discard it. As a data-efficient algorithm, BOEM-SLAM addresses the growing computational costs and storage requirements of the SLAM backend. To demonstrate the performance of our algorithm we compare BOEM-SLAM to other fundamental approaches on both synthetic data and the EuRoC dataset. For evaluation on the EuRoC dataset, we use the open source okvis frontend and apply the Lie group state space representation and visual outlier removal. Overall, BOEM-SLAM shows a considerably lower computation time with comparable estimation performance. For example, the processing time of BOEM-SLAM is 50 times smaller than the optimization-based method using simulated data and 5 times smaller than the optimization-based method in the EuRoC dataset experiments.},
  archive   = {C_IROS},
  author    = {Tsang-Kai Chang and Alexandra Pogue and Ankur Mehta},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982015},
  pages     = {6420-6427},
  title     = {BOEM-SLAM: A block online EM algorithm for the visual-inertial SLAM backend},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient 2D graph SLAM for sparse sensing. <em>IROS</em>,
6404–6411. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simultaneous localization and mapping (SLAM) plays a vital role in mapping unknown spaces and aiding autonomous navigation. Virtually all state-of-the-art solutions today for 2D SLAM are designed for dense and accurate sensors such as laser range-finders (LiDARs). However, these sensors are not suitable for resource-limited nano robots, which become increasingly capable and ubiquitous nowadays, and these robots tend to mount economical and low-power sensors that can only provide sparse and noisy measurements. This introduces a challenging problem called SLAM with sparse sensing. This work addresses the problem by adopting the form of the state-of-the-art graph-based SLAM pipeline with a novel frontend and an improvement for loop closing in the backend, both of which are designed to work with sparse and uncertain range data. Experiments show that the maps constructed by our algorithm have superior quality compared to prior works on sparse sensing. Furthermore, our method is capable of running in real-time on a modern PC with an average processing time of 1/100th the input interval time.},
  archive   = {C_IROS},
  author    = {Hanzhi Zhou and Zichao Hu and Sihang Liu and Samira Khan},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981200},
  pages     = {6404-6411},
  title     = {Efficient 2D graph SLAM for sparse sensing},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast structural representation and structure-aware loop
closing for visual SLAM. <em>IROS</em>, 6396–6403. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Perceptual Aliasing is one of the main problems in simultaneous localization and mapping (SLAM). Wrong associations between different places may lead to failure of the whole map. Research on structure information is rarely investigated among existing solutions to this problem. In cases of visual SLAM without sensors, such as LiDAR or Inertial Measurement Unit (IMU), structure information can rarely be obtained due to the sparsity of 3D points, which also makes structure analysis complex. This study provides a spherical harmonics (SH) based fast structural representation (SH-FS) in visual SLAM using sparse point clouds, which extracts the structure information from sparse points into single vector. SH-FS was applied in conventional feature-based loop closing process. Furthermore, a structure-aware loop closing method in visual SLAM was proposed to improve the robustness of SLAM systems. Moreover, our methods show a favorable performance in extensive experiments on different large-scale real world datasets.},
  archive   = {C_IROS},
  author    = {Shuxiang Xie and Ryoichi Ishikawa and Ken Sakurada and Masaki Onishi and Takeshi Oishi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981496},
  pages     = {6396-6403},
  title     = {Fast structural representation and structure-aware loop closing for visual SLAM},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). S3LAM: Structured scene SLAM. <em>IROS</em>, 6389–6395. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a new SLAM system that uses the semantic segmentation of objects and structures in the scene. Semantic information is relevant as it contains high level information which may make SLAM more accurate and robust. Our contribution is twofold: i) A new SLAM system based on ORB-SLAM2 that creates a semantic map made of clusters of points corresponding to objects instances and structures in the scene. ii) A modification of the classical Bundle Adjustment formulation to constrain each cluster using geometrical priors, which improves both camera localization and reconstruction and enables a better understanding of the scene. We evaluate our approach on sequences from several public datasets and show that it improves camera pose estimation with respect to state of the art.},
  archive   = {C_IROS},
  author    = {Mathieu Gonzalez and Eric Marchand and Amine Kacete and Jerome Royan},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981370},
  pages     = {6389-6395},
  title     = {S3LAM: Structured scene SLAM},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). InCOpt: Incremental constrained optimization using the bayes
tree. <em>IROS</em>, 6381–6388. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we investigate the problem of incre-mentally solving constrained non-linear optimization problems formulated as factor graphs. Prior incremental solvers were either restricted to the unconstrained case or required periodic batch relinearizations of the objective and constraints which are expensive and detract from the online nature of the algorithm. We present InCOpt, an Augmented Lagrangian-based incremental constrained optimizer that views matrix operations as message passing over the Bayes tree. We first show how the linear system, resulting from linearizing the constrained objective, can be represented as a Bayes tree. We then propose an algorithm that views forward and back substitutions, which naturally arise from solving the Lagrangian, as upward and downward passes on the tree. Using this formulation, In-COpt can exploit properties such as fluid/online relinearization leading to increased accuracy without a sacrifice in runtime. We evaluate our solver on different applications (navigation and manipulation) and provide an extensive evaluation against existing constrained and unconstrained solvers.},
  archive   = {C_IROS},
  author    = {Mohamad Qadri and Paloma Sodhi and Joshua G. Mangelson and Frank Dellaert and Michael Kaess},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982178},
  pages     = {6381-6388},
  title     = {InCOpt: Incremental constrained optimization using the bayes tree},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AIB-MDP: Continuous probabilistic motion planning for
automated vehicles by leveraging action independent belief spaces.
<em>IROS</em>, 6373–6380. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While automated research vehicles are already populating the roads, their commercial availability at scale is still to come. Presumably, one of the key challenges is to derive behaviors that are safe and comfortable but at the same time not overcautious, despite considerable uncertainties. These uncertainties stem from imperfect perception, occlusions and limited sensor range, but also from the unknown future behavior of other traffic participants. A holistic uncertainty treatment, for example in a general POMDP formulation, often induces a strong limitation on the action space due to the need for real-time capability. Further, related approaches often do not account for the need for verifiable safety, including traffic rule compliance. The proposed approach is targeted towards scenarios with clear precedence. It is based on an MDP with an action-independent belief (AIB-MDP): We assume that the future belief over the trajectories of other traffic participants is independent of the ego vehicle&#39;s behavior. Thus, the future belief can be predicted and simplified in an upstream module, independent of motion planning. This modularization facilitates subsequent ego motion planning in a continuous action space despite the thorough uncertainty consideration. The improved performance compared to state-of-the-art is demonstrated in three example scenarios.},
  archive   = {C_IROS},
  author    = {Maximilian Naumann and Christoph Stiller},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981696},
  pages     = {6373-6380},
  title     = {AIB-MDP: Continuous probabilistic motion planning for automated vehicles by leveraging action independent belief spaces},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Information-aware guidance for magnetic anomaly based
navigation. <em>IROS</em>, 6347–6354. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the absence of an absolute positioning system, such as GPS, autonomous vehicles are subject to accumu-lation of positional error which can interfere with reliable performance. Improved navigational accuracy without GPS enables vehicles to achieve a higher degree of autonomy and reliability, both in terms of decision making and safety. This paper details the use of two navigation systems for autonomous agents using magnetic field anomalies to localize themselves within a map; both techniques use the information content in the environment in distinct ways and are aimed at reducing the localization uncertainty. The first method is based on a nonlinear observability metric of the vehicle model, while the second is an information theory based technique which minimizes the expected entropy of the system. These conditions are used to design guidance laws that minimize the localization uncertainty and are verified both in simulation and hardware experiments are presented for the observability approach.},
  archive   = {C_IROS},
  author    = {J. Humberto Ramos and Jaejeong Shin and Kyle Volle and Paul Buzaud and Kevin Brink and Prashant Ganesh},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981709},
  pages     = {6347-6354},
  title     = {Information-aware guidance for magnetic anomaly based navigation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Temporal context for robust maritime obstacle detection.
<em>IROS</em>, 6340–6346. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robust maritime obstacle detection is essential for fully autonomous unmanned surface vehicles (USVs). The currently widely adopted segmentation-based obstacle detection methods are prone to misclassification of object reflections and sun glitter as obstacles, producing many false positive detections, effectively rendering the methods impractical for USV navigation. However, water-turbulence-induced temporal appearance changes on object reflections are very distinctive from the appearance dynamics of true objects. We harness this property to design WaSR-T, a novel maritime obstacle detection network, that extracts the temporal context from a sequence of recent frames to reduce ambiguity. By learning the local temporal characteristics of object reflection on the water surface, WaSR-T substantially improves obstacle detection accuracy in the presence of reflections and glitter. Compared with existing single-frame methods, WaSR-T reduces the number of false positive detections by 41\% overall and by over 53\% within the danger zone of the boat, while preserving a high recall, and achieving new state-of-the-art performance on the challenging MODS maritime obstacle detection benchmark. The code, pretrained models and extended datasets are available at: https://github.com/lojzezust/WaSR-T},
  archive   = {C_IROS},
  author    = {Lojze Žust and Matej Kristan},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982043},
  pages     = {6340-6346},
  title     = {Temporal context for robust maritime obstacle detection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bubble planner: Planning high-speed smooth quadrotor
trajectories using receding corridors. <em>IROS</em>, 6332–6339. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Quadrotors are agile platforms. With human experts, they can perform extremely high-speed flights in cluttered environments. However, fully autonomous flight at high speed remains a significant challenge. In this work, we propose a motion planning algorithm based on the corridor-constrained minimum control effort trajectory optimization (MINCO) framework. Specifically, we use a series of overlapping spheres to represent the free space of the environment and propose two novel designs that enable the algorithm to plan high-speed quadrotor trajectories in real-time. One is a sampling-based corridor generation method that generates spheres with large overlapped areas (hence overall corridor size) between two neighboring spheres. The second is a Receding Horizon Corridors (RHC) strategy, where part of the previously generated corridor is reused in each replan. Together, these two designs enlarge the corridor spaces in accordance with the quadrotor&#39;s current state and hence allow the quadrotor to maneuver at high speeds. We benchmark our algorithm against other state-of-the-art planning methods to show its superiority in simulation. Comprehensive ablation studies are also conducted to show the necessity of the two designs. The proposed method is finally evaluated on an autonomous LiDAR-navigated quadrotor UAV in woods environments, achieving flight speeds over 13.7m/s without any prior map of the environment or external localization facility.},
  archive   = {C_IROS},
  author    = {Yunfan Ren and Fangcheng Zhu and Wenyi Liu and Zhepei Wang and Yi Lin and Fei Gao and Fu Zhang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981518},
  pages     = {6332-6339},
  title     = {Bubble planner: Planning high-speed smooth quadrotor trajectories using receding corridors},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A robust and fast occlusion-based frontier method for
autonomous navigation in unknown cluttered environments. <em>IROS</em>,
6324–6331. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Navigation through unknown, cluttered environments is a fundamental and challenging task for autonomous vehicles as they must deal with a myriad of obstacle configurations typically unknown a priori. Challenges arise because obstacles of unknown shapes and dimensions can create occlusions limiting sensor field of view and leading to uncertainty in motion planning. In this paper we propose to leverage such occlusions to quickly explore and cover unknown cluttered environments. Specifically, this work presents a novel occlusion-aware frontier-based approach that estimates gaps in point cloud data and shadows in the field of view to generate waypoints to navigate. Our scheme also proposes a breadcrumbing technique to save states of interest during exploration that can be exploited in future missions. For the latter aspect we focus primarily on the generation of the minimum number of breadcrumbs that will increase coverage and visibility of an explored environment. Extensive simulations and experiment results on an unmanned ground vehicle (UGV) are demonstrated to validate the proposed technique, showing improvements over traditional state of the art frontier-based exploration methods.},
  archive   = {C_IROS},
  author    = {Nicholas Mohammad and Nicola Bezzo},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982059},
  pages     = {6324-6331},
  title     = {A robust and fast occlusion-based frontier method for autonomous navigation in unknown cluttered environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Drift reduced navigation with deep explainable features.
<em>IROS</em>, 6316–6323. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern autonomous vehicles (AVs) often rely on vision, LIDAR, and even radar-based simultaneous localization and mapping (SLAM) frameworks for precise localization and navigation. However, modern SLAM frameworks often lead to unacceptably high levels of drift (i.e., localization error) when AVs observe few visually distinct features or encounter occlusions due to dynamic obstacles. This paper argues that minimizing drift must be a key desiderata in AV motion planning, which requires an AV to take active control decisions to move towards feature-rich regions while also minimizing conventional control cost. To do so, we first introduce a novel data-driven perception module that observes LIDAR point clouds and estimates which features/regions an AV must navigate towards for drift minimization. Then, we introduce an interpretable model predictive controller (MPC) that moves an AV toward such feature-rich regions while avoiding visual occlusions and gracefully trading off drift and control cost. Our experiments on challenging, dynamic scenarios in the state-of-the-art CARLA simulator indicate our method reduces drift up to 76.76\% compared to benchmark approaches.},
  archive   = {C_IROS},
  author    = {Mohd Omama and Sripada V. S. Sundar and Sandeep Chinchali and Arun Kumar Singh and K. Madhava Krishna},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981330},
  pages     = {6316-6323},
  title     = {Drift reduced navigation with deep explainable features},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient extrinsic calibration of multi-sensor 3D LiDAR
systems for autonomous vehicles using static objects information.
<em>IROS</em>, 6285–6292. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For an autonomous vehicle, the ability to sense its surroundings and to build an overall representation of the environment by fusing different sensor data streams is fundamental. To this end, the poses of all sensors need to be accurately determined. Traditional calibration methods are based on: 1) using targets specifically designed for calibration purposes in controlled environments, 2) optimizing a quality metric of the point clouds collected while traversing an unknown but static environment, or 3) optimizing the match among persensor incremental motion observations along a motion path fulfilling special requirements. In real scenarios, however, the online applicability of these methods can be limited, as they are typically highly dynamic, contain degenerate paths, and require fast computations. In this paper, we propose an approach that tackles some of these challenges by formulating the calibration problem as a joint but structured optimization problem of all sensor calibrations that takes as input a summary of the point cloud information consisting of ground points and pole detections. We demonstrate the efficiency and quality of the results of the proposed approach in a set of experiments with LiDAR simulation and real data from an urban trip.},
  archive   = {C_IROS},
  author    = {Brahayam Ponton and Magda Ferri and Lars König and Marcus Bartels},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981606},
  pages     = {6285-6292},
  title     = {Efficient extrinsic calibration of multi-sensor 3D LiDAR systems for autonomous vehicles using static objects information},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual-inertial-aided online MAV system identification.
<em>IROS</em>, 6277–6284. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {System modeling and parameter identification of micro aerial vehicles (MAV) are crucial for robust autonomy, especially under highly dynamic motions. Visual-inertial-aided online parameter identification has recently seen research attention due to the demanding of adaptation to platform configuration changes with minimal onboard sensor requirements. To this end, we design an online MAV system identification algorithm to tightly fuse visual, inertial and MAV aerodynamic information within a lightweight multi-state constraint Kalman filter (MSCKF) framework. In particular, while one could blindly fuse the MAV dynamic-induced relative motion constraints in EKF, we numerically show that due to the (quadrotor) MAV system modeling inaccuracy, they often become overconfident and negatively impact the state estimates. As such, we leverage the Schmidt-Kalman filter (SKF) for MAV system parameter identification to prevent corruption of state estimates. Through extensive simulations and real-world experiments, we validate the proposed SKF-based scheme and demonstrate its ability to perform robust system identification even in the presence of an inconsistent MAV dynamic model under different motions.},
  archive   = {C_IROS},
  author    = {Chuchu Chen and Yulin Yang and Patrick Geneva and Woosik Lee and Guoquan Huang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982263},
  pages     = {6277-6284},
  title     = {Visual-inertial-aided online MAV system identification},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Continuous calibration and narrow compensation algorithm to
estimate a joint axis under the various conditions with unit sensor.
<em>IROS</em>, 6270–6276. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Wearable robots have been developed to aid or substitute the gait locomotion of humans. To assist gait locomotion based on the intention of a wearer, a gait pattern analysis is required with a wearable sensor by measuring body information, i.e., a joint angular velocity. However, measuring a precise joint angular velocity is difficult because the attachment position of a sensor has a curvature and an anatomical joint axis which is invisible. Therefore, a sensor calibration algorithm, which aligns a sensor axis into an anatomical joint axis, is required to provide an optimal assist for a wearer. Hence, in this paper, a new and simple sensor calibration algorithm is proposed with a unit sensor. Since a wearer shakes the body or collides with the ground when walking, the attachment position of a sensor may be changed. Thus, a continuous sensor compensation algorithm is also proposed. Additionally, the effectiveness of this new algorithm is demonstrated by gait locomotion experiments on various paths.},
  archive   = {C_IROS},
  author    = {Wonjeong Seo and Haseok Lee and Jungsu Choi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981747},
  pages     = {6270-6276},
  title     = {Continuous calibration and narrow compensation algorithm to estimate a joint axis under the various conditions with unit sensor},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Extrinsic calibration of a 2D laser rangefinder and a
depth-camera using an orthogonal trihedron. <em>IROS</em>, 6264–6269.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {2D laser range-finders and depth-cameras are usually equipped on service robots. But there are rarely calibration methods of them. This paper proposes an extrinsic calibration method of a 2D laser range-finder and a depth-camera using an orthogonal trihedron. The trihedron with orthogonal assumptions is taken as a reference frame to roughly estimate the relative pose between the sensors by solving a perspective-three-point (P3P) problem and basis-to-basis correspondence. Then, the estimated relative pose is refined via non-linear optimization based on line-to-plane constraints. Unlike other works which require enough motion, only one-shot observation is required, and it is insensitive to sensor ranging noise and the manufacturing errors of calibration targets. Verified by simulation and real experiments, the proposed method is simple, effective and accurate.},
  archive   = {C_IROS},
  author    = {Zhengbin Li and Haiqing Dong and Dong Liu and Yabin Ding},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981482},
  pages     = {6264-6269},
  title     = {Extrinsic calibration of a 2D laser rangefinder and a depth-camera using an orthogonal trihedron},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TEScalib: Targetless extrinsic self-calibration of LiDAR and
stereo camera for automated driving vehicles with uncertainty analysis.
<em>IROS</em>, 6256–6263. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present TEScalib, a novel extrinsic self-calibration approach of LiDAR and stereo camera using the geometric and photometric information of surrounding environments without any calibration targets for automated driving vehicles. Since LiDAR and stereo camera are widely used for sensor data fusion on automated driving vehicles, their extrinsic calibration is highly important. However, most of the LiDAR and stereo camera calibration approaches are mainly target-based and therefore time consuming. Even the newly developed targetless approaches in last years are either inaccurate or unsuitable for driving platforms. To address those problems, we introduce TEScalib. By applying a 3D mesh reconstruction-based point cloud registration, the geometric information is used to estimate the LiDAR to stereo camera extrinsic parameters accurately and robustly. To calibrate the stereo camera, a photometric error function is builded and the LiDAR depth is involved to transform key points from one camera to another. During driving, these two parts are processed iteratively. Besides that, we also propose an uncertainty analysis for reflecting the reliability of the estimated extrinsic parameters. Our TEScalib approach evaluated on the KITTI dataset achieves very promising results.},
  archive   = {C_IROS},
  author    = {Haohao Hu and Fengze Han and Frank Bieder and Jan-Hendrik Pauls and Christoph Stiller},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981651},
  pages     = {6256-6263},
  title     = {TEScalib: Targetless extrinsic self-calibration of LiDAR and stereo camera for automated driving vehicles with uncertainty analysis},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Industrial robot parameter identification using a
constrained instrumental variable method. <em>IROS</em>, 6250–6255. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot identification is a prolific topic that has a long history with results spanning recent decades. Recent years have witnessed a renew of interest in this problem due in part to a rapid increase in robotic hardware platforms capable of accurate model-based control. The most popular methods exploit the fact that the inverse dynamic model is linear to the dynamic parameters. Because we identify robots with closed-loop procedures, an Instrumental Variable approach called IDIM-IV (Inverse Dynamic Identification Model with Instrumental Variable estimation) that combines the direct and inverse dynamic models to prevent from correlation between errors has been successfully validated. However, IDIM-IV does not guarantee that the direct dynamic model will be well-posed during its iterations because of possible modeling errors. In this paper, we combine physical constraints and IDIM-IV to address this deficiency for IDIM-IV. This new constrained IV approach, called PC-IDIM-IV (Physically Consistent IDIM-IV), consists of two nested iterative algorithms: an outer one that is IDIM-IV and an inner one that accounts for the physical constraints solved by a Gauss-Newton algorithm. Experimental results and comparisons with other methods carried out with the TX40 robot show the feasibility of PC-IDIM-IV.},
  archive   = {C_IROS},
  author    = {Ardiani Fabio and Alexandre Janot and Benoussaad Mourad},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981816},
  pages     = {6250-6255},
  title     = {Industrial robot parameter identification using a constrained instrumental variable method},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CSA-SVM method for internal cavitation defects detection and
its application of district heating pipes. <em>IROS</em>, 6242–6249. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The goal of this paper is to develop an ultrasonic detection device that can be mounted on an underwater snake vehicle (USV) for underwater district heating pipe (DHP) detection in the future. Ultrasonic detection technology (UDT) is the detection means used, and the cavitation defects in polyurethane (PUR) layer of DHPs are the object being detected. Due to the large thickness of PUR layer and the complex interface information of multi-layer structure, detecting defects of DHPs quantitatively is a difficult task. To address this issue, this paper proposes an approach that combines feature extraction and crow search algorithm (CSA) optimized support vector machine (SVM). Firstly, the main parameters and detection method of UDT are designed after investigation. Secondly, defective signals are pre-processed by signal processing to extract the features form three domains. Finally, four different classifiers are used to identify cavitation defects based on the feature-set. When compared among optimized random forest (RF), k-nearest neighbor (KNN), and ordinary SVM, the experimental results show that CSA-SVM had the highest accuracy in defect size prediction, and the validation-experiment verifies the practicability and feasibility of the CSA-SVM classifier. All experiments illustrate that the issue could be well solved by our method.},
  archive   = {C_IROS},
  author    = {Yanran Chen and Shugen Ma and Longchuan Li and Zhiqing Li and Yulin Yang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981830},
  pages     = {6242-6249},
  title     = {CSA-SVM method for internal cavitation defects detection and its application of district heating pipes},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DXQ-net: Differentiable LiDAR-camera extrinsic calibration
using quality-aware flow. <em>IROS</em>, 6235–6241. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate LiDAR-camera extrinsic calibration is a precondition for many multi-sensor systems in mobile robots. Most calibration methods rely on laborious manual operations and calibration targets. While working online, the calibration methods should be able to extract information from the environment to construct the cross-modal data association. Convolutional neural networks (CNNs) have powerful feature extraction ability and have been used for calibration. However, most of the past methods solve the extrinsic as a regression task, without considering the geometric constraints involved. In this paper, we propose a novel end-to-end extrinsic calibration method named DXQ-Net, using a differentiable pose estimation module for generalization. We formulate a probabilistic model for LiDAR-camera calibration flow, yielding a prediction of uncertainty to measure the quality of LiDAR-camera data association. Testing experiments illustrate that our method achieves a competitive with other methods for the translation component and state-of-the-art performance for the rotation component. Generalization experiments illustrate that the generalization performance of our method is significantly better than other deep learning-based methods.},
  archive   = {C_IROS},
  author    = {Xin Jing and Xiaqing Ding and Rong Xiong and Huanjun Deng and Yue Wang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981418},
  pages     = {6235-6241},
  title     = {DXQ-net: Differentiable LiDAR-camera extrinsic calibration using quality-aware flow},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design interface mapping for efficient free-form
tele-manipulation. <em>IROS</em>, 6221–6226. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion tracking interfaces are intuitive for free-form teleoperation tasks. However, efficient manipulation control can be difficult with such interfaces because of issues like the interference of unintended motions and the limited precision of human motion control. The limitation in control efficiency reduces the operator&#39;s performance and increases their workload and frustration during robot teleoperation. To improve the efficiency, we proposed separating controlled degrees of freedom (DoFs) and adjusting the motion scaling ratio of a motion tracking interface. The motion tracking of handheld controllers from a Virtual Reality system was used for the interface. We separated the translation and rotational control into: 1) two controllers held in the dominant and non-dominant hands and 2) hand pose tracking and trackpad inputs of a controller. We scaled the control mapping ratio based on 1) the environmental constraints and 2) the teleoperator&#39;s control speed. We further conducted a user study to investigate the effectiveness of the proposed methods in increasing efficiency. Our results show that the separation of position and orientation control into two controllers and the environment-based scaling methods perform better than their alternatives.},
  archive   = {C_IROS},
  author    = {Achyuthan Unni Krishnan and Tsung-Chi Lin and Zhi Li},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982149},
  pages     = {6221-6226},
  title     = {Design interface mapping for efficient free-form tele-manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast reflexive grasping with a proprioceptive teleoperation
platform. <em>IROS</em>, 6213–6220. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a proprioceptive teleoperation system that uses a reflexive grasping algorithm to enhance the speed and robustness of pick-and-place tasks. The system consists of two manipulators that use quasi-direct-drive actuation to provide highly transparent force feedback. The end-effector has bimodal force sensors that measure 3-axis force information and 2-dimensional contact location. This information is used for anti-slip and re-grasping reflexes. When the user makes contact with the desired object, the re-grasping reflex aligns the gripper fingers with antipodal points on the object to maximize the grasp stability. The reflex takes only 150ms to correct for inaccurate grasps chosen by the user, so the user&#39;s motion is only minimally disturbed by the execution of the re-grasp. Once antipodal contact is established, the anti-slip reflex ensures that the gripper applies enough normal force to prevent the object from slipping out of the grasp. The combination of proprioceptive manipulators and reflexive grasping allows the user to complete teleoperated tasks with precision at high speed.},
  archive   = {C_IROS},
  author    = {Andrew SaLoutos and Elijah Stanger–Jones and Sangbae Kim},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981383},
  pages     = {6213-6220},
  title     = {Fast reflexive grasping with a proprioceptive teleoperation platform},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Manipulability-aware shared locomanipulation motion
generation for teleoperation of mobile manipulators. <em>IROS</em>,
6205–6212. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The teleoperation of mobile manipulators may pose significant challenges, demanding complex interfaces and causing a substantial burden to the human operator due to the need to switch continuously from the manipulation of the arm to the control of the mobile platform. Hence, several works have considered to exploit shared control techniques to overcome this issue and, in general, to facilitate the task execution. This work proposes a manipulability-aware shared locoma-nipulation motion generation method to facilitate the execution of telemanipulation tasks with mobile manipulators. The method uses the manipulability level of the end-effector to control the generation of the mobile base and manipulator motions, facilitating their simultaneous control by the operator while executing telemanipulation tasks. Therefore, the operator can exclusively control the end -effector, while the underlying ar-chitecture generates the mobile platform commands depending on the end-effector manipulability level. The effectiveness of this approach is demonstrated with a number of experiments in which the CENTAURO robot, a hybrid leg-wheel platform with an anthropomorphic upper body, is teleoperated to execute a set of telemanipulation tasks.},
  archive   = {C_IROS},
  author    = {Davide Torielli and Luca Muratore and Nikos Tsagarakis},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982220},
  pages     = {6205-6212},
  title     = {Manipulability-aware shared locomanipulation motion generation for teleoperation of mobile manipulators},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Haptic teleoperation of high-dimensional robotic systems
using a feedback MPC framework. <em>IROS</em>, 6197–6204. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Model Predictive Control (MPC) schemes have proven their efficiency in controlling high degree-of-freedom (DoF) complex robotic systems. However, they come at a high computational cost and an update rate of about tens of hertz. This relatively slow update rate hinders the possibility of stable haptic teleoperation of such systems since the slow feedback loops can cause instabilities and loss of transparency to the operator. This work presents a novel framework for transparent teleoperation of MPC-controlled complex robotic systems. In particular, we employ a feedback MPC approach [1] and exploit its structure to account for the operator input at a fast rate which is independent of the update rate of the MPC loop itself. We demonstrate our framework on a mobile manipulator platform and show that it significantly improves haptic teleoperation&#39;s transparency and stability. We also highlight that the proposed feedback structure is constraint satisfactory and does not violate any constraints defined in the optimal control problem. To the best of our knowledge, this work is the first realization of the bilateral teleoperation of a legged manipulator using a whole-body MPC framework.},
  archive   = {C_IROS},
  author    = {Jin Cheng and Firas Abi-Farraj and Farbod Farshidian and Marco Hutter},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981290},
  pages     = {6197-6204},
  title     = {Haptic teleoperation of high-dimensional robotic systems using a feedback MPC framework},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Skill-CPD: Real-time skill refinement for shared autonomy in
manipulator teleoperation. <em>IROS</em>, 6189–6196. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Advanced wireless communication networks provide lower latency and a higher transmission rate. Although this is an enabler for many new teleoperation applications, the risk of network instability or packet drop is still unavoidable. Real-time manipulator teleoperation requires data transmission with no discontinuity. Shared autonomy (SA) is a standard method to mitigate this issue. In this way, if the data from the remote side is unavailable, the controller can continue based on the previously observed models. However, due to the spatial gap between human and robot trajectories, indisputable fluctuations occur, which cause issues in teleoperation applications. This motivates us to propose a new skill refinement strategy to modify the previously trained skill and mitigate the sudden unwanted motions within the control takeover phase. To this end, our approach comprises applying the Hidden Semi-Markov Model (HSMM) and Linear Quadratic Tracker (LQT) in combination to learn and predict the user&#39;s intentions and then exploiting Coherent Point Drift (CPD) to refine the executable trajectory. We test our method both in simulation and in the real world for 2D English letter drawing and 3D robot-assisted feeding scenarios. Our experimental results using the Kinova® Movo platform show that the proposed refinement approach generates a stable trajectory and mitigates the control switching inconsistency. All comprehensive experiments and source code is available at: http://cxdcxd.github.io/SkillCPD.},
  archive   = {C_IROS},
  author    = {Edwin Babaians and Dong Yang and Mojtaba Karimi and Xiao Xu and Serkut Ayvasik and Eckehard Steinbach},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982077},
  pages     = {6189-6196},
  title     = {Skill-CPD: Real-time skill refinement for shared autonomy in manipulator teleoperation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Block-based novel haptic data reduction for time-delayed
teleoperation. <em>IROS</em>, 6183–6188. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work proposes a novel haptic data reduction scheme for time-delayed teleoperation by coding information as blocks. State-of-the-art (SOTA) haptic data reduction approaches are mainly sampled-based schemes. They encode haptic signals sample by sample in order to minimize the introduced coding delay. In contrast, our proposed block-based coding approach transmits a sample block as a single unit (haptic packet). Although it introduces additional algorithmic delays that are proportional to the block length, block coding has benefits since the packet rate is easy to control, the coding approach can be lossless, and the intra-block information can be employed to improve the force feedback quality. We further develop an energy adjustment approach that uses the information in a block to mitigate force oscillations caused by the Time Domain Passivity Approach. Simulation experiments and subjective tests demonstrate that our method reduces network load and significantly increases force feedback quality compared with the SOTA sample-based coding schemes, particularly for mid- to high-latency networks and low packet rates.},
  archive   = {C_IROS},
  author    = {Ming Gui and Xiao Xu and Eckehard Steinbach},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982156},
  pages     = {6183-6188},
  title     = {Block-based novel haptic data reduction for time-delayed teleoperation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Variable impedance control for safety and usability in
telemanipulation. <em>IROS</em>, 6177–6182. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, haptic telemanipulation has been introduced to control robots remotely with an input device that generates force feedback. Compliant control strategies are needed to ensure safe interaction between humans and robots. Accurate and precise manipulation requires a stiff setup of the impedance parameters, while safety demands for low stiffness. This paper proposes an impedance-based control approach that combines stiff manipulation with a safety mechanism that adapts compliance when required. We introduce three system modes: operation, safety and recovery mode. If the external forces exceed a defined force threshold, the system switches to the compliant safety mode. A user input triggers the recovery process that increases the stiffness back to its nominal value. This paper suggests an energy tank, which limits the change of stiffness to ensure stability during recovering phase. We validate the functionality of this approach using a real telemanipulation setup and show that the suggested tank enables recovery even from large displacements.},
  archive   = {C_IROS},
  author    = {Stephan Andreas Schwarz and Ulrike Thomas},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982118},
  pages     = {6177-6182},
  title     = {Variable impedance control for safety and usability in telemanipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analysis of user behavior and workload during simultaneous
tele-operation of multiple mobile manipulators. <em>IROS</em>,
6169–6176. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper discusses the tele-operation system for multiple mobile manipulators. If a single person could freely tele-operate multiple mobile manipulators simultaneously, it would be a great step toward the goal of “avatar-symbiotic society” allowing people to live beyond the constraints of their bodies, space, and time. At present, however, such a tele-operation system has not been developed. Therefore, we built a prototype system to tele-operate two mobile manipulators and conducted a subject experiment on a pick-and-place task to investigate the tele-operator&#39;s task performance, workload and gaze behavior. By analyzing these results, we obtained guidelines to design tele-operation system for multiple robots.},
  archive   = {C_IROS},
  author    = {Tatsuya Aoki and Tomoaki Nakamura and Takayuki Nagai},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981934},
  pages     = {6169-6176},
  title     = {Analysis of user behavior and workload during simultaneous tele-operation of multiple mobile manipulators},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep augmentation for electrode shift compensation in
transient high-density sEMG: Towards application in neurorobotics.
<em>IROS</em>, 6148–6153. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Going beyond the traditional sparse multi-channel peripheral human-machine interface that has been used widely in neurorobotics, high-density surface electromyography (HD-sEMG) has shown significant potential for decoding upper-limb motor control. We have recently proposed heterogeneous temporal dilation of LSTM in a deep neural network architecture for a large number of gestures (&gt;60), securing spatial resolution and fast convergence. However, several fundamental questions remain unanswered. One problem targeted explicitly in this paper is the issue of “electrode shift,” which can happen specifically for high-density systems and during doffing and donning the sensor grid. Another real-world problem is the question of transient versus plateau classification, which connects to the temporal resolution of neural interfaces and seamless control. In this paper, for the first time, we implement gesture prediction on the transient phase of HD-sEMG data while robustifying the human-machine interface decoder to electrode shift. For this, we propose the concept of deep data augmentation for transient HD-sEMG. We show that without using the proposed augmentation, a slight shift of 10mm may drop the decoder&#39;s performance to as low as 20\%. Combining the proposed data augmentation with a 3D Convolutional Neural Network (CNN), we recovered the performance to 84.6\% while securing a high spatiotemporal resolution, robustifying to the electrode shift, and getting closer to large-scale adoption by the end-users, enhancing resiliency.},
  archive   = {C_IROS},
  author    = {Tianyun Sun and Jacqueline Libby and JohnRoss Rizzo and S. Farokh Atashzar},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981786},
  pages     = {6148-6153},
  title     = {Deep augmentation for electrode shift compensation in transient high-density sEMG: Towards application in neurorobotics},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Givenness hierarchy informed optimal document planning for
situated human-robot interaction. <em>IROS</em>, 6109–6115. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots that use natural language in collaborative tasks must refer to objects in their environment. Recent work has shown the utility of the linguistic theory of the Givenness Hierarchy (GH) in generating appropriate referring forms. But before referring expression generation, collaborative robots must determine the content and structure of a sequence of utterances, a task known as document planning in the natural language generation community. This problem presents additional challenges for robots in situated contexts, where described objects change both physically and in the minds of their interlocutors. In this work, we consider how robots can “think ahead” about the objects they must refer to and how to refer to them, sequencing object references to form a coherent, easy to follow chain. Specifically, we leverage GH to enable robots to plan their utterances in a way that keeps objects at a high cognitive status, which enables use of concise, anaphoric referring forms. We encode these linguistic insights as a mixed integer program within a planning context, formulating constraints to concisely and efficiently capture GH-theoretic cognitive properties. We demonstrate that this GH-informed planner generates sequences of utterances with high intersentential coherence, which we argue should enable substantially more efficient and natural human-robot dialogue.},
  archive   = {C_IROS},
  author    = {Kevin Spevak and Zhao Han and Tom Williams and Neil T. Dantam},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981811},
  pages     = {6109-6115},
  title     = {Givenness hierarchy informed optimal document planning for situated human-robot interaction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hey haru, let’s be friends! Using the tiers of friendship to
build rapport through small talk with the tabletop robot haru.
<em>IROS</em>, 6101–6108. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Conversation can play an essential role in forging bonds between humans and social robots, but participants need to feel like they are being listened to, remembered, and cared about in order to effectively build rapport. In this paper, we propose a novel strategy for conducting small talk with a social robot. Our approach is known as the Tiers of Friendship. It is centered around three core design elements: 1) Persuasive content and character is provided through topic modules created by professional creative writers to ensure engaging conversational content and a compelling personality for the social robot. 2) Conversational memory is achieved by allowing topic modules to specify required information that can be learned through conversation or recalled from previous interactions and organizing topic modules into a hierarchy that enforces information requirements between topics. 3) Dynamicity in conversation is promoted through topic navigation that supports fluid transitions to topics of human interest and employs elements of random ordering to create fresh conversation experiences. In this paper, we show how the Tiers of Friendship can be used to generate conversation content for a social robot that encourages the development of rapport. We describe a working implementation of a small talk system for a social robot based on the Tiers of Friendship that combines off-the-shelf ASR and NLU components and custom robot behavior components implemented via behavior trees on ROS. Finally, in order to evaluate our approach&#39;s effectiveness, we conduct an elicitation survey that evaluates conversations in terms of perceived engagement, personality traits, and rapport expectation and discuss the implications for social robotics.},
  archive   = {C_IROS},
  author    = {Eric Nichols and Sarah Rose Siskind and Levko Ivanchuk and Guillermo Pérez and Waki Kamino and Selma Šabanović and Randy Gomez},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981369},
  pages     = {6101-6108},
  title     = {Hey haru, let&#39;s be friends! using the tiers of friendship to build rapport through small talk with the tabletop robot haru},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic network model for multi-domain end-to-end
task-oriented dialogue system. <em>IROS</em>, 6095–6100. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dialogue State Tracking (DST) is an important part in task-oriented dialog system, whose target is to infer the current dialog states and user intentions according to the dialog history information. To this end, we have achieved improvements to the existing work and proposed a dynamic network model suitable for multi-domain dialog, which can explicitly use domain information and better cope with zero-shot tasks. The model is composed of three modules: an encoder, a decoder and a slot classifier. The encoder module introduces a mixed-separate framework so that it can obtain the feature information of each domain on the premise of extracting the shared information between all domains. The experimental results show that the model achieves joint accuracy of 48.38\% for the five domains of MultiWOZ, which is superior to existing models. Besides, by simulating the zero-shot scenario, the knowledge transferability of the model has also been well proven. Finally, in order to verify the effectiveness of the robot simulation system, this paper also uses the robot simulation technology to simulate the common tasks of helping users complete the task of taking items in the home environment service.},
  archive   = {C_IROS},
  author    = {F. D. Zhao and M. L. Qiu and X. S. Li and D. D. Guo},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981938},
  pages     = {6095-6100},
  title     = {Dynamic network model for multi-domain end-to-end task-oriented dialogue system},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lightmyography based decoding of human intention using
temporal multi-channel transformers. <em>IROS</em>, 6087–6094. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For the development of muscle-machine interfaces (MuMIs), researchers have relied mainly on Electromyography (EMG) signals. However, these signals require complex hardware systems, as well as specialized signal processing and feature extraction methods. To overcome these issues, in our previous work, we proposed a novel MuMI for decoding human intention and motion, called Lightmyography (LMG). To improve the performance of this interface even further, in this work, we employ two novel deep learning techniques called Temporal Multi-Channel Transformer (TMC-T) and Temporal Multi-Channel Vision Transformer (TMC-ViT) for the classification of hand gestures based on the LMG data. The performance of these two Transformer-based methods is evaluated and compared with other well-known deep learning and classical machine learning methods. This work also addresses the influence of varying parameters defined during the training phase of decoding models, such as the size and shape of the input data packet. A series of data augmentation techniques were also employed to generate synthetic data and increase the dataset size so as to train deep learning models more efficiently.},
  archive   = {C_IROS},
  author    = {Ricardo V. Godoy and Anany Dwivedi and Mojtaba Shahmohammadi and Minas Liarokapis},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981514},
  pages     = {6087-6094},
  title     = {Lightmyography based decoding of human intention using temporal multi-channel transformers},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multimodal aerial-tethered robot for tree canopy
exploration. <em>IROS</em>, 6080–6086. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Forest canopies are the biggest habitat for terrestrial life, yet our understanding of environmental processes and biodiversity inside the canopy continues to be limited due to labour and resource intensive data collection. Existing aerial and climbing robots also struggle to access these complex environments, while animals easily navigate them using multiple means of locomotion. Following this insight we present a robot with multimodal mobility obtained by combining aerial and tethered locomotion. After the robot is deployed at the top of the tree, it can descend with the tether and maneuver around leaves and branches with its thrusters. The tether increases robustness and safety and allows for resting as well as emergency retrieval of the system. The aerial locomotion grants the system the ability to move in a conical 3D space constrained by the tether. We modelled the static system and validated the impact of design parameters on it. A simple control architecture for teleoperation is discussed and its performance is analyzed. The proposed multimodal mobility is demonstrated in preliminary outdoor tests, which show how our robot can move within the canopy while continuously monitoring the environment.},
  archive   = {C_IROS},
  author    = {Steffen Kirchgeorg and Stefano Mintchev},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981121},
  pages     = {6080-6086},
  title     = {Multimodal aerial-tethered robot for tree canopy exploration},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Instance segmentation for autonomous log grasping in
forestry operations. <em>IROS</em>, 6064–6071. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Wood logs picking is a challenging task to automate. Indeed, logs usually come in cluttered configurations, randomly orientated and overlapping. Recent work on log picking automation usually assume that the logs&#39; pose is known, with little consideration given to the actual perception problem. In this paper, we squarely address the latter, using a data-driven approach. First, we introduce a novel dataset, named TimberSeg 1.0, that is densely annotated, i.e., that includes both bounding boxes and pixel-level mask annotations for logs. This dataset comprises 220 images with 2500 individually segmented logs. Using our dataset, we then compare three neural network architectures on the task of individual logs detection and segmentation; two region-based methods and one attention-based method. Unsurprisingly, our results show that axis-aligned proposals, failing to take into account the directional nature of logs, underperform with 19.03 mAP. A rotation-aware proposal method significantly improve results to 31.83 mAP. More interestingly, a Transformer-based approach, without any inductive bias on rotations, outperformed the two others, achieving a mAP of 57.53 on our dataset. Our use case demonstrates the limitations of region-based approaches for cluttered, elongated objects. It also highlights the potential of attention-based methods on this specific task, as they work directly at the pixel-level. These encouraging results indicate that such a perception system could be used to assist the operators on the short-term, or to fully automate log picking operations in the future.},
  archive   = {C_IROS},
  author    = {Jean-Michel Fortin and Olivier Gamache and Vincent Grondin and François Pomerleau and Philippe Giguère},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982286},
  pages     = {6064-6071},
  title     = {Instance segmentation for autonomous log grasping in forestry operations},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AFR: An efficient buffering algorithm for cloud robotic
systems. <em>IROS</em>, 6032–6039. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Communication between robots and the server is a major problem for cloud robotic systems. In this paper, we address the problem caused by data loss during such communications and propose an efficient buffering algorithm, called AFR, to solve the problem. We model the problem into an optimization problem to maximize the received Quantity of Information (QoI). Our AFR algorithm is formally proved to achieve near-optimal QoI, which has a lower bound that is a constant multiple of the unrealizable optimal QoI. We implement our AFR algorithm in ROS without changing the API for the applications. Our experiments on two cloud robot applications show that our AFR algorithm can efficiently and effectively reduce the impact of data loss. For the remote mapping application, the RMSE caused by data loss can be reduced by about 20\%. For the remote tracking application, the probability of tracking failure caused by data loss can be reduced from about 40\%-60\% to under 10\%. Meanwhile, our AFR algorithm introduces time overhead of under 10 microseconds.},
  archive   = {C_IROS},
  author    = {Yu-Ping Wang and Hao-Ning Wang and Zi-Xin Zou and Dinesh Manocha},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981400},
  pages     = {6032-6039},
  title     = {AFR: An efficient buffering algorithm for cloud robotic systems},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A flexible and robust vision trap for automated part feeder
design. <em>IROS</em>, 6017–6023. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fast, robust, and flexible part feeding is essential for enabling automation of low volume, high variance assembly tasks. An actuated vision-based solution on a traditional vibratory feeder, referred to here as a vision trap, should in principle be able to meet these demands for a wide range of parts. However, in practice, the flexibility of such a trap is limited as an expert is needed to both identify manageable tasks and to configure the vision system. We propose a novel approach to vision trap design in which the identification of manageable tasks is automatic and the configuration of these tasks can be delegated to an automated feeder design system. We show that the trap&#39;s capabilities can be formalized in such a way that it integrates seamlessly into the ecosystem of automated feeder design. Our results on six canonical parts show great promise for autonomous configuration of feeder systems.},
  archive   = {C_IROS},
  author    = {Rasmus Laurvig Haugaard and Thorbjϕrn Mosekjær Iversen and Anders Glent Buch and Aljaz Kramberger and Simon Faarvang Mathiesen},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981302},
  pages     = {6017-6023},
  title     = {A flexible and robust vision trap for automated part feeder design},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Trifocal tensor and relative pose estimation from 8 lines
and known vertical direction. <em>IROS</em>, 6001–6008. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a relative pose estimation algorithm based on lines knowing the vertical direction associated to each image. We demonstrate that a closed-form solution requiring only eight lines between three views is possible. As a linear solution, it is shown that our approach outperforms the standard trifocal estimation based on 13 triplets of lines and can be efficiently inserted into an hypothesize-and-test framework such as RANSAC. We also study our approach on different singular configurations of lines. The method is evaluated on both synthetic data and real-world sequences from KITTI and the Zürich Urban Micro Aerial Vehicle datasets. Our method is compared to 13 lines algorithm as well to points based methods such as 7-points, 5-points and 3-points.},
  archive   = {C_IROS},
  author    = {Banglei Guan and Pascal Vasseur and Cédric Demonceaux},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981481},
  pages     = {6001-6008},
  title     = {Trifocal tensor and relative pose estimation from 8 lines and known vertical direction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual odometry in HDR environments by using spatially
varying exposure camera. <em>IROS</em>, 5995–6000. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The accuracy and robustness of visual odometry (VO) is significantly affected by the high dynamic range (HDR) environments, because traditional cameras have a limited dynamic range and inevitably miss information in both overexposed and underexposed areas. To overcome the above challenge, we use an spatially varying exposure (SVE) camera, which captures four images with different exposure levels simultaneously. Then, we propose a VO pipeline that leverages the advantages of the SVE camera. Specifically, we extract ORB features from four images in parallel firstly instead of fusing four images, then perform merging and filtering to provide more robust features. We demonstrate that the proposed system outperforms comparable state-of-the-art methods in terms of robustness and accuracy. The real-time performance of the proposed system is also guaranteed due to the elaborate design of the parallel algorithm.},
  archive   = {C_IROS},
  author    = {Keyang Ye and Liuzheng Gao and Banglei Guan},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981538},
  pages     = {5995-6000},
  title     = {Visual odometry in HDR environments by using spatially varying exposure camera},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SESR: Self-ensembling sim-to-real instance segmentation for
auto-store bin picking. <em>IROS</em>, 5987–5994. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Instance segmentation is an important task for supporting robotic grasping in auto-store scenarios. Accurate segmentation usually relies on the quantity and quality of available annotated training data. However, it requires tremendous cost to obtain these labels. In this work, without requiring any human annotations on real data, our proposed self-ensembling sim-to-real network, namely SESR, is able to generate precise instance masks for a wide variety of supermarket goods. We design our SESR with a teacher model and a student model trained with a self-ensembling strategy. We adopt different levels of consistency to bridge the sim-to-real gap and boost the model generalization ability. Also, we compile an auto-store bin-picking dataset covering various goods. Extensive experiments on both unseen scenarios and unseen objects validate the effectiveness and superiority of our method over others, and the robot arm demonstrations further show that our segmentation results can support real-time auto-store bin picking.},
  archive   = {C_IROS},
  author    = {Biqi Yang and Xiaojie Gao and Kai Chen and Rui Cao and Yidan Feng and Xianzhi Li and Qi Dou and Chi-Wing Fu and Yun-Hui Liu and Pheng-Ann Heng},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981845},
  pages     = {5987-5994},
  title     = {SESR: Self-ensembling sim-to-real instance segmentation for auto-store bin picking},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CPQNet: Contact points quality network for robotic grasping.
<em>IROS</em>, 5981–5986. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In typical data-based grasping methods, a grasp based on parallel-jaw grippers is parameterized by the center of the gripper, the rotation angle, and the gripper opening width so as to predict the quality and pose of grasps at every pixel. In contrast, a grasp is represented using only two contact points for contact-points-based grasp representation, which allows for fusion with tactile sensors more naturally. In this work, we propose a method using contact-points-based grasp representation to get a robust grasp using only one contact points quality map generated by a neural network, which significantly reduces the complexity of the network with fewer parameters. We provide a synthetic dataset including depth image and contact points quality map generated by thousands of 3D models. We also provide the method for data generation, which can be used for contact-points-based multi-fingers grasp. Experiments show that contact points quality network can plan an available grasp in 0.15 seconds. The grasping success rate for unknown household objects is 94\%. Our method is also available for deformable objects with a success rate of 95\%. The dataset and reference code can be found on the project website: https://sites.google.com/view/cpqnet.},
  archive   = {C_IROS},
  author    = {Zhihao Li and Pengfei Zeng and Jionglong Su and Qingda Guo and Ning Ding and Jiaming Zhang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981372},
  pages     = {5981-5986},
  title     = {CPQNet: Contact points quality network for robotic grasping},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RGB-x classification for electronics sorting. <em>IROS</em>,
5973–5980. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Effectively disassembling and recovering materials from waste electrical and electronic equipment (WEEE) is a critical step in moving global supply chains from carbon-intensive, mined materials to recycled and renewable ones. Conventional recycling processes rely on shredding and sorting waste streams, but for WEEE, which is comprised of numerous dissimilar materials, we explore targeted disassembly of numerous objects for improved material recovery. Many WEEE objects share many key features and therefore can look quite similar, but their material composition and internal component layout can vary, and thus it is critical to have an accurate classifier for subsequent disassembly steps for accurate material separation and recovery. This work introduces RGB-X, a multi-modal image classification approach, that utilizes key features from external RGB images with those generated from X-ray images to accurately classify electronic objects. More specifically, this work develops Iterative Class Activation Mapping (iCAM), a novel network architecture that explicitly focuses on the finer-details in the multi-modal feature maps that are needed for accurate electronic object classification. In order to train a classifier, electronic objects lack large and well annotated X-ray datasets due to expense and need of expert guidance. To overcome this issue, we present a novel way of creating a synthetic dataset using domain randomization applied to the X-ray domain. The combined RGB-X approach gives us an accuracy of 98.6\% on 10 generations of modern smartphones, which is greater than their individual accuracies of 89.1\% (RGB) and 97.9\% (X-ray) independently. We provide experimental results 1 3 3 Experimental work done at Biorobotics Lab, Robotics Institute, Carnegie Mellon University to corroborate our results.},
  archive   = {C_IROS},
  author    = {Abhimanyu Fnu and Tejas Zodage and Umesh Thillaivasan and Xinyue Lai and Rahul Chakwate and Javier Santillan and Emma Oti and Ming Zhao and Ralph Boirum and Howie Choset and Matthew Travers},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981860},
  pages     = {5973-5980},
  title     = {RGB-X classification for electronics sorting},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A miniature continuum robot with integrated piezoelectric
beacon transducers and its ultrasonic shape detection in robot-assisted
minimally invasive surgeries. <em>IROS</em>, 5945–5950. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Minimally invasive surgeries (MIS) or natural orifice transluminal endoscopic surgeries (NOTES) such as the transurethral resection of bladder tumor (TURBT) require the surgical robot to be miniaturized to perform surgical procedures in confined spaces. However, the surgical robot&#39;s tiny size poses problems in its fabrication and shape sensing. In this paper, a miniature continuum surgical robot is proposed with a unique laminated structure which can be fabricated through a 2D lamination process and converted into 3D through folding. This multi-material laminated structure also facilitates the integration of tiny piezoelectric transducers on the robot&#39;s surface as beacons to generate ultrasonic waves for shape detection. A novel beacon total focusing method (b-TFM) algorithm is developed to process the received ultrasonic data and create a high-quality ultrasonic image from which the shape of the continuum robot can be extracted. The proposed robot and the ultrasonic shape detection method are validated through simulations and experiments. The error in the open-loop trajectory control is less than 4 mm without compensation, and the error in the ultrasonic shape detection is less than 1 mm. This confirms the possibility of improving the trajectory control accuracy by using the detected shape as a feedback for closed-loop control.},
  archive   = {C_IROS},
  author    = {Zhanpeng Yin and Yan Hong and Xiaoyu Sun and Zhiyuan Shen and Yingxuan Zhang and Feng Ju and Bruce W. Drinkwater},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981700},
  pages     = {5945-5950},
  title     = {A miniature continuum robot with integrated piezoelectric beacon transducers and its ultrasonic shape detection in robot-assisted minimally invasive surgeries},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design, teleoperation control and experimental validation of
a dexterous robotic flexible endoscope for laparoscopic surgery.
<em>IROS</em>, 5937–5944. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing robotic endoscopes for laparoscopic surgery, predominantly rigid or limited in dexterity, occupy a large motion space 1 , The large occupied motion space necessitates large incisions and reduces the motion space for surgeons to simultaneously operate other surgical instruments. Meanwhile, surgeons only have limited view adjustment capability to avoid occlusion and they often have to lift/push some organs to observe occluded target lesions in some operations such as cholecystectomy. The situation gets worse when the operations are on obese patients. In this paper, we develop a novel dexterous robotic flexible endoscope (DRFE), which is comprised of a concentric cable-driven structure and a 2-DoF articulated joint attached to the end of DRFE, for laparoscopic surgery. The proposed design occupies much less motion space both inside and outside human body as compared to conventional robotic flexible endoscopes. When used in surgery, the part of the endoscope outside the body can remain still, which reduces the risk of expanding the incision and simplifies the structure of the remote center mechanism. Simulation and experimental studies are performed to validate the effectiveness of the proposed device in the improvement of vision occlusion and usability. Initial results reveal that the DRFE is highly dexterous and accurate in observing lesions with vision occlusion.},
  archive   = {C_IROS},
  author    = {Xin Ma and Xuchen Wang and Rui Cao and Kwok Wai Samuel Au},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981536},
  pages     = {5937-5944},
  title     = {Design, teleoperation control and experimental validation of a dexterous robotic flexible endoscope for laparoscopic surgery},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Model-free and uncalibrated visual-feedback control of
magnetically-actuated flexible endoscopes. <em>IROS</em>, 5930–5936. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Magnetically-actuated flexible endoscopes (MAFE) have been well used in minimally-invasive surgery because they can be steered by a magnetic field thus more flexible than traditional endoscopes. Model-free and uncalibrated visual-feedback control makes it possible to manipulate MAFE with a magnetic field without external tracking systems. Because no extra sensor is required to obtain position and posture information, the size of MAFE can be made smaller. However, the traditional control method focuses on 2DoF control, which lacks control over the posture of the end of MAFE. This may result in unnecessary contact between MAFE and tissue and cause injury during the advancement of the endoscope. In this letter, we propose algorithms to enhance the pose control of MAFE to 4DoF and 5DoF based on model-free and uncalibrated visual-feedback control. Experiments in structured environments verify that the control algorithms are able to realize 4DoF manual navigation and 5DoF automatic navigation.},
  archive   = {C_IROS},
  author    = {Jiewen Tan and Junnan Xue and Xing Yang and Sishen Yuan and Wei Liu and Hongliang Ren and Shuang Song and Jiaole Wang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982055},
  pages     = {5930-5936},
  title     = {Model-free and uncalibrated visual-feedback control of magnetically-actuated flexible endoscopes},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robotic actuation and control of a catheter for structural
intervention cardiology. <em>IROS</em>, 5907–5913. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Structural intervention cardiology (SIC) interventions are crucial procedures for correcting heart valves, walls, and muscle form defects. However, the possibility of embolization or perforation, as well as the lack of transparent vision and autonomous surgical equipment, make it difficult for the clinician. This paper proposes a robot-assisted tendon-driven catheter and machine learning-based path planner to overcome these challenges. Firstly, an analytical inverse kinematic model is constructed to convert the tip location in the Cartesian space to the tendons&#39; displacement. Then inverse reinforcement learning algorithm is employed to calculate the optimal path to avoid possible collisions between the catheter tip and the atrial wall. Moreover, a closed-loop feedback controller is adopted to improve positioning accuracy in a direct distal position measurement manner. Simulation and experiments are designed and conducted to demonstrate the feasibility and performance of the proposed system.},
  archive   = {C_IROS},
  author    = {Xiu Zhang and Maria Chiara Palumbo and Francesca Perico and Mattia Magro and Andrea Fortuna and Tommaso Magni and Emiliano Votta and Alice Segato and Elena De Momi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981676},
  pages     = {5907-5913},
  title     = {Robotic actuation and control of a catheter for structural intervention cardiology},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Time-optimal synchronous terminal trajectory planning for
coupling motions of robotic flexible endoscope. <em>IROS</em>,
5901–5906. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The robotic flexible endoscope is developed rapidly in the field of surgery robots due to its high flexibility and safety. However, some inherent features, e.g., high nonlinearity, material creep, complex dynamic hysteresis behaviors, and the unknown coupling effects between bending and twisting motions, can lead to the significant degradation on three-dimensional (3-D) positioning performance of the endoscope. Aiming at these challenges, this paper built a practical multi-motion hysteresis phenomenon model for the bending and twisting motions of the robotic flexible endoscope with consideration of the coupling effects. Then, the time-optimal synchronous terminal motion planner is first proposed for the 3-D motions of the robotic endoscope to decouple the coupling effects in an intuitive separate control scheme. Finally, a series of hardware experiments are conducted on a robotic flexible ureteroscope platform. The accuracy of the proposed model and the trajectory-planning-based decoupling strategy is comprehensively validated. Particularly, the experimental results with the proposed trajectory planner show the satisfactory performance of vibration suppression and over-shoot suppression.},
  archive   = {C_IROS},
  author    = {Xiangyu Wang and Ningbo Yu and Jianda Han and Yongchun Fang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982044},
  pages     = {5901-5906},
  title     = {Time-optimal synchronous terminal trajectory planning for coupling motions of robotic flexible endoscope},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to assess danger from movies for cooperative escape
planning in hazardous environments. <em>IROS</em>, 5885–5892. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There has been a plethora of work towards im-proving robot perception and navigation, yet their application in hazardous environments, like during a fire or an earthquake, is still at a nascent stage. We hypothesize two key challenges here: first, it is difficult to replicate such scenarios in the real world, which is necessary for training and testing purposes. Second, current systems are not fully able to take advantage of the rich multi-modal data available in such hazardous environments. To address the first challenge, we propose to harness the enormous amount of visual content available in the form of movies and TV shows, and develop a dataset that can represent hazardous environments encountered in the real world. The data is annotated with high-level danger ratings for realistic disaster images, and corresponding keywords are provided that summarize the content of the scene. In response to the second challenge, we propose a multi-modal danger estimation pipeline for collaborative human-robot escape scenarios. Our Bayesian framework improves danger estimation by fusing information from robot&#39;s camera sensor and language inputs from the human. Furthermore, we augment the estimation module with a risk-aware planner that helps in identifying safer paths out of the dangerous environment. Through extensive simulations, we exhibit the advantages of our multi-modal perception framework that gets translated into tangible benefits such as higher success rate in a collaborative human-robot mission.},
  archive   = {C_IROS},
  author    = {Vikram Shree and Sarah Allen and Beatriz Asfora and Jacopo Banfi and Mark Campbell},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982279},
  pages     = {5885-5892},
  title     = {Learning to assess danger from movies for cooperative escape planning in hazardous environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to act with affordance-aware multimodal neural
SLAM. <em>IROS</em>, 5877–5884. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent years have witnessed an emerging paradigm shift toward embodied artificial intelligence, in which an agent must learn to solve challenging tasks by interacting with its environment. There are several challenges in solving embodied multimodal tasks, including long-horizon planning, vision-and-language grounding, and efficient exploration. We focus on a critical bottleneck, namely the performance of planning and navigation. To tackle this challenge, we propose a Neural SLAM approach that, for the first time, utilizes several modalities for exploration, predicts an affordance-aware semantic map, and plans over it at the same time. This signif-icantly improves exploration efficiency, leads to robust long-horizon planning, and enables effective vision-and-language grounding. With the proposed Affordance-aware Multimodal Neural SLAM (AMSLAM) approach, we obtain more than 40\% improvement over prior published work on the ALFRED benchmark and set a new state-of-the-art generalization per-formance at a success rate of 23.48\% on the test unseen scenes.},
  archive   = {C_IROS},
  author    = {Zhiwei Jia and Kaixiang Lin and Yizhou Zhao and Qiaozi Gao and Govind Thattai and Gaurav S. Sukhatme},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981261},
  pages     = {5877-5884},
  title     = {Learning to act with affordance-aware multimodal neural SLAM},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Influence of variable leg elasticity on the stability of
quadrupedal gaits. <em>IROS</em>, 5869–5876. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Several template models have been developed to facilitate the analysis of limit-cycles for quadrupedal locomotion. The parameters in the model are usually fixed; however, biology shows that animals change their leg stiffness according to the locomotion velocity, and this adaptability invariably affects the stability of the gait. This paper provides an analysis of the influence of this variable leg stiffness on the stability of different quadrupedal gaits. The analysis exploits a simplified quadrupedal model with compliant legs and shoulder joints represented as torsional springs. This model can reproduce the most common quadrupedal gaits observed in nature. The stability of such emerging gaits is then checked. Afterward, an optimization process is used to search for the system parameters that guarantee maximum gait stability. Our study shows that using the highest feasible leg swing frequency and adopting a leg stiffness that increases with the speed of locomotion noticeably improves the gait stability over a wide range of horizontal velocities while reducing the oscillations of the trunk. This insight can be applied in the design of novel elastic quadrupedal robots, where variable stiffness actuators could be employed to improve the overall locomotion behavior.},
  archive   = {C_IROS},
  author    = {Federico Del Fatti and Anna Sesselmann and Máximo A. Roa},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982204},
  pages     = {5869-5876},
  title     = {Influence of variable leg elasticity on the stability of quadrupedal gaits},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SMA-NBO: A sequential multi-agent planning with nominal
belief-state optimization in target tracking. <em>IROS</em>, 5861–5868.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In target tracking with mobile multi-sensor sys-tems, sensor deployment impacts the observation capabilities and the resulting state estimation quality. Based on a partially observable Markov decision process (POMDP) formulation comprised of the observable sensor dynamics, unobservable target states, and accompanying observation laws, we present a distributed information-driven solution approach to the multi-agent target tracking problem, namely, sequential multi-agent nominal belief-state optimization (SMA-NBO). SMA-NBO seeks to minimize the expected tracking error via receding horizon control including a heuristic expected cost-to-go (HECTG). SMA-NBO incorporates a computationally efficient approximation of the target belief-state over the horizon. The agent-by-agent decision-making is capable of leveraging on-board (edge) compute for selecting (sub-optimal) target-tracking maneuvers exhibiting non-myopic cooperative fleet behavior. The opti-mization problem explicitly incorporates semantic information defining target occlusions from a world model. To illustrate the efficacy of our approach, a random occlusion forest environment is simulated. SMA-NBO is compared to other baseline approaches. The simulation results show SMA-NBO 1) maintains tracking performance and reduces the computational cost by replacing the calculation of the expected target trajectory with a single sample trajectory based on maximum a posteriori estimation; 2) generates cooperative fleet decision by sequentially optimizing single-agent policy with efficient usage of other agents&#39; policy of intent; 3) aptly incorporates the multiple weighted trace penalty (MWTP) HECTG, which improves tracking performance with a computationally efficient heuristic.},
  archive   = {C_IROS},
  author    = {Tianqi Li and Lucas W. Krakow and Swaminathan Gopalswamy},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981459},
  pages     = {5861-5868},
  title     = {SMA-NBO: A sequential multi-agent planning with nominal belief-state optimization in target tracking},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Perceive, represent, generate: Translating multimodal
information to robotic motion trajectories. <em>IROS</em>, 5855–5860.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present Perceive-Represent-Generate (PRG), a novel three-stage framework that maps perceptual information of different modalities (e.g., visual or sound), corresponding to a series of instructions, to a sequence of movements to be executed by a robot. In the first stage, we perceive and preprocess the given inputs, isolating individual commands from the complete instruction provided by a human user. In the second stage we encode the individual commands into a multimodal latent space, employing a deep generative model. Finally, in the third stage we convert the latent samples into individual trajectories and combine them into a single dynamic movement primitive, allowing its execution by a robotic manipulator. We evaluate our pipeline in the context of a novel robotic handwriting task, where the robot receives as input a word through different perceptual modalities (e.g., image, sound), and generates the corresponding motion trajectory to write it, creating coherent and high-quality handwritten words.},
  archive   = {C_IROS},
  author    = {Fábio Vital and Miguel Vasco and Alberto Sardinha and Francisco Melo},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981788},
  pages     = {5855-5860},
  title     = {Perceive, represent, generate: Translating multimodal information to robotic motion trajectories},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conservative filtering for heterogeneous decentralized data
fusion in dynamic robotic systems. <em>IROS</em>, 5840–5847. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a method for Bayesian multi-robot peer-to-peer data fusion where any pair of autonomous robots hold non-identical, but overlapping parts of a global joint probability distribution, representing real world inference tasks (e.g., mapping, tracking). It is shown that in dynamic stochastic systems, filtering, which corresponds to marginalization of past variables, results in direct and hidden dependencies between variables not mutually monitored by the robots, which might lead to an overconfident fused estimate. The paper makes both theoretical and practical contributions by providing (i) a rigorous analysis of the origin of the dependencies and (ii) a conservative filtering algorithm for heterogeneous data fusion in dynamic systems that can be integrated with existing fusion algorithms. This work uses factor graphs as both the analysis tool and the inference engine. Each robot in the network maintains a local factor graph and communicates only relevant parts of it (a sub-graph) to its neighboring robot. We discuss the applicability to various multi-robot robotic applications and demonstrate the performance using a multi-robot multi-target tracking simulation, showing that the proposed algorithm produces conservative estimates at each robot.},
  archive   = {C_IROS},
  author    = {Ofer Dagan and Nisar R. Ahmed},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981414},
  pages     = {5840-5847},
  title     = {Conservative filtering for heterogeneous decentralized data fusion in dynamic robotic systems},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time distributed multi-robot target tracking via
virtual pheromones. <em>IROS</em>, 5833–5839. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Actively searching for targets using a multi-agent system in an unknown environment poses a two-pronged prob-lem, where on the one hand we need agents to cover as much of the environment as possible and on the other have a higher density of agents where there are potential targets to maximize detection performance. This paper proposes a fully distributed solution for an ad hoc network of agents to cooperatively search an unknown environment and actively track found targets. The solution combines a distributed pheromone-based coverage control strategy with a distributed target selection mechanism.},
  archive   = {C_IROS},
  author    = {Joseph Prince Mathew and Cameron Nowzari},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981262},
  pages     = {5833-5839},
  title     = {Real-time distributed multi-robot target tracking via virtual pheromones},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning of balance controller considering changes in body
state for musculoskeletal humanoids. <em>IROS</em>, 5809–5816. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The musculoskeletal humanoid is difficult to modelize due to the flexibility and redundancy of its body, whose state can change over time, and so balance control of its legs is challenging. There are some cases where ordinary PID controls may cause instability. In this study, to solve these problems, we propose a method of learning a correlation model among the joint angle, muscle tension, and muscle length of the ankle and the zero moment point to perform balance control. In addition, information on the changing body state is embedded in the model using parametric bias, and the model estimates and adapts to the current body state by learning this information online. This makes it possible to adapt to changes in upper body posture that are not directly taken into account in the model, since it is difficult to learn the complete dynamics of the whole body considering the amount of data and computation. The model can also adapt to changes in body state, such as the change in footwear and change in the joint origin due to recalibration. The effectiveness of this method is verified by a simulation and by using an actual musculoskeletal humanoid, Musashi.},
  archive   = {C_IROS},
  author    = {Kento Kawaharazuka and Yoshimoto Ribayashi and Akihiro Miki and Yasunori Toshimitsu and Temma Suzuki and Kei Okada and Masayuki Inaba},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981051},
  pages     = {5809-5816},
  title     = {Learning of balance controller considering changes in body state for musculoskeletal humanoids},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Development of a conveyor-type object release mechanism for
a parallel gripper with a mushroom-shaped gecko-inspired surface.
<em>IROS</em>, 5787–5793. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A surface microstructure that mimics the surface of a gecko&#39;s foot can exert a large gripping force with a small contact force. If such a structure is applied to the fingertips of a two-fingered parallel gripper, stable grasping can be achieved independent of the wetting and frictional state of the contact surface. However, the adhesive force of the microstructure is large while releasing the object, which hinders the release of the object. In this study, we developed a release method using a conveyor mechanism that easily peels off in the direction of rotation with a focus on the characteristics of the micro-protrusion structure. This mechanism is driven in conjunction with the gripper&#39;s grasping and releasing motions. Our experiments confirmed that the gripper can stably release the object using the proposed mechanism. The proposal in this paper is a mechanism that dynamically changes the adhesive force on a fingertip by mechanically switching the surface state in accordance with the gripper&#39;s grasping and releasing states. This idea can be applied to not only surface microstructure such as gecko-inspired surfaces but also adhesive surfaces such as adhesive tape, and provides novel knowledge in the field of robotics as a method of mechanically changing the fingertip adhesive force.},
  archive   = {C_IROS},
  author    = {Shunsuke Nagahama and Atsushi Nakao and Shigeki Sugano},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981232},
  pages     = {5787-5793},
  title     = {Development of a conveyor-type object release mechanism for a parallel gripper with a mushroom-shaped gecko-inspired surface},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A control architecture of a distributed actuator system for
a bio-inspired spine. <em>IROS</em>, 5781–5786. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Control of an articulated spine is important for humanoids&#39; dynamic and balanced motion. Although there have been many spinal structures for humanoids, their actuation is still limited due to the usage of geared motors for joints. This paper introduces position control of a distributed electrome-chanical spine in a vertical plane. The spine dynamics model is approximated as an open chain. Gravitational and spring torques are compensated for the control. Moreover, torque-to-current conversion for the actuator is developed. Experimental results show the implemented control of the electromechanical spine for undulatory motions.},
  archive   = {C_IROS},
  author    = {Bonhyun Ku and Arijit Banerjee},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981571},
  pages     = {5781-5786},
  title     = {A control architecture of a distributed actuator system for a bio-inspired spine},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Realization of seated walk by a musculoskeletal humanoid
with buttock-contact sensors from human constrained teaching.
<em>IROS</em>, 5774–5780. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this study, seated walk, a movement of walking while sitting on a chair with casters, is realized on a musculoskeletal humanoid from human teaching. The body is balanced by using buttock-contact sensors implemented on the planar interskeletal structure of the human mimetic musculoskeletal robot. Also, we develop a constrained teaching method in which one-dimensional control command, its transition, and a transition condition are described for each state in advance, and a threshold value for each transition condition such as joint angles and foot contact sensor values is determined based on human teaching. Complex behaviors can be easily generated from simple inputs. In the musculoskeletal humanoid MusashiOLegs, forward, backward, and rotational movements of seated walk are realized.},
  archive   = {C_IROS},
  author    = {Kento Kawaharazuka and Kei Okada and Masayuki Inaba},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982103},
  pages     = {5774-5780},
  title     = {Realization of seated walk by a musculoskeletal humanoid with buttock-contact sensors from human constrained teaching},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Online learning feedback control considering hysteresis for
musculoskeletal structures. <em>IROS</em>, 5767–5773. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While the musculoskeletal humanoid has various biomimetic benefits, its complex modeling is difficult, and many learning control methods have been developed. However, for the actual robot, the hysteresis of its joint angle tracking is still an obstacle, and realizing target posture quickly and accurately has been difficult. Therefore, we develop a feedback control method considering the hysteresis. To solve the problem in feedback controls caused by the closed-link structure of the musculoskeletal body, we update a neural network representing the relationship between the error of joint angles and the change in target muscle lengths online, and realize target joint angles accurately in a few trials. We compare the performance of several configurations with various network structures and loss definitions, and verify the effectiveness of this study on an actual musculoskeletal humanoid, Musashi.},
  archive   = {C_IROS},
  author    = {Kento Kawaharazuka and Kei Okada and Masayuki Inaba},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981052},
  pages     = {5767-5773},
  title     = {Online learning feedback control considering hysteresis for musculoskeletal structures},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TIGRIS: An informed sampling-based algorithm for informative
path planning. <em>IROS</em>, 5760–5766. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Informative path planning is an important and challenging problem in robotics that remains to be solved in a manner that allows for wide-spread implementation and real-world practical adoption. Among various reasons for this, one is the lack of approaches that allow for informative path planning in high-dimensional spaces and non-trivial sensor constraints. In this work we present a sampling-based approach that allows us to tackle the challenges of large and high-dimensional search spaces. This is done by performing informed sampling in the high-dimensional continuous space and incorporating potential information gain along edges in the reward estimation. This method rapidly generates a global path that maximizes information gain for the given path budget constraints. We discuss the details of our implementation for an example use case of searching for multiple objects of interest in a large search space using a fixed-wing UAV with a forward-facing camera. We compare our approach to a sampling-based planner baseline and demonstrate how our contributions allow our approach to consistently out-perform the baseline by 18.0\%. With this we thus present a practical and generalizable informative path planning framework that can be used for very large environments, limited budgets, and high dimensional search spaces, such as robots with motion constraints or high-dimensional configuration spaces. [Code] a a Codebase: https://github.com/castacks/tigris [Video] b b Video: https://youtu.be/bMw5nUGL5GQ},
  archive   = {C_IROS},
  author    = {Brady Moon and Satrajit Chatterjee and Sebastian Scherer},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981992},
  pages     = {5760-5766},
  title     = {TIGRIS: An informed sampling-based algorithm for informative path planning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quantity over quality: Training an AV motion planner with
large scale commodity vision data. <em>IROS</em>, 5752–5759. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the Autonomous Vehicle (AV) industry shifting towards machine-learned approaches for motion plan-ning [1], the performance of self-driving systems is starting to rely heavily on large quantities of expert driving demon-strations. However, collecting this demonstration data typically involves expensive HD sensor suites (LiDAR + RADAR + cameras), which quickly becomes financially infeasible at the scales required. This motivates the use of commodity sensors like cameras for data collection, which are an order of mag-nitude cheaper than HD sensor suites, but offer lower fidelity. Leveraging these sensors for training an AV motion planner opens a financially viable path to observe the ‘long tail’ of driving events. As our main contribution we show it is possible to train a high-performance motion planner using commodity vision data which outperforms planners trained on HD-sensor data for a fraction of the cost. To the best of our knowledge, we are the first to demonstrate this using real-world data. We compare the performance of the autonomy system on these two different sensor configurations, and show that we can compensate for the lower sensor fidelity by means of increased quantity: a planner trained on 100h of commodity vision data outperforms the one with 25h of expensive HD data (see Fig. 1). We also share the engineering challenges we had to tackle to make this work.},
  archive   = {C_IROS},
  author    = {Lukas Platinsky and Tayyab Naseer and Hui Chen and Ben Haines and Haoyue Zhu and Hugo Grimmett and Luca Del Pero},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982116},
  pages     = {5752-5759},
  title     = {Quantity over quality: Training an AV motion planner with large scale commodity vision data},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online complete coverage path planning of a reconfigurable
robot using glasius bio-inspired neural network and genetic algorithm.
<em>IROS</em>, 5744–5751. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Area coverage is crucial for robotics applications such as cleaning, painting, exploration, and inspections. Hinged reconfigurable robots have been introduced for these application domains to improve the area coverage performance. However, the existing coverage algorithms of hinged reconfigurable robots require improvements in the aspects; consideration of beyond a limited set of reconfigurable shapes, coordinated reconfiguration and navigation, and online decision-making. Therefore, this paper proposes a novel online Complete Coverage Path Planning (CCPP) method for a hinged reconfigurable robot. The proposed CCPP method is designed with two sub-methods, the Global Coverage Path Planning (GCPP) and Local Coverage Path Planning (LCPP). The GCPP method has been implemented, adapting a Glasius Bio-inspired Neural Network (GBNN) that performs online path planning considering a fixed shape for the robot. Obstacle regions that the GCPP would not adequately cover due to access constraints are covered by the LCPP method that considers concurrent reconfiguration and navigation of the robot. A genetic algorithm determines the reconfiguration parameters that ascertain collision-free coverage and access of obstacle regions. Experimental results validate that the proposed online CCPP method is effective in ascertaining the complete area coverage in heterogeneous environments, including dynamic workspaces. Furthermore, the deployment of the LCPP method can considerably improve the coverage.},
  archive   = {C_IROS},
  author    = {S. M. Bhagya P. Samarakoon and M. A. Viraj J. Muthugala and Mohan Rajesh Elara},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981667},
  pages     = {5744-5751},
  title     = {Online complete coverage path planning of a reconfigurable robot using glasius bio-inspired neural network and genetic algorithm},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Imitation learning and model integrated excavator trajectory
planning. <em>IROS</em>, 5737–5743. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automated excavation is promising to improve the safety and efficiency of excavators, and trajectory planning is one of the most important techniques. In this paper, we propose a two-stage method that integrates data-driven imitation learning and model-based trajectory optimization to generate optimal trajectories for autonomous excavators. We firstly train a deep neural network using demonstration data to mimic the operation patterns of human experts under various terrain states including their geometry shape and material type. Then, we use a stochastic trajectory optimization method to improve the trajectory generated by the neural network to guarantee kinematics feasibility, improve smoothness, satisfy hard constraints, and achieve desired excavation volumes. We test the proposed algorithm on a Franka robot arm equipped with a bucket end-effector. We further evaluate our method on different material types, such as sand and rigid blocks. The ex-perimental results show that the proposed two-stage algorithm by combining expert knowledge and model optimization can increase the excavation weights by up to 24.77\% meanwhile with low variance.},
  archive   = {C_IROS},
  author    = {Qiangqiang Guo and Zhixian Ye and Liyang Wang and Liangjun Zhang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981220},
  pages     = {5737-5743},
  title     = {Imitation learning and model integrated excavator trajectory planning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Jerk-continuous online trajectory generation for robot
manipulator with arbitrary initial state and kinematic constraints.
<em>IROS</em>, 5730–5736. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents an online trajectory generation algorithm using a sinusoidal jerk profile. The generator takes initial acceleration, velocity and position as input, and plans a multi-segment trajectory to a goal position under jerk, acceleration, and velocity limits. By analyzing the critical constraints and conditions, the corresponding closed-form solution for the time factors and trajectory profiles are derived. The proposed algorithm was first derived in Mathematica and then converted into a C++ implementation. Finally, the algorithm was utilized and demonstrated in ROS &amp; Gazebo using a UR3 robot. Both the Mathematica and C++ implementations can be accessed at https://github.com/Haoran-Zhao/Jerk-continuous-online-trajectory-generator-with-constraints.git},
  archive   = {C_IROS},
  author    = {Haoran Zhao and Nihal Abdurahiman and Nikhil Navkar and Julien Leclerc and Aaron T. Becker},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981086},
  pages     = {5730-5736},
  title     = {Jerk-continuous online trajectory generation for robot manipulator with arbitrary initial state and kinematic constraints},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Locomotion policy guided traversability learning using
volumetric representations of complex environments. <em>IROS</em>,
5722–5729. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite the progress in legged robotic locomotion, autonomous navigation in unknown environments remains an open problem. Ideally, the navigation system utilizes the full potential of the robots&#39; locomotion capabilities while operating within safety limits under uncertainty. The robot must sense and analyze the travers ability of the surrounding terrain, which depends on the hardware, locomotion control, and terrain properties. It may contain information about the risk, energy, or time consumption needed to traverse the terrain. To avoid hand-crafted traversability cost functions we propose to collect traversability information about the robot and locomotion policy by simulating the traversal over randomly generated terrains using a physics simulator. Thousand of robots are simulated in parallel controlled by the same locomotion policy used in reality to acquire 57 years of real-world locomotion experience equivalent. For deployment on the real robot, a sparse convolutional network is trained to predict the simulated traversability cost, which is tailored to the deployed locomotion policy, from an entirely geometric representation of the envi-ronment in the form of a 3D voxel-occupancy map. This rep-resentation avoids the need for commonly used elevation maps, which are error-prone in the presence of overhanging obstacles and multi-floor or low-ceiling scenarios. The effectiveness of the proposed travers ability prediction network is demonstrated for path planning for the legged robot ANY mal in various indoor and natural environments.},
  archive   = {C_IROS},
  author    = {Jonas Frey and David Hoeller and Shehryar Khattak and Marco Hutter},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982190},
  pages     = {5722-5729},
  title     = {Locomotion policy guided traversability learning using volumetric representations of complex environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Elevation state-space: Surfel-based navigation in uneven
environments for mobile robots. <em>IROS</em>, 5715–5721. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a new method for robot motion planning and navigation in uneven environments through a surfel representation of underlying point clouds. The proposed method addresses the shortcomings of state-of-the-art navigation methods by incorporating both kinematic and physical constraints of a robot with standard motion planning algorithms (e.g., those from the Open Motion Planning Library), thus enabling efficient sampling-based planners for challenging uneven terrain navigation on raw point cloud maps. Unlike techniques based on Digital Elevation Maps (DEMs), our novel surfel-based state-space formulation and implementation are based on raw point cloud maps, allowing for the modeling of overlapping surfaces such as bridges, piers, and tunnels. Experimental results demonstrate the robustness of the proposed method for robot navigation in real and simulated unstructured environments. The proposed approach also optimizes planners&#39; performances by boosting their success rates up to 5x for challenging unstructured terrain planning and navigation, thanks to our surfel-based approach&#39;s robot constraint-aware sampling strategy. Finally, we provide an open-source implementation of the proposed method to benefit the robotics community.},
  archive   = {C_IROS},
  author    = {Fetullah Atas and Grzegorz Cielniak and Lars Grimstad},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981647},
  pages     = {5715-5721},
  title     = {Elevation state-space: Surfel-based navigation in uneven environments for mobile robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Disk-graph probabilistic roadmap: Biased distance sampling
for path planning in a partially unknown environment. <em>IROS</em>,
5707–5714. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a new sampling-based path planning approach, focusing on the challenges linked to autonomous exploration. Our method relies on the definition of a disk graph of free-space bubbles, from which we derive a biased sampling function that expands the graph towards known free space for maximal navigability and frontiers discovery. The proposed method demonstrates an exploratory behavior similar to Rapidly-exploring Random Trees, while retaining the connectivity and flexibility of a graph-based planner. We demonstrate the interest of our method by first comparing its path planning capabilities against state-of-the-art approaches, before discussing exploration-specific aspects, namely replanning capabilities and incremental construction of the graph. A simple frontiers-driven exploration controller derived from our planning method is also demonstrated using the Pioneer platform.},
  archive   = {C_IROS},
  author    = {T. Noël and S. Kabbour and A. Lehuger and E. Marchand and F. Chaumette},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981136},
  pages     = {5707-5714},
  title     = {Disk-graph probabilistic roadmap: Biased distance sampling for path planning in a partially unknown environment},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Linear MPC-based motion planning for autonomous surgery.
<em>IROS</em>, 5699–5706. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Within the context of Robotic Minimally Invasive Surgery (R-MIS), we propose a novel linear model predictive controller formulation for the coordination of multiple autonomous robotic arms. The controller is synthesized by formulating a linear approximation of non-linear constraints, which allows the controller to be both computationally faster and better performing due to the increased prediction horizon allowed within the real-time control requirements for the proposed surgical application. The solution is validated under the expected constraints of a surgical scenario in which multiple laparoscopic tools must move and coordinate in a shared environment.},
  archive   = {C_IROS},
  author    = {Marco Minelli and Alessio Sozzi and Giacomo De Rossi and Federica Ferraguti and Saverio Farsoni and Francesco Setti and Riccardo Muradore and Marcello Bonfè and Cristian Secchi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982166},
  pages     = {5699-5706},
  title     = {Linear MPC-based motion planning for autonomous surgery},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonlinear model predictive control with cost function
scheduling for a wheeled mobile robot. <em>IROS</em>, 5664–5670. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Designing a cost function for nonlinear model predictive control (MPC) with a sparse/binary stage cost is challenging. This paper proposes a novel MPC approach with a scheduled quadratic stage cost function that approximates the true stage cost in order to optimally control a nonlinear system with a sparse/binary stage cost. The cost function parameter is optimally scheduled by a parameter scheduling policy obtained by solving a Markov decision process (MDP) constructed from sampled trajectories from any nonlinear MPC solver. The pro-posed approach is implemented into a differential drive wheeled mobile robot (WMR) designed for smart warehousing via the robot operating system (ROS) framework. The simulation and experimental results successfully demonstrate the effectiveness of our MPC approach in cases of the point stabilization problem of a differential drive WMR.},
  archive   = {C_IROS},
  author    = {Jaehyun Lim and Hyeonwoo Lee and Jongeun Choi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981066},
  pages     = {5664-5670},
  title     = {Nonlinear model predictive control with cost function scheduling for a wheeled mobile robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The concept of rod-driven locomotion for spherical lunar
exploration robots. <em>IROS</em>, 5656–5663. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A spherical robotic probe has several advantages in rough environments and has therefore raised interest for application in planetary exploration. A sphere is well-suited to protect high-sensitive payloads, however, the locomotion system for planetary surfaces raises several challenges. This paper presents a novel locomotion system consisting of linear actuators which are usable in a multi-functional fashion. Apart from pushing and bringing leverage for locomotion the extendable rods enable a tripod mode for improved sensing. The developed solutions offer a mathematical-physical system description, simple algorithms for the control of locomotion and balancing as well as general calculations for determining the maximum achievable performance parameters of such a robot. The first built prototype shows the basic suitability of the system and reveals directions for further research.},
  archive   = {C_IROS},
  author    = {Jasper Zevering and Dorit Borrmann and Anton Bredenbeck and Andreas Nüchter},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981887},
  pages     = {5656-5663},
  title     = {The concept of rod-driven locomotion for spherical lunar exploration robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Microspine design for additive manufacturing. <em>IROS</em>,
5640–5647. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Microspine grippers allow robots to ascend steep rocky slopes and cliff faces, enabling scientific exploration of exposed strata on Earth and other solar system bodies. Historically, the Shape Deposition Manufacturing (SDM) process has been used to fabricate multi-material suspensions for load-sharing among multiple microspines. We instead apply the Hybrid Deposition Manufacturing (HDM) process to microspine fabrication, and we further propose a novel 3D-printed microspine suspension design that can be manufactured via Fused Deposition Manufacturing (FDM) alone, using a single flexible material with an embedded fishhook. We use a model of microspine stiffness that allows designers to compensate for order-of-magnitude changes in material tensile modulus by adjusting geometric parameters of the design. The stiffness model and the FDM microspine design are validated through tensile testing, and mechanical properties of the HDM and FDM designs are compared against a standard SDM microspine design. We demonstrate that the FDM process can produce microspines with equivalent normal and axial stiffness and superior maximum load and fatigue response to SDM microspines, and discuss additional advantages of the FDM process for rapid prototyping and broader accessibility.},
  archive   = {C_IROS},
  author    = {Paul Nadan and Dinesh K. Patel and Catherine Pavlov and Spencer Backus and Aaron M. Johnson},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981634},
  pages     = {5640-5647},
  title     = {Microspine design for additive manufacturing},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SCALER: A tough versatile quadruped free-climber robot.
<em>IROS</em>, 5632–5639. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces SCALER, a quadrupedal robot that demonstrates climbing on bouldering walls, over-hangs, ceilings and trotting on the ground. SCALER is one of the first high-degrees of freedom four-limbed robots that can free-climb under the Earth&#39;s gravity and one of the most mechanically efficient quadrupeds on the ground. Where other state-of-the-art climbers specialize in climbing, SCALER promises practical free-climbing with payload and ground locomotion, which realizes true versatile mobility. A new climbing gait, SKATE gait, increases the payload by utilizing the SCALER body linkage mechanism. SCALER achieves a maximum normalized locomotion speed of 1.87 /s, or 0.56 m/s on the ground and 1.0 /min, or 0.35 m/min in bouldering wall climbing. Payload capacity reaches 233\% of the SCALER weight on the ground and 35\% on the vertical wall. Our GOAT gripper, a mechanically adaptable underactuated two-finger gripper, successfully grasps convex and non-convex objects and supports SCALER.},
  archive   = {C_IROS},
  author    = {Yusuke Tanaka and Yuki Shirai and Xuan Lin and Alexander Schperberg and Hayato Kato and Alexander Swerdlow and Naoya Kumagai and Dennis Hong},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981555},
  pages     = {5632-5639},
  title     = {SCALER: A tough versatile quadruped free-climber robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). OmniWheg: An omnidirectional wheel-leg transformable robot.
<em>IROS</em>, 5626–5631. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents the design, analysis, and performance evaluation of an omnidirectional transformable wheel-leg robot called OmniWheg. We design a novel mechanism consisting of a separable omni-wheel and 4-bar linkages, allowing the robot to transform between omni-wheeled and legged modes smoothly. In wheeled mode, the robot can move in all directions and efficiently adjust the relative position of its wheels, while it can overcome common obstacles in legged mode, such as stairs and steps. Unlike other articles studying whegs, this implementation with omnidirectional wheels allows the correction of misalignments between right and left wheels before traversing obstacles, which effectively improves the success rate and simplifies the preparation process before the wheel-leg transformation. We describe the design concept, mechanism, and the dynamic characteristic of the wheel-leg structure. We then evaluate its performance in various scenarios, including passing obstacles, climbing steps of different heights, and turning/moving omnidirectionally. Our results confirm that this mobile platform can overcome common indoor obstacles and move flexibly on the flat ground with the new transformable wheel-leg mechanism, while keeping a high degree of stability.},
  archive   = {C_IROS},
  author    = {Ruixiang Cao and Jun Gu and Chen Yu and Andre Rosendo},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982030},
  pages     = {5626-5631},
  title     = {OmniWheg: An omnidirectional wheel-leg transformable robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Magnetic field modeling of linear halbach array for
wallclimbing robot based on radial basis function neural network.
<em>IROS</em>, 5620–5625. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Aiming at the problem that it is difficult to calculate the force of permanent magnets in the magnetic field, this paper proposes a nonlinear mechanical model of linear array magnetic field based on radial basis function neural network (RBFNN). Combined with the linear Halbach array adsorption module of the wall-climbing robot, the three-dimensional geometric magnetic fields of four typical linear array permanent magnets were constructed, and the theoretical models of the interaction between the magnetic fields were given respectively. Further, the finite element simulation calculation of the magnetic force was carried out using COMSOL Multiphysics. According to the parametric scanning results of the orthogonal test, a nonlinear intelligent prediction model of the force between magnetic fields with local loss sensitivity is established by using the RBFNN numerical fitting method. The average deviation of the network test set is 1.19, and the standard deviation is 0.80. The intelligent prediction model has strong generalization performance, faster convergence speed and stronger flexibility, which provides a theoretical basis for the interaction and control of array magnetic fields.},
  archive   = {C_IROS},
  author    = {Xiaofei Liu and Zhengkun Yi and Xinyu Wu and Wanfeng Shang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981433},
  pages     = {5620-5625},
  title     = {Magnetic field modeling of linear halbach array for wallclimbing robot based on radial basis function neural network},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Model-free neural lyapunov control for safe robot
navigation. <em>IROS</em>, 5572–5579. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Model-free Deep Reinforcement Learning (DRL) controllers have demonstrated promising results on various challenging non-linear control tasks. While a model-free DRL algorithm can solve unknown dynamics and high-dimensional problems, it lacks safety assurance. Although safety constraints can be encoded as part of a reward function, there still exists a large gap between an RL controller trained with this modified reward and a safe controller. In contrast, instead of implicitly encoding safety constraints with rewards, we explicitly colearn a Twin Neural Lyapunov Function (TNLF) with the control policy in the DRL training loop and use the learned TNLF to build a runtime monitor. Combined with the path generated from a planner, the monitor chooses appropriate waypoints that guide the learned controller to provide collision-free control trajectories. Our approach inherits the scalability advantages from DRL while enhancing safety guarantees. Our experimental evaluation demonstrates the effectiveness of our approach compared to DRL with augmented rewards and constrained DRL methods over a range of high-dimensional safety-sensitive navigation tasks.},
  archive   = {C_IROS},
  author    = {Zikang Xiong and Joe Eappen and Ahmed H. Qureshi and Suresh Jagannathan},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981632},
  pages     = {5572-5579},
  title     = {Model-free neural lyapunov control for safe robot navigation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Vision-guided quadrupedal locomotion in the wild with
multi-modal delay randomization. <em>IROS</em>, 5556–5563. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Developing robust vision-guided controllers for quadrupedal robots in complex environments with various obstacles, dynamical surroundings and uneven terrains is very challenging. While Reinforcement Learning (RL) provides a promising paradigm for agile locomotion skills with vision inputs in simulation, it is still very challenging to deploy the vision-guided RL policy in the real world. Our key insight is that the asynchronous multi-modal observations, caused by different latencies in different components of the real robot, create a large sim2real gap for a RL policy. In this paper, we propose Multi-Modal Delay Randomization (MMDR) to address this issue when training in simulation. Specifically, we randomize the selections for both the proprioceptive states and the visual observations in time during training, aiming to simulate the asynchronous inputs when deploying to the real robot. With this technique, we are able to train a RL policy for end-to-end locomotion control in simulation, which can be directly deployed on the real A1 quadruped robot running in the wild. We evaluate our method in different outdoor environments with complex terrain and obstacles. We show that the robot can smoothly maneuver at a high speed while avoiding the obstacles, achieving significant improvement over the baselines. Our project page with videos is at https://mehooz.github.io/mmdr-wild/.},
  archive   = {C_IROS},
  author    = {Chieko Sarah Imai and Minghao Zhang and Yuchen Zhang and Marcin Kierebiński and Ruihan Yang and Yuzhe Qin and Xiaolong Wang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981072},
  pages     = {5556-5563},
  title     = {Vision-guided quadrupedal locomotion in the wild with multi-modal delay randomization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Maintaining robot localizability with bayesian cramér-rao
lower bounds. <em>IROS</em>, 5533–5539. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate and real-time position estimates are cru-cial for mobile robots. This work focuses on ranging-based positioning systems, which rely on distance measurements between known points, called anchors, and a tag to localize. The topology of the network formed by the anchors strongly influences the tag&#39;s localizability, i.e., its ability to be accurately localized. Here, the tag and some anchors are supposed to be carried by robots, which allows enhancing the positioning accuracy by planning the anchors&#39; motions. We leverage Bayesian Cramer-Rao Lower Bounds (CRLBs) on the estimates&#39; covariance in order to quantify the tag&#39;s localizability. This class of CRLBs can capture prior information on the tag&#39;s position and take it into account when deploying the anchors. We propose a method to decrease a potential function based on the Bayesian CRLB in order to maintain the localizability of the tag while having some prior knowledge about its position distribution. Then, we present a new experiment highlighting the link between the localizability potential and the precision expected in practice. Finally, two real-time anchor motion planners are demonstrated with ranging measurements in the presence or absence of prior information about the tag&#39;s position.},
  archive   = {C_IROS},
  author    = {Justin Cano and Corentin Chauffaut and Eric Chaumette and Gaël Pages and Jerome Le Ny},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981427},
  pages     = {5533-5539},
  title     = {Maintaining robot localizability with bayesian cramér-rao lower bounds},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Highly-efficient binary neural networks for visual place
recognition. <em>IROS</em>, 5493–5500. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {VPR is a fundamental task for autonomous navigation as it enables a robot to localize itself in the workspace when a known location is detected. Although accuracy is an essential requirement for a VPR technique, computational and energy efficiency are not less important for real-world applications. CNN-based techniques archive state-of-the-art VPR performance but are computationally intensive and energy demanding. Binary neural networks (BNN) have been recently proposed to address VPR efficiently. Although a typical BNN is an order of magnitude more efficient than a CNN, its processing time and energy usage can be further improved. In a typical BNN, the first convolution is not completely binarized for the sake of accuracy. Consequently, the first layer is the slowest network stage, requiring a large share of the entire computational effort. This paper presents a class of BNNs for VPR that combines depthwise separable factorization and binarization to replace the first convolutional layer to improve computational and energy efficiency. Our best model achieves higher VPR performance while spending considerably less time and energy to process an image than a BNN using a non-binary convolution as a first stage.},
  archive   = {C_IROS},
  author    = {Bruno Ferrarini and Michael Milford and Klaus D. McDonald-Maier and Shoaib Ehsan},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981978},
  pages     = {5493-5500},
  title     = {Highly-efficient binary neural networks for visual place recognition},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Acoustic localization and communication using a MEMS
microphone for low-cost and low-power bio-inspired underwater robots.
<em>IROS</em>, 5470–5477. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Having accurate localization capabilities is one of the fundamental requirements of autonomous robots. For underwater vehicles, the choices for effective localization are limited due to limitations of GPS use in water and poor environ-mental visibility that makes camera-based methods ineffective. Popular inertial navigation methods for underwater localization using Doppler-velocity log sensors, sonar, high-end inertial navigation systems, or acoustic positioning systems require bulky expensive hardware which are incompatible with low-cost, bio-inspired underwater robots. In this paper, we introduce an approach for underwater robot localization inspired by GPS methods known as acoustic pseudoranging. Our method allows us to potentially localize multiple bio-inspired robots equipped with commonly available micro electro-mechanical systems microphones. This is achieved through estimating the time difference of arrival of acoustic signals sent simultaneously through four speakers with a known constellation geometry. We also leverage the same acoustic framework to perform one-way communication with the robot to execute some primitive motions. To our knowledge, this is the first application of the approach for the on-board localization of small bio-inspired robots in water. Hardware schematics and the accompanying code are released to aid further development in the field 3 3 https://github.com/rpl-cmu/underwater-acoustic-pseudoranging.},
  archive   = {C_IROS},
  author    = {Akshay Hinduja and Yunsik Ohm and Jiahe Liao and Carmel Majidi and Michael Kaess},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981355},
  pages     = {5470-5477},
  title     = {Acoustic localization and communication using a MEMS microphone for low-cost and low-power bio-inspired underwater robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Energy-efficient tunable-stiffness soft robots using second
moment of area actuation. <em>IROS</em>, 5464–5469. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The optimal stiffness for soft swimming robots depends on swimming speed, which means no single stiffness can maximise efficiency in all swimming conditions. Tunable-stiffness would produce an increased range of high-efficiency swimming speeds for robots with flexible propulsors and enable soft control surfaces for steering underwater vehicles. We propose and demonstrate a method for tunable soft robotic stiffness using inflatable rubber tubes to stiffen a silicone foil through pressure and second moment of area change. We achieved double the effective stiffness of the system for an input pressure change from 0 to 0.8 bar and 2 J energy input. We achieved a resonant amplitude gain of 5 to 7 times the input amplitude and tripled the high-gain frequency range compared to a foil with fixed stiffness. These results show that changing second moment of area is an energy effective approach to tunable-stiffness robots.},
  archive   = {C_IROS},
  author    = {L. Micklem and G.D. Weymouth and B. Thornton},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981704},
  pages     = {5464-5469},
  title     = {Energy-efficient tunable-stiffness soft robots using second moment of area actuation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Topology optimized multi-material self-healing actuator with
reduced out of plane deformation. <em>IROS</em>, 5448–5455. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in soft robotics in academia have led to the adoption of soft grippers in industrial settings. Due to their soft bending actuators, these grippers can handle delicate objects with great care. However, due to their flexibility, the actuators are prone to out-of-plane deformations upon asymmetric loading. These undesired deformations lead to reduced grasp performance and may cause instability or failure of the grip. While the state-of-the-art contributions describe complex designs to limit those deformations, this work focuses on a complementary path investigating the material distribution. In this paper, a novel bending actuator is developed with improved out-of-plane deformation resistance by optimizing the material distribution in multi-material designs composed of two polymers with different mechanical properties. This is made possible by the strong interfacial strength of Diels-Alder chemical bonds in the used polymers, which have a self-healing capability. A Solid Isotropic Material with Penalization (SIMP) topology optimization is performed to increase the out-of-plane resistance. The actuator is simulated using FEA COMSOL in which the (hyper) elastic materials are simulated by Mooney-Rivlin models, fitted on experimental uniaxial tensile test data. This multi-material actuator and a reference single material actuator were manufactured and modeled. Via experimental characterization and validation in FEA simulations, it is shown that the actuator out-of-plane stiffness, characterized by the in-plane bending angle and out-of-plane bending angle, can be increased by an optimized multi-material composition, without changing the geometrical shape of the actuator.},
  archive   = {C_IROS},
  author    = {Zhanwei Wang and Seppe Terryn and Julie Legrand and Pasquale Ferrentino and Seyedreza Kashef Tabrizian and Joost Brancart and Ellen Roels and Guy Van Assche and Bram Vanderborght},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981297},
  pages     = {5448-5455},
  title     = {Topology optimized multi-material self-healing actuator with reduced out of plane deformation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Soft, multi-layer, disposable, kirigami based robotic
grippers: On handling of delicate, contaminated, and everyday objects.
<em>IROS</em>, 5440–5447. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Grasping and manipulation are complex and demanding tasks, especially when executed in dynamic and unstructured environments. Typically, such tasks are executed by rigid articulated end-effectors, with a plethora of actuators that need sophisticated sensing and complex control laws to execute them efficiently. Soft robotics offers an alternative that allows for simplified execution of these demanding tasks, enabling the creation of robust, efficient, lightweight, and affordable solutions that are easy to control and operate. In this work, we introduce a new class of soft, kirigami-based robotic grippers, we study their post-contact behavior, and we investigate different cut patterns for their development. We follow an experimental approach in which several designs are proposed and employed in a series of grasping and force exertion tests to compare their capabilities and post-contact behavior. The results of such experiments indicate a clear relationship between degree of reconfiguration and grasping force, and provide key insights into the effect of the cut patterns in the performance of the designs. These findings are then used in the design process of an improved version of multi-layer, disposable kirigami grippers that are fabricated employing simple 3D printed layers and silicone rubber using the concept of Hybrid Deposition Manufacturing (HDM). A series of experimental results demonstrate that the proposed design and manufacturing methods can enable the creation of soft, kirigami-based grippers with superior grasping capabilities that can handle delicate, contaminated, and everyday life objects and can even be disposed off in an automated way (e.g., after handling hazardous materials, such as medical waste).},
  archive   = {C_IROS},
  author    = {Joao Buzzatto and Mojtaba Shahmohammadi and Junbang Liang and Felipe Sanches and Saori Matsunaga and Rintaro Haraguchi and Toshisada Mariyama and Bruce MacDonald and Minas Liarokapis},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981625},
  pages     = {5440-5447},
  title     = {Soft, multi-layer, disposable, kirigami based robotic grippers: On handling of delicate, contaminated, and everyday objects},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A geometric design approach for continuum robots by
piecewise approximation of freeform shapes. <em>IROS</em>, 5416–5423.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As soft, continuum robots see increasing areas of application, many scenarios have arisen where it is necessary to consider the geometric shape of the robot. The current approaches to robot kinematics, such as the piecewise constant-curvature (PCC) model, are effective in representing simple overall robot geometry and estimating the end-effector state, but they are less intuitive for planning robots that involve complex geometries. In this work, we propose a solution to the geometric design problem by a two-part approach: a freeform spline defines a “shape curve” that describes the overall geometry of the robot, and then a “kinematic curve” composed of shapes that are feasible to replicate with continuum robots is fitted to the shape curve. As an implementation of this approach, we specifically explore the application of piecewise cubic Bezier curves in designing the shape curve of the robot, and pairs of arcs to construct the kinematic curves. Finally, the approach is applied to a tip-extension “vine” robot that is designed and fabricated to “grow” along a designed path and access the top surface of an obstacle.},
  archive   = {C_IROS},
  author    = {Sicheng Wang and Laura H. Blumenschein},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981430},
  pages     = {5416-5423},
  title     = {A geometric design approach for continuum robots by piecewise approximation of freeform shapes},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-propelled soft everting toroidal robot for navigation
and climbing in confined spaces. <em>IROS</em>, 5409–5415. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There are many spaces inaccessible to humans where robots could help deliver sensors and equipment. Many of these spaces contain three-dimensional passageways and uneven terrain that pose challenges for robot design and control. Everting toroidal robots, which move via simultaneous eversion and inversion of their body material, are promising for navigation in these types of spaces. We present a novel soft everting toroidal robot that propels itself using a motorized device inside an air-filled membrane. Our robot requires only a single control signal to move, can conform to its environment, and can climb vertically with a motor torque that is independent of the force used to brace the robot against its environment. We derive and validate models of the forces involved in its motion, and we demonstrate the robot&#39;s ability to navigate a maze and climb a pipe.},
  archive   = {C_IROS},
  author    = {Nelson G. Badillo Perez and Margaret M. Coad},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981175},
  pages     = {5409-5415},
  title     = {Self-propelled soft everting toroidal robot for navigation and climbing in confined spaces},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rigid skeleton enhanced dexterous soft finger possessing
proprioception. <em>IROS</em>, 5402–5408. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents a humanoid soft robotics finger design with rigid skeletons and proprioceptive sensors. This 4-DOFs dexterous finger has soft joints and rigid phalanxes, which is about the size of human hand. To enhance the overall stiffness and for human-like behavior and configuration, rigid-soft actuators which we called quasi-joints are introduced. Although their lengths are shortened in this design, the soft actuators can still bend over 90°, exhibiting joint-like flexion and abduction/adduction. Thus interphalangeal joints (IPs) and metacarpophalangeal joint (MCP) are realized. EGaIn soft sensors are embedded into the structure for bending detection. In addition, multi-step molding fabrication method is introduced for this complex multi-material structure. This rigid-soft finger is a preliminary work and modular part of a highly dexterous humanoid soft robotic hand.},
  archive   = {C_IROS},
  author    = {Ruichen Zhen and Li Jiang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981292},
  pages     = {5402-5408},
  title     = {Rigid skeleton enhanced dexterous soft finger possessing proprioception},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards high-definition maps: A framework leveraging
semantic segmentation to improve NDT map compression and descriptivity.
<em>IROS</em>, 5370–5377. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {High-Definition (HD) maps are needed for robust navigation of autonomous vehicles, limited by the on-board storage capacity. To solve this, we propose a novel framework, Environment-Aware Normal Distributions Transform (EA-NDT), that significantly improves compression of standard NDT map representation. The compressed representation of EA-NDT is based on semantic-aided clustering of point clouds resulting in more optimal cells compared to grid cells of standard NDT. To evaluate EA-NDT, we present an open-source implementation that extracts planar and cylindrical primitive features from a point cloud and further divides them into smaller cells to represent the data as an EA-NDT HD map. We collected an open suburban environment dataset and evaluated EA-NDT HD map representation against the standard NDT representation. Compared to the standard NDT, EA-NDT achieved consistently at least 1.5× higher map compression while maintaining the same descriptive capability. Moreover, we showed that EA-NDT is capable of producing maps with significantly higher descriptivity score when using the same number of cells than the standard NDT.},
  archive   = {C_IROS},
  author    = {Petri Manninen and Heikki Hyyti and Ville Kyrki and Jyri Maanpää and Josef Taher and Juha Hyyppä},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982050},
  pages     = {5370-5377},
  title     = {Towards high-definition maps: A framework leveraging semantic segmentation to improve NDT map compression and descriptivity},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scalable fiducial tag localization on a 3D prior map via
graph-theoretic global tag-map registration. <em>IROS</em>, 5347–5353.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an accurate and scalable method for fiducial tag localization on a 3D prior environmental map. The proposed method comprises three steps: 1) visual odometry-based landmark SLAM for estimating the relative poses between fiducial tags, 2) geometrical matching-based global tag-map registration via maximum clique finding, and 3) tag pose refinement based on direct camera-map alignment with normalized information distance. Through simulation-based evaluations, the proposed method achieved a 98\% global tag-map registration success rate and an average tag pose estimation accuracy of a few centimeters. Experimental results in a real environment demonstrated that it enables to localize over 110 fiducial tags placed in an environment in 25 minutes for data recording and post-processing.},
  archive   = {C_IROS},
  author    = {Kenji Koide and Shuji Oishi and Masashi Yokozuka and Atsuhiko Banno},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981079},
  pages     = {5347-5353},
  title     = {Scalable fiducial tag localization on a 3D prior map via graph-theoretic global tag-map registration},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 3D lidar reconstruction with probabilistic depth completion
for robotic navigation. <em>IROS</em>, 5339–5346. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safe motion planning in robotics requires planning into space which has been verified to be free of obstacles. However, obtaining such environment representations using lidars is challenging by virtue of the sparsity of their depth measurements. We present a learning-aided 3D lidar reconstruction framework that upsamples sparse lidar depth measurements with the aid of overlapping camera images so as to generate denser reconstructions with more definitively free space than can be achieved with the raw lidar measurements alone. We use a neural network with an encoder-decoder structure to predict dense depth images along with depth uncertainty estimates which are fused using a volumetric mapping system. We conduct experiments on real-world outdoor datasets captured using a handheld sensing device and a legged robot. Using input data from a 16-beam lidar mapping a building network, our experiments showed that the amount of estimated free space was increased by more than 40\% with our approach. We also show that our approach trained on a synthetic dataset generalises well to real-world outdoor scenes without additional fine-tuning. Finally, we demonstrate how motion planning tasks can benefit from these denser reconstructions.},
  archive   = {C_IROS},
  author    = {Yifu Tao and Marija Popović and Yiduo Wang and Sundara Tejaswi Digumarti and Nived Chebrolu and Maurice Fallon},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981531},
  pages     = {5339-5346},
  title     = {3D lidar reconstruction with probabilistic depth completion for robotic navigation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Voxfield: Non-projective signed distance fields for online
planning and 3D reconstruction. <em>IROS</em>, 5331–5338. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Creating accurate maps of complex, unknown environments is of utmost importance for truly autonomous navigation robot. However, building these maps online is far from trivial, especially when dealing with large amounts of raw sensor readings on a computation and energy constrained mobile system, such as a small drone. While numerous approaches tackling this problem have emerged in recent years, the mapping accuracy is often sacrificed as systematic approximation errors are tolerated for efficiency&#39;s sake. Motivated by these challenges, we propose Voxfield, a mapping framework that can generate maps online with higher accuracy and lower computational burden than the state of the art. Built upon the novel formulation of non-projective truncated signed distance fields (TSDFs), our approach produces more accurate and complete maps, suitable for surface reconstruction. Additionally, it enables efficient generation of Euclidean signed distance fields (ESDFs), useful e.g., for path planning, that does not suffer from typical approximation errors. Through a series of experiments with public datasets, both real-world and synthetic, we demonstrate that our method beats the state of the art in map coverage, accuracy and computational time. Moreover, we show that Voxfield can be utilized as a back-end in recent multi-resolution mapping frameworks, producing high quality maps even in large-scale experiments. Finally, we validate our method by running it onboard a quadrotor, showing it can generate accurate ESDF maps usable for real-time path planning and obstacle avoidance.},
  archive   = {C_IROS},
  author    = {Yue Pan and Yves Kompis and Luca Bartolomei and Ruben Mascaro and Cyrill Stachniss and Margarita Chli},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981318},
  pages     = {5331-5338},
  title     = {Voxfield: Non-projective signed distance fields for online planning and 3D reconstruction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual servoing with geometrically interpretable neural
perception. <em>IROS</em>, 5300–5306. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An increasing number of nonspecialist robotic users demand easy-to-use machines. In the context of visual servoing, the removal of explicit image processing is becoming a trend, allowing an easy application of this technique. This work presents a deep learning approach for solving the perception problem within the visual servoing scheme. An artificial neural network is trained using the supervision coming from the knowledge of the controller and the visual features motion model. In this way, it is possible to give a geometrical interpretation to the estimated visual features, which can be used in the analytical law of the visual servoing. The approach keeps perception and control decoupled, conferring flexibility and interpretability on the whole framework. Simulated and real experiments with a robotic manipulator validate our approach.},
  archive   = {C_IROS},
  author    = {Antonio Paolillo and Mirko Nava and Dario Piga and Alessandro Giusti},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982163},
  pages     = {5300-5306},
  title     = {Visual servoing with geometrically interpretable neural perception},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Monocular depth estimation for equirectangular videos.
<em>IROS</em>, 5293–5299. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Depth estimation from panoramic imagery has received minimal attention in contrast to standard perspective imagery, which constitutes the majority of the literature on the key research topic. The vast - and frequently complete - field of view provided by such panoramic photographs makes them appealing for a variety of applications, including robots, autonomous vehicles, and virtual reality. Consumer-level camera systems capable of capturing such images are likewise growing more affordable, and may be desirable complements to autonomous systems&#39; sensor packages. They do, however, introduce significant distortions and violate some assumptions regarding perspective view images. Additionally, many state-of-the-art algorithms are not designed for its projection model, and their depth estimation performance tends to degrade when being applied to panoramic imagery. This paper presents a novel technique for adapting view synthesis-based depth estimation models to omnidirectional vision. Specifically, we: 1) integrate a “virtual” spherical camera model into the training pipeline, facilitating the model training, 2) exploit spherical convolutional layers to perform convolution operations on equirectangular images, handling the severe distortion, and 3) propose an optical flow-based masking scheme to mitigate the effect of unwanted pixels during training. Our qualitative and quantitative results demonstrate that these simple yet efficient designs result in significantly improved depth estimations when compared to previous approaches.},
  archive   = {C_IROS},
  author    = {Helmi Fraser and Sen Wang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982157},
  pages     = {5293-5299},
  title     = {Monocular depth estimation for equirectangular videos},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Markerless suture needle 6D pose tracking with robust
uncertainty estimation for autonomous minimally invasive robotic
surgery. <em>IROS</em>, 5286–5292. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Suture needle localization is necessary for autonomous suturing. Previous approaches in autonomous suturing often relied on fiducial markers rather than markerless detection schemes for localizing a suture needle due to the in-consistency of markerless detections. However, fiducial markers are not practical for real-world applications and can often be occluded from environmental factors in surgery (e.g., blood). Therefore in this work, we present a robust tracking approach for estimating the 6D pose of a suture needle when using inconsistent detections. We define observation models based on suture needles&#39; geometry that captures the uncertainty of the detections and fuse them temporally in a probabilistic fashion. In our experiments, we compare different permutations of the observation models in the suture needle localization task to show their effectiveness. Our proposed method outperforms previous approaches in localizing a suture needle. We also demonstrate the proposed tracking method in an autonomous suture needle regrasping task and ex vivo environments**The code is available at https://github.com/ucsdarclab/suture-needle-tracking..},
  archive   = {C_IROS},
  author    = {Zih-Yun Chiu and Albert Z Liao and Florian Richter and Bjorn Johnson and Michael C. Yip},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981966},
  pages     = {5286-5292},
  title     = {Markerless suture needle 6D pose tracking with robust uncertainty estimation for autonomous minimally invasive robotic surgery},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tracking monocular camera pose and deformation for SLAM
inside the human body. <em>IROS</em>, 5278–5285. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Monocular SLAM in deformable scenes will open the way to multiple medical applications like computer-assisted navigation in endoscopy, automatic drug delivery or autonomous robotic surgery. In this paper we propose a novel method to simultaneously track the camera pose and the 3D scene deformation, without any assumption about environment topology or shape. The method uses an illumination-invariant photometric method to track image features and estimates camera motion and deformation combining reprojection error with spatial and temporal regularization of deformations. Our results in simulated colonoscopies show the method&#39;s accuracy and robustness in complex scenes under increasing levels of deformation. Our qualitative results in human colonoscopies from Endomapper dataset show that the method is able to successfully cope with the challenges of real endoscopies: deformations, low texture and strong illumination changes. We also compare with previous tracking methods in simpler scenarios from Hamlyn dataset where we obtain competitive performance, without needing any topological assumption.},
  archive   = {C_IROS},
  author    = {Juan J. Gómez Rodríguez and J.M.M. Montiel and Juan D. Tardós},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981203},
  pages     = {5278-5285},
  title     = {Tracking monocular camera pose and deformation for SLAM inside the human body},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Ego+x: An egocentric vision system for global 3D human pose
estimation and social interaction characterization. <em>IROS</em>,
5271–5277. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Egocentric vision is an emerging topic, which has demonstrated great potential in assistive healthcare scenarios, ranging from human-centric behavior analysis to personal social assistance. Within this field, due to the heterogeneity of visual perception from first-person views, egocentric pose estimation is one of the most significant prerequisites for enabling various downstream applications. However, existing methods for egocentric pose estimation mainly focus on predicting the pose represented in the camera coordinates from a single image, which ignores the latent cues in the temporal domain and results in less accuracy. In this paper, we propose Ego+X, an egocentric vision based system for 3D canonical pose estimation and human-centric social interaction characterization. Our system is composed of two head-mounted egocentric cameras, where one is faced downwards and the other looks outwards. By leveraging the global context provided by visual SLAM, we first propose Ego-Glo for spatial-accurate and temporal-consistent egocentric 3D pose estimation in the canonical coordinate system. With the help of an egocentric camera looking outwards, we then propose Ego-Soc by extending Ego-Glo to various social interaction tasks, e.g., object detection and human-human interaction. Quantitative and qualitative experiments have been conducted to demonstrate the effectiveness of our proposed Ego+X.},
  archive   = {C_IROS},
  author    = {Yuxuan Liu and Jianxin Yang and Xiao Gu and Yao Guo and Guang-Zhong Yang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981710},
  pages     = {5271-5277},
  title     = {Ego+X: An egocentric vision system for global 3D human pose estimation and social interaction characterization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LapSeg3D: Weakly supervised semantic segmentation of point
clouds representing laparoscopic scenes. <em>IROS</em>, 5265–5270. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The semantic segmentation of surgical scenes is a prerequisite for task automation in robot assisted interventions. We propose LapSeg3D, a novel DNN-based approach for the voxel-wise annotation of point clouds representing surgical scenes. As the manual annotation of training data is highly time consuming, we introduce a semi-autonomous clustering-based pipeline for the annotation of the gallbladder, which is used to generate segmented labels for the DNN. When evaluated against manually annotated data, LapSeg3D achieves an F1 score of 0.94 for gallbladder segmentation on various datasets of ex-vivo porcine livers. We show LapSeg3D to generalize accurately across different gallbladders and datasets recorded with different RGB-D camera systems.},
  archive   = {C_IROS},
  author    = {Benjamin Alt and Christian Kunz and Darko Katic and Rayan Younis and Rainer Jäkel and Beat Peter Müller-Stich and Martin Wagner and Franziska Mathis-Ullrich},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981178},
  pages     = {5265-5270},
  title     = {LapSeg3D: Weakly supervised semantic segmentation of point clouds representing laparoscopic scenes},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Koopman pose predictions for temporally consistent human
walking estimations. <em>IROS</em>, 5257–5264. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We tackle the problem of tracking the human lower body as an initial step toward an automatic motion assessment system for clinical mobility evaluation, using a multimodal system that combines Inertial Measurement Unit (IMU) data, RGB images, and point cloud depth measurements. This system applies the factor graph representation to an optimization problem that provides 3-D skeleton joint estimations. In this paper, we focus on improving the temporal consistency of the estimated human trajectories to greatly extend the range of operability of the depth sensor. More specifically, we introduce a new factor graph factor based on Koopman theory that embeds the nonlinear dynamics of several lower-limb movement activities. This factor performs a two-step process: first, a custom activity recognition module based on spatial temporal graph convolutional networks recognizes the walking activity; then, a Koopman pose prediction of the subsequent skeleton is used as an a priori estimation to drive the optimization problem toward more consistent results. We tested the performance of this module a dataset composed of multiple clinical lower-limb mobility tests, and we show that our approach reduces outliers on the skeleton form by almost 1 m, while preserving natural walking trajectories at depths up to more than 10 m.},
  archive   = {C_IROS},
  author    = {Marc Mitjans and David M. Levine and Louis N. Awad and Roberto Tron},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981204},
  pages     = {5257-5264},
  title     = {Koopman pose predictions for temporally consistent human walking estimations},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Construction of a simulator to reproduce changes in running
due to motion strategies using spring-loaded inverted pendulum model.
<em>IROS</em>, 5249–5256. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study aims to construct a running simulator based on a motion generation and control system that enables the description of motion strategies using the spring-loaded inverted pendulum (SLIP) model. The problems of stability and robustness encountered in the running simulation with the SLIP model are elucidated, and stable running is achieved by controlling the stiffness and the attitude angle dynamically at touchdown, as well as human energy adjustment that is introduced to consider the active motion strategy. As a result, passive and active control by humans can be expressed, and a framework that can express the changes in running due to motion strategies is constructed. Finally, we discuss the possibility of describing and elucidating the motion strategies.},
  archive   = {C_IROS},
  author    = {Masaki Kitagawa and Takayuki Tanaka and Akihiko Murai},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981926},
  pages     = {5249-5256},
  title     = {Construction of a simulator to reproduce changes in running due to motion strategies using spring-loaded inverted pendulum model},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A centaur system for assisting human walking with load
carriage. <em>IROS</em>, 5242–5248. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Walking with load is a common task in daily life and disaster rescue. Long-term load carriage may cause irreversible damage to the human body. Although remarkable progress has been made in the field of wearable robots, it is still far from avoiding interference to human legs, which will lead to energy consumption. In this paper, a novel wearable robot, Centaur, for assisting load carriage has been proposed. The Centaur system consists of two rigid robotic legs of two degrees-of-freedom (DOFs) to transfer load weight to the ground. Different from exoskeletons, the robotic legs of the Centaur are placed behind the human rather than attached to human limbs, which can provide a larger support polygon and avoid additional interference to the wearer. Additionally, the Centaur can attain the locomotion stability of the quadruped while maintaining the motion agility of the biped itself. This paper also presents an interactive motion control strategy based on the human-robot interaction force. This control strategy incorporates legged robotics walking controller and real-time walking trajectory planning to realize the cooperative walking with human beings. Finally, experiments of human walking with load carriage have been conducted on flat terrain to verify the concept of the Centaur system. The result demonstrates that the Centaur system can effectively reduce 70.03\% of load weight during the single stance phase, which indicates that the Centaur system provides a new solution for assisting human walking with load-carriage.},
  archive   = {C_IROS},
  author    = {Ping Yang and Haoyun Yan and Bowen Yang and Jianquan Li and Kailin Li and Yuquan Leng and Chenglong Fu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981394},
  pages     = {5242-5248},
  title     = {A centaur system for assisting human walking with load carriage},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Human-to-robot manipulability domain adaptation with
parallel transport and manifold-aware ICP. <em>IROS</em>, 5218–5225. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Manipulability ellipsoids efficiently capture the human pose and reveal information about the task at hand. Their use in task-dependent robot teaching - particularly their transfer from a teacher to a learner - can advance emulation of human-like motion. Although in recent literature focus is shifted towards manipulability transfer between two robots, the adaptation to the capabilities of the other kinematic system is to date not addressed and research in transfer from human to robot is still in its infancy. This work presents a novel manipulability domain adaptation method for the transfer of manipulability information to the domain of another kinematic system. As manipulability matrices/ellipsoids are symmetric positive-definite (SPD) they can be viewed as points on the Riemannian manifold of SPD matrices. We are the first to address the problem of manipulability transfer from the perspective of point cloud registration. We propose a manifold-aware Iterative Closest Point algorithm (ICP) with parallel transport initialization. Furthermore, we introduce a correspondence matching heuristic for manipulability ellipsoids based on inherent geometric features. We confirm our method in simulation experiments with 2-DoF manipulators as well as 7-DoF models representing the human-arm kinematics.},
  archive   = {C_IROS},
  author    = {Anna Reithmeir and Luis Figueredo and Sami Haddadin},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981796},
  pages     = {5218-5225},
  title     = {Human-to-robot manipulability domain adaptation with parallel transport and manifold-aware ICP},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A riemannian take on human motion analysis and retargeting.
<em>IROS</em>, 5210–5217. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dynamic motions of humans and robots are widely driven by posture-dependent nonlinear interactions between their degrees of freedom. However, these dynamical effects remain mostly overlooked when studying the mechanisms of human movement generation. Inspired by recent works, we hypothesize that human motions are planned as sequences of geodesic synergies, and thus correspond to coordinated joint movements achieved with piecewise minimum energy. The underlying computational model is built on Riemannian geometry to account for the inertial characteristics of the body. Through the analysis of various human arm motions, we find that our model segments motions into geodesic synergies, and successfully predicts observed arm postures, hand trajectories, as well as their respective velocity profiles. Moreover, we show that our analysis can further be exploited to transfer arm motions to robots by reproducing individual human synergies as geodesic paths in the robot configuration space.},
  archive   = {C_IROS},
  author    = {Holger Klein and Noémie Jaquier and Andre Meixner and Tamim Asfour},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982127},
  pages     = {5210-5217},
  title     = {A riemannian take on human motion analysis and retargeting},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gastrocnemius and power amplifier soleus spring-tendons
achieve fast human-like walking in a bipedal robot. <em>IROS</em>,
5202–5209. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Legged locomotion in humans is governed by natural dynamics of the human body and neural control. One mechanism that is assumed to contribute to the high efficiency of human walking is the impulsive ankle push-off, which potentially powers the swing leg catapult. However, the mechanics of the human lower leg with its complex muscle-tendon units spanning over single and multiple joints is not yet understood. Legged robots allow testing the interaction between complex leg mechanics, control, and environment in real-world walking gait. We developed a 0.49 m tall, 2.2 kg anthropomorphic bipedal robot with Soleus and Gastrocnemius muscle-tendon units represented by linear springs, acting as mono- and biarticular elastic structures around the robot&#39;s ankle and knee joints. We tested the influence of three Soleus and Gastrocnemius spring-tendon configurations on the ankle power curves, the coordination of the ankle and knee joint movements, the total cost of transport, and walking speed. We controlled the robot with a feed-forward central pattern generator, leading to walking speeds between 0.35 m/s and 0.57 m/s at 1.0 Hz locomotion frequency, at 0.35 m leg length. We found differences between all three configurations; the Soleus spring-tendon modulates the robot&#39;s speed and energy efficiency likely by ankle power amplification, while the Gastrocnemius spring-tendon changes the movement coordination between ankle and knee joints during push-off.},
  archive   = {C_IROS},
  author    = {Bernadett Kiss and Emre Cemal Gonen and An Mo and Alexander Badri–Spröwitz and Alexandra Buchmann and Daniel Renjewski},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981725},
  pages     = {5202-5209},
  title     = {Gastrocnemius and power amplifier soleus spring-tendons achieve fast human-like walking in a bipedal robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Understanding spatio-temporal relations in human-object
interaction using pyramid graph convolutional network. <em>IROS</em>,
5195–5201. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human activities recognition is an important task for an intelligent robot, especially in the field of human-robot collaboration, it requires not only the label of sub-activities but also the temporal structure of the activity. In order to automatically recognize both the label and the temporal structure in sequence of human-object interaction, we propose a novel Pyramid Graph Convolutional Network (PGCN), which employs a pyramidal encoder-decoder architecture consisting of an attention based graph convolution network and a temporal pyramid pooling module for downsampling and upsampling interaction sequence on the temporal axis, respectively. The system represents the 2D or 3D spatial relation of human and objects from the detection results in video data as a graph. To learn the human-object relations, a new attention graph convolutional network is trained to extract condensed information from the graph representation. To segment action into sub-actions, a novel temporal pyramid pooling module is proposed, which upsamples compressed features back to the original time scale and classifies actions per frame. We explore various attention layers, namely spatial attention, temporal attention and channel attention, and combine different upsampling decoders to test the performance on action recognition and segmentation. We evaluate our model on two challenging datasets in the field of human-object interaction recognition, i.e. Bimanual Actions and IKEA Assembly datasets. We demonstrate that our classifier significantly improves both framewise action recognition and segmentation, e.g., F1 micro and F1@50 scores on Bimanual Actions dataset are improved by 4.3\% and 8.5\% respectively.},
  archive   = {C_IROS},
  author    = {Hao Xing and Darius Burschka},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981771},
  pages     = {5195-5201},
  title     = {Understanding spatio-temporal relations in human-object interaction using pyramid graph convolutional network},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On safety testing, validation, and characterization with
scenario-sampling: A case study of legged robots. <em>IROS</em>,
5179–5186. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The dynamic response of the legged robot locomotion is non-Lipschitz and can be stochastic due to environmental uncertainties. To test, validate, and characterize the safety performance of legged robots, existing solutions on observed and inferred risk can be incomplete and sampling inefficient. Some formal verification methods suffer from the model precision and other surrogate assumptions. In this paper, we propose a scenario sampling based testing framework that characterizes the overall safety performance of a legged robot by specifying (i) where (in terms of a set of states) the robot is potentially safe, and (ii) how safe the robot is within the specified set. The framework can also help certify the commercial deployment of the legged robot in real-world environment along with human and compare safety performance among legged robots with different mechanical structures and dynamic properties. The proposed framework is further deployed to evaluate a group of state-of-the-art legged robot locomotion controllers from various model-based, deep neural network involved, and reinforcement learning based methods in the literature. Among a series of intended work domains of the studied legged robots (e.g. tracking speed on sloped surface, with abrupt changes on demanded velocity, and against adversarial push-over disturbances), we show that the method can adequately capture the overall safety characterization and the subtle performance insights. Many of the observed safety outcomes, to the best of our knowledge, have never been reported by the existing work in the legged robot literature.},
  archive   = {C_IROS},
  author    = {Bowen Weng and Guillermo A. Castillo and Wei Zhang and Ayonga Hereid},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981359},
  pages     = {5179-5186},
  title     = {On safety testing, validation, and characterization with scenario-sampling: A case study of legged robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dependability analysis of deep reinforcement learning based
robotics and autonomous systems through probabilistic model checking.
<em>IROS</em>, 5171–5178. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While Deep Reinforcement Learning (DRL) provides transformational capabilities to the control of Robotics and Autonomous Systems (RAS), the black-box nature of DRL and uncertain deployment environments of RAS pose new challenges on its dependability. Although existing works impose constraints on the DRL policy to ensure successful completion of the mission, it is far from adequate to assess the DRL-driven RAS in a holistic way considering all dependability properties. In this paper, we formally define a set of dependability properties in temporal logic and construct a Discrete-Time Markov Chain (DTMC) to model the dynamics of risk/failures of a DRL-driven RAS interacting with the stochastic environment. We then conduct Probabilistic Model Checking (PMC) on the designed DTMC to verify those properties. Our experimental results show that the proposed method is effective as a holistic assessment framework while uncovering conflicts between the properties that may need trade-offs in training. Moreover, we find that the standard DRL training cannot improve dependability properties, thus requiring bespoke optimisation objectives. Finally, our method offers sensitivity analysis of dependability properties to disturbance levels from environments, providing insights for the assurance of real RAS.},
  archive   = {C_IROS},
  author    = {Yi Dong and Xingyu Zhao and Xiaowei Huang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981794},
  pages     = {5171-5178},
  title     = {Dependability analysis of deep reinforcement learning based robotics and autonomous systems through probabilistic model checking},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enpheeph: A fault injection framework for spiking and
compressed deep neural networks. <em>IROS</em>, 5155–5162. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Research on Deep Neural Networks (DNNs) has focused on improving performance and accuracy for real-world deployments, leading to new models, such as Spiking Neural Networks (SNNs), and optimization techniques, e.g., quantization and pruning for compressed networks. However, the deployment of these innovative models and optimization techniques introduces possible reliability issues, which is a pillar for DNNs to be widely used in safety-critical applications, e.g., autonomous driving. Moreover, scaling technology nodes have the associated risk of multiple faults happening at the same time, a possibility not addressed in state-of-the-art resiliency analyses. Towards better reliability analysis for DNNs, we present enpheeph, a Fault Injection Framework for Spiking and Compressed DNNs. The enpheeph framework enables optimized execution on specialized hardware devices, e.g., GPUs, while providing complete customizability to investigate different fault models, emulating various reliability constraints and use-cases. Hence, the faults can be executed on SNNs as well as compressed networks with minimal-to-none modifications to the underlying code, a feat that is not achievable by other state-of-the-art tools. To evaluate our enpheeph framework, we analyze the resiliency of different DNN and SNN models, with different compression techniques. By injecting a random and increasing number of faults, we show that DNNs can show a reduction in accuracy with a fault rate as low as $7\times 10^{-7}$ faults per parameter, with an accuracy drop higher than 40\%. Run-time overhead when executing enpheeph is less than 20\% of the baseline execution time when executing 100 000 faults concurrently, at least 10× lower than state-of-the-art frameworks, making enpheeph future-proof for complex fault injection scenarios. We release the source code of our enpheeph framework under an open-source license at https://github.com/Alexei95/enpheeph.},
  archive   = {C_IROS},
  author    = {Alessio Colucci and Andreas Steininger and Muhammad Shafique},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982181},
  pages     = {5155-5162},
  title     = {Enpheeph: A fault injection framework for spiking and compressed deep neural networks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HiddenGems: Efficient safety boundary detection with active
learning. <em>IROS</em>, 5147–5154. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Evaluating safety performance in a resource-efficient way is crucial for the development of autonomous systems. Simulation of parameterized scenarios is a popular testing strategy but parameter sweeps can be prohibitively expensive. To address this, we propose HiddenGems: a sample-efficient method for discovering the boundary between compliant and non-compliant behavior via active learning. Given a parameterized scenario, one or more compliance metrics, and a simulation oracle, HiddenGems maps the compliant and noncompliant domains of the scenario. The methodology enables critical test case identification, comparative analysis of different versions of the system under test, as well as verification of design objectives. We evaluate HiddenGems on a scenario with a jaywalker crossing in front of an autonomous vehicle and obtain compliance boundary estimates for collision, lane keep, and acceleration metrics individually and in combination, with 6 times fewer simulations than a parameter sweep. We also show how HiddenGems can be used to detect and rectify a failure mode for an unprotected turn with 86\% fewer simulations.},
  archive   = {C_IROS},
  author    = {Aleksandar Petrov and Carter Fang and Khang Minh Pham and You Hong Eng and James Guo Ming Fu and Scott Drew Pendleton},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982243},
  pages     = {5147-5154},
  title     = {HiddenGems: Efficient safety boundary detection with active learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). How do we fail? Stress testing perception in autonomous
vehicles. <em>IROS</em>, 5139–5146. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous vehicles (AVs) rely on environment perception and behavior prediction to reason about agents in their surroundings. These perception systems must be robust to adverse weather such as rain, fog, and snow. However, validation of these systems is challenging due to their complexity and dependence on observation histories. This paper presents a method for characterizing failures of LiDAR-based perception systems for AVs in adverse weather conditions. We develop a methodology based in reinforcement learning to find likely failures in object tracking and trajectory prediction due to sequences of disturbances. We apply disturbances using a physics-based data augmentation technique for simulating LiDAR point clouds in adverse weather conditions. Experiments performed across a wide range of driving scenarios from a real-world driving dataset show that our proposed approach finds high likelihood failures with smaller input disturbances compared to baselines while remaining computationally tractable. Identified failures can inform future development of robust perception systems for AVs.},
  archive   = {C_IROS},
  author    = {Harrison Delecki and Masha Itkina and Bernard Lange and Ransalu Senanayake and Mykel J. Kochenderfer},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981724},
  pages     = {5139-5146},
  title     = {How do we fail? stress testing perception in autonomous vehicles},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adhesion risk assessment of an aircraft inspection robot for
improving operator awareness. <em>IROS</em>, 5131–5138. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vacuum-adhesion-based climbing robots have been developed to cater to the demands in the cleaning and inspection work of airplanes. A robot intended to clean and inspect an airplane faces a Risk of Adhesion (RoA) based on the robot and the surface conditions, such as worn-out suction cups. These sorts of underlying conditions are not easily noticeable for an operator of a robot and might lead to catastrophic events. Therefore, the ability of a robot to self-assess the RoA in a scenario and notify the operator is crucial for ensuring safety. Particularly, an aircraft inspection robot should have adhesion awareness. This paper proposes a novel method to self-assess the RoA of a vacuum-adhesion-based robot intended to clean and inspect airplanes. The RoA is assessed by a fuzzy inference system that analyzes the present pressure difference and the current duty setting of the vacuum pump of a robot. The robot operator can collaborate with the robot to take precautions based on the assessed RoA to ensure safety. The outcomes of the experiment conducted on an airplane skin validate the ability of the proposed method to assess the RoA associated with heterogeneous operating conditions effectively. Thus, the utilization of the proposed method would improve the safety of a vacuum-adhesion-based robot intended to clean and inspect airplanes.},
  archive   = {C_IROS},
  author    = {M. A. Viraj and J. Muthugala and Manuel Vega-Heredia and Nay Htet Lin and S. M. Bhagya and P. Samarakoon and Mohan Rajesh Elara},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982230},
  pages     = {5131-5138},
  title     = {Adhesion risk assessment of an aircraft inspection robot for improving operator awareness},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An analytical study of motion of autonomous vehicles under
imperfect sensing. <em>IROS</em>, 5123–5130. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A fully tested autonomous system works predictably under ideal or assumed environment. However, its behavior is not fully defined when some components malfunction or fail. In this paper, we consider automated guided vehicle (AGV), equipped with multiple sensors, executing a traversal task in a static unknown environment. We have analytically studied the system, computed a set of performance and safety metrics, and validated it with simulation results in Webots. We have also analyzed the effect on system performance under independent and correlated sensing errors. We have also performed sensitivity analysis to identify the most critical components in any given system; and this can be utilized to increase the reliability of the system and its conformance to safety objectives.},
  archive   = {C_IROS},
  author    = {Swagata Biswas and Himadri Sekhar Paul and Saurabh Bagchi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981329},
  pages     = {5123-5130},
  title     = {An analytical study of motion of autonomous vehicles under imperfect sensing},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Qualitative belief space planning via compositions.
<em>IROS</em>, 5099–5106. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Planning under uncertainty is a fundamental problem in robotics. Classical approaches rely on a metrical representation of the world and robot&#39;s states to infer the next course of action. While these approaches are considered accurate, they are often susceptible to metric errors and tend to be costly regarding memory and time consumption. However, in some cases, relying on qualitative geometric information alone is sufficient. Hence, the issues described above become an unnecessary burden. This work presents a novel qualitative Belief Space Planning (BSP) approach, highly suitable for platforms with low-cost sensors and particularly appealing in sparse environment scenarios. Our algorithm generalizes its predecessors by avoiding any deterministic assumptions. Moreover, it smoothly incorporates spatial information propagation techniques, known as compositions. We demonstrate our algorithm in simulations and the advantage of using compositions in particular.},
  archive   = {C_IROS},
  author    = {Itai Zilberman and Vadim Indelman},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981502},
  pages     = {5099-5106},
  title     = {Qualitative belief space planning via compositions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accelerated reinforcement learning for temporal logic
control objectives. <em>IROS</em>, 5077–5082. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the problem of learning control policies for mobile robots, modeled as unknown Markov Decision Processes (MDPs), that are tasked with temporal logic missions, such as sequencing, coverage, or surveillance. The MDP captures uncertainty in the workspace structure and the outcomes of control decisions. The control objective is to synthesize a control policy that maximizes the probability of accomplishing a high-level task, specified as a Linear Temporal Logic (LTL) formula. To address this problem, we propose a novel accelerated model-based reinforcement learning (RL) algorithm for LTL control objectives that is capable of learning control policies significantly faster than related approaches. Its sample-efficiency relies on biasing exploration towards directions that may contribute to task satisfaction. This is accomplished by leveraging an automaton representation of the LTL task as well as a continuously learned MDP model. Finally, we provide comparative experiments that demonstrate the sample efficiency of the proposed method against recent RL methods for LTL objectives.},
  archive   = {C_IROS},
  author    = {Yiannis Kantaros},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981759},
  pages     = {5077-5082},
  title     = {Accelerated reinforcement learning for temporal logic control objectives},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RAMIEL: A parallel-wire driven monopedal robot for high and
continuous jumping. <em>IROS</em>, 5017–5024. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Legged robots with high locomotive performance have been extensively studied, and various leg structures have been proposed. Especially, a leg structure that can achieve both continuous and high jumps is advantageous for moving around in a three-dimensional environment. In this study, we propose a parallel wire-driven leg structure, which has one DoF of linear motion and two DoFs of rotation and is controlled by six wires, as a structure that can achieve both continuous jumping and high jumping. The proposed structure can simultaneously achieve high controllability on each DoF, long acceleration distance and high power required for jumping. In order to verify the jumping performance of the parallel wire-driven leg structure, we have developed a parallel wire-driven monopedal robot, RAMIEL. RAMIEL is equipped with quasi-direct drive, high power wire winding mechanisms and a lightweight leg, and can achieve a maximum jumping height of 1.6 m and a maximum of seven continuous jumps.},
  archive   = {C_IROS},
  author    = {Temma Suzuki and Yasunori Toshimitsu and Yuya Nagamatsu and Kento Kawaharazuka and Akihiro Miki and Yoshimoto Ribayashi and Masahiro Bando and Kunio Kojima and Yohei Kakiuchi and Kei Okada and Masayuki Inaba},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981963},
  pages     = {5017-5024},
  title     = {RAMIEL: A parallel-wire driven monopedal robot for high and continuous jumping},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data-driven kinematic control scheme for cable-driven
parallel robots allowing collisions. <em>IROS</em>, 5003–5008. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cable-Driven Parallel Robots (CDPRs) have been proposed for a variety of applications such as material handling, rehabilitation, and instrumentation. However, the collision-free constraint of CDPRs limits the workspace of CDPRs and the feasible position of anchor points. To address the collision-free constraint of CDPRs, a data-driven kinematic control scheme is developed for CDPRs, enabling a CDPR to control its pose even if suffering collisions between a cable and the base or the end-effector. To deal with the collisions, the data-driven kinematic control scheme utilizes a motion model obtained based on data samples of the motion of the CDPR, rather than the Jacobian matrix of the CDPR, to map a control law in the task space to the time derivative of the length of cables in the joint space. To evaluate the effectiveness of the developed data-driven kinematic control scheme, experiments of controlling a suspended CDPR with two cables allowing collisions are conducted.},
  archive   = {C_IROS},
  author    = {Yongwei Zou and Yusheng Hu and Huanhui Cao and Yuchen Xu and Yuanjie Yu and Wenjie Lu and Hao Xiong},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981997},
  pages     = {5003-5008},
  title     = {Data-driven kinematic control scheme for cable-driven parallel robots allowing collisions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). End-point stiffness and joint viscosity control of
musculoskeletal robotic arm using muscle redundancy. <em>IROS</em>,
4997–5002. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study focuses on replicating the muscu-loskeletal system of human arms for mimicking its movement. Muscle redundancy is critical for regulating the mechanical impedance of arms and legs. However, when implementing muscle redundancy on robots, making an ill-posed problem that cannot determine the muscle forces uniquely. In this paper, first, a method for controlling end-point stiffness in the muscle space for the joint and muscle redundant system is described. Next, the muscle model imitating the nonlinear viscosity characteristic of human muscles is introduced. Then, a method to control the joint viscosity by adjusting the internal forces of muscles adequately without affecting the stiffness control directly is proposed. Finally, numerical simulations are performed to investigate the effectiveness of the proposed method.},
  archive   = {C_IROS},
  author    = {Shoki Tsuboi and Hitoshi Kino and Kenji Tahara},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981712},
  pages     = {4997-5002},
  title     = {End-point stiffness and joint viscosity control of musculoskeletal robotic arm using muscle redundancy},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Kinematics-inertial fusion for localization of a 4-cable
underactuated suspended robot considering cable sag. <em>IROS</em>,
4989–4996. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Suspended Cable-Driven Parallel Robots (SCDPR) have intriguing capabilities on large scales but still have open challenges in precisely estimating the end-effector pose. The cables exhibit a downward curved shape, also known as cable sag which needs to be accounted for in the pose estimation. The catenary equations can accurately describe this phenomenon but are only accurate in equilibrium conditions. Thus, pose estimation for large-scale SCDPR in dynamic motion is an open challenge. This work proposes a real-time pose estimation algorithm for dynamic trajectories of SCDPRs, which is accurate over large areas. We present a novel approach that considers cable sag to reduce the estimation error for large scales while also employing an Inertial Measurement Unit (IMU) to improve estimation accuracy for dynamic motion. Our approach reduces the RMSE to less than a third compared to standard methods not considering cable sag. Similarly, the inclusion of the IMU reduces the RMSE in dynamic situations by 40\% compared to non-IMU aided approaches considering cable sag. Further-more, we evaluate our Extended Kalman Filter (EKF) based algorithm on a real system with ground truth pose information.},
  archive   = {C_IROS},
  author    = {Eren Allak and Rooholla Khorrambakht and Christian Brommer and Stephan Weiss},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981168},
  pages     = {4989-4996},
  title     = {Kinematics-inertial fusion for localization of a 4-cable underactuated suspended robot considering cable sag},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning with yourself: A tangible twin robot system to
promote STEM education. <em>IROS</em>, 4981–4988. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a customized programmable robotic system, TanTwin (Tangible Twin), designed to promote STEM education for K-12 children. Firstly, TanTwin is implemented based on a wheel-robot with standard LEGO bricks. With several deep neural networks, a child can convert a captured portrait of himself/herself into standard LEGO bricks, therefore he/she can build a tangible twin robot of him-selflherself automatically. Besides, to adapt to the customized appearance, the corresponding visual element and content of the robotic system were also changed by a rule-based adaption algorithm. To demonstrate the effectiveness of TanTwin and to investigate whether tangible twin robots could contribute to children&#39;s learning, we conducted a controlled experimental study to compare learning with a TanTwin and with a standard robot system through measuring students&#39; cognitive learning outcomes. The pre-/post- knowledge test results indicated that learning with a tangible twin robot leads to significantly better learning outcomes. Given the results, we validate our system and customization technology can promote STEM education.},
  archive   = {C_IROS},
  author    = {Jiasi Gao and Jiangtao Gong and Guyue Zhou and Haole Guo and Tong Qi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981423},
  pages     = {4981-4988},
  title     = {Learning with yourself: A tangible twin robot system to promote STEM education},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards reproducible evaluations for flying drone
controllers in virtual environments. <em>IROS</em>, 4973–4980. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Research attention on natural user interfaces (NUIs) for drone flights are rising. Nevertheless, NUIs are highly diversified, and primarily evaluated by different physical environments leading to hard-to-compare performance between such solutions. We propose a virtual environment, namely VRFlightSim, enabling comparative evaluations with enriched drone flight details to address this issue. We first replicated a state-of-the-art (SOTA) interface and designed two tasks (crossing and pointing) in our virtual environment. Then, two user studies with 13 participants demonstrate the necessity of VRFlightSim and further highlight the potential of open-data interface designs.},
  archive   = {C_IROS},
  author    = {Zheng Li and Yiming Huang and Yui-Pan Yau and Pan Hui and Lik-Hang Lee},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981123},
  pages     = {4973-4980},
  title     = {Towards reproducible evaluations for flying drone controllers in virtual environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semi-automatic infrared calibration for augmented reality
systems in surgery. <em>IROS</em>, 4957–4964. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Augmented reality (AR) has the potential to improve the immersion and efficiency of computer-assisted orthopaedic surgery (CAOS) by allowing surgeons to maintain focus on the operating site rather than external displays in the operating theatre. Successful deployment of AR to CAOS requires a calibration that can accurately calculate the spatial relationship between real and holographic objects. Several studies attempt this calibration through manual alignment or with additional fiducial markers in the surgical scene. We propose a calibration system that offers a direct method for the calibration of AR head-mounted displays (HMDs) with CAOS systems, by using infrared-reflective marker-arrays widely used in CAOS. In our fast, user-agnostic setup, a HoloLens 2 detected the pose of marker arrays using infrared response and time-of-flight depth obtained through sensors onboard the HMD. Registration with a commercially available CAOS system was achieved when an IR marker-array was visible to both devices. Study tests found relative-tracking mean errors of 2.03 mm and 1.12° when calculating the relative pose between two static marker-arrays at short ranges. When using the calibration result to provide in-situ holographic guidance for a simulated wire- insertion task, a pre-clinical test reported mean errors of 2.07 mm and 1.54° when compared to a pre-planned trajectory.},
  archive   = {C_IROS},
  author    = {Hisham Iqbal and Ferdinando Rodriguez y Baena},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982215},
  pages     = {4957-4964},
  title     = {Semi-automatic infrared calibration for augmented reality systems in surgery},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A torque controlled approach for virtual remote centre of
motion implementation. <em>IROS</em>, 4949–4956. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel torque controller for the implementation virtual remote center of motion. The controller allows the system to implement the required behavior and guarantees the satisfaction of the remote center of motion constraint. Exploiting the Udwadia-Kalaba equation for constrained dynamic systems, the controller is synthesized considering the dynamic effect the constraint produces on the manipulator, achieving more effective control with respect to kinematic strategies, and allowing the implementation of compliance behaviors. Simulations and experimental validation with a KUKA LWR 4+ with 7 degrees of freedom has been performed to check the performances of the proposed controller. Results show the effectiveness of the proposed controller with different control action, and the capability to interact with the environment by implementing compliant motion control.},
  archive   = {C_IROS},
  author    = {Marco Minelli and Cristian Secchi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981619},
  pages     = {4949-4956},
  title     = {A torque controlled approach for virtual remote centre of motion implementation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A deep learning technique as a sensor fusion for enhancing
the position in a virtual reality micro-environment. <em>IROS</em>,
4935–4940. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most virtual reality (VR) applications use a commercial controller for interaction. However, a typical virtual reality controller (VRC) lacks positional precision and accu-racy in millimeter-scale scenarios. This lack of precision and accuracy is caused by built-in sensors drift. Therefore, the tracking performance of a VRC needs to be enhanced for millimeter-scale scenarios. Herein, we introduce a novel way of enhancing the tracking performance of a commercial VRC in a millimeter-scale environment using a deep learning (DL) al-gorithm. Specifically, we use a long short-term memory (LSTM) model trained with data collected from a linear motor, an IMU sensor, and a VRC. We integrate the virtual environment developed in Unity software with the LSTM model running in Python. We designed three experimental conditions: the VRC, Kalman filter (KF), and LSTM modes. Furthermore, we evaluate tracking performances in the three conditions and two other experimental scenarios, namely stationary and dynamic. In the stationary experimental scenario, the system is left motionless for 10 s. By contrast, in the dynamic experimental scenarios, the linear stage moves the system by 12 mm along the X, Y, and Z axes. The experimental results indicate that the deep learning model outperforms the standard controllers positional performance by 85.69\% and 92.14\% in static and dynamic situations, respectively.},
  archive   = {C_IROS},
  author    = {John David Prieto Prada and Miguel Luna and Sang Hyun Park and Cheol Song},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981239},
  pages     = {4935-4940},
  title     = {A deep learning technique as a sensor fusion for enhancing the position in a virtual reality micro-environment},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). WFH-VR: Teleoperating a robot arm to set a dining table
across the globe via virtual reality. <em>IROS</em>, 4927–4934. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an easy-to-deploy, virtual reality-based teleoperation system for controlling a robot arm. The proposed system is based on a consumer-grade virtual reality device (Oculus Quest 2) with a low-cost robot arm (a LoCoBot) to allow easy replication and set up. The proposed Work-from-Home Virtual Reality (WFH-VR) system allows the user to feel an intimate connection with the real remote robot arm. Virtual representations of the robot and objects to be manipulated in the real-world are presented in VR by streaming data pertaining to orientation and poses. The user studies suggest that 1) the proposed telerobotic system is effective under conditions both with and without network latency, whereas a method that simply streams video does not. This design enables the system implemented at an arbitrary distance from the actual work site. 2) The proposed system allows novices to perform manipulation tasks requiring higher dexterity than traditional keyboard controls can support, such as setting tableware. All results, hardware settings, and questionnaire feedback can be obtained at https://arg-nctu.github.io/projects/vr-robot-arm.html.},
  archive   = {C_IROS},
  author    = {Lai Sum Yim and Quang TN Vo and Ching-I Huang and Chi-Ruei Wang and Wren McQueary and Hsueh-Cheng Wang and Haikun Huang and Lap-Fai Yu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981729},
  pages     = {4927-4934},
  title     = {WFH-VR: Teleoperating a robot arm to set a dining table across the globe via virtual reality},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Immersive view and interface design for teleoperated aerial
manipulation. <em>IROS</em>, 4919–4926. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The recent momentum in aerial manipulation has led to an interest in developing virtual reality interfaces for aerial physical interaction tasks with simple, intuitive, and reliable control and perception. However, this requires the use of expensive subsystems and there is still a research gap between interface design, user evaluations and the effect on aerial manipulation tasks. Here, we present a methodology for low-cost available drone systems with a Unity-based interface for immersive FPV teleoperation. We applied our approach in a flight track where a cluttered environment is used to simulate a demanding aerial manipulation task inspired by forestry drones and canopy sampling. Through objective measures of teleoperation performance and subjective questionnaires, we found that operators performed worse using the FPV interface and had higher perceived levels of cognitive load when compared to traditional interface design. Additional analysis of physiological measures highlighted that objective stress levels and cognitive load were also influenced by task duration and perceived performance, providing an insight into what interfaces could target to support teleoperator requirements during aerial manipulation tasks.},
  archive   = {C_IROS},
  author    = {Basaran Bahadir Kocer and Harvey Stedman and Patryk Kulik and Izaak Caves and Nejra Van Zalk and Vijay M. Pawar and Mirko Kovac},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981656},
  pages     = {4919-4926},
  title     = {Immersive view and interface design for teleoperated aerial manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Photometric single-view dense 3D reconstruction in
endoscopy. <em>IROS</em>, 4904–4910. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual SLAM inside the human body will open the way to computer-assisted navigation in endoscopy. However, due to space limitations, medical endoscopes only provide monocular images, leading to systems lacking true scale. In this paper, we exploit the controlled lighting in colonoscopy to achieve the first in-vivo 3D reconstruction of the human colon using photometric stereo on a calibrated monocular endoscope. Our method works in a real medical environment, providing both a suitable in-place calibration procedure and a depth estimation technique adapted to the colon&#39;s tubular geometry. We validate our method on simulated colonoscopies, obtaining a mean error of 7\% on depth estimation, which is below 3 mm on average. Our qualitative results on the EndoMapper dataset show that the method is able to correctly estimate the colon shape in real human colonoscopies, paving the ground for truescale monocular SLAM in endoscopy.},
  archive   = {C_IROS},
  author    = {Víctor M. Batlle and J.M.M. Montiel and Juan D. Tardós},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981742},
  pages     = {4904-4910},
  title     = {Photometric single-view dense 3D reconstruction in endoscopy},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). When geometry is not enough: Using reflector markers in
lidar SLAM. <em>IROS</em>, 4880–4887. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Lidar-based SLAM systems perform well in a wide range of circumstances by relying on the geometry of the environment. However, even mature and reliable approaches struggle when the environment contains structureless areas such as long hallways. To allow the use of lidar-based SLAM in such environments, we propose to add reflector markers in specific locations that would otherwise be difficult. We present an algorithm to reliably detect these markers and two approaches to fuse the detected markers with geometry-based scan matching. The performance of the proposed methods is demonstrated on real-world datasets from several industrial environments.},
  archive   = {C_IROS},
  author    = {Gerhard Kurz and Sebastian A. Scherer and Peter Biber and David Fleer},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981522},
  pages     = {4880-4887},
  title     = {When geometry is not enough: Using reflector markers in lidar SLAM},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gravity-constrained point cloud registration. <em>IROS</em>,
4873–4879. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual and lidar Simultaneous Localization and Mapping (SLAM) algorithms benefit from the Inertial Measurement Unit (IMU) modality. The high-rate inertial data complement the other lower-rate modalities. Moreover, in the absence of constant acceleration, the gravity vector makes two attitude angles out of three observable in the global coordinate frame. In visual odometry, this is already being used to reduce the 6-Degrees Of Freedom (DOF) pose estimation problem to 4-DOF. In lidar SLAM, the gravity measurements are often used as a penalty in the back-end global map optimization to prevent map deformations. In this work, we propose an Iterative Closest Point (ICP)-based front-end which exploits the observable DOF and provides pose estimates aligned with the gravity vector. We believe that this front-end has the potential to support the loop closure identification, thus speeding up convergences of global map optimizations. The presented approach has been extensively tested against accurate ground-truth localization in large-scale outdoor environments as well as in the Subterranean Challenge organized by Defense Advanced Research Projects Agency (DARPA). We show that it can reduce the localization drift by 30\% when compared to the standard 6-DOF ICP. Moreover, the code is readily available to the community as a part of the libpointmatcher library.},
  archive   = {C_IROS},
  author    = {Vladimír Kubelka and Maxime Vaidis and François Pomerleau},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981916},
  pages     = {4873-4879},
  title     = {Gravity-constrained point cloud registration},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MOTSLAM: MOT-assisted monocular dynamic SLAM using
single-view depth estimation. <em>IROS</em>, 4865–4872. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual SLAM systems targeting static scenes have been developed with satisfactory accuracy and robustness. Dynamic 3D object tracking has then become a significant capability in visual SLAM with the requirement of under-standing dynamic surroundings in various scenarios including autonomous driving, augmented and virtual reality. However, performing dynamic SLAM solely with monocular images remains a challenging problem due to the difficulty of asso-ciating dynamic features and estimating their positions. In this paper, we present MOTSLAM, a dynamic visual SLAM system with the monocular configuration that tracks both poses and bounding boxes of dynamic objects. MOTSLAM first performs multiple object tracking (MOT) with associated both 2D and 3D bounding box detection to create initial 3D objects. Then, neural-network-based monocular depth estimation is applied to fetch the depth of dynamic features. Finally, camera poses, object poses, and both static, as well as dynamic map points, are jointly optimized using a novel bundle adjustment. Our experiments on the KITTI dataset demonstrate that our system has reached best performance on both camera ego-motion and object tracking on monocular dynamic SLAM.},
  archive   = {C_IROS},
  author    = {Hanwei Zhang and Hideaki Uchiyama and Shintaro Ono and Hiroshi Kawasaki},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982280},
  pages     = {4865-4872},
  title     = {MOTSLAM: MOT-assisted monocular dynamic SLAM using single-view depth estimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Floorplan-aware camera poses refinement. <em>IROS</em>,
4857–4864. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Processing large indoor scenes is a challenging task, as scan registration and camera trajectory estimation methods accumulate errors across time. As a result, the quality of reconstructed scans is insufficient for some applications, such as visual-based localization and navigation, where the correct position of walls is crucial. For many indoor scenes, there exists an image of a technical ftoorplan that contains information about the geometry and main structural elements of the scene, such as walls, partitions, and doors. We argue that such a ftoorplan is a useful source of spatial information, which can guide a 3D model optimization. The standard RGB-D 3D reconstruction pipeline consists of a tracking module applied to an RGB-D sequence and a bundle adjustment (BA) module that takes the posed RGB-D sequence and corrects the camera poses to improve consistency. We propose a novel optimization algorithm expanding conventional BA that leverages the prior knowledge about the scene structure in the form of a ftoorplan. Our experiments on the Redwood dataset and our self-captured data demonstrate that utilizing ftoorplan improves accuracy of 3D reconstructions.},
  archive   = {C_IROS},
  author    = {Anna Sokolova and Filipp Nikitin and Anna Vorontsova and Anton Konushin},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981148},
  pages     = {4857-4864},
  title     = {Floorplan-aware camera poses refinement},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Group-<span class="math inline"><em>k</em></span> consistent
measurement set maximization for robust outlier detection.
<em>IROS</em>, 4849–4856. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a method for the robust selection of measurements in a simultaneous localization and mapping (SLAM) framework. Existing methods check consistency or compatibility on a pairwise basis, however many measurement types are not sufficiently constrained in a pairwise scenario to determine if either measurement is inconsistent with the other. This paper presents group- $k$ consistency maximization ( $\mathrm{G}k\text{CM}$ ) that estimates the largest set of measurements that is internally group- $k$ consistent. Solving for the largest set of group- $k$ consistent measurements can be formulated as an instance of the maximum clique problem on generalized graphs and can be solved by adapting current methods. This paper evaluates the performance of $\mathrm{G}k\text{CM}$ using simulated data and compares it to pairwise consistency maximization (PCM) presented in previous work.},
  archive   = {C_IROS},
  author    = {Brendon Forsgren and Ram Vasudevan and Michael Kaess and Timothy W. McLain and Joshua G. Mangelson},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982057},
  pages     = {4849-4856},
  title     = {Group-$k$ consistent measurement set maximization for robust outlier detection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Motion planning for HyTAQs: A topology-guided unified NMPC
approach. <em>IROS</em>, 4835–4840. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this study, a topology-guided unified nonlinear model predictive control (NMPC) approach is proposed for autonomous navigation of a class of Hybrid Terrestrial and Aerial Quadrotors (HyTAQs) in unknown environments. The approach can fully exploit the hybrid terrestrial-aerial locomotion of the vehicle and as such ensure a high navigation efficiency. A unified terrestrial-aerial NMPC is first formulated with a type of complementarity constraints involving the hybrid dynamics, together with the collision avoidance constraints for safety. Further, a topological roadmap with both terrestrial and aerial paths is leveraged to guide the kinodynamic path searching and thus the unified NMPC. Then, a complete and distinctive navigation framework is established and validated on our self-developed HyTAQ. Compared with the existing unified terrestrial-aerial planning methods, ours takes the vehicle dynamics into account for the first attempt and achieves a more reasonable decision of modes switching. Experimental results are presented to demonstrate the effectiveness and superiority of the proposed approach.},
  archive   = {C_IROS},
  author    = {Tong Wu and Yimin Zhu and Lixian Zhang and Jianan Yang and Yihang Ding},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981106},
  pages     = {4835-4840},
  title     = {Motion planning for HyTAQs: A topology-guided unified NMPC approach},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). P2EG: Prediction and planning integrated robust
decision-making for automated vehicle negotiating in narrow lane with
explorative game. <em>IROS</em>, 4819–4826. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the narrow lane scene of autonomous driving, it is critical for the ego car to recognize the intentions of social vehicles and cooperate with them. However, cooperating with social vehicles is challenging due to insufficient information. This paper proposes an Explorative Game that adopts Participant Game and Perfect Bayesian Equilibrium to exploratively perform some aggressive actions to obtain additional information, thus the autonomous vehicle can cooperate robustly and efficiently. Explorative Game assumes each vehicle maintains a unique belief about the current situation and attributes insecurity and instability to the conflict of various Perfect Bayesian Equilibriums formed by various beliefs. Aggressive actions enable the ego car to proactively guide social vehicles to cooperate as it expects and encourage them to express their intentions as quickly and clearly as possible so that the equilibriums can converge and the conflict can be eliminated. Additional information reduces the error between the actual intentions of social vehicles and the estimated intentions from the ego car, helping rationally prune potential interactions and update parameters of the reward function. We demonstrate our algorithm on recorded data as well as virtual environments with manually controlled social vehicles to prove the efficiency of cooperation and the robustness of decision-making. And it has been running for more than 20 kilometers in the real world.},
  archive   = {C_IROS},
  author    = {Qianyi Zhang and Xiao Li and Ethan He and Shuguang Ding and Naizheng Wang and Jingtai Liu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981332},
  pages     = {4819-4826},
  title     = {P2EG: Prediction and planning integrated robust decision-making for automated vehicle negotiating in narrow lane with explorative game},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RIANet: Road graph and image attention network for urban
autonomous driving. <em>IROS</em>, 4805–4810. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a novel autonomous driving framework, called a road graph and image attention network (RIANet), which computes the attention scores of objects in the image using the road graph feature. The process of the proposed method is as follows: First, the feature encoder module encodes the road graph, image, and additional features of the scene. The attention network module then incorporates the encoded features and computes the scene context feature via the attention mechanism. Finally, the low-level controller mod-ule drives the ego-vehicle based on the scene context feature. In the experiments, we use an urban scene driving simulator named CARLA to train and test the proposed method. The results show that the proposed method outperforms existing autonomous driving methods.},
  archive   = {C_IROS},
  author    = {Timothy Ha and Jeongwoo Oh and Hojun Chung and Gunmin Lee and Songhwai Oh},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982184},
  pages     = {4805-4810},
  title     = {RIANet: Road graph and image attention network for urban autonomous driving},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GPU-parallelized iterative LQR with input constraints for
fast collision avoidance of autonomous vehicles. <em>IROS</em>,
4797–4804. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collision avoidance in emergency situations is a crucial and challenging task in motion planning for autonomous vehicles. Especially in the field of optimization-based planning using nonlinear model predictive control, many efforts to achieve real-time performance are still ongoing. Among various approaches, the iterative linear quadratic regulator (iLQR) is known as an efficient means of nonlinear optimization. Additionally, parallel computing architectures, such as GPUs, are more widely applied in autonomous vehicles. In this paper, we propose 1) a parallel computing framework for iLQR with input constraints considering the characteristics of the problem and 2) a proper environmental formulation that can be covered with single-precision floating-point computation of the GPU. The GPU-accelerated framework was tested on a real-time simulation-in-the-loop system using CarMaker and ROS at a 20 Hz sampling rate on a low-performance mobile computer and was compared against the same framework realized with a CPU.},
  archive   = {C_IROS},
  author    = {Yeongseok Lee and Minsu Cho and Kyung-Soo Kim},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982026},
  pages     = {4797-4804},
  title     = {GPU-parallelized iterative LQR with input constraints for fast collision avoidance of autonomous vehicles},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A unified MPC design approach for AGV path following.
<em>IROS</em>, 4789–4796. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a unified approach to the design of Model Predictive Controllers (MPC), custom-tailored for path following by Automated Guided Vehicles (AGVs). The approach can be applied in a unified manner to several relevant AGV kinematic configurations, including tricycle, differential, and double steer-drive. By leveraging Linear Parameter Varying (LPV) MPC, it provides maximum maneuverability and industrial-grade positioning accuracy. We incorporate state-of-the-art optimized velocity planning, to maximize vehicle utilization. Experimental validation is performed on three different kinematic configurations, including a real forklift with tricycle configuration, using industrially-relevant positioning maneuvers.},
  archive   = {C_IROS},
  author    = {Mirko Kokot and Damjan Miklić and Tamara Petrović},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981575},
  pages     = {4789-4796},
  title     = {A unified MPC design approach for AGV path following},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the importance of label encoding and uncertainty
estimation for robotic grasp detection. <em>IROS</em>, 4781–4788. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automated grasping of arbitrary objects is an essential skill for many applications such as smart manufacturing and human robot interaction. This makes grasp detection a vital skill for automated robotic systems. Recent work in model-free grasp detection uses point cloud data as input and typically outperforms the earlier work on RGB(D)-based methods. We show that RGB(D)-based methods are being underestimated due to suboptimal label encodings used for training. Using the evaluation pipeline of the GraspNet-1Billion dataset, we investigate different encodings and propose a novel encoding that significantly improves grasp detection on depth images. Additionally, we show shortcomings of the 2D rectangle grasps supplied by the GraspNet-1Billion dataset and propose a filtering scheme by which the ground truth labels can be improved significantly. Furthermore, we apply established methods for uncertainty estimation on our trained models since knowing when we can trust the model&#39;s decisions provides an advantage for real-world application. By doing so, we are the first to directly estimate uncertainties of detected grasps. We also investigate the applicability of the estimated aleatoric and epistemic uncertainties based on their theoretical properties. Additionally, we demonstrate the correlation between estimated uncertainties and grasp quality, thus improving selection of high quality grasp detections. By all these modifications, our approach using only depth images can compete with point-cloud-based approaches for grasp detection despite the lower degree of freedom for grasp poses in 2D image space.},
  archive   = {C_IROS},
  author    = {Benedict Stephan and Dustin Aganian and Lars Hinneburg and Markus Eisenbach and Steffen Müller and Horst-Michael Gross},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981866},
  pages     = {4781-4788},
  title     = {On the importance of label encoding and uncertainty estimation for robotic grasp detection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scene editing as teleoperation: A case study in 6DoF kit
assembly. <em>IROS</em>, 4773–4780. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Studies in robot teleoperation have been centered around action specifications-from continuous joint control to discrete end-effector pose control. However, these “robot-centric” interfaces often require skilled operators with extensive robotics expertise. To make teleoperation accessible to nonexpert users, we propose the framework “Scene Editing as Teleoperation” (SEaT), where the key idea is to transform the traditional “robot-centric” interface into a “scene-centric” interface-instead of controlling the robot, users focus on specifying the task&#39;s goal by manipulating digital twins of the real-world objects. As a result, a user can perform teleoperation without any expert knowledge of the robot hardware. To achieve this goal, we utilize a category-agnostic scene-completion algorithm that translates the real-world workspace (with unknown objects) into a manipulable virtual scene representation and an action-snapping algorithm that refines the user input before generating the robot&#39;s action plan. To train the algorithms, we procedurely generated a large-scale, diverse kit-assembly dataset that contains object-kit pairs that mimic real-world object-kitting tasks. Our experiments in simulation and on a real-world system demonstrate that our framework improves both the efficiency and success rate for 6DoF kit-assembly tasks. A user study demonstrates that SEaT framework participants achieve a higher task success rate and report a lower subjective workload compared to an alternative robot-centric interface.},
  archive   = {C_IROS},
  author    = {Yulong Li and Shubham Agrawal and Jen-Shuo Liu and Steven K. Feiner and Shuran Song},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982158},
  pages     = {4773-4780},
  title     = {Scene editing as teleoperation: A case study in 6DoF kit assembly},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph-structured policy learning for multi-goal manipulation
tasks. <em>IROS</em>, 4765–4772. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-goal policy learning for robotic manipu-lation is challenging. Prior successes have used state-based representations of the objects or provided demonstration data to facilitate learning. In this paper, by hand-coding a high-level discrete representation of the domain, we show that policies to reach dozens of goals can be learned with a single network using Q-learning from pixels. The agent focuses learning on simpler, local policies which are sequenced together by planning in the abstract space. We compare our method against standard multi-goal RL baselines, as well as other methods that leverage the discrete representation, on a challenging block construction domain. We find that our method can build more than a hundred different block structures, and demonstrate forward transfer to structures with novel objects. Lastly, we deploy the policy learned in simulation on a real robot.},
  archive   = {C_IROS},
  author    = {David Klee and Ondrej Biza and Robert Platt},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981295},
  pages     = {4765-4772},
  title     = {Graph-structured policy learning for multi-goal manipulation tasks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A two-stage learning architecture that generates
high-quality grasps for a multi-fingered hand. <em>IROS</em>, 4757–4764.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate the problem of planning stable grasps for object manipulations using an 18-DOF robotic hand with four fingers. The main challenge here is the high-dimensional search space, and we address this problem using a novel two-stage learning process. In the first stage, we train an autoregressive network called the hand-pose-generator, which learns to generate a distribution of valid 6D poses of the palm for a given volumetric object representation. In the second stage, we employ a network that regresses 12D finger joint configurations and a scalar grasp quality from given object representations and palm poses. To train our networks, we use synthetic training data generated by a novel grasp planning algorithm, which also proceeds stage-wise: first the palm pose, then the finger positions. Here, we devise a Bayesian Optimization scheme for the palm pose and a physics-based grasp pose metric to rate stable grasps. In experiments on the YCB benchmark data set, we show a grasp success rate of over 83\%, as well as qualitative results grasping unknown objects on a real robot system.},
  archive   = {C_IROS},
  author    = {Dominik Winkelbauer and Berthold Bäuml and Matthias Humt and Nils Thuerey and Rudolph Triebel},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981133},
  pages     = {4757-4764},
  title     = {A two-stage learning architecture that generates high-quality grasps for a multi-fingered hand},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning a state estimator for tactile in-hand manipulation.
<em>IROS</em>, 4749–4756. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of estimating the pose of an object which is being manipulated by a multi-fingered robotic hand by only using proprioceptive feedback. To address this challenging problem, we propose a novel variant of differentiable particle filters, which combines two key extensions. First, our learned proposal distribution incorporates recent measurements in a way that mitigates weight degeneracy. Second, the particle update works on non-euclidean manifolds like Lie-groups, enabling learning-based pose estimation in 3D on SE(3). We show that the method can represent the rich and often multi-modal distributions over poses that arise in tactile state estimation. The models are trained in simulation, but by using domain randomization, we obtain state estimators that can be employed for pose estimation on a real robotic hand (equipped with joint torque sensors). Moreover, the estimator runs fast, allowing for online usage with update rates of more than 100 Hz on a single CPU core. We quantitatively evaluate our method and benchmark it against other approaches in simulation. We also show qualitative experiments on the real torque-controlled DLR-Hand II.},
  archive   = {C_IROS},
  author    = {Lennart Röstel and Leon Sievers and Johannes Pitz and Berthold Bäuml},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981730},
  pages     = {4749-4756},
  title     = {Learning a state estimator for tactile in-hand manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Context and intention aware 3D human body motion prediction
using an attention deep learning model in handover tasks. <em>IROS</em>,
4743–4748. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work explores how contextual information and human intention affect the motion prediction of humans during a handover operation with a social robot. By classifying human intention in four different classes, we developed a model able to generate a different motion for each intention class. Furthermore, the model uses a multi-headed attention architecture to add contextual information to the pipeline, such as the position of the robot end effector (REE) or the position of obstacles in the interaction scene. We generate predictions up to two and half seconds in the future given an input sequence of one second containing the previous motion of the human. The results show an improvement of the prediction accuracy, both for the full skeleton prediction and the human hand used for the delivery. The model also allows to generate different sequences with the desired human intention.},
  archive   = {C_IROS},
  author    = {Javier Laplaza and Francesc Moreno-Noguer and Alberto Sanfeliu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981465},
  pages     = {4743-4748},
  title     = {Context and intention aware 3D human body motion prediction using an attention deep learning model in handover tasks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 6D robotic assembly based on RGB-only object pose
estimation. <em>IROS</em>, 4736–4742. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vision-based robotic assembly is a crucial yet challenging task as the interaction with multiple objects requires high levels of precision. In this paper, we propose an integrated 6D robotic system to perceive, grasp, manipulate and assemble blocks with tight tolerances. Aiming to provide an off-the-shelf RGB-only solution, our system is built upon a monocular 6D object pose estimation network trained solely with synthetic images leveraging physically-based rendering. Subsequently, pose-guided 6D transformation along with collision-free assembly is proposed to construct any designed structure with arbitrary initial poses. Our novel 3-axis calibration operation further enhances the precision and robustness by disentangling 6D pose estimation and robotic assembly. Both quantitative and qualitative results demonstrate the effectiveness of our proposed 6D robotic assembly system.},
  archive   = {C_IROS},
  author    = {Bowen Fu and Sek Kun Leong and Xiaocong Lian and Xiangyang Ji},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982262},
  pages     = {4736-4742},
  title     = {6D robotic assembly based on RGB-only object pose estimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SEED: Series elastic end effectors in 6D for visuotactile
tool use. <em>IROS</em>, 4684–4691. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose the framework of Series Elastic End Effectors in 6D (SEED), which combines a spatially compliant element with visuotactile sensing to grasp and manipulate tools in the wild. Our framework generalizes the benefits of series elasticity to 6-dof, while providing an abstraction of control using visuotactile sensing. We propose an algorithm for relative pose estimation from visuotactile sensing, and a spatial hybrid force-position controller capable of achieving stable force interaction with the environment. We demonstrate the effectiveness of our framework on tools that require regulation of spatial forces. Video link: https://youtu.be/2-YuIfspDrk.},
  archive   = {C_IROS},
  author    = {H.J. Terry Suh and Naveen Kuppuswamy and Tao Pang and Paul Mitiguy and Alex Alspach and Russ Tedrake},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982092},
  pages     = {4684-4691},
  title     = {SEED: Series elastic end effectors in 6D for visuotactile tool use},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Extrinsic dexterous manipulation with a direct-drive hand: A
case study. <em>IROS</em>, 4660–4667. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper explores a novel approach to dexterous manipulation, aimed at levels of speed, precision, robustness, and simplicity suitable for practical deployment. The enabling technology is a Direct-drive Hand (DDHand) comprising two fingers, two DOFs each, that exhibit high speed and a light touch. The test application is the dexterous manipulation of three small and irregular parts, moving them to a grasp suitable for a subsequent assembly operation, regardless of initial presentation. We employed four primitive behaviors that use ground contact as a “third finger”, prior to or during the grasp process: pushing, pivoting, toppling, and squeeze-grasping. In our experiments, each part was presented from 30 to 90 times randomly positioned in each stable pose. Success rates varied from 83\% to 100\%. The time to manipulate and grasp was 6.32 seconds on average, varying from 2.07 to 16 seconds. In some cases, performance was robust, precise, and fast enough for practical applications, but in other cases, pose uncertainty required time-consuming vision and arm motions. The paper concludes with a discussion of further improvements required to make the primitives robust, eliminate uncertainty, and reduce this dependence on vision and arm motion.},
  archive   = {C_IROS},
  author    = {Arnav Gupta and Yuemin Mao and Ankit Bhatia and Xianyi Cheng and Jonathan King and Yifan Hou and Matthew T. Mason},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981569},
  pages     = {4660-4667},
  title     = {Extrinsic dexterous manipulation with a direct-drive hand: A case study},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The good grasp, the bad grasp, and the plateau in
tactile-based grasp stability prediction. <em>IROS</em>, 4653–4659. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Research around tactile sensing for grasp stability prediction in robotic manipulators continues to be popular, however few works are able to achieve a high classification accuracy. Due to simulation complexity, data-driven methods are often forced to rely on experimental data, yielding small, often unbalanced, data sets. In this work, the authors use a 3972 sample data set to explore the effects of the data set composition on the performance of a classifier. While maintaining a similar overall accuracy, the ability to recognize a grasp failure was significantly impacted by the composition of the data set. The authors propose an autonomous pipeline designed to generate more diverse failure grasps. On failure-rich data, a tactile-based classifier with a balanced training set achieved a classification accuracy of 84.68\% while maintaining a recall of the grasp failure class of 76\%. This represents a 71.79\% improvement in recall over a model trained on a larger but unbalanced data set.},
  archive   = {C_IROS},
  author    = {Jennifer Kwiatkowski and Mohammad Jolaei and Alexandre Bernier and Vincent Duchaine},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981360},
  pages     = {4653-4659},
  title     = {The good grasp, the bad grasp, and the plateau in tactile-based grasp stability prediction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel human-safe robotic gripper: An application of a
programmable permanent magnet actuator. <em>IROS</em>, 4646–4652. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While collaborative robotic arms offer significant safety benefits, safety of the overall manipulator system cannot be guaranteed unless equally strict safety requirements are satisfied by the accompanying end-effector. Current robot grippers are not made in a way that fulfills such a requirement, resulting in collaborative robots needing to operate in a protected environment. This paper presents a novel permanent magnet actuator inside of a conventional industrial electric gripper which results in an end-effector that has an unmatched force range of 1-2N to 43N and exhibits interesting characteristics suited to the requirements of a safe gripper such as torque holding without power, variable stiffness and force sensing.},
  archive   = {C_IROS},
  author    = {Chandramouly Ulagaoozhian and Vincent Duchaine},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981155},
  pages     = {4646-4652},
  title     = {A novel human-safe robotic gripper: An application of a programmable permanent magnet actuator},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Designing underactuated graspers with dynamically variable
geometry using potential energy map based analysis. <em>IROS</em>,
4638–4645. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we present a potential energy map based approach that provides a framework for the design and control of a robotic grasper. Unlike other potential energy map approaches, our framework considers friction for a more realistic perspective on grasper performance. Our analysis establishes the importance of considering dynamically variable geometry in grasper design, namely palm width, link lengths, and transmission ratios, which are assumed to be able to change in real-time. Our analysis assumes a two-phalanx tendon-pulley underactuated grasper, but it can be extended to other underactuated mechanisms. We demonstrate the utility of these novel potential energy maps and the method used to generate them in order by showing how various design parameters impact the grasping and in-hand manipulation performance of a particular design across a range of object sizes and friction coefficients. Optimal grasping designs have palms that scale with object size and transmission ratios that scale with the coefficient of friction. Using a custom in-hand manipulation metric, we compared the in-hand manipulation capabilities of a grasper that only dynamically varied its palm size, link lengths, and transmission ratios to a grasper with a variable palm and controllable actuation efforts. The analysis revealed the advantage of dynamically variable geometry; by varying only its palm size, link lengths, and transmission ratios in real-time, safe, caged in-hand manipulation of a wide range of objects could be performed.},
  archive   = {C_IROS},
  author    = {Connor L. Yako and Shenli Yuan and J. Kenneth Salisbury},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982148},
  pages     = {4638-4645},
  title     = {Designing underactuated graspers with dynamically variable geometry using potential energy map based analysis},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enabling massage actions: An interactive parallel robot with
compliant joints. <em>IROS</em>, 4632–4637. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a parallel massage robot with compliant joints based on the series elastic actuator (SEA), offering a unified force-position control approach. First, the kinematic and static force models are established for obtaining the corresponding control variables. Then, a novel force-position control strategy is proposed to separately control the force-position along the normal direction of the surface and another two-direction displacement, without the requirement of a robotic dynamics model. To evaluate its performance, we implement a series of robotic massage experiments. The results demonstrate that the proposed massage manipulator can successfully achieve desired forces and motion patterns of massage tasks, arriving at a high-score user experience.},
  archive   = {C_IROS},
  author    = {Huixu Dong and Yue Feng and Chen Qiu and Ye Pan and Miao He and I-Ming Chen},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981235},
  pages     = {4632-4637},
  title     = {Enabling massage actions: An interactive parallel robot with compliant joints},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tactile-sensitive NewtonianVAE for high-accuracy industrial
connector insertion. <em>IROS</em>, 4625–4631. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An industrial connector insertion task requires submillimeter positioning and grasp pose compensation for a plug. Thus, highly accurate estimation of the relative pose between a plug and socket is fundamental for achieving the task. World models are promising technologies for visuomotor control because they obtain appropriate state representation to jointly optimize feature extraction and latent dynamics model. Recent studies show that the Newto-nianVAE, a type of the world model, acquires latent space equivalent to mapping from images to physical coordinates. Proportional control can be achieved in the latent space of NewtonianVAE. However, applying NewtonianVAE to high-accuracy industrial tasks in physical environments is an open problem. Moreover, the existing framework does not consider the grasp pose compensation in the obtained latent space. In this work, we proposed tactile-sensitive Newtonian-VAE and applied it to a USB connector insertion with grasp pose variation in the physical environments. We adopted a GelSight-type tactile sensor and estimated the insertion position compensated by the grasp pose of the plug. Our method trains the latent space in an end-to-end manner, and no additional engineering and annotation are required. Simple proportional control is available in the obtained latent space. Moreover, we showed that the original NewtonianVAE fails in some situations, and demonstrated that domain knowledge induction improves model accuracy. This domain knowledge can be easily obtained using robot specification and grasp pose error measurement. We demonstrated that our proposed method achieved a 100\% success rate and 0.3 mm positioning accuracy in the USB connector insertion task in the physical environment. It outperformed SOTA CNN-based two-stage goal pose regression with grasp pose compensation using coordinate transformation.},
  archive   = {C_IROS},
  author    = {Ryo Okumura and Nobuki Nishio and Tadahiro Taniguchi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981610},
  pages     = {4625-4631},
  title     = {Tactile-sensitive NewtonianVAE for high-accuracy industrial connector insertion},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-directional bicycle robot for bridge inspection with
steel defect detection system. <em>IROS</em>, 4617–4624. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel design of a multi-directional bicycle robot, which is developed for the inspection of steel structures, in particular, steel-reinforced bridges. The locomotion concept is based on arranging two magnetic wheels in a bicycle-like configuration with two independent steering actuators. This configuration allows the robot to possess multi-directional mobility. An additional free joint helps the robot adapt naturally to non-flat and complex steel structures. The robot&#39;s design provides the advantage of being mechanically simple and providing high-level mobility across diverse steel structures. In addition, a visual sensor is equipped that allows the data collection for steel defect detection with offline training and validation. The paper also provides a novel pipeline for Steel Defect Detection, which utilizes multiple datasets (one for training and one for validation) from real bridges. The quantitative results have been reported for three Deep Encoder-Decoder Networks (i.e., LinkNet, UNet, DeepLab) with their corresponding Encoder modules (i.e., ResNet-18, ResNet-34, RegNet-X2, EfficientNet-B0, and EfficientNet-B2). Due to space concerns, the qualitative results have been outlined in Appendix, with a link in Fig. 11 caption to access the result provided.},
  archive   = {C_IROS},
  author    = {Habib Ahmed and Son Thanh Nguyen and Duc La and Chuong Phuoc Le and Hung Manh La},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981325},
  pages     = {4617-4624},
  title     = {Multi-directional bicycle robot for bridge inspection with steel defect detection system},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mobile manipulation leveraging multiple views.
<em>IROS</em>, 4585–4592. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While both navigation and manipulation are chal-lenging topics in isolation, many tasks require the ability to both navigate and manipulate in concert. To this end, we propose a mobile manipulation system that leverages novel navigation and shape completion methods to manipulate an object with a mobile robot. Our system utilizes uncertainty in the initial estimation of a manipulation target to calculate a predicted next-best-view. Without the need of localization, the robot then uses the predicted panoramic view at the next-best-view location to navigate to the desired location, capture a second view of the object, create a new model that predicts the shape of object more accurately than a single image alone, and uses this model for grasp planning. We show that the system is highly effective for mobile manipulation tasks through simulation experiments using real world data, as well as ablations on each component of our system.},
  archive   = {C_IROS},
  author    = {David Watkins-Valls and Peter K Allen and Henrique Maia and Madhavan Seshadri and Jonathan Sanabria and Nicholas Waytowich and Jacob Varley},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981277},
  pages     = {4585-4592},
  title     = {Mobile manipulation leveraging multiple views},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Safe drone flight with time-varying backup controllers.
<em>IROS</em>, 4577–4584. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The weight, space, and power limitations of small aerial vehicles often prevent the application of modern control techniques without significant model simplifications. Moreover, high-speed agile behavior, such as that exhibited in drone racing, make these simplified models too unreliable for safety-critical control. In this work, we introduce the concept of time-varying backup controllers (TBCs): user-specified maneuvers combined with backup controllers that generate reference trajectories which guarantee the safety of nonlinear systems. TBCs reduce conservatism when compared to traditional backup controllers and can be directly applied to multi-agent coordination to guarantee safety. Theoretically, we provide conditions under which TBCs strictly reduce conservatism, describe how to switch between several TBC&#39;s and show how to embed TBCs in a multi-agent setting. Experimentally, we verify that TBCs safely increase operational freedom when filtering a pilot&#39;s actions and demonstrate robustness and computational efficiency when applied to decentralized safety filtering of two quadrotors.},
  archive   = {C_IROS},
  author    = {Andrew Singletary and Aiden Swann and Ivan Dario Jimenez Rodriguez and Aaron D. Ames},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981741},
  pages     = {4577-4584},
  title     = {Safe drone flight with time-varying backup controllers},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Risk-aware motion planning for collision-tolerant aerial
robots subject to localization uncertainty. <em>IROS</em>, 4561–4568.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper contributes a novel strategy towards risk-aware motion planning for collision-tolerant aerial robots subject to localization uncertainty. Attuned to the fact that micro aerial vehicles are often tasked to navigate within GPS-denied, possibly unknown, confined and obstacle-filled environments the proposed method exploits collision-tolerance at the robot design level to mitigate the risks of collisions especially as their likelihood increases with growing uncertainty. Accounting for the maximum kinetic energy with which an impact is considered safe, alongside the robot dynamics, the planner builds a set of admissible uncertainty-aware and collision-inclusive paths over a horizon involving multiple motion steps. The first step of the best path is executed by the robot, while the procedure is then repeated in a receding horizon manner. Evaluated in extensive simulation studies and experimental results with a collision-tolerant flying robot, the planner successfully considers the interplay between uncertainty and the likelihood of a collision, balances the risks of possible impacts and enables to navigate safely within highly cluttered environments.},
  archive   = {C_IROS},
  author    = {Paolo De Petris and Mihir Dharmadhikari and Huan Nguyen and Kostas Alexis},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981597},
  pages     = {4561-4568},
  title     = {Risk-aware motion planning for collision-tolerant aerial robots subject to localization uncertainty},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robot-assisted nuclear disaster response: Report and
insights from a field exercise. <em>IROS</em>, 4545–4552. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper reports on insights by robotics researchers that participated in a 5-day robot-assisted nuclear disaster response field exercise conducted by Kerntechnische Hilfdienst GmbH (KHG) in Karlsruhe, Germany. The German nuclear industry established KHG to provide a robot-assisted emergency response capability for nuclear accidents. We present a systematic description of the equipment used; the robot operators&#39; training program; the field exercise and robot tasks; and the protocols followed during the exercise. Additionally, we provide insights and suggestions for advancing disaster response robotics based on these observations. Specifically, the main degradation in performance comes from the cognitive and attentional demands on the operator. Furthermore, robotic platforms and modules should aim to be robust and reliable in addition to their ease of use. Last, as emergency response stakeholders are often skeptical about using autonomous systems, we suggest adopting a variable autonomy paradigm to integrate autonomous robotic capabilities with the human-in-the-loop gradually. This middle ground between teleoperation and autonomy can increase end-user acceptance while directly alleviating some of the operator&#39;s robot control burden and maintaining the resilience of the human-in-the-loop.},
  archive   = {C_IROS},
  author    = {Manolis Chiou and Georgios-Theofanis Epsimos and Grigoris Nikolaou and Pantelis Pappas and Giannis Petousakis and Stefan Mühl and Rustam Stolkin},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981881},
  pages     = {4545-4552},
  title     = {Robot-assisted nuclear disaster response: Report and insights from a field exercise},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reachability based trajectory generation combining global
graph search in task space and local optimization in configuration
space. <em>IROS</em>, 4513–4520. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a trajectory planning framework for a robot that exploits a pre-computed database of end-effector trajectories as the guidance of optimization-based inverse kinematics. We constructed a reachable graph of a robot offline, which represents feasible end-effector paths with corresponding configurations. When performing the online trajectory planning, we applied A* search to the reachable graph to find a feasible path between input start and goal globally in the task space. Its cost function has the separated term dependent on the robot, which comes from the manipulability of configurations preserved in the reachable graph, and that is dependent on the environment. Then, we solve optimization-based inverse kinematics to generate an optimal joint trajectory while utilizing the end-effector trajectory and its corresponding configurations as the guidance to avoid local optimum. We evaluated our framework quantitatively by comparing it with existing methods to confirm that it achieved a high success rate and quality of results while suppressing its computational time. We also qualitatively proved its practicality by applying it to the material handling task in the real-world. This result shows that it improved the performance of the optimization-based inverse kinematics avoiding local optimum and applicability to the different environments of the pre-computed motion database.},
  archive   = {C_IROS},
  author    = {Iori Kumagai and Masaki Murooka and Mitsuharu Morisawa and Fumio Kanehiro},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981906},
  pages     = {4513-4520},
  title     = {Reachability based trajectory generation combining global graph search in task space and local optimization in configuration space},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Making robotics swarm flow more smoothly: A regular virtual
tube model. <em>IROS</em>, 4498–4504. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a model of a class of regular virtual tubes that can generate safe, feasible, and smooth space for a robotics swarm in an obstacle-dense environment, especially for a drone swarm based on the flocking model. The regular principles are first proposed, and the regular conditions are then formulated based on the principles. A method to obtain a regular virtual tube is also presented based on trajectory planning and regular conditions. The proposed method&#39;s effectiveness and robustness are comprehensively demonstrated in a simulation environment with random obstacles.},
  archive   = {C_IROS},
  author    = {Pengda Mao and Quan Quan},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981842},
  pages     = {4498-4504},
  title     = {Making robotics swarm flow more smoothly: A regular virtual tube model},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BITKOMO: Combining sampling and optimization for fast
convergence in optimal motion planning. <em>IROS</em>, 4492–4497. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Optimal sampling based motion planning and trajectory optimization are two competing frameworks to generate optimal motion plans. Both frameworks have complementary properties: Sampling based planners are typically slow to converge, but provide optimality guarantees. Trajectory optimizers, however, are typically fast to converge, but do not provide global optimality guarantees in nonconvex problems, e.g. scenarios with obstacles. To achieve the best of both worlds, we introduce a new planner, BITKOMO, which integrates the asymptotically optimal Batch Informed Trees (BIT*) planner with the K-Order Markov Optimization (KOMO) trajectory optimization framework. Our planner is anytime and maintains the same asymptotic optimality guarantees provided by BIT*, while also exploiting the fast convergence of the KOMO trajectory optimizer. We experimentally evaluate our planner on manipulation scenarios that involve high dimensional configuration spaces, with up to two 7-DoF manipulators, obstacles and narrow passages. BITKOMO performs better than KOMO by succeeding even when KOMO fails, and it outperforms BIT* in terms of convergence to the optimal solution.},
  archive   = {C_IROS},
  author    = {Jay Kamat and Joaquim Ortiz-Haro and Marc Toussaint and Florian T. Pokorny and Andreas Orthey},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981732},
  pages     = {4492-4497},
  title     = {BITKOMO: Combining sampling and optimization for fast convergence in optimal motion planning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving the efficiency of sampling-based motion planners
via runtime predictions for motion-planning problems with dynamics.
<em>IROS</em>, 4486–4491. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While sampling-based approaches have made significant progress, motion planning with dynamics still poses significant challenges as the planner has to generate not only collision-free but also dynamically-feasible trajectories that enable the robot to reach its goal. To improve the efficiency of sampling-based motion planners, this paper develops a framework, termed Motion-Planning Runtime Prediction (MPRP), that relies on machine learning to train models to predict the expected runtime of a planner. When solving a new motion-planning problem, the trained model is then incorporated into the motion planner to more effectively guide the search toward parts of the state space that are associated with low expected runtime predictions. This paper applies the MPRP framework to state-of-the-art sampling-based motion planners to obtain new planners, which are shown to be significantly faster.},
  archive   = {C_IROS},
  author    = {Hoang-Dung Bui and Yuanjie Lu and Erion Plaku},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981753},
  pages     = {4486-4491},
  title     = {Improving the efficiency of sampling-based motion planners via runtime predictions for motion-planning problems with dynamics},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Homology-class guided rapidly-exploring random tree for
belief space planning. <em>IROS</em>, 4478–4485. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, an efficient homology guided belief space planning method for obstacle-cluttered environments is presented. The proposed planner follows a two-step approach. First, a h-signature guided rapidly-exploring random tree (HRRT) algorithm is proposed to provide nominal trajecto-ries in different homology classes by constructing homology aware sub-trees in a parallel manner. The HRRT planner is extended to a h-signature guided RRT * algorithm, where an inter-homology-class rewire procedure is proposed, increasing the probability of discovering homology classes in narrow space/passages. The iLQG-based belief space planning algorithm is then employed to find locally optimal trajectories minimizing uncertainties in each homology class.},
  archive   = {C_IROS},
  author    = {Ran Hao and M. Cenk Çavusoğlu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981602},
  pages     = {4478-4485},
  title     = {Homology-class guided rapidly-exploring random tree for belief space planning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Confidence-rich localization and mapping based on particle
filter for robotic exploration. <em>IROS</em>, 4471–4477. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper mainly studies the localization and mapping of range sensing robots in the confidence-rich map (CRM) and then extends it to provide a full state estimate for information-theoretic exploration. Most previous works about active simultaneous localization and mapping and exploration always assumed the known robot poses or utilized inaccurate information metrics to approximate pose uncertainty, resulting in imbalanced exploration performance and efficiency in the unknown environment. This inspires us to extend the confidence-rich mutual information (CRMI) with measurable pose uncertainty. Specifically, we propose a Rao- Blackwellized particle filter-based localization and mapping scheme (RBPF -CLAM) for CRM, then we develop a new closed-form weighting method to improve the localization accuracy without scan matching. We further derive the uncertain CRMI (UCRMI) with the weighted particles by a more accurate approximation. Simulations and experimental evaluations show the localization accuracy and exploration performance of the proposed methods.},
  archive   = {C_IROS},
  author    = {Yang Xu and Ronghao Zheng and Senlin Zhang and Meiqin Liu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981251},
  pages     = {4471-4477},
  title     = {Confidence-rich localization and mapping based on particle filter for robotic exploration},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Global data association for SLAM with 3D grassmannian
manifold objects. <em>IROS</em>, 4463–4470. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Using pole and plane objects in lidar SLAM can increase accuracy and decrease map storage requirements compared to commonly-used point cloud maps. However, place recognition and geometric verification using these landmarks is challenging due to the requirement for global matching without an initial guess. Existing works typically only leverage either pole or plane landmarks, limiting application to a restricted set of environments. We present a global data association method for loop closure in lidar scans using 3D line and plane objects simultaneously and in a unified manner. The main novelty of this paper is in the representation of line and plane objects extracted from Iidar scans on the manifold of affine subspaces, known as the affine Grassmannian. Line and plane correspondences are matched using our graph-based data association framework and subsequently registered in the least-squares sense. Compared to pole-only approaches and plane-only approaches, our 3D affine Grassmannian method yields a 71\% and 325\% increase respectively to loop closure recall at 100\% precision on the KITTI dataset and can provide frame alignment with less than 10 cm and 1 deg of error.},
  archive   = {C_IROS},
  author    = {Parker C. Lusk and Jonathan P. How},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981075},
  pages     = {4463-4470},
  title     = {Global data association for SLAM with 3D grassmannian manifold objects},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A 2D georeferenced map aided visual-inertial system for
precise UAV localization. <em>IROS</em>, 4455–4462. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Precise geolocalization is crucial for unmanned aerial vehicles (UAVs). However, most current deployed UAVs rely on the global navigation satellite systems (GNSS) for geolocalization. In this paper, we propose to use a lightweight visual-inertial system with a 2D georeferenced map to obtain accurate geodetic positions for UAVs. The proposed system firstly integrates a micro inertial measurement unit (MIMU) and a monocular camera to build a visual-inertial odometry (VIO) to consecutively estimate the UAV&#39;s motion states and reconstruct the 3D position of the observed visual features in the local world frame. To obtain the geolocation, the visual features tracked by the odometry are further registered to the 2D georeferenced map. While most conventional methods perform image-level aerial image registration, we propose to align the reconstructed 3D points with the map, and then use the registered 3D points to relocalize the vehicle in the geodetic frame, which helps to improve the geolocalization accuracy. Finally, a pose graph is deployed to fuse the geolocation from the point registration and the local navigation result from the visual-inertial odometry (VIO) to obtain smooth and drift-free geolocalization results. We have validated the proposed method by installing the sensors to a UAV body rigidly and have conducted two real-world flights in different environments with unknown initials. The results show that the proposed method can achieve less than 4m position error in flight at about 100m high and less than 9m position error in flight at about 300m high.},
  archive   = {C_IROS},
  author    = {Mao Jun and Zhang Lilian and He Xiaofeng and Qu Hao and Hu Xiaoping},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982254},
  pages     = {4455-4462},
  title     = {A 2D georeferenced map aided visual-inertial system for precise UAV localization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SO-PFH: Semantic object-based point feature histogram for
global localization in parking lot. <em>IROS</em>, 4431–4438. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Global localization is essential for autonomous mobile systems, especially indoor applications where the GPS signal is denied. Although the appearance-based methods have been successfully applied in various localization tasks, they face various challenges such as light variation, viewpoint changing, and dynamic interference. Additionally, the appearance-based methods usually require a visual feature point map, which increases the storage burden. This paper proposes a novel global localization solution that leverages sparse and repetitive semantic object information. The proposal can fulfill global localization based on object-level maps that are self-built or externally provided. In this solution, the semantic objects are firstly modeled with a point cloud. Then, the object&#39;s semantic information is embedded into the geometry of the corresponding point, and the Semantic Object-based Point Feature Histogram (SO-PFH) descriptors of the modeled point clouds are estimated. Finally, the global localization is executed by applying a Geometric Consistency Filter-based RANdom SAmple Consensus (GCF-RANSAC) method to match point clouds. Experiments and simulations are conducted in indoor parking lots. The results demonstrate the effectiveness of the proposed method.},
  archive   = {C_IROS},
  author    = {Jixin Lv and Chao Meng and Yue Wang and Jie Sun and Rong Xiong and Shiliang Pu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981094},
  pages     = {4431-4438},
  title     = {SO-PFH: Semantic object-based point feature histogram for global localization in parking lot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LF-VIO: A visual-inertial-odometry framework for large
field-of-view cameras with negative plane. <em>IROS</em>, 4423–4430. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual-inertial-odometry has attracted extensive attention in the field of autonomous driving and robotics. The size of Field of View (FoV) plays an important role in Visual-Odometry (VO) and Visual-Inertial-Odometry (VIO), as a large FoV enables to perceive a wide range of surrounding scene elements and features. However, when the field of the camera reaches the negative half plane, one cannot simply use $[u, v, 1]^{T}$ to represent the image feature points anymore. To tackle this issue, we propose LF-VIO, a real-time VIO framework for cameras with extremely large FoV.We leverage a threedimensional vector with unit length to represent feature points, and design a series of algorithms to overcome this challenge. To address the scarcity of panoramic visual odometry datasets with ground-truth location and pose, we present the PALVIO dataset, collected with a Panoramic Annular Lens (PAL) system with an entire FoV of 36 $0^{\circ}\times(40^{\circ}\sim 120^{\circ})$ and an IMU sensor. With a comprehensive variety of experiments, the proposed LF-VIO is verified on both the established PALVIO benchmark and a public fisheye camera dataset with a FoV of $360^{\circ}\times(0^{\circ}\sim 93.5^{\circ})$ . LF-VIO outperforms state-of-the-art visual-inertial-odometry methods. Our dataset and code are made publicly available at https://github.com/flysoaryun/LF-VIO},
  archive   = {C_IROS},
  author    = {Ze Wang and Kailun Yang and Hao Shi and Peng Li and Fei Gao and Kaiwei Wang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981217},
  pages     = {4423-4430},
  title     = {LF-VIO: A visual-inertial-odometry framework for large field-of-view cameras with negative plane},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CFP-SLAM: A real-time visual SLAM based on coarse-to-fine
probability in dynamic environments. <em>IROS</em>, 4399–4406. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The dynamic factors in the environment will lead to the decline of camera localization accuracy due to the violation of the static environment assumption of SLAM algorithm. Recently, some related works generally use the combination of semantic constraints and geometric constraints to deal with dynamic objects, but problems can still be raised, such as poor real-time performance, easy to treat people as rigid bodies, and poor performance in low dynamic scenes. In this paper, a dynamic scene-oriented visual SLAM algorithm based on object detection and coarse-to-fine static probability named CFP-SLAM is proposed. The algorithm combines semantic constraints and geometric constraints to calculate the static probability of objects, keypoints and map points, and takes them as weights to participate in camera pose estimation. Extensive evaluations show that our approach can achieve almost the best results in high dynamic and low dynamic scenarios compared to the state-of-the-art dynamic SLAM methods, and shows quite high real-time ability.},
  archive   = {C_IROS},
  author    = {Xinggang Hu and Yunzhou Zhang and Zhenzhong Cao and Rong Ma and Yanmin Wu and Zhiqiang Deng and Wenkai Sun},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981826},
  pages     = {4399-4406},
  title     = {CFP-SLAM: A real-time visual SLAM based on coarse-to-fine probability in dynamic environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributed riemannian optimization with lazy communication
for collaborative geometric estimation. <em>IROS</em>, 4391–4398. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present the first distributed optimization al-gorithm with lazy communication for collaborative geometric estimation, the backbone of modern collaborative simultaneous localization and mapping (SLAM) and structure-from-motion (SfM) applications. Our method allows agents to cooperatively reconstruct a shared geometric model on a central server by fusing individual observations, but without the need to transmit potentially sensitive information about the agents themselves (such as their locations). Furthermore, to alleviate the burden of communication during iterative optimization, we design a set of communication triggering conditions that enable agents to selectively upload a targeted subset of local information that is useful to global optimization. Our approach thus achieves significant communication reduction with minimal impact on optimization performance. As our main theoretical contribution, we prove that our method converges to first-order critical points with a global sublinear convergence rate. Numerical evaluations on bundle adjustment problems from collaborative SLAM and SfM datasets show that our method performs competitively against existing distributed techniques, while achieving up to 78\% total communication reduction.},
  archive   = {C_IROS},
  author    = {Yulun Tian and Amrit Singh Bedi and Alec Koppel and Miguel Calvo-Fullana and David M. Rosen and Jonathan P. How},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981256},
  pages     = {4391-4398},
  title     = {Distributed riemannian optimization with lazy communication for collaborative geometric estimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Probabilistic data association for semantic SLAM at scale.
<em>IROS</em>, 4359–4364. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With advances in image processing and machine learning, it is now feasible to incorporate semantic information into the problem of simultaneous localisation and mapping (SLAM). Previously, SLAM was carried out using lower level geometric features (points, lines, and planes) which are often view-point dependent and error prone in visually repetitive environments. Semantic information can improve the ability to recognise previously visited locations, as well as maintain sparser maps for long term SLAM applications. However, SLAM in repetitive environments has the critical problem of assigning measurements to the landmarks which generated them. In this paper, we use k-best assignment enumeration to compute marginal assignment probabilities for each measurement landmark pair, in real time. We present numerical studies on the KITTI dataset to demonstrate the effectiveness and speed of the proposed framework.},
  archive   = {C_IROS},
  author    = {Elad Michael and Tyler Summers and Tony A. Wood and Chris Manzie and Iman Shames},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981750},
  pages     = {4359-4364},
  title     = {Probabilistic data association for semantic SLAM at scale},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Closing the loop: Graph networks to unify semantic objects
and visual features for multi-object scenes. <em>IROS</em>, 4352–4358.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In Simultaneous Localization and Mapping (SLAM), Loop Closure Detection (LCD) is essential to minimize drift when recognizing previously visited places. Visual Bag- of-Words (vBoW) has been an LCD algorithm of choice for many state-of-the-art SLAM systems. It uses a set of visual features to provide robust place recognition but fails to perceive the semantics or spatial relationship between feature points. Previous work has mainly focused on addressing these issues by combining vBoW with semantic and spatial information from objects in the scene. However, they are unable to exploit spatial information of local visual features and lack a structure that unifies semantic objects and visual features, therefore limiting the symbiosis between the two components. This paper proposes SymbioLCD2, which creates a unified graph structure to integrate semantic objects and visual features symbiotically. Our novel graph-based LCD system utilizes the unified graph structure by applying a Weisfeiler-Lehman graph kernel with temporal constraints to robustly predict loop closure candidates. Evaluation of the proposed system shows that having a unified graph structure incorporating semantic objects and visual features improves LCD prediction accuracy, illustrating that the proposed graph structure provides a strong symbiosis between these two complementary components. It also outperforms other Machine Learning algorithms - such as SVM, Decision Tree, Random Forest, Neural Network and GNN based Graph Matching Networks. Furthermore, it has shown good performance in detecting loop closure candidates earlier than state-of-the-art SLAM systems, demonstrating that extended semantic and spatial awareness from the unified graph structure significantly impacts LCD performance.},
  archive   = {C_IROS},
  author    = {Jonathan J.Y. Kim and Martin Urschler and Patricia J. Riddle and Jorg S. Wicker},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981542},
  pages     = {4352-4358},
  title     = {Closing the loop: Graph networks to unify semantic objects and visual features for multi-object scenes},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PrePARE: Predictive proprioception for agile failure event
detection in robotic exploration of extreme terrains. <em>IROS</em>,
4338–4343. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Legged robots can traverse a wide variety of terrains, some of which may be challenging for wheeled robots, such as stairs or highly uneven surfaces. However, quadruped robots face stability challenges on slippery surfaces. This can be resolved by adjusting the robot&#39;s locomotion by switching to more conservative and stable locomotion modes, such as crawl mode (where three feet are in contact with the ground always) or amble mode (where one foot touches down at a time) to prevent potential falls. To tackle these challenges, we propose an approach to learn a model from past robot experience for predictive detection of potential failures. Accordingly, we trigger gait switching merely based on proprioceptive sensory information. To learn this predictive model, we propose a semi-supervised process for detecting and annotating ground truth slip events in two stages: We first detect abnormal occurrences in the time series sequences of the gait data using an unsupervised anomaly detector, and then, the anomalies are verified with expert human knowledge in a replay simulation to assert the event of a slip. These annotated slip events are then used as ground truth examples to train an ensemble decision learner for predicting slip probabilities across terrains for traversability. We analyze our model on data recorded by a legged robot on multiple sites with slippery terrain. We demonstrate that a potential slip event can be predicted up to 720 ms ahead of a potential fall with an average precision greater than 0.95 and an average F-score of 0.82. Finally, we validate our approach in real-time by deploying it on a legged robot and switching its gait mode based on slip event detection.},
  archive   = {C_IROS},
  author    = {Sharmita Dey and David Fan and Robin Schmid and Anushri Dixit and Kyohei Otsu and Thomas Touma and Arndt F. Schilling and Ali-Akbar Agha-Mohammadi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981660},
  pages     = {4338-4343},
  title     = {PrePARE: Predictive proprioception for agile failure event detection in robotic exploration of extreme terrains},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RoBiGAN: A bidirectional wasserstein GAN approach for online
robot fault diagnosis via internal anomaly detection. <em>IROS</em>,
4332–4337. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Complex robots in challenging scenarios require constant monitoring of their state and adaptation of their behavior to ensure robustness, reliability and longevity. While known possible errors can be specifically surveilled, other prob-lems can be fully unforeseen, requiring detection systems able to identify novel faults. We detect possible faults as anomalies on various internal sensor data, utilizing unsupervised learning techniques. A bidirectional Wasserstein GAN approach for anomaly detection on multivariate, highly dependent time-series data is implemented and trained on a small amount of non-anomalous robot sensor data. This model is then used for inference on the on-board hardware of a robot without parallel processing units. We evaluate multiple variants of the architecture using manually introduced anomalies in the form of different weights attached to the robot&#39;s legs. Overall we are able to show that RoBiGAN is able to consistently detect and localize small anomalies in an online scenario, with little to no robot specific modeling needed.},
  archive   = {C_IROS},
  author    = {Tristan Schnell and Katrin Bott and Lennart Puck and Timothée Buettner and Arne Roennau and Rüdiger Dillmann},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982240},
  pages     = {4332-4337},
  title     = {RoBiGAN: A bidirectional wasserstein GAN approach for online robot fault diagnosis via internal anomaly detection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Transmissibility-based DAgger for fault classification in
connected autonomous vehicles. <em>IROS</em>, 4318–4323. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fault mitigation in Connected Autonomous Vehicle (CAV) platoons is faster and more reliable if the fault structure is known. In this paper we propose using transmissibility operators, which are relationships that relate a set of velocities with another in the platoon, to classify the faults. Transmissibility operators were shown to be exceptional in signals estimation; however, its also shown to be noncausal and thus can only be used offline. To this end, we propose using Data Aggregation (DAgger), which is an extension in imitation learning to transfer the classification experience from transmissibility operators to a novice machine learning agent to be used online. A heterogeneous CAV platoon was modeled with three different faults separately. These faults are actuator disturbances, false data injection attacks, and communication time delay. The proposed algorithm is then tested on the platoon model and then applied to an experimental setup that consists of three autonomous robots. The overall classification accuracy achieved was 95.8\% for the experiment.},
  archive   = {C_IROS},
  author    = {Abdelrahman Khalil and Mohammad Al Janaideh and Lourdes Peña Castillo and Octavia A. Dobre},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982083},
  pages     = {4318-4323},
  title     = {Transmissibility-based DAgger for fault classification in connected autonomous vehicles},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design, modeling and control for a tilt-rotor VTOL UAV in
the presence of actuator failure. <em>IROS</em>, 4310–4317. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Enabling vertical take-off and landing while pro-viding the ability to fly long ranges opens the door to a wide range of new real-world aircraft applications while improving many existing tasks. Tiltrotor vertical take-off and landing (VTOL) unmanned aerial vehicles (UAVs) are a better choice than fixed-wing and multirotor aircraft for such applications. Prior works on these aircraft have addressed the aerodynamic performance, design, modeling, and control. However, a less explored area is the study of their potential fault tolerance due to their inherent redundancy, which allows them to tol-erate some degree of actuation failure. This paper introduces tolerance to several types of actuator failures in a tiltrotor VTOL aircraft. We discuss the design and modeling of a custom tiltrotor VTOL UAV, which is a combination of a fixed-wing aircraft and a quadrotor with tilting rotors, where the four propellers can be rotated individually. Then, we analyze the feasible wrench space the vehicle can generate and design the dynamic control allocation so that the system can adapt to actuator failures, benefiting from the configuration redundancy. The proposed approach is lightweight and is implemented as an extension to an already-existing flight control stack. Extensive experiments validate that the system can maintain the controlled flight under different actuator failures. To the best of our knowledge, this work is the first study of the tiltrotor VTOL&#39;s fault-tolerance that exploits the configuration redundancy. The source code and simulation can be accessed from https://theairlab.org/vtol.},
  archive   = {C_IROS},
  author    = {Mohammadreza Mousaei and Junyi Geng and Azarakhsh Keipour and Dongwei Bai and Sebastian Scherer},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981806},
  pages     = {4310-4317},
  title     = {Design, modeling and control for a tilt-rotor VTOL UAV in the presence of actuator failure},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning symbolic failure detection for grasping and mobile
manipulation tasks. <em>IROS</em>, 4302–4309. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to detect failure during task execution and to recover from failure is vital for autonomous robots performing tasks in previously unknown environments. In this paper, we present an approach for failure detection during the execution of grasping and mobile manipulation tasks by a humanoid robot. The approach combines multi-modal sensory information consisting of proprioceptive, force and visual information to learn task models from multiple successful task executions, in order to detect failures and to externalize them for humans in an interpretable way. To this end, we define symbolic action predicates based on multi-modal sensory information to allow high-level state estimation based on action-specific decision trees. To allow symbolic failure detection, we then learn task models that are represented as Markov chains. We evaluated the approach in several pick-and-place and mobile manipulation tasks performed by a humanoid robot in a decommissioning and a household scenario. The evaluation shows that the learned task models are capable of detecting failure with an F1-score of 93\%.},
  archive   = {C_IROS},
  author    = {Patrick Hegemann and Tim Zechmeister and Markus Grotz and Kevin Hitzler and Tamim Asfour},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982223},
  pages     = {4302-4309},
  title     = {Learning symbolic failure detection for grasping and mobile manipulation tasks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Model-free unsupervised anomaly detection of a general
robotic system using a stacked LSTM and its application to a fixed-wing
unmanned aerial vehicle. <em>IROS</em>, 4287–4293. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the growing application of various robots in real life, the need for an automatic anomaly detection system for robots is necessary for safety. In this paper, we develop an anomaly detection method using a stacked LSTM that can be applied to any robot controlled by a feedback control. Our method does not need installation of additional sensors. Our method is model-free and unsupervised because it does not require the analytical model of the system and the training data does not require faulty operation conditions. We validate our method on real fixed-wing unmanned aerial vehicle flight data containing control surface failure scenarios. We demonstrate the superiority of the proposed algorithm over existing anomaly detection methods in the literature. Our code is available at https://github.com/superhumangod/Model-free-unsupervised-anomaly-detection.},
  archive   = {C_IROS},
  author    = {Jae-Hyeon Park and Soham Shanbhag and Dong Eui Chang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981950},
  pages     = {4287-4293},
  title     = {Model-free unsupervised anomaly detection of a general robotic system using a stacked LSTM and its application to a fixed-wing unmanned aerial vehicle},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Diaphragm ankle actuation for efficient series elastic
legged robot hopping. <em>IROS</em>, 4279–4286. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The observation of the anatomy of agile animals and their locomotion capabilities emphasizes the importance of fast and lightweight legs and confirms the intrinsic compliance integrated into muscle-tendon units as a major ingredient for energy efficient and robust locomotion. This quality is especially relevant for distal leg segments which are subject to aggressive dynamics. Legged robots are accordingly designed to improve dynamic performance by lightweight mechanisms combined with series elastic actuation systems. However, so far no designs are available that feature all characteristics of a perfect distal legged locomotion actuator such as a lightweight and low-inertia structure, with high mechanical efficiency, no stick and sliding friction, and low mechanical complexity. With this goal in mind, we propose a novel robotic leg which integrates all above features. Specifically, we develop, implement, and characterize a bioinspired robot leg that features a lightweight Series ELastic Diaphragm distal Actuator (SELDA) for active control of foot motion. We conducted experiments to compare two leg configurations, with and without foot actuation, to demonstrate the effectiveness of the proposed solution in agile forward hopping controlled by a central pattern generator. We studied how tuning SELDA&#39;s activation timing can adjust the robot&#39;s hopping height by 11\% and its forward velocity by 14\%, even with comparatively low power injection to the distal joint.},
  archive   = {C_IROS},
  author    = {Marco Bolignari and An Mo and Marco Fontana and Alexander Badri-Spröwitz},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981060},
  pages     = {4279-4286},
  title     = {Diaphragm ankle actuation for efficient series elastic legged robot hopping},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The relationship between incremental changes in orientation
and slip speed estimation using the fingerprint effect. <em>IROS</em>,
4271–4278. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The fingerprint effect, wherein vibrations are produced with frequencies related to the speed of a surface sliding across fingerprint ridges and the period of those ridges, has been studied for use in both slip detection and texture recognition. Here, we use a simple bioinspired sensor with parallel, straight, fingerprint-like ridges and a single ferroelectric ceramic transducer to show that the fingerprint effect is orientation dependent and that, if the orientation is known, it can be used to estimate slip speed. Our results, obtained at sliding speeds of 15 mm/s, 20 mm/s, and 25 mm/s and orientations from 0° – 90°, clearly demonstrate this dependence. Additionally, we use our results to run a simulation, using MATLAB software, of real-time slip speed estimation. The simulation shows that the fingerprint effect can be used for real-time slip-speed estimation.},
  archive   = {C_IROS},
  author    = {Robert Kovenburg and Andrew Slezak and Chase George and Richard Gale and Burak Aksak},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981657},
  pages     = {4271-4278},
  title     = {The relationship between incremental changes in orientation and slip speed estimation using the fingerprint effect},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bioinspired antagonist-agonist artificial muscles for
humanoid eyeball motions. <em>IROS</em>, 4265–4270. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Natural eyeball motions in humanoid robots can contribute to friendly communication, thus improving the human-robot interaction. In this paper, we develop antagonist-agonist artificial muscles for humanoid eyeball motions, by using dielectric elastomer actuators (DEAs). Inspired by human eyeballs, the artificial muscles consist of two pairs of DEA: one pair for the horizontal motion, and the other for the vertical motion. The fabrication time of actuators can be significantly decreased due to their simple structure. The antagonist-agonist actuator outperforms the dielectric elastomer minimum energy structure in terms of actuation displacement and response time. We conduct experiments in a lifesize human face model. The experiments demonstrate the capability of antagonist-agonist artificial muscles to mimic eyeball motions in the horizontal, vertical, and diagonal directions. Future work includes modeling and control of artificial muscles for optimal performance of various humanoid eyeball motions.},
  archive   = {C_IROS},
  author    = {Zhen Luo and Zhipeng Xu and Jisen Li and Jian Zhu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982226},
  pages     = {4265-4270},
  title     = {Bioinspired antagonist-agonist artificial muscles for humanoid eyeball motions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tactile perception for growing robots via discrete curvature
measurements. <em>IROS</em>, 4257–4264. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft, growing robots have the ability to conform to their environment and traverse highly curved paths that would typically prove challenging for other robot designs. As they navigate through these constrained and cluttered environments, there is often significant interaction between the robot and its surroundings. In this work, we propose a method to enable tactile perception for growing robots, which utilizes commercially available, flexible sensors that measure the curvature of the robot shape at multiple locations. Our method consists of both a pouch design to enable seamless integration of the sensors with the material of the growing robot, as well as an algorithm for determining the location of point contacts along the robot body. We validate our proposed approach experimentally using a 3.5 cm robot that can grow to be 53 cm long. We show that we can localize a force applied to various locations along its length with an average error of $3.444\pm1.38$ cm when the robot is unactuated and $4.62\pm 0.95$ cm when the robot is actuated. Additionally, we characterize the minimum distance required for our tactile sensing approach to discriminate between two separate contact points along the robot body to be 23.5 cm. Finally, we apply our method to a growing robot exploring an unknown environment and show that we are able to effectively determine when and where the growing robot collides with an unknown obstacle.},
  archive   = {C_IROS},
  author    = {Micah Bryant and Connor Watson and Tania K. Morimoto},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981273},
  pages     = {4257-4264},
  title     = {Tactile perception for growing robots via discrete curvature measurements},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual confined-space navigation using an efficient learned
bilinear optic flow approximation for insect-scale robots.
<em>IROS</em>, 4250–4256. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual navigation for insect-scale robots is very challenging because in such a small scale, the size, weight, and power (SWaP) constraints do not appear to permit visual navigation techniques such as SLAM (Simultaneous Localization and Mapping) because they are likely to be too power-hungry. We propose to use a biology-inspired approach, which we term the bilinear optic flow approximation, that is more computationally efficient. We build on previous work that has shown that the bilinear approximation can be used for visual servoing. Here, we show that a bilinear approximator can be learned that is able to stabilize the heading of a robot while performing continuous forward motion in a corridor-shaped environment. This is a necessary capability for confined-space navigation that insect-sized robots are likely to perform. In this work, we describe the underlining methodology of the method and built a 2D visual simulation environment and omnidirectional camera model to validate our results.},
  archive   = {C_IROS},
  author    = {Zhitao Yu and Gioele Zardini and Andrea Censi and Sawyer Fuller},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981585},
  pages     = {4250-4256},
  title     = {Visual confined-space navigation using an efficient learned bilinear optic flow approximation for insect-scale robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Imitation behavior of the outer edge of the foot by
humanoids using a simplified contact state representation.
<em>IROS</em>, 4243–4249. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There is a way to utilize humanoid robots to mimic human behavior by taking advantage of their human-like proportions. In general, motion capture is used; in this case, the posture of the body links can be taken. However, this method does not provide detailed information on the contact state, which is important for actions that involve contact with objects. In this study, we focused on the foot, which has not been paid much attention among the parts where contact and manipulation with objects are important, and developed a device to measure the contact pressure distribution at the outer edge of the sole. We proposed an index, SS-COP, which simply reflects the contact on the curved surface of the sole for this device and a robot foot with lateral force sensation and realized a behavior that imitates the foot condition of a humanoid robot by using this index.},
  archive   = {C_IROS},
  author    = {Yoshimoto Ribayashi and Kento Kawaharazuka and Yasunori Toshimitsu and Daiki Kusuyama and Akihiro Miki and Koki Shinjo and Masahiro Baudo and Temma Suzuki and Yuta Kojio and Kei Okada and Masayuki Inaba},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981673},
  pages     = {4243-4249},
  title     = {Imitation behavior of the outer edge of the foot by humanoids using a simplified contact state representation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design optimization of an ultrafast-striking mantis shrimp
microrobot. <em>IROS</em>, 4236–4242. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mantis shrimp produce one of the fastest strikes in the animal kingdom, their striking appendages reaching tip velocities of tens of meters per second underwater. Their ultrafast movement is capable of crushing the shells of prey and generating cavitation bubbles, and has long raised interest from the scientific community. To study the underlying mechanisms and operating principles behind these behaviors, prior research has developed physical models that mimic the motions and speeds of mantis shrimp. That microrobot demonstrated speeds of approximately 5 m/s in water and 26 m/s in air. Here we utilize an accurate dynamical model of the four-bar mechanism and geometric latch observed in biological shrimp in a numerical trajectory optimization approach to find the design changes that can maximize the microrobot&#39;s striking velocities. Through a suboptimization problem maximizing the energy loaded in the mechanism&#39;s spring, we manage to improve the performance of the microrobot by over 58\%, reaching tip velocities of $41.2 \pm 0.6$ m/s.},
  archive   = {C_IROS},
  author    = {Sandra C. Wells and Nak-Seung P. Hyun and Emma Steinhardt and Tran H. Nguyen and Robert J. Wood},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981352},
  pages     = {4236-4242},
  title     = {Design optimization of an ultrafast-striking mantis shrimp microrobot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design and control of a multi-modal soft gripper inspired by
elephant fingers. <em>IROS</em>, 4228–4235. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft grippers have the potential to solve many existing manipulation challenges, particularly in agile industry applications. However, existing soft grippers are often limited in the range of objects they can pick, or by cluttered environments. We present a design inspired by the nose and fingers at the end of an elephant&#39;s trunk, which can pick both by suction and pinching, allowing increased grasping diversity. In addition, we observe an emergent grasping mode, a hybrid of pinching and suction where the cup aperture is morphed online, using embedded soft fingers, to form a seal over challenging objects. An algorithmic grasping strategy, based-on analytical grasping models and primitive objects, is presented. With this, we predict grasping performance and show increased grasping range compared to other soft gripper designs. Finally, the gripper and grasping strategy are successfully applied to grasping more varied everyday objects, demonstrating exploitation of this multi-modal gripping for adaptive grasping.},
  archive   = {C_IROS},
  author    = {Shogo Washio and Kieran Gilday and Fumiya Iida},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982126},
  pages     = {4228-4235},
  title     = {Design and control of a multi-modal soft gripper inspired by elephant fingers},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A flexible calibration algorithm for high-speed bionic
vision system based on galvanometer. <em>IROS</em>, 4222–4227. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traditional gimbal-based bionic eye systems usually use a multi-degree-of-freedom mechanical platform to move the camera freely, which makes the structure complex and bulky. The galvanometer-based reflective bionic eye system uses a galvanometer to replace the traditional mechanical rotation structure, which separates the camera from the gimbal system, greatly simplifying the structure. However, there are currently few methods for calibrating such systems, mostly for object detection and tracking. In this paper, a flexible method for high-precision calibration of a galvanometer-based reflective bionic eye system is proposed. In this method, a planar target is used for the calibration of the bionic eye system. The effectiveness and accuracy of the method are evaluated by the reprojection error of the control voltage and the spatial localization of the binocular system. Experiments show that the error of the control voltage after calibration is less than 0.2\%. At an indoor distance of about 7 m, the RMSE of spatial visual localization is less than 0.3 cm.},
  archive   = {C_IROS},
  author    = {Qing Li and Mengjuan Chen and Qingyi Gu and Idaku Ishii},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981947},
  pages     = {4222-4227},
  title     = {A flexible calibration algorithm for high-speed bionic vision system based on galvanometer},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Physics embedded neural network vehicle model and
applications in risk-aware autonomous driving using latent features.
<em>IROS</em>, 4182–4189. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Non-holonomic vehicle motion has been studied extensively using physics-based models. Common approaches when using these models interpret the wheel/ground interactions using a linear tire model and thus may not fully capture the nonlinear and complex dynamics under various environments. On the other hand, neural network models have been widely employed in this domain, demonstrating powerful function approximation capabilities. However, these black-box learning strategies completely abandon the existing knowledge of well-known physics. In this paper, we seamlessly combine deep learning with a fully differentiable physics model to endow the neural network with available prior knowledge. The proposed model shows better generalization performance than the vanilla neural network model by a large margin. We also show that the latent features of our model can accurately represent lateral tire forces without the need for any additional training. Lastly, We develop a risk-aware model predictive controller using proprioceptive information derived from the latent features. We validate our idea in two autonomous driving tasks under unknown friction, outperforming the baseline control framework.},
  archive   = {C_IROS},
  author    = {Taekyung Kim and Hojin Lee and Wonsuk Lee},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981303},
  pages     = {4182-4189},
  title     = {Physics embedded neural network vehicle model and applications in risk-aware autonomous driving using latent features},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). T3VIP: Transformation-based <span
class="math inline">3D</span> video prediction. <em>IROS</em>,
4174–4181. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For autonomous skill acquisition, robots have to learn about the physical rules governing the 3D world dynamics from their own past experience to predict and reason about plausible future outcomes. To this end, we propose a transformation-based 3D video prediction (T3VIP) approach that explicitly models the 3D motion by decomposing a scene into its object parts and predicting their corresponding rigid transformations. Our model is fully unsupervised, captures the stochastic nature of the real world, and the observational cues in image and point cloud domains constitute its learning signals. To fully leverage all the 2D and 3D observational signals, we equip our model with automatic hyperparameter optimization (HPO) to interpret the best way of learning from them. To the best of our knowledge, our model is the first generative model that provides an RGB-D video prediction of the future for a static camera. Our extensive evaluation with simulated and real-world datasets demonstrates that our formulation leads to interpretable 3D models that predict future depth videos while achieving on-par performance with 2D models on RGB video prediction. Moreover, we demonstrate that our model outperforms 2D baselines on visuomotor control. Videos, code, dataset, and pre-trained models are available at http://t3vip.cs.uni-freiburg.de.},
  archive   = {C_IROS},
  author    = {Iman Nematollahi and Erick Rosete-Beas and Seyed Mahdi B. Azad and Raghu Rajan and Frank Hutter and Wolfram Burgard},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981187},
  pages     = {4174-4181},
  title     = {T3VIP: Transformation-based $3\mathrm{D}$ video prediction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning neuro-symbolic relational transition models for
bilevel planning. <em>IROS</em>, 4166–4173. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In robotic domains, learning and planning are complicated by continuous state spaces, continuous action spaces, and long task horizons. In this work, we address these challenges with Neuro-Symbolic Relational Transition Models (NSRTs), a novel class of models that are data-efficient to learn, compatible with powerful robotic planning methods, and generalizable over objects. NSRTs have both symbolic and neural components, enabling a bilevel planning scheme where symbolic AI planning in an outer loop guides continuous planning with neural models in an inner loop. Experiments in four robotic planning domains show that NSRTs can be learned very data-efficiently, and then used for fast planning in new tasks that require up to 60 actions and involve many more objects than were seen during training.},
  archive   = {C_IROS},
  author    = {Rohan Chitnis and Tom Silver and Joshua B. Tenenbaum and Tomás Lozano-Pérez and Leslie Pack Kaelbling},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981440},
  pages     = {4166-4173},
  title     = {Learning neuro-symbolic relational transition models for bilevel planning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive sampling site selection for robotic exploration in
unknown environments. <em>IROS</em>, 4120–4125. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomously selecting the right sequence of locations to sample is critical during exploration missions in unknown environments, with constraints on the number of samples that can be collected, and a possibility of system failure. A key idea for decision-making in unknown environments is to exploit side information available to the agent, combined with the information gained from samples collected so far, to estimate the sampling values. In this paper, we pose the problem of sampling site selection as a problem of finding the optimal policy in a Markov decision process modeling the unknown sampling values and the outcomes associated with sampling attempts at different locations. Our solution exploits the fact that the partially unknown rewards of this Markov decision process are correlated to each other to devise a strategy that attempts to maximize the total sample value while also ensuring that the agent achieves its minimum mission requirement. We validate the utility of the proposed approach by evaluating the method against a baseline strategy that pursues collecting the samples that are estimated to be of the highest value. Our evaluations use a simulated sampling problem on Martian terrain and using OceanWATERS, a high-fidelity simulator of a future Europa lander mission.},
  archive   = {C_IROS},
  author    = {Pranay Thangeda and Melkior Ornik},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982113},
  pages     = {4120-4125},
  title     = {Adaptive sampling site selection for robotic exploration in unknown environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to grasp on the moon from 3D octree observations
with deep reinforcement learning. <em>IROS</em>, 4112–4119. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Extraterrestrial rovers with a general-purpose robotic arm have many potential applications in lunar and planetary exploration. Introducing autonomy into such systems is desirable for increasing the time that rovers can spend gathering scientific data and collecting samples. This work investigates the applicability of deep reinforcement learning for vision-based robotic grasping of objects on the Moon. A novel simulation environment with procedurally-generated datasets is created to train agents under challenging conditions in unstructured scenes with uneven terrain and harsh illumination. A model-free off-policy actor-critic algorithm is then employed for end-to-end learning of a policy that directly maps compact octree observations to continuous actions in Cartesian space. Experimental evaluation indicates that 3D data representations enable more effective learning of manipulation skills when compared to traditionally used image-based observations. Domain randomization improves the generalization of learned policies to novel scenes with previously unseen objects and different illumination conditions. To this end, we demonstrate zero-shot sim-to-real transfer by evaluating trained agents on a real robot in a Moon-analogue facility. The source code and datasets are available at https://github.com/AndrejOrsula/drl_grasping.},
  archive   = {C_IROS},
  author    = {Andrej Orsula and Simon Bøgh and Miguel Olivares-Mendez and Carol Martinez},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981661},
  pages     = {4112-4119},
  title     = {Learning to grasp on the moon from 3D octree observations with deep reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A sampling based approach to robust planning for a planetary
lander. <em>IROS</em>, 4106–4111. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Planning for autonomous operation in unknown environments poses a number of technical challenges. The agent must ensure robustness to unknown phenomena, un-predictable variation in execution, and uncertain resources, all while maximizing its objective. These challenges are ex-acerbated in the context of space missions where uncertainty is often higher, long communication delays necessitate robust autonomous execution, and severely constrained computational resources limit the scope of planning techniques that can be used. We examine this problem in the context of a Europa Lander concept mission where an autonomous lander must collect valuable data and communicate that data back to Earth. We model the problem as a hierarchical task network, framing it as a utility maximization problem constrained by a strictly monotonically decreasing energy resource. We propose a novel deterministic planning framework that uses periodic replanning and sampling-based optimization to better handle model uncertainty and execution variation, while remaining computationally tractable. We demonstrate the efficacy of our framework through simulations of a Europa Lander concept mission in which our approach outperforms several baselines in utility maximization and robustness.},
  archive   = {C_IROS},
  author    = {Connor Basich and Joseph A. Russino and Steve Chien and Shlomo Zilberstein},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981083},
  pages     = {4106-4111},
  title     = {A sampling based approach to robust planning for a planetary lander},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 3D human pose estimation in weightless environments using a
fisheye camera. <em>IROS</em>, 4100–4105. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Three-dimensional (3D) human pose estimation is one of the most basic tasks for human-interacting robots. Especially in weightless environments such as the International Space Station (ISS), wherein objects may move with a higher degree of freedom compared to on the ground, a camera with a wider field of view (FOV) is crucial in improving the probability of capturing surrounding humans. To this end, we propose a learning-based 3D human pose estimation (3D-HPE) using a fisheye camera targeted at weightless environments. One impediment is that 3D-HPE trained on existing datasets are incapable of addressing the adverse effects of strong fisheye distortion and weightlessness as existing human detection and pose estimation datasets are recorded on the ground using typical rectilinear cameras. To overcome this difficulty, we integrate virtual camera projection to perform a detected human-centered undistortion from fisheye to rectilinear images. We also include upside-down augmentation during training to improve the performance toward weightlessness. Our results show that these two techniques successfully mitigate the adverse effects of weightlessness and fisheye distortion.},
  archive   = {C_IROS},
  author    = {Koji Minoda and Takehisa Yairi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982265},
  pages     = {4100-4105},
  title     = {3D human pose estimation in weightless environments using a fisheye camera},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal and risk-aware path planning considering
localization uncertainty for space exploration rovers. <em>IROS</em>,
4092–4099. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The reliability of autonomous traverses of rovers is critical. It may be jeopardized by the accumulation of errors and the uncertainty propagation of their localization systems. Moreover, space environments are usually harsh, challenging and unpredictable. Teleoperation is complex due to the significant and unavoidable delay. For these reasons, a path planner that provides some level of autonomy with guarantees could increase the success rate of planetary exploration missions. This paper proposes a path planning solution that tackles increasing localization uncertainty and makes a trade-off between the collision risk and the path length. The planner uses the the Fast Marching Method (FMM) to produce a costmap aware of this uncertainty and calculate the optimal path for a level of confidence. This paper additionally presents several simulation and experimental using a wheeled robotic vehicle within a lunar analogue facility.},
  archive   = {C_IROS},
  author    = {J. Ricardo Sánchez-Ibáñez and Pedro J. Sanchez-Cuevas and Miguel Olivares-Mendez},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981179},
  pages     = {4092-4099},
  title     = {Optimal and risk-aware path planning considering localization uncertainty for space exploration rovers},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Trajectory optimization and following for a three degrees of
freedom overactuated floating platform. <em>IROS</em>, 4084–4091. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Space robotics applications, such as Active Space Debris Removal (ASDR), require representative testing before launch. A commonly used approach to emulate the microgravity environment in space is air-bearing based platforms on flat-floors, such as the European Space Agency&#39;s Orbital Robotics and GNC Lab (ORGL). This work proposes a control architecture for a floating platform at the ORGL, equipped with eight solenoid-valve-based thrusters and one reaction wheel. The control architecture consists of two main components: a trajectory planner that finds optimal trajectories connecting two states and a trajectory follower that follows any physically feasible trajectory. The controller is first evaluated within an introduced simulation, achieving a 100\% success rate at finding and following trajectories to the origin within a Monte-Carlo test. Individual trajectories are also successfully followed by the physical system. In this work, we showcase the ability of the controller to reject disturbances and follow a straight-line trajectory within tens of centimeters.},
  archive   = {C_IROS},
  author    = {A. Bredenbeck and S. Vyas and M. Zwick and D. Borrmann and M.A. Olivares-Mendez and A. Nüchter},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981294},
  pages     = {4084-4091},
  title     = {Trajectory optimization and following for a three degrees of freedom overactuated floating platform},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Planning to practice: Efficient online fine-tuning by
composing goals in latent space. <em>IROS</em>, 4076–4083. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {General-purpose robots require diverse repertoires of behaviors to complete challenging tasks in real-world unstructured environments. To address this issue, goal-conditioned reinforcement learning aims to acquire policies that can reach configurable goals for a wide range of tasks on command. However, such goal-conditioned policies are notoriously difficult and time-consuming to train from scratch. In this paper, we propose Planning to Practice (PTP), a method that makes it practical to train goal-conditioned policies for long-horizon tasks that require multiple distinct types of interactions to solve. Our approach is based on two key ideas. First, we decompose the goal-reaching problem hierarchically, with a high-level planner that sets intermediate subgoals using conditional subgoal generators in the latent space for a low-level model-free policy. Second, we propose a hybrid approach which first pre-trains both the conditional subgoal generator and the policy on previously collected data through offline reinforcement learning, and then fine-tunes the policy via online exploration. This fine-tuning process is itself facilitated by the planned subgoals, which breaks down the original target task into short-horizon goal-reaching tasks that are significantly easier to learn. We conduct experiments in both the simulation and real world, in which the policy is pre-trained on demonstrations of short primitive behaviors and fine-tuned for temporally extended tasks that are unseen in the offline data. Our experimental results show that PTP can generate feasible sequences of subgoals that enable the policy to efficiently solve the target tasks. 1 1 Supplementary video: sites.google.com/view/planning-to-practice},
  archive   = {C_IROS},
  author    = {Kuan Fang and Patrick Yin and Ashvin Nair and Sergey Levine},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981999},
  pages     = {4076-4083},
  title     = {Planning to practice: Efficient online fine-tuning by composing goals in latent space},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SafeTAC: Safe tsallis actor-critic reinforcement learning
for safer exploration. <em>IROS</em>, 4070–4075. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Satisfying safety constraints is the top priority in safe reinforcement learning (RL). However, without proper exploration, an overly conservative policy such as freezing at the same position can be generated. To this end, we utilize maximum entropy RL methods for exploration. In particular, an RL method with Tsallis entropy maximization, called Tsallis actor-critic (TAC), is used to synthesize policies which can explore with more promising actions. In this paper, we propose a Tsallis entropy-regularized safe RL method for safer exploration, called SafeTAC. For more expressiveness, we extend the TAC to use a Gaussian mixture model policy, which improves the safety performance. To stabilize the training process, the retrace estimators for safety critics are formulated, and a safe policy update rule using a trust region method is proposed.},
  archive   = {C_IROS},
  author    = {Dohyeong Kim and Jaeseok Heo and Songhwai Oh},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982140},
  pages     = {4070-4075},
  title     = {SafeTAC: Safe tsallis actor-critic reinforcement learning for safer exploration},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PM-FSM: Policies modulating finite state machine for robust
quadrupedal locomotion. <em>IROS</em>, 4063–4069. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep reinforcement learning (deep RL) has emerged as an effective tool for developing controllers for legged robots. However, vanilla deep RL often requires a tremendous amount of training samples and is not feasible for achieving robust behaviors. Instead, researchers have investigated a novel policy architecture by incorporating human experts&#39; knowledge, such as Policies Modulating Trajectory Generators (PMTG). This architecture builds a recurrent control loop by combining a parametric trajectory generator (TG) and a feedback policy network to achieve more robust behaviors. In this work, we propose Policies Modulating Finite State Machine (PM-FSM) by replacing TGs with contact-aware finite state machines (FSM), which offers more flexible control of each leg. This invention offers an explicit notion of contact events to the policy to negotiate unexpected perturbations. We demonstrated that the proposed architecture could achieve more robust behaviors in various scenarios, such as challenging terrains or external perturbations, on both simulated and real robots.},
  archive   = {C_IROS},
  author    = {Ren Liu and Nitish Sontakke and Sehoon Ha},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982259},
  pages     = {4063-4069},
  title     = {PM-FSM: Policies modulating finite state machine for robust quadrupedal locomotion},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online model learning for shape control of deformable linear
objects. <em>IROS</em>, 4056–4062. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traditional approaches to manipulating the state of deformable linear objects (DLOs) - i.e., cables, ropes - rely on model-based planning. However, constructing an accurate dynamic model of a DLO is challenging due to the complexity of interactions and a high number of degrees of freedom. This renders the task of achieving a desired DLO shape particularly difficult and motivates the use of model-free alternatives, which while maintaining generality suffer from a high sample complexity. In this paper, we bridge the gap between these fundamentally different approaches and propose a framework that learns dynamic models of DLOs through trial-and-error interaction. Akin to model-based reinforcement learning (RL), we interleave learning and exploration to solve a 3D shape control task for a DLO. Our approach requires only a fraction of the interaction samples of the current state-of-the-art model-free RL alternatives to achieve superior shape control performance. Unlike offline model learning, our approach does not require expert knowledge for data collection, retains the ability to explore, and automatically selects relevant experience.},
  archive   = {C_IROS},
  author    = {Yuxuan Yang and Johannes A. Stork and Todor Stoyanov},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981080},
  pages     = {4056-4062},
  title     = {Online model learning for shape control of deformable linear objects},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analyzing and overcoming degradation in warm-start
reinforcement learning. <em>IROS</em>, 4048–4055. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement Learning (RL) for robotic applications can benefit from a warm-start where the agent is initialized with a pretrained behavioral policy. However, when transitioning to RL updates, degradation in performance can occur, which may compromise the robot&#39;s safety. This degradation, which constitutes an inability to properly utilize the pretrained policy, is attributed to extrapolation error in the value function, a result of high values being assigned to Out-Of-Distribution actions not present in the behavioral policy&#39;s data. We investigate why the magnitude of degradation varies across policies and why the policy fails to quickly return to behavioral performance. We present visual confirmation of our analysis and draw comparisons to the Offline RL setting which suffers from similar difficulties. We propose a novel method, Confidence Constrained Learning (CCL) for Warm-Start RL, that reduces degradation by balancing between the policy gradient and constrained learning according to a confidence measure of the Q-values. For the constrained learning component we propose a novel objective, Positive Q-value Distance (CCL-PQD). We investigate a variety of constraint-based methods that aim to overcome the degradation, and find they constitute solutions for a multi-objective optimization problem between maximimal performance and miniminal degradation. Our results demonstrate that hyperparameter tuning for CCL-PQD produces solutions on the Pareto Front of this multi-objective problem, allowing the user to balance between performance and tolerable compromises to the robot&#39;s safety.},
  archive   = {C_IROS},
  author    = {Benjamin Wexler and Elad Sarafian and Sarit Kraus},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981286},
  pages     = {4048-4055},
  title     = {Analyzing and overcoming degradation in warm-start reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning visual robotic control efficiently with contrastive
pre-training and data augmentation. <em>IROS</em>, 4040–4047. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in unsupervised representation learning significantly improved the sample efficiency of training Reinforcement Learning policies in simulated environments. However, similar gains have not yet been seen for real-robot reinforcement learning. In this work, we focus on enabling data-efficient real-robot learning from pixels. We present Contrastive Pre-training and Data Augmentation for Efficient Robotic Learning (CoDER), a method that utilizes data augmentation and unsupervised learning to achieve sample-efficient training of real-robot arm policies from sparse rewards. While contrastive pre-training, data augmentation, demonstrations, and reinforcement learning are alone insufficient for efficient learning, our main contribution is showing that the combination of these disparate techniques results in a simple yet data-efficient method. We show that, given only 10 demonstrations, a single robotic arm can learn sparse-reward manipulation policies from pixels, such as reaching, picking, moving, pulling a large object, flipping a switch, and opening a drawer in just 30 minutes of mean real-world training time. We include videos and code on the project website: https://sites.google.com/view/efficient-robotic-manipulation/home.},
  archive   = {C_IROS},
  author    = {Albert Zhan and Ruihan Zhao and Lerrel Pinto and Pieter Abbeel and Michael Laskin},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981055},
  pages     = {4040-4047},
  title     = {Learning visual robotic control efficiently with contrastive pre-training and data augmentation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). L2C2: Locally lipschitz continuous constraint towards stable
and smooth reinforcement learning. <em>IROS</em>, 4032–4039. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a new regularization technique for reinforcement learning (RL) towards making policy and value functions smooth and stable. RL is known for the instability of the learning process and the sensitivity of the acquired policy to noise. Several methods have been proposed to resolve these problems, and in summary, the smoothness of policy and value functions learned mainly in RL contributes to these problems. However, if these functions are extremely smooth, their expressiveness would be lost, resulting in not obtaining the global optimal solution. This paper therefore considers RL under local Lipschitz continuity constraint, so-called L2C2. By designing the spatio-temporal locally compact space for L2C2 from the state transition at each time step, the moderate smoothness can be achieved without loss of expressiveness. Numerical noisy simulations verified that the proposed L2C2 outperforms the task performance while smoothing out the robot action generated from the learned policy.},
  archive   = {C_IROS},
  author    = {Taisuke Kobayashi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981812},
  pages     = {4032-4039},
  title     = {L2C2: Locally lipschitz continuous constraint towards stable and smooth reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning time-optimized path tracking with or without
sensory feedback. <em>IROS</em>, 4024–4031. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a learning-based approach that allows a robot to quickly follow a reference path defined in joint space without exceeding limits on the position, velocity, acceleration and jerk of each robot joint. Contrary to offline methods for time-optimal path parameterization, the reference path can be changed during motion execution. In addition, our approach can utilize sensory feedback, for instance, to follow a reference path with a bipedal robot without losing balance. With our method, the robot is controlled by a neural network that is trained via reinforcement learning using data generated by a physics simulator. From a mathematical perspective, the problem of tracking a reference path in a time-optimized manner is formalized as a Markov decision process. Each state includes a fixed number of waypoints specifying the next part of the reference path. The action space is designed in such a way that all resulting motions comply with the specified kinematic joint limits. The reward function finally reflects the trade-off between the execution time, the deviation from the desired reference path and optional additional objectives like balancing. We evaluate our approach with and without additional objectives and show that time-optimized path tracking can be successfully learned for both industrial and humanoid robots. In addition, we demonstrate that networks trained in simulation can be successfully transferred to a real robot.},
  archive   = {C_IROS},
  author    = {Jonas C. Kiemel and Torsten Kröger},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982053},
  pages     = {4024-4031},
  title     = {Learning time-optimized path tracking with or without sensory feedback},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automated flexible needle trajectory planning for keyhole
neurosurgery using reinforcement learning. <em>IROS</em>, 4018–4023. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Planning a safe trajectory for minimally invasive (keyhole) neurosurgery procedures require avoiding critical anatomical structures such as blood vessels and ventricles while optimizing the needle trajectory parameters such as length and curvature to comply with the needle kinematics. In this paper, we propose a reinforcement learning-based method for obtaining kinematically feasible trajectories for flexible needle insertions. Proposed approach utilizes Bezier curve control points that are generated by a reward-based reinforcement learning framework, called Flexible Needle Path Generation (FNPG). FNPG framework is trained using an environment that consists of (1) critical structures (e.g. ventricles) obtained through atlas based segmentation of MRI-T1 images, (2) blood vessels segmented from MR angiography (MRA) data and, (3) simulated brain tumor with varying size and location. The curvilinear paths obtained through the FNPG framework are compared with the traditional sampling based algorithm RRT * . The results show that the FNPG approach can produce smoother and shorter trajectories compared to RRT * while avoiding the critical anatomical structures.},
  archive   = {C_IROS},
  author    = {Jayesh Kumar and Chinmay Satish Raut and Niravkumar Patel},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981164},
  pages     = {4018-4023},
  title     = {Automated flexible needle trajectory planning for keyhole neurosurgery using reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive gradient-descent extended kalman filter for pose
estimation of mobile robots with sparse reference signals.
<em>IROS</em>, 4010–4017. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a novel extended Kalman filter (EKF) along with its adaptive variant for effective magnetic, angular rate and gravity (MARG) sensor-only pose estimation of mobile robots operated longer periods in reference-denied environments. First, a gradient-descent orientation-based EKF framework is derived, which formulates the MARG-based pose propagation with both bandpass-filtered and bias compensated external acceleration signals. The proposed approach uses two correction signals beside the orientation update, namely, virtual observations and sparse reference signals are incorporated in the state correction. Next, the instantaneous dynamics is characterized by accelerometer/gyroscope signals-based measures and an adaptive strategy is derived for real-time tuning of EKF parameters. The algorithm is fine tuned in an optimization framework on an appropriate database. This database of ground truth and raw MARG measurements contains 16 robot motion scenarios, where both slow motions and agile maneuvers are performed on different terrains. The conducted analysis highlights that the proposed algorithms outperform the standard approaches, moreover, the adaptive strategy further improves the performance by 13\%. The comprehensive performance evaluation demonstrates the efficacy of the new algorithms, thereby these robust approaches are proposed in environments characterized by sparse reference measurements.},
  archive   = {C_IROS},
  author    = {Akos Odry and Istvan Kecskes and Dominik Csik and Hashim A. Hashim and Peter Sarcevic},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981893},
  pages     = {4010-4017},
  title     = {Adaptive gradient-descent extended kalman filter for pose estimation of mobile robots with sparse reference signals},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FAST-LIVO: Fast and tightly-coupled sparse-direct
LiDAR-inertial-visual odometry. <em>IROS</em>, 4003–4009. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To achieve accurate and robust pose estimation in Simultaneous Localization and Mapping (SLAM) task, multisensor fusion is proven to be an effective solution and thus provides great potential in robotic applications. This paper proposes FAST-LIVO, a fast LiDAR-Inertial-Visual Odometry system, which builds on two tightly-coupled and direct odometry subsystems: a VIO subsystem and a LIO subsystem. The LIO subsystem registers raw points (instead of feature points on e.g., edges or planes) of a new scan to an incrementally-built point cloud map. The map points are additionally attached with image patches, which are then used in the VIO subsystem to align a new image by minimizing the direct photometric errors without extracting any visual features (e.g., ORB or FAST corner features). To further improve the VIO robustness and accuracy, a novel outlier rejection method is proposed to reject unstable map points that lie on edges or are occluded in the image view. Experiments on both open data sequences and our customized device data are conducted. The results show our proposed system outperforms other counterparts and can handle challenging environments at reduced computation cost. The system supports both multi-line spinning LiDARs and emerging solid-state LiDARs with completely different scanning patterns, and can run in real-time on both Intel and ARM processors. We open source our code and dataset of this work on Github 2 2 https://github.com/hku-mars/FAST-LIVO to benefit the robotics community.},
  archive   = {C_IROS},
  author    = {Chunran Zheng and Qingyan Zhu and Wei Xu and Xiyuan Liu and Qizhi Guo and Fu Zhang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981107},
  pages     = {4003-4009},
  title     = {FAST-LIVO: Fast and tightly-coupled sparse-direct LiDAR-inertial-visual odometry},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploring mmWave radar and camera fusion for high-resolution
and long-range depth imaging. <em>IROS</em>, 3995–4002. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic geo-fencing and surveillance systems require accurate monitoring of objects if/when they violate perimeter restrictions. In this paper, we seek a solution for depth imaging of such objects of interest at high accuracy (few tens of cm) over extended ranges (up to 300 meters) from a single vantage point, such as a pole mounted platform. Unfortunately, the rich literature in depth imaging using camera, lidar and radar in isolation struggles to meet these tight requirements in real-world conditions. This paper proposes Metamoran, a solution that explores long-range depth imaging of objects of interest by fusing the strengths of two complementary technologies: mmWave radar and camera. Unlike cameras, mmWave radars offer excellent cm-scale depth resolution even at very long ranges. However, their angular resolution is at least 10x worse than camera systems. Fusing these two modalities is natural, but in scenes with high clutter and at long ranges, radar reflections are weak and experience spurious artifacts. Metamoran&#39;s core contribution is to leverage image segmentation and monocular depth estimation on camera images to help declutter radar and discover true object reflections. We perform a detailed evaluation of Metamoran&#39;s depth imaging capabilities in 400 diverse scenarios. Our evaluation shows that Metamoran estimates the depth of static objects up to 90 m away and moving objects up to 305 m away and with a median error of 28 cm, an improvement of 13 x over a naive radar+camera baseline and 23 x compared to monocular depth estimation.},
  archive   = {C_IROS},
  author    = {Akarsh Prabhakara and Diana Zhang and Chao Li and Sirajum Munir and Aswin C. Sankaranarayanan and Anthony Rowe and Swarun Kumar},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982080},
  pages     = {3995-4002},
  title     = {Exploring mmWave radar and camera fusion for high-resolution and long-range depth imaging},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual-tactile multimodality for following deformable linear
objects using reinforcement learning. <em>IROS</em>, 3987–3994. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Manipulation of deformable objects is a challenging task for a robot. It would be problematic to use a single sensory input to track the behaviour of such objects: vision can be subjected to occlusions, whereas tactile inputs cannot capture the global information that is useful for the task. In this paper, we study the problem of using vision and tactile inputs together to complete the task of following deformable linear objects, for the first time. We create a Reinforcement Learning agent using different sensing modalities and investigate how its behaviour can be boosted using visual-tactile fusion, compared to using a single sensing modality. To this end, we developed a benchmark in simulation for manipulating the deformable linear objects using multimodal sensing inputs. The policy of the agent uses distilled information, e.g., the pose of the object in both visual and tactile perspectives, instead of the raw sensing signals, so that it can be directly transferred to real environments. In this way, we disentangle the perception system and the learned control policy. Our extensive experiments show that the use of both vision and tactile inputs, together with proprioception, allows the agent to complete the task in up to 92\% of cases, compared to 77\% when only one of the signals is given. Our results can provide valuable insights for the future design of tactile sensors and for deformable objects manipulation. Code and videos can be found at: https://github.com/lpecyna/SoftSlidingGym.},
  archive   = {C_IROS},
  author    = {Leszek Pecyna and Siyuan Dong and Shan Luo},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982218},
  pages     = {3987-3994},
  title     = {Visual-tactile multimodality for following deformable linear objects using reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Shape estimation of concentric tube robots using single
point position measurement. <em>IROS</em>, 3972–3978. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate shape estimation of concentric tube robots (CTRs) using mathematical models remains a challenge, reinforcing the need to develop techniques for accurate and real-time shape sensing of CTRs. In this paper, we develop a fusion algorithm that predicts the robot&#39;s shape by combining a mathematical model of the CTR with a measurement of the Cartesian coordinates of the robot&#39;s tip using an electro-magnetic sensor. We experimentally validated our method in static and dynamic scenarios with and without external loading. Results demonstrated that the fusion algorithm improves the error of model-based shape prediction by an average of 44.3\%, corresponding to 2.43\% of the robot&#39;s arc length. Furthermore, we demonstrate that our method can be used in real-time to simultaneously track the robot&#39;s tip position and predict its shape.},
  archive   = {C_IROS},
  author    = {Emile Mackute and Balint Thamo and Kevin Dhaliwal and Mohsen Khadem},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982174},
  pages     = {3972-3978},
  title     = {Shape estimation of concentric tube robots using single point position measurement},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VAST: Visual and spectral terrain classification in
unstructured multi-class environments. <em>IROS</em>, 3956–3963. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Terrain classification is a challenging task for robots operating in unstructured environments. Existing classification methods make simplifying assumptions, such as a reduced number of classes, clearly segmentable roads, or good lighting conditions, and focus primarily on one sensor type. These assumptions do not translate well to off-road vehicles, which operate in varying terrain conditions. To provide mobile robots with the capability to identify the terrain being traversed and avoid undesirable surface types, we propose a multimodal sensor suite capable of classifying different terrains. We capture high resolution macro images of surface texture, spectral reflectance curves, and localization data from a 9 degrees of freedom (DOF) inertial measurement unit (IMU) on 11 different terrains at different times of day. Using this dataset, we train individual neural networks on each of the modalities, and then combine their outputs in a fusion network. The fused network achieved an accuracy of 99.98\% percent on the test set, exceeding the results of the best individual network component by 0.98\%. We conclude that a combination of visual, spectral, and IMU data provides meaningful improvement over state of the art in terrain classification approaches. The data created for this research is available at https://github.com/RIVeR-Lab/vast_data.},
  archive   = {C_IROS},
  author    = {Nathaniel Hanson and Michael Shaham and Deniz Erdoğmuş and Taşkin Padir},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982078},
  pages     = {3956-3963},
  title     = {VAST: Visual and spectral terrain classification in unstructured multi-class environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust real-time LiDAR-inertial initialization.
<em>IROS</em>, 3948–3955. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For most LiDAR-inertial odometry, accurate initial states, including temporal offset and extrinsic transfor-mation between LiDAR and 6-axis IMUs, play a significant role and are often considered as prerequisites. However, such information may not be always available in customized LiDAR-inertial systems. In this paper, we propose LI-Init: a full and real-time LiDAR-inertial system initialization process that calibrates the temporal offset and extrinsic parameter between LiDARs and IMUs, and also the gravity vector and IMU bias by aligning the state estimated from LiDAR measurements with that measured by IMU. We implement the proposed method as an initialization module, which can automatically detects the degree of excitation of the collected data and calibrate, on-the-fly, the temporal offset, extrinsic, gravity vector, and IMU bias, which are then used as high-quality initial state values for real-time LiDAR-inertial odometry systems. Experiments conducted with different types of LiDARs and LiDAR-inertial combinations show the robustness, adaptability and efficiency of our initialization method. The implementation of our LiDAR-inertial initialization procedure LI-Init and test data are open-sourced on Github 1 1 https://www.github.com/hku-mars/LiDAR IMU Init and also integrated into a state-of-the-art LiDAR-inertial odometry system FAST-LIO2.},
  archive   = {C_IROS},
  author    = {Fangcheng Zhu and Yunfan Ren and Fu Zhang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982225},
  pages     = {3948-3955},
  title     = {Robust real-time LiDAR-inertial initialization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning an interpretable model for driver behavior
prediction with inductive biases. <em>IROS</em>, 3940–3947. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To plan safe maneuvers and act with foresight, autonomous vehicles must be capable of accurately predicting the uncertain future. In the context of autonomous driving, deep neural networks have been successfully applied to learning pre-dictive models of human driving behavior from data. However, the predictions suffer from cascading errors, resulting in large inaccuracies over long time horizons. Furthermore, the learned models are black boxes, and thus it is often unclear how they arrive at their predictions. In contrast, rule-based models-which are informed by human experts-maintain long-term coherence in their predictions and are human-interpretable. However, such models often lack the sufficient expressiveness needed to capture complex real-world dynamics. In this work, we begin to close this gap by embedding the Intelligent Driver Model, a popular hand-crafted driver model, into deep neural networks. Our model&#39;s transparency can offer considerable advantages, e.g., in debugging the model and more easily interpreting its predictions. We evaluate our approach on a simulated merging scenario, showing that it yields a robust model that is end-to-end trainable and provides greater transparency at no cost to the model&#39;s predictive accuracy.},
  archive   = {C_IROS},
  author    = {Salar Arbabi and Davide Tavernini and Saber Fallah and Richard Bowden},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981142},
  pages     = {3940-3947},
  title     = {Learning an interpretable model for driver behavior prediction with inductive biases},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Metabolic efficiency improvement of human walking by
shoulder stress reduction through load transfer backpack. <em>IROS</em>,
3934–3939. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The dynamic load attached to the load gravity imposes an excessive burden to human shoulders during load carriage, resulting in possible muscle injuries and additional physical exertion. This paper proposes an active suspension backpack, capable of transferring partial load from human shoulders to pelvis and alleviating the dynamic load through separated panels and motor actuation, to reduce pressure on human shoulders and improve walking metabolic efficiency. Based on the human body motion in the vertical direction, the dynamical model of the human-backpack system with shoulder interaction force measured by a soft ballonet with an embedded air pressure sensor is introduced, and an impedance controller has been implemented to maintain a relatively small and constant pressure on the shoulder. In an experimental case study, we presents preliminary results of three healthy subjects performing a treadmill walking with a 20kg load in ACTIVE configuration where the shoulder pressure shows a decrease by 30\% along with a reduction of the metabolic energy consumption by 16.4\%, compared with the load LOCKED case.},
  archive   = {C_IROS},
  author    = {Yu Cao and Jian Huang and Xiaolong Li and Mengshi Zhang and Caihua Xiong and Samer Mohammed and Yaonan Zhu and Yasuhisa Hasegawa},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981129},
  pages     = {3934-3939},
  title     = {Metabolic efficiency improvement of human walking by shoulder stress reduction through load transfer backpack},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A-RIFT: Visual substitution of force feedback for a
zero-cost interface in telemanipulation. <em>IROS</em>, 3926–3933. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an accessible robot interface for telemanipulation (A-RIFT), which preserves the haptic channel partially in a zero-additional-cost interface by visual substitution of force feedback (VSFF). This work explores a gap in the literature, resulting from the focus on performance improvements in telerobotics at increasing interface costs. Unlike most telemanipulation interfaces for high-degree-of-freedom robotic systems, this one requires minimal training and can be run in a web browser under high latency conditions, using an Internet connected computer with the user&#39;s own mouse and keyboard. To evaluate the performance of the system, we ran a controlled user study (N=12) to test how different distances (local vs. remote) and VSFF (on vs. off) affect the system&#39;s usability. As expected, participants in remote conditions performed worse than those in closer proximity. Despite several participants claiming that the visual display of force feedback did not help them, our analysis of their task performance showed that operators in remote condition actually performed statistically significantly better with the visual force feedback display than without it. These results indicate a promising new interface design direction for low-cost telemanipulation.},
  archive   = {C_IROS},
  author    = {Alexander Moortgat-Pick and Peter So and Michael J Sack and Emma G Cunningham and Benjamin P Hughes and Anna Adamczyk and Andriy Sarabakha and Leila Takayama and Sami Haddadin},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981365},
  pages     = {3926-3933},
  title     = {A-RIFT: Visual substitution of force feedback for a zero-cost interface in telemanipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Affective behavior learning for social robot haru with
implicit evaluative feedback. <em>IROS</em>, 3881–3888. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a human-in-the-loop reinforcement learning mechanism to help robots learn emotional behavior. Unlike the previous methods of providing explicit feedback via pressing keyboard buttons or mouse clicks, we provide a more natural way for ordinary people to train social robots how to perform social tasks according to their preferences - facial expressions. The whole experiment is carried out on the desktop robot Haru, which is mainly used for the research of emotion and empathy participation. Our experimental results show that through learning from implicit feedback of facial features, Haru can quickly understand and dynamically adapt to individual preferences, and obtain a similar performance to learning from explicit feedback. In addition, we observe that the recognition error of human feedback will cause a “temporary regress” of the robot&#39;s learning performance, which is more obvious at the beginning of the training process. This phenomenon is shown to be correlated with the accuracy of recognizing negative implicit feedback.},
  archive   = {C_IROS},
  author    = {Hui Wang and Jinying Lin and Zhen Ma and Yurii Vasylkiv and Heike Brock and Keisuke Nakamura and Randy Gomez and Bo He and Guangliang Li},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981752},
  pages     = {3881-3888},
  title     = {Affective behavior learning for social robot haru with implicit evaluative feedback},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). STheReO: Stereo thermal dataset for research in odometry and
mapping. <em>IROS</em>, 3857–3864. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a stereo thermal camera dataset (STheReO) with multiple navigation sensors to encourage thermal SLAM researches. A thermal camera measures infrared rays beyond the visible spectrum therefore it could provide a simple yet robust solution to visually degraded environments where existing visual sensor-based SLAM would fail. Existing thermal camera datasets mostly focused on monocular configuration using the thermal camera with RGB cameras in a visually challenging environment. A few stereo thermal rig were examined but in computer vision perspective without supporting sequential images for state estimation algorithms. To encourage the academia for the evolving stereo thermal SLAM, we obtain nine sequences in total across three spatial locations and three different times per location (e.g., morning, day, and night) to capture the variety of thermal characteristics. By using the STheReO dataset, we hope diverse types of researches will be made, including but not limited to odometry, mapping, and SLAM (e.g., thermal-LiDAR mapping or long-term thermal localization). Our datasets are available at https://sites.google.com/view/rpmsthereo/.},
  archive   = {C_IROS},
  author    = {Seungsang Yun and Minwoo Jung and Jeongyun Kim and Sangwoo Jung and Younghun Cho and Myung-Hwan Jeon and Giseop Kim and Ayoung Kim},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981857},
  pages     = {3857-3864},
  title     = {STheReO: Stereo thermal dataset for research in odometry and mapping},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FusionPortable: A multi-sensor campus-scene dataset for
evaluation of localization and mapping accuracy on diverse platforms.
<em>IROS</em>, 3851–3856. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Combining multiple sensors enables a robot to maximize its perceptual awareness of environments and enhance its robustness to external disturbance, crucial to robotic navigation. This paper proposes the FusionPortable benchmark, a complete multi-sensor dataset with a diverse set of sequences for mobile robots. This paper presents three contributions. We first advance a portable and versatile multi-sensor suite that offers rich sensory measurements: 10Hz LiDAR point clouds, 20Hz stereo frame images, high-rate and asynchronous events from stereo event cameras, 200Hz inertial readings from an IMU, and 10Hz GPS signal. Sensors are already temporally synchronized in hardware. This device is lightweight, self-contained, and has plug-and-play support for mobile robots. Second, we construct a dataset by collecting 17 sequences that cover a variety of environments on the campus by exploiting multiple robot platforms for data collection. Some sequences are challenging to existing SLAM algorithms. Third, we provide ground truth for the decouple localization and mapping performance evaluation. We additionally evaluate state-of-the-art SLAM approaches and identify their limitations. The dataset, consisting of raw sensor measurements, ground truth, calibration data, and evaluated algorithms, will be released.},
  archive   = {C_IROS},
  author    = {Jianhao Jiao and Hexiang Wei and Tianshuai Hu and Xiangcheng Hu and Yilong Zhu and Zhijian He and Jin Wu and Jingwen Yu and Xupeng Xie and Huaiyang Huang and Ruoyu Geng and Lujia Wang and Ming Liu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982119},
  pages     = {3851-3856},
  title     = {FusionPortable: A multi-sensor campus-scene dataset for evaluation of localization and mapping accuracy on diverse platforms},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). OdomBeyondVision: An indoor multi-modal multi-platform
odometry dataset beyond the visible spectrum. <em>IROS</em>, 3845–3850.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a multimodal indoor odometry dataset, OdomBeyondVision, featuring multiple sensors across the different spectrum and collected with different mobile platforms. Not only does OdomBeyondVision contain the traditional navigation sensors, sensors such as IMUs, mechanical LiDAR, RGBD camera, it also includes several emerging sensors such as the single-chip mmWave radar, LWIR thermal camera and solid-state LiDAR. With the above sensors on UAV, UGV and handheld platforms, we respectively recorded the multimodal odometry data and their movement trajectories in various indoor scenes and different illumination conditions. We release the exemplar radar, radar-inertial and thermal-inertial odometry implementations to demonstrate their results for future works to compare against and improve upon. The full dataset including toolkit and documentation is publicly available at: https://github.com/MAPS-Lab/OdomBeyondVision.},
  archive   = {C_IROS},
  author    = {Peize Li and Kaiwen Cai and Muhamad Risqi U. Saputra and Zhuangzhuang Dai and Chris Xiaoxuan Lu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981865},
  pages     = {3845-3850},
  title     = {OdomBeyondVision: An indoor multi-modal multi-platform odometry dataset beyond the visible spectrum},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-modal lidar dataset for benchmarking general-purpose
localization and mapping algorithms. <em>IROS</em>, 3837–3844. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Lidar technology has evolved significantly over the last decade, with higher resolution, better accuracy, and lower cost devices available today. In addition, new scanning modalities and novel sensor technologies have emerged in recent years. Public datasets have enabled benchmarking of algorithms and have set standards for the cutting edge technology. However, existing datasets are not representative of the technological landscape, with only a reduced number of lidars available. This inherently limits the development and comparison of general-purpose algorithms in the evolving landscape. This paper presents a novel multi-modal lidar dataset with sensors showcasing different scanning modalities (spinning and solid-state), sensing technologies, and lidar cameras. The focus of the dataset is on low-drift odometry, with ground truth data available in both indoors and outdoors environment with sub-millimeter accuracy from a motion capture (MOCAP) system. For comparison in longer distances, we also include data recorded in larger spaces indoors and outdoors. The dataset contains point cloud data from spinning lidars and solid-state lidars. Also, it provides range images from high resolution spinning lidars, RGB and depth images from a lidar camera, and inertial data from built-in IMUs. This is, to the best of our knowledge, the lidar dataset with the most variety of sensors and environments where ground truth data is available. This dataset can be widely used in multiple research areas, such as 3D LiDAR simultaneous localization and mapping (SLAM), performance comparison between multi-modal lidars, appearance recognition and loop closure detection. The datasets are available at: https://github.com/TIERS/tiers-lidars-dataset.},
  archive   = {C_IROS},
  author    = {Li Qingqing and Yu Xianjia and Jorge Peña Queralta and Tomi Westerlund},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981078},
  pages     = {3837-3844},
  title     = {Multi-modal lidar dataset for benchmarking general-purpose localization and mapping algorithms},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BEV-SLAM: Building a globally-consistent world map using
monocular vision. <em>IROS</em>, 3830–3836. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to produce large-scale maps for nav-igation, path planning and other tasks is a crucial step for autonomous agents, but has always been challenging. In this work, we introduce BEV-SLAM, a novel type of graph-based SLAM that aligns semantically-segmented Bird&#39;s Eye View (BEV) predictions from monocular cameras. We introduce a novel form of occlusion reasoning into BEV estimation and demonstrate its importance to aid spatial aggregation of BEV predictions. The result is a versatile SLAM system that can operate across arbitrary multi-camera configurations and can be seamlessly integrated with other sensors. We show that the use of multiple cameras significantly increases performance, and achieves lower relative error than high-performance GPS. The resulting system is able to create large, dense, globally-consistent world maps from monocular cameras mounted around an ego vehicle. The maps are metric and correctly-scaled, making them suitable for downstream navigation tasks.},
  archive   = {C_IROS},
  author    = {James Ross and Oscar Mendez and Avishkar Saha and Mark Johnson and Richard Bowden},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981258},
  pages     = {3830-3836},
  title     = {BEV-SLAM: Building a globally-consistent world map using monocular vision},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). S-MKI: Incremental dense semantic occupancy reconstruction
through multi-entropy kernel inference. <em>IROS</em>, 3824–3829. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous robots are often required to acquire high-level prior knowledge by continuously reconstructing the semantics and geometry of the surrounding scene, which is the basis of exploration and planning. Most existing continuous semantic mapping algorithms cannot distinguish potential differences in voxels, resulting in an over-inflated map. Furthermore, fixed-size query ranges introduce high computational complexity. Based on the limitation of over-inflation and inefficiency, this paper proposes a novel incremental continuous semantic occupancy mapping algorithm (S-MKI). The key innovation of this work comes from the two models in the preprocessing stage. On the one hand, Redundant Voxel Filter Model utilizes context entropy to filter out redundant voxels to improve the confidence of the final map, where objects have accurate boundaries with sharp edges. On the other hand, Adaptive Kernel Length Model adaptively adjusts the kernel length with class entropy, which reduces the inherent amount of training data. The final multientropy kernel inference function is formulated to integrate these two models to infer sparse noisy sensor data into dense accurate 3D maps. Experimental results conducted in both indoors and outdoors datasets validate that S-MKI outperforms existing methods.},
  archive   = {C_IROS},
  author    = {Yinan Deng and Meiling Wang and Danwei Wang and Yufeng Yue},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982101},
  pages     = {3824-3829},
  title     = {S-MKI: Incremental dense semantic occupancy reconstruction through multi-entropy kernel inference},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical road topology learning for urban mapless
driving. <em>IROS</em>, 3816–3823. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The majority of current approaches in autonomous driving rely on High-Definition (HD) maps which detail the road geometry and surrounding area. Yet, this reliance is one of the obstacles to mass deployment of autonomous vehicles due to poor scalability of such prior maps. In this paper, we tackle the problem of online road map extraction via leveraging the sensory system aboard the vehicle itself. To this end, we design a structured model where a graph representation of the road network is generated in a hierarchical fashion within a fully convolutional network. The method is able to handle complex road topology and does not require a user in the loop.},
  archive   = {C_IROS},
  author    = {Li Zhang and Faezeh Tafazzoli and Gunther Krehl and Runsheng Xu and Timo Rehfeld and Manuel Schier and Arunava Seal},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981820},
  pages     = {3816-3823},
  title     = {Hierarchical road topology learning for urban mapless driving},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ECDT: Event clustering for simultaneous feature detection
and tracking. <em>IROS</em>, 3808–3815. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Contrary to other standard cameras, event cam-eras interpret the world in an entirely different manner; as a collection of asynchronous events. Despite event camera&#39;s unique data output, many event feature detection and tracking algorithms have shown significant progress by making detours to frame-based data representations. This paper questions the need to do so and proposes a novel event data-friendly method that achieve simultaneous feature detection and tracking, called event Clustering-based Detection and Tracking (eCDT). Our method employs a novel clustering method, named as k-NN Classifier-based Spatial Clustering and Applications with Noise (KCSCAN), to cluster adjacent polarity events to retrieve event trajectories. With the aid of a Head and Tail Descriptor Matching process, event clusters that reappear in a different polarity are continually tracked, elongating the feature tracks. Thanks to our clustering approach in spatio-temporal space, our method automatically solves feature detection and feature tracking simultaneously. Also, eCDT can extract feature tracks at any frequency with an adjustable time window, which does not corrupt the high temporal resolution of the original event data. Our method achieves 30\% better feature tracking ages compared with the state-of-the-art approach while also having a low error approximately equal to it.},
  archive   = {C_IROS},
  author    = {Sumin Hu and Yeeun Kim and Hyungtae Lim and Alex Junho Lee and Hyun Myung},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981451},
  pages     = {3808-3815},
  title     = {ECDT: Event clustering for simultaneous feature detection and tracking},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Category-independent articulated object tracking with factor
graphs. <em>IROS</em>, 3800–3807. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots deployed in human-centric environments may need to manipulate a diverse range of articulated objects, such as doors, dishwashers, and cabinets. Articulated objects often come with unexpected articulation mechanisms that are inconsistent with categorical priors: for example, a drawer might rotate about a hinge joint instead of sliding open. We propose a category-independent framework for predicting the articulation models of unknown objects from sequences of RGB-D images. The prediction is performed by a two-step process: first, a visual perception module tracks object part poses from raw images, and second, a factor graph takes these poses and infers the articulation model including the current configuration between the parts as a 6D twist. We also propose a manipulation-oriented metric to evaluate predicted joint twists in terms of how well a compliant robot controller would be able to manipulate the articulated object given the predicted twist. We demonstrate that our visual perception and factor graph modules outperform baselines on simulated data and show the applicability of our factor graph on real world data.},
  archive   = {C_IROS},
  author    = {Nick Heppert and Toki Migimatsu and Brent Yi and Claire Chen and Jeannette Bohg},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982029},
  pages     = {3800-3807},
  title     = {Category-independent articulated object tracking with factor graphs},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Smart visual beacons with asynchronous optical
communications using event cameras. <em>IROS</em>, 3793–3799. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Event cameras are bio-inspired dynamic vision sensors that respond to changes in image intensity with a high temporal resolution, high dynamic range and low latency. These sensor characteristics are ideally suited to enable visual target tracking in concert with a broadcast visual communication channel for smart visual beacons with applications in distributed robotics. Visual beacons can be constructed by high-frequency modulation of Light Emitting Diodes (LEDs) such as vehicle headlights, Internet of Things (IoT) LEDs, smart building lights, etc., that are already present in many real-world scenarios. The high temporal resolution characteristic of the event cameras allows them to capture visual signals at far higher data rates compared to classical frame-based cameras. In this paper, we propose a novel smart visual beacon architecture with both LED modulation and event camera demodulation algorithms. We quantitatively evaluate the relationship between LED transmission rate, communication distance and the message transmission accuracy for the smart visual beacon communication system that we prototyped. The proposed method achieves up to 4 kbps in an indoor environment and lossless transmission over a distance of 100 meters, at a transmission rate of 500 bps, in full sunlight, demonstrating the potential of the technology in an outdoor environment.},
  archive   = {C_IROS},
  author    = {Ziwei Wang and Yonhon Ng and Jack Henderson and Robert Mahony},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982016},
  pages     = {3793-3799},
  title     = {Smart visual beacons with asynchronous optical communications using event cameras},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scene-level tracking and reconstruction without object
priors. <em>IROS</em>, 3785–3792. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present the first real-time system capable of tracking and reconstructing, individually, every visible object in a given scene, without any form of prior on the rigidness of the objects, texture existence, or object category. In contrast with previous methods such as Co-Fusion and MaskFusion that first segment the scene into individual objects and then process each object independently, the proposed method dynamically segments the non-rigid scene as part of the tracking and reconstruction process. When new measurements indicate topology change, reconstructed models are updated in real-time to reflect that change. Our proposed system can provide the live geometry and deformation of all visible objects in a novel scene in real-time, which makes it possible to be integrated seamlessly into numerous existing robotics applications that rely on object models for grasping and manipulation. The capabilities of the proposed system are demonstrated in challenging scenes that contain multiple rigid and non-rigid objects. Supplementary material, including video, can be found at https://github.com/changhaonan/STAR-no-prior.},
  archive   = {C_IROS},
  author    = {Haonan Chang and Abdeslam Boularias},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982159},
  pages     = {3785-3792},
  title     = {Scene-level tracking and reconstruction without object priors},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DirectTracker: 3D multi-object tracking using direct image
alignment and photometric bundle adjustment. <em>IROS</em>, 3777–3784.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Direct methods have shown excellent performance in the applications of visual odometry and SLAM. In this work we propose to leverage their effectiveness for the task of 3D multi-object tracking. To this end, we propose DirectTracker, a framework that effectively combines direct image alignment for the short-term tracking and sliding-window photometric bundle adjustment for 3D object detection. Object proposals are estimated based on the sparse sliding-window pointcloud and further refined using an optimization-based cost function that carefully combines 3D and 2D cues to ensure consistency in image and world space. We propose to evaluate 3D tracking using the recently introduced higher-order tracking accuracy (HOTA) metric and the generalized intersection over union sim-ilarity measure to mitigate the limitations of the conventional use of intersection over union for the evaluation of vision-based trackers. We perform evaluation on the KITTI Tracking benchmark for the Car class and show competitive performance in tracking objects both in 2D and 3D.},
  archive   = {C_IROS},
  author    = {Mariia Gladkova and Nikita Korobov and Nikolaus Demmel and Aljoša Ošep and Laura Leal-Taixé and Daniel Cremers},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981260},
  pages     = {3777-3784},
  title     = {DirectTracker: 3D multi-object tracking using direct image alignment and photometric bundle adjustment},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving 3D markerless pose estimation of animals in the
wild using low-cost cameras. <em>IROS</em>, 3770–3776. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tracking the 3D motion of agile animals in the wild will enable new insight into the design of robotic controllers. However, in-field 3D pose estimation of high-speed wildlife such as cheetahs is still a challenge [1]. In this work, we aim to solve two of these challenges: unnatural pose estimates during highly occluded sequences and synchronization error between multi-view data. We expand on our previous Full Trajectory Estimation (FTE) method with two significant additions: Pairwise FTE (PW-FTE) and Shutter-delay FTE (SD-FTE). The PW-FTE expands on image-dependent pairwise terms, produced by a convolutional neural network (CNN), to infer occluded 2D keypoints, while SD-FTE uses shutter delay estimation to correct the synchronization error. Lastly, we combine both methods into PW-SD-FTE and perform a quantitative and qualitative analysis on a subset of AcinoSet, the video dataset of rapid and agile motions of cheetahs. We found that SD-FTE has significant benefits in tracking the position of the cheetah in the world frame, while PW-FTE provided a more robust 3D pose estimate during events of high occlusion. The PW-SD-FTE was found to retain both advantages, resulting in an improved baseline for AcinoSet. Code and data can be found at https://github.com/African-Robotics-Unit/AcinoSet/tree/pw_sd_fte.},
  archive   = {C_IROS},
  author    = {Naoya Muramatsu and Zico da Silva and Daniel Joska and Fred Nicolls and Amir Patel},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981746},
  pages     = {3770-3776},
  title     = {Improving 3D markerless pose estimation of animals in the wild using low-cost cameras},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). An efficient and accurate solution to camera pose
estimation problem from point and line correspondences based on null
space analysis. <em>IROS</em>, 3762–3769. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose an accurate and simultaneously efficient solution to perspective-n-point-and-line (PnPL) problem by null space analysis. Although many PnPL-like methods have been proposed, it is hard to obtain the optimal solution considering both calculation efficiency and accuracy at the same time. Based on the remarkable EOPnP method, the proposed algorithm integrates linear-expressed line constraints with original point constraints, leading to a new solution named EOPnPL. For line error measurement, instead of using the algebraic error built with reprojected endpoints and image lines, we adopt line error function containing the distance from the reprojected midpoint of the model line segment to the image line and giving it a weight 4 times as that of the endpoints. A system of linear homogeneous equations only involving the rotation are derived, containing point and line constraints. Minima are obtained by a null space analysis according to rotation constraints and then the solution is selected by reprojection errors before iterative refinement. Experimental results show that the proposed method showcases great performances in both simulations and real-data experiment on VGG dataset. Great advantages are presented by EOPnPL compared to PnL methods when using only lines. Among the state-of-the-arts methods, the proposed method stands out with high accuracy under ordinary condition and comparable results in planar condition with the speed close to the fastest.},
  archive   = {C_IROS},
  author    = {Yi Zhang and Yueqiang Zhang and Biao Hu and Yihe Yin and Wenjun Chen and Xiaolin Liu and Qifeng Yu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982023},
  pages     = {3762-3769},
  title     = {An efficient and accurate solution to camera pose estimation problem from point and line correspondences based on null space analysis},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self supervised learning for multiple object tracking in 3D
point clouds. <em>IROS</em>, 3754–3761. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multiple object tracking in 3D point clouds has applications in mobile robots and autonomous driving. This is a challenging problem due to the sparse nature of the point clouds and the added difficulty of annotation in 3D for supervised learning. To overcome these challenges, we propose a neural network architecture that learns effective object features and their affinities in a self supervised fashion for multiple object tracking in 3D point clouds captured with LiDAR sensors. For self supervision, we use two approaches. First, we generate two augmented LiDAR frames from a single real frame by applying translation, rotation and cutout to the objects. Second, we synthesize a LiDAR frame using CAD models or primitive geometric shapes and then apply the above three augmentations to them. Hence, the ground truth object locations and associations are known in both frames for self supervision. This removes the need to annotate object associations in real data, and additionally the need for training data collection and annotation for object detection in synthetic data. To the best of our knowledge, this is the first self supervised multiple object tracking method for 3D data. Our model achieves state of the art results.},
  archive   = {C_IROS},
  author    = {Aakash Kumar and Jyoti Kini and Ajmal Mian and Mubarak Shah},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981793},
  pages     = {3754-3761},
  title     = {Self supervised learning for multiple object tracking in 3D point clouds},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning moving-object tracking with FMCW LiDAR.
<em>IROS</em>, 3747–3753. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a learning-based moving-object tracking method utilizing the newly developed LiDAR sensor, Frequency Modulated Continuous Wave (FMCW) LiDAR. Compared with most existing commercial LiDAR sensors, FMCW LiDAR can provide additional Doppler velocity information to each 3D point of the point clouds. Benefiting from this, we can generate instance labels as ground truth in a semi-automatic manner. Given the labels, we propose a contrastive learning framework, which pulls together the features from the same instance in embedding space and pushes apart the features from different instances, to improve the tracking quality. Extensive experiments are conducted on the recorded driving data, and the results show that our method outperforms the baseline methods by a large margin.},
  archive   = {C_IROS},
  author    = {Yi Gu and Hongzhi Cheng and Kafeng Wang and Dejing Dou and Chengzhong Xu and Hui Kong},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981346},
  pages     = {3747-3753},
  title     = {Learning moving-object tracking with FMCW LiDAR},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robot-assisted drilling on curved surfaces with haptic
guidance under adaptive admittance control. <em>IROS</em>, 3723–3730.
(<a href="https://doi.org/10.1109/IROS47612.2022.9982000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Drilling a hole on a curved surface with a desired angle is prone to failure when done manually, due to the difficulties in drill alignment and also inherent instabilities of the task, potentially causing injury and fatigue to the workers. On the other hand, it can be impractical to fully automate such a task in real manufacturing environments because the parts arriving at an assembly line can have various complex shapes where drill point locations are not easily accessible, making automated path planning difficult. In this work, an adaptive admittance controller with 6 degrees of freedom is developed and deployed on a KUKA LBR iiwa 7 cobot such that the operator is able to manipulate a drill mounted on the robot with one hand comfortably and open holes on a curved surface with haptic guidance of the cobot and visual guidance provided through an AR interface. Real-time adaptation of the admittance damping provides more transparency when driving the robot in free space while ensuring stability during drilling. After the user brings the drill sufficiently close to the drill target and roughly aligns to the desired drilling angle, the haptic guidance module fine tunes the alignment first and then constrains the user movement to the drilling axis only, after which the operator simply pushes the drill into the workpiece with minimal effort. Two sets of experiments were conducted to investigate the potential benefits of the haptic guidance module quantitatively (Experiment I) and also the practical value of the proposed pHRI system for real manufacturing settings based on the subjective opinion of the participants (Experiment II). The results of Experiment I, conducted with 3 naive participants, show that the haptic guidance improves task completion time by 26\% while decreasing human effort by 16\% and muscle activation levels by 27\% compared to no haptic guidance condition. The results of Experiment II, conducted with 3 experienced industrial workers, show that the proposed system is perceived to be easy to use, safe, and helpful in carrying out the drilling task.},
  archive   = {C_IROS},
  author    = {Alireza Madani and Pouya P. Niaz and Berk Guler and Yusuf Aydin and Cagatay Basdogan},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982000},
  pages     = {3723-3730},
  title     = {Robot-assisted drilling on curved surfaces with haptic guidance under adaptive admittance control},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast and comfortable interactive robot-to-human object
handover. <em>IROS</em>, 3701–3706. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Transferring tools and objects to human hands is an important ability of collaborative robots. Most of the existing approaches focus on handover affordance, however, the comfort of receiving objects with human hands is often neglected. In this paper, we use advanced deep learning models to pre-generate handover target configurations that are convenient for human grasping based on the characteristics of the objects and tools, and then the robot grasps and passes the objects to the human. Experimental results on a mobile collaborative robot show that our proposed framework can robustly and efficiently deliver different shapes and types of objects to a human hand of any pose within the robot&#39;s field of view in a target pose that is convenient for grasping and can quickly deliver objects to a new target location even after the human hand moves to a new position.},
  archive   = {C_IROS},
  author    = {Chongxi Meng and Tianwei Zhang and Tin lun Lam},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981484},
  pages     = {3701-3706},
  title     = {Fast and comfortable interactive robot-to-human object handover},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A null-space based approach for a safe and effective
human-robot collaboration. <em>IROS</em>, 3694–3700. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {During physical human robot collaboration, it is important to be able to implement a time-varying interactive behaviour while ensuring robust stability. Admittance control and passivity theory can be exploited for achieving these objectives. Nevertheless, when the admittance dynamics is time-varying, it can happen that, for ensuring a passive and stable behaviour, some spurious dissipative effects have to be introduced in the admittance dynamics. These effects are perceived by the user and degrade the collaborative performance. In this paper we exploit the task redundancy of the manipulator in order to harvest energy in the null space and to avoid spurious dynamics on the admittance. The proposed architecture is validated by simulations and by experiments onto a collaborative robot.},
  archive   = {C_IROS},
  author    = {Federico Benzi and Cristian Secchi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981682},
  pages     = {3694-3700},
  title     = {A null-space based approach for a safe and effective human-robot collaboration},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Position-based treadmill drive with wire traction for
experience of level ground walking from gait acceleration state to
steady state. <em>IROS</em>, 3687–3693. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A treadmill system has a large potential to provide humans with an augmented walking experience in real-life without a spatial limitation. However, a treadmill gait is different from walking on level ground. In previous studies, the adaptive belt speed control of a treadmill was developed to achieve a self-paced walking for making the users&#39; treadmill gait similar to their level ground gait. Such studies have focused on steady-state walking and regulating the user&#39;s position on the treadmill. A normal gait can be divided into an acceleration state after gait initiation, a steady state, and a deceleration state for stopping. The objective of this study is to develop a treadmill system with a wire tension application enabling a human to experience a similar gait to a level ground gait during the transition phase from an acceleration state to a steady state. We developed a treadmill 4 m long × 1 m wide. To allow a user to move on the treadmill during the gait acceleration phase, an insensitive zone where a user can move without the treadmill belt drive was set. In addition, the treadmill was equipped with a wire traction system to apply a traction force canceling the effect of the belt floor acceleration of the treadmill when the belt speed of the treadmill changes. Through an experiment with six participants, the proposed treadmill system allowed the users to move in an acceleration state with the same head acceleration pattern as with level ground walking and cancel the inertial effect with the wire traction, which enabled the users to transition to a steady state from an acceleration state.},
  archive   = {C_IROS},
  author    = {Tamon Miyake and Shunya Itano and Mitsuhiro Kamezaki and Shigeki Sugano},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981711},
  pages     = {3687-3693},
  title     = {Position-based treadmill drive with wire traction for experience of level ground walking from gait acceleration state to steady state},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A framework of rehabilitation-assisted robot skill
representation, learning, and modulation via manifold-mappings and
gaussian processes. <em>IROS</em>, 3680–3686. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Stroke survivors usually have dyskinesia, who have an urgent need for rehabilitation-assist training. To reduce the labor of rehabilitation therapists, this paper attempts to investigate an effective rehabilitation-assisted robot skill acquisition framework, which is inspired by the scheme of robot learning from demonstration (LfD). Since most of the current LfD methods were implemented with rigorous assumptions that the considering motion features are only represented on an individual manifold. Meanwhile, despite many advancements that have been achieved on time-position trajectories and position-velocity trajectories, those methods are restricted to Euclidean space and can not be applied to learn those dexterous and compliant rehabilitation-assisted robot skills such as position-orientation trajectories and force-stiffness trajectories, etc. In this paper, we propose a novel skill acquisition framework for rehabilitation-assisted robot using manifold-mappings and Gaussian processes, which allows the robot to 1) simultaneously considering the robot position, orientation, force as well as stiffness by manifold-mappings among d-dimensional Euclidean space $\mathcal{R}^{d}$ , special orthogonal group $S\mathcal{O}$ (3), and Riemannian space $\mathcal{M}$ , respectively, which resulting in accurate motion and compliant behavior; 2) retrieving skill representation by encap-sulating the variability of multiple high-dimensional demon-strations that with input-dependent noises; 3) implementing the via-points-based trajectory modulation by considering task constraints or environmental changes. To simplify the writing, we named the proposed framework as Multi-motion Features Fusion-based Robot Skill Learning (MF 2 RoSL). To effectively evaluate the effectiveness of our proposed method, an upper limb rehabilitation training system with a collaborative Kinova robot is developed. The training exercises of our system are determined according to the Brunnstrom therapeutic approach to the management of hemiplegic patients, including the 3-DoFs movement of the shoulder joint and a 7-DoF movement of an insertion/extraction task for assessing the activities of daily living (ADL). Results indicate that our proposed MF 2 RoSL method allows the robot to learn rehabilitation skills from the therapist and can be rapidly adapted to new patients.},
  archive   = {C_IROS},
  author    = {Hongmin Wu and Zhihao Xu and Wu Yan and Yangmin Ou and Zhaoyang Liao and Xuefeng Zhou},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982172},
  pages     = {3680-3686},
  title     = {A framework of rehabilitation-assisted robot skill representation, learning, and modulation via manifold-mappings and gaussian processes},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modulo cellulo: Modular versatile tangible educational
robots. <em>IROS</em>, 3666–3671. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This article presents the novel modular version of the robotic platform Cellulo, a versatile handheld robot initially designed as an educational robot. The use of Cellulo in different contexts and applications over the years has highlighted the need for modularity. Modularity adds versatility by increasing the spectrum of functionalities of the robot, as well as more robustness. Modulo Cellulo consists of three modules: a main module, a battery module, and an interaction module. We describe the new Modulo Cellulo platform, the different modules design, the mechanical and electrical inter-connectivity between them, the new adaptive controller, and the application development framework. As a show case, we present the addition of the reconfigurable robot Mori as a module for Cellulo, in an activity envisioning the collaboration between reconfigurable swarm robots.},
  archive   = {C_IROS},
  author    = {Hala Khodr and Kevin Holdcroft and Yi-Shiun Wu and Victor Borja and Hadrien Sprumont and Barbara Bruno and Jamie Paik and Pierre Dillenbourg},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981983},
  pages     = {3666-3671},
  title     = {Modulo cellulo: Modular versatile tangible educational robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep learning classification of touch gestures using
distributed normal and shear force. <em>IROS</em>, 3659–3665. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When humans socially interact with another agent (e.g., human, pet, or robot) through touch, they do so by applying varying amounts of force with different directions, locations, contact areas, and durations. While previous work on touch gesture recognition has focused on the spatio-temporal distribution of normal forces, we hypothesize that the addition of shear forces will permit more reliable classification. We present a soft, flexible skin with an array of tri-axial tactile sensors for the arm of a person or robot. We use it to collect data on 13 touch gesture classes through user studies and train a Convolutional Neural Network (CNN) to learn spatio-temporal features from the recorded data. The network achieved a recognition accuracy of 74\% with normal and shear data, compared to 66\% using only normal force data. Adding distributed shear data improved classification accuracy for 11 out of 13 touch gesture classes.},
  archive   = {C_IROS},
  author    = {Hojung Choi and Dane Brouwer and Michael A. Lin and Kyle T. Yoshida and Carine Rognon and Benjamin Stephens-Fripp and Allison M. Okamura and Mark R. Cutkosky},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981457},
  pages     = {3659-3665},
  title     = {Deep learning classification of touch gestures using distributed normal and shear force},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning-based six-axis force/torque estimation using
GelStereo fingertip visuotactile sensing. <em>IROS</em>, 3651–3658. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visuotactile sensors have recently attracted much attention in robot communities due to the benefit of high spatial resolution sensing. However, force/torque estimation by visuotactile sensors remains a challenging problem. In this paper, we propose a learning-based six-axis force/torque estimation network using GelStereo visuotactile sensor, which can provide two-dimensional (2D) and three-dimensional (3D) displacements of markers embedded in the sensor surface. The convolutional neural networks are employed to extract multi-modal tactile deformation features; and a novel contact positional encoding method is proposed to eliminate the influence of translation invariance in convolutional operators. The well-trained model achieves the best RMSE of 0.290 N in force and 0.0084 Nm in torque. Furthermore, the proposed force/torque estimation network is integrated with a force-feedback policy for adaptive grasping tasks. The experimental results demonstrate the effectiveness of the proposed method and its potential application in robotic grasping and manipulation tasks.},
  archive   = {C_IROS},
  author    = {Chaofan Zhang and Shaowei Cui and Yinghao Cai and Jingyi Hu and Rui Wang and Shuo Wang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981100},
  pages     = {3651-3658},
  title     = {Learning-based six-axis Force/Torque estimation using GelStereo fingertip visuotactile sensing},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tactile pattern super resolution with taxel-based sensors.
<em>IROS</em>, 3644–3650. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In contrast to sophisticated means of visual su-per resolution (SR), not much work has been done in the tactile SR field. Existing tactile SR algorithms for taxel-based sensors mainly focus on enhancing the localization accuracy, and generally associate with a specific type of hardware, sometimes not applicable to generic taxel-based tactile sensors. Inspired by image SR, we investigate the tactile pattern SR in this paper, and present how to transform successful image SR schemes, e.g. Convolutional Neural Network (CNN) and Generative Adversarial Network (GAN) to serve the tactile SR. We propose two tactile SR models, i.e. TactileSRCNN and TactileSRGAN, and establish a new tactile pattern SR dataset for model learning. The ground truth of high resolution (HR) tactile patterns in the dataset is obtained via multi-sampling (i.e. overlapping reception) and registration of low resolution (LR) sensor. One key contribution of this research lies in achieving ×100 (from 3×4×4 to 40×40) times tactile pattern SR with a one-time tapping of 3-axis taxel-based sensor. Different from existing tactile SR algorithms which improves the localization accuracy of a single contact point, the proposed scheme can provide multi-point contact detection to robotic applications.},
  archive   = {C_IROS},
  author    = {Bing Wu and Qian Liu and Qiang Zhang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981062},
  pages     = {3644-3650},
  title     = {Tactile pattern super resolution with taxel-based sensors},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual pressure estimation and control for soft robotic
grippers. <em>IROS</em>, 3628–3635. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft robotic grippers facilitate contact-rich manipulation, including robust grasping of varied objects. Yet the beneficial compliance of a soft gripper also results in significant deformation that can make precision manipulation challenging. We present visual pressure estimation &amp; control (VPEC), a method that infers pressure applied by a soft gripper using an RGB image from an external camera. We provide results for visual pressure inference when a pneumatic gripper and a tendon-actuated gripper make contact with a flat surface. We also show that VPEC enables precision manipulation via closed-loop control of inferred pressure images. In our evaluation, a mobile manipulator (Stretch RE1 from Hello Robot) uses visual servoing to make contact at a desired pressure; follow a spatial pressure trajectory; and grasp small low-profile objects, including a microSD card, a penny, and a pill. Overall, our results show that visual estimates of applied pressure can enable a soft gripper to perform precision manipulation.},
  archive   = {C_IROS},
  author    = {Patrick Grady and Jeremy A. Collins and Samarth Brahmbhatt and Christopher D. Twigg and Chengcheng Tang and James Hays and Charles C. Kemp},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982073},
  pages     = {3628-3635},
  title     = {Visual pressure estimation and control for soft robotic grippers},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 2D vs. 3D LiDAR-based person detection on mobile robots.
<em>IROS</em>, 3604–3611. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Person detection is a crucial task for mobile robots navigating in human-populated environments. LiDAR sensors are promising for this task, thanks to their accurate depth measurements and large field of view. Two types of LiDAR sensors exist: the 2D LiDAR sensors, which scan a single plane, and the 3D LiDAR sensors, which scan multiple planes, thus forming a volume. How do they compare for the task of person detection? To answer this, we conduct a series of exper-iments, using the public, large-scale JackRabbot dataset and the state-of-the-art 2D and 3D LiDAR-based person detectors (DR-SPAAM and CenterPoint respectively). Our experiments include multiple aspects, ranging from the basic performance and speed comparison, to more detailed analysis on localization accuracy and robustness against distance and scene clutter. The insights from these experiments highlight the strengths and weaknesses of 2D and 3D LiDAR sensors as sources for person detection, and are especially valuable for designing mobile robots that will operate in close proximity to surrounding humans (e.g. service or social robot).},
  archive   = {C_IROS},
  author    = {Dan Jia and Alexander Hermans and Bastian Leibe},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981519},
  pages     = {3604-3611},
  title     = {2D vs. 3D LiDAR-based person detection on mobile robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). One object at a time: Accurate and robust structure from
motion for robots. <em>IROS</em>, 3598–3603. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A gaze-fixating robot perceives distance to the fixated object and relative positions of surrounding objects immediately, accurately, and robustly. We show how fixation, which is the act of looking at one object while moving, exploits regularities in the geometry of 3D space to obtain this information. These regularities introduce rotation-translation couplings that are not commonly used in structure from motion. To validate, we use a Franka Emika Robot with an RGB camera. We a) find that error in distance estimate is less than 5 mm at a distance of 15 cm, and b) show how relative position can be used to find obstacles under challenging scenarios. We combine accurate distance estimates and obstacle information into a reactive robot behavior that is able to pick up objects of unknown size, while impeded by unforeseen obstacles.},
  archive   = {C_IROS},
  author    = {Aravind Battaje and Oliver Brock},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981953},
  pages     = {3598-3603},
  title     = {One object at a time: Accurate and robust structure from motion for robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Child engagement estimation in heterogeneous child-robot
interactions using spatiotemporal visual cues. <em>IROS</em>, 3584–3589.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots are increasingly introduced in various Child-Robot Interactions with educational, entertainment or even therapeutic goals. In order to achieve qualitative inter-actions, robots need to adjust their behavior according to children&#39;s response. A robot&#39;s ability to successfully estimate partner&#39;s engagement is of great importance towards this direction. In this research we propose a method to estimate the engagement level of children during heterogeneous and challenging child-robot interactions. Our method uses the spatiotemporal residual $\mathrm{R}(2+1)\mathrm{D}$ blocks to simultaneously leverage the rich RGB and temporal information, which is crucial for the engagement estimation. We present results on three different groups of data, including the PInSoRo open dataset, proving our method&#39;s robustness and improvement over previous works.},
  archive   = {C_IROS},
  author    = {Dafni Anagnostopoulou and Niki Efthymiou and Christina Papailiou and Petros Maragos},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981908},
  pages     = {3584-3589},
  title     = {Child engagement estimation in heterogeneous child-robot interactions using spatiotemporal visual cues},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MV6D: Multi-view 6D pose estimation on RGB-d frames using a
deep point-wise voting network. <em>IROS</em>, 3568–3575. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Estimating 6D poses of objects is an essential computer vision task. However, most conventional approaches rely on camera data from a single perspective and therefore suffer from occlusions. We overcome this issue with our novel multi-view 6D pose estimation method called MV6D which accurately predicts the 6D poses of all objects in a cluttered scene based on RGB-D images from multiple perspectives. We base our approach on the PVN3D network that uses a single RGB-D image to predict keypoints of the target objects. We extend this approach by using a combined point cloud from multiple views and fusing the images from each view with a DenseFusion layer. In contrast to current multi-view pose detection networks such as CosyPose, our MV6D can learn the fusion of multiple perspectives in an end-to-end manner and does not require multiple prediction stages or subsequent fine tuning of the prediction. Furthermore, we present three novel photorealistic datasets of cluttered scenes with heavy occlusions. All of them contain RGB-D images from multiple perspectives and the ground truth for instance semantic segmentation and 6D pose estimation. MV 6D significantly outperforms the state-of-the-art in multi-view 6D pose estimation even in cases where the camera poses are known inaccurately. Furthermore, we show that our approach is robust towards dynamic camera setups and that its accuracy increases incrementally with an increasing number of perspectives.},
  archive   = {C_IROS},
  author    = {Fabian Duffhauss and Tobias Demmler and Gerhard Neumann},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982268},
  pages     = {3568-3575},
  title     = {MV6D: Multi-view 6D pose estimation on RGB-D frames using a deep point-wise voting network},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BSA - bi-stiffness actuation for optimally exploiting
intrinsic compliance and inertial coupling effects in elastic joint
robots. <em>IROS</em>, 3536–3543. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Compliance in actuation has been exploited to generate highly dynamic maneuvers such as throwing that take advantage of the potential energy stored in joint springs. However, the energy storage and release could not be well-timed yet. On the contrary, for multi-link systems, the natural system dynamics might even work against the actual goal. With the introduction of variable stiffness actuators, this problem has been partially addressed. With a suitable optimal control strategy, the approximate decoupling of the motor from the link can be achieved to maximize the energy transfer into the distal link prior to launch. However, such continuous stiffness variation is complex and typically leads to oscillatory swing-up motions instead of clear launch sequences. To circumvent this issue, we investigate decoupling for speed maximization with a dedicated novel actuator concept denoted Bi-Stiffness Actuation. With this, it is possible to fully decouple the link from the joint mechanism by a switch-and-hold clutch and simultaneously keep the elastic energy stored. We show that with this novel paradigm, it is not only possible to reach the same optimal performance as with power-equivalent variable stiffness actuation, but even directly control the energy transfer timing. This is a major step forward compared to previous optimal control approaches, which rely on optimizing the full time-series control input.},
  archive   = {C_IROS},
  author    = {Dennis Ossadnik and Mehmet C. Yildirim and Fan Wu and Abdalla Swikir and Hugo T. M. Kussaba and Saeed Abdolshah and Sami Haddadin},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981928},
  pages     = {3536-3543},
  title     = {BSA - bi-stiffness actuation for optimally exploiting intrinsic compliance and inertial coupling effects in elastic joint robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A robot factors approach to designing modular hardware.
<em>IROS</em>, 3528–3535. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots are increasingly being called on to operate in settings and on tasks originally designed for humans, or where humans are also expected to work. Accordingly, the hardware and tools to be packaged, operated, or maintained are typically designed for use by humans, not robots. Robot autonomy in such cases can be expedited by a “robot factors” approach to the design of hardware, analogous to ergonomics for humans, taking typical current robot capabilities into account during the design process. In this paper, we present two case studies of redesigning mission-critical hardware in space habitats to facilitate autonomous robot operation. In both cases, hardware that previously required dexterous bi-manual manipulation is redesigned such that the entire maintenance task can be completed by a single robotic arm with a standard parallel jaw gripper. We demonstrate successful autonomous replacement of modules in the two hardware systems, and characterize how orientation and compliance of a grasp helps compensate for positioning errors. Based on our findings, we identify several key design strategies that underpin the robot factors approach to designing robot-friendly hardware, including consolidating compound actions into simpler mechanisms, constraining required motions to a single axis, and introducing mechanical compliance to mitigate the effects of pose uncertainties.},
  archive   = {C_IROS},
  author    = {Nathan Melenbrink and Clark Teeple and Justin Werfel},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981773},
  pages     = {3528-3535},
  title     = {A robot factors approach to designing modular hardware},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust cartesian kinematics estimation for task-space
control systems. <em>IROS</em>, 3512–3519. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We discuss a novel method for estimating task Cartesian position and velocity in robot manipulators. This is done by model-based fusion of inertial measurement units with motor encoders. The model is developed to robustly handle the uncertainties in the trajectory. Thus, not only the approach benefits from high fidelity and bandwidth thanks to multiple-sensory fusion, but it also enforces stability despite poorly formulated motions. This empowers the method to be utilized in complex closed-loop applications, where both task position and velocity information is required.},
  archive   = {C_IROS},
  author    = {Seyed Ali Baradaran Birjandi and Niels Dehio and Abderrahmane Kheddar and Sami Haddadin},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981233},
  pages     = {3512-3519},
  title     = {Robust cartesian kinematics estimation for task-space control systems},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward FBG-sensorized needle shape prediction in tissue
insertions. <em>IROS</em>, 3505–3511. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Complex needle shape prediction remains an issue for planning of surgical interventions of flexible needles. In this paper, we validate a theoretical method for flexible needle shape prediction allowing for non-uniform curvatures, extending upon a previous sensor-based model which combines curvature measurements from fiber Bragg grating (FBG) sensors and the mechanics of an inextensible elastic rod to determine and predict the 3D needle shape during insertion. We evaluate the model&#39;s effectiveness in single-layer isotropic tissue for shape sensing and shape prediction capabilities. Experiments on a four-active area, FBG-sensorized needle were performed in varying single-layer isotropic tissues under stereo vision to provide 3D ground truth of the needle shape. The results validate a viable 3D needle shape prediction model accounting for non-uniform curvatures in flexible needles with mean needle shape sensing and prediction root-mean-square errors of 0.479 mm and 0.892 mm, respectively.},
  archive   = {C_IROS},
  author    = {Dimitri A. Lezcano and Min Jung Kim and Iulian I. Iordachita and Jin Seob Kim},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981856},
  pages     = {3505-3511},
  title     = {Toward FBG-sensorized needle shape prediction in tissue insertions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Foot-operated tele-impedance interface for robot
manipulation tasks in interaction with unpredictable environments.
<em>IROS</em>, 3497–3504. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tele-impedance increases interaction performance between a robotic tool and unstructured/unpredictable en-vironments during teleoperation. However, the existing tele-impedance interfaces have several ongoing issues, such as long calibration times and various obstructions for the human operator. In addition, they are all designed to be controlled by the operator&#39;s arms, which can cause difficulties when both arms are used, as in bi-manual teleoperation. To resolve these issues, we designed a novel foot-based tele-impedance control method inspired by the human limb stiffness ellipse modulation. The proposed mechanical interface design includes a disc and a foot pressure sensor that controls the orientation and size/shape of the stiffness ellipse, respectively. We evaluated the disc interface control method in an experimental study with 12 participants, who performed a complex drilling task in a virtual environment. The results show the ability of the operator to use the proposed interface in order to dynamically adapt to different phases of the task and changes in the environment. In addition, a comparison with low and high uniform impedance modes demonstrates a superior interaction performance of the proposed method.},
  archive   = {C_IROS},
  author    = {Stijn Klevering and Winfred Mugge and David A. Abbink and Luka Peternel},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981065},
  pages     = {3497-3504},
  title     = {Foot-operated tele-impedance interface for robot manipulation tasks in interaction with unpredictable environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Steady-state manifold of riderless motorcycles.
<em>IROS</em>, 3491–3496. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Keeping balance is one of the most important tasks of a motorcycle. The steady-state manifold is proposed in this paper to explore the inherent dynamics and the balance properties of a riderless motorcycle. The dynamic and kinematic characteristics are analyzed based on the manifold and are validated by simulation. Comparing to traditional control method, the usefulness of the manifold in control is shown through the design of a novel control strategy. Furthermore, based on the analysis and the simulation, the potential applications of the manifold for control and planning are summarized.},
  archive   = {C_IROS},
  author    = {Yu Tian and Zhang Chen and Yang Deng and Boyi Wang and Bin Liang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981899},
  pages     = {3491-3496},
  title     = {Steady-state manifold of riderless motorcycles},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-axis reorientation of a free-falling omnidirectional
wheeled robot. <em>IROS</em>, 3484–3490. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents reorientation manoeuvres applied to an omnidirectional wheeled robot for impact mitigation during short falls. The proposed robot architecture aims to build upon recent innovations in reorientation robots to attain fast, multi-axis reorientation. Indeed, the use of omnidirectional wheels allows for simplifications to be made with respect to previous mobile robot architectures that make the proposed architecture more efficient for free fall reorientation, while still maintaining free roaming capabilities. To test these improvements, a prototype is built and a free roam and two free fall demonstrations are completed. On the one hand, the free roam demonstration validates that translation along both horizontal axes and rotation about the yaw axis are achieved with the presented prototype. On the other hand, the first free fall demonstration shows that a worst case scenario of a 180-degree reorientation about one axis can be completed in just under 0.45 seconds (one-metre fall) and the second free fall demonstration validates that the prototype is capable of simultaneous reorientation about both the roll and pitch axes. Therefore, the fast, multi-axis reorientation capabilities of the developed prototype are verified.},
  archive   = {C_IROS},
  author    = {Mark Charlet and Thierry Laliberté and Clément Gosselin},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981821},
  pages     = {3484-3490},
  title     = {Multi-axis reorientation of a free-falling omnidirectional wheeled robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modular and hybrid numerical-analytical approach - a case
study on improving computational efficiency for series-parallel hybrid
robots. <em>IROS</em>, 3476–3483. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modeling closed loop mechanisms is a necessity for the control and simulation of various systems and poses a great challenge to rigid body dynamics algorithms. Solving the forward and inverse dynamics for such systems require resolution of loop closure constraints which are often solved via numerical procedures. This brings an additional burden to these algorithms as they have to stabilize and control the loop closure errors. In order to avoid this issue, analytical solutions are preferred for commonly studied parallel mechanisms. This paper has two contributions. Firstly, it reports a case study on a modular and hybrid numerical-analytical approach to model and control series-parallel hybrid robots which are subjected to large number of holonomic constraints. The approach exploits the modularity in the robot design to combine analytical loop closure for the known submechanisms and numerical loop closure for submechanisms where analytical solutions are not available. This offers an edge over purely numerical approach in terms of computational efficiency. Secondly, an adaption of the constraint embedding approach in Articulated Body Algorithm (ABA) is presented which yields a recursive algorithm in minimal coordinates for computing the forward dynamics of series-parallel hybrid systems. The proposed modification exploits the Lie group formulations and allows easy implementation of recursive forward dynamics of constrained systems in state of the art multi-body solvers.},
  archive   = {C_IROS},
  author    = {Rohit Kumar and Shivesh Kumar and Andreas Müller and Frank Kirchner},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981474},
  pages     = {3476-3483},
  title     = {Modular and hybrid numerical-analytical approach - a case study on improving computational efficiency for series-parallel hybrid robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sim-to-real transfer of image-based autonomous guidewire
navigation trained by deep deterministic policy gradient with behavior
cloning for fast learning. <em>IROS</em>, 3468–3475. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Percutaneous coronary intervention (PCI) is a frequently used surgical treatment for cardiovascular disease, one of the leading cause of death in the world. In traditional PCI, a doctor navigates a thin guidewire in a patient&#39;s vessel toward a target location by looking into live X-ray angiogram images of the patient. Recently, researchers are using reinforcement learning to automate this guidewire navigation process without attaching any sensor to the guidewire tip. These researchers use a real vessel phantom to train their behavior policy using reinforcement learning. Training a reinforcement learning algorithm on a real setup can give a good guidewire control on that setup, but it is under question whether the trained algorithm can be applied to other vessel structures. We can make various vessel phantoms and train the algorithm on the setups, but it can be really time and money consuming. In this paper, we devise a method for sim-to-real transfer of a guidewire navigation trained by reinforcement learning using only images. We pretrain our behavior policy using data collected by running an expert algorithm in the virtual environment. Then, we train the behavior policy by deep deterministic policy gradient (DDPG) in a virtual environment. With behavior cloning, our method learns to successfully navigate a guidewire in much shorter time than training DDPG from scratch without behavior cloning. After done with the training, we transfer the behavior policy trained in the virtual environment to the guidewire navigation in a real vessel phantom. Our trained behavior policy navigates the guidewire to destinations successfully in all test episodes and navigates faster than the expert algorithm. Experiment video is available at: https: //youtu.be/HCEbIhZsXqw},
  archive   = {C_IROS},
  author    = {Yongjun Cho and Jae-Hyeon Park and Jaesoon Choi and Dong Eui Chang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982168},
  pages     = {3468-3475},
  title     = {Sim-to-real transfer of image-based autonomous guidewire navigation trained by deep deterministic policy gradient with behavior cloning for fast learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust Sim2Real transfer with the da vinci research kit: A
study on camera, lighting, and physics domain randomization.
<em>IROS</em>, 3429–3435. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous surgical robotics is a growing area of research, with advances being made in the areas of vision and control. Central to this research is the need for simulations to facilitate data collection and simulate learning environments for Reinforcement Learning (RL) agents. Recent simulators have facilitated RL policy generation, but lack a robust sim2real pipeline and a proven vision-based policy that can use any type of camera including the da Vinci Surgical System (dVSS) Endoscope. To solve this, we build a ROS-based sim2real pipeline that incorporates a Unity3D da Vinci Research Kit (dVRK) simulation, modular kinematics, and shared interfaces. We examine the vision-based task of cube pushing, and train RL policies to execute in real life through Domain Randomization. Our experiments evaluate model success in simulation and two camera systems: OAK-1 and the dVSS Endoscope. Our results indicate that Domain Randomization is effective at bridging the sim2real gap, and even extends to the difficult endoscope scenario. We achieve 100\% transfer success rate on both OAK-1 and the dVSS Endoscope, with gains of over 60\% compared to a base model with no Domain Randomization. We examine the various randomization parameters, including lighting, camera, and physics variables, and determine that all parameters play a significant role in bridging the sim2real gap. Testing across extreme lighting and camera configurations not seen in simulation, our models continue to perform well, with 85\% accuracy on the OAK-1 camera. Our future work will extend to other tasks and more complex policies to take advantage of stereo-camera imaging. Further project information is available at https://medcvr.utm.utoronto.ca/iros2022-sim2real.html},
  archive   = {C_IROS},
  author    = {Mustafa Haiderbhai and Radian Gondokaryono and Thomas Looi and James M. Drake and Lueder A. Kahrs},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981573},
  pages     = {3429-3435},
  title     = {Robust Sim2Real transfer with the da vinci research kit: A study on camera, lighting, and physics domain randomization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ultrasound tracking and closed-loop control of a
magnetically-actuated biomimetic soft robot. <em>IROS</em>, 3422–3428.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Small untethered soft robots have potential for diverse applications, particularly in constrained spaces where the use of a tethered device would be infeasible. Examples include biomedical applications such as brachytherapy, fine-needle biospy and micro-needle drug delivery. To advance soft robots towards these applications, there is a need to establish methods for tracking and control using clinically-relevant methods. This study demonstrates motion planning and magnetic control of a soft untethered robot, using ultrasound images as feedback. The closed-loop control of the Millipede soft robot is first validated using a camera-based tracker, where the deviation between the planned path and the trajectory of the robot is 1.71 mm. Afterwards, two methods for ultrasound-based tracking capable of estimating the pose of the robot are proposed, a geometric approach and a convolutional neural network (CNN), and their performance is compared using a video camera as ground truth. Following this, the CNN method replaces the camera tracker to estimate the position and orientation of the robot. The closed-loop system using ultrasound images guides the robot through the workspace while avoiding virtual obstacles, and achieves an average tracking error of 1.59 mm and an angle error of 2.24°.},
  archive   = {C_IROS},
  author    = {Artur João Anjos de Oliveira and Jorge Batista and Sarthak Misra and Venkatasubramanian Kalpathy Venkiteswaran},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981635},
  pages     = {3422-3428},
  title     = {Ultrasound tracking and closed-loop control of a magnetically-actuated biomimetic soft robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lumen shape reconstruction using a soft robotic balloon
catheter and electrical impedance tomography. <em>IROS</em>, 3414–3421.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Incorrectly sized balloon catheters can lead to increased post-surgical complications, yet even with preoperative imaging, correct selection remains a challenge. With limited feedback during surgery, it is difficult to verify correct deployment. We propose the use of integrated impedance measurements and Electrical Impedance Tomography (EIT) imaging to assess the deformation of the balloon and determine the size and shape of the surrounding lumen. Previous work using single impedance measurements, or pressure data and analytical models, whilst demonstrating high sizing accuracy, have assumed a circular cross section. Here we extend these methods by adding a multitude of electrodes to detect elliptical and occluded lumen and obtain EIT images to localise deformations. Using a 14 Fr (5.3 mm) catheter as an example, numerical simulations were performed to find the optimal electrode configuration of two rings of 8 electrodes spaced 10 mm apart. The simulations predicted that the maximum detectable aspect ratio decreased from 0.9 for a 14mm balloon to 0.5 at 30mm. The sizing and ellipticity detection results were verified experimentally. A prototype robotic balloon catheter was constructed to automatically inflate a compliant balloon while simultaneously recording EIT and pressure data. Data were collected in experiments replicating stenotic vessels with an elliptical and asymmetrical profile, and the widening of a lumen during angioplasty. After calibration, the system was able to correctly localise the occlusion and detect aspect ratios of 0.75. EIT images further localised the occlusion and visualised the dilation of the lumen during balloon inflation.},
  archive   = {C_IROS},
  author    = {James Avery and Mark Runciman and Cristina Fiani and Elena Monfort Sanchez and Saina Akhond and Zhuang Liu and Kirill Aristovich and George Mylonas},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981150},
  pages     = {3414-3421},
  title     = {Lumen shape reconstruction using a soft robotic balloon catheter and electrical impedance tomography},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Autonomous emergency landing for multicopters using deep
reinforcement learning. <em>IROS</em>, 3392–3399. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents a pipeline for autonomous emergency landing for multicopters, such as rotary wing Unmanned Aerial Vehicles (UAVs), using deep Reinforcement Learning (RL). Mechanical malfunctions, strong winds, sudden battery life drops (e.g, due to cold weather), failure in localization or GPS jamming are not uncommon and all constitute emergency situations that require a UAV to abort its mission early and land as quickly as possible in the immediate vicinity. To this end, it is crucial for a UAV that is deployed in real missions to be able to detect a safe landing spot efficiently and proceed to land autonomously, avoiding damage to both its integrity and the surroundings. Driven by the advances in semantic segmentation and depth completion using machine learning, the proposed architecture uses deep RL to infer actions from semantic and depth information, flying the robot towards secure areas, while respecting safety constraints. Thanks to our robust training strategy and the choice of these mid-level representations as input to the RL agent, we show that our policy can directly transfer to the real world, without the need for any additional fine-tuning. In a series of challenging experiments both in simulation and with a real platform, we demonstrate that our planner guides a rotorcraft UAV to a safe landing spot up to 1.5 times faster and with double success rate than the state of the art (including a commercially available solution), paving the way towards realistically deployable UAVs.},
  archive   = {C_IROS},
  author    = {Luca Bartolomei and Yves Kompis and Lucas Teixeira and Margarita Chli},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981152},
  pages     = {3392-3399},
  title     = {Autonomous emergency landing for multicopters using deep reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reactive motion planning for rope manipulation and collision
avoidance using aerial robots. <em>IROS</em>, 3384–3391. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work we address the challenging problem of manipulating a flexible link, like a rope, with an aerial robot. Inspired by spraying tasks in construction and maintenance scenarios, we consider the case in which an autonomous end-effector (e.g., a spray nozzle moved by a robot or a human operator) is connected to a fixed point by a rope (e.g., a hose). To avoid collisions between the rope and the environment while the end-effector moves, we propose the use of an aerial robot as a flying companion to properly manipulate the rope away from collisions. The aerial robot is attached to the rope between the end-effector and the fixed point. Assuming no direct control of the end-effector (e.g., when operated by a human), we design a reactive and fast motion planner for the aerial robot. Grounding on the theory of Forced Geometric Fabrics, we design a motion planner that generates trajectories to drive the aerial robot to follow the end-effector, while manipulating the rope to avoid collisions in cluttered environments. To include the complex behavior of the flexible link, we propose a rope model that estimates its real-time state under forces and position-based interactions, as well as collisions with obstacle surfaces. Finally, we evaluate the system behavior and the motion planner performance in simulations, as well as in real-world experiments on an original spray painting application.},
  archive   = {C_IROS},
  author    = {Liping Shi and Michael Pantic and Olov Andersson and Marco Tognon and Roland Siegwart and Rune Hylsberg Jacobsen},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981658},
  pages     = {3384-3391},
  title     = {Reactive motion planning for rope manipulation and collision avoidance using aerial robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hand-crafted features for floating plastic detection.
<em>IROS</em>, 3378–3383. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Plastic waste is a global concern that has a negative impact on the oceans and wildlife health. This paper focuses on detection of floating plastics in aerial images taken from unmanned aerial vehicles (UAVs). It proposes a new method for plastic detection in marine environments, based on SIFT descriptor and color histograms for feature extraction, as an alternative to state-of-the-art object detectors based on convolutional neural networks (CNNs), Our approach is named SURFACE: “SIFT featURes For plAstiC dEtection”. We investigate how different color-spaces and image resolutions impact the extraction of SIFT features and compare SURFACE to ResNet CNN. Also, we provide a detailed comparison with YOLO and Faster-RCNN object detection models and show that SURFACE achieves approximately the same accuracy while being faster and less memory consuming. The dataset acquired during this research will be publicly available.},
  archive   = {C_IROS},
  author    = {Matija Sukno and Ivana Palunko},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981320},
  pages     = {3378-3383},
  title     = {Hand-crafted features for floating plastic detection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Folding knots using a team of aerial robots. <em>IROS</em>,
3372–3377. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {From ancient times, humans have been using cables and ropes to tie, carry, and manipulate objects by folding knots. However, automating knot folding is challenging because it requires dexterity to move a cable over and under itself. In this paper, we propose a method to fold knots in midair using a team of aerial vehicles. We take advantage of the fact that vehicles are able to fly in between cable segments without any re-grasping. So the team grasps the cable from the floor, and releases it once the knot is folded. Based on a composition of catenary curves, we simplify the complexity of dealing with an infinite-dimensional configuration space of the cable, and formally propose a new knot representation. Such representation allows us to design a trajectory that can be used to fold knots using a leader-follower approach. We show that our method works for different types of knots in simulations. Additionally, we show that our solution is also computationally efficient and can be executed in real-time.},
  archive   = {C_IROS},
  author    = {Diego S. D’Antonio and David Saldaña},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981363},
  pages     = {3372-3377},
  title     = {Folding knots using a team of aerial robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient sampling-based multirotors kinodynamic planning
with fast regional optimization and post refining. <em>IROS</em>,
3356–3363. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For real-time multirotor kinodynamic planning, the efficiency of sampling-based methods is usually hindered by difficult-to-sample homotopy classes like narrow passages. In this paper, we address this issue by a hybrid scheme. We firstly propose a fast regional optimizer exploiting the information of local environments and then integrate it into a bidirectional global sampling process. The incorporation of the local optimization shows significantly improved success rates and less planning time in various types of challenging environments. We further present a refinement module utilizing the same framework as the regional optimizer. It comprehensively investigates the resulting trajectory of the global sampling and improves its smoothness with nearly negligible computation effort. Benchmark results illustrate that our proposed method can better exploit a previous trajectory compared to the state-of-the-art ones. The planning methods are applied to generate trajectories for a quadrotor system in simulation and real-world, and their capability is validated in real-time applications.},
  archive   = {C_IROS},
  author    = {Hongkai Ye and Neng Pan and Qianhao Wang and Chao Xu and Fei Gao},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981707},
  pages     = {3356-3363},
  title     = {Efficient sampling-based multirotors kinodynamic planning with fast regional optimization and post refining},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic parameter adaptation for quadrotor trajectory
planning. <em>IROS</em>, 3348–3355. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Online trajectory planners enable quadrotors to safely and smoothly navigate in unknown cluttered environments. However, tuning parameters is challenging since modern planners have become too complex to mathematically model and predict their interaction with unstructured environments. This work takes humans out of the loop by proposing a planner parameter adaptation framework that formulates objectives into two complementary categories and optimizes them asynchronously. Objectives evaluated with and without trajectory execution are optimized using Bayesian Optimization (BayesOpt) and Particle Swarm Optimization (PSO), respectively. By combining two kinds of objectives, the total convergence rate of the black-box optimization is accelerated while the dimension of optimized parameters can be increased. Benchmark comparisons demonstrate its superior performance over other strategies. Tests with changing obstacle densities validate its real-time environment adaption, which is difficult for prior manual tuning. Real-world flights with different drone platforms, environments, and planners show the proposed framework&#39;s scalability and effectiveness.},
  archive   = {C_IROS},
  author    = {Xin Zhou and Chao Xu and Fei Gao},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981552},
  pages     = {3348-3355},
  title     = {Automatic parameter adaptation for quadrotor trajectory planning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automated aerial screwing with a fully actuated aerial
manipulator. <em>IROS</em>, 3340–3347. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The tasks that unmanned aerial vehicles (UAVs) have taken upon have progressively grown in complexity over the years, alongside with the level of autonomy with which they are carried out. In this work, we present an example of aerial screwing operations with a fully-actuated tilt-rotor platform. Key contributions include a new control framework to automate screwing operations through a robust hole search and in-hole detection algorithm. These are achieved without a-priori knowledge of the exact hole location, and without the use of external tools, such as vision based hole detection or force sensors. Wrench coupling is implemented to account for the platform&#39;s kinematic constraints during screwing. The application of a constant contact force and a compliant response to induced disturbances are obtained with the use of admittance control. The full framework is validated with extensive flight experiments that demonstrate the effectiveness of each subsystem, as well as the complete architecture. We also validate the robustness of the detection algorithm against false positives. Within the results we demonstrate the ability to perform the automated task with a 86\% success rate over 35 flights, and measured hole search time of 9s (median value).},
  archive   = {C_IROS},
  author    = {Micha Schuster and David Bernstein and Paul Reck and Salua Hamaza and Michael Beitelschmidt},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981979},
  pages     = {3340-3347},
  title     = {Automated aerial screwing with a fully actuated aerial manipulator},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Memory-augmented reinforcement learning for image-goal
navigation. <em>IROS</em>, 3316–3323. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we present a memory-augmented approach for image-goal navigation. Earlier attempts, including RL-based and SLAM-based approaches have either shown poor generalization performance, or are heavily-reliant on pose/depth sensors. Our method is based on an attention-based end-to-end model that leverages an episodic memory to learn to navigate. First, we train a state-embedding network in a self-supervised fashion, and then use it to embed previously-visited states into the agent&#39;s memory. Our navigation policy takes advantage of this information through an attention mechanism. We validate our approach with extensive evaluations, and show that our model establishes a new state of the art on the challenging Gibson dataset. Furthermore, we achieve this impressive performance from RGB input alone, without access to additional information such as position or depth, in stark contrast to related work.},
  archive   = {C_IROS},
  author    = {Lina Mezghan and Sainbayar Sukhbaatar and Thibaut Lavril and Oleksandr Maksymets and Dhruv Batra and Piotr Bojanowski and Karteek Alahari},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981090},
  pages     = {3316-3323},
  title     = {Memory-augmented reinforcement learning for image-goal navigation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). H-VLO: Hybrid LiDAR-camera fusion for self-supervised
odometry. <em>IROS</em>, 3302–3307. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a hybrid visual-LiDAR odometry (H-VLO) framework that fuses predicted visual depth map and completed LiDAR map. Compared to the previous visual-LiDAR odometry methods, our approach leverages 2D feature matching and 3D association by utilizing deep depth map, deep flow map and deep LiDAR depth completion networks. Rather than extraction of the depth values from LiDAR measurements for each visual feature, our method first densifies a LiDAR scan with a deep depth completion network and then fuses it with visual deep depth map estimation in a Bayesian framework. This method reduces pose estimation drift by improving feature-to-feature and point-to-feature matching, as well as scale recovery. The evaluations on the public KITTI odometry benchmark show that our technique achieves better or at least comparable estimates than the state-of-the-art visual-LiDAR and monocular visual odometry approaches.},
  archive   = {C_IROS},
  author    = {Eren Aydemir and Naida Fetic and Mustafa Unel},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981111},
  pages     = {3302-3307},
  title     = {H-VLO: Hybrid LiDAR-camera fusion for self-supervised odometry},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VI-IKD: High-speed accurate off-road navigation using
learned visual-inertial inverse kinodynamics. <em>IROS</em>, 3294–3301.
(<a href="https://doi.org/10.1109/IROS47612.2022.9982060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One of the key challenges in high-speed off-road navigation on ground vehicles is that the kinodynamics of the vehicle-terrain interaction can differ dramatically depending on the terrain. Previous approaches to addressing this challenge have considered learning an inverse kinodynamics (IKD) model, conditioned on inertial information of the vehicle to sense the kinodynamic interactions. In this paper, we hypothesize that to enable accurate high-speed off-road navigation using a learned IKD model, in addition to inertial information from the past, one must also anticipate the kinodynamic interactions of the vehicle with the terrain in the future. To this end, we introduce Visual-Inertial Inverse Kinodynamics (VI-IKD), a novel learning based IKD model that is conditioned on visual information from a terrain patch ahead of the robot in addition to past inertial information, enabling it to anticipate kinodynamic interactions in the future. We validate the effectiveness of VI-IKD in accurate high-speed off-road navigation experimentally on a scale 1/5 UT-AlphaTruck off-road autonomous vehicle in both indoor and outdoor environments and show that compared to other state-of-the-art approaches, VI-IKD enables more accurate and robust off-road navigation on a variety of different terrains at speeds of up to 3.5m/s.},
  archive   = {C_IROS},
  author    = {Haresh Karnan and Kavan Singh Sikand and Pranav Atreya and Sadegh Rabiee and Xuesu Xiao and Garrett Warnell and Peter Stone and Joydeep Biswas},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982060},
  pages     = {3294-3301},
  title     = {VI-IKD: High-speed accurate off-road navigation using learned visual-inertial inverse kinodynamics},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stubborn: A strong baseline for indoor object navigation.
<em>IROS</em>, 3287–3293. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a strong baseline that surpasses the performance of previously published methods on the Habitat Challenge task of navigating to a target object in indoor environments. Our method is motivated from primary failure modes of prior state-of-the-art: poor exploration, inaccurate object identification, and agent getting trapped due to imprecise map construction. We make three contributions to mitigate these issues: (i) First, we show that existing map-based methods fail to effectively use semantic clues for exploration. We present a semantic-agnostic exploration strategy (called Stubborn) without any learning that surprisingly outperforms prior work. (ii) We propose a strategy for integrating temporal information to improve object identification. (iii) Lastly, due to inaccurate depth observation the agent often gets trapped in small regions. We develop a multi-scale collision map for obstacle identification that mitigates this issue. Website: https://github.com/Improbable-AI/Stubborn},
  archive   = {C_IROS},
  author    = {Haokuan Luo and Albert Yue and Zhang-Wei Hong and Pulkit Agrawal},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981646},
  pages     = {3287-3293},
  title     = {Stubborn: A strong baseline for indoor object navigation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatio-temporal graph localization networks for image-based
navigation. <em>IROS</em>, 3279–3286. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Localization in topological maps is essential for image-based navigation using an RGB camera. Localization using only one camera can be challenging in medium-to-large-sized environments because similar-looking images are often observed repeatedly, especially in indoor environments. To overcome this issue, we propose a learning-based localization method that simultaneously utilizes the spatial consistency from topological maps and the temporal consistency from time-series images captured by a robot. Our method combines a convolutional neural network (CNN) to embed image features and a recurrent-type graph neural network to perform accurate localization. When training our model, it is difficult to obtain the ground truth (GT) pose of the robot when capturing images in real-world environments. Hence, we propose a sim2real transfer approach with semi-supervised learning that leverages simulator images with the GT pose in addition to real images. We evaluated the proposed method quantitatively and qualitatively and compared it with several state-of-the-art baselines. The proposed method outperformed the baselines in environments where the map contained similar images. Moreover, we evaluated an image-based navigation system incorporating our localization method and confirmed that navigation accuracy significantly improved in the simulator and real environments compared to the other baseline methods.},
  archive   = {C_IROS},
  author    = {Takahiro Niwa and Shun Taguchi and Noriaki Hirose},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981958},
  pages     = {3279-3286},
  title     = {Spatio-temporal graph localization networks for image-based navigation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The probabilistic robot kinematics model and its application
to sensor fusion. <em>IROS</em>, 3263–3270. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots with elasticity in structural components can suffer from undesired end-effector positioning imprecision, which exceeds the accuracy requirements for successful manipulation. We present the Probabilistic-Product-Of-Exponentials robot model, a novel approach for kinematic modeling of robots. It does not only consider the robot&#39;s deterministic geometry but additionally models time-varying and configuration-dependent errors in a probabilistic way. Our robot model allows to propagate the errors along the kinematic chain and to compute their influence on the end-effector pose. We apply this model in the context of sensor fusion for manipulator pose correction for two different robotic systems. The results of a simulation study, as well as of an experiment, demonstrate that probabilistic configuration-dependent error modeling of the robot kinematics is crucial in improving pose estimation results.},
  archive   = {C_IROS},
  author    = {Lukas Meyer and Klaus H. Strobl and Rudolph Triebel},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981399},
  pages     = {3263-3270},
  title     = {The probabilistic robot kinematics model and its application to sensor fusion},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). All you need is LUV: Unsupervised collection of labeled
images using UV-fluorescent markings. <em>IROS</em>, 3241–3248. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning-based perception systems in robotics often requires large-scale image segmentation annotation. Current approaches rely on human labelers, which can be expensive, or simulation data, which can visually differ from real data. This paper proposes Labels from UltraViolet (LUV), a novel framework that enables rapid, automated, inexpensive, high quality data collection in real. LUV uses transparent, UV-fluorescent paint with programmable UV LEDs to collect paired images of a scene in standard and UV lighting. This makes it possible to autonomously extract segmentation masks and keypoints via color thresholding. We apply LUV to a suite of diverse robot perception tasks: locating fabric keypoints, cable segmentation, and surgical needle detection to evaluate its labeling quality, flexibility, and data collection rate. Results suggest that LUV is 180–2500 times faster than a human labeler across the tasks while retaining accuracy and strong task performance. Code, datasets, visualizations, and supplementary material can be found at https://sites.google.com/berkeley.edu/luv.},
  archive   = {C_IROS},
  author    = {Brijen Thananjeyan and Justin Kerr and Huang Huang and Joseph E. Gonzalez and Ken Goldberg},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981768},
  pages     = {3241-3248},
  title     = {All you need is LUV: Unsupervised collection of labeled images using UV-fluorescent markings},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simultaneous contact location and object pose estimation
using proprioception and tactile feedback. <em>IROS</em>, 3233–3240. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Joint estimation of grasped object pose and extrinsic contacts is central to robust and dexterous manipulation. In this paper, we propose a novel state-estimation algorithm that jointly estimates contact location and object pose in 3D using exclusively proprioception and tactile feedback. Our approach leverages two complementary particle filters: one to estimate contact location (CPFGrasp) and another to estimate object poses (SCOPE). We implement and evaluate our approach on real-world single-arm and dual-arm robotic systems. We demonstrate that by bringing two objects into contact, the robots can infer contact location and object poses simultaneously. Our proposed method can be applied to a number of downstream tasks that require accurate pose estimates, such as tool use and assembly. Code and data can be found at https://github.com/MMintLab/scope.},
  archive   = {C_IROS},
  author    = {Andrea Sipos and Nima Fazeli},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981762},
  pages     = {3233-3240},
  title     = {Simultaneous contact location and object pose estimation using proprioception and tactile feedback},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sim2Real instance-level style transfer for 6D pose
estimation. <em>IROS</em>, 3225–3232. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, synthetic data has been widely used in the training of 6D pose estimation networks, in part because it automatically provides perfect annotation at low cost. However, there are still non-trivial domain gaps, such as differences in textures/materials, between synthetic and real data. These gaps have a measurable impact on performance. To solve this problem, we introduce a simulation to reality (sim2real) instance-level style transfer for 6D pose estimation network training. Our approach transfers the style of target objects individually, from synthetic to real, without human intervention. This improves the quality of synthetic data for training pose estimation networks. We also propose a complete pipeline from data collection to the training of a pose estimation network and conduct extensive evaluation on a real-world robotic platform. Our evaluation shows significant improvement achieved by our method in both pose estimation performance and the realism of images adapted by the style transfer.},
  archive   = {C_IROS},
  author    = {Takuya Ikeda and Suomi Tanishige and Ayako Amma and Michael Sudano and Hervé Audren and Koichi Nishiwaki},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981878},
  pages     = {3225-3232},
  title     = {Sim2Real instance-level style transfer for 6D pose estimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DGBench: An open-source, reproducible benchmark for dynamic
grasping. <em>IROS</em>, 3218–3224. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces DGBench, a fully reproducible open-source testing system to enable benchmarking of dynamic grasping in environments with unpredictable relative motion between robot and object. We use the proposed benchmark to compare several visual perception arrangements. Traditional perception systems developed for static grasping are unable to provide feedback during the final phase of a grasp due to sensor minimum range, occlusion, and a limited field of view. A multi-camera eye-in-hand perception system is presented that has advantages over commonly used camera configurations. We quantitatively evaluate the performance on a real robot with an image-based visual servoing grasp controller and show a significantly improved success rate on a dynamic grasping task.},
  archive   = {C_IROS},
  author    = {Ben Burgess-Limerick and Chris Lehnert and Jürgen Leitner and Peter Corke},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981670},
  pages     = {3218-3224},
  title     = {DGBench: An open-source, reproducible benchmark for dynamic grasping},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 3D visual-based tension control in strip-like deformable
objects using a catenary model. <em>IROS</em>, 3210–3217. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, there has been a growing interest in robotic manipulation of deformable objects. In order to perform certain tasks, the robot must control the shape of the object while taking care not to apply excessive stresses so as not to deform it irreversibly. This is the case when extracting elasto-plastic objects in strips from an industrial reel. In order to control the mechanical stresses within the object, we propose a vision-based control scheme to minimize tension by regulating the angular velocity of a motorized reel on which they are wound. In this paper, we propose a method, based on a catenary model and visual feedback from a low-cost RGB-D camera, to estimate the tension distribution along a rubber strip. Thus, the control strategy aims to achieve a desired tension value by varying the length of the suspended portion of the manipulated strip. Simulation and experimental results validate the proposed approach for strip-like objects of various dimensions.},
  archive   = {C_IROS},
  author    = {N. Roca Filella and A. Koessler and B.C. Bouzgarrou and J.-A. Corrales Ramon},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982197},
  pages     = {3210-3217},
  title     = {3D visual-based tension control in strip-like deformable objects using a catenary model},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tactile-guided dynamic object planar manipulation.
<em>IROS</em>, 3203–3209. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Planar pushing is a fundamental robot manipulation task with most algorithms built upon the quasi-static as-sumption. Under this assumption the end-effector should apply force on the pushed object along the full moving trajectory. This means that the target position must lie in the robot&#39;s workspace. To enable a robot to deliver objects outside of its workspace and facilitate faster delivery, the quasi-static assumption should be lifted in favour of dynamical manipulation. In this work, we propose a two-staged data-driven manipulation method to hit an unknown object to reach a target position. This expands the reachability of the manipulated object beyond the robot&#39;s workspace. The robot equipped with a tactile sensor first explores for the stable pushing region (SPR) on the given object by using a gain-scheduling PD control with the contact centre estimated to maintain full contact between the object and the end-effector. In the second stage, a learning-based approach is used to generate the impulse the object should receive at the SPR to reach a target sliding distance. The performance of proposed method is evaluated on a KUKA LBR iiwa 14 R820 robot manipulator and a XELA tactile sensor.},
  archive   = {C_IROS},
  author    = {Boyuan Liang and Wenyu Liang and Yan Wu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981270},
  pages     = {3203-3209},
  title     = {Tactile-guided dynamic object planar manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fixture-aware DDQN for generalized environment-enabled
grasping. <em>IROS</em>, 3151–3158. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper expands on the problem of grasping an object that can only be grasped by a single parallel gripper when a fixture (e.g., wall, heavy object) is harnessed. Preceding work that tackle this problem are limited in that the employed networks implicitly learn specific targets and fixtures to leverage. However, the notion of a usable fixture can vary in different environments, at times without any outwardly noticeable differences. In this paper, we propose a method to relax this limitation and further handle environments where the fixture location is unknown. The problem is formulated as visual affordance learning in a partially observable setting. We present a self-supervised reinforcement learning algorithm, Fixture-Aware Double Deep Q-Network (FA-DDQN), that processes the scene observation to 1) identify the target object based on a reference image, 2) distinguish possible fixtures based on interaction with the environment, and finally 3) fuse the information to generate a visual affordance map to guide the robot to successful Slide-to-Wall grasps. We demonstrate our proposed solution in simulation and in real robot experiments to show that in addition to achieving higher success than baselines, it also performs zero-shot generalization to novel scenes with unseen object configurations.},
  archive   = {C_IROS},
  author    = {Eddie Sasagawa and Changhyun Choi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982182},
  pages     = {3151-3158},
  title     = {Fixture-aware DDQN for generalized environment-enabled grasping},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Development of pneumatically driven tensegrity manipulator
without mechanical springs. <em>IROS</em>, 3145–3150. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper reports a tensegrity manipulator driven by 40 pneumatic cylinders without mechanical springs. In general, tensegrity robots use mechanical springs to achieve a stable/curved tensegrity structure, and this is true even when a component extends/retracts with an actuator. The stiffness of the mechanical spring should be high to increase the stiffness of the entire structure and improve the control response, but low to deform the structure. This fact means that the introduction of mechanical springs causes serious trade-offs in its design and control. In this study, we use pneumatic actuators not only for active deformation but also for passive. In this paper, we introduce the design and control system and then show the difference in response characteristics between the case with and without a spring, demonstrating the importance of the approach without a mechanical spring.},
  archive   = {C_IROS},
  author    = {Yuhei Yoshimitsu and Kenta Tsukamoto and Shuhei Ikemoto},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982208},
  pages     = {3145-3150},
  title     = {Development of pneumatically driven tensegrity manipulator without mechanical springs},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MasKGrasp: Mask-based grasping for scenes with multiple
general real-world objects. <em>IROS</em>, 3137–3144. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce a mask-based grasping method that discerns multiple objects within the scene regard-less of transparency or specularity and finds the optimal grasp position avoiding clutter. Conventional vision-based robotic grasping approaches often fail to extend to the scenes containing transparent objects due to their different visual appearance. To handle the different visual characteristics, we first segment both transparent and opaque objects into instance masks, which serve as the domain-agnostic intermediate representation of both object types, using a neural network. While there exists no labelled training dataset that strongly represents both object types, we overcome the limitation by augmenting transparent objects on an existing large-scale dataset. Then, given the object instance masks, our method selects the top K discrete masks and robustly estimates grasp poses avoiding clutter. Through experiments, we verify that the instance masks are light-weight yet provide sufficient information for vision-based grasping agnostic of various appearances. On an unseen real-world test environment with complex objects, our method substantially outperforms previous methods without fine-tuning.},
  archive   = {C_IROS},
  author    = {Junho Lee and Junhwa Hur and Inwoo Hwang and Young Min Kim},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982130},
  pages     = {3137-3144},
  title     = {MasKGrasp: Mask-based grasping for scenes with multiple general real-world objects},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal nonprehensile interception strategy for objects in
flight. <em>IROS</em>, 3130–3136. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Intercepting an object in flight through nonpre-hensile manipulation is a challenging problem, which is aimed at catching and stopping a flying object using little contacts without completely restraining its relative motion to the robot. This paper presents a two-stage optimal trajectory generation method to tackle this problem. At the pre-catching stage, optimal position and attitude trajectories of the robot&#39;s end-effector to approach the object are generated by a variational method. At the post-catching stage, the end-effector&#39;s trajectories are generated to optimally eliminate the translational and rotational motion of the object and a convex-MPC algorithm combined with admittance control is used to realize the trajectory tracking. A series of simulations and experiments have been conducted to verify the effectiveness of the proposed method.},
  archive   = {C_IROS},
  author    = {Cheng Zhou and Yanbo Long and Ying Cao and Longfei Zhao and Bidan Huang and Yu Zheng},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981714},
  pages     = {3130-3136},
  title     = {Optimal nonprehensile interception strategy for objects in flight},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel simulation-based quality metric for evaluating
grasps on 3D deformable objects. <em>IROS</em>, 3123–3129. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Evaluation of grasps on deformable $3\mathrm{D}$ objects is a little-studied problem, even if the applicability of rigid object grasp quality measures for deformable ones is an open question. A central issue with most quality measures is their dependence on contact points, which for deformable objects depend on the deformations. This paper proposes a grasp quality measure for deformable objects that uses information about object deformation to calculate the grasp quality. Grasps are evaluated by simulating the deformations during grasping and predicting the contacts between the gripper and the grasped object. The contact information is then used as input for a new grasp quality metric to quantify the grasp quality. The approach is benchmarked against two classical rigid-body quality metrics on over 600 grasps in the Isaac gym simulation and over 50 real-world grasps. Experimental results show an average improvement of 18\% in the grasp success rate for deformable objects compared to the classical rigid-body quality metrics. Furthermore, the proposed approach is approximately fifteen times faster to calculate than the shake task, which, to date, is one of the most reliable approaches to quantify a grasp on a deformable object.},
  archive   = {C_IROS},
  author    = {Tran Nguyen Le and Jens Lundell and Fares J. Abu-Dakka and Ville Kyrki},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981169},
  pages     = {3123-3129},
  title     = {A novel simulation-based quality metric for evaluating grasps on 3D deformable objects},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Absolute position detection in 7-phase sensorless electric
stepper motor. <em>IROS</em>, 3115–3122. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Absolute position detection in sensorless electric stepper motors potentially allows for higher space efficiency, improved shock resistance, simplified installation, reduced number of parts and lowered cost. A prototype is demonstrated measuring 42 × 42 × 34 mm 3 with seven coils arranged in a star configuration. The rotor is ϕ 25.8 × 12.5 mm 2 and has 51 teeth which are irregularly spaced. At the driver side, the coil currents are measured during motion in order to reconstruct the absolute position of the motor. Calibration and smoothing techniques are used to reduce systematic and stochastic measurement errors, respectively. The motor is able to detect and correct its position after externally-induced stalls at the tested motor speeds from 40 rpm to 108 rpm. The holding torque is 0.23 N m at an armature current of 1 A; on average the torque is 7\% lower than that of a reference bipolar stepper motor with the same dimensions. The results show that dynamic position sensing and correction are possible for a range of velocities, but not at standstill. The driver requires seven current sensors and sufficient computational power, and proper calibration of motor intrinsics is required beforehand. The presented technology could make existing 3-D printers and other machines with open-loop stepper motors more robust and increase the range of operating speeds and accelerations, without the adverse side-effects of increased complexity and cost associated with dedicated position sensors.},
  archive   = {C_IROS},
  author    = {Vincent Groenhuis and Gijs Rolff and Koen Bosman and Leon Abelmann and Stefano Stramigioli},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981190},
  pages     = {3115-3122},
  title     = {Absolute position detection in 7-phase sensorless electric stepper motor},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robowflex: Robot motion planning with MoveIt made easy.
<em>IROS</em>, 3108–3114. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robowflex is a software library for robot motion planning in industrial and research applications, leveraging the popular Moveit library and Robot Operating System (ROS) middleware. Robowflex provides an augmented API for crafting and manipulating motion planning queries within a single program, making motion planning with Moveit easy. Robowflex&#39;s high-level API simplifies many common use-cases while still providing low-level access to the Moveit library when needed. Robowflex is particularly useful for 1) developing new motion planners, 2) evaluating motion planners, and 3) complex problems that use motion planning as a subroutine (e.g., task and motion planning). Robowflex also provides visualization capabilities, integrations to other robotics libraries (e.g., DART and Tesseract), and is complementary to other robotics packages. With our library, the user does not need to be an expert at ROS or Moveit to set up motion planning queries, extract information from results, and directly interface with a variety of software components. We demonstrate its efficacy through several example use-cases.},
  archive   = {C_IROS},
  author    = {Zachary Kingston and Lydia E. Kavraki},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981698},
  pages     = {3108-3114},
  title     = {Robowflex: Robot motion planning with MoveIt made easy},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gesture2Vec: Clustering gestures using representation
learning methods for co-speech gesture generation. <em>IROS</em>,
3100–3107. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Co-speech gestures are a principal component in conveying messages and enhancing interaction experiences between humans and critical ingredients in human-agent interaction, including virtual agents and robots. Existing machine learning approaches have yielded only marginal success in learning speech-to-motion at the frame level. Current methods generate repetitive gesture sequences that lack appropriateness with respect to the speech context. To tackle this challenge, we take inspiration from successes in natural language processing on context and long-term dependencies, and propose a new framework that views text-to-gesture as machine translation, where gestures are words in another (non-verbal) language. We propose a vector-quantized variational autoencoder structure as well as training techniques to learn a rigorous representation of gesture sequences. We then translate input text into a discrete sequence of associated gesture chunks in the learned gesture space. Ultimately, we use translated gesture tokens from the input text as an input to the autoencoder&#39;s decoder to produce gesture sequences. Subjective and objective evaluations confirm the success of our approach in terms of appropriateness, human-likeness, and diversity. We also introduce new objective metrics using the quantized gesture representation.},
  archive   = {C_IROS},
  author    = {Payam Jome Yazdian and Mo Chen and Angelica Lim},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981117},
  pages     = {3100-3107},
  title     = {Gesture2Vec: Clustering gestures using representation learning methods for co-speech gesture generation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intuitive &amp; efficient human-robot collaboration via
real-time approximate bayesian inference. <em>IROS</em>, 3093–3099. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The combination of collaborative robots and end-to-end AI, promises flexible automation of human tasks in factories and warehouses. However, such promise seems a few breakthroughs away. In the meantime, humans and cobots will collaborate helping each other. For these collaborations to be effective and safe, robots need to model, predict and exploit human&#39;s intents for responsive decision making processes. Approximate Bayesian Computation (ABC) is approach to perform probabilistic predictions upon uncertain quantities. ABC includes priors conveniently, leverages sampling algorithms for inference and is flexible to benefit from complex models, e.g. via simulators. However, ABC is known to be computationally too intensive to run at interactive frame rates required for effective human-robot collaboration tasks. In this paper, we formulate human intent prediction as an ABC problem and describe two key performance innovations which allow computations at interactive rates. Our real-world experiments with a collaborative robot set-up, demonstrate the viability of our proposed approach. Experimental evaluations convey the advantages and value of human intent prediction for packing cooperative tasks. Qualitative results show how anticipating human&#39;s intent improves human-robot collaboration without compromising safety. Quantitative task fluency metrics confirm the qualitative claims.},
  archive   = {C_IROS},
  author    = {Javier Felip and David Gonzalez-Aguirre and Lama Nachman},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982251},
  pages     = {3093-3099},
  title     = {Intuitive &amp; efficient human-robot collaboration via real-time approximate bayesian inference},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robotic detection of a human-comprehensible gestural
language for underwater multi-human-robot collaboration. <em>IROS</em>,
3085–3092. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a motion-based robotic communication framework that enables non-verbal communication among autonomous underwater vehicles (AUVs) and human divers. We design a gestural language for AUV-to-AUV communication which can be easily understood by divers observing the conversation - unlike typical radio frequency, light, or audio-based AUV communication. To allow AUVs to visually understand a gesture from another AUV, we propose a deep network (RRCommNet) which exploits a self-attention mechanism to learn to recognize each message by extracting maximally discriminative spatio-temporal features. We train this network on diverse simulated and real-world data. Our experimental evaluations, both in simulation and in closed-water robot trials, demonstrate that the proposed RRCommNet architecture is able to decipher gesture-based messages with an average accuracy of 88-94\% on simulated data and 73-83\% on real data (depending on the version of the model used). Further, by performing a message transcription study with human participants, we also show that the proposed language can be understood by humans with an overall transcription accuracy of 88\%. Finally, we discuss the inference runtime of RRCommNet on embedded GPU hardware, for real-time use on board AUVs in the field.},
  archive   = {C_IROS},
  author    = {Sadman Sakib Enan and Michael Fulton and Junaed Sattar},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981450},
  pages     = {3085-3092},
  title     = {Robotic detection of a human-comprehensible gestural language for underwater multi-human-robot collaboration},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robot dance generation with music based trajectory
optimization. <em>IROS</em>, 3069–3076. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Musical dancing is an ubiquitous phenomenon in the human society. Providing robots the ability to dance has the potential to make the human robot co-existence more acceptable in our society. Hence, dancing robots have generated a considerable research interest in the recent years. In this paper, we present a novel formalization of robot dancing as planning and control of optimally timed actions based on beat timings and additional features extracted from the music. We showcase the use of this formulation in three different variations: with input of human expert choreography, imitation of a predefined choreography, and automated generation of a novel choreography. Our method has been validated on four different musical pieces, both in simulation and on a real robot, using the upper-body humanoid robot RH5 Manus.},
  archive   = {C_IROS},
  author    = {Melya Boukheddimi and Daniel Harnack and Shivesh Kumar and Rohit Kumar and Shubham Vyas and Octavio Arriaga and Frank Kirchner},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981462},
  pages     = {3069-3076},
  title     = {Robot dance generation with music based trajectory optimization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robot learning to paint from demonstrations. <em>IROS</em>,
3053–3060. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic painting tasks in the real world are often made complicated by the highly complex and stochastic nature of the dynamics that underlie, e.g., physical contact between the painting tool and a canvas, color blendings between painting mediums, and many more. Simulation-based inverse graphics algorithms, for example, can not be directly transferred to the real-world due in large to the considerable gap in the viable range of painting strokes the robot can accurately generate onto the physical canvas. In this paper, we aim at minimizing this gap by appealing to a data-driven skill learning approach. The core idea lies in allowing the robot to learn continuous stroke-level skills that jointly encodes action trajectories and painted outcomes from an extensive collection of human demonstrations. We demonstrate the efficacy of our method through extensive real-world experiments using a 4-dof torque-controllable manipulator with a digital canvas(iPad).},
  archive   = {C_IROS},
  author    = {Younghyo Park and Seunghun Jeon and Taeyoon Lee},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981633},
  pages     = {3053-3060},
  title     = {Robot learning to paint from demonstrations},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automated design of task specific additively manufacturable
coupled serial chain mechanisms for tracing predefined planar
trajectories. <em>IROS</em>, 3047–3052. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents the automatic design of additively manufacturable serially linked one degree of freedom manipulators whose end effectors move along individually prescribed 2D trajectories. The kinematic coupling of the links is done by using gear stages consisting of spur gears and toothed belt gears. The basic design parameters of these mechanisms are determined using a Fourier series. The calculated Fourier elements with their respective frequency, amplitude and phase are interpreted as rotating 2D vectors which represent the manipulator links. Based on this, the kinematic coupling of the links is calculated and the corresponding gears are designed. All parts of these mechanisms, including the toothed belts, can be manufactured using a low cost 3D printing process. The software for the automated design of these manipulators from Fourier decomposition to CAD file generation has been implemented in MATLAB. To validate the automated design process, various test mechanisms were manufactured and examined for accuracy and precision.},
  archive   = {C_IROS},
  author    = {Simon Schiele and Sebastian Baumgartner and Simon Laudahn and Tim C. Lueth},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981723},
  pages     = {3047-3052},
  title     = {Automated design of task specific additively manufacturable coupled serial chain mechanisms for tracing predefined planar trajectories},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Geometric savitzky-golay filtering of noisy rotations on
SO(3) with simultaneous angular velocity and acceleration estimation.
<em>IROS</em>, 2962–2968. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper focuses on the problem of smoothing a rotation trajectory corrupted by noise, while simultaneously estimating its corresponding angular velocity and angular acceleration. To this end, we develop a geometric version of the Savitzky-Golay filter on SO(3) that avoids following the conventional practice of first converting the rotation trajectory into Euler-like angles, performing the filtering in this new set of local coordinates, and finally converting the result back on SO (3). In particular, the estimation of the angular acceleration requires the computation of the right-trivialized second covariant derivative of the exponential map on SO (3) with respect to the (+) Cartan-Schouten connection. We provide an explicit expression for this derivative, creating a link to seemingly unrelated existing results concerning the first derivative of the exponential map on SE (3). A numerical example is provided in which we demonstrate the effectiveness and straightforward applicability of the proposed approach. An open implementation of the new geometric Savitzky-Golay filter is also provided.},
  archive   = {C_IROS},
  author    = {Maarten Jongeneel and Alessandro Saccon},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981409},
  pages     = {2962-2968},
  title     = {Geometric savitzky-golay filtering of noisy rotations on SO(3) with simultaneous angular velocity and acceleration estimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to herd amongst obstacles from an optimized
surrogate. <em>IROS</em>, 2954–2961. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper investigates how a shepherd robot can efficiently steer a coherent group by intelligently moving behind the group in obstacle-filled environments. It was been shown that a model trained by deep reinforcement learning can guide a small number (2–4) of agents among obstacles. However, herding a larger group becomes significantly more challenging because it exhibits the characteristics similar to manipulating a deformable object, i.e., the system is dynamic and the problem is highly underactuated. To overcome these challenges, we show that a model can be trained more effectively via an optimized surrogate, such as a potential field that optimizes the control quality of the group without explicitly considering the placement of the shepherd. Our experiments demonstrate that the trained model is robust to noise for group behaviors and environments. Compared to the rule-based method, the proposed approach maintains a higher probability of guiding the sheep and better control quality.},
  archive   = {C_IROS},
  author    = {Jixuan Zhi and Jyh-Ming Lien},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982269},
  pages     = {2954-2961},
  title     = {Learning to herd amongst obstacles from an optimized surrogate},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Energy-aware planning-scheduling for autonomous aerial
robots. <em>IROS</em>, 2946–2953. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present an online planning-scheduling approach for battery-powered autonomous aerial robots. The approach consists of simultaneously planning a coverage path and scheduling onboard computational tasks. We further derive a novel variable coverage motion robust to air-borne constraints and an empirically motivated energy model. The model includes the energy contribution of the schedule based on an automatic computational energy modeling tool. Our experiments show how an initial flight plan is adjusted online as a function of the available battery, accounting for uncertainty. Our approach remedies possible in-flight failure in case of unexpected battery drops, e.g., due to adverse atmospheric conditions, and increases the overall fault tolerance.},
  archive   = {C_IROS},
  author    = {Adam Seewald and Héctor García de Marina and Henrik Skov Midtiby and Ulrik Pagh Schultz},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981285},
  pages     = {2946-2953},
  title     = {Energy-aware planning-scheduling for autonomous aerial robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic replanning with posterior sampling. <em>IROS</em>,
2938–2945. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When navigating to a goal in an uncertain environment, a robot must simultaneously navigate the exploration-exploitation tradeoff: should it aim to gain information and reduce uncertainty, or should it simply brave the unknown? We formalize this as the Bayesian dynamic motion planning problem, and we analyze how several strategies from the literature balance these concerns via determinization and planning. Within the framework of determinization in the face of uncertainty, we shift the burden of exploration to determinization rather than planning. Dynamic Replanning with Posterior Sampling (DRPS) is very efficient: each iteration consists of a single posterior update and a shortest path query. Relative to comparative baselines across seven datasets of 2D planning problems, DRPS has a higher percentage of success, traverses lower or comparable total distances, and accelerates total planning time by 4–7×. Across a dataset of larger 7D Baxter manipulator planning problems, DRPS reduces total distance by 40\% and total planning time by 18×.},
  archive   = {C_IROS},
  author    = {Brian Hou and Siddhartha S. Srinivasa},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981617},
  pages     = {2938-2945},
  title     = {Dynamic replanning with posterior sampling},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Risk-aware off-road navigation via a learned speed
distribution map. <em>IROS</em>, 2931–2937. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion planning in off-road environments re-quires reasoning about both the geometry and semantics of the scene (e.g., a robot may be able to drive through soft bushes but not a fallen log). In many recent works, the world is classified into a finite number of semantic categories that often are not sufficient to capture the ability (i.e., the speed) with which a robot can traverse off-road terrain. Instead, this work proposes a new representation of traversability based exclusively on robot speed that can be learned from data, offers interpretability and intuitive tuning, and can be easily integrated with a variety of planning paradigms in the form of a costmap. Specifically, given a dataset of experienced trajectories, the proposed algorithm learns to predict a distribution of speeds the robot could achieve, conditioned on the environment semantics and commanded speed. The learned speed distribution map is converted into costmaps with a risk-aware cost term based on conditional value at risk (CVaR). Numerical simulations demonstrate that the proposed risk-aware planning algorithm leads to faster average time-to-goals compared to a method that only considers expected behavior, and the planner can be tuned for slightly slower, but less variable behavior. Furthermore, the approach is integrated into a full autonomy stack and demonstrated in a high-fidelity Unity environment and is shown to provide a 30\% improvement in the success rate of navigation.},
  archive   = {C_IROS},
  author    = {Xiaoyi Cai and Michael Everett and Jonathan Fink and Jonathan P. How},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982200},
  pages     = {2931-2937},
  title     = {Risk-aware off-road navigation via a learned speed distribution map},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Terrain-aware learned controllers for sampling-based
kinodynamic planning over physically simulated terrains. <em>IROS</em>,
2925–2930. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper explores learning an effective controller for improving the efficiency of kinodynamic planning for vehicular systems navigating uneven terrains. It describes the pipeline for training the corresponding controller and using it for motion planning purposes. The training process uses a soft actor-critic approach with hindsight experience replay to train a model, which is parameterized by the incline of the robot&#39;s local terrain. This trained model is then used during the expansion process of an asymptotically optimal kinodynamic planner to generate controls that allow the robot to reach desired local states. It is also used to define a heuristic cost-to-go function for the planner via a wavefront operation that estimates the cost of reaching the global goal. The cost-to-go function is used both for selecting nodes for expansion as well as for generating local goals for the controller to expand towards. The accompanying experimental section applies the integrated planning solution on models of all-terrain robots in a variety of physically simulated terrains. It shows that the proposed terrain-aware controller and the proposed wavefront function based on the cost-to-go model enable motion planners to find solutions in less time and with lower cost than alternatives. An ablation study emphasizes the benefits of a learned controller that is parameterized by the incline of the robot&#39;s local terrain as well as of an incremental training process for the controller.},
  archive   = {C_IROS},
  author    = {Troy McMahon and Aravind Sivaramakrishnan and Kushal Kedia and Edgar Granados and Kostas E. Bekris},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982136},
  pages     = {2925-2930},
  title     = {Terrain-aware learned controllers for sampling-based kinodynamic planning over physically simulated terrains},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Comprehensive reactive safety: No need for a trajectory if
you have a strategy. <em>IROS</em>, 2903–2910. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safety guarantees in motion planning for autonomous driving typically involve certifying the trajectory to be collision-free under any motion of the uncontrollable participants in the environment, such as the human-driven vehicles on the road. As a result they usually employ a conservative bound on the behavior of such participants, such as reachability analysis. We point out that planning trajectories to rigorously avoid the entirety of the reachable regions is unnecessary and too restrictive, because observing the environment in the future will allow us to prune away most of them; disregarding this ability to react to future updates could prohibit solutions to scenarios that are easily navigated by human drivers. We propose to account for the autonomous vehicle&#39;s reactions to future environment changes by a novel safety framework, Comprehensive Reactive Safety. Validated in simulations in several urban driving scenarios such as unprotected left turns and lane merging, the resulting planning algorithm called Reactive ILQR demonstrates strong negotiation capabilities and better safety at the same time.},
  archive   = {C_IROS},
  author    = {Fang Da},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981757},
  pages     = {2903-2910},
  title     = {Comprehensive reactive safety: No need for a trajectory if you have a strategy},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pose refinement with joint optimization of visual points and
lines. <em>IROS</em>, 2888–2894. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {High-precision camera re-localization technology in a pre-established 3D environment map is the basis for many tasks, such as Augmented Reality, Robotics and Autonomous Driving. The point-based visual re-localization approaches are well-developed in recent decades, but are insufficient in some feature-less cases. In this paper, we design a complete pipeline for camera pose refinement with points and lines, which contains the innovatively designed line extracting CNN named VLSE, the line matching and the pose optimization approaches. We adopt a novel line representation and customize a hybrid convolution block based on the Stacked Hourglass network [1], to detect accurate and stable line features on images. Then we apply a geometric-based strategy to obtain precise 2D-3D line correspondences using epipolar constraint and reprojection filtering. A following point-line joint cost function is constructed to optimize the camera pose with the initial coarse pose from the pure point-based localization. Sufficient experiments are conducted on open datasets, i.e, line extractor on Wireframe and YorkUrban, localization performance on InLoc ducl and duc2, to confirm the effectiveness of our point-line joint pose optimization method.},
  archive   = {C_IROS},
  author    = {Shuang Gao and Jixiang Wan and Yishan Ping and Xudong Zhang and Shuzhou Dong and Yuchen Yang and Haikuan Ning and Jijunnan Li and Yandong Guo},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981420},
  pages     = {2888-2894},
  title     = {Pose refinement with joint optimization of visual points and lines},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gaussian variational inference with covariance constraints
applied to range-only localization. <em>IROS</em>, 2872–2879. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate and reliable state estimation is becoming increasingly important as robots venture into the real world. Gaussian variational inference (GVI) is a promising alternative for nonlinear state estimation, which estimates a full probability density for the posterior instead of a point estimate as in maximum a posteriori (MAP)-based approaches. GVI works by optimizing for the parameters of a multivariate Gaussian (MVG) that best agree with the observed data. However, such an optimization procedure must ensure the parameter constraints of a MVG are satisfied; in particular, the inverse covariance matrix must be positive definite. In this work, we propose a tractable algorithm for performing state estimation using GVI that guarantees that the inverse covariance matrix remains positive definite and is well-conditioned throughout the optimization procedure. We evaluate our method extensively in both simulation and real-world experiments for range-only localization. Our results show GVI is consistent on this problem, while MAP is over-confident.},
  archive   = {C_IROS},
  author    = {Abhishek Goudar and Wenda Zhao and Timothy D. Barfoot and Angela P. Schoellig},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981520},
  pages     = {2872-2879},
  title     = {Gaussian variational inference with covariance constraints applied to range-only localization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semantic topological descriptor for loop closure detection
within 3D point clouds in outdoor environment. <em>IROS</em>, 2856–2863.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Loop closure detection has the potential to correct the drift of trajectories and build a global consistent map in LiDAR SLAM, however it remains a challenging problem in outdoor environment due to the sparsity of 3D point clouds data, large-scale scenes and moving objects. Inspired by the way humans perceive the environment through recognizing objects and identifying their relations, this paper presents a novel descriptor that contains semantic and topological information for loop closure detection. Unlike most existing methods that extract features from the raw point clouds or use all semantic objects, we directly discard point clouds representing pedestrians and vehicles after semantic segmentation. Then, we propose a semantic topological graph representation from the remaining point clouds and convert this graph into a descriptor. Additionally, we propose a two-stage algorithm for matching descriptors to efficiently determine the loop. Our method has been extensively evaluated using the KITTI dataset and outperforms state-of-the-art methods, especially in the challenging situations such as viewpoint changes and dynamic scenes.},
  archive   = {C_IROS},
  author    = {Ming Liao and Yunzhou Zhang and Jinpeng Zhang and Liang Liang and Sonya Coleman and Dermot Kerr},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981965},
  pages     = {2856-2863},
  title     = {Semantic topological descriptor for loop closure detection within 3D point clouds in outdoor environment},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ROLL: Long-term robust LiDAR-based localization with
temporary mapping in changing environments. <em>IROS</em>, 2841–2847.
(<a href="https://doi.org/10.1109/IROS47612.2022.9982153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Long-term scene changes pose challenges to localization systems using a pre-built map. This paper presents a LiDAR-based system that provides robust localization against those challenges. Our method starts with activation of a mapping process temporarily when global matching towards the pre-built map is unreliable. The temporary map will be merged onto the pre-built map for later localization sessions once reliable matching is obtained again. We further integrate a LiDAR inertial odometry (LIO) to provide motion-compensated LiDAR scans and a reliable pose initial estimate for the global matching module. To generate a smooth real-time trajectory for navigation purposes, we fuse poses from odometry and global matching by solving a pose graph optimization problem. We evaluate our localization system with extensive experiments on the NCLT dataset including a variety of changing indoor and outdoor environments, and the results demonstrate a robust and accurate long-term localization performance. The implementations are open sourced on GitHub 1 1 https://github.com/HaisenbergPeng/ROLL.},
  archive   = {C_IROS},
  author    = {Bin Peng and Hongle Xie and Weidong Chen},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982153},
  pages     = {2841-2847},
  title     = {ROLL: Long-term robust LiDAR-based localization with temporary mapping in changing environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SLAM-supported self-training for 6D object pose estimation.
<em>IROS</em>, 2833–2840. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent progress in object pose prediction provides a promising path for robots to build object-level scene representations during navigation. However, as we deploy a robot in novel environments, the out-of-distribution data can degrade the prediction performance. To mitigate the domain gap, we can potentially perform self-training in the target domain, using predictions on robot-captured images as pseudo labels to fine-tune the object pose estimator. Unfortunately, the pose predictions are typically outlier-corrupted, and it is hard to quantify their uncertainties, which can result in low-quality pseudo-labeled data. To address the problem, we propose a SLAM-supported self-training method, leveraging robot understanding of the 3D scene geometry to enhance the object pose inference performance. Combining the pose predictions with robot odometry, we formulate and solve pose graph optimization to refine the object pose estimates and make pseudo labels more consistent across frames. We incorporate the pose prediction covariances as variables into the optimization to automatically model their uncertainties. This automatic covariance tuning (ACT) process can fit 6D pose prediction noise at the component level, leading to higher-quality pseudo training data. We test our method with the deep object pose estimator (DOPE) on the YCB video dataset and in real robot experiments. It achieves respectively 34.3\% and 17.8\% accuracy enhancements in pose prediction on the two tests. Our code is available at https://github.com/520xyxyzq/slam-super-6d.},
  archive   = {C_IROS},
  author    = {Ziqi Lu and Yihao Zhang and Kevin Doherty and Odin Severinsen and Ethan Yang and John Leonard},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981145},
  pages     = {2833-2840},
  title     = {SLAM-supported self-training for 6D object pose estimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LayoutSLAM: Object layout based simultaneous localization
and mapping for reducing object map distortion. <em>IROS</em>,
2825–2832. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There is an increasing demand for robots that can be substituted for humans in various tasks. Mobile robots are being introduced in factories, stores, and public facilities for carrying goods and cleaning. In factories and stores, desks and shelves are arranged such that the work and movement of personnel are reduced. The surrounding furniture is also set to ensure that a single task can be performed in the same place. It is essential to study the intelligence of robots using information from such layouts, wherein human labor and movements are optimized. However, There is no method of map construction or location estimation that uses the characteristics of furniture arrangements that facilitate human work in a work space. Therefore, this study proposes a method for object mapping using layouts in crowded workspaces. Graphically represent the characteristics of furniture placement that make it easy for people to work in a workspace. The links in the graph represent the connections between the objects in the layout property. The nodes are the objects, and the weights of the links represent the strength of the layout properties. This graph is optimized by GraphSLAM to construct a map that considers the arrangement&#39;s characteristics. Using the graph structure improves the map&#39;s accuracy while allowing for relative changes in placement. The results show a 50.44\% improvement in accuracy in a space with 18 desks, followed by two variations of similar desk layouts. The same improvement in accuracy was also observed when the relative positioning of objects changed significantly in each variation, such as a change to the left or right on the same side.},
  archive   = {C_IROS},
  author    = {Kenta Gunji and Kazunori Ohno and Shotaro Kojima and Ranulfo Bezerra and Yoshito Okada and Masashi Konyo and Satoshi Tadokoro},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981492},
  pages     = {2825-2832},
  title     = {LayoutSLAM: Object layout based simultaneous localization and mapping for reducing object map distortion},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust change detection based on neural descriptor fields.
<em>IROS</em>, 2817–2824. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to reason about changes in the environment is crucial for robots operating over extended periods of time. Agents are expected to capture changes during operation so that actions can be followed to ensure a smooth progression of the working session. However, varying viewing angles and accumulated localization errors make it easy for robots to falsely detect changes in the surrounding world due to low observation overlap and drifted object associations. In this paper, based on the recently proposed category-level Neural Descriptor Fields (NDFs), we develop an object-level online change detection approach that is robust to partially overlapping observations and noisy localization results. Utilizing the shape completion capability and SE(3)-equivariance of NDFs, we represent objects with compact shape codes encoding full object shapes from partial observations. The objects are then organized in a spatial tree structure based on object centers recovered from NDFs for fast queries of object neighborhoods. By associating objects via shape code similarity and comparing local object-neighbor spatial layout, our proposed approach demonstrates robustness to low observation overlap and localization noises. We conduct experiments on both synthetic and real-world sequences and achieve improved change detection results compared to multiple baseline methods. Project web-page: ?http://yilundu.github.io/ndf_change},
  archive   = {C_IROS},
  author    = {Jiahui Fu and Yilun Du and Kurran Singh and Joshua B. Tenenbaum and John J. Leonard},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981246},
  pages     = {2817-2824},
  title     = {Robust change detection based on neural descriptor fields},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Are we ready for robust and resilient SLAM? A framework for
quantitative characterization of SLAM datasets. <em>IROS</em>,
2810–2816. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reliability of SLAM systems is considered one of the critical requirements in modern autonomous systems. This directed the efforts to developing many state-of-the-art systems, creating challenging datasets, and introducing rigorous metrics to measure SLAM performance. However, the link between datasets and performance in the robustness/resilience context has rarely been explored. In order to fill this void, characterization of the operating conditions of SLAM systems is essential in order to provide an environment for quantitative measurement of robustness and resilience. In this paper, we argue that for proper evaluation of SLAM performance, the characterization of SLAM datasets serves as a critical first step. The study starts by reviewing previous efforts for quantitative characterization of SLAM datasets. Then, the problem of perturbation characterization is discussed and the linkage to SLAM robustness/resilience is established. After that, we pro-pose a novel, generic and extendable framework for quantitative analysis and comparison of SLAM datasets. Additionally, a description of different characterization parameters is provided. Finally, we demonstrate the application of our framework by presenting the characterization results of three SLAM datasets: KITTI, EuroC-MAV, and TUM-VI highlighting the level of insights achieved by the proposed framework.},
  archive   = {C_IROS},
  author    = {Islam Ali and Hong Zhang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981283},
  pages     = {2810-2816},
  title     = {Are we ready for robust and resilient SLAM? a framework for quantitative characterization of SLAM datasets},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Keyframe selection with information occupancy grid model for
long-term data association. <em>IROS</em>, 2786–2793. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As the basics of Visual Simultaneous Localization And Mapping (VSLAM), keyframes play an essential role. In previous works, keyframes are selected according to a series of view change-based strategies for short-term data association (STDA). However, the texture enrichment of frames is always ignored, resulting in the failure of long-term data association (LTDA). In this paper, we propose an information enrichment selection strategy with an information occupancy grid model and a deep descriptor. Frame is expressed by a deep global descriptor for a statistical explainable abstraction, in which the texture enrichment is indicated. Based on the abstraction, an information occupancy grid model is established to measure the information enrichment and the potential LTDA ability. Evaluations on variant datasets are conducted, showing the advantage of our proposed method in terms of keyframe selection and tracking precision. Also, the statistical explainability of the deep descriptor is provided. The proposed keyframe selection strategy can improve LTDA and tracking precision, especially in situations with repeated observations and loop-closures.},
  archive   = {C_IROS},
  author    = {Weinan Chen and Hanjing Ye and Lei Zhu and Chao Tang and Changfei Fu and Yonggang Chen and Hong Zhang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981050},
  pages     = {2786-2793},
  title     = {Keyframe selection with information occupancy grid model for long-term data association},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). One RING to rule them all: Radon sinogram for place
recognition, orientation and translation estimation. <em>IROS</em>,
2778–2785. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {LiDAR-based global localization is a fundamental problem for mobile robots. It consists of two stages, place recognition and pose estimation, which yields the current orientation and translation, using only the current scan as query and a database of map scans. Inspired by the definition of a recognized place, we consider that a good global localization solution should keep the pose estimation accuracy with a lower place density. Following this idea, we propose a novel framework towards sparse place-based global localization, which utilizes a unified and learning-free representation, Radon sinogram (RING), for all sub-tasks. Based on the theoretical derivation, a translation invariant descriptor and an orientation invariant metric are proposed for place recognition, achieving certifiable robustness against arbitrary orientation and large translation between query and map scan. In addition, we also utilize the property of RING to propose a global convergent solver for both orientation and translation estimation, arriving at global localization. Evaluation of the proposed RING based framework validates the feasibility and demonstrates a superior performance even under a lower place density.},
  archive   = {C_IROS},
  author    = {Sha Lu and Xuecheng Xu and Huan Yin and Zexi Chen and Rong Xiong and Yue Wang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981308},
  pages     = {2778-2785},
  title     = {One RING to rule them all: Radon sinogram for place recognition, orientation and translation estimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DH-LC: Hierarchical matching and hybrid bundle adjustment
towards accurate and robust loop closure. <em>IROS</em>, 2770–2777. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A loop closure module plays an important role in visual SLAM systems, which can reduce the accumulat-ed drift. This task faces the challenges of large viewpoint changes and expensive computational costs when optimizing the global map. This paper proposes DH-LC, a novel accurate and robust loop closure method that consists of hierarchical spatial feature matching (HSFM) and hybrid bundle adjustment (HBA). HSFM estimates a reliable relative pose between the query image and the retrieval image in a coarse-to-fine way. Specifically, 3D points are firstly triangulated and then clus-tered according to the spatial distribution. The cluster centers estimate coarse cube-level matching pairs in a larger perception field which can tolerate large viewpoint changes. HBA optimizes the global map efficiently by adaptively selecting incremental bundle adjustment or full bundle adjustment according to the accumulated drift and relative pose verification in the temporal window. Experimental results demonstrate that our proposed method easily detects loops in large viewpoint changes and efficiently optimizes the global map. When compared with the state-of-the-art methods, our method increases loop closure recall and improves SLAM localization accuracy with reducing the accumulated drift.},
  archive   = {C_IROS},
  author    = {Xiongfeng Peng and Zhihua Liu and Qiang Wang and Yun-Tae Kim},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981061},
  pages     = {2770-2777},
  title     = {DH-LC: Hierarchical matching and hybrid bundle adjustment towards accurate and robust loop closure},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MS-cubic: A modularized manufacturing system with
scalability, portability and parallelism modular design suitable for
drilling, welding and picking and feasibility verification through
drilling experiment. <em>IROS</em>, 2762–2769. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The existing manufacturing systems based on processes involving the transportation of workpiece are unsuitable for large products such as air mobility systems. This study proposes a novel ultra-complex manufacturing system called the “Modularized-Structure and Multiple-Points Simultaneous Machining System (MS-cubic)” based on the concept of intelligent space, which simultaneously performs multiple types of machining processes without moving a workpiece. The system can simultaneously process multiple points and flexibly change its workspace by modularizing its structure. This paper presents a discussion on the requirements and constraints to generate a feasible design of the rail module and the machining unit, which are two main elements of MS-cubic. The performance of the prototype MS-cubic is evaluated, and its stiffness is observed to be sufficient to perform drilling. Furthermore, the modularized design of system enables the fluid and electric power supply for the machining process.},
  archive   = {C_IROS},
  author    = {Takehito Yoshida and Amane Toriyama and Shin&#39;ichi Warisawa and Rui Fukui},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981791},
  pages     = {2762-2769},
  title     = {MS-cubic: A modularized manufacturing system with scalability, portability and parallelism modular design suitable for drilling, welding and picking and feasibility verification through drilling experiment},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AssembleRL: Learning to assemble furniture from their point
clouds. <em>IROS</em>, 2748–2753. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The rise of simulation environments has enabled learning-based approaches for assembly planning, which is otherwise a labor-intensive and daunting task. Assembling furniture is especially interesting since furniture are intricate and pose challenges for learning-based approaches. Surprisingly, humans can solve furniture assembly mostly given a 2D snapshot of the assembled product. Although recent years have witnessed promising learning-based approaches for furniture assembly, they assume the availability of correct connection labels for each assembly step, which are expensive to obtain in practice. In this paper, we alleviate this assumption and aim to solve furniture assembly with as little human expertise and supervision as possible. To be specific, we assume the availability of the assembled point cloud, and comparing the point cloud of the current assembly and the point cloud of the target product, obtain a novel reward signal based on two measures: Incorrectness and incompleteness. We show that our novel reward signal can train a deep network to successfully assemble different types of furniture. Code and networks available here: https://github.com/METU-KALFA/AssembleRL.},
  archive   = {C_IROS},
  author    = {Ozgur Aslan and Burak Bolat and Batuhan Bal and Tugba Tumer and Erol Sahin and Sinan Kalkan},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982112},
  pages     = {2748-2753},
  title     = {AssembleRL: Learning to assemble furniture from their point clouds},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-object grasping - efficient robotic picking and
transferring policy for batch picking. <em>IROS</em>, 2741–2747. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In a typical fulfillment center, the order fulfilling process is managed by a warehouse management system (WMS). For efficiency, WMS usually applies batch picking, also called multi-order picking, to collect the same items for multiple orders. Suppose an item appears in multiple orders, instead of repeatedly revisiting the exact picking location multiple times, a picker will be instructed to pick up multiple same items at once and bring them to a sorting station, also called a re-bin station. It is at the re-bin station, where the workers sort the picked items into separate orders. We have seen many robotic technologies being developed for sorting. However, we have not seen any feasible robotic technology for batch picking. Transferring multiple objects between bins is a common task. In robotics, a standard approach is to transfer a single object at a time. However, grasping multiple objects and transferring them at once is more efficient. This paper presents a set of novel strategies for efficiently grasping and transferring multiple objects. The grasping strategies enable a robotic hand to grasp multiple objects by identifying an optimal ready hand configuration (pre-grasp), calculating a flexion synergy based on the desired quantity of objects to be grasped, and utilizing a deep learning model to signal the completion of a grasp. The transferring strategies demonstrate an approach that models the problem as a Markov decision process (MDP) and defines specific grasping actions to efficiently transfer objects when the required quantity is larger than the capability of a single grasp. Using the MDP model, the approach can generate an optimal pick-transfer policy that minimizes the number of transfers. The complete proposed approach has been evaluated in both a simulation environment and on a real robotic system. The proposed approach reduces the number of transfers by 59\% and the number of lifts by 58\% compared to an optimal single object pick-transfer solution.},
  archive   = {C_IROS},
  author    = {Adheesh Shenoy and Tianze Chen and Yu Sun},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981799},
  pages     = {2741-2747},
  title     = {Multi-object grasping - efficient robotic picking and transferring policy for batch picking},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Systematic evaluation and analysis on hybrid strategies of
automatic agent last-mile delivery. <em>IROS</em>, 2733–2740. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper focuses on problems associated with the deployment of automatic agents for last-mile delivery. We propose a framework and methodology to systematically evaluate and compare different hybrid strategies. Performance metrics in agent noise, delivery time, energy consumption, coverage rate, package throughput, and system costs are defined rigorously and modeled mathematically. Using the methodology, we conduct a case study in the city of Boston for four agent delivery strategies, including a hybrid strategy proposed in this paper. The proposed strategy utilizes available space in public transits&#39; cabins during off-peak hours to relocate the agent traveling start locations. Simulations and analyses show that hybrid strategies outperform the Agent-Only delivery strategy in terms of noise exposure, energy consumption, and coverage rate. The performance of hybrid strategies highly depends on the characteristics of the ground transportation methods accompanying agents. Thus, the methods of ground transportation should carefully be examined and selected for each case and strategy in real-world applications.},
  archive   = {C_IROS},
  author    = {Xiaotong Zhang and Abdullatif Al Alsheikh and Kamal Youcef-Toumi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981064},
  pages     = {2733-2740},
  title     = {Systematic evaluation and analysis on hybrid strategies of automatic agent last-mile delivery},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reconstructed student-teacher and discriminative networks
for anomaly detection. <em>IROS</em>, 2725–2732. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Anomaly detection is an important problem in computer vision; however, the scarcity of anomalous samples makes this task difficult. Thus, recent anomaly detection methods have used only “normal images” with no abnormal areas for training. In this work, a powerful anomaly detection method is proposed based on student-teacher feature pyramid matching (STPM), which consists of a student and teacher network. Generative models are another approach to anomaly detection. They reconstruct normal images from an input and compute the difference between the predicted normal and the input. Unfortunately, STPM does not have the ability to generate normal images. To improve the accuracy of STPM, this work uses a student network, as in generative models, to reconstruct normal features. This improves the accuracy; however, the anomaly maps for normal images are not clean because STPM does not use anomaly images for training, which decreases the accuracy of the image-level anomaly detection. To further improve accuracy, a discriminative network trained with pseudo-anomalies from anomaly maps is used in our method, which consists of two pairs of student-teacher networks and a discriminative network. The method displayed high accuracy on the MVTec anomaly detection dataset.},
  archive   = {C_IROS},
  author    = {Shinji Yamada and Satoshi Kamiya and Kazuhiro Hotta},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981509},
  pages     = {2725-2732},
  title     = {Reconstructed student-teacher and discriminative networks for anomaly detection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A compliant thorax design for robustness and elastic energy
exchange in flapping-wing robots. <em>IROS</em>, 2693–2700. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Flapping wing insects benefit from a compliant thorax that provides elastic energy exchange and resiliency to wing collisions. In this paper, we present a flapping wing robot that uses an underactuated compliant transmission inspired by the insect thorax. We developed a novel fabrication method that combines carbon fiber (CF) laminate and soft robotics fabrication techniques for transmission construction. The transmission design is optimized to achieve desired wingstroke requirements and to allow for independent motion of each wing. We validate these design choices in bench-top tests measuring transmission compliance and kinematics. We integrate the transmission with laminate wings and two types of actuation, demonstrating elastic energy exchange and limited lift-off capabilities Lastly, we tested collision mitigation through flapping wing experiments that obstructed the motion of a wing. These experiments demonstrate that an underactuated compliant, transmission can provide resilience and robustness to flapping wing robots.},
  archive   = {C_IROS},
  author    = {Hang Gao and James Lynch and Nick Gravish},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981927},
  pages     = {2693-2700},
  title     = {A compliant thorax design for robustness and elastic energy exchange in flapping-wing robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Amoeba-inspired swimming through isoperimetric modulation of
body shape. <em>IROS</em>, 2685–2692. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work we present the design of a swimming robot that is inspired by the body shape modulation of small microorganisms. Amoebas are small single celled organisms that locomote through deformation and shape change of their body. To achieve similar shape modulation for swimming propulsion in a robot we developed a novel flexible appendage using tape springs. A tape spring is an elongated strip of metal with a curved cross-section that can act as a stiff structure when loaded against the curvature, while it can easily buckle when loaded with the curvature. We develop a tape spring appendage that is capable of freely deforming its perimeter through two actuation inputs. In the first portion of this paper we develop the kinematics of the appendage mechanisms and compare with experiment. Next we present the design of a surface locomoting robot that uses two appendages for propulsion. From the appendage kinematics we derive the local connection vector field for locomotion kinematics and study the optimal gait for forward swimming. Lastly, we demonstrate robot swimming performance in open water conditions. The novel appendage design in this robot is advantageous because it enables omnidirectional movement, the appendages will not tangle in debris, and they are robust to collisions and contact with structures.},
  archive   = {C_IROS},
  author    = {Curtis Sparks and Nathan Justus and Ross Hatton and Nick Gravish},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982120},
  pages     = {2685-2692},
  title     = {Amoeba-inspired swimming through isoperimetric modulation of body shape},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Development of a stingray-inspired high-frequency propulsion
platform with variable wavelength. <em>IROS</em>, 2679–2684. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Undulatory fin motions in fish-like robots are typically created using intricate arrays of servo motors. Motor arrays offer impressive versatility in terms of kinematics, but their complexity leads to constraints on size, hydrodynamic force production, and power consumption, particularly when studying propulsive performance at high-frequencies. Here we present an alternative design that uses a single motor and a tunable rotary cam-train system to achieve a spectrum of fin motions running from oscillation (wavenumber 1). Our platform enables thrust, lift, power, and wake measurements at prescribed pitch amplitudes, frequencies, and wavenumbers. We demonstrated the platform&#39;s oscillating and undulating capabilities via force and wake measurements in a water tank. Studies of fin wavenumber offer design insights for fish-like underwater robots, particularly those with stingray-inspired designs.},
  archive   = {C_IROS},
  author    = {Qiang Zhong and Yicong Fu and Leo Liu and Daniel B. Quinn},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981761},
  pages     = {2679-2684},
  title     = {Development of a stingray-inspired high-frequency propulsion platform with variable wavelength},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Comparative model evaluation with a symmetric three-link
swimming robot. <em>IROS</em>, 2672–2678. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we present swimming and modeling for Trident, a three-link lamprey inspired robot that is able to climb on flat smooth walls. We explore two gaits proposed to work for linear swimming, and three gaits for turning maneuvers. We compare the experimental results obtained from these swimming experiments with two different reduced order fluid interaction models, one a previously published potential flow model, and the other a slender cylinder model we developed. We find that depending on the the parameters of swimming chosen, we are able to move forward, backward and sideways with a peak speed of 2.5 cm/s. We identify the conditions when these models apply and aspects that will require additional complexity.},
  archive   = {C_IROS},
  author    = {Brian J. Van Stratum and Max P. Austin and Kourosh Shoele and Jonathan E. Clark},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981131},
  pages     = {2672-2678},
  title     = {Comparative model evaluation with a symmetric three-link swimming robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward dexterous flapping flight: Effective large yaw torque
generation by <span class="math inline">2 × 2</span>-degrees-of-freedom
flapping wings. <em>IROS</em>, 2650–2657. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Through the efforts of robotic engineers and inspired by the flapping flight of smaller creatures (e.g., insects and hummingbirds), the untethered stable hovering of flapping micro-aerial vehicles (FMAVs) has been achieved. Now, engineers are evaluating how to improve the mobility of these vehicles. The maneuverability of insects and birds in flight, such as their sharp turns and their takeoffs and landings from vertical walls, is what researchers originally expected from FMAVs. However, in previous studies, just one active (or one active plus one passive) degree of freedom (DoF) was given to the main actuation of the wing, and the range of movement of the wings was small. In addition, the magnitude of the attitude control torque that could be generated was relatively small when compared with multi-rotors. However, with the recent developments of small motors and drive-circuit technology, the realization of untethered flight by FMAVs equipped with four or more motors seems possible. This study utilized numerical calculation to investigate the advantage of a flapping-flight robot equipped with two pairs of left and right wings capable of stroke and twisting movements with two independent DoFs ( $2\times 2$ -DoF FMAV). The results of the numerical studies confirmed that, compared with the split-factor method widely used in past studies, the $2\times 2$ -DoF FMAV can generate higher yaw torque without the need for additional large driving torque. This shows that various agile flight functions are possible.},
  archive   = {C_IROS},
  author    = {M. Hamamoto},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981990},
  pages     = {2650-2657},
  title     = {Toward dexterous flapping flight: Effective large yaw torque generation by $2\times 2$-degrees-of-freedom flapping wings},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RoSA: A mechatronically synthesized dataset for rotodynamic
system anomaly detection. <em>IROS</em>, 2642–2649. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The time-series datasets commonly applied for anomaly detection research showcase specific suboptimal properties. This work novelly conceptualizes condition state synthesis to improve the data-synthetic pipeline of an anomalous-event dataset. We demonstrate two technical contributions in this study. First, we propose a methodology to formulate, accelerate and enrich the condition state synthetic process. The proposed method includes three critical phases: analysis of a rotodynamic plant, systematic design of its condition state space, and development of a Markovian model for controlled state transitions. Second, a Rotodynamic System with Synthetic Anomaly dataset is constructed. It is a large-scale time-series dataset featuring controlled, abundant and diverse anomalous condition states, and per-time-step condition state labels. A comprehensive learning-based case study is conducted to illustrate that these unique features tangibly benefit anomaly detection research. Potential usages of the proposed dataset as an anomaly detection study benchmark are discussed.},
  archive   = {C_IROS},
  author    = {Yip Fun Yeung and Alex Paul-Ajuwape and Farida Tahiry and Mikio Furokawa and Takayuki Hirano and Kamal Youcef-Toumi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982146},
  pages     = {2642-2649},
  title     = {RoSA: A mechatronically synthesized dataset for rotodynamic system anomaly detection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Co-optimization of acrobot design and controller for
increased certifiable stability. <em>IROS</em>, 2636–2641. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unlike fully actuated systems, the control of underactuated robots necessitates the use of passive dynamics to fulfill control objectives. Hence, there is an increased interdependence between their design parameters and the closed loop performance. This paper proposes a novel approach for co-optimization of robot design and controller parameters for increased certifiable stability obtained with means of region of attraction analysis and gradient free optimization. In particular, it discusses the co-optimization problem of a gymnastic acrobot robot where the design and the controller are optimized to have a large region of attraction (ROA) taking into account the closed loop dynamics of the non-linear system stabilized by a linear quadratic regulator (LQR) controller. The results are validated by extensive simulation of the acrobot&#39;s closed loop dynamics.},
  archive   = {C_IROS},
  author    = {Lasse Jenning Maywald and Felix Wiebe and Shivesh Kumar and Mahdi Javadi and Frank Kirchner},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981825},
  pages     = {2636-2641},
  title     = {Co-optimization of acrobot design and controller for increased certifiable stability},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Shape and motion optimization of rigid planar effectors for
contact trajectory satisfaction. <em>IROS</em>, 2628–2635. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a framework for co-optimizing the shape and motion of rigid robotic effectors for planar tasks. While planning object and robot-object contact trajectories is extensively studied, designing an effector that can execute the planned trajectories receives less attention. As such, our framework synthesizes an object trajectory and object-effector contact trajectory into an effector trajectory and shape that (a) does not penetrate the object, (b) makes contact with the object as specified, and (c) optimizes a user-specified objective. This simplifies manipulator control by encoding task-specific contact information in the effector&#39;s geometry. Our key insight is posing these requirements as constraints in the effector&#39;s reference frame, preventing the need for explicit parameterization of the effector shape. This prevents artificial restrictions on the shape design space. Importantly, it also facilitates posing the shape and motion design problem as a tractable nonlinear program. Our method is particularly useful for problems where the shape of the effector surface must be precisely chosen to achieve a task. We apply our method to several such problems, including jar-opening and picking up objects in constrained spaces. We evaluate the performance and computational cost of our method, and provide a physical experiment of a robotic arm picking up a screwdriver from a table with a designed tool.},
  archive   = {C_IROS},
  author    = {Rebecca H. Jiang and Neel Doshi and Ravi Gondhalekar and Alberto Rodriguez},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981918},
  pages     = {2628-2635},
  title     = {Shape and motion optimization of rigid planar effectors for contact trajectory satisfaction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enhanced quadruped locomotion of a rat robot based on the
lateral flexion of a soft actuated spine. <em>IROS</em>, 2622–2627. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In nature, the movement of quadrupeds is completed under the combined action of the spine and the legs. Inspired by this, this paper explores the effect of a lateral flexing spine on the locomotion of a rat robot. Benefiting from the regular lateral flexion of a soft actuated spine, the rat robot exhibits enhance step length of its hind legs and increased translational velocity by coordinating the opposite movements of the left and right sides. Furthermore, this paper introduces a mathematical model of the effect of the flexible spine on the robot velocity. Finally, extensive experiments are conducted in simulations and on the physical rat robot. Compared with the locomotion without a flexing spine, the simulation results show that the velocity of the robot can be increased up to 218.29\%, which is in line with the theoretical results from the proposed mathematical model. Limited by the gap between simulation and the real world, the experiment results of the physical rat robot show a slight performance than the theoretical results. But the physical rat robot can still enhance its translational velocity with the help of a lateral flexing spine.},
  archive   = {C_IROS},
  author    = {Yuhong Huang and Zhenshan Bing and Florian Walter and Alex Rohregger and Zitao Zhang and Kai Huang and Fabrice O. Morin and Alois Knoll},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981674},
  pages     = {2622-2627},
  title     = {Enhanced quadruped locomotion of a rat robot based on the lateral flexion of a soft actuated spine},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Set-point control for a ground-based reconfigurable robot.
<em>IROS</em>, 2616–2621. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reconfigurable mobile robots are well suited for inspection tasks in legacy nuclear facilities where access is restricted and the environment is often cluttered. A reconfig-urable snake robot, MIRRAX, has previously been developed to investigate such facilities. The joints used for the robot&#39;s reconfiguration introduce additional constraints on the robot&#39;s control, such as balance, on top of the existing actuator and collision constraints. This paper presents a set-point controller for MIRRAX using vector-field inequalities to enforce hard constraints on the robot&#39;s balance, actuator limits, and collision avoidance in a single quadratic programming formulation. The controller has been evaluated in simulation and early experiments in some scenarios. The results show that the controller generates feasible control inputs that enable the robot to retain its balance while moving with less oscillation and operating within the actuation and collision constraints.},
  archive   = {C_IROS},
  author    = {Wei Cheah and Bruno Vilhena Adorno and Simon Watson and Barry Lennox},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981358},
  pages     = {2616-2621},
  title     = {Set-point control for a ground-based reconfigurable robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Beyond the limit automated driving with performance
constrained reachability analysis. <em>IROS</em>, 2585–2592. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Professional human drivers usually have more than one driving strategy to handle incoming traffic situations. These different strategies activate different performance characteristics of the vehicle, enabling the driver to minimize the risk in a variety of situations by optimizing the strategy selection. In the same spirit, we define a novel concept of strategy-wise performance metric and creatively combine this performance metric with reachability analysis to evaluate candidate control strategies. Such a performance evaluation produces solid guarantees on which strategies will not qualify for the given traffic scenario. Then we automate the strategy selection process by weighing and minimizing the overall risk of each strategy candidate.},
  archive   = {C_IROS},
  author    = {Tong Zhao and Ekim Yurtsever and Giorgio Rizzoni},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981708},
  pages     = {2585-2592},
  title     = {Beyond the limit automated driving with performance constrained reachability analysis},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). OCTOANTS: A heterogeneous lightweight intelligent
multi-robot collaboration system with resource-constrained IoT devices.
<em>IROS</em>, 2556–2563. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As the focus on highly intelligent robots continues, a problem that cannot be ignored has emerged: resource con-straints. Considering the game problem of resource limitation and the level of intelligence, we focus on lightweight intelligence. This work is a further refinement of our previous work, a heterogeneous lightweight intelligent multi-robot system. In-spired by the nature creatures “octopus” and “ants”. First, we propose a heterogeneous centralized-distributed architecture, which can make robots collaboration more flexible and non-redundant. Second, to reflect lightweight intelligence, we use the Raspberry Pi, a low computing and power consumption internet of things (IoT) device, as a processing platform and first propose a quantitative definition of the lightweight intelligent system. Then, combining the centralized-distributed architecture and the lightweight computing platform, we propose an adapted algorithm called OCTOANTS and apply it to the simultaneous localization and mapping (SLAM) field. The OCTOANTS architecture consists of one brain and eight tentacles, which can achieve complex things with proper collaboration between them. Finally, we use heterogeneous cameras and heterogeneous algorithms to form a lightweight intelligent collaborative system that can run in the real world. On the low-grade platform Raspberry Pi our heterogeneous tentacles frame rate can reach 41fps and 99.8fps respectively, power consumption is only 2W and 1.2W. At the same time, our heterogeneous system is on average 7.2\% more accurate than the state-of-the-art homogeneous system and can be applied to a wider range of application scenarios, demonstrating the superiority and feasibility of our OCTOANTS.},
  archive   = {C_IROS},
  author    = {Qian Zhang and Ruiyang Quan and Siqin Qimuge and Peimin Xia and Jiaheng Wang and Xin Zan and Fangshi Wang and Changchuan Chen and Qi Wei and Huichan Zhao and Xinjun Liu and Fei Qiao},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982135},
  pages     = {2556-2563},
  title     = {OCTOANTS: A heterogeneous lightweight intelligent multi-robot collaboration system with resource-constrained IoT devices},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Torque-actuated multimodal locomotion of ferrofluid robot
with environment and task adaptability. <em>IROS</em>, 2542–2547. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft microrobotics have recently been an active field that advances microrobotics with new robot design, locomotion, and applications. In this paper, we study the ferrofluid robot (FR), which has soft nature and exhibits paramagnetism. Currently, the FR locomotion is usually realized by magnetic force. To enable the FR with more locomotion modes for environment and task adaptability, we program three dynamic field forms and realize three corresponding torque-actuated locomotion modes: Rolling, Wobbling, and Oscillating. The torque actuation of the FR is formulated, and the three locomotion modes are characterized. With the implementation of automated tracking and control algorithms, the controllability of these modes is testified. We then fabricate different environments to validate the adaptability of the FR that can switch its locomotion mode accordingly. Finally, utilizing the oscillating mode and wobbling mode, we demonstrate the transport of lipophilic and hydrophilic cargoes, respectively, showing the task adaptability.},
  archive   = {C_IROS},
  author    = {Lidong Yang and Mengmeng Sun and Li Zhang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981041},
  pages     = {2542-2547},
  title     = {Torque-actuated multimodal locomotion of ferrofluid robot with environment and task adaptability},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). A passive, asymmetrically-compliant knee joint improves
obstacle traversal in an insect-scale legged robot. <em>IROS</em>,
2534–2541. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Insects can locomote readily in challenging environments, such as over steep inclines and across obstacle-laden terrains, which still frustrate robots of similar size. In this work, inspired by the passive compliant properties of insect limbs, we use the insect-scale Harvard Ambulatory Microrobot and multilayer microfabrication techniques as platform to study the ability of passive mechanisms to improve open-loop running in rough terrains. We tested the performance of different limb designs in vivo, exploring how the magnitude, directionality, and distribution of compliance incorporated into the leg impacted robot performance. Limbs were evaluated on both a featureless substrate and an increasingly-adversarial 3D-printed terrain designed to mimic natural environments. We tested the limbs using a trotting gait in the quasi-static (2 Hz) and body dynamics (25 Hz) stride frequency regimes. Performance was reported as bodylengths traveled per gait cycle on the featureless substrate, and as the largest feature height the robot was able to overcome in the terrain. The work presented here provides design principles for a passive limb that expands the terrain accessible to small robot; we find a limb with a single asymmetrical joint is able to improve quasi-static terrain traversal by 203\% relative to a rigid limb.},
  archive   = {C_IROS},
  author    = {Perrin E. Schiebel and Michelle C. Yuen and Robert J. Wood},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981497},
  pages     = {2534-2541},
  title     = {A passive, asymmetrically-compliant knee joint improves obstacle traversal in an insect-scale legged robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). A passive, asymmetrically-compliant knee joint improves
obstacle traversal in an insect-scale legged robot. <em>IROS</em>,
2526–2533. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Insects can locomote readily in challenging envi-ronments, such as over steep inclines and across obstacle-laden terrains, which still frustrate robots of similar size. In this work, inspired by the passive compliant properties of insect limbs, we use the insect-scale Harvard Ambulatory Microrobot and multilayer microfabrication techniques as platform to study the ability of passive mechanisms to improve open-loop running in rough terrains. We tested the performance of different limb designs in vivo, exploring how the magnitude, directionality, and distribution of compliance incorporated into the leg impacted robot performance. Limbs were evaluated on both a featureless substrate and an increasingly-adversarial 3D-printed terrain designed to mimic natural environments. We tested the limbs using a trotting gait in the quasi-static (2 Hz) and body dynamics (25 Hz) stride frequency regimes. Performance was reported as bodylengths traveled per gait cycle on the featureless substrate, and as the largest feature height the robot was able to overcome in the terrain. The work presented here provides design principles for a passive limb that expands the terrain accessible to small robot; we find a limb with a single asymmetrical joint is able to improve quasi-static terrain traversal by 203\% relative to a rigid limb.},
  archive   = {C_IROS},
  author    = {Perrin E. Schiebel and Michelle C. Yuen and Robert J. Wood},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982232},
  pages     = {2526-2533},
  title     = {A passive, asymmetrically-compliant knee joint improves obstacle traversal in an insect-scale legged robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Origami robot self-folding by magnetic induction.
<em>IROS</em>, 2519–2525. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inspired by the traditional art of paper folding, origami, autonomous production of 3D structures from 2D sheets can be achieved by the implementation of self-folding techniques. One technique to achieve such transformation is the usage of thermo-responsive smart materials such as self-folding polymeric films, which can be controlled by heat to shrink. Achieving remote self-folding with a practical approach remains a major challenge due to the requirement for specific environments, or having to accompany electronics on origami, which limits the complexity of the origami design. In this paper, we present a wireless method to trigger the thermo-responsive self-folding process of the origami robots through magnetic induction. The proposed method is applicable for all electrically conductive materials and can wirelessly fold a mobile origami robot with a size of 32 × 30 mm 2 . This method eliminates the need for inclusion of electronics on the origami or usage of complicated trigger methods and environmental conditions, allowing the robot to fold in a wider range of applications such as in constrained spaces.},
  archive   = {C_IROS},
  author    = {Jialun Liu and Xiao Chen and Quentin Lahondes and Kaan Esendag and Dana Damian and Shuhei Miyashita},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981604},
  pages     = {2519-2525},
  title     = {Origami robot self-folding by magnetic induction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Impact makes a sound and sound makes an impact: Sound guides
representations and explorations. <em>IROS</em>, 2512–2518. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sound is one of the most informative and abundant modalities in the real world while being robust to sense without contacts by small and cheap sensors that can be placed on mobile devices. Although deep learning is capable of extracting information from multiple sensory inputs, there has been little use of sound for the control and learning of robotic actions. For unsupervised reinforcement learning, an agent is expected to actively collect experiences and jointly learn representations and policies in a self-supervised way. We build realistic robotic manipulation scenarios with physics-based sound simulation and propose the Intrinsic Sound Curiosity Module (ISCM). The ISCM provides feedback to a reinforcement learner to learn robust representations and to reward a more efficient exploration behavior. We perform experiments with sound enabled during pre-training and disabled during adaptation, and show that representations learned by ISCM outperform the ones by vision-only baselines and pre-trained policies can accelerate the learning process when applied to downstream tasks.},
  archive   = {C_IROS},
  author    = {Xufeng Zhao and Cornelius Weber and Muhammad Burhan Hafez and Stefan Wermter},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981510},
  pages     = {2512-2518},
  title     = {Impact makes a sound and sound makes an impact: Sound guides representations and explorations},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NavDreams: Towards camera-only RL navigation among humans.
<em>IROS</em>, 2504–2511. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomously navigating a robot in everyday crowded spaces requires solving complex perception and planning challenges. When using only monocular image sensor data as input, classical two-dimensional planning approaches cannot be used. While images present a significant challenge when it comes to perception and planning, they also allow capturing potentially important details, such as complex geometry, body movement, and other visual cues. In order to successfully solve the navigation task from only images, algorithms must be able to model the scene and its dynamics using only this channel of information. We investigate whether the world model concept, which has shown state-of-the-art results for modeling and learning policies in Atari games as well as promising results in 2D LiDAR-based crowd navigation, can also be applied to the camera-based navigation problem. To this end, we create simulated environments where a robot must navigate past static and moving humans without colliding in order to reach its goal. We find that state-of-the-art methods are able to achieve success in solving the navigation problem, and can generate dream-like predictions of future image-sequences which show consistent geometry and moving persons. We are also able to show that policy performance in our high-fidelity sim2real simulation scenario transfers to the real world by testing the policy on a real robot. We make our simulator, models and experiments available at https://github.com/danieldugas/NavDreams.},
  archive   = {C_IROS},
  author    = {Daniel Dugas and Olov Andersson and Roland Siegwart and Jen Jen Chung},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982045},
  pages     = {2504-2511},
  title     = {NavDreams: Towards camera-only RL navigation among humans},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Advanced skills by learning locomotion and local navigation
end-to-end. <em>IROS</em>, 2497–2503. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The common approach for local navigation on challenging environments with legged robots requires path planning, path following and locomotion, which usually requires a locomotion control policy that accurately tracks a commanded velocity. However, by breaking down the navigation problem into these sub-tasks, we limit the robot&#39;s capabilities since the individual tasks do not consider the full solution space. In this work, we propose to solve the complete problem by training an end-to-end policy with deep reinforcement learning. Instead of continuously tracking a precomputed path, the robot needs to reach a target position within a provided time. The task&#39;s success is only evaluated at the end of an episode, meaning that the policy does not need to reach the target as fast as possible. It is free to select its path and the locomotion gait. Training a policy in this way opens up a larger set of possible solutions, which allows the robot to learn more complex behaviors. We compare our approach to velocity tracking and additionally show that the time dependence of the task reward is critical to successfully learn these new behaviors. Finally, we demonstrate the successful deployment of policies on a real quadrupedal robot. The robot is able to cross challenging terrains, which were not possible previously, while using a more energy-efficient gait and achieving a higher success rate. Supplementary videos can be found on the project website: https://sites.google.com/leggedrobotics.com/end-to-end-loco-navigation},
  archive   = {C_IROS},
  author    = {Nikita Rudin and David Hoeller and Marko Bjelonic and Marco Hutter},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981198},
  pages     = {2497-2503},
  title     = {Advanced skills by learning locomotion and local navigation end-to-end},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ordinal inverse reinforcement learning applied to robot
learning with small data. <em>IROS</em>, 2490–2496. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Over the last decade, the ability to teach actions to robots in a user-friendly way has gained relevance, and a practical way of teaching robots a new task is to use Inverse Reinforcement Learning (IRL). In IRL, an expert teacher shows the robot a desired behaviour and an agent builds a model of the reward. The agent can also infer a policy that performs in an optimal way within the limitations of the knowledge provided to it. However, most IRL approaches assume an (almost) optimal performance of the teaching agent, which might become unpractical if the teacher is not actually an expert. In addition, most IRL focus on discrete state-action spaces that limit their applicability to certain real-world problems such as within the context of direct Policy Search (PS) reinforcement learning. Therefore, in this paper we introduce Ordinal Inverse Reinforcement Learning (OrdIRL) for continuous state variables, in which the teacher can qualitatively evaluate robot performance by selecting one among the predefined performance levels (e.g. {bad, medium, good} for three tiers of performance). Once the OrdIRL has fit an ordinal distribution to the data, we propose to use Bayesian Optimization (BO) to either gain knowledge on the inferred model (exploration) or find a policy or action that maximizes the expected reward given the prior knowledge on the reward (exploitation). In the case of large-dimensional state-action spaces, we use Dimensionality Reduction (DR) techniques and perform the BO in the latent space. Experimental results on simulation and with a robot arm show how this approach allows for learning the reward function with small data.},
  archive   = {C_IROS},
  author    = {Adrià Colomé and Carme Torras},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981731},
  pages     = {2490-2496},
  title     = {Ordinal inverse reinforcement learning applied to robot learning with small data},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning skills to navigate without a master: A sequential
multi-policy reinforcement learning algorithm. <em>IROS</em>, 2483–2489.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Solving complex problems using reinforcement learning necessitates breaking down the problem into manageable tasks, and learning policies to solve these tasks. These policies, in turn, have to be controlled by a master policy that takes high-level decisions. Hence learning policies involves hierarchical decision structures. However, training such methods in practice may lead to poor generalization, with either sub-policies executing actions for too few time steps or devolving into a single policy altogether. In our work, we introduce an alternative approach to learn such skills sequentially without using an overarching hierarchical policy. We propose this method in the context of environments where a major component of the objective of a learning agent is to prolong the episode for as long as possible. We refer to our proposed method as Sequential Soft Option Critic. We demonstrate the utility of our approach on navigation and goal-based tasks in a flexible simulated 3D navigation environment that we have developed. We also show that our method outperforms prior methods such as Soft Actor-Critic and Soft Option Critic on various environments, including the Atari River Raid environment and the Gym-Duckietown self-driving car simulator.},
  archive   = {C_IROS},
  author    = {Ambedkar Dukkipati and Rajarshi Banerjee and Ranga Shaarad Ayyagari and Dhaval Parmar Udaybhai},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981607},
  pages     = {2483-2489},
  title     = {Learning skills to navigate without a master: A sequential multi-policy reinforcement learning algorithm},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A contact-safe reinforcement learning framework for
contact-rich robot manipulation. <em>IROS</em>, 2476–2482. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning shows great potential to solve complex contact-rich robot manipulation tasks. However, the safety of using RL in the real world is a crucial problem, since unexpected dangerous collisions might happen when the RL policy is imperfect during training or in unseen scenarios. In this paper, we propose a contact-safe reinforcement learning framework for contact-rich robot manipulation, which maintains safety in both the task space and joint space. When the RL policy causes unexpected collisions between the robot arm and the environment, our framework is able to immediately detect the collision and ensure the contact force to be small. Furthermore, the end-effector is enforced to perform contact-rich tasks compliantly, while keeping robust to external disturbances. We train the RL policy in simulation and transfer it to the real robot. Real world experiments on robot wiping tasks show that our method is able to keep the contact force small both in task space and joint space even when the policy is under unseen scenario with unexpected collision, while rejecting the disturbances on the main task.},
  archive   = {C_IROS},
  author    = {Xiang Zhu and Shucheng Kang and Jianyu Chen},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981185},
  pages     = {2476-2482},
  title     = {A contact-safe reinforcement learning framework for contact-rich robot manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). How to spend your robot time: Bridging kickstarting and
offline reinforcement learning for vision-based robotic manipulation.
<em>IROS</em>, 2468–2475. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning (RL) has been shown to be effective at learning control from experience. However, RL typically requires a large amount of online interaction with the environment. This limits its applicability to real-world settings, such as in robotics, where such interaction is expensive. In this work we investigate ways to minimize online interactions in a target task, by reusing a suboptimal policy we might have access to, for example from training on related prior tasks, or in simulation. To this end, we develop two RL algorithms that can speed up training by using not only the action distributions of teacher policies, but also data collected by such policies on the task at hand. We conduct a thorough experimental study of how to use suboptimal teachers on a challenging robotic manipulation benchmark on vision-based stacking with diverse objects. We compare our methods to offline, online, offline-to-online, and kickstarting RL algorithms. By doing so, we find that training on data from both the teacher and student, enables the best performance for limited data budgets. We examine how to best allocate a limited data budget - on the target task - between the teacher and the student policy, and report experiments using varying budgets, two teachers with different degrees of suboptimality, and five stacking tasks that require a diverse set of behaviors. Our analysis, both in simulation and in the real world, shows that our approach is the best across data budgets, while standard offline RL from teacher rollouts is surprisingly effective when enough data is given.},
  archive   = {C_IROS},
  author    = {Alex X. Lee and Coline Devin and Jost Tobias Springenberg and Yuxiang Zhou and Thomas Lampe and Abbas Abdolmaleki and Konstantinos Bousmalis},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981126},
  pages     = {2468-2475},
  title     = {How to spend your robot time: Bridging kickstarting and offline reinforcement learning for vision-based robotic manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Safety guided policy optimization. <em>IROS</em>, 2462–2467.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In reinforcement learning (RL), exploration is essential to achieve a globally optimal policy but unconstrained exploration can cause damages to robots and nearby people. To handle this safety issue in exploration, safe RL has been proposed to keep the agent under the specified safety constraints while maximizing cumulative rewards. This paper introduces a new safe RL method which can be applied to robots to operate under the safety constraints while learning. The key component of the proposed method is the safeguard module. The safeguard predicts the constraints in the near future and corrects actions such that the predicted constraints are not violated. Since actions are safely modified by the safeguard during exploration and policies are trained to imitate the corrected actions, the agent can safely explore. Additionally, the safeguard is sample efficient as it does not require long horizontal trajectories for training, so constraints can be satisfied within short time steps. The proposed method is extensively evaluated in simulation and experiments using a real robot. The results show that the proposed method achieves the best performance while satisfying safety constraints with minimal interaction with environments in all experiments.},
  archive   = {C_IROS},
  author    = {Dohyeong Kim and Yunho Kim and Kyungjae Lee and Songhwai Oh},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981030},
  pages     = {2462-2467},
  title     = {Safety guided policy optimization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Safe reinforcement learning for legged locomotion.
<em>IROS</em>, 2454–2461. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Designing control policies for legged locomotion 1 1In this work, we specifically consider quadruped locomotion. is complex due to the under-actuated and non-continuous robot dynamics. Model-free reinforcement learning provides promising tools to tackle this challenge. However, a major bottleneck of applying model-free reinforcement learning in real world is safety. In this paper, we propose a safe reinforcement learning framework that switches between a safe recovery policy that prevents the robot from entering unsafe states, and a learner policy that is optimized to complete the task. The safe recovery policy takes over the control when the learner policy violates safety constraints, and hands over the control back when there are no future safety violations. We design the safe recovery policy so that it ensures safety of quadruped locomotion while minimally intervening in the learning process. Furthermore, we theoretically analyze the proposed framework and provide an upper bound on the task performance. We verify the proposed framework in four tasks on a simulated and real quadrupedal robot: efficient gait, catwalk, two-leg balance, and pacing. On average, our method achieves 48.6\% fewer falls and comparable or better rewards than the baseline methods in simulation. When deployed it on real-world quadruped robot, our training pipeline enables 34\% improvement in energy efficiency for the efficient gait, 40.9\% narrower of the feet placement in the catwalk, and two times more jumping duration in the two-leg balance. Our method achieves less than five falls over the duration of 115 minutes of hardware time. 2 2Video is included in the submission and the project website: https://sites.google.com/view/saferlleggedlocomotion/},
  archive   = {C_IROS},
  author    = {Tsung-Yen Yang and Tingnan Zhang and Linda Luu and Sehoon Ha and Jie Tan and Wenhao Yu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982038},
  pages     = {2454-2461},
  title     = {Safe reinforcement learning for legged locomotion},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multical: Spatiotemporal calibration for multiple IMUs,
cameras and LiDARs. <em>IROS</em>, 2446–2453. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Spatiotemporal calibration of sensors, especially of those which do not share their fields of view, is becoming increasingly important in the fields of autonomous driving and robotics. This paper presents a general sensor calibration method, named Multical, that makes use of multiple planar calibration targets whose poses will be estimated alongside spatiotemporal calibration. Multical exploits continuous-time curves to represent the state of the sensor platform during data collection, and thus is a general framework to calibrate different kinds of sensors and deal with both spatial as well as temporal offsets. Multical includes algorithms to estimate the initial guesses of spatial transformations between sensors, and also the relative poses between calibration targets. Users do not need to provide any extrinsic priors. We apply the proposed calibration approach to both simulated and real-world experiments, and the results demonstrate the high fidelity of the proposed method.},
  archive   = {C_IROS},
  author    = {Xiangyang Zhi and Jiawei Hou and Yiren Lu and Laurent Kneip and Sören Schwertfeger},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982031},
  pages     = {2446-2453},
  title     = {Multical: Spatiotemporal calibration for multiple IMUs, cameras and LiDARs},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Monocular event visual inertial odometry based on
event-corner using sliding windows graph-based optimization.
<em>IROS</em>, 2438–2445. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Event cameras are biologically-inspired vision sensors that capture pixel-level illumination changes instead of the intensity image at a fixed frame rate. They offer many advantages over the standard cameras, such as high dynamic range, high temporal resolution (low latency), no motion blur, etc. Therefore, developing state estimation algorithms based on event cameras offers exciting opportunities for autonomous systems and robots. In this paper, we propose monocular visual-inertial odometry for event cameras based on event-corner feature detection and matching with well-designed feature management. More specifically, two different kinds of event representations based on time surface are designed to realize event-corner feature tracking (for front-end incremental estimation) and matching (for loop closure detection). Furthermore, the proposed event representations are used to set mask for detecting the event-corner feature based on the raw event-stream, which ensures the uniformly distributed and spatial consistency characteristic of the event-corner feature. Finally, a tightly coupled, graph-based optimization framework is designed to obtain high-accurate state estimation through fusing pre-integrated IMU measurements and event-corner observations. We validate quantitatively the performance of our system on different resolution event cameras: DAVIS240C (240*180, public dataset, achieve state-of-the-art), DAVIS346 (346*240, real-test), DVXplorer (640*480 real-test). Furthermore, we demonstrate qualitatively the accuracy, robustness, loop closure, and re-localization performance of our framework on different large-scale datasets, and an autonomous quadrotor flight using our Event Visual-inertial Odometry (EVIO) framework. Videos of all the evaluations are presented on the project website.},
  archive   = {C_IROS},
  author    = {Weipeng Guan and Peng Lu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981970},
  pages     = {2438-2445},
  title     = {Monocular event visual inertial odometry based on event-corner using sliding windows graph-based optimization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal multi-robot formations for relative pose estimation
using range measurements. <em>IROS</em>, 2431–2437. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In multi-robot missions, relative position and attitude information between robots is valuable for a variety of tasks such as mapping, planning, and formation control. In this paper, the problem of estimating relative poses from a set of inter-robot range measurements is investigated. Specifically, it is shown that the estimation accuracy is highly dependent on the true relative poses themselves, which prompts the desire to find multi-robot formations that provide the best estimation performance. By direct maximization of Fischer information, it is shown in simulation and experiment that large improvements in estimation accuracy can be obtained by optimizing the formation geometry of a team of robots.},
  archive   = {C_IROS},
  author    = {Charles Champagne Cossette and Mohammed Ayman Shalaby and David Saussié and Jérôme Le Ny and James Richard Forbes},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981301},
  pages     = {2431-2437},
  title     = {Optimal multi-robot formations for relative pose estimation using range measurements},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scalable and modular ultra-wideband aided inertial
navigation. <em>IROS</em>, 2423–2430. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Navigating accurately in potentially GPS-denied environments is a perquisite of autonomous systems. Relative localization based on ultra-wideband (UWB) is - especially indoors - a promising technology. In this paper, we present a probabilistic filter based Modular Multi-Sensor Fusion (MMSF) approach with the capability of using efficiently all information in a fully meshed UWB ranging network. This allows an accurate mobile agent state estimation and the calibration of the ranging network&#39;s spatial constellation. We advocate a new paradigm that includes elements from Collaborative State Estimation (CSE) and allows us considering all stationary UWB anchors and the mobile agent as a decentralized set of estimtors/filters. With this, our method can include all meshed (inter-)sensor observations tightly coupled in a modular estimator. We show that the application of our CSE-inspired method in such a context breaks the computational barrier. Otherwise, it would, for the sakeof complexity-reduction, prohibit the use of all available information or would lead to significant estimator inconsistencies due to coarse approximations. We compare the proposed approach against different MMSF strategies in terms of execution time, accuracy, and filter credibility on both synthetic data and on a dataset from real Unmanned Aerial Vehicles (UAVs).},
  archive   = {C_IROS},
  author    = {Roland Jung and Stephan Weiss},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981937},
  pages     = {2423-2430},
  title     = {Scalable and modular ultra-wideband aided inertial navigation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HD-CCSOM: Hierarchical and dense collaborative continuous
semantic occupancy mapping through label diffusion. <em>IROS</em>,
2417–2422. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The collaborative operation of multiple robots can make up for the shortcomings of a single robot, such as limited field of perception or sensor failure. multirobots collaborative semantic mapping can enhance their comprehensive contextual understanding of the environment. However, existing multirobots collaborative semantic mapping algorithms mainly apply discrete occupancy map inference, and do not compensate for inconsistent labels of local maps caused by differences in robot perspectives, which leads to greatly reduced availability and accuracy of the final global map. To address the challenges of discontinuous maps and inconsistent semantic labels, this paper proposes a novel hierarchical and dense collaborative continuous semantic occupancy mapping algorithm (HD-CCSOM). This work decomposes and formulates robot collaborative continuous semantic occupancy mapping problem at two levels. At the single robot level, the multi-entropy kernel inference method smoothly processes the registered semantic point cloud and infers a local continuous semantic occupancy map for each robot. At the collaborative robots level, the local maps are fused into a global enhanced and consistent semantic map via the label diffusion method based on a graph model. The proposed algorithm has been validated on public datasets and in simulated and real scenes, demonstrating significant improvements in mapping accuracy and efficiency.},
  archive   = {C_IROS},
  author    = {Yinan Deng and Meiling Wang and Yi Yang and Yufeng Yue},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981756},
  pages     = {2417-2422},
  title     = {HD-CCSOM: Hierarchical and dense collaborative continuous semantic occupancy mapping through label diffusion},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A portable multiscopic camera for novel view and time
synthesis in dynamic scenes. <em>IROS</em>, 2409–2416. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a portable multiscopic camera system with a dedicated model for novel view and time synthesis in dynamic scenes. Our goal is to render high-quality images for a dynamic scene from any viewpoint at any time using our portable multiscopic camera. To achieve such novel view and time synthesis, we develop a physical multiscopic camera equipped with five cameras to train a neural radiance field (NeRF) in both time and spatial domains for dynamic scenes. Our model maps a 6D coordinate (3D spatial position, 1D temporal coordinate, and 2D viewing direction) to view-dependent and time-varying emitted radiance and volume density. Volume rendering is applied to render a photo-realistic image at a specified camera pose and time. To improve the robustness of our physical camera, we propose a camera parameter optimization module and a temporal frame interpolation module to promote information propagation across time. We conduct experiments on both real-world and synthetic datasets to evaluate our system, and the results show that our approach outperforms alternative solutions qualitatively and quantitatively. Our code and dataset are available at https://yuenfuilau.github.io/.},
  archive   = {C_IROS},
  author    = {Tianjia Zhang and Yuen-Fui Lau and Qifeng Chen},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982040},
  pages     = {2409-2416},
  title     = {A portable multiscopic camera for novel view and time synthesis in dynamic scenes},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AFT-VO: Asynchronous fusion transformers for multi-view
visual odometry estimation. <em>IROS</em>, 2402–2408. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion estimation approaches typically employ sensor fusion techniques, such as the Kalman Filter, to handle individual sensor failures. More recently, deep learning-based fusion approaches have been proposed, increasing the performance and requiring less model-specific implementations. However, current deep fusion approaches often assume that sensors are synchronised, which is not always practical, especially for low-cost hardware. To address this limitation, in this work, we propose AFT-VO, a novel transformer-based sensor fusion architecture to estimate VO from multiple sensors. Our framework combines predictions from asynchronous multi-view cameras and accounts for the time discrepancies of measurements coming from different sources. Our approach first employs a Mixture Density Network (MDN) to estimate the probability distributions of the 6-DoF poses for every camera in the system. Then a novel transformer-based fusion module, AFT-VO, is introduced, which combines these asynchronous pose estimations, along with their confidences. More specifically, we introduce Discretiser and Source Encoding techniques which enable the fusion of multi-source asynchronous signals. We evaluate our approach on the popular nuScenes and KITTI datasets. Our experiments demonstrate that multi-view fusion for VO estimation provides robust and accurate trajectories, outperforming the state of the art in both challenging weather and lighting conditions.},
  archive   = {C_IROS},
  author    = {Nimet Kaygusuz and Oscar Mendez and Richard Bowden},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981835},
  pages     = {2402-2408},
  title     = {AFT-VO: Asynchronous fusion transformers for multi-view visual odometry estimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards autonomous control of surgical instruments using
adaptive-fusion tracking and robot self-calibration. <em>IROS</em>,
2395–2401. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to track surgical instruments in realtime is crucial for autonomous Robotic Assisted Surgery (RAS). Recently, the fusion of visual and kinematic data has been proposed to track surgical instruments. However, these methods assume that both sensors are equally reliable, and cannot successfully handle cases where there are significant perturbations in one of the sensors&#39; data. In this paper, we address this problem by proposing an enhanced fusion-based method. The main advantage of our method is that it can adjust fusion weights to adapt to sensor perturbations and failures. Another problem is that before performing an autonomous task, these robots have to be repetitively recalibrated by a human for each new patient to estimate the transformations between the different robotic arms. To address this problem, we propose a self-calibration algorithm that empowers the robot to autonomously calibrate the transformations by itself in the beginning of the surgery. We applied our fusion and selfcalibration algorithms for autonomous ultrasound tissue scanning and we showed that the robot achieved stable ultrasound imaging when using our method. Our performance evaluation shows that our proposed method outperforms the state-of-art both in normal and challenging situations.},
  archive   = {C_IROS},
  author    = {Chiyu Wang and João Cartucho and Daniel Elson and Ara Darzi and Stamatia Giannarou},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981141},
  pages     = {2395-2401},
  title     = {Towards autonomous control of surgical instruments using adaptive-fusion tracking and robot self-calibration},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generalized laplace particle filter on lie groups applied to
ambiguous doppler navigation. <em>IROS</em>, 2387–2394. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Particle filters are suited to solve nonlinear and non-Gaussian estimation problems which find numerous applications in autonomous systems navigation. Previous works on Laplace Particle Filter on Lie groups (LG-LPF) demonstrated its robustness and accuracy on challenging navigation scenarios compared to classic particle filters. Nevertheless, LG-LPF is applicable when the prior probability density and the likelihood have a predominant mode, which narrows the scope of applications of this method. Thus, this paper proposes a generalized strategy to use LG-LPF while keeping its benefits. The core idea is to compute an accurate multimodal importance function based on local optimizations and resample the particles accordingly. This approach is compared to a Laplace Particle Filter (LPF) designed in the Euclidean space, on a UAV navigation scenario with ambiguous Doppler measurements. The Lie group approach shows improved accuracy and robustness in every case, even with a reduced number of particles.},
  archive   = {C_IROS},
  author    = {Clément Chahbazian and Nicolas Merlinge and Karim Dahia and Bénédicte Winter-Bonnet and Aurélien Blanc and Christian Musso},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982086},
  pages     = {2387-2394},
  title     = {Generalized laplace particle filter on lie groups applied to ambiguous doppler navigation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Registering articulated objects with human-in-the-loop
corrections. <em>IROS</em>, 2343–2350. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Remotely programming robots to execute tasks often relies on registering objects of interest in the robot&#39;s environment. Frequently, these tasks involve articulating objects such as opening or closing a valve. However, existing human-in-the-loop methods for registering objects do not consider articulations and the corresponding impact to the geometry of the object, which can cause the methods to fail. In this work, we present an approach where the registration system attempts to automatically determine the object model, pose, and articulation for user-selected points using nonlinear fitting and the iterative closest point algorithm. When the fitting is incorrect, the operator can iteratively intervene with corrections after which the system will refit the object. We present an implementation of our fitting procedure for one degree-of-freedom (DOF) objects with revolute joints and evaluate it with a user study that shows that it can improve user performance, in measures of time on task and task load, ease of use, and usefulness compared to a manual registration approach. We also present a situated example that integrates our method into an end-to-end system for articulating a remote valve.},
  archive   = {C_IROS},
  author    = {Michael Hagenow and Emmanuel Senft and Evan Laske and Kimberly Hambuchen and Terrence Fong and Robert Radwin and Michael Gleicher and Bilge Mutlu and Michael Zinn},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981949},
  pages     = {2343-2350},
  title     = {Registering articulated objects with human-in-the-loop corrections},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Core processes in intelligent robotic lab assistants:
Flexible liquid handling. <em>IROS</em>, 2335–2342. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Laboratory automation is a suitable solution to establish higher reproducibility with less manual work and thus higher quality standards in life sciences. To date, mobile robots are capable of performing autonomous pick-and-place tasks in the laboratory, and specialized pipetting machines can be used for sequenced liquid handling. However, the complex and creative process of developing new research protocols requires flexible robotic systems that can perform tasks such as pipetting in more versatile ways. In addition, the correct technique, according to ISO standards, has a great influence on precision and accuracy and therefore on reproducibility. This paper introduces our Intelligent Robotic Lab Assistants in the framework of our holistic, human-like, but standardized paradigm for collaborative lab automation, AI.Laboratory. Our system demonstrates mastery of pipetting following ISO 8655 as a force-sensitive robotic manipulation skill, which is a key component of our taxonomy of cell culture skills and the first steps toward true intelligent robotic laboratory assistants. This intelligent robotic pipetting skill is a versatile tool for general handling of µL-liquids, using only standard laboratory equipment that can be flexibly positioned in the robot&#39;s workspace. To demonstrate its pipetting performance, flexible handling of small volumes from 10 µL to 1000 µL was experimentally validated to the ISO 8655 standard, demonstrating superhuman performance that outperformed laymen, human experts, and other commercial and non-commercial robotic pipetting systems.},
  archive   = {C_IROS},
  author    = {Dennis Knobbe and Henning Zwirnmann and Moritz Eckhoff and Sami Haddadin},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981636},
  pages     = {2335-2342},
  title     = {Core processes in intelligent robotic lab assistants: Flexible liquid handling},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robotic powder grinding with a soft jig for laboratory
automation in material science. <em>IROS</em>, 2320–2326. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Grinding materials into a fine powder is a time-consuming task in material science that is generally performed by hand, as current automated grinding machines might not be suitable for preparing small-sized samples. This study presents a robotic powder grinding system for laboratory automation in material science applications that observe the powder&#39;s state to improve the grinding outcome. We developed a soft jig consisting of off-the-shelf gel materials and 3D-printed parts, which can be used with any robot arm to perform powder grinding. The jig&#39;s physical softness allows for safe grinding without force sensing. In addition, we developed a visual feedback system that observes the powder distribution and decides where to grind and when to gather. The results showed that our system could grind 79 percent of the powder to a particle size smaller than 200 μm by using the soft jig and visual feedback. This ratio was 57\% when using only the soft jig without feedback. Our system can be used immediately in laboratories to alleviate the workload of researchers.},
  archive   = {C_IROS},
  author    = {Yusaku Nakajima and Masashi Hamaya and Yuta Suzuki and Takafumi Hawai and Felix von Drigalski and Kazutoshi Tanaka and Yoshitaka Ushiku and Kanta Ono},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981081},
  pages     = {2320-2326},
  title     = {Robotic powder grinding with a soft jig for laboratory automation in material science},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LODM: Large-scale online dense mapping for UAV.
<em>IROS</em>, 2281–2288. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes an online large-scale dense mapping method for UAVs with a height of 150–250 meters. We first fuse the GPS with the visual odometry to estimate the scaled poses and sparse points. In order to use the depth of sparse points for depth map, we propose Sparse Confidence Cascade View-Aggregation MVSNet (SCCVA-MVSNet), which projects the depth-converged points in the sliding window on keyframes to obtain a sparse depth map. To weigh the confidence of the depth of each sparse point, we construct sparse confidence by the photometric error. The images of all keyframes, coarse depth, and confidence as the input of CVA-MVSNet to extract features and construct 3D cost volumes with adaptive view aggregation to balance the different stereo baselines between the keyframes. Our proposed network utilizes sparse features point information, the output of the network better maintains the consistency of the scale. Our experiments show that MVSNet using sparse feature point information outperforms image-only MVSNet, and our online reconstruction results are comparable to offline reconstruction methods. To benefit the research community, we open our code at https://github.com/hjxwhy/LODM.git},
  archive   = {C_IROS},
  author    = {Jianxin Huang and Laijian Li and Xiangrui Zhao and Xiaolei Lang and Deye Zhu and Yong Liu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981994},
  pages     = {2281-2288},
  title     = {LODM: Large-scale online dense mapping for UAV},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Elevation mapping for locomotion and navigation using GPU.
<em>IROS</em>, 2273–2280. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Perceiving the surrounding environment is crucial for autonomous mobile robots. An elevation map provides a memory-efficient and simple yet powerful geometric represen-tation of the terrain for ground robots. The robots can use this information for navigation in an unknown environment or perceptive locomotion control over rough terrain. Depending on the application, various post processing steps may be incorpo-rated, such as smoothing, inpainting or plane segmentation. In this work, we present an elevation mapping pipeline leveraging GPU for fast and efficient processing with additional features both for navigation and locomotion. We demonstrated our map-ping framework through extensive hardware experiments. Our mapping software was successfully deployed for underground exploration during DARPA Subterranean Challenge and for various experiments of quadrupedal locomotion.},
  archive   = {C_IROS},
  author    = {Takahiro Miki and Lorenz Wellhausen and Ruben Grandia and Fabian Jenelten and Timon Homberger and Marco Hutter},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981507},
  pages     = {2273-2280},
  title     = {Elevation mapping for locomotion and navigation using GPU},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robot-aided microbial density estimation and mapping.
<em>IROS</em>, 2265–2272. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Estimating the microbial infestation profile of an area is essential for an effective cleaning process. However, current methods used to inspect the microbial infestation within a spatial region are manual and laborious. For large regions that require automated cleaning, conventional methods of microbial examination are not practical. We propose a novel robot-aided microbial density estimation and mapping framework using an in-house developed biosensor payload onboard a mobile robot. The biosensor estimates the degree of microbial infestation in Relative Light Units (RLU) using the natural bio-luminescence reaction. The global distribution of microbial infestation is approximated through the Radial Basis Function (RBF) and Nearest Neighbour (NN) interpolation algorithms. The proposed method is implemented on an in-house developed mobile robot called Beluga. The framework&#39;s validation and usefulness are demonstrated quantitatively through real-world experiment trials.},
  archive   = {C_IROS},
  author    = {J. J. J. Pey and A. P. Povendhan and T. Pathmakumar and M. R. Elara},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981278},
  pages     = {2265-2272},
  title     = {Robot-aided microbial density estimation and mapping},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to complete object shapes for object-level mapping
in dynamic scenes. <em>IROS</em>, 2257–2264. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel object-level mapping system that can simultaneously segment, track, and reconstruct objects in dynamic scenes. It can further predict and complete their full geometries by conditioning on reconstructions from depth inputs and a category-level shape prior with the aim that completed object geometry leads to better object reconstruction and tracking accuracy. For each incoming RGB-D frame, we perform instance segmentation to detect objects and build data associations between the detection and the existing object maps. A new object map will be created for each unmatched detection. For each matched object, we jointly optimise its pose and latent geometry representations using geometric residual and differential rendering residual towards its shape prior and completed geometry. Our approach shows better tracking and reconstruction performance compared to methods using traditional volumetric mapping or learned shape prior approaches. We evaluate its effectiveness by quantitatively and qualitatively testing it in both synthetic and real-world sequences.},
  archive   = {C_IROS},
  author    = {Binbin Xu and Andrew J. Davison and Stefan Leutenegger},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981545},
  pages     = {2257-2264},
  title     = {Learning to complete object shapes for object-level mapping in dynamic scenes},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GeoROS: Georeferenced real-time orthophoto stitching with
unmanned aerial vehicle. <em>IROS</em>, 2250–2256. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simultaneous orthophoto stitching during the flight of Unmanned Aerial Vehicles (UAV) can greatly promote the practicability and instantaneity of diverse applications such as emergency disaster rescue, digital agriculture, and cadastral survey, which is of remarkable interest in aerial photogrammetry. However, the inaccurately estimated camera poses and the intuitive fusion strategy of existing methods lead to misalignment and distortion artifacts in orthophoto mosaics. To address these issues, we propose a Georeferenced Real-time Orthophoto Stitching method (GeoROS), which can achieve efficient and accurate camera pose estimation through exploiting geolocation information in monocular visual simultaneous localization and mapping (SLAM) and fuse transformed images via orthogonality-preserving criterion. Specifically, in the SLAM process, georeferenced tracking is employed to acquire high-quality initial camera poses with a geolocation based motion model and facilitate non-linear pose optimization. Meanwhile, we design a georeferenced mapping scheme by introducing robust geolocation constraints in joint optimization of camera poses and the position of landmarks. Finally, aerial images warped with localized cameras are fused by considering both the orthogonality of camera orientation relative to the ground plane and the pixel centrality to fulfill global orthorectification. Besides, we construct two datasets with global navigation satellite system (GNSS) information of different scenarios and validate the superiority of our GeoROS method compared with state-of-the-art methods in accuracy and efficiency.},
  archive   = {C_IROS},
  author    = {Guangze Gao and Mengke Yuan and Zhihao Ma and Jiaming Gu and Weiliang Meng and Shibiao Xu and Xiaopeng Zhang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981560},
  pages     = {2250-2256},
  title     = {GeoROS: Georeferenced real-time orthophoto stitching with unmanned aerial vehicle},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-camera-LiDAR auto-calibration by joint
structure-from-motion. <em>IROS</em>, 2242–2249. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multiple sensors, especially cameras and LiDARs, are widely used in autonomous vehicles. In order to fuse data from different sensors accurately, precise calibrations are required, including camera intrinsic parameters, and relative poses between multiple cameras and LiDARs. However, most existing camera-LiDAR calibration methods need to place manually designed calibration objects in multiple locations and multiple times, which are time-consuming and labor-intensive, and are not suitable for frequent use. To address that, in this paper we proposed a novel calibration pipeline that can automatically calibrate multiple cameras and multiple LiDARs in a Structure-from-Motion (SfM) process. In our pipeline, we first perform a global SfM on all images with the help of rough LiDAR data to get the initial poses of all sensors. Then, feature points on lines and planes are extracted from both SfM point cloud and LiDARs. With these features, a global Bundle Adjustment is performed to minimize the point reprojection errors, point-to-line errors, and point-to-plane errors together. During this minimization process, camera intrinsic parameters, camera and LiDAR poses, and SfM point cloud are refined jointly. The proposed method uses the characteristics of natural scenes, does not require manually designed calibration objects, and incorporates all calibration parameters into a unified optimization framework. Experiments on autonomous vehicles with different sensor configurations demonstrate the effectiveness and robustness of the proposed method.},
  archive   = {C_IROS},
  author    = {Diantao Tu and Baoyu Wang and Hainan Cui and Yuqian Liu and Shuhan Shen},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981532},
  pages     = {2242-2249},
  title     = {Multi-camera-LiDAR auto-calibration by joint structure-from-motion},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-supervised wide baseline visual servoing via 3D
equivariance. <em>IROS</em>, 2227–2233. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One of the challenging input settings for visual servoing is when the initial and goal camera views are far apart. Such settings are difficult because the wide baseline can cause drastic changes in object appearance and cause occlusions. This paper presents a novel self-supervised visual servoing method for wide baseline images which does not require 3D ground truth supervision. Existing approaches that regress absolute camera pose with respect to an object require 3D ground truth data of the object in the forms of 3D bounding boxes or meshes. We learn a coherent visual representation by leveraging a geometric property called 3D equivariance—the representation is transformed in a predictable way as a function of 3D transformation. To ensure that the feature-space is faithful to the underlying geodesic space, a geodesic preserving constraint is applied in conjunction with the equivariance. We design a Siamese network that can effectively enforce these two geometric properties without requiring 3D supervision. With the learned model, the relative transformation can be inferred simply by following the gradient in the learned space and used as feedback for closed-loop visual servoing. Our method is evaluated on objects from the YCB dataset, showing meaningful outperformance on a visual servoing task, or object alignment task with respect to state-of-the-art approaches that use 3D supervision. Ours yields more than 35\% average distance error reduction and more than 90\% success rate with 3cm error tolerance.},
  archive   = {C_IROS},
  author    = {Jinwook Huh and Jungseok Hong and Suveer Garg and Hyun Soo Park and Volkan Isler},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981907},
  pages     = {2227-2233},
  title     = {Self-supervised wide baseline visual servoing via 3D equivariance},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DIJE: Dense image jacobian estimation for robust robotic
self-recognition and visual servoing. <em>IROS</em>, 2219–2226. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For robots to move in the real world, they must first correctly understand the state of its own body and the tools that it holds. In this research, we propose DIJE, an algorithm to estimate the image Jacobian for every pixel. It is based on an optical flow calculation and a simplified Kalman Filter that can be efficiently run on the whole image in real time. It does not rely on markers nor knowledge of the robotic structure. We use the DIJE in a self-recognition process which can robustly distinguish between movement by the robot and by external entities, even when the motion overlaps. We also propose a visual servoing controller based on DIJE, which can learn to control the robot&#39;s body to conduct reaching movements or bimanual tool-tip control. The proposed algorithms were implemented on a physical musculoskeletal robot and its performance was verified. We believe that such global estimation of the visuomotor policy has the potential to be extended into a more general framework for manipulation.},
  archive   = {C_IROS},
  author    = {Yasunori Toshimitsu and Kento Kawaharazuka and Akihiro Miki and Kei Okada and Masayuki Inaba},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981868},
  pages     = {2219-2226},
  title     = {DIJE: Dense image jacobian estimation for robust robotic self-recognition and visual servoing},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Vision-based rotational control of an agile observation
satellite. <em>IROS</em>, 2211–2218. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent Earth observation satellites are now equipped with new instrument that allows image feedback in real-time. Problematic such as ground target tracking, moving or not, can now be addressed by precisely controlling the satellite attitude. In this paper, we propose to consider this problem using a visual servoing approach. While focusing on the target, the control scheme has also to take into account the satellite motion induced by its orbit, Earth rotational velocities, potential target own motion, but also rotational velocities and accelerations constraints of the system. We show the efficiency of our system using both simulation (considering real Earth image) and experiments on a robot that replicates actual high resolution satellite constraints.},
  archive   = {C_IROS},
  author    = {Maxime Robic and Renaud Fraisse and Eric Marchand and François Chaumette},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981398},
  pages     = {2211-2218},
  title     = {Vision-based rotational control of an agile observation satellite},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An event-triggered visual servoing predictive control
strategy for the surveillance of contour-based areas using multirotor
aerial vehicles. <em>IROS</em>, 2203–2210. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, an Event-triggered Image-based Visual Servoing Nonlinear Model Predictive Controller (ET-IBVS-NMPC) for multirotor aerial vehicles is presented. The proposed scheme is developed for the autonomous surveillance of contour-based areas with different characteristics (e.g. forest paths, coastlines, road pavements). For this purpose, an appropriately trained Deep Neural Network (DNN) is employed for the accurate detection of the contours. In an effort to reduce the remarkably large computational cost required by an IBVS-NMPC algorithm, a triggering condition is designed to define when the Optimal Control Problem (OCP) should be resolved and new control inputs will be calculated. Between two successive triggering instants, the control input trajectory is applied to the robot in an open-loop fashion, which means that no control input computations are required. As a result, the system&#39;s computing effort and energy consumption are lowered, while its autonomy and flight duration are increased. The visibility and input constraints, as well as the external disturbances, are all taken into account throughout the control design. The efficacy of the proposed strategy is demonstrated through a series of real-time experiments using a quadrotor and an octorotor both equipped with a monocular downward looking camera.},
  archive   = {C_IROS},
  author    = {Sotirios N. Aspragkathos and Mario Sinani and George C. Karras and Fotis Panetsos and Kostas J. Kyriakopoulos},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981176},
  pages     = {2203-2210},
  title     = {An event-triggered visual servoing predictive control strategy for the surveillance of contour-based areas using multirotor aerial vehicles},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal shape servoing with task-focused convergence
constraints. <em>IROS</em>, 2197–2202. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most deformable object manipulation tasks still rely on skillful human operators. To automate such tasks, a robotic system should not only be able to deform an object to a desired shape but also servo its deformation along a specific path towards the desired shape. We propose a shape servoing control scheme to automate such tasks. Our scheme controls the deformation trajectory towards the desired shape by imposing task-focused convergence constraints. The constraints impose how fast the different regions of the object converge to the desired shape. Integrating such a behavior in shape servoing forms our main contribution. Experiments, carried out on rubber layer assembly tasks, show that our control scheme outperforms a state-of-the-art shape servoing scheme.},
  archive   = {C_IROS},
  author    = {Victor H. Giraud and Maxime Padrin and Mohammadreza Shetab-Bushehri and Chedli Bouzgarrou and Youcef Mezouar and Erol Ozgur},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981902},
  pages     = {2197-2202},
  title     = {Optimal shape servoing with task-focused convergence constraints},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conditional visual servoing for multi-step tasks.
<em>IROS</em>, 2190–2196. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual Servoing has been effectively used to move a robot into specific target locations or to track a recorded demonstration. It does not require manual programming, but it is typically limited to settings where one demonstration maps to one environment state. We propose a modular approach to extend visual servoing to scenarios with multiple demonstration sequences. We call this conditional servoing, as we choose the next demonstration conditioned on the observation of the robot. This method presents an appealing strategy to tackle multi-step problems, as individual demonstrations can be combined flexibly into a control policy. We propose different selection functions and compare them on a shape-sorting task in simulation. With the reprojection error yielding the best overall results, we implement this selection function on a real robot and show the efficacy of the proposed conditional servoing. For videos of our experiments, please check out our project page: https://lmb.informatik.uni-freiburg.de/projects/conditional_servoing/},
  archive   = {C_IROS},
  author    = {Sergio Izquierdo and Max Argus and Thomas Brox},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981443},
  pages     = {2190-2196},
  title     = {Conditional visual servoing for multi-step tasks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Skeleton-based adaptive visual servoing for control of
robotic manipulators in configuration space. <em>IROS</em>, 2182–2189.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel visual servoing method that controls a robotic manipulator in the configuration space as opposed to the classical vision-based control methods solely focusing on the end effector pose. We first extract the robot&#39;s shape from depth images using a skeletonization algorithm and represent it using parametric curves. We then adopt an adaptive visual servoing scheme that estimates the Jacobian online relating the changes of the curve parameters and the joint velocities. The proposed scheme does not only enable controlling a manipulator in the configuration space, but also demonstrates a better transient response while converging to the goal configuration compared to the classical adaptive visual servoing methods. We present simulations and real robot experiments that demonstrate the capabilities of the proposed method and analyze its performance, robustness, and repeatability compared to the classical algorithms.},
  archive   = {C_IROS},
  author    = {Abhinav Gandhi and Sreejani Chatterjee and Berk Calli},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981159},
  pages     = {2182-2189},
  title     = {Skeleton-based adaptive visual servoing for control of robotic manipulators in configuration space},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An offline geometric model for controlling the shape of
elastic linear objects. <em>IROS</em>, 2175–2181. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a new approach to control the shape of deformable objects with robots. Specifically, we consider a fixed-length elastic linear object lying on a 2D workspace. Our main idea is to encode the object&#39;s deformation behavior in an offline constant Jacobian matrix. To derive this Jacobian, we use geometric deformation modeling and combine recent work from the fields of deformable object control and multirobot systems. Based on this Jacobian, we then propose a robotic control law that is capable of driving a set of shape features on the object toward prescribed values. Our contribution relative to existing approaches is that at run-time we do not need to measure the full shape of the object or to estimate/simulate a deformation model. This simplification is achieved thanks to having abstracted the deformation behavior as an offline model. We illustrate the proposed approach in simulation and in experiments with real deformable linear objects.},
  archive   = {C_IROS},
  author    = {Omid Aghajanzadeh and Miguel Aranda and Gonzalo López-Nicolás and Roland Lenain and Youcef Mezouar},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981466},
  pages     = {2175-2181},
  title     = {An offline geometric model for controlling the shape of elastic linear objects},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VR facial animation for immersive telepresence avatars.
<em>IROS</em>, 2167–2174. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {VR Facial Animation is necessary in applications requiring clear view of the face, even though a VR headset is worn. In our case, we aim to animate the face of an operator who is controlling our robotic avatar system. We propose a real-time capable pipeline with very fast adaptation for specific operators. In a quick enrollment step, we capture a sequence of source images from the operator without the VR headset which contain all the important operator-specific appearance information. During inference, we then use the operator keypoint information extracted from a mouth camera and two eye cameras to estimate the target expression and head pose, to which we map the appearance of a source still image. In order to enhance the mouth expression accuracy, we dynamically select an auxiliary expression frame from the captured sequence. This selection is done by learning to transform the current mouth keypoints into the source camera space, where the alignment can be determined accurately. We, furthermore, demonstrate an eye tracking pipeline that can be trained in less than a minute, a time efficient way to train the whole pipeline given a dataset that includes only complete faces, show exemplary results generated by our method, and discuss performance at the ANA Avatar XPRIZE semifinals.},
  archive   = {C_IROS},
  author    = {Andre Rochow and Max Schwarz and Michael Schreiber and Sven Behnke},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981892},
  pages     = {2167-2174},
  title     = {VR facial animation for immersive telepresence avatars},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ProTAMP: Probabilistic task and motion planning considering
human action for harmonious collaboration. <em>IROS</em>, 2159–2166. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For the proper functioning of mobile manipulator-type autonomous robot performing complicated tasks in a human-robot coexistence environment, tasks and motions must be planned simultaneously. In such environments, a human and robot should collaborate with each other. Therefore, the robot must act in accordance with the human and avoid useless actions duplicated with those of humans. However, any action undertaken by a human has uncertainty, and thus, predicting them correctly is challenging. This study proposed probabilistic task and motion planning considering both deterministic and probabilistic environment changes caused by robot and human actions temporarily and spatially, respectively. First, the environmental changes were modeled, where the robot is capable of recognizing the possibility of environmental changes. Second, in task planning, the probabilities of each environmental change owing to human actions was minimized. Finally, in motion planning, a movement path connecting each task in a planned order was planned, thereby enabling the robot to perform actions not duplicated with those by a human. Furthermore, the plans generated were compared without considering possibility of human actions and the effectiveness of the proposed method was verified. Consequently, the proposed method was confirmed to reduce the time required for finishing the tasks.},
  archive   = {C_IROS},
  author    = {Shunsuke Mochizuki and Yosuke Kawasaki and Masaki Takahashi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982074},
  pages     = {2159-2166},
  title     = {ProTAMP: Probabilistic task and motion planning considering human action for harmonious collaboration},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multimodal object categorization with reduced user load
through human-robot interaction in mixed reality. <em>IROS</em>,
2143–2150. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Enabling robots to learn from interactions with users is essential to perform service tasks. However, as a robot categorizes objects from multimodal information obtained by its sensors during interactive onsite teaching, the inferred names of unknown objects do not always match the human user&#39;s expectation, especially when the robot is introduced to new environments. Confirming the learning results through natural speech interaction with the robot often puts an additional burden on the user who can only listen to the robot to validate the results. Therefore, we propose a human-robot interface to reduce the burden on the user by visualizing the inferred results in mixed reality (MR). In particular, we evaluate the proposed interface on the system usability scale (SUS) and the NASA task load index (NASA-TLX) with three experimental object categorization scenarios based on multimodal latent Dirichlet allocation (MLDA) in which the robot: 1) does not share the inferred results with the user at all, 2) shares the inferred results through speech interaction with the user (baseline), and 3) shares the inferred results with the user through an MR interface (proposed). We show that providing feedback through an MR interface significantly reduces the temporal, physical, and mental burden on the human user compared to speech interaction with the robot.},
  archive   = {C_IROS},
  author    = {Hitoshi Nakamura and Lotfi El Hafi and Akira Taniguchi and Yoshinobu Hagiwara and Tadahiro Taniguchi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981374},
  pages     = {2143-2150},
  title     = {Multimodal object categorization with reduced user load through human-robot interaction in mixed reality},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). “I’m confident this will end poorly”: Robot proficiency
self-assessment in human-robot teaming. <em>IROS</em>, 2127–2134. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human-robot teams are expected to accomplish complex tasks in high-risk and uncertain environments. In domains such as space exploration or search &amp; rescue, a human operator may not be a robotics expert, but will need to establish a baseline understanding of the robot&#39;s capabilities with respect to a given task in order to appropriately utilize and rely on the robot. This willingness to rely, also known as trust, is based partly on the operator&#39;s belief in the robot&#39;s task proficiency. If trust is too high, the operator may unknowingly push the robot beyond its capabilities. If trust is too low, the operator may not utilize it when they otherwise could have, wasting precious time and resources. In this work, we discuss results from an online human-subjects study investigating how a robot communicated report of its task proficiency with respect to an operator&#39;s expectations affects trust and performance in a navigation task. Our results show that communication of a robot self-assessment helped operators understand when reliance on the robot was appropriate given the task and conditions. This led to improvements in task performance, informed choices of autonomy level, and increased trust.},
  archive   = {C_IROS},
  author    = {Nicholas Conlon and Daniel Szafir and Nisar Ahmed},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981653},
  pages     = {2127-2134},
  title     = {“I&#39;m confident this will end poorly”: Robot proficiency self-assessment in human-robot teaming},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Safety-based dynamic task offloading for human-robot
collaboration using deep reinforcement learning. <em>IROS</em>,
2119–2126. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots with constrained hardware resources usually rely on Multi-access Edge Computing infrastructures to offload computationally expensive tasks to meet real-time and safety requirements. Offloading every task might not be the best option due to dynamic changes in the network conditions and can result in network congestion or failures. This work proposes a task offloading strategy for mobile robots in a Human-Robot Collaboration scenario that optimizes the edge resource usage and reduces network delays, leading to safety enhancement. The solution utilizes a Deep Reinforcement Learning (DRL) agent that observes safety and network metrics to dynamically decide at runtime if (i) a less accurate model should run on the robot; (ii) a more complex model should run on the edge; or (iii) the previous output should be reused through temporal coherence verification. Experiments are performed in a simulated warehouse where humans and robots have close interactions and safety needs are high. Our results show that the proposed DRL solution outperforms the baselines in several aspects. The edge is used only when the network performance is reliable, reducing the number of failures (up to 47\%). The latency is not only decreased (up to 68\%) but also adapted to the safety requirements (risk × latency reduced up to 48\%), avoiding unnecessary network congestion in safe situations and letting other devices use the network. Overall, the safety metrics get improved, such as the increased time in the safe zone by up to 3.1\%.},
  archive   = {C_IROS},
  author    = {Franco Ruggeri and Ahmad Terra and Alberto Hata and Rafia Inam and Iolanda Leite},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981366},
  pages     = {2119-2126},
  title     = {Safety-based dynamic task offloading for human-robot collaboration using deep reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Controller design of a robotic assistant for the transport
of large and fragile objects. <em>IROS</em>, 2111–2118. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper deals with the design of a robotic assistant for the transport of large and fragile objects. We propose a new collaborative robotic controller that fulfills the main requirements of co-transportation tasks of large and fragile objects: to execute any trajectory in a collaborative mode while minimizing the stress applied on the object by both partners in order to avoid damaging it. This controller prevents the robot from applying torques on the object while maintaining a desired orientation of the object along the transport trajectory in order to follow the operator. An original feature of our approach is to care about torques applied by both partners (not only by operator) during any co-manipulation trajectory execution. It leads to a novel outcome: the minimization of stress applied by both partners on a large and fragile object during its transport on any trajectory. We demonstrate the effectiveness of this approach in a collaborative transportation task.},
  archive   = {C_IROS},
  author    = {Julie Dumora and Julien Nicolas and Franck Geffard},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981993},
  pages     = {2111-2118},
  title     = {Controller design of a robotic assistant for the transport of large and fragile objects},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multi-granularity scene segmentation network for
human-robot collaboration environment perception. <em>IROS</em>,
2105–2110. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human-robot collaboration (HRC) has been considered as a promising paradigm towards futuristic human-centric smart manufacturing, to meet the thriving needs of mass personalization. In this context, existing robotic systems normally adopt a single-granularity semantic segmentation scheme for environment perception, which lacks the flexibility to be implemented to various HRC situations. To fill the gap, this study proposes a multi-granularity scene segmentation network. Inspired by some recent network designs, we construct an encoder network with two ConvNext-T backbones for RGB and depth respectively, and an decoder network consisting of multi-scale supervision and multi-granularity segmentation branches. The proposed model is demonstrated in a human-robot collaborative battery disassembly scenario and further evaluated in comparison with state-of-the-art RGB-D semantic segmentation methods on the NYU-Depth V2 dataset.},
  archive   = {C_IROS},
  author    = {Junming Fan and Pai Zheng and Carman K.M. Lee},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981684},
  pages     = {2105-2110},
  title     = {A multi-granularity scene segmentation network for human-robot collaboration environment perception},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-purpose tactile perception based on deep learning in a
new tendon-driven optical tactile sensor. <em>IROS</em>, 2099–2104. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we create a new tendon-connected multi-functional optical tactile sensor, MechTac, for object perception in the field of view (TacTip) and location of touching points in the blind area of vision (TacSide). In a multi-point touch task, the information of the TacSide and the TacTip are overlapped to commonly affect the distribution of papillae pins on the TacTip. Since the effects of TacSide are much less obvious to those affected on the TacTip, a perceiving out-of-view neural network (O 2 VNet) is created to separate the mixed information with unequal affection. To reduce the dependence of the O 2 VNet on the grayscale information of the image, we create one new binarized convolutional (BConv) layer in front of the backbone of the O 2 VNet. The O 2 VNet can not only achieve real-time temporal sequence prediction (34 ms per image), but also attain the average classification accuracy of 99.06\%. The experimental results show that the O 2 VNet can hold a high classification accuracy even facing the image contrast changes.},
  archive   = {C_IROS},
  author    = {Zhou Zhao and Zhenyu Lu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981477},
  pages     = {2099-2104},
  title     = {Multi-purpose tactile perception based on deep learning in a new tendon-driven optical tactile sensor},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semi-supervised disentanglement of tactile contact geometry
from sliding-induced shear. <em>IROS</em>, 2092–2098. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The sense of touch is fundamental to human dexterity. When mimicked in robotic touch, particularly by use of soft optical tactile sensors, it suffers from distortion due to motion-dependent shear. This complicates tactile tasks like shape reconstruction and exploration that require information about contact geometry. In this work, we pursue a semi-supervised approach to remove shear while preserving contact-only information. We validate our approach by showing a match between the model-generated unsheared images with their counterparts from vertically tapping onto the object. The model-generated unsheared images give faithful reconstruction of contact-geometry otherwise masked by shear, along with robust estimation of object pose then used for sliding exploration and full reconstruction of several planar shapes. We show that our semi-supervised approach achieves comparable performance to its fully supervised counterpart across all validation tasks with an order of magnitude less supervision. The semi-supervised method is thus more computational and labeled sample-efficient. We expect it will have broad applicability to wide range of complex tactile exploration and manipulation tasks performed via a shear-sensitive sense of touch.},
  archive   = {C_IROS},
  author    = {Anupam K. Gupta and Alex Church and Nathan F. Lepora},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982165},
  pages     = {2092-2098},
  title     = {Semi-supervised disentanglement of tactile contact geometry from sliding-induced shear},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sparse PointPillars: Maintaining and exploiting input
sparsity to improve runtime on embedded systems. <em>IROS</em>,
2025–2031. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bird&#39;s Eye View (BEV) is a popular representation for processing 3D point clouds, and by its nature is fundamentally sparse. Motivated by the computational limitations of mobile robot platforms, we create a fast, high-performance BEV 3D object detector that maintains and exploits this input sparsity to decrease runtimes over non-sparse baselines and avoids the tradeoff between pseudoimage area and runtime. We present results on KITTI, a canonical 3D detection dataset, and Matterport-Chair, a novel Matterport3D-derived chair detection dataset from scenes in real furnished homes. We evaluate runtime characteristics using a desktop GPU, an embedded ML accelerator, and a robot CPU, demonstrating that our method results in significant detection speedups (2 × or more) for embedded systems with only a modest decrease in detection quality. Our work represents a new approach for practitioners to optimize models for embedded systems by maintaining and exploiting input sparsity throughout their entire pipeline to reduce runtime and resource usage while preserving detection performance. All models, weights, experimental configurations, and datasets used are publicly available 1 1 https://vedder.io/sparse_point_pillars.},
  archive   = {C_IROS},
  author    = {Kyle Vedder and Eric Eaton},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981550},
  pages     = {2025-2031},
  title     = {Sparse PointPillars: Maintaining and exploiting input sparsity to improve runtime on embedded systems},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Early recall, late precision: Multi-robot semantic object
mapping under operational constraints in perceptually-degraded
environments. <em>IROS</em>, 2017–2024. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Semantic object mapping in uncertain, perceptually degraded environments during long-range multi-robot autonomous exploration tasks such as search-and-rescue is important and challenging. During such missions, high recall is desirable to avoid missing true target objects and high precision is also critical to avoid wasting valuable operational time on false positives. Given recent advancements in visual perception algorithms, the former is largely solvable autonomously, but the latter is difficult to address without the supervision of a human operator. However, operational constraints such as mission time, computational requirements and mesh network bandwidth can make the operator&#39;s task infeasible unless properly managed. We propose the Early Recall, Late Precision (EaRLaP) semantic object mapping pipeline to solve this problem. EaRLaP was used by Team CoSTAR in DARPA Subterranean Challenge, where it successfully detected all the artifacts encountered by the team of robots. We will discuss these results and the performance of the EaRLaP on various datasets.},
  archive   = {C_IROS},
  author    = {Xianmei Lei and Taeyeon Kim and Nicolas Marchal and Daniel Pastor and Barry Ridge and Frederik Schöller and Edward Terry and Fernando Chavez and Thomas Touma and Kyohei Otsu and Benjamin Morrell and Ali Agha},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982267},
  pages     = {2017-2024},
  title     = {Early recall, late precision: Multi-robot semantic object mapping under operational constraints in perceptually-degraded environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Instance segmentation with cross-modal consistency.
<em>IROS</em>, 2009–2016. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Segmenting object instances is a key task in machine perception, with safety-critical applications in robotics and autonomous driving. We introduce a novel approach to instance segmentation that jointly leverages measurements from multiple sensor modalities, such as cameras and LiDAR. Our method learns to predict embeddings for each pixel or point that give rise to a dense segmentation of the scene. Specifically, our technique applies contrastive learning to points in the scene both across sensor modalities and the temporal domain. We demonstrate that this formulation encourages the models to learn embeddings that are invariant to viewpoint variations and consistent across sensor modalities. We further demonstrate that the embeddings are stable over time as objects move around the scene. This not only provides stable instance masks, but can also provide valuable signals to downstream tasks, such as object tracking. We evaluate our method on the Cityscapes and KITTI-360 datasets. We further conduct a number of ablation studies, demonstrating benefits when applying additional inputs for the contrastive loss.},
  archive   = {C_IROS},
  author    = {Alex Zihao Zhu and Vincent Casser and Reza Mahjourian and Henrik Kretzschmar and Sören Pirk},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982285},
  pages     = {2009-2016},
  title     = {Instance segmentation with cross-modal consistency},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving single-view mesh reconstruction for unseen
categories via primitive-based representation and mesh augmentation.
<em>IROS</em>, 2001–2008. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As most existing works of single-view 3D reconstruction aim at learning the better mapping functions to directly transform the 2D observation into the corresponding 3D shape for achieving state-of-the-art performance, there often comes a potential concern on having the implicit bias towards the seen classes learnt in their models (i.e. reconstruction intertwined with the classification) thus leading to poor generalizability for the unseen object categories. Moreover, such implicit bias typically stemmed from adopting the object-centered coordinate in their model designs, in which the reconstructed 3D shapes of the same class are all aligned to the same canonical pose regardless of different view-angles in the 2D observations. To this end, we propose an end-to-end framework to reconstruct the 3D mesh from a single image, where the reconstructed mesh is not only view-centered (i.e. its 3D pose respects the viewpoint of the 2D observation) but also preliminarily represented as a composition of volumetric 3D primitives before being further deformed into the fine-grained mesh to capture the shape details. In particular, the usage of volumetric primitives is motivated from the assumption that there generally exists some similar shape parts shared across various object categories, learning to estimate the primitive-based 3D model thus becomes more generalizable to the unseen categories. Furthermore, we advance to propose a novel mesh augmentation strategy, CvxRearrangement, to enrich the distribution of training shapes, which contributes to increasing the robustness of our proposed model and achieves better generalization. Extensive experiments demonstrate that our proposed method provides superior performance on both unseen and seen classes in comparison to several representative baselines of single-view 3D reconstruction.},
  archive   = {C_IROS},
  author    = {Yu-Liang Kuo and Wei-Jan Ko and Chen-Yi Chiu and Wei-Chen Chiu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982024},
  pages     = {2001-2008},
  title     = {Improving single-view mesh reconstruction for unseen categories via primitive-based representation and mesh augmentation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast hierarchical learning for few-shot object detection.
<em>IROS</em>, 1993–2000. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Transfer learning based approaches have recently achieved promising results on the few-shot detection task. These approaches however suffer from “catastrophic forgetting” issue due to finetuning of base detector, leading to sub-optimal performance on the base classes. Furthermore, the slow convergence rate of stochastic gradient descent (SGD) results in high latency and consequently restricts real-time applications. We tackle the aforementioned issues in this work. We pose few-shot detection as a hierarchical learning problem, where the novel classes are treated as the child classes of existing base classes and the background class. The detection heads for the novel classes are then trained using a specialized optimization strategy, leading to significantly lower training times compared to SGD. Our approach obtains competitive novel class performance on few-shot MS-COCO benchmark, while completely retaining the performance of the initial model on the base classes. We further demonstrate the application of our approach to a new class-refined few-shot detection task.},
  archive   = {C_IROS},
  author    = {Yihang She and Goutam Bhat and Martin Danelljan and Fisher Yu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981327},
  pages     = {1993-2000},
  title     = {Fast hierarchical learning for few-shot object detection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CloudAttention: Efficient multi-scale attention scheme for
3D point cloud learning. <em>IROS</em>, 1986–1992. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Processing 3D data efficiently has always been a challenge. Spatial operations on large-scale point clouds, stored as sparse data, require extra cost. Attracted by the success of transformers, researchers are using multi-head attention for vision tasks. However, attention calculations in transformers come with quadratic complexity in the number of inputs and miss spatial intuition on sets like point clouds. We redesign set transformers in this work and incorporate them into a hierarchical framework for shape classification and part and scene segmentation. We propose our local attention unit, which captures features in a spatial neighborhood. We also compute efficient and dynamic global cross attentions by leveraging sampling and grouping at each iteration. Finally, to mitigate the non-heterogeneity of point clouds, we propose an efficient Multi-Scale Tokenization (MST), which extracts scale-invariant tokens for attention operations. The proposed hierarchical model achieves state-of-the-art shape classification in mean accuracy and yields results on par with the previous segmentation methods while requiring significantly fewer computations. Our proposed architecture predicts segmentation labels with around half the latency and parameter count of the previous most effi-cient method with comparable performance. The code is available at https://github.com/YigeWang-WHU/CloudAttention.},
  archive   = {C_IROS},
  author    = {Mahdi Saleh and Yige Wang and Nassir Navab and Benjamin Busam and Federico Tombari},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982276},
  pages     = {1986-1992},
  title     = {CloudAttention: Efficient multi-scale attention scheme for 3D point cloud learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conditional patch-based domain randomization: Improving
texture domain randomization using natural image patches. <em>IROS</em>,
1979–1985. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Using Domain Randomized synthetic data for training deep learning systems is a promising approach for addressing the data and the labeling requirements for supervised techniques to bridge the gap between simulation and the real world. We propose a novel approach for generating and applying class-specific Domain Randomization textures by using randomly cropped image patches from real-world data. In evaluation against the current Domain Randomization texture application techniques, our approach outperforms the highest performing technique by 4.94 AP and 6.71 AP when solving object detection and semantic segmentation tasks on the YCB-M [1] real-world robotics dataset. Our approach is a fast and inexpensive way of generating Domain Randomized textures while avoiding the need to handcraft texture distributions currently being used.},
  archive   = {C_IROS},
  author    = {Mohammad Ani and Hector Basevi and Aleš Leonardis},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981381},
  pages     = {1979-1985},
  title     = {Conditional patch-based domain randomization: Improving texture domain randomization using natural image patches},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). New objects on the road? No problem, we’ll learn them too.
<em>IROS</em>, 1972–1978. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Object detection plays an essential role in providing localization, path planning, and decision making capabilities in autonomous navigation systems. However, existing object detection models are trained and tested on a fixed number of known classes. This setting makes the object detection model difficult to generalize well in real-world road scenarios while encountering an unknown object. We address this problem by introducing our framework that handles the issue of unknown object detection and updates the model when unknown object labels are available. Next, our solution includes three major components that address the inherent problems present in the road scene datasets. The novel components are a) Feature-Mix that improves the unknown object detection by widening the gap between known and unknown classes in latent feature space, b) Focal regression loss handling the problem of improving small object detection and intra-class scale variation, and c) Curriculum learning further enhances the detection of small objects. We use Indian Driving Dataset (IDD) and Berkeley Deep Drive (BDD) dataset for evaluation. Our solution provides state-of-the-art performance on open-world evaluation metrics. We hope this work will create new directions for open-world object detection for road scenes, making it more reliable and robust autonomous systems.},
  archive   = {C_IROS},
  author    = {Deepak Kumar Singh and Shyam Nandan Rai and K J Joseph and Rohit Saluja and Vineeth N Balasubramanian and Chetan Arora and Anbumani Subramanian and C.V. Jawahar},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981886},
  pages     = {1972-1978},
  title     = {New objects on the road? no problem, we&#39;ll learn them too},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time IMU-based learning: A classification of contact
materials. <em>IROS</em>, 1965–1971. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In modern highly dynamic robot manipulation, collisions between a robot and objects may be intentionally executed to improve performance. To distinguish between these deliberate contacts and accidental collisions beyond the limit of state-of-the-art human-robot interactions, new sensing approaches are required. This work seeks an easy-to-implement and real-time capable solution to detect the identity of the impacted material. We developed an inertial measurement unit (IMU) based setup that records vibration signals occurring after collisions. Furthermore, a data-set was generated in an unsupervised learning manner using the measurements of collision experiments with several materials commonly used in realistic applications. The data-set was used to train an artificial neural network to classify the type of material involved. Our results show that the neural net detects collisions and a detailed distinction between materials is achieved, even with estimating different human body parts. The unsupervised data-set generation allows for a simple integration of new classes, which provides broader applicability of our approach. As the calculations are running faster than the control cycle of the robot, the output of our classifier can be used in real-time to decide about the robots reaction behavior.},
  archive   = {C_IROS},
  author    = {Carlos Magno C. O. Valle and Alexander Kurdas and Edmundo Pozo Fortunić and Saeed Abdolshah and Sami Haddadin},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981139},
  pages     = {1965-1971},
  title     = {Real-time IMU-based learning: A classification of contact materials},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design and characterization of 3D printed, open-source
actuators for legged locomotion. <em>IROS</em>, 1957–1964. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Impressive animal locomotion capabilities are mediated by the co-evolution of the skeletal morphology and muscular properties. Legged robot performance would also likely benefit from the co-optimization of actuators and leg morphology. However, development of custom actuators for legged robots is expensive and time consuming, discouraging application-specific actuator optimization. This paper presents open-source designs for two quasi-direct-drive actuators with performance regimes appropriate for an 8–15 kg robot, built from off the shelf and 3D-printed components for less than $200 USD each. The mechanical, electrical, and thermal properties of each actuator are characterized and compared to benchmark data. Actuators subjected to 420k strides of gait data experienced only a 2\% reduction in efficiency and 26 mrad in backlash growth, demonstrating viability for rigorous and sustained research applications. We present a thermal solution that nearly doubles the thermally-driven torque limits of our plastic actuator design. The performance results are comparable to traditional metallic actuators for use in high-speed legged robots of the same scale. These 3D printed designs demonstrate an approach for designing and characterizing low-cost, highly customizable and reproducible actuators, democratizing the field of actuator design and enabling co-design and optimization of actuators and robot legs.},
  archive   = {C_IROS},
  author    = {Karthik Urs and Challen Enninful Adu and Elliott J. Rouse and Talia Y. Moore},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981940},
  pages     = {1957-1964},
  title     = {Design and characterization of 3D printed, open-source actuators for legged locomotion},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adjustable lever mechanism with double parallel link
platforms for robotic limbs. <em>IROS</em>, 1950–1956. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For universal robotic limbs, having a large workspace with high stiffness and adjustable output properties is important to adapt to various situations. A combination of parallel mechanisms that can change output characteristics is promising to meet these demands. As such, we propose a lever mechanism with double parallel link platforms. This mechanism is composed of a lever mechanism with the effort point and the pivot point; each is supported by a parallel link mechanism. First, we calculated the differential kinematics of this mechanism. Next, we investigated the workspace of the mechanism. The proposed mechanism can reach nearer positions than the posture with the most shrinking actuators thanks to the three-dimensional movable effort point. Then, we confirmed that this mechanism could change the output force profile at the end-effector by changing the lever ratio. The main change is the directional change of the maximum output force. The change range is larger when the squatting depth is larger. The changing tendency of the shape of the maximum output force profile by the position of the pivot plate depends on the force balance of the actuators. These analytical results show the potential of the proposed mechanism and would aid in the design of this mechanism for robotic limbs.},
  archive   = {C_IROS},
  author    = {Satoshi Nishikawa and Daigo Tokunaga and Kazuo Kiguchi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981521},
  pages     = {1950-1956},
  title     = {Adjustable lever mechanism with double parallel link platforms for robotic limbs},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fold-based complex joints for a 3 DoF 3R parallel robot
design. <em>IROS</em>, 1936–1941. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This contribution demonstrates the usage of fold-based joints to create a novel 3 DoF 3R(RPaR) parallel robot design. Multiple folding mechanisms are introduced, fulfilling the function of revolute, prismatic, and spherical joints. Folding mechanisms are here tested regarding their applicability in parallel kinematic robots taking advantage of beneficial properties such as increased stiffness, flat-foldability and compressed states, easy cleaning as well as lightweight designs. The designed delta robot structure is then analysed for its motion behaviour, workspace dimensions and validated by a 3D printed model. Further, scalability possibilities are presented.},
  archive   = {C_IROS},
  author    = {Judith U. Merz and Markus M. Huber and Franz Irlinger and Tim C. Lueth and Janik Pfitzner and Burkhard Corves},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981031},
  pages     = {1936-1941},
  title     = {Fold-based complex joints for a 3 DoF 3R parallel robot design},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 3D-printable low-reduction cycloidal gearing for robotics.
<em>IROS</em>, 1929–1935. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The recent trend towards low reduction gearing in robotic actuation has revitalised the need for high-performance gearing concepts. In this work we propose compact low-reduction cycloidal gearing, that is 3D-printable and combined with off-the-shelf components. This approach presents an enormous potential for high performance-to-cost implementations. After discussing parameter selection and design considerations, we present a prototype that is combined with a low-cost brushless motor to demonstrate its potential. Extensive experimental results demonstrate high performance, including &gt;40Nm torque, low friction and play, and high impact robustness. The results show that the proposed approach can yield viable gearbox designs.},
  archive   = {C_IROS},
  author    = {Wesley Roozing and Glenn Roozing},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982006},
  pages     = {1929-1935},
  title     = {3D-printable low-reduction cycloidal gearing for robotics},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design of a modular continuum robot with alterable
compliance using tubular-actuation. <em>IROS</em>, 1923–1928. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Compliance is good. However, it is challenging for one compliant continuum robot to finish both high precision manipulation and environmental-adapted motions. In this paper, a modular continuum robot with the alterable compliance characteristic is proposed. Besides, an actuation module is also proposed using a tubular-screw mechanism for non-slippage transmission. Kinematic analyses and dynamic co-simulation are performed to study the continuum robot. Furthermore, two potential application scenarios of pick-and-place manipulation and confined space navigating are carried out to demonstrate the advantages of the alterable compliance design. This study presents a capable continuum robotic solution for non-structural inspection tasks, with potential for in-situ applications in restricted and hazardous environments.},
  archive   = {C_IROS},
  author    = {Mingyuan Wang and Liang Du and Sheng Bao and Jianjun Yuan and Jinshu Zhou and Shugen Ma},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982238},
  pages     = {1923-1928},
  title     = {Design of a modular continuum robot with alterable compliance using tubular-actuation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Plate harmonic reducer with a profiled groove wave
generator. <em>IROS</em>, 1900–1907. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this study, a mechanism that realizes a novel structural form of the harmonic reducer is introduced. Conventional robots often use various mechanical reducers owing to low torque and high-speed characteristics of electric motors. Among them, harmonic reducers are frequently used because of their compact size and backlash-free precision. The plate harmonic reducer which uses the same topological geometry and reducing mechanism as the conventional harmonic reducer is a novel type of strain gear that changes its shape to a plate form for axial deformation. It has unique differences in terms of axial thickness, torsional stiffness, and efficiency due to its morphological characteristics. This study introduces and analyzes the reducing principle of the plate harmonic reducer and describes the methodological solutions for realization. Finally, the theoretical performance improvement and operating feasibility of the plate harmonic reducer are analyzed using finite element method and a 3D-printed prototype model.},
  archive   = {C_IROS},
  author    = {Seungbin You and Jaesug Jung and Eunho Sung and Jaeheung Park},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981317},
  pages     = {1900-1907},
  title     = {Plate harmonic reducer with a profiled groove wave generator},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Novel supernumerary robotic limb based on variable stiffness
actuators for hemiplegic patients assistance. <em>IROS</em>, 1892–1899.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Loss of upper extremity motor control and function is an unremitting symptom in post-stroke patients. This would impose hardships on accomplishing their daily life activities. Supernumerary robotic limbs (SRLs) were introduced as a solution to regain the lost Degrees of Freedom (DoFs) by introducing an independent new limb. The actuation systems in SRL can be categorized into rigid and soft actuators. Soft actuators have proven advantageous over their rigid counterparts through intrinsic safety, cost, and energy efficiency. However, they suffer from low stiffness, which jeopardizes their accuracy. Variable Stiffness Actuators (VSAs) are newly developed technologies that have been proven to ensure accuracy and safety. In this paper, we introduce the novel Supernumerary Robotic Limb based on Variable Stiffness Actuators. Based on our knowledge, the proposed proof-of-concept SRL is the first that utilizes Variable Stiffness Actuators. The developed SRL would assist post-stroke patients in bi-manual tasks, e.g., eating with a fork and knife. The modeling, design, and realization of the system are illustrated. The proposed SRL was evaluated and verified for its accuracy via predefined trajectories. The safety was verified by utilizing the momentum observer for collision detection, and several post-collision reaction strategies were evaluated through the Soft Tissue Injury Test. The assistance process is qualitatively verified through standard user-satisfaction questionnaire.},
  archive   = {C_IROS},
  author    = {Basma B. Hasanen and Mohammad I. Awad and Mohamed N. Boushaki and Zhenwei Niu and Mohammed A. Ramadan and Irfan Hussain},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981932},
  pages     = {1892-1899},
  title     = {Novel supernumerary robotic limb based on variable stiffness actuators for hemiplegic patients assistance},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Development of a cable-driven growing sling to assist
patient transfer. <em>IROS</em>, 1862–1867. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As the aging of society continues to accelerate, the number of elderly patients is increasing, as is the demand for manpower to care for them. In particular, there is an urgent need for bedridden patient care. However, limitations in the supply of human resources have caused an increase in the burden for care. In particular, nursing personnel often experience inconvenience and difficulties owing to the great deal of effort required to transfer a patient from bed to wheelchair, or vice versa. The most difficult process during the patient transfer is inserting the sling under the patient. Aiming to solve this problem, a mechanical Growing Sling was devised. The proposed sling adapts a growing mechanism comprising a low-friction fabric and steel shafts, and the sling is inserted under the patient by towing the steel shafts with cables connected to a motor. For the comfort and safety of the sling insertion, the required towing force was analyzed to find the minimum diameter of the shaft. The results from experimental evaluations using the proposed sling verified that it can be inserted under the patient without moving the patient, and with an acceptable level of pressure being applied to the patient.},
  archive   = {C_IROS},
  author    = {MyungJoong Lee and Yonghwan Moon and Jeongryul Kim and Seungjun Lee and Keri Kim and HyunKi In},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981770},
  pages     = {1862-1867},
  title     = {Development of a cable-driven growing sling to assist patient transfer},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual servo control of COVID-19 nasopharyngeal swab
sampling robot. <em>IROS</em>, 1855–1861. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this study, we present a visual servo control framework for fully automated nasopharyngeal swab robots. The proposed framework incorporates a deep learning-based nostril detection with a cascade approach to reliably identify the nostrils with high accuracy in real time. In addition, a partitioned visual servoing scheme that combines image-based visual servoing with axial control is formulated for accurately positioning the sampling swabs at the nostril with a multi-DOF robot arm. As the visual servoing is designed to minimize an error between the detected nostril and the swab, it can compensate for potential errors in real operation, such as positioning error by inaccurate camera-robot calibration and kinematic error by unavoidable swab deflection. The performance of the visual servo control was tested on a head phantom model for 30 unused swabs, and then compared with a method referring to only the 3D nostril target for control. Consequently, the swabs reached the nostril target with less than an average error of 1.2±0.5 mm and a maximum error of 2.0 mm via the visual servo control, while the operation without visual feedback yielded an average error of 10.6±2.3 mm and a maximum error of 16.2 mm. The partitioned visual servoing allows the swab to rapidly converge to the nostril target within 1.0 s without control instability. Finally, the swab placement at the nostril among the entire procedure of fully automated NP swab was successfully demonstrated on a human subject via the visual servo control.},
  archive   = {C_IROS},
  author    = {Guebin Hwang and Jongwon Lee and Sungwook Yang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981349},
  pages     = {1855-1861},
  title     = {Visual servo control of COVID-19 nasopharyngeal swab sampling robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A training-evaluation method for nursing telerobot operator
with unsupervised trajectory segmentation. <em>IROS</em>, 1848–1854. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To cope with the difficulty of training and eval-uation for nursing telerobot operator. This paper proposes a training-evaluation method for operator with unsupervised trajectory segmentation. To evaluate the dexterity and proce-dural knowledge of the operators objectively, we propose a new unsupervised model TSC-CRP that can automatically segment trajectory from nursing robotic training sessions. By comparing the segmented sub-trajectories and the standard sub-trajectory process, the method can provide objective evaluation and meaningful feedback without the intervention from experts. Experiments show that TSC-CRP has higher segmentation accuracy than other unsupervised methods, and it can identify the operators with different skill levels. In practical, the proposed training-evaluation system allows to provide an in-depth analysis of operator action to assess their skills precisely.},
  archive   = {C_IROS},
  author    = {Jiexin Xie and Deliang Zhu and Jiaxin Wang and Shijie Guo},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981224},
  pages     = {1848-1854},
  title     = {A training-evaluation method for nursing telerobot operator with unsupervised trajectory segmentation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Force-guided alignment and file feedrate control for
robot-assisted endodontic treatment. <em>IROS</em>, 1841–1847. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Due to the precise manipulations required in dental surgery, robotic technologies have been applied to dentistry. So far, most dental robots are designed for implant surgery, helping dentists accurately place the implant to the desired position and depth. This paper presents the DentiBot, the first robot designed for dental endodontic treatment. Without visual feedback, the DentiBot is integrated with a force and torque sensor to monitor the contact between the root canal and endodontic file. Additionally, DentiBot is implemented with force-guided alignment and file feedrate control to autonomously adjust surgical path and compensate for patient movement in real-time while protecting against endodontic file fracture. The feasibility of robot-assisted endodontic treatment is verified by the pre-clinical evaluation performed on acrylic root canal models.},
  archive   = {C_IROS},
  author    = {Hao-Fang Cheng and Yi-Chan Li and Yi-Ching Ho and Cheng-Wei Chen},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981393},
  pages     = {1841-1847},
  title     = {Force-guided alignment and file feedrate control for robot-assisted endodontic treatment},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enforcing vision-based localization using perception
constrained n-MPC for multi-rotor aerial vehicles. <em>IROS</em>,
1818–1824. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work introduces a Nonlinear Model Predictive Control (N-MPC) for camera-equipped Unmanned Aerial Vehicles (UAVs), which controls at the motor level the UAV motion to ensure the quality of vision-based state estimation while performing other tasks. The controller ensures visibility over a sufficient amount of features, while optimizing their coverage, based on an assessment of the estimation quality. The controller works for the very broad class of generic multirotor UAVs, including platforms with any number of propellers, which can be both collinear, as in the quadrotor, and fixedly-tilted. The low-level inputs are computed in real-time and realistically constrained, in terms of maximum motor torque. This allows the platform to exploit its full actuation capabilities to maintain the visibility over the set of points of interest. Our implementation is tested in Gazebo simulations and in mocap-free real experiments, and features a visual-inertial state estimation based on Kalman filter. The software is provided open-source.},
  archive   = {C_IROS},
  author    = {Martin Jacquet and Antonio Franchi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981643},
  pages     = {1818-1824},
  title     = {Enforcing vision-based localization using perception constrained N-MPC for multi-rotor aerial vehicles},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards edible drones for rescue missions: Design and flight
of nutritional wings. <em>IROS</em>, 1802–1809. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Drones have shown to be useful aerial vehicles for unmanned transport missions such as food and medical supply delivery. This can be leveraged to deliver life-saving nutrition and medicine for people in emergency situations. However, commercial drones can generally only carry 10\%–30\% of their own mass as payload, which limits the amount of food delivery in a single flight. One novel solution to noticeably increase the food-carrying ratio of a drone, is recreating some structures of a drone, such as the wings, with edible materials. We thus propose a drone, which is no longer only a food-transporting aircraft, but itself is partially edible, increasing its food-carrying mass ratio to 50\%, owing to its edible wings. Furthermore, should the edible drone be left behind in the environment after performing its task in an emergency situation, it will be more biodegradable than its non-edible counterpart, leaving less waste in the environment. Here we describe the choice of materials and scalable design of edible wings, and validate the method in a flight-capable prototype that can provide 300 kcal and carry a payload of 80 g of water.},
  archive   = {C_IROS},
  author    = {Bokeon Kwak and Jun Shintake and Lu Zhang and Dario Floreano},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981956},
  pages     = {1802-1809},
  title     = {Towards edible drones for rescue missions: Design and flight of nutritional wings},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unmanned aircraft system-based radiological mapping of
buildings. <em>IROS</em>, 1794–1801. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The article focuses on acquiring a 3D radiation map of a building via a two-phase survey performed with an unmanned aircraft system (UAS). First, a model of the stud-ied building is created by means of photogrammetry. Then, radiation data are collected using a 2-inch NaI(Tl) detector in a regular grid at a distance of 2 m from all accessible surfaces of the building (i.e., the walls and the roof). The data are then georeferenced, filtered, projected to the building model, and interpolated to yield the detailed radiation map. A method to estimate the parameters of the radiation sources located inside is introduced and successfully tested, providing a localization accuracy in the order of meters. This task is aimed to deliver the proof of concept for employing such a mapping technique within nuclear safeguards. The acquisition of the radiation data was performed via a manual flight to ensure an appropriate safety level; in this context, it should be noted that the autonomous flight mode still requires major improvements in terms of safety.},
  archive   = {C_IROS},
  author    = {Tomas Lazna and Petr Gabrlik and Petr Sladek and Tomas Jilek and Ludek Zalud},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981415},
  pages     = {1794-1801},
  title     = {Unmanned aircraft system-based radiological mapping of buildings},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multirotor long-reach aerial pruning with wire-suspended
saber saw. <em>IROS</em>, 1787–1793. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pruning work at high altitude is a dangerous work with a high risk of accidents for human workers. In this research, we propose a multirotor flying robot that is equipped with a wire-suspended device and performs pruning task. We use a saber saw as a cutting tool. If the cutting tool is installed on the body of the multirotor platform, it is difficult for the flying robot to approach the desired work point if there are obstacles such as other branches around the target branch to be pruned. Therefore, in this study, we propose a saber saw suspended from the body of the multirotor platform with two wires. The wire-suspended device is equipped with a saber saw and four ducted fans that produces thrust in any direction on the horizontal plane. This ducted fan system can be used to suppress the swing of the wire-suspended device to make positioning of the saber saw blade to the target point easier, and to improve the efficiency of the cutting and reduce the cutting time by providing a pushing force to the saber saw. As a result, the pruning work could be performed efficiently. Experiments have demonstrated that aerial pruning is possible using the long-reach wire-suspended saber saw.},
  archive   = {C_IROS},
  author    = {Ryo Miyazaki and Wataru Matori and Takamasa Kominami and Hannibal Paul and Kazuhiro Shimonomura},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981306},
  pages     = {1787-1793},
  title     = {Multirotor long-reach aerial pruning with wire-suspended saber saw},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Precise position control of a multi-rotor UAV with a
cable-suspended mechanism during water sampling. <em>IROS</em>,
1780–1786. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the problem of water sampling by using a multirotor UAV with a cable-suspended mechanism. In order to ensure the safe execution of the sampling procedure and the stabilization of the vehicle, the disturbances, induced by the water flow and transferred through the cable, have to be identified. Specifically, an estimate of the disturbances is extracted by integrating a depth sensor, a load cell, an ultrasonic sensor and a downward-looking camera into the UAV&#39;s sensor suite and fusing the respective measurements. Gaussian Processes are afterwards employed so as to learn the uncertain disturbances in real time and in a non-parametric manner. The predicted disturbances are incorporated into a geometric control scheme which is capable of stabilizing the UAV above the desired sampling position while compensating for the aforementioned disturbances. The performance of the proposed control strategy is demonstrated through both simulation and experimental results.},
  archive   = {C_IROS},
  author    = {Fotis Panetsos and George C. Karras and Sotirios N. Aspragkathos and Kostas J. Kyriakopoulos},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981057},
  pages     = {1780-1786},
  title     = {Precise position control of a multi-rotor UAV with a cable-suspended mechanism during water sampling},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rotor array synergies for aerial modular reconfigurable
robots. <em>IROS</em>, 1772–1779. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Aerial Modular Reconfigurable Robots (AMRRs) are scalable systems consisting of rotor modules capable of rearrangement during flight. The potential to dynamically change any shape for a given task poses the question: what arrangements offer the most aerodynamic benefit for the task of flying? Answering this requires understanding how adjacent rotors in various configurations influence each another. Intuitively, aerodynamic models such as momentum theory suggest that close rotor proximity decreases performance due to the upstream rotor flow fields interacting. However, effects such as vortex interaction or viscous flow entrainment (used by the Dyson bladeless fan) may offer benefits not captured by the modelling assumptions of computational analysis or simulation. Thus, this work takes an experimental approach, testing thrust performance of rotors in independent configurations of lines, square lattices, and hexagons with various inter-rotor spacings. It was found that inter-rotor spacing did not significantly change thrust performance, but that hexagonal arrangements outperformed line and grid lattices. Smoke tests indicated that hexagon configurations entrained air in the central cavity resulting in a thrust improvement. An inter-rotor spacing of 1.51 rotor diameters gave the best performance increase, roughly equal to that of an additional rotor. This suggests that by placing rotors in an array of six hollow hexagonal honeycombs, thrust performance could theoretically be increased by up to 27.3 per cent, for no additional mass.},
  archive   = {C_IROS},
  author    = {Benjamin Moshirian and Pauline E.I. Pounds},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982034},
  pages     = {1772-1779},
  title     = {Rotor array synergies for aerial modular reconfigurable robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robotic interestingness via human-informed few-shot object
detection. <em>IROS</em>, 1756–1763. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Interestingness recognition is crucial for decision making in autonomous exploration for mobile robots. Previous methods proposed an unsupervised online learning approach that can adapt to environments and detect interesting scenes quickly, but lack the ability to adapt to human-informed interesting objects. To solve this problem, we introduce a human-interactive framework, AirInteraction, that can detect human-informed objects via few-shot online learning. To reduce the communication bandwidth, we first apply an online unsupervised learning algorithm on the unmanned vehicle for interestingness recognition and then only send the potential interesting scenes to a base-station for human inspection. The human operator is able to draw and provide bounding box annotations for particular interesting objects, which are sent back to the robot to detect similar objects via few-shot learning. Only using few human-labeled examples, the robot can learn novel interesting object categories during the mission and detect interesting scenes that contain the objects. We evaluate our method on various interesting scene recognition datasets. To the best of our knowledge, it is the first human-informed few-shot object detection framework for autonomous exploration.},
  archive   = {C_IROS},
  author    = {Seungchan Kim and Chen Wang and Bowen Li and Sebastian Scherer},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981461},
  pages     = {1756-1763},
  title     = {Robotic interestingness via human-informed few-shot object detection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Benchmarking augmentation methods for learning robust
navigation agents: The winning entry of the 2021 iGibson challenge.
<em>IROS</em>, 1748–1755. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in deep reinforcement learning and scalable photorealistic simulation have led to increasingly mature embodied AI for various visual tasks, including navigation. However, while impressive progress has been made for teaching embodied agents to navigate static environments, much less progress has been made on more dynamic environments that may include moving pedestrians or movable obstacles. In this study, we aim to benchmark different augmentation techniques for improving the agent&#39;s performance in these challenging environments. We show that adding several dynamic obstacles into the scene during training confers significant improvements in test-time generalization, achieving much higher success rates than baseline agents. We find that this approach can also be combined with image augmentation methods to achieve even higher success rates. Additionally, we show that this approach is also more robust to sim-to-sim transfer than image augmentation methods. Finally, we demonstrate the effectiveness of this dynamic obstacle augmentation approach by using it to train an agent for the 2021 iGibson Challenge at CVPR, where it achieved 1 st place for Interactive Navigation.},
  archive   = {C_IROS},
  author    = {Naoki Yokoyama and Qian Luo and Dhruv Batra and Sehoon Ha},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981743},
  pages     = {1748-1755},
  title     = {Benchmarking augmentation methods for learning robust navigation agents: The winning entry of the 2021 iGibson challenge},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Resilient detection and recovery of autonomous systems
operating under on-board controller cyber attacks. <em>IROS</em>,
1741–1747. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cyber-attacks, failures, and implementation errors inside the controller of an autonomous system can affect its correct behavior leading to unsafe states and degraded performance. In this paper, we focus on such problems specifically on cyber-attacks that manipulate controller parameters like the gains in a feedback controller or that triggers different behaviors or block inputs based on specific values of the state and tracking error. If such attacks are undetected, they can lead to the partial or complete loss of system&#39;s control authority, resulting in a hijacking and leading the autonomous system towards unforeseen states. To deal with this problem, we propose a runtime monitoring and recovery scheme in which: 1) we leverage the residual between the expected and the received measurements to detect inconsistencies in the generated inputs and 2) provide a recovery method for counteracting the malicious effects to allow for resilient operations by manipulating the reference signal and state vector provided to the system to avoid the affected regions in the state and error space. We validate our approach with Matlab simulations and experiments on unmanned ground vehicles resiliently performing operations in the presence of malicious attacks to on-board controllers.},
  archive   = {C_IROS},
  author    = {Paul J Bonczek and Nicola Bezzo},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981844},
  pages     = {1741-1747},
  title     = {Resilient detection and recovery of autonomous systems operating under on-board controller cyber attacks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NAUTS: Negotiation for adaptation to unstructured terrain
surfaces. <em>IROS</em>, 1733–1740. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When robots operate in real-world off-road environments with unstructured terrains, the ability to adapt their navigational policy is critical for effective and safe navigation. However, off-road terrains introduce several challenges to robot navigation, including dynamic obstacles and terrain uncertainty, leading to inefficient traversal or navigation failures. To address these challenges, we introduce a novel approach for adaptation by negotiation that enables a ground robot to adjust its navigational behaviors through a negotiation process. Our approach first learns prediction models for various navigational policies to function as a terrain-aware joint local controller and planner. Then, through a new negotiation process, our approach learns from various policies&#39; interactions with the environment to agree on the optimal combination of policies in an online fashion to adapt robot navigation to unstructured off-road terrains on the fly. Additionally, we implement a new optimization algorithm that offers the optimal solution for robot negotiation in real-time during execution. Experimental results have validated that our method for adaptation by negotiation outperforms previous methods for robot navigation, especially over unseen and uncertain dynamic terrains.},
  archive   = {C_IROS},
  author    = {Sriram Siva and Maggie Wigness and John G. Rogers and Long Quang and Hao Zhang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982207},
  pages     = {1733-1740},
  title     = {NAUTS: Negotiation for adaptation to unstructured terrain surfaces},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Teaching agents how to map: Spatial reasoning for
multi-object navigation. <em>IROS</em>, 1725–1732. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the context of visual navigation, the capacity to map a novel environment is necessary for an agent to exploit its observation history in the considered place and efficiently reach known goals. This ability can be associated with spatial rea-soning, where an agent is able to perceive spatial relationships and regularities, and discover object characteristics. Recent work introduces learnable policies parametrized by deep neural networks and trained with Reinforcement Learning (RL). In classical RL setups, the capacity to map and reason spatially is learned end-to-end, from reward alone. In this setting, we introduce supplementary supervision in the form of auxiliary tasks designed to favor the emergence of spatial perception capabilities in agents trained for a goal-reaching downstream objective. We show that learning to estimate metrics quantifying the spatial relationships between an agent at a given location and a goal to reach has a high positive impact in Multi-Object Navigation settings. Our method significantly improves the performance of different baseline agents, that either build an explicit or implicit representation of the environment, even matching the performance of incomparable oracle agents taking ground-truth maps as input. A learning-based agent from the literature trained with the proposed auxiliary losses was the winning entry to the Multi-Object Navigation Challenge, part of the CVPR 2021 Embodied AI Workshop.},
  archive   = {C_IROS},
  author    = {Pierre Marza and Laetitia Matignon and Olivier Simonin and Christian Wolf},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982216},
  pages     = {1725-1732},
  title     = {Teaching agents how to map: Spatial reasoning for multi-object navigation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Navigating underground environments using simple topological
representations. <em>IROS</em>, 1717–1724. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Underground environments are some of the most challenging for autonomous navigation. The long, featureless corridors, loose and slippery soils, bad illumination and unavailability of global localization make many traditional approaches struggle. In this work, a topological-based navigation system is presented that enables autonomous navigation of a ground robot in mine-like environments relying exclusively on a high-level topological representation of the tunnel network. The topological representation is used to generate high-level topological instructions used by the agent to navigate through corridors and intersections. A convolutional neural network (CNN) is used to detect all the galleries accessible to a robot from its current position. The use of a CNN proves to be a reliable approach to this problem, capable of detecting the galleries correctly in a wide variety of situations. The CNN is also able to detect galleries even in the presence of obstacles, which motivates the development of a reactive navigation system that can effectively exploit the predictions of the gallery detection.},
  archive   = {C_IROS},
  author    = {Lorenzo Cano and Alejandro R. Mosteo and Danilo Tardioli},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981336},
  pages     = {1717-1724},
  title     = {Navigating underground environments using simple topological representations},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Navigation among movable obstacles with object localization
using photorealistic simulation. <em>IROS</em>, 1711–1716. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While mobile navigation has been focused on obstacle avoidance, Navigation Among Movable Obstacles (NAMO) via interaction with the environment, is a problem that is still open and challenging. This paper, presents a novel system integration to handle NAMO using visual feedback. In order to explore the capabilities of our introduced system, we explore the solution of the problem via graph-based path planning in a photorealistic simulator (NVIDIA Isaac Sim), in order to identify if the simulation-to-reality (sim2real) problem in robot navigation can be resolved. We consider the case where a wheeled robot navigates in a warehouse, in which movable boxes are common obstacles. We enable online real-time object localization and obstacle movability detection, to either avoid objects or, if it is not possible, to clear them out from the robot planned path by using pushing actions. We firstly test the integrated system in photorealistic environments, and we then validate the method on a real-world mobile wheeled robot (UCL MPPL) and its on-board sensory and computing system.},
  archive   = {C_IROS},
  author    = {Kirsty Ellis and Henry Zhang and Danail Stoyanov and Dimitrios Kanoulas},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981587},
  pages     = {1711-1716},
  title     = {Navigation among movable obstacles with object localization using photorealistic simulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RARA: Zero-shot Sim2Real visual navigation with following
foreground cues. <em>IROS</em>, 1704–1710. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The gap between simulation and the real-world restrains many machine learning breakthroughs in computer vision and reinforcement learning from being applicable in the real world. In this work, we tackle this gap for the specific case of camera-based navigation, formulating it as following a visual cue in the foreground with arbitrary backgrounds. The visual cue in the foreground can often be simulated realistically, such as a line, gate or cone. The challenge then lies in coping with the unknown backgrounds and integrating both. As such, the goal is to train a visual agent on data captured in an empty simulated environment except for this foreground cue and test this model directly in a visually diverse real world. In order to bridge this big gap, we show it&#39;s crucial to combine following techniques namely: Randomized augmentation of the fore- and background, regularization with both deep supervision and triplet loss and finally abstraction of the dynamics by using waypoints rather than direct velocity commands. The various techniques are ablated in our experimental results both qualitatively and quantitatively finally demonstrating a successful transfer from simulation to the real world. Code will be made available on publication 2 2 Project page: github.com/kkelchte/tgbg.},
  archive   = {C_IROS},
  author    = {Klaas Kelchtermans and Tinne Tuytelaars},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982066},
  pages     = {1704-1710},
  title     = {RARA: Zero-shot Sim2Real visual navigation with following foreground cues},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An open-source motion planning framework for mobile
manipulators using constraint-based task space control with linear MPC.
<em>IROS</em>, 1671–1678. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an open source motion planning framework for ROS, which uses constraint and optimization based task space control to generate trajectories for the whole body of mobile manipulators. Motion goals are defined as constraints which are enforced on task space functions. They map the controllable degrees of freedom of a system onto custom task spaces, which can, but do not have to be, the Cartesian space. We use this expressive tool from motion control to pre-compute trajectories in order to utilize the fact that most robots offer controllers to follow such trajectories. As a result, our framework only requires a kinematic model of the robot to control it. In addition, we extend the constraint-based motion control approach with linear MPC to explicitly optimize for velocity, acceleration and jerk simultaneously, which allows us to enforce constraints on all derivatives in both joint and task space at the same time. As a result, we can reuse predefined motion goals on any robot without modifications. Our framework was tested on four different robots to show its generality.},
  archive   = {C_IROS},
  author    = {Simon Stelter and Georg Bartels and Michael Beetz},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982245},
  pages     = {1671-1678},
  title     = {An open-source motion planning framework for mobile manipulators using constraint-based task space control with linear MPC},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Articulated object interaction in unknown scenes with
whole-body mobile manipulation. <em>IROS</em>, 1647–1654. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A kitchen assistant needs to operate human-scale objects, such as cabinets and ovens, in unmapped environments with dynamic obstacles. Autonomous interactions in such environments require integrating dexterous manipulation and fluid mobility. While mobile manipulators in different form factors provide an extended workspace, their real-world adoption has been limited. Executing a high-level task for general objects requires a perceptual understanding of the object as well as adaptive whole-body control among dynamic obstacles. In this paper, we propose a two-stage architecture for autonomous interaction with large articulated objects in unknown environments. The first stage, object-centric planner, only focuses on the object to provide an action-conditional sequence of states for manipulation using RGB-D data. The second stage, agent-centric planner, formulates the whole-body motion control as an optimal control problem that ensures safe tracking of the generated plan, even in scenes with moving obstacles. We show that the proposed pipeline can handle complex static and dynamic kitchen settings for both wheel-based and legged mobile manipulators. Compared to other agent-centric planners, our proposed planner achieves a higher success rate and a lower execution time. We also perform hardware tests on a legged mobile manipulator to interact with various articulated objects in a kitchen. For additional material, please check: www.pair.toronto.edularticulated-mm/.},
  archive   = {C_IROS},
  author    = {Mayank Mittal and David Hoeller and Farbod Farshidian and Marco Hutter and Animesh Garg},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981779},
  pages     = {1647-1654},
  title     = {Articulated object interaction in unknown scenes with whole-body mobile manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Task-oriented contact optimization for pushing manipulation
with mobile robots. <em>IROS</em>, 1639–1646. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work addresses the problem of transporting an object along a desired planar trajectory by pushing with mobile robots. More specifically, we concentrate on establishing optimal contacts between the object and the robots to execute the given task with minimum effort. We present a task-oriented contact placement optimization strategy for object pushing that allows calculating optimal contact points minimizing the amplitude of forces required to execute the task. Exploiting the optimized contact configuration, a motion controller uses the computed contact forces in feed-forward and position error feedback terms to realize the desired trajectory tracking task. Simulations and real experiments results confirm the validity of our approach.},
  archive   = {C_IROS},
  author    = {Filippo Bertoncelli and Mario Selvaggio and Fabio Ruggiero and Lorenzo Sabattini},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982177},
  pages     = {1639-1646},
  title     = {Task-oriented contact optimization for pushing manipulation with mobile robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel design and evaluation of a dactylus-equipped
quadruped robot for mobile manipulation. <em>IROS</em>, 1633–1638. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Quadruped robots are usually equipped with ad-ditional arms for manipulation, negatively impacting price and weight. On the other hand, the requirements of legged locomotion mean that the legs of such robots often possess the needed torque and precision to perform manipulation. In this paper, we present a novel design for a small-scale quadruped robot equipped with two leg-mounted manipulators inspired by crustacean chelipeds and knuckle-walker forelimbs. By making use of the actuators already present in the legs, we can achieve manipulation using only 3 additional motors per limb. The design enables the use of small and inexpensive actuators relative to the leg motors, further reducing cost and weight. The moment of inertia impact on the leg is small thanks to an integrated cable/pulley system. As we show in a suite of tele-operation experiments, the robot is capable of performing single- and dual-limb manipulation, as well as transitioning between manipulation modes. The proposed design performs similarly to an additional arm while weighing and costing 5 times less per manipulator and enabling the completion of tasks requiring 2 manipulators.},
  archive   = {C_IROS},
  author    = {Yordan Tsvetkov and Subramanian Ramamoorthy},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982229},
  pages     = {1633-1638},
  title     = {A novel design and evaluation of a dactylus-equipped quadruped robot for mobile manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A solution to adaptive mobile manipulator throwing.
<em>IROS</em>, 1625–1632. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile manipulator throwing is a promising method to increase the flexibility and efficiency of dynamic manipulation in factories. Its major challenge is to efficiently plan a feasible throw under a wide set of task specifications. We show that the mobile manipulator throwing problem can be simplified to a planar problem, hence greatly reducing the computational costs. Using machine learning approaches, we build a model of the object&#39;s inverted flying dynamics and the robot&#39;s kinematic feasibility, which enables throwing motion generation within 1 ms for given query of target position. Thanks to the computational efficiency of our method, we show that the system is adaptive under disturbance, via replanning on the fly for alternative solutions, instead of sticking to the original throwing plan.},
  archive   = {C_IROS},
  author    = {Yang Liu and Aradhana Nayak and Aude Billard},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981231},
  pages     = {1625-1632},
  title     = {A solution to adaptive mobile manipulator throwing},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On robotic manipulation of flexible flat cables: Employing a
multi-modal gripper with dexterous tips, active nails, and a
reconfigurable suction cup module. <em>IROS</em>, 1602–1608. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A popular solution for connecting different components in modern electronics, such as mobile phones, laptops, tablets, etc, is the use of flexible flat cables (FFC). Typically, it takes hours of repetition from a highly trained worker, or a high precision autonomous robot with specialised end effectors to reliably manage the installation of these cables. Human workers are prone to error, and cannot work endlessly without a break, while the robots often come with a significant expense, and require a substantial amount of time to program and reprogram. Additionally, the use of sophisticated sensing elements further increases the complexity of the required control system. As a result, the performance and robustness of such systems is far from sufficient, hindering their mass adoption. The manipulation of FFCs is also quite challenging. In this work, we focus on the robotic manipulation of a plethora of flexible cables, proposing a multi-modal gripper with locally-dexterous tips and active fingernails. The fingers of the gripper are equipped with: i) locally-dexterous fingertips that accommodate manipulation-capable degrees of freedom, ii) a combination of Nitinol-based active fingernails and suction cups that allow picking up and handling of cables that rest on flat surfaces, and iii) compliant finger-pads that conform to the object surface to increase grasping stability. The proposed robotic gripper is equipped with a camera and a perception system that allow for the execution of complex cable manipulation and assembly tasks in dynamic environments.},
  archive   = {C_IROS},
  author    = {Joao Buzzatto and Jayden Chapman and Mojtaba Shahmohammadi and Felipe Sanches and Mahla Nejati and Saori Matsunaga and Rintaro Haraguchi and Toshisada Mariyama and Bruce MacDonald and Minas Liarokapis},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981313},
  pages     = {1602-1608},
  title     = {On robotic manipulation of flexible flat cables: Employing a multi-modal gripper with dexterous tips, active nails, and a reconfigurable suction cup module},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computation and selection of secure gravity based caging
grasps of planar objects. <em>IROS</em>, 1571–1578. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Gravity based caging grasps are robotic grasps where the robot hand passively supports an object against gravity. When a robot hand supports an object at a local minimum of the object gravitational energy, the robot hand forms a basket like grasp of the object. Any object movement in a basket grasp requires an increase of the object gravitational energy, thus allowing secure object pickup and transport with robot hands that use a small number fingers. The basket grasp depth measures the minimal additional energy the object must acquire to escape the basket grasp. This paper describes a computation scheme that determines the depth of entire sets of candidate basket grasps associated with alternative finger placements on the object boundary before pickup. The computation relies on categorization of escape stances that mark the basket grasp depth: double-support escapes are first analyzed and computed, then single-support escapes are analyzed and computed. The minimum energy combination of both types of escape stances defines the depth of entire sets of candidate basket grasps, which is then used to identify the deepest and hence most secure basket grasp. The computation scheme is fully implemented and demonstrated on several examples with reported run-times.},
  archive   = {C_IROS},
  author    = {A. Shirizly and E. D. Rimon},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982151},
  pages     = {1571-1578},
  title     = {Computation and selection of secure gravity based caging grasps of planar objects},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-finger grasping like humans. <em>IROS</em>, 1564–1570.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots with multi-fingered grippers could perform advanced manipulation tasks for us if we were able to properly specify to them what to do. In this study, we take a step in that direction by making a robot grasp an object like a grasping demonstration performed by a human. We propose a novel optimization-based approach for transferring human grasp demonstrations to any multi-fingered grippers, which produces robotic grasps that mimic the human hand orientation and the contact area with the object, while alleviating interpenetration. Extensive experiments with the Allegro and BarrettHand grippers show that our method leads to grasps more similar to the human demonstration than existing approaches, without requiring any gripper-specific tuning. We confirm these findings through a user study and validate the applicability of our approach on a real robot.},
  archive   = {C_IROS},
  author    = {Yuming Du and Philippe Weinzaepfel and Vincent Lepetit and Romain Brégier},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981805},
  pages     = {1564-1570},
  title     = {Multi-finger grasping like humans},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ICK-track: A category-level 6-DoF pose tracker using
inter-frame consistent keypoints for aerial manipulation. <em>IROS</em>,
1556–1563. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots that are supposed to interact with or manipulate objects in the world must be able to track the poses of objects in their sensor data. Thus, Detecting and tracking the 6-DoF poses of targeted objects is important for aerial manipulation and is still in the early stage due to the high dynamics and limited onboard capacity of such systems. In this paper, we propose ICK-Track, a novel method for onboard category-level object 6-DoF pose tracking that can be applied to aerial manipulation without using any pre-defined object CAD models. It first utilizes a semi-supervised video segmentation to detect objects in the eye-in-hand RGB-D camera stream to segment the 3D points of objects. Then, canonical keypoints are extracted using iterative farthest point sampling. We propose a novel inter-frame consistent keypoints generation network to generate the corresponding keypoint pairs, which are used together with ICP to estimate the pose changes of objects for tracking. Experimental results show that our method is more robust to viewpoint changes and runs faster than the state-of-the-art methods on category-level pose tracking. We further test our proposed method on a real aerial manipulator. A demo video showing the use of our method on a real aerial manipulator and the implementation of our method are available at: https://github.com/S-JingTao/ICK-Track.},
  archive   = {C_IROS},
  author    = {Jingtao Sun and Yaonan Wang and Mingtao Feng and Danwei Wang and Jiawen Zhao and Cyrill Stachniss and Xieyuanli Chen},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982183},
  pages     = {1556-1563},
  title     = {ICK-track: A category-level 6-DoF pose tracker using inter-frame consistent keypoints for aerial manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning suction cup dynamics from motion capture: Accurate
prediction of an object’s vertical motion during release. <em>IROS</em>,
1541–1547. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Suction grippers are the most common pick-and-place end effectors used in industry. However, there is little literature on creating and validating models to predict their force interaction with objects in dynamic conditions. In this paper, we study the interaction dynamics of an active vacuum suction gripper during the vertical release of an object. Object and suction cup motions are recorded using a motion capture system. As the object&#39;s mass is known and can be changed for each experiment, a study of the object&#39;s motion can lead to an estimate of the interaction force generated by the suction gripper. We show that, by learning this interaction force, it is possible to accurately predict the object&#39;s vertical motion as a function of time. This result is the first step toward 3D motion prediction when releasing an object from a suction gripper.},
  archive   = {C_IROS},
  author    = {Menno Lubbers and Job van Voorst and Maarten Jongeneel and Alessandro Saccon},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982211},
  pages     = {1541-1547},
  title     = {Learning suction cup dynamics from motion capture: Accurate prediction of an object&#39;s vertical motion during release},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Class-incremental gesture recognition learning with
out-of-distribution detection. <em>IROS</em>, 1503–1508. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Gesture recognition is a popular human-computer interaction technology, which has been widely applied in many fields (e.g., autonomous driving, medical care, VR and AR). However, 1) most existing gesture recognition methods focus on the fixed recognition scenarios with several gestures, which could lead to memory consumption and computational effort when continuously learning new gestures; 2) Meanwhile, the performance of popular class-incremental methods degrades significantly for previously learned classes (i.e., catastrophic forgetting) due to the ambiguity and variability of gestures. To tackle these challenges, we propose a novel class-incremental gesture recognition method with out-of-distribution (OOD) detection, which can continuously adapt to new gesture classes and achieve high performance for both learned and new gestures. Specifically, we construct an episodic memory with a subset of learned training samples to preserve the previous knowledge from forgetting. Moreover, the OOD detection-based memory management is developed for exploring the most representative and informative core set from the learned datasets. When a new gesture recognition task with strange classes comes, rehearsal enhancement is adopted to increase the diversity of memory exemplars for better fitting the real characteristics of gesture recognition. After deriving an effective class-incremental gesture recognition strategy, we perform experiments on two representative datasets to validate the superiority of our method. Evaluation experiments demonstrate that our proposed method substantially outperforms the state-of-the-art methods with about 2.17\%-3.81\% improvement under different class-incremental learning scenarios.},
  archive   = {C_IROS},
  author    = {Mingxue Li and Yang Cong and Yuyang Liu and Gan Sun},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981167},
  pages     = {1503-1508},
  title     = {Class-incremental gesture recognition learning with out-of-distribution detection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical reinforcement learning for precise soccer
shooting skills using a quadrupedal robot. <em>IROS</em>, 1479–1486. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the problem of enabling quadrupedal robots to perform precise shooting skills in the real world using reinforcement learning. Developing algorithms to enable a legged robot to shoot a soccer ball to a given target is a challenging problem that combines robot motion control and planning into one task. To solve this problem, we need to consider the dynamics limitation and motion stability during the control of a dynamic legged robot. Moreover, we need to consider motion planning to shoot the hard-to-model deformable ball rolling on the ground with uncertain friction to a desired location. In this paper, we propose a hierarchical framework that leverages deep reinforcement learning to train (a) a robust motion control policy that can track arbitrary motions and (b) a planning policy to decide the desired kicking motion to shoot a soccer ball to a target. We deploy the proposed framework on an A1 quadrupedal robot and enable it to accurately shoot the ball to random targets in the real world.},
  archive   = {C_IROS},
  author    = {Yandong Ji and Zhongyu Li and Yinan Sun and Xue Bin Peng and Sergey Levine and Glen Berseth and Koushil Sreenath},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981984},
  pages     = {1479-1486},
  title     = {Hierarchical reinforcement learning for precise soccer shooting skills using a quadrupedal robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Characterization of real-time haptic feedback from
multimodal neural network-based force estimates during teleoperation.
<em>IROS</em>, 1471–1478. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Force estimation using neural networks is a promising approach to enable haptic feedback in minimally invasive surgical robots without end-effector force sensors. Various network architectures have been proposed, but none have been tested in real time with surgical-like manipulations. Thus, questions remain about the real-time transparency and stability of force feedback from neural network-based force estimates. We characterize the real-time impedance transparency and stability of force feedback rendered on a da Vinci Research Kit teleoperated surgical robot using neural networks with visiononly, state-only, and state and vision inputs. Networks were trained on an existing dataset of teleoperated manipulations without force feedback. To measure real-time stability and transparency during teleoperation with force feedback to the operator, we modeled a one-degree-of-freedom human and surgeon-side manipulandum that moved the patient-side robot to perform manipulations on silicone artificial tissue over various robot and camera configurations, and tools. We found that the networks using state inputs displayed more transparent impedance than a vision-only network. However, state-based networks displayed large instability when used to provide force feedback during lateral manipulation of the silicone. In contrast, the vision-only network showed consistent stability in all the evaluated directions. We confirmed the performance of the vision-only network for real-time force feedback in a demonstration with a human teleoperator.},
  archive   = {C_IROS},
  author    = {Zonghe Chua and Allison M. Okamura},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981662},
  pages     = {1471-1478},
  title     = {Characterization of real-time haptic feedback from multimodal neural network-based force estimates during teleoperation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PCBot: A minimalist robot designed for swarm applications.
<em>IROS</em>, 1463–1470. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Complexity, cost, and power requirements for the actuation of individual robots can play a large factor in limiting the size of robotic swarms. Here we present PCBot, a minimalist robot that can precisely move on an orbital shake table using a bi-stable solenoid actuator built directly into its PCB. This allows the actuator to be built as part of the automated PCB manufacturing process, greatly reducing the impact it has on manual assembly. Thanks to this novel actuator design, PCBot has merely five major components and can be assembled in under 20 seconds, potentially enabling them to be easily mass-manufactured. Here we present the electro-magnetic and mechanical design of PCBot. Additionally, a prototype robot is used to demonstrate its ability to move in a straight line as well as follow given paths.},
  archive   = {C_IROS},
  author    = {Jingxian Wang and Michael Rubenstein},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981377},
  pages     = {1463-1470},
  title     = {PCBot: A minimalist robot designed for swarm applications},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning visual feedback control for dynamic cloth folding.
<em>IROS</em>, 1455–1462. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic manipulation of cloth is a challenging task due to the high dimensionality of the configuration space and the complexity of dynamics affected by various material properties. The effect of complex dynamics is even more pronounced in dynamic folding, for example, when a square piece of fabric is folded in two by a single manipulator. To account for the complexity and uncertainties, feedback of the cloth state using e.g. vision is typically needed. However, construction of visual feedback policies for dynamic cloth folding is an open problem. In this paper, we present a solution that learns policies in simulation using Reinforcement Learning (RL) and transfers the learned policies directly to the real world. In addition, to learn a single policy that manipulates multiple materials, we randomize the material properties in simulation. We evaluate the contributions of visual feedback and material randomization in real-world experiments. The experimental results demonstrate that the proposed solution can fold successfully different fabric types using dynamic manipulation in the real world. Code, data, and videos are available at https://sites.google.com/view/dynamic-cloth-folding.},
  archive   = {C_IROS},
  author    = {Julius Hietala and David Blanco–Mulero and Gokhan Alcan and Ville Kyrki},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981376},
  pages     = {1455-1462},
  title     = {Learning visual feedback control for dynamic cloth folding},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PI-ARS: Accelerating evolution-learned visual-locomotion
with predictive information representations. <em>IROS</em>, 1447–1454.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Evolution Strategy (ES) algorithms have shown promising results in training complex robotic control policies due to their massive parallelism capability, simple implementation, effective parameter-space exploration, and fast training time. However, a key limitation of ES is its scalability to large capacity models, including modern neural network architectures. In this work, we develop Predictive Information Augmented Random Search (PI-ARS) to mitigate this limitation by leveraging recent advancements in representation learning to reduce the parameter search space for ES. Namely, PI-ARS combines a gradient-based representation learning technique, Predictive Information (PI), with a gradient-free ES algorithm, Augmented Random Search (ARS), to train policies that can process complex robot sensory inputs and handle highly nonlinear robot dynamics. We evaluate PI-ARS on a set of challenging visual-locomotion tasks where a quadruped robot needs to walk on uneven stepping stones, quincuncial piles, and moving platforms, as well as to complete an indoor navigation task. Across all tasks, PI-ARS demonstrates significantly better learning efficiency and performance compared to the ARS baseline. We further validate our algorithm by demonstrating that the learned policies can successfully transfer to a real quadruped robot, for example, achieving a 100\% success rate on the real-world stepping stone environment, dramatically improving prior results achieving 40\% success.},
  archive   = {C_IROS},
  author    = {Kuang-Huei Lee and Ofir Nachum and Tingnan Zhang and Sergio Guadarrama and Jie Tan and Wenhao Yu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981952},
  pages     = {1447-1454},
  title     = {PI-ARS: Accelerating evolution-learned visual-locomotion with predictive information representations},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Going in blind: Object motion classification using
distributed tactile sensing for safe reaching in clutter. <em>IROS</em>,
1440–1446. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic manipulators navigating cluttered shelves or cabinets may find it challenging to avoid contact with obstacles. Indeed, rearranging obstacles may be necessary to access a target. Rather than planning explicit motions that place obstacles into a desired pose, we suggest allowing incidental contacts to rearrange obstacles while monitoring contacts for safety. Bypassing object identification, we present a method for categorizing object motions from tactile data collected from incidental contacts with a capacitive tactile skin on an Allegro Hand. We formalize tactile cues associated with categories of object motion, demonstrating that they can determine with &gt; 90\% accuracy whether an object is movable and whether a contact is causing the object to slide stably (safe contact) or tip (unsafe).},
  archive   = {C_IROS},
  author    = {Rachel Thomasson and Etienne Roberge and Mark R. Cutkosky and Jean-Philippe Roberge},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981924},
  pages     = {1440-1446},
  title     = {Going in blind: Object motion classification using distributed tactile sensing for safe reaching in clutter},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new gripper that acts as an active and passive joint to
facilitate prehensile grasping and locomotion. <em>IROS</em>, 1425–1431.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Among primates, the prehensile nature of the hand is vital for greater adaptability and a secure grip over the substrate/branches, particularly for arm-swinging motion or brachiation. Though various brachiation mechanisms that are mechanically equivalent to underactuated pendulum models are reported in the literature, not much attention has been given to the hand design that facilitates both locomotion and within-hand manipulation. In this paper, we propose a new robotic gripper design, equipped with shape conformable active gripping surfaces that can act as an active or passive joint and adapt to substrates with different shapes and sizes. A floating base serial chain, named GraspMaM, equipped with two such grippers, increases the versatility by performing a range of locomotion and manipulation modes without using dedicated systems. The unique gripper design allows the robot to estimate the passive joint state while arm-swinging and exhibits a dual relationship between manipulation and locomotion. We report the design details of the multimodal gripper and how it can be adapted for the brachiation motion assuming it as an articulated suspended pendulum model. Further, the system parameters of the physical prototype are estimated, and experimental results for the brachiation mode are discussed to validate and show the effectiveness of the proposed design.},
  archive   = {C_IROS},
  author    = {Nagamanikandan Govindan and Shashank Ramesh and Asokan Thondiyath},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981475},
  pages     = {1425-1431},
  title     = {A new gripper that acts as an active and passive joint to facilitate prehensile grasping and locomotion},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Closed-loop next-best-view planning for target-driven
grasping. <em>IROS</em>, 1411–1416. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Picking a specific object from clutter is an essential component of many manipulation tasks. Partial observations often require the robot to collect additional views of the scene before attempting a grasp. This paper proposes a closed-loop next-best-view planner that drives exploration based on occluded object parts. By continuously predicting grasps from an up-to-date scene reconstruction, our policy can decide online to finalize a grasp execution or to adapt the robot&#39;s trajectory for further exploration. We show that our reactive approach decreases execution times without loss of grasp success rates compared to common camera placements and handles situations where the fixed baselines fail. Video and code are available at https://github.com/ethz-asl/active_grasp.},
  archive   = {C_IROS},
  author    = {Michel Breyer and Lionel Ott and Roland Siegwart and Jen Jen Chung},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981472},
  pages     = {1411-1416},
  title     = {Closed-loop next-best-view planning for target-driven grasping},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual manipulation relationship detection based on gated
graph neural network for robotic grasping. <em>IROS</em>, 1404–1410. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Exploring the relationship among objects and giving the correct operation sequence is vital for robotic manipulation. However, most previous algorithms only model the relationship between pairs of objects independently, ignoring the interaction effect between them, which may generate redundant or missing relations in complex scenes, such as multi-object stacking and partial occlusion. To solve this problem, a Gated Graph Neural Network (GGNN) is designed for visual manipulation relationship detection, which can help robots detect targets in complex scenes and obtain the appropriate grasping order. Firstly, the robot extracts feature from the input image and estimate object categories. Then GGNN is used to effectively capture the dependencies between objects in the whole scene, update the relevant features, and output the grasping sequence. In addition, by embedding positional encoding into pair object features, accurate context information is obtained to reduce the adverse effects of complex scenes. Finally, the constructed algorithm is applied to the physical robot for grasping. Experiment results on the Visual Ma-nipulation Relationship Dataset (VMRD) and the large-scale relational grasp dataset named REGRAD show that our method significantly improves the accuracy of relationship detection in complex scenes, and can be well generalized in the real world.},
  archive   = {C_IROS},
  author    = {Mengyuan Ding and Yaxin Liu and Chenjie Yang and Xuguang Lan},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981077},
  pages     = {1404-1410},
  title     = {Visual manipulation relationship detection based on gated graph neural network for robotic grasping},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simultaneous object reconstruction and grasp prediction
using a camera-centric object shell representation. <em>IROS</em>,
1396–1403. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Being able to grasp objects is a fundamental component of most robotic manipulation systems. In this paper, we present a new approach to simultaneously reconstruct a mesh and a dense grasp quality map of an object from a depth image. At the core of our approach is a novel camera-centric object representation called the “object shell” which is composed of an observed “entry image” and a predicted “exit image”. We present an image-to-image residual ConvNet architecture in which the object shell and a grasp-quality map are predicted as separate output channels. The main advantage of the shell representation and the corresponding neural network architecture, ShellGrasp-Net, is that the input-output pixel correspondences in the shell representation are explicitly represented in the architecture. We show that this coupling yields superior generalization capabilities for object reconstruction and accurate grasp quality estimation implicitly considering the object geometry. Our approach yields an efficient dense grasp quality map and an object geometry estimate in a single forward pass. Both of these outputs can be used in a wide range of robotic manipulation applications. With rigorous experimental validation, both in simulation and on a real setup, we show that our shell-based method can be used to generate precise grasps and the associated grasp quality with over 90\% accuracy. Diverse grasps computed on shell reconstructions allow the robot to select and execute grasps in cluttered scenes with more than 93\% success rate.},
  archive   = {C_IROS},
  author    = {Nikhil Chavan-Dafle and Sergiy Popovych and Shubham Agrawal and Daniel D. Lee and Volkan Isler},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981955},
  pages     = {1396-1403},
  title     = {Simultaneous object reconstruction and grasp prediction using a camera-centric object shell representation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GE-grasp: Efficient target-oriented grasping in dense
clutter. <em>IROS</em>, 1388–1395. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Grasping in dense clutter is a fundamental skill for autonomous robots. However, the crowdedness and oc-clusions in the cluttered scenario cause significant difficul-ties to generate valid grasp poses without collisions, which results in low efficiency and high failure rates. To address these, we present a generic framework called GE-Grasp for robotic motion planning in dense clutter, where we leverage diverse action primitives for occluded object removal and present the generator-evaluator architecture to avoid spatial collisions. Therefore, our GE-Grasp is capable of grasping objects in dense clutter efficiently with promising success rates. Specifically, we define three action primitives: target-oriented grasping for target capturing, pushing, and nontarget-oriented grasping to reduce the crowdedness and occlusions. The gen-erators effectively provide various action candidates referring to the spatial information. Meanwhile, the evaluators assess the selected action primitive candidates, where the optimal action is implemented by the robot. Extensive experiments in simulated and real-world environments show that our approach outperforms the state-of-the-art methods of grasping in clutter with respect to motion efficiency and success rates. Moreover, we achieve comparable performance in the real world as that in the simulation environment, which indicates the strong gen-eralization ability of our GE-Grasp. Supplementary material is available at: https://github.com/CaptainWuDaoKou/GE-Grasp.},
  archive   = {C_IROS},
  author    = {Zhan Liu and Ziwei Wang and Sichao Huang and Jie Zhou and Jiwen Lu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981499},
  pages     = {1388-1395},
  title     = {GE-grasp: Efficient target-oriented grasping in dense clutter},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learn from interaction: Learning to pick via reinforcement
learning in challenging clutter. <em>IROS</em>, 1382–1387. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bin picking is a challenging problem in robotics due to high dimensional action space, partially visible objects, and contact-rich environments. State-of-the-art methods for bin picking are often simplified as planar manipulation, or learn policy based on human demonstration and motion primitives. The designs have escalated in complexity while still failing to reach the generality and robustness of human picking ability. Here, we present an end-to-end reinforcement learning (RL) framework to produce an adaptable and robust policy for picking objects in diverse real-world environments, including but not limited to tilted bins and corner objects. We present a novel solution to incorporate object interaction in policy learning. The object interaction is represented by the poses of objects. The policy learning is based on two neural networks with asymmetric state inputs. One acts on the object interaction information, while the other acts on the depth observation and proprioceptive signals of robots. The results of experiment shows remarkable zero-shot generalization from simulation to the real world and extensive real-world experiments show the effectiveness of the approach.},
  archive   = {C_IROS},
  author    = {Chao Zhao and Jungwon Seo},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981530},
  pages     = {1382-1387},
  title     = {Learn from interaction: Learning to pick via reinforcement learning in challenging clutter},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scale estimation with dual quadrics for monocular object
SLAM. <em>IROS</em>, 1374–1381. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The scale ambiguity problem is inherently unsolvable to monocular SLAM without the metric baseline between moving cameras. In this paper, we present a novel scale estimation approach based on an object-level SLAM system. To obtain the absolute scale of the reconstructed map, we formulate an optimization problem to make the scaled dimensions of objects conform to the distribution of their sizes in the physical world, without relying on any prior information about gravity direction. The dual quadric is adopted to represent objects for its ability to describe objects compactly and accurately, thus providing reliable dimensions for scale estimation. In the proposed monocular object-level SLAM system, semantic objects are initialized first from fitted 3-D oriented bounding boxes and then further optimized under constraints of 2-D detections and 3-D map points. Experiments on indoor and outdoor public datasets show that our approach outperforms existing methods in terms of accuracy and robustness.},
  archive   = {C_IROS},
  author    = {Shuangfu Song and Junqiao Zhao and Tiantian Feng and Chen Ye and Lu Xiong},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981554},
  pages     = {1374-1381},
  title     = {Scale estimation with dual quadrics for monocular object SLAM},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IMU preintegration for 2D SLAM problems using lie groups.
<em>IROS</em>, 1367–1373. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {2D SLAM is useful for mobile robots that are constrained to a 2D plane, for example in a warehouse, simplifying calculations in respect to the 3D case. The use of an IMU in such a context can enrich the estimation and make it more robust. In this paper we reformulate the IMU preintegration widely used in 3D problems for the 2D case, making use of Lie Theory. The Lie theory based formalization, first derived for a perfectly horizontal plane, allows us to easily extend it to problems where the plane is not orthogonal to the gravity vector. We implement the theory in a factor graph based estimation library, and carry out experiments to validate it on a mobile platform. Two experiments are carried out; on a horizontal and a sloped environment, and the sensor data is processed using our two 2D methods and a state-of-the-art 3D method.},
  archive   = {C_IROS},
  author    = {Idril Geer and Joan Vallvé and Joan Solà},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981876},
  pages     = {1367-1373},
  title     = {IMU preintegration for 2D SLAM problems using lie groups},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scale-aware direct monocular odometry. <em>IROS</em>,
1360–1366. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a generic framework for scale-aware direct monocular odometry based on depth prediction from a deep neural network. In contrast with previous methods where depth information is only partially exploited, we formulate a novel depth prediction residual which allows us to incorporate multi-view depth information. In addition, we propose to use a truncated robust cost function which prevents considering inconsistent depth estimations. The photometric and depth-prediction measurements are integrated into a tightly-coupled optimization leading to a scale-aware monocular system which does not accumulate scale drift. Our proposal does not particularize for a concrete neural network, being able to work along with the vast majority of the existing depth prediction solutions. We demonstrate the validity and generality of our proposal evaluating it on the KITTI odometry dataset, using two publicly available neural networks and comparing it with similar approaches and the state-of-the-art for monocular and stereo SLAM. Experiments show that our proposal largely outperforms classic monocular SLAM, being 5 to 9 times more precise, beating similar approaches and having an accuracy which is closer to that of stereo systems.},
  archive   = {C_IROS},
  author    = {Carlos Campos and Juan D. Tardós},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982173},
  pages     = {1360-1366},
  title     = {Scale-aware direct monocular odometry},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). DRG-SLAM: A semantic RGB-d SLAM using geometric features
for indoor dynamic scene. <em>IROS</em>, 1352–1359. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual SLAM methods based on point features have achieved acceptable results in texture-rich static scenes, but they often suffer from a deficiency of texture and the existence of dynamic objects in real indoor scenes, which limits the application of these methods. In this paper, we have presented DRG-SLAM, which combines line features and plane features into point features to improve the robustness of the system. We tested the proposed algorithm on publicly available datasets, and the results demonstrate that the algorithm has superior accuracy and robustness in indoor dynamic scenes compared with the state-of-the-art methods.},
  archive   = {C_IROS},
  author    = {Yanan Wang and Kun Xu and Yaobin Tian and Xilun Ding},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981238},
  pages     = {1352-1359},
  title     = {DRG-SLAM: A semantic RGB-D SLAM using geometric features for indoor dynamic scene},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FEJ-VIRO: A consistent first-estimate jacobian
visual-inertial-ranging odometry. <em>IROS</em>, 1336–1343. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, Visual-Inertial Odometry (VIO) has achieved many significant progresses. However, VIO meth-ods suffer from localization drift over long trajectories. In this paper, we propose a First-Estimates Jacobian Visual-Inertial-Ranging Odometry (FEJ-VIRO) to reduce the localization drifts of VIO by incorporating ultra-wideband (UWB) ranging measurements into the VIO framework consistently. Consid-ering that the initial positions of UWB anchors are usually unavailable, we propose a long-short window structure to initialize the UWB anchors&#39; positions as well as the covariance for state augmentation. After initialization, the FEJ - VIRO estimates the UWB anchors&#39; positions simultaneously along with the robot poses. We further analyze the observability of the visual-inertial-ranging estimators and proved that there are four unobservable directions in the ideal case, while one of them vanishes in the actual case due to the gain of spurious information. Based on these analyses, we leverage the FEJ technique to enforce the unobservable directions, hence reducing inconsistency of the estimator. Finally, we validate our analysis and evaluate the proposed FEJ-VIRO with both simulation and real-world experiments.},
  archive   = {C_IROS},
  author    = {Shenhan Jia and Yanmei Jiao and Zhuqing Zhang and Rong Xiong and Yue Wang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981413},
  pages     = {1336-1343},
  title     = {FEJ-VIRO: A consistent first-estimate jacobian visual-inertial-ranging odometry},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NDD: A 3D point cloud descriptor based on normal
distribution for loop closure detection. <em>IROS</em>, 1328–1335. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Loop closure detection is a key technology for long-term robot navigation in complex environments. In this paper, we present a global descriptor, named Normal Distribution Descriptor (NDD), for 3D point cloud loop closure detection. The descriptor encodes both the probability density score and entropy of a point cloud as the descriptor. We also propose a fast rotation alignment process and use correlation coefficient as the similarity between descriptors. Experimental results show that our approach outperforms the state-of-the-art point cloud descriptors in both accuracy and efficency. The source code is available and can be integrated into existing LiDAR odometry and mapping (LOAM) systems.},
  archive   = {C_IROS},
  author    = {Ruihao Zhou and Li He and Hong Zhang and Xubin Lin and Yisheng Guan},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981180},
  pages     = {1328-1335},
  title     = {NDD: A 3D point cloud descriptor based on normal distribution for loop closure detection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ULSM: Underground localization and semantic mapping with
salient region loop closure under perceptually-degraded environment.
<em>IROS</em>, 1320–1327. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simultaneous Localization and Mapping (SLAM) has greatly assisted in exploring perceptually-degraded underground environments, such as human-made tunnels, mine tunnels, and caves. However, the recurring sensor failures and spurious loop closures in these scenes bring significant challenges to applying SLAM. This paper proposes an architecture for underground localization and semantic mapping (ULSM) that promotes the robustness of odometry estimation and map-building. In this architecture, a two-stage robust motion compensation method is proposed to adapt to sensor-failure situations. The proposed salient region loop closure detection contributes to avoiding spurious loop closures. Meanwhile, the 2D pose as the initial value for point cloud registration is estimated without additional input. We also design a multi-robot cooperative mapping scheme based on descriptors of the salient region. Extensive experiments are conducted on datasets collected in the Tunnel Circuit of DARPA Subterranean Challenge.},
  archive   = {C_IROS},
  author    = {Junhui Wang and Bin Tian and Rui Zhang and Long Chen},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982170},
  pages     = {1320-1327},
  title     = {ULSM: Underground localization and semantic mapping with salient region loop closure under perceptually-degraded environment},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inference of multi-class STL specifications for multi-label
human-robot encounters. <em>IROS</em>, 1305–1311. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper is interested in formalizing human trajectories in human-robot encounters. Inspired by robot navigation tasks in human-crowded environments, we consider the case where a human and a robot walk towards each other, and where humans have to avoid colliding with the incoming robot. Further, humans may describe different be-haviors, ranging from being in a hurry/minimizing completion time to maximizing safety. We propose a decision tree-based algorithm to extract STL formulae from multi-label data. Our inference algorithm learns STL specifications from data containing multiple classes, where instances can be labelled by one or many classes. We base our evaluation on a dataset of trajectories collected through an online study reproducing human-robot encounters.},
  archive   = {C_IROS},
  author    = {Alexis Linard and Ilaria Torre and Iolanda Leite and Jana Tumova},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982088},
  pages     = {1305-1311},
  title     = {Inference of multi-class STL specifications for multi-label human-robot encounters},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). UAV-miniUGV hybrid system for hidden area exploration and
manipulation. <em>IROS</em>, 1297–1304. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel hybrid system (both hardware and software) of an Unmanned Aerial Vehicle (UAV) carrying a miniature Unmanned Ground Vehicle (miniUGV) to perform a complex search and manipulation task. This system leverages the heterogeneous robots to accomplish a task that cannot be done using a single robot system. It enables the UAV to explore a hidden space with a narrow opening through which the miniUGV can easily enter and escape. The hidden space is assumed to be navigable for the miniUGV. The miniUGV uses Infrared (IR) sensors and a monocular camera to search an object in the hidden space. The proposed system takes advantage of a wider field of view (fov) of camera as well as the stochastic nature of the object detection algorithms to guide the miniUGV in the hidden space to find the object. Upon finding the object the miniUGV grabs it using visual servoing and then returns back to its start point from where the UAV retracts it back and transports the object to a safe place. In case there is no object found in the hidden space, UAV continues the aerial search. The tethered miniUGV gives the UAV an ability to act beyond its reach and perform a search and manipulation task which was not possible before for any of the robots individually. The system has a wide range of applications and we have demonstrated its feasibility through repetitive experiments.},
  archive   = {C_IROS},
  author    = {Durgakant Pushp and Swapnil Kalhapure and Kaushik Das and Lantao Liu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981957},
  pages     = {1297-1304},
  title     = {UAV-miniUGV hybrid system for hidden area exploration and manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Incremental path planning algorithm via topological mapping
with metric gluing. <em>IROS</em>, 1290–1296. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an incremental topology-based motion planner that, while planning paths in the configuration space, performs metric gluing on the constructed Vietoris-Rips simplicial complex of each sub-space (voxel). By incrementally capturing topological and geometric information in batches of voxel graphs, our algorithm avoids the time overhead of analyzing the properties of the entire configuration space. We theoretically prove in this paper that the simplices of all voxel graphs joined together are homotopy-equivalent to the union of the simplices in the configuration space. Experiments were carried out in seven different environments using various robots, including the articulated linkage robot, the Kuka YouBot, and the PR2 robot. In all environments, the results show that our algorithm achieves better convergence for path cost and computation time with a memory-efficient roadmap than state-of-the-art methods.},
  archive   = {C_IROS},
  author    = {Aakriti Upadhyay and Boris Goldfarb and Chinwe Ekenna},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981379},
  pages     = {1290-1296},
  title     = {Incremental path planning algorithm via topological mapping with metric gluing},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fair planning for mobility-on-demand with temporal logic
requests. <em>IROS</em>, 1283–1289. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobility-on-demand systems are transforming the way we think about the transportation of people and goods. Most research effort has been placed on scalability issues for systems with a large number of agents and simple pickup/drop-off demands. In this paper, we consider fair multi-vehicle route planning with streams of complex, temporal logic transportation demands. We consider an approximately envy-free fair allocation of demands to limited-capacity vehicles based on agents&#39; accumulated utility over a finite time horizon, representing for example monetary reward or utilization level. We propose a scalable approach based on the construction of assignment graphs that relate agents to routes and demands, and pose the problem as an Integer Linear Program (ILP). Routes for assignments are computed using automata-based methods for each vehicle and demands sets of size at most the capacity of the vehicle while taking into account their pickup wait time and delay tolerances. In addition, we integrate utility-based weights in the assignment graph and ILP to ensure approximative fair allocation. We demonstrate the computational and operational performance of our methods in ride-sharing case studies over a large environment in mid-Manhattan and Linear Temporal Logic demands with stochastic arrival times. We show that our method significantly decreases the utility deviation between agents and the vacancy rate.},
  archive   = {C_IROS},
  author    = {Kaier Liang and Cristian-Ioan Vasile},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981291},
  pages     = {1283-1289},
  title     = {Fair planning for mobility-on-demand with temporal logic requests},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sensor observability index: Evaluating sensor alignment for
task-space observability in robotic manipulators. <em>IROS</em>,
1276–1282. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a preliminary definition and analysis of the novel concept of sensor observability index. The goal is to analyse and evaluate the performance of distributed directional or axial-based sensors to observe specific axes in task space as a function of joint configuration in serial robot manipulators. For example, joint torque sensors are often used in serial robot manipulators and assumed to be perfectly capable of estimating end effector forces, but certain joint configurations may cause one or more task-space axes to be unobservable as a result of how the joint torque sensors are aligned. The proposed sensor observability provides a method to analyse the quality of the current robot configuration to observe the task space. Parallels are drawn between sensor observability and the traditional kinematic Jacobian for the particular case of joint torque sensors in serial robot manipulators. Although similar information can be retrieved from kinematic analysis of the Jacobian transpose in serial manipulators, sensor observability is shown to be more generalizable in terms of analysing non-joint-mounted sensors and other sensor types. In addition, null-space analysis of the Jacobian transpose is susceptible to false observability singularities. Simulations and experiments using the robot Baxter demonstrate the importance of maintaining proper sensor observability in physical interactions.},
  archive   = {C_IROS},
  author    = {Christopher Yee Wong and Wael Suleiman},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982209},
  pages     = {1276-1282},
  title     = {Sensor observability index: Evaluating sensor alignment for task-space observability in robotic manipulators},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robustness-based synthesis for stochastic systems under
signal temporal logic tasks. <em>IROS</em>, 1269–1275. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We develop a method for synthesizing control policies for stochastic, linear, time-varying systems that must perform tasks specified in signal temporal logic. We build upon an efficient, sampling-based framework that computes the probability of the system satisfying its specification. By exploiting the properties of linear systems and robustness score in temporal logic specifications, we obtain sample-efficient gradients of the satisfaction probability with respect to con-troller parameters. Therefore, by applying gradient descent we obtain locally optimized controllers that maximize the chances of satisfying the specification. We demonstrate our approach through examples of a mobile robot and a mobile manipulator in simulation.},
  archive   = {C_IROS},
  author    = {Guy Scher and Sadra Sadraddini and Hadas Kress-Gazit},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982233},
  pages     = {1269-1275},
  title     = {Robustness-based synthesis for stochastic systems under signal temporal logic tasks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Classification of time-series data using boosted decision
trees. <em>IROS</em>, 1263–1268. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Time-series data classification is central to the analysis and control of autonomous systems, such as robots and self-driving cars. Temporal logic-based learning algorithms have been proposed recently as classifiers of such data. However, current frameworks are either inaccurate for real-world applications, such as autonomous driving, or they generate long and complicated formulae that lack interpretability. To address these limitations, we introduce a novel learning method, called Boosted Concise Decision Trees (BCDTs), to generate binary classifiers that are represented as Signal Temporal Logic (STL) formulae. Our algorithm leverages an ensemble of Concise Decision Trees (CDTs) to improve the classification performance, where each CDT is a decision tree that is empowered by a set of techniques to generate simpler formulae and improve interpretability. The effectiveness and classification performance of our algorithm are evaluated on naval surveillance and urban-driving case studies.},
  archive   = {C_IROS},
  author    = {Erfan Aasi and Cristian Ioan Vasile and Mahroo Bahreinian and Calin Belta},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982105},
  pages     = {1263-1268},
  title     = {Classification of time-series data using boosted decision trees},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimizing demonstrated robot manipulation skills for
temporal logic constraints. <em>IROS</em>, 1255–1262. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For performing robotic manipulation tasks, the core problem is determining suitable trajectories that fulfill the task requirements. Various approaches to compute such trajectories exist, being learning and optimization the main driving techniques. Our work builds on the learning-from-demonstration (LfD) paradigm, where an expert demonstrates motions, and the robot learns to imitate them. However, expert demonstrations are not sufficient to capture all sorts of task specifications, such as the timing to grasp an object. In this paper, we propose a new method that considers formal task specifications within LfD skills. Precisely, we leverage Signal Temporal Logic (STL), an expressive form of temporal properties of systems, to formulate task specifications and use black-box optimization (BBO) to adapt an LfD skill accordingly. We demonstrate our approach in simulation and on a real industrial setting using several tasks that showcase how our approach addresses the LfD limitations using STL and BBO.},
  archive   = {C_IROS},
  author    = {Akshay Dhonthi and Philipp Schillinger and Leonel Rozo and Daniele Nardi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981384},
  pages     = {1255-1262},
  title     = {Optimizing demonstrated robot manipulation skills for temporal logic constraints},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bio-inspired 2D vertical climbing with a novel tripedal
robot. <em>IROS</em>, 1239–1246. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Climbing robots have the potential to revolutionize the maintenance and inspection operations of many types of vertical structures. In nature, parrots exhibit a remarkable capacity for manipulation during climbing behaviors, for which robotics can benefit from studying. In this paper we present a novel tripedal robot that is inspired by the morphology of these impressive birds, which use their legs and beak in a tripedal fashion when climbing. We propose several foot placement, trajectory generation, and control methods for this system along with performance evaluation in simulation. A video of select simulations and live bird data is included in the supplementary material, and can also be found at https://youtu.be/vRVGralyQgQ.},
  archive   = {C_IROS},
  author    = {Clyde Webster and Felix H. Kong and Robert Fitch},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981088},
  pages     = {1239-1246},
  title     = {Bio-inspired 2D vertical climbing with a novel tripedal robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributed coach-based reinforcement learning controller
for snake robot locomotion. <em>IROS</em>, 1231–1238. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning commonly suffers from slow convergence speed and requires thousands of episodes, which makes it hard to be applied for physical robotic applications. Little research has been studied for snake robot control using RL because of the additional difficulty of high redundancy of freedom. Existing methods either adopts an asynchronous A3C structure or a joint state representation. We propose a distributed coach-based deep learning method for snake robot control, which can greatly expedite the training speed with less episodes. The major contributions include: 1) a completely distributed graphical formulation; 2) an explicit stochastic density propagation rule for each robot link; 3) various interaction models with uncertainty estimation. The preliminary results of both simulation and real-world experiments have demonstrated the promising performance in comparison with state-of-the-art.},
  archive   = {C_IROS},
  author    = {Yuanyuan Jia and Shugen Ma},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981749},
  pages     = {1231-1238},
  title     = {Distributed coach-based reinforcement learning controller for snake robot locomotion},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design and experiments of snake robots with docking
function. <em>IROS</em>, 1208–1214. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel snake robot with the docking function, which can help the snake robots to connect with each other to achieve a stronger one with double length and double degrees of freedom. First, the mechanical design of the snake robot with docking function is introduced, including the body link and the head-tail passive docking mechanical structure. Second, the control system is built, and the control strategies of locomotion and docking are separately proposed. Then, the visual perception function is implemented for the target recognition during the docking process. Finally, the prototype is developed. The mobility and the docking function are fully verified and analyzed through the physical experiments.},
  archive   = {C_IROS},
  author    = {Fatao Qin and Xiaojie Duan and Shihao Ma and Jinglun Yuan and Xiangyu Wang and Jianming Wang and Xuan Xiao},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981896},
  pages     = {1208-1214},
  title     = {Design and experiments of snake robots with docking function},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A creeping snake-like robot with partial actuation.
<em>IROS</em>, 1202–1207. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Enlightened by the creeping gait of natural snakes, snake-like robots swing joints side to side at similar tracks for generating propelling forces. However, it is not always essential to control all joints of a snake-like robot to realize the creeping gait. Therefore, in this paper, a creeping snake-like robot with partially actuated joints has been investigated, towards reducing the redundancy caused by full actuation. Essentially, this approach is composed of the following two concepts: 1) joint equipped with torsion spring mechanism bridges the passive joint to generate rhythm oscillation, and 2) harmonic joint trajectories assist the robot in generating more efficient locomotion. We hereafter demonstrated that the actuated joint dominates passive dynamics of the system, which contributes to overall motion. Meanwhile, different spring stiffness affects the motion performance. Additionally, the interaction between robot and environment through Coulomb friction has been considered to reveal the contributing factors that assist the snake-like robot to yield better locomotion performance.},
  archive   = {C_IROS},
  author    = {Yiming Cao and Longchuan Li and Shugen Ma},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981782},
  pages     = {1202-1207},
  title     = {A creeping snake-like robot with partial actuation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Embodying rather than encoding: Undulation with binary
input. <em>IROS</em>, 1196–1201. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Undulation is the most common gait generated by legless creatures, which enables their robust and efficient locomotion in various environments. Such advantages inspired the control design of many kinds of locomotion robots. Despite their technical details, most of them realize the undulation gait via tracking predetermined trajectories called serpenoid curves, which are a group of sinusoidal waveforms with specified phase differences. This technique, however, sounds quite redundant in terms of sensing and control. Here, we investigate the research question: whether the sinusoidal waveform is necessary to be encoded in the control signal to make the whole body an “S-shape”? We use a 4-link rigid body dynamics model as a simple example, by which numerical simulations are conducted. Together with theoretical analysis, we show that undulation gait emerges naturally based on embodied position controller and filter, where binary actuation torques are required only. Our results not only discover locomotion mechanisms for significantly reducing the sensing and control requirement of generating artificial undulation gait, but also provide additional understandings for biological systems from the mechanical engineering point of view.},
  archive   = {C_IROS},
  author    = {Longchuan Li and Shugen Ma and Isao Tokuda and Yang Tian and Yiming Cao and Makoto Nokata and Zhiqing Li},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982001},
  pages     = {1196-1201},
  title     = {Embodying rather than encoding: Undulation with binary input},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). In-hand manipulation exploiting bending and compression
deformations of caterpillar-locomotion-inspired fingers. <em>IROS</em>,
1188–1195. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel method of realizing in-hand manipulation inspired by the peristaltic motion of a large-sized caterpillar. The sharp contrast between the proposed soft-bodied finger and the conventional hard/rigid robotic ones is peristaltic motion with compression and bending deformations. The design is based on the biological fact that large-size caterpillars (e.g., Bombyx mori) utilize bending and compression/extension of the body to produce crawling locomotion. Exploiting the multi-modal deformations, we demonstrated that the prototype hand comprising two proposed fingers could rotate and transport grasped objects. We also observed that the time gap of two-finger motion is required to stabilize in-hand manipulation. The design can provide new insights into designing a gripper inspired by soft-bodied creatures.},
  archive   = {C_IROS},
  author    = {Tomoya Onodera and Noriyasu Iwamoto and Takuya Umedachi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982281},
  pages     = {1188-1195},
  title     = {In-hand manipulation exploiting bending and compression deformations of caterpillar-locomotion-inspired fingers},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The flatworm-like mesh robot WORMESH-II: Steering control of
pedal wave locomotion. <em>IROS</em>, 1182–1187. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {WORMESH is a unique robot concept inspired by flatworm locomotion and its key feature is the use of multiple traveling waves for locomotion. This paper presents the steering method for anisotropic module configuration (AMC) of WORMESH-II based on the kinematics of skid steering of mobile robots. AMC of WORMESH-II used two parallel pedal waves to generate locomotion. The kinematic model of WORMESH-II shows its longitudinal and angular velocities depend on the summation and the difference of two synchronous left and right pedal waves amplitudes $A_{l}$ and $A_{r}$ respectively. When both pedal waves have the same amplitude, the robot moves on a straight line, whereas the trajectory becomes a curve for different wave amplitudes. The radius of curve trajectory is inversely proportional to $\vert A_{l}-A_{r}\vert$ . The proposed method was ineffective when $A_{i}\approx 0(i=l,\ r)$ . The proposed method was confirmed by the dynamic simulation of WORMESH-II using a physics engine. Moreover, the recommended skid steering method was tested using the prototype and verified.},
  archive   = {C_IROS},
  author    = {G.V.C. Rasanga and K. Hirashi and R. Hodoshima and S. Kotosaka},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982155},
  pages     = {1182-1187},
  title     = {The flatworm-like mesh robot WORMESH-II: Steering control of pedal wave locomotion},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A passive control framework for a bilateral leader-follower
robotic surgical setup imposing RCM and active constraints.
<em>IROS</em>, 1175–1181. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the problem of controlling a bilateral leader-follower robotic surgical set-up to allow kinesthetic haptic feedback to the user when the instrument approaches a forbidden area like sensitive organs arteries or veins that should be protected from injuries during surgery. The leader is a haptic device while the follower is a general purpose manipulator holding an elongated tool with an articulated instrument that should be manipulated through an entry port. We propose a control framework that is proved passive, incorporating a target admittance model for the follower that is designed in a way to impose a remote center of motion (RCM) while being subject to repulsive forces generated by properly designed artificial potentials associated with forbidden areas. Simulation and experimental results utilizing a virtual intraoperative environment provided as a point cloud of a kidney and its surrounding vessels characterized as forbidden areas, validate and demonstrate the performance of the proposed control scheme.},
  archive   = {C_IROS},
  author    = {Theodora Kastritsi and Zoe Doulgeri},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982206},
  pages     = {1175-1181},
  title     = {A passive control framework for a bilateral leader-follower robotic surgical setup imposing RCM and active constraints},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Three-dimensional dynamic running with a point-foot biped
based on differentially flat SLIP. <em>IROS</em>, 1169–1174. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel framework for point- foot biped running in three-dimensional space. The proposed approach generates center of mass (CoM) reference trajectories based on a differentially flat spring-loaded inverted pendulum (SLIP) model. A foothold planner is used to select touch down location that renders optimal CoM trajectory for upcoming step in real time. Dynamically feasible trajectories of CoM and orientation are subsequently generated by a simplified single rigid body (SRB) model based model predictive control (MPC). A task-space controller is then applied online to compute whole- body joint torques which embeds these target dynamics into the robot. The proposed approach is evaluated on physical simulation of a 12 degree-of-freedom (DoF), 7.95 kg point-foot bipedal robot. The robot achieves stable running at at varying speeds with maximum value of 1.1 m/s. The proposed scheme is shown to be able to reject vertical disturbances of 8 N. s and lateral disturbance of 6.5 N. s applied at the robot base.},
  archive   = {C_IROS},
  author    = {Zejun Hong and Hua Chen and Wei Zhang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981516},
  pages     = {1169-1174},
  title     = {Three-dimensional dynamic running with a point-foot biped based on differentially flat SLIP},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adapting rapid motor adaptation for bipedal robots.
<em>IROS</em>, 1161–1168. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in legged locomotion have en-abled quadrupeds to walk on challenging terrains. However, bipedal robots are inherently more unstable and hence it&#39;s harder to design walking controllers for them. In this work, we leverage recent advances in rapid adaptation for locomotion control, and extend them to work on bipedal robots. Similar to existing works, we start with a base policy which produces actions while taking as input an estimated extrinsics vector from an adaptation module. This extrinsics vector contains information about the environment and enables the walking controller to rapidly adapt online. However, the extrinsics estimator could be imperfect, which might lead to poor performance of the base policy which expects a perfect estimator. In this paper, we propose A-RMA (Adapting RMA), which additionally adapts the base policy for the imperfect extrinsics estimator by finetuning it using model-free RL. We demonstrate that A-RMA outperforms a number of RL-based baseline controllers and model-based controllers in simulation, and show zero-shot deployment of a single A-RMA policy to enable a bipedal robot, Cassie, to walk in a variety of different scenarios in the real world beyond what it has seen during training. Videos and results at https: //ashish-kmr.github.io/a-rma/},
  archive   = {C_IROS},
  author    = {Ashish Kumar and Zhongyu Li and Jun Zeng and Deepak Pathak and Koushil Sreenath and Jitendra Malik},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981091},
  pages     = {1161-1168},
  title     = {Adapting rapid motor adaptation for bipedal robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Parallel monte carlo tree search with batched rigid-body
simulations for speeding up long-horizon episodic robot planning.
<em>IROS</em>, 1153–1160. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel Parallel Monte Carlo tree search with Batched Simulations (PMBS) algorithm for accelerating long-horizon, episodic robotic planning tasks. Monte Carlo tree search (MCTS) is an effective heuristic search algorithm for solving episodic decision-making problems whose underlying search spaces are expansive. Leveraging a GPU-based large-scale simulator, PMBS introduces massive parallelism into MCTS for solving planning tasks through the batched execution of a large number of concurrent simulations, which allows for more efficient and accurate evaluations of the expected cost-to-go over large action spaces. When applied to the challenging manipulation tasks of object retrieval from clutter, PMBS achieves a speedup of over 30× with an improved solution quality, in comparison to a serial MCTS implementation. We show that PMBS can be directly applied to real robot hardware with negligible sim-to-real differences. Supplementary material, including video, can be found at https://github.com/arc-l/pmbs.},
  archive   = {C_IROS},
  author    = {Baichuan Huang and Abdeslam Boularias and Jingjin Yu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981962},
  pages     = {1153-1160},
  title     = {Parallel monte carlo tree search with batched rigid-body simulations for speeding up long-horizon episodic robot planning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rearrangement-based manipulation via kinodynamic planning
and dynamic planning horizons. <em>IROS</em>, 1145–1152. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot manipulation in cluttered environments of-ten requires complex and sequential rearrangement of multiple objects in order to achieve the desired reconfiguration of the target objects. Due to the sophisticated physical interactions involved in such scenarios, rearrangement-based manipulation is still limited to a small range of tasks and is especially vulnerable to physical uncertainties and perception noise. This paper presents a planning framework that leverages the efficiency of sampling-based planning approaches, and closes the manipulation loop by dynamically controlling the planning horizon. Our approach interleaves planning and execution to progressively approach the manipulation goal while correcting any errors or path deviations along the process. Meanwhile, our framework allows the definition of manipulation goals without requiring explicit goal configurations, enabling the robot to flexibly interact with all objects to facilitate the manipulation of the target ones. With extensive experiments both in simulation and on a real robot, we evaluate our framework on three manipulation tasks in cluttered environments: grasping, relocating, and sorting. In comparison with two baseline approaches, we show that our framework can significantly improve planning efficiency, robustness against physical uncertainties, and task success rate under limited time budgets.},
  archive   = {C_IROS},
  author    = {Kejia Ren and Lydia E. Kavraki and Kaiyu Hang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981599},
  pages     = {1145-1152},
  title     = {Rearrangement-based manipulation via kinodynamic planning and dynamic planning horizons},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Controlling the cascade: Kinematic planning for n-ball toss
juggling. <em>IROS</em>, 1139–1144. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dynamic movements are ubiquitous in human motor behavior as they tend to be more efficient and can solve a broader range of skill domains than their quasi-static counterparts. For decades, robotic juggling tasks have been among the most frequently studied dynamic manipulation problems since the required dynamic dexterity can be scaled to arbitrarily high difficulty. However, successful approaches have been limited to basic juggling skills, indicating a lack of understanding of the required constraints for dexterous toss juggling. We present a detailed analysis of the toss juggling task, identifying the key challenges of the switching contacts task and formalizing it as a trajectory optimization problem. Building on our state-of-the-art, real-world toss juggling platform, we reach the theoretical limits of toss juggling in simulation, evaluate a resulting real-time controller in environments of varying difficulty and achieve robust toss juggling of up to 17 balls on two anthropomorphic manipulators. https://sites.google.com/view/controlling-the-cascade},
  archive   = {C_IROS},
  author    = {Kai Ploeger and Jan Peters},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981678},
  pages     = {1139-1144},
  title     = {Controlling the cascade: Kinematic planning for N-ball toss juggling},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep reinforcement learning based on local GNN for
goal-conditioned deformable object rearranging. <em>IROS</em>,
1131–1138. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Object rearranging is one of the most common deformable manipulation tasks, where the robot needs to rearrange a deformable object into a goal configuration. Previous studies focus on designing an expert system for each specific task by model-based or data-driven approaches and the application scenarios are therefore limited. Some research has been attempting to design a general framework to obtain more advanced manipulation capabilities for deformable rearranging tasks, with lots of progress achieved in simulation. However, transferring from simulation to reality is difficult due to the limitation of the end-to-end CNN architecture. To address these challenges, we design a local GNN (Graph Neural Network) based learning method, which utilizes two representation graphs to encode keypoints detected from images. Self-attention is applied for graph updating and cross-attention is applied for generating manipulation actions. Extensive experiments have been conducted to demonstrate that our framework is effective in multiple 1-D (rope, rope ring) and 2-D (cloth) rearranging tasks in simulation and can be easily transferred to a real robot by fine-tuning a keypoint detector.},
  archive   = {C_IROS},
  author    = {Yuhong Deng and Chongkun Xia and Xueqian Wang and Lipeng Chen},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981669},
  pages     = {1131-1138},
  title     = {Deep reinforcement learning based on local GNN for goal-conditioned deformable object rearranging},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Transfer learning for machine learning-based detection and
separation of entanglements in bin-picking applications. <em>IROS</em>,
1123–1130. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a Domain Randomization and a Domain Adaptation approach to transfer experience for entanglement detection and separation from simulation into a real-world bin-picking application. We investigate the influence of different randomization options in image processing and use a CycleGAN as a further Domain Adaptation method to synthesize simulation data as realistically as possible. On the basis of this adapted data we re-train our detection and separation methods and validate the usefulness of these Sim-to-Real methods. In numerous real-world experiments we show that we achieve a significant increase of up to 71.74\% in the performance of the overall system by using the Sim-to-Real approaches as opposed to the direct transfer.},
  archive   = {C_IROS},
  author    = {Marius Moosmann and Felix Spenrath and Johannes Rosport and Philipp Melzer and Werner Kraus and Richard Bormann and Marco F. Huber},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981082},
  pages     = {1123-1130},
  title     = {Transfer learning for machine learning-based detection and separation of entanglements in bin-picking applications},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning goal-oriented non-prehensile pushing in cluttered
scenes. <em>IROS</em>, 1116–1122. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pushing objects through cluttered scenes is a challenging task, especially when the objects to be pushed have initially unknown dynamics and touching other entities has to be avoided to reduce the risk of damage. In this paper, we approach this problem by applying deep reinforcement learning to generate pushing actions for a robotic manipulator acting on a planar surface where objects have to be pushed to goal locations while avoiding other items in the same workspace. With the latent space learned from a depth image of the scene and other observations of the environment, such as contact information between the end effector and the object as well as distance to the goal, our framework is able to learn contact-rich pushing actions that avoid collisions with other objects. As the experimental results with a six degrees of freedom robotic arm show, our system is able to successfully push objects from start to end positions while avoiding nearby objects. Furthermore, we evaluate our learned policy in comparison to a state-of-the-art pushing controller for mobile robots and show that our agent performs better in terms of success rate, collisions with other objects, and continuous object contact in various scenarios.},
  archive   = {C_IROS},
  author    = {Nils Dengler and David Großklaus and Maren Bennewitz},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981873},
  pages     = {1116-1122},
  title     = {Learning goal-oriented non-prehensile pushing in cluttered scenes},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Controlled fabrication of micro-chain robot using
magnetically guided arraying microfluidic devices. <em>IROS</em>,
1090–1095. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The magnetic microrobot has become a promising approach in many biomedical applications due to its small volume, flexible motion, and untethered micromachines. The micro-chain robot is one of the most popular magnetic microrobots. However, the uncontrollable magnetic moment direction and quantity of the magnetic beads consisted in the existing self-assembled micro-chain robot limit their locomotion and applications. This paper proposed an on-chip micro-chain robot fabrication method to assemble the magnetic beads with controllable magnetic moment direction and quantity. The bead quantity can be controlled by the structure limits of the microchannel, and the direction of the magnetic moment can be adj usted by the integrated external magnetic field. The assembled magnetic beads are then glued by the hydrogel under UV exposure. The micro-chain robots with different quantities and magnetic moment directions of the magnetic beads were successfully fabricated and tested in experiments. Due to the array structure of the microfluidic device, batch manufacturing of low-cost magnetic robots was achieved in our method. The movement of dual-bead microrobots with two orthogonal magnetic moment directions was analyzed and compared. One of the dual-bead microrobots was applied in the transportation of the hydrogel module using pushing and pulling modes. It indicated that the proposed controllable on-chip fabrication of the magnetic micro-chain robots has the potential to enhance the microrobot ability in biomedical applications.},
  archive   = {C_IROS},
  author    = {Xiaoqing Tang and Xiaoming Liu and Yuyang Li and Dan Liu and Yuke Li and Masaru Kojima and Qiang Huang and Tatsuo Arai},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981455},
  pages     = {1090-1095},
  title     = {Controlled fabrication of micro-chain robot using magnetically guided arraying microfluidic devices},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time acoustic holography with physics-based deep
learning for acoustic robotic manipulation. <em>IROS</em>, 1083–1089.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Acoustic holography is a newly emerging and promising technique to dynamically generate arbitrary desired holographic acoustic field in 3D space for contactless robotic manipulation. The latest technology supporting complex dynamic holographic acoustic field reconstruction is through phased transducer array (PTA), where the phase profile of emitted acoustic wave from discrete transducers is controlled independently by sophisticated circuits to modulate the acoustic interference field. While the forward kinematics of a phased array based robotic manipulation system is simple and straightforward, the inverse kinematics of the required holographic acoustic field is mathematically non-linear and unsolvable, which substantially limits the application of dynamic holographic acoustic field for robot manipulation. In this work, we propose a physics-based deep learning framework for this phase retrieval inverse kinematics problem so that the target complex hologram could be reconstructed precisely with average MAE of 0.022 and in real time with prediction time of 47 milliseconds on GPU. The accuracy and real time of the proposed method for dynamic holographic acoustic field reconstruction from PTA are demonstrated experimentally.},
  archive   = {C_IROS},
  author    = {Chengxi Zhong and Zhenhuan Sun and Kunyong Lyu and Yao Guo and Song Liu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981395},
  pages     = {1083-1089},
  title     = {Real-time acoustic holography with physics-based deep learning for acoustic robotic manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling and characterization of artificial bacteria
flagella with micro-structured soft-magnetic teeth. <em>IROS</em>,
1062–1067. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sub-structures such as micro-structured magnetic teeth fabricated with an artificial bacteria flagellum (ABF) are designed for achieving more motion modes, higher precision, and better controllability. To achieve these, a more precise model considering the non-circular cross-sectional features is setup without simplifying the structure as a helical filament with a circular cross-section as having been used in previous investigations, making it possible to include the effects of the substructures into the motion equation. Analyses and experiments verified the correctness. Besides of the geometric effects, our experimental observation also shows an anomalous step-out frequency appeared in an ABF. This asynchronous motion is attributed to the lag of magnetization with respect to the external rotating magnetic field due to the geometries and the soft-magnetic materials of the ribbons, which is different from the regular asynchronous motion solely caused by low Reynolds number of fluid to microscopic swimmers. While the lag of magnetization can be further attributed initiatively to the soft magnetic materials adopted, the feasibility to arrange the easy axis will enable many new possibilities, which is of particular interest in generating more modes for swarms such as cascade stepping out of ABFs with the same nominal overall sizes and for more precise positioning using stepping motion.},
  archive   = {C_IROS},
  author    = {Zejie Yu and Chaojian Hou and Shuideng Wang and Kun Wang and Donglei Chen and Wenqi Zhang and Zhi Qu and Zhiyong Sun and Bo Song and Chao Zhou and Lixin Dong},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981435},
  pages     = {1062-1067},
  title     = {Modeling and characterization of artificial bacteria flagella with micro-structured soft-magnetic teeth},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A micro-robotic approach for the correction of angular
deviations in AFM samples from generic topographic data. <em>IROS</em>,
1055–1061. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This article proposes a method for the correction of angular deviations caused during the fixing process of samples prepared for Atomic Force Microscopy (AFM). The correction is done using the angular control of a 6-DOF PPPS parallel platform were the sample is placed, while the AFM scan is performed by a 3-DOF serial cartesian robot with a tuning fork probe designed to perform FM-AFM. The method uses the generic x, y, and z data provided by the AFM after performing a scan on a free surface of the sample substrate. This is used to calculate the plane that closest approximates the points by solving a system of linear equations. This plane is then used to estimate the angular corrections that the 6-DOF parallel robot has to do in order to compensate the deviations. The proposed algorithm can be performed iteratively in order to refine the correction. The method also does not require any special preparation of the substrate. It only requires to have a free surface to scan. Experiments are performed using this algorithm to correct the orientation deviation of a substrate of V1 High-grade mica. The results show that the method is able to correct the angular deviation of the sample relatively to the AFM probe with an error of 0.2° after only two iterations of the algorithm.},
  archive   = {C_IROS},
  author    = {Freddy Romero Leiro and Ali Bazaei and Stéphane Régnier and Mokrane Boudaoud},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981512},
  pages     = {1055-1061},
  title     = {A micro-robotic approach for the correction of angular deviations in AFM samples from generic topographic data},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical learning and control for in-hand
micromanipulation using multiple laser-driven micro-tools.
<em>IROS</em>, 1047–1054. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Laser-driven micro-tools are formulated by treating highly-focused laser beams as actuators, to control the tool&#39;s motion to contact then manipulate a micro object, which allows it to manipulate opaque micro objects, or large cells without causing photodamage. However, most existing laser-driven tools are limited to relatively simple tasks, such as moving and caging, and cannot carry out in-hand dexterous tasks. This is mainly because in-hand manipulation involves continuously coordinating multiple laser beams, micro-tools, and the object itself, which has high degrees of freedom (DoF) and poses up challenge for planner and controller design. This paper presents a new hierarchical formulation for the grasping and manipulation of micro objects using multiple laser-driven micro-tools. In hardware, multiple laser-driven tools are assembled to act as a robotic hand to carry out in-hand tasks (e.g., rotating); in software, a hierarchical scheme is developed to shrunken the action space and coordinate the motion of multiple tools, subject to both the parametric uncertainty in the tool and the unknown dynamic model of the object. Such a formulation provides potential for achieving robotic in-hand manipulation at a micro scale. The performance of the proposed system is validated in simulation studies under different scenarios.},
  archive   = {C_IROS},
  author    = {Yongyi Jia and Yu Chen and Hao Liu and Xiu Li and Xiang Li},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982033},
  pages     = {1047-1054},
  title     = {Hierarchical learning and control for in-hand micromanipulation using multiple laser-driven micro-tools},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficiently learning manipulations by selecting structured
skill representations. <em>IROS</em>, 1039–1046. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A key challenge in learning to perform manipulation tasks is selecting a suitable skill representation. While specific skill representations are often easier to learn, they are often only suitable for a narrow set of tasks. In most prior works, roboticists manually provide the robot with a suitable skill representation to use e.g. a neural network or DMPs. By contrast, we propose to allow the robot to select the most appropriate skill representation for the underlying task. Given the large space of skill representations, we utilize a single demonstration to select a small set of potential task-relevant representations. This set is then further refined using reinforcement learning to select the most suitable skill representation. Experiments in both simulation and real world show how our proposed approach leads to improved sample efficiency and enables directly learning on the real robot.},
  archive   = {C_IROS},
  author    = {Mohit Sharma and Oliver Kroemer},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981422},
  pages     = {1039-1046},
  title     = {Efficiently learning manipulations by selecting structured skill representations},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-supervised feature learning from partial point clouds
via pose disentanglement. <em>IROS</em>, 1031–1038. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-supervised learning on point clouds has gained a lot of attention recently, since it addresses the label-efficiency and domain-gap problems on point cloud tasks. In this paper, we propose a novel self-supervised framework to learn informative features from partial point clouds. We leverage partial point clouds scanned by LiDAR that contain both content and pose attributes, and we show that disentangling such two factors from partial point clouds enhances feature learning. To this end, our framework consists of three main parts: 1) a completion network to capture holistic semantics of point clouds; 2) a pose regression network to understand the viewing angle where partial data is scanned from; 3) a partial reconstruction network to encourage the model to learn content and pose features. To demonstrate the robustness of the learnt feature representations, we conduct several downstream tasks including classification, part segmentation, and registration, with comparisons against state-of-the-art methods. Our method not only outperforms existing self-supervised methods, but also shows a better generalizability across synthetic and real-world datasets.},
  archive   = {C_IROS},
  author    = {Meng-Shiun Tsai and Pei-Ze Chiang and Yi-Hsuan Tsai and Wei-Chen Chiu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981390},
  pages     = {1031-1038},
  title     = {Self-supervised feature learning from partial point clouds via pose disentanglement},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MO-transformer: A transformer-based multi-object point cloud
reconstruction network. <em>IROS</em>, 1024–1030. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a new network for reconstructing multi-object point cloud. Different from previous networks which reconstruct multi-object point cloud as a whole, our network iteratively reconstructs each individual object point cloud from a frame of multi-object point cloud. To achieve this goal, we have designed MO-Transformer, a transformer-based autoregressive network. During training, MO-Transformer takes a frame of multi-object point cloud and individual object point clouds as input. During testing, MO-Transformer iteratively reconstructs individual object point clouds only based on the input multi-object point cloud. To train the proposed MO-Transformer, we design a new loss function called separate Chamfer distance (SCD). In addition, we prove that SCD is an upper bound of the traditional Chamfer distance calculated based on the entire multi-object point cloud. The reconstruction experiment verifies the efficacy of our network in multi-object point cloud reconstruction. Furthermore, the reconstruction experiment also investigates the effect of different dimensions using a series of datasets. The ablation study experiment verifies the necessity of SCD in training MO-Transformer.},
  archive   = {C_IROS},
  author    = {Erli Lyu and Zhengyan Zhang and Wei Liu and Jiaole Wang and Shuang Song and Max Q.-H. Meng},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981837},
  pages     = {1024-1030},
  title     = {MO-transformer: A transformer-based multi-object point cloud reconstruction network},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Explainable knowledge graph embedding: Inference
reconciliation for knowledge inferences supporting robot actions.
<em>IROS</em>, 1008–1015. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learned knowledge graph representations supporting robots contain a wealth of domain knowledge that drives robot behavior. However, there does not exist an inference reconciliation framework that expresses how a knowledge graph representation affects a robot&#39;s sequential decision making. We use a pedagogical approach to explain the inferences of a learned, black-box knowledge graph representation, a knowledge graph embedding. Our interpretable model uses a decision tree classifier to locally approximate the predictions of the black-box model and provides natural language explanations interpretable by non-experts. Results from our algorithmic evaluation affirm our model design choices, and the results of our user studies with non-experts support the need for the proposed inference reconciliation framework. Critically, results from our simulated robot evaluation indicate that our explanations enable non-experts to correct erratic robot behaviors due to nonsensical beliefs within the black-box.},
  archive   = {C_IROS},
  author    = {Angel Daruna and Devleena Das and Sonia Chernova},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982104},
  pages     = {1008-1015},
  title     = {Explainable knowledge graph embedding: Inference reconciliation for knowledge inferences supporting robot actions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). COMPASS: Contrastive multimodal pretraining for autonomous
systems. <em>IROS</em>, 1000–1007. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning representations that generalize across tasks and domains is challenging yet necessary for autonomous systems. Although task-driven approaches are appealing, de-signing models specific to each application can be difficult in the face of limited data, especially when dealing with highly variable multimodal input spaces arising from different tasks in different environments. We introduce the first general-purpose pretraining pipeline, COntrastive Multimodal Pretraining for AutonomouS Systems (COMPASS), to overcome the limitations of task-specific models and existing pretraining approaches. COMPASS constructs a multimodal graph by considering the essential information for autonomous systems and the proper-ties of different modalities. Through this graph, multimodal signals are connected and mapped into two factorized spatio-temporal latent spaces: a “motion pattern space” and a “current state space.” By learning from multimodal correspondences in each latent space, COMPASS creates state representations that models necessary information such as temporal dynamics, geometry, and semantics. We pretrain COMPASS on a large-scale multimodal simulation dataset TartanAir [1] and evaluate it on drone navigation, vehicle racing, and visual odometry tasks. The experiments indicate that COMPASS can tackle all three scenarios and can also generalize to unseen environments and real-world data. 1 1 Our code implementation can be found at https://github.com/microsoft/COMPASS},
  archive   = {C_IROS},
  author    = {Shuang Ma and Sai Vemprala and Wenshan Wang and Jayesh K. Gupta and Yale Song and Daniel McDufft and Ashish Kapoor},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982241},
  pages     = {1000-1007},
  title     = {COMPASS: Contrastive multimodal pretraining for autonomous systems},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Playful interactions for representation learning.
<em>IROS</em>, 992–999. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ahstract- One of the key challenges in visual imitation learning is collecting large amounts of expert demonstrations for a given task. While methods for collecting human demonstrations are becoming easier with teleoperation methods and the use of low-cost assistive tools, we often still require 100–1000 demonstrations for every task to learn a visual representation and policy. To address this, we turn to an alternate form of data that does not require task-specific demonstrations - play. Playing is a fundamental method children use to learn a set of skills and behaviors and visual representations in early learning. Importantly, play data is diverse, task-agnostic, and relatively cheap to obtain. In this work, we propose to use playful interactions in a self-supervised manner to learn visual representations for downstream tasks. We collect 2 hours of playful data in 19 diverse environments and use self-predictive learning to extract visual representations. Given these representations, we train policies using imitation learning for two downstream tasks: Pushing and Stacking. We demonstrate that our visual representations generalize better than standard behavior cloning and can achieve similar performance with only half the number of required demonstrations. Our representations, which are trained from scratch, compare favorably against ImageNet pretrained representations. Finally, we provide an experimental analysis on the effects of different pretraining modes on downstream task learning.},
  archive   = {C_IROS},
  author    = {Sarah Young and Jyothish Pari and Pieter Abbeel and Lerrel Pinto},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981307},
  pages     = {992-999},
  title     = {Playful interactions for representation learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DreamingV2: Reinforcement learning with discrete world
models without reconstruction. <em>IROS</em>, 985–991. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The present paper proposes a novel reinforce-ment learning method with world models, DreamingV2, a collaborative extension of DreamerV2 and Dreaming. Dream- erV2 is a cutting-edge model-based reinforcement learning from pixels that uses discrete world models to represent latent states with categorical variables. Dreaming is also a form of reinforcement learning from pixels that attempts to avoid the auto encoding process in general world model training by involving a reconstruction-free contrastive learning objective. The proposed DreamingV2 is a novel approach of adopting both the discrete representation of DreamingV2 and the reconstruction-free objective of Dreaming. Compared to DreamerV2 and other recent model-based methods without reconstruction, DreamingV2 achieves the best scores on five simulated challenging 3D robot arm tasks. We believe that DreamingV2 will be a reliable solution for robot learning since its discrete representation is suitable to describe discontinuous environments, and the reconstruction-free fashion well manages complex vision observations.},
  archive   = {C_IROS},
  author    = {Masashi Okada and Tadahiro Taniguchi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981405},
  pages     = {985-991},
  title     = {DreamingV2: Reinforcement learning with discrete world models without reconstruction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reshaping robot trajectories using natural language
commands: A study of multi-modal data alignment using transformers.
<em>IROS</em>, 978–984. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Natural language is the most intuitive medium for us to interact with other people when expressing commands and instructions. However, using language is seldom an easy task when humans need to express their intent towards robots, since most of the current language interfaces require rigid templates with a static set of action targets and commands. In this work, we provide a flexible language-based interface for human-robot collaboration, which allows a user to reshape existing trajectories for an autonomous agent. We take advantage of recent advancements in the field of large language models (BERT and CLIP) to encode the user command, and then combine these features with trajectory information using multi-modal attention transformers. We train the model using imitation learning over a dataset containing robot trajectories modified by language commands, and treat the trajectory generation process as a sequence prediction problem, analogously to how language generation architectures operate. We evaluate the system in multiple simulated trajectory scenarios, and show a significant performance increase of our model over baseline approaches. In addition, our real-world experiments with a robot arm show that users significantly prefer our natural language interface over traditional methods such as kinesthetic teaching or cost-function programming. Our study shows how the field of robotics can take advantage of large pre-trained language models towards creating more intuitive interfaces between robots and machines. Project webpage: https://arthurfenderbucker.github.io/NL_trajectory_reshaper/},
  archive   = {C_IROS},
  author    = {Arthur Bucker and Luis Figueredo and Sami Haddadinl and Ashish Kapoor and Shuang Ma and Rogerio Bonatti},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981810},
  pages     = {978-984},
  title     = {Reshaping robot trajectories using natural language commands: A study of multi-modal data alignment using transformers},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal localizability criterion for positioning with
distance-deteriorated relative measurements. <em>IROS</em>, 947–953. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Position estimation in Multi-Robot Systems (MRS) relies on relative angle or distance measurements between the robots, which generally deteriorate as distances increase. Moreover, the localization accuracy is strongly influenced both by the quality of the raw measurements but also by the overall geometry of the network. In this paper, we design a cost function that accounts for these two issues and can be used to develop motion planning algorithms that optimize the localizability in MRS, i.e., the ability of individual robots to localize themselves accurately. This cost function is based on computing new Cramér Rao Lower Bounds characterizing the achievable positioning performance with range and angle measurements that deteriorate with increasing distances. We describe a gradient-based motion-planning algorithm for MRS deployment that can be implemented in a distributed manner, as well as a non-myopic strategy to escape local minima. Finally, we test the proposed methodology experimentally for range measurements obtained using ultra-wide band transceivers and illustrate the improvements resulting from leveraging the more accurate measurement model in the robot placement algorithms.},
  archive   = {C_IROS},
  author    = {Justin Cano and Gaël Pages and Eric Chaumette and Jerome Le Ny},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981718},
  pages     = {947-953},
  title     = {Optimal localizability criterion for positioning with distance-deteriorated relative measurements},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Level set-based camera pose estimation from multiple 2D/3D
ellipse-ellipsoid correspondences. <em>IROS</em>, 939–946. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose an object-based camera pose estimation from a single RGB image and a pre-built map of objects, represented with ellipsoidal models. We show that contrary to point correspondences, the definition of a cost function characterizing the projection of a 3D object onto a 2D object detection is not straightforward. We develop an ellipse-ellipse cost based on level sets sampling, demonstrate its nice properties for handling partially visible objects and compare its performance with other common metrics. Finally, we show that the use of a predictive uncertainty on the detected ellipses allows a fair weighting of the contribution of the correspondences which improves the computed pose. The code is released at gitlab.inria.fr/tangram/level-set-based-camera-pose-estimation.},
  archive   = {C_IROS},
  author    = {Matthieu Zins and Gilles Simon and Marie-Odile Berger},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981161},
  pages     = {939-946},
  title     = {Level set-based camera pose estimation from multiple 2D/3D ellipse-ellipsoid correspondences},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Probabilistic object maps for long-term robot localization.
<em>IROS</em>, 931–938. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots deployed in settings such as warehouses and parking lots must cope with frequent and substantial changes when localizing in their environments. While many previous localization and mapping algorithms have explored methods of identifying and focusing on long-term features to handle change in such environments, we propose a different approach - can a robot understand the distribution of movable objects and relate it to observations of such objects to reason about global localization? In this paper, we present probabilistic object maps (POMs), which represent the distributions of movable objects using pose-likelihood sample pairs derived from prior trajectories through the environment and use a Gaussian process classifier to generate the likelihood of an object at a query pose. We also introduce POM-Localization, which uses an observation model based on POMs to perform inference on a factor graph for globally consistent long-term localization. We present empirical results showing that POM-Localization is indeed effective at producing globally consistent localization estimates in challenging real-world environments and that POM-Localization improves trajectory estimates even when the POM is formed from partially incorrect data.},
  archive   = {C_IROS},
  author    = {Amanda Adkins and Taijing Chen and Joydeep Biswas},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981316},
  pages     = {931-938},
  title     = {Probabilistic object maps for long-term robot localization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast scan context matching for omnidirectional 3D scan.
<em>IROS</em>, 925–930. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous robots need to recognize the environment by identifying the scene. Scan context is one of global descriptors, and it encodes the three-dimensional scan data of the scene for the identification in a matrix form. Scan context is in a matrix form that is simple to store, but the matching of scan contexts can require computational effort because the descriptor is orientation-dependent. Because a scan context of an omnidirectional LiDAR scan becomes periodic in azimuth, this paper proposes to compute the scan context matching efficiently incorporating the cross-correlation with fast Fourier transform, and, hence, the method is named fast scan context matching. The effectiveness of the proposed method for computation time, accuracy, and robustness are reported in this paper. It is also shown that the method was also tested as a loop closure detector of a SLAM package as a practical application and that the proposed method outperformed the conventional scan context matching.},
  archive   = {C_IROS},
  author    = {Hikaru Kihara and Makoto Kumon and Kei Nakatsuma and Tomonari Furukawa},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981143},
  pages     = {925-930},
  title     = {Fast scan context matching for omnidirectional 3D scan},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust onboard localization in changing environments
exploiting text spotting. <em>IROS</em>, 917–924. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robust localization in a given map is a crucial component of most autonomous robots. In this paper, we address the problem of localizing in an indoor environment that changes and where prominent structures have no correspondence in the map built at a different point in time. To overcome the discrepancy between the map and the observed environment caused by such changes, we exploit human-readable localization cues to assist localization. These cues are readily available in most facilities and can be detected using RGB camera images by utilizing text spotting. We integrate these cues into a Monte Carlo localization framework using a particle filter that operates on 2D LiDAR scans and camera data. By this, we provide a robust localization solution for environments with structural changes and dynamics by humans walking. We evaluate our localization framework on multiple challenging indoor scenarios in an office environment. The experiments suggest that our approach is robust to structural changes and can run on an onboard computer. We release an open source implementation of our approach 1 1 https://github.com/PRBonn/tmcl, which uses off-the-shelf text spotting, written in C++ with a ROS wrapper.},
  archive   = {C_IROS},
  author    = {Nicky Zimmerman and Louis Wiesmann and Tiziano Guadagnino and Thomas Läbe and Jens Behley and Cyrill Stachniss},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981049},
  pages     = {917-924},
  title     = {Robust onboard localization in changing environments exploiting text spotting},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LiDAR-aided visual-inertial localization with semantic maps.
<em>IROS</em>, 910–916. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate and robust localization is an essential task for autonomous driving systems. In this paper, we propose a novel 3D LiDAR-aided visual-inertial localization method. Our method fully explores the complementarity of visual and LiDAR observations. On the one hand, the association between semantic features in images and a given semantic map provides constraints for the absolute pose. On the other hand, LiDAR odometry (LO) can provide an accurate and robust 6DOF relative pose. The Error State Kalman Filter (ESKF) framework is exploited to estimate the vehicle pose relative to the semantic map, which fuses the global constraints between the image and the semantic map, the relative pose from the LO, and the raw IMU data. The method achieves centimeter-level localization accuracy in a variety of challenging scenarios. We validate the robustness and accuracy of our method in real-world scenes over 50 km. The experimental results show that the proposed method is able to achieve an average lateral accuracy of 0.059 m and longitudinal accuracy of 0.158 m, which demonstrates the practicality of the proposed system in autonomous driving applications.},
  archive   = {C_IROS},
  author    = {Hao Li and Liangliang Pan and Ji Zhao},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982152},
  pages     = {910-916},
  title     = {LiDAR-aided visual-inertial localization with semantic maps},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluating human-like explanations for robot actions in
reinforcement learning scenarios. <em>IROS</em>, 894–901. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Explainable artificial intelligence is a research field that tries to provide more transparency for autonomous intelligent systems. Explainability has been used, particularly in reinforcement learning and robotic scenarios, to better understand the robot decision-making process. Previous work, however, has been widely focused on providing technical explanations that can be better understood by AI practitioners than non-expert end-users. In this work, we make use of human-like explanations built from the probability of success to complete the goal that an autonomous robot shows after performing an action. These explanations are intended to be understood by people who have no or very little experience with artificial intelligence methods. This paper presents a user trial to study whether these explanations that focus on the probability an action has of succeeding in its goal constitute a suitable explanation for non-expert end-users. The results obtained show that non-expert participants rate robot explanations that focus on the probability of success higher and with less variance than technical explanations generated from Q-values, and also favor counterfactual explanations over standalone explanations.},
  archive   = {C_IROS},
  author    = {Francisco Cruz and Charlotte Young and Richard Dazeley and Peter Vamplew},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981334},
  pages     = {894-901},
  title     = {Evaluating human-like explanations for robot actions in reinforcement learning scenarios},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Examining distance in UAV gesture perception. <em>IROS</em>,
879–885. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unmanned aerial vehicles (UAVs) are becoming more common, presenting the need for effective human-robot communication strategies that address the unique nature of unmanned aerial flight. Visual communication via drone flight paths, also called gestures, may prove to be an ideal method. However, the effectiveness of visual communication techniques is dependent on several factors including an observer&#39;s position relative to a UAV. Previous work has studied the maximum line-of-sight at which observers can identify a small UAV [1]. However, this work did not consider how changes in distance may affect an observer&#39;s ability to perceive the shape of a UAV&#39;s motion. In this study, we conduct a series of online surveys to evaluate how changes in line-of-sight distance and gesture size affect observers&#39; ability to identify and distinguish between UAV gestures. We first examine observers&#39; ability to accurately identify gestures when adjusting a gesture&#39;s size relative to the size of a UAV. We then measure how observers&#39; ability to identify gestures changes with respect to varying line-of-sight distances. Lastly, we consider how altering the size of a UAV gesture may improve an observer&#39;s ability to identify drone gestures from varying distances. Our results show that increasing the gesture size across varying UAV to gesture ratios did not have a significant effect on participant response accuracy. We found that between 17 m and 75 m from the observer, their ability to accurately identify a drone gesture was inversely proportional to the distance between the observer and the drone. Finally, we found that maintaining a gesture&#39;s apparent size improves participant response accuracy over changing line-of-sight distances.},
  archive   = {C_IROS},
  author    = {Karissa Jelonek and Paul Fletcher and Brittany Duncan and Carrick Detweiler},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981971},
  pages     = {879-885},
  title     = {Examining distance in UAV gesture perception},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An adaptive, affordable, humanlike arm hand system for deaf
and DeafBlind communication with the american sign language.
<em>IROS</em>, 871–878. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To communicate, the ~ 1.5 million Americans living with deafblindess use tactile American Sign Language (t-ASL). To provide Deafßilind (DB) individuals with a means of using their primary communication language without the use of an interpreter, we developed an assistive technology that promotes their autonomy. The TATUM (Tactile ASL Translational User Mechanism) anthropomorphic arm hand system leverages previous developments of a fingerspelling hand to sign more complex ASL words and phrases. The TATUM hand-wrist system is attached onto a 4 DOF robot arm and a human motion recognition and human to robot gesture transfer framework is used for signing recognition and replication. In particular, signing trajectories based on vision-based motion capture data from a sign demonstrator were used to control the robot&#39;s actuators. The performance of the system was evaluated through tactile based sign recognition performed by a blinded user and for its accuracy with novice, sighted users.},
  archive   = {C_IROS},
  author    = {Che-Ming Chang and Felipe Sanches and Geng Gao and Samantha Johnson and Minas Liarokapis},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982052},
  pages     = {871-878},
  title     = {An adaptive, affordable, humanlike arm hand system for deaf and DeafBlind communication with the american sign language},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Keeping humans in the loop: Teaching via feedback in
continuous action space environments. <em>IROS</em>, 863–870. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Interactive Reinforcement Learning (IntRL) allows human teachers to accelerate the learning process of Reinforcement Learning (RL) robots. However, IntRL has largely been limited to tasks with discrete-action spaces in which actions are relatively slow. This limits IntRL&#39;s application to more complicated and challenging robotic tasks, the very tasks that modern RL is particularly well-suited for. We seek to bridge this gap by presenting Continuous Action-space Interactive Reinforcement learning (CAIR): the first continuous action-space IntRL algorithm that is capable of using teacher feedback to out-perform state-of-the-art RL algorithms in those tasks. CAIR combines policies learned from the environment and the teacher into a single policy that proportionally weights the two policies based on their agreement. This allows a CAIR agent to learn a relatively stable policy despite potentially noisy or coarse teacher feedback. We validate our approach in two simulated robotics tasks with easy-to-design and - understand heuristic oracle teachers. Furthermore, we validate our approach in a human subjects study through Amazon Mechanical Turk and show CAIR out-performs the prior state-of-the-art in Interactive RL.},
  archive   = {C_IROS},
  author    = {Isaac Sheidlower and Allison Moore and Elaine Short},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982282},
  pages     = {863-870},
  title     = {Keeping humans in the loop: Teaching via feedback in continuous action space environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Holo-SpoK: Affordance-aware augmented reality control of
legged manipulators. <em>IROS</em>, 856–862. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although there is extensive research regarding legged manipulators, comparatively little focuses on their User Interfaces (UIs). Towards extending the state-of-art in this domain, in this work, we integrate a Boston Dynamics (BD) Spot® with a light-weight 7 DoF Kinova® robot arm and a Robotiq® 2F-85 gripper into a legged manipulator. Furthermore, we jointly control the robotic platform using an affordance-aware Augmented Reality (AR) Head-Mounted Display (HMD) UI developed for the Microsoft HoloLens 2. We named the combined platform Holo-SpoK. Moreover, we explain how this manipulator colocalises with the HoloLens 2 for its control through AR. In addition, we present the details of our algorithms for autonomously detecting grasp-ability affordances and for the refinement of the positions obtained via vision-based colocalisation. We validate the suitability of our proposed methods with multiple navigation and manipulation experiments. To the best of our knowledge, this is the first demonstration of an AR HMD UI for controlling legged manipulators.},
  archive   = {C_IROS},
  author    = {Rodrigo Chacón Quesada and Yiannis Demiris},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981989},
  pages     = {856-862},
  title     = {Holo-SpoK: Affordance-aware augmented reality control of legged manipulators},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Task decoupling in preference-based reinforcement learning
for personalized human-robot interaction. <em>IROS</em>, 848–855. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Intelligent robots designed to interact with hu-mans in the real world need to adapt to the preferences of different individuals. Preference-based reinforcement learning (RL) has shown great potential for teaching robots to learn personalized behaviors from interacting with humans with-out a meticulous, hand-crafted reward function, replaced by learning reward based on a human&#39;s preferences between two robot trajectories. However, poor feedback efficiency and poor exploration in the state and reward spaces make current preference-based RL algorithms perform poorly in complex interactive tasks. To improve the performance of preference-based RL, we incorporate prior knowledge of the task into preference-based RL. Specifically, we decouple the task from preference in human-robot interaction. We utilize a sketchy task reward derived from task priori to instruct robots to conduct more effective task exploration. Then a learned reward from preference-based RL is used to optimize the robot&#39;s policy to align with human preferences. In addition, these two parts are combined organically via reward shaping. The experimental results show that our method is a practical and effective solution for personalized human-robot interaction. Code is available at https://github.com/Wenminggong/PbRL_for_PHRI.},
  archive   = {C_IROS},
  author    = {Mingjiang Liu and Chunlin Chen},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981076},
  pages     = {848-855},
  title     = {Task decoupling in preference-based reinforcement learning for personalized human-robot interaction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient 2D LIDAR-based map updating for long-term
operations in dynamic environments. <em>IROS</em>, 832–839. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Long-time operations of autonomous vehicles and mobile robots in logistics and service applications are still a challenge. To avoid a continuous re-mapping, the map can be updated to obtain a consistent representation of the current environment. In this paper, we propose a novel LIDAR-based occupancy grid map updating algorithm for dynamic environments taking into account possible localisation and measurement errors. The proposed approach allows robust long-term operations as it can detect changes in the working area even in presence of moving elements. Results highlighting map quality and localisation performance, both in simulation and experiments, are reported.},
  archive   = {C_IROS},
  author    = {Elisa Stefanini and Enrico Ciancolini and Alessandro Settimi and Lucia Pallottino},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982047},
  pages     = {832-839},
  title     = {Efficient 2D LIDAR-based map updating for long-term operations in dynamic environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 360ST-mapping: An online semantics-guided topological
mapping module for omnidirectional visual SLAM. <em>IROS</em>, 802–807.
(<a href="https://doi.org/10.1109/IROS47612.2022.9982142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As an abstract representation of the environment structure, a topological map has advantageous properties for path-planning and navigation. Here we proposed an online topological mapping method, 360ST-Mapping, using omnidirectional vision. The 360° field-of-view allows the agent to obtain consistent observation and incrementally extract topological environment information. Moreover, we leverage semantic infor-mation to guide topological place recognition, further improving performance. The topological map possessing semantic infor-mation has the potential to support semantics-related advanced tasks. After integrating the topological mapping module into the omnidirectional visual SLAM system, we conducted extensive experiments in several large-scale indoor scenes and validated the method&#39;s effectiveness.},
  archive   = {C_IROS},
  author    = {Hongji Liu and Huajian Huang and Sai-Kit Yeung and Ming Liu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982142},
  pages     = {802-807},
  title     = {360ST-mapping: An online semantics-guided topological mapping module for omnidirectional visual SLAM},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ranging-aided ground robot navigation using UWB nodes at
unknown locations. <em>IROS</em>, 786–793. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ranging information from ultra-wideband (UWB) ranging radios can be used to improve estimated navigation accuracy of a ground robot with other on-board sensors. However, all ranging-aided navigation methods demand the locations of ranging nodes to be known, which is not suitable for time-pressed situations, dynamic cluttered environments, or collaborative navigation applications. This paper describes a new ranging-aided navigation approach that does not require the locations of ranging radios. Our approach formulates relative pose constraints using ranging readings. The formulation is based on geometric relationships between each stationary ranging node and two ranging antennas on the moving robot across time. Our experiments show that estimated navigation accuracy of the ground robot is substantially enhanced with ranging information using our approach under a variety of scenarios, when ranging nodes are placed at unknown locations. We analyze and compare our performance with a traditional ranging-aided method, which requires mapping the positions of ranging nodes. We also demonstrate the applicability of our approach for collaborative navigation in large-scale unknown environments, by using ranging information from one mobile robot to improve navigation estimation of the other robot. This application does not require the installation of ranging nodes at fixed locations.},
  archive   = {C_IROS},
  author    = {Abhinav Rajvanshi and Han-Pang Chiu and Alex Krasner and Mikhail Sizintsev and Glenn Murray and Supun Samarasekera},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981186},
  pages     = {786-793},
  title     = {Ranging-aided ground robot navigation using UWB nodes at unknown locations},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-agent relative pose estimation with UWB and
constrained communications. <em>IROS</em>, 778–785. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inter-agent relative localization is critical for any multi-robot system operating in the absence of external positioning infrastructure or prior environmental knowledge. We propose a novel inter-agent relative 2D pose estimation system where each participating agent is equipped with several ultra-wideband (UWB) ranging tags. Prior work typically supplements noisy UWB range measurements with additional continuously transmitted data, such as odometry, making these approaches scale poorly with increased swarm size or decreased communication throughput. This approach addresses these concerns by using only locally collected UWB measurements with no additionally transmitted data. By modeling observed ranging biases and systematic antenna obstructions in our proposed optimization solution, our experimental results demonstrate an improved mean position error (while remaining competitive in other metrics) over a similar state-of-the-art approach that additionally relies on continuously transmitted odometry.},
  archive   = {C_IROS},
  author    = {Andrew Fishberg and Jonathan P. How},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982005},
  pages     = {778-785},
  title     = {Multi-agent relative pose estimation with UWB and constrained communications},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NARF22: Neural articulated radiance fields for
configuration-aware rendering. <em>IROS</em>, 770–777. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Articulated objects pose a unique challenge for robotic perception and manipulation. Their increased number of degrees-of-freedom makes tasks such as localization computationally difficult, while also making the process of realworld dataset collection unscalable. With the aim of addressing these scalability issues, we propose Neural Articulated Radiance Fields (NARF22), a pipeline which uses a fully-differentiable, configuration-parameterized Neural Radiance Field (NeRF) as a means of providing high quality renderings of articulated objects. NARF22 requires no explicit knowledge of the object structure at inference time. We propose a two-stage partsbased training mechanism which allows the object rendering models to generalize well across the configuration space even if the underlying training data has as few as one configuration represented. We demonstrate the efficacy of NARF22 by training configurable renderers on a real-world articulated tool dataset collected via a Fetch mobile manipulation robot. We show the applicability of the model to gradient-based inference methods through a configuration estimation and 6 degree-of-freedom pose refinement task.},
  archive   = {C_IROS},
  author    = {Stanley Lewis and Jana Pavlasek and Odest Chadwicke Jenkins},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982194},
  pages     = {770-777},
  title     = {NARF22: Neural articulated radiance fields for configuration-aware rendering},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning 6-DoF task-oriented grasp detection via implicit
estimation and visual affordance. <em>IROS</em>, 762–769. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Currently, task-oriented grasp detection approaches are mostly based on pixel-level affordance detection and semantic segmentation. These pixel-level approaches heavily rely on the accuracy of a 2D affordance mask, and the generated grasp candidates are restricted to a small workspace. To mitigate these limitations, we firstly construct a novel affordance-based grasp dataset and propose a 6-DoF task-oriented grasp detection framework, which takes the observed object point cloud as input and predicts diverse 6-DoF grasp poses for different tasks. Specifically, our implicit estimation network and visual affordance network in this framework could directly predict coarse grasp candidates, and corresponding 3D affordance heatmap for each potential task, respectively. Furthermore, the grasping scores from coarse grasps are combined with heatmap values to generate more accurate and finer candidates. Our proposed framework shows significant improvements compared to baselines for existing and novel objects on our simulation dataset. Although our framework is trained based on the simulated objects and environment, the final generated grasp candidates can be accurately and stably executed in the real robot experiments when the object is randomly placed on a support surface.},
  archive   = {C_IROS},
  author    = {Wenkai Chen and Hongzhuo Liang and Zhaopeng Chen and Fuchun Sun and Jianwei Zhang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981900},
  pages     = {762-769},
  title     = {Learning 6-DoF task-oriented grasp detection via implicit estimation and visual affordance},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Augment-connect-explore: A paradigm for visual action
planning with data scarcity. <em>IROS</em>, 754–761. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual action planning particularly excels in applications where the state of the system cannot be computed explicitly, such as manipulation of deformable objects, as it enables planning directly from raw images. Even though the field has been significantly accelerated by deep learning techniques, a crucial requirement for their success is the availability of a large amount of data. In this work, we propose the Augment-Connect-Explore (ACE) paradigm to enable visual action planning in cases of data scarcity. We build upon the Latent Space Roadmap (LSR) framework which performs planning with a graph built in a low dimensional latent space. In particular, ACE is used to i) Augment the available training dataset by autonomously creating new pairs of datapoints, ii) create new unobserved Connections among representations of states in the latent graph, and iii) Explore new regions of the latent space in a targeted manner. We validate the proposed approach on both simulated box stacking and real-world folding task showing the applicability for rigid and deformable object manipulation tasks, respectively.},
  archive   = {C_IROS},
  author    = {Martina Lippi and Michael C. Welle and Petra Poklukar and Alessandro Marino and Danica Kragic},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982199},
  pages     = {754-761},
  title     = {Augment-connect-explore: A paradigm for visual action planning with data scarcity},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Investigation of factorized optical flows as mid-level
representations. <em>IROS</em>, 746–753. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce a new concept of incorporating factorized flow maps as mid-level representations, for bridging the perception and the control modules in modular learning based robotic frameworks. To investigate the advantages of factorized flow maps and examine their interplay with the other types of mid-level representations, we further develop a configurable framework, along with four different environments that contain both static and dynamic objects, for analyzing the impacts of factorized optical flow maps on the performance of deep reinforcement learning agents. Based on this framework, we report our experimental results on various scenarios, and offer a set of analyses to justify our hypothesis. Finally, we validate flow factorization in real world scenarios.},
  archive   = {C_IROS},
  author    = {Hsuan-Kung Yang and Tsu-Ching Hsiao and Ting-Hsuan Liao and Hsu-Shen Liu and Li-Yuan Tsao and Tzu-Wen Wang and Shan-Ya Yang and Yu-Wen Chen and Huang-Ru Liao and Chun-Yi Lee},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981638},
  pages     = {746-753},
  title     = {Investigation of factorized optical flows as mid-level representations},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Domain invariant siamese attention mask for small object
change detection via everyday indoor robot navigation. <em>IROS</em>,
739–745. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The problem of image change detection via every-day indoor robot navigation is explored from a novel perspective of the self-attention technique. Detecting semantically non-distinctive and visually small changes remains a key challenge in the robotics community. Intuitively, these small non-distinctive changes may be better handled by the recent paradigm of the attention mechanism, which is the basic idea of this work. However, existing self-attention models require significant retraining cost per domain, so it is not directly applicable to robotics applications. We propose a new self-attention technique with an ability of unsupervised on-the-fly domain adaptation, which introduces an attention mask into the intermediate layer of an image change detection model, without modifying the input and output layers of the model. Experiments, in which an indoor robot aims to detect visually small changes in everyday navigation, demonstrate that our attention technique significantly boosts the state-of-the-art image change detection model. Our datset is available at https://github.com/KojiTakeda00/Small_object_change_detection},
  archive   = {C_IROS},
  author    = {Koji Takeda and Kanji Tanaka and Yoshimasa Nakamura},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982196},
  pages     = {739-745},
  title     = {Domain invariant siamese attention mask for small object change detection via everyday indoor robot navigation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards two-view 6D object pose estimation: A comparative
study on fusion strategy. <em>IROS</em>, 723–730. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current RGB-based 6D object pose estimation methods have achieved noticeable performance on datasets and real world applications. However, predicting 6D pose from single 2D image features is susceptible to disturbance from changing of environment and textureless or resemblant object surfaces. Hence, RGB-based methods generally achieve less competitive results than RGBD-based methods, which deploy both image features and 3D structure features. To narrow down this performance gap, this paper proposes a framework for 6D object pose estimation that learns implicit 3D information from 2 RGB images. Combining the learned 3D information and 2D image features, we establish more stable correspondence between the scene and the object models. To seek for the methods best utilizing 3D information from RGB inputs, we conduct an investigation on three different approaches, including Early-Fusion, Mid-Fusion, and Late-Fusion. We ascertain the Mid-Fusion approach is the best approach to restore the most precise 3D keypoints useful for object pose estimation. The experiments show that our method outperforms state-of-the-art RGB-based methods, and achieves comparable results with RGBD-based methods.},
  archive   = {C_IROS},
  author    = {Jun Wu and Lilu Liu and Yue Wang and Rong Xiong},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981630},
  pages     = {723-730},
  title     = {Towards two-view 6D object pose estimation: A comparative study on fusion strategy},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). J-RR: Joint monocular depth estimation and semantic edge
detection exploiting reciprocal relations. <em>IROS</em>, 715–722. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Depth estimation and semantic edge detection are two key tasks in computer vision, which have made great progress. To date, how to associatively predict the depth and the semantic edge is rarely explored. In this work, we first propose a flexible two-branch framework that can make the two tasks take advantage of each other, achieving a win-win situation. Specifically, for the semantic edge detection branch, an Enhanced Edge Weighting strategy (EEW) is designed, which learns weight information from the by-product of depth branch, depth edge, to enhance edge perception in features. Meanwhile, we make depth estimation benefit from semantic edge detection through introducing Depth Edge Semantic Classification module (DESC). Furthermore, a double reconstruction (D-reconstruction) approach is presented, together with semantic edge-guided disparity smoothing loss to mitigate the ambiguities of the self-supervised manner for depth estimation. Experiments on the Cityscapes dataset demonstrate that our framework outperforms the state-of-the-art method in depth estimation along with a significant improvement in semantic edge detection.},
  archive   = {C_IROS},
  author    = {Deming Wu and Dongchen Zhu and Guanghui Zhang and Wenjun Shi and Xiaolin Zhang and Jiamao Li},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981343},
  pages     = {715-722},
  title     = {J-RR: Joint monocular depth estimation and semantic edge detection exploiting reciprocal relations},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A camera-based deep-learning solution for visual attention
zone recognition in maritime navigational operations. <em>IROS</em>,
699–706. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The visual attention of navigators is imperative to understand the logic of navigation as well as the surveillance of navigators&#39; status and operation. Current studies are implemented with the help of wearable eye-tracker glasses; yet, the high expenditure demanded by such equipment and service and its limitations on usability have impeded related research from further development. In this letter, the authors propose a framework, which is the first attempt in the maritime domain, to provide a camera-based deep-learning (CaBDeeL) visual attention recognition solution that outperforms the intrusive eye tracker regarding its shortcomings. A wide-angle camera is configured in front of the navigator in the advanced ship-bridge simulator in a way that visual attention reflected by their facial and head movements is captured in the front view. A pair of eye-tracker glasses is used to classify the captured visual attention images, which then form the primary database. During the process of classifying camera-captured images, a convolutional neural network (CNN) is built as an automatic classifier. The CNN is applied to two scenarios, and it shows an overall 95\% accuracy.},
  archive   = {C_IROS},
  author    = {Baiheng Wu and Peihua Han and Motoyasu Kanazawa and Hans Petter Hildre and Luman Zhao and Houxiang Zhang and Guoyuan Li},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981783},
  pages     = {699-706},
  title     = {A camera-based deep-learning solution for visual attention zone recognition in maritime navigational operations},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effects of multiple avatar images presented consecutively
with temporal delays on self-body recognition. <em>IROS</em>, 684–690.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-body awareness refers to the recognition of one&#39;s body as one&#39;s own and consists of two senses: “sense of body ownership” and “sense of agency.” In telexistence/telepresence robot operation, time delays in the robot&#39;s motion degrade self-body awareness of the robot body. We investigated how self-body recognition can be affected in a telexistence robot operation in a VR space when the robot is presented with a real robot arm that simulates a real robot with a delay and a virtual robot arm, or several virtual robot arms, with a delay less than that of the real robot. These experimental conditions include a ‘Predictive Display,’ which is well known as a time delay countermeasure. The results suggest that virtual robot arms presented consecutively with less delay than a real robot arm do not induce a sense of body ownership to the real robot arm, but they enhance the sense of agency to the real robot arm, and that sense of agency is stronger when the task requires precision.},
  archive   = {C_IROS},
  author    = {Eimei Oyama and Yuya Ioka and Arvin Agah and Hiroyuki Okada and Sotaro Shimada},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981048},
  pages     = {684-690},
  title     = {Effects of multiple avatar images presented consecutively with temporal delays on self-body recognition},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling human response to robot errors for timely error
detection. <em>IROS</em>, 676–683. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In human-robot collaboration, robot errors are inevitable—damaging user trust, willingness to work together, and task performance. Prior work has shown that people naturally respond to robot errors socially and that in social interactions it is possible to use human responses to detect errors. However, there is little exploration in the domain of nonsocial, physical human-robot collaboration such as assembly and tool retrieval. In this work, we investigate how people&#39;s organic, social responses to robot errors may be used to enable timely automatic detection of errors in physical human-robot interactions. We conducted a data collection study to obtain facial responses to train a real-time detection algorithm and a case study to explore the generalizability of our method with different task settings and errors. Our results show that natural social responses are effective signals for timely detection and localization of robot errors even in nonsocial contexts and that our method is robust across a variety of task contexts, robot errors, and user responses. This work contributes to robust error detection without detailed task specifications.},
  archive   = {C_IROS},
  author    = {Maia Stiber and Russell Taylor and Chien-Ming Huang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981726},
  pages     = {676-683},
  title     = {Modeling human response to robot errors for timely error detection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EMG-based hybrid impedance-force control for human-robot
collaboration on ultrasound imaging. <em>IROS</em>, 670–675. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ultrasound (US) imaging is a common but physically demanding task in the medical field, and sonographers may need to put in considerable physical effort for producing high-quality US images. During physical human-robot interaction on US imaging, robot compliance is a critical feature that can ensure human user safety while automatic force regulation ability can help to improve task performance. However, higher robot compliance may mean lower force regulation accuracy, and vice versa. Especially, the contact/non-contact status transition can largely affect the control system stability. In this paper, a novel electromyography (EMG)-based hybrid impedance-force control system is developed for US imaging task. The proposed control system incorporates the robot compliance and force regulation ability via a hybrid controller while the EMG channel enables the user to online modulate the trade-off between the two features as necessary. Two experiments are conducted to examine the hybrid controller and show the necessity of involving an EMG-based modulator. A proof-of-concept study on US imaging is performed with implementing the proposed EMG-based control system, and the effectiveness is demonstrated. The proposed control system is promising to ensure robot&#39;s stability and patient&#39;s safety, thus obtain high-quality US images, while monitoring and reducing sonographer&#39;s fatigue. Furthermore, it can be easily adapted to other physically demanding tasks in the field of medicine.},
  archive   = {C_IROS},
  author    = {Teng Li and Hongjun Xing and Hamid D. Taghirad and Mahdi Tavakoli},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981615},
  pages     = {670-675},
  title     = {EMG-based hybrid impedance-force control for human-robot collaboration on ultrasound imaging},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robot trajectory adaptation to optimise the trade-off
between human cognitive ergonomics and workplace productivity in
collaborative tasks. <em>IROS</em>, 663–669. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In hybrid industrial environments, workers&#39; comfort and positive perception of safety are essential requirements for successful acceptance and usage of collaborative robots. This paper proposes a novel human-robot interaction framework in which the robot behaviour is adapted online according to the operator&#39;s cognitive workload and stress. The method exploits the generation of B-spline trajectories in the joint space and formulation of a multi-objective optimisation problem to online adjust the total execution time and smoothness of the robot trajectories. The former ensures human efficiency and productivity of the workplace, while the latter contributes to safeguarding the user&#39;s comfort and cognitive ergonomics. The performance of the proposed framework was evaluated in a typical industrial task. Results demonstrated its capability to enhance the productivity of the human-robot dyad while mitigating the cognitive workload induced in the worker.},
  archive   = {C_IROS},
  author    = {Marta Lagomarsino and Marta Lorenzini and Elena De Momi and Arash Ajoudani},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981424},
  pages     = {663-669},
  title     = {Robot trajectory adaptation to optimise the trade-off between human cognitive ergonomics and workplace productivity in collaborative tasks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). You are in my way: Non-verbal social cues for legible robot
navigation behaviors. <em>IROS</em>, 657–662. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {People and robots may need to cross each other in narrow spaces when they are sharing environments. It is expected that autonomous robots will behave in these contexts safely but also show social behaviors. Thereby, developing an acceptable behavior for autonomous robots in the area mentioned above is a foreseeable problem for the Human-Robot Interaction (HRI) field. Our current work focuses on integrating legible non-verbal behaviors into the robot&#39;s social navigation to make nearby humans aware of its intended trajectory. Results from a within-subjects study involving 33 participants show that deictic gestures as navigational cues for humanoid robots result in fewer navigation conflicts than the use of a simulated gaze. Additionally, an increase in the perceived anthropomorphism is found when the robot uses the deictic gesture as a cue. These findings show the importance of social behaviors for people avoidance and suggest a paradigm of such behaviors in future humanoid robotic applications.},
  archive   = {C_IROS},
  author    = {Georgios Angelopoulos and Alessandra Rossi and Claudia Di Napoli and Silvia Rossi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981754},
  pages     = {657-662},
  title     = {You are in my way: Non-verbal social cues for legible robot navigation behaviors},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). To ask for help or not to ask: A predictive approach to
human-in-the-loop motion planning for robot manipulation tasks.
<em>IROS</em>, 649–656. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a predictive system for non-prehensile, physics-based motion planning in clutter with a human-in-the-loop. Recent shared-autonomous systems present motion planning performance improvements when high-level reasoning is provided by a human. Humans are usually good at quickly identifying high-level actions in high-dimensional spaces, and robots are good at converting high-level actions into valid robot trajectories. In this paper, we present a novel framework that permits a single human operator to effectively guide a fleet of robots in a virtual warehouse. The robots are tackling the problem of Reaching Through Clutter (RTC), where they are reaching onto cluttered shelves to grasp a goal object while pushing other obstacles out of the way. We exploit information from the motion planning algorithm to predict which robot requires human help the most and assign that robot to the human. With twenty virtual robots and a single human-operator, the results suggest that this approach improves the system&#39;s overall performance compared to a baseline with no predictions. The results also show that there is a cap on how many robots can effectively be guided simultaneously by a single human operator.},
  archive   = {C_IROS},
  author    = {Rafael Papallas and Mehmet R. Dogar},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981679},
  pages     = {649-656},
  title     = {To ask for help or not to ask: A predictive approach to human-in-the-loop motion planning for robot manipulation tasks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SPARCS: Structuring physically assistive robotics for
caregiving with stakeholders-in-the-loop. <em>IROS</em>, 641–648. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing work in physical robot caregiving is limited in its ability to provide long-term assistance. This is majorly due to (i) lack of well-defined problems, (ii) diversity of tasks, and (iii) limited access to stakeholders from the caregiving community. We propose Structuring Physically Assistive Robotics for Caregiving with Stakeholders-in-the-loop (SPARCS) to address these challenges. SPARCS is a framework for physical robot caregiving comprising (i) Building Blocks, models that define physical robot caregiving scenarios, (ii) Structured Workflows, hierarchical workflows that enable us to answer the Whats and Hows of physical robot caregiving, and (iii) SPARCS-box, a web-based platform to facilitate dialogue between all stakeholders. We collect clinical data for six care recipients with varying disabilities and demonstrate the use of SPARCS in designing well-defined caregiving scenarios and identifying their care requirements. All the data and workflows are available on SPARCS-box. We demonstrate the utility of SPARCS in building a robot-assisted feeding system for one of the care recipients. We also perform experiments to show the adaptability of this system to different caregiving scenarios. Finally, we identify open challenges in physical robot caregiving by consulting care recipients and caregivers. Supplementary material can be found at emprise.cs.cornell.edu/sparcs.},
  archive   = {C_IROS},
  author    = {Rishabh Madan and Rajat Kumar Jenamani and Vy Thuy Nguyen and Ahmed Moustafa and Xuefeng Hu and Katherine Dimitropoulou and Tapomayukh Bhattacharjee},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981936},
  pages     = {641-648},
  title     = {SPARCS: Structuring physically assistive robotics for caregiving with stakeholders-in-the-loop},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feeling the pressure: The influence of vibrotactile patterns
on feedback perception. <em>IROS</em>, 634–640. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tactile feedback is necessary for closing the sen-sorimotor loop in prosthetic and tele-operable control, which would allow for more precise manipulation and increased acceptance of use of such devices. Pressure stimuli are commonly presented to users in haptic devices through a sensory substitution to vibration. The precise nature of this substitution affects pressure sensitivity, as well as the comfort and intuitiveness of the device for the user. This study determines the effects of different vibrational encodings for pressure on user-preference and performance in a 4-alternative absolute identification task. 4 different encoding patterns for pressure were examined: short pulse and long pulse amplitude modulation along with sine and square wave frequency modulation. Of the methods examined, frequency modulation methods led to the best discrimination of stimuli (p $p$ = 0.098). This suggests that prostheses or teleoperated devices utilising vibrotactile feedback may benefit from implementing a discrete frequency-based sinusoidal pattern to indicate changes in grip force.},
  archive   = {C_IROS},
  author    = {Alexander Smith and Benjamin Ward-Cherrier and Appolinaire Etoundi and Martin J. Pearson},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981594},
  pages     = {634-640},
  title     = {Feeling the pressure: The influence of vibrotactile patterns on feedback perception},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Haptic feedback relocation from the fingertips to the wrist
for two-finger manipulation in virtual reality. <em>IROS</em>, 628–633.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Relocation of haptic feedback from the fingertips to the wrist has been considered as a way to enable haptic interaction with mixed reality virtual environments while leaving the fingers free for other tasks. We present a pair of wrist-worn tactile haptic devices and a virtual environment to study how various mappings between fingers and tactors affect task performance. The haptic feedback rendered to the wrist reflects the interaction forces occurring between a virtual object and virtual avatars controlled by the index finger and thumb. We performed a user study comparing four different finger-to-tactor haptic feedback mappings and one no-feedback condition as a control. We evaluated users&#39; ability to perform a simple pick-and-place task via the metrics of task completion time, path length of the fingers and virtual cube, and magnitudes of normal and shear forces at the fingertips. We found that multiple mappings were effective, and there was a greater impact when visual cues were limited. We discuss the limitations of our approach and describe next steps toward multi-degree-of-freedom haptic rendering for wrist-worn devices to improve task performance in virtual environments.},
  archive   = {C_IROS},
  author    = {Jasmin E. Palmer and Mine Sarac and Aaron A. Garza and Allison M. Okamura},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981392},
  pages     = {628-633},
  title     = {Haptic feedback relocation from the fingertips to the wrist for two-finger manipulation in virtual reality},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Perception of mechanical properties via wrist haptics:
Effects of feedback congruence. <em>IROS</em>, 620–627. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite non-co-location, haptic stimulation at the wrist can potentially provide feedback regarding interactions at the fingertips without encumbering the user&#39;s hand. Here we investigate how two types of skin deformation at the wrist (normal and shear) relate to the perception of the mechanical properties of virtual objects. We hypothesized that a congruent mapping (i.e. when the most relevant interaction forces during a virtual interaction spatially match the haptic feedback at the wrist) would result in better perception than other mappings. We performed an experiment where haptic devices at the wrist rendered either normal or shear feedback during manipulation of virtual objects with varying stiffness, mass, or friction properties. Perception of mechanical properties was more accurate with congruent skin stimulation than noncongruent. In addition, discrimination performance and subjective reports were positively influenced by congruence. This study demonstrates that users can perceive mechanical properties via haptic feedback provided at the wrist with a consistent mapping between haptic feedback and interaction forces at the fingertips, regardless of congruence.},
  archive   = {C_IROS},
  author    = {Mine Sarac and Massimiliano Di Luca and Allison M. Okamura},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982079},
  pages     = {620-627},
  title     = {Perception of mechanical properties via wrist haptics: Effects of feedback congruence},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cutaneous feedback interface for teleoperated in-hand
manipulation. <em>IROS</em>, 605–611. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In-hand pivoting is one of the important manipulation skills that leverage robot grippers&#39; extrinsic dexterity to perform repositioning tasks to compensate for environmental uncertainties and imprecise motion execution. Although many researchers have been trying to solve pivoting problems using mathematical modeling or learning-based approaches, the problems remain as open challenges. On the other hand, humans perform in-hand manipulation with remarkable precision and speed. Hence, the solution could be provided by making full use of this intrinsic human skill through dexterous teleoperation. For dexterous teleoperation to be successful, interfaces that enhance and complement haptic feedback are of great necessity. In this paper, we propose a cutaneous feedback interface that complements the somatosensory information humans rely on when performing dexterous skills. The interface is designed based on five-bar link mechanisms and provides two contact points in the index finger and thumb for cutaneous feedback. By integrating the interface with a commercially available haptic device, the system can display information such as grasping force, shear force, friction, and grasped object&#39;s pose. Passive pivoting tasks inside a numerical simulator Isaac Sim is conducted to evaluate the effect of the proposed cutaneous feedback interface.},
  archive   = {C_IROS},
  author    = {Yaonan Zhu and Jacinto Colan and Tadayoshi Aoyama and Yasuhisa Hasegawa},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982247},
  pages     = {605-611},
  title     = {Cutaneous feedback interface for teleoperated in-hand manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EMG-based feedback modulation for increased transparency in
teleoperation. <em>IROS</em>, 599–604. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In interacting with stiff environments through teleoperated systems, time delays cause a mismatch between haptic feedback and the expected feedback by the operator. This mismatch causes artefacts in the feedback, which decrease transparency, but so does filtering these artefacts. Through modelling of operator stiffness and the expected feedback force with EMG, the artifacts can be selectively filtered without loss of transparency. We developed several feedback modulation techniques to bring the feedback force closer to the expected force: 1) the average between the modelled operator force and the feedback force, 2) a low pass filter and 3) a scaling modulation. To control for overdamping, a transparency check is included. We show that the averaging approach yields significantly better contacts than unmodulated feedback. None of the modulation algorithms differ significantly from the unmodulated feedback in transparency.},
  archive   = {C_IROS},
  author    = {Luc Schoot Uiterkamp and Francesco Porcini and Gwenn Englebienne and Antonio Frisoli and Douwe Dresscher},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981162},
  pages     = {599-604},
  title     = {EMG-based feedback modulation for increased transparency in teleoperation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A large-area wearable soft haptic device using stacked
pneumatic pouch actuation. <em>IROS</em>, 591–598. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While haptics research has traditionally focused on the fingertips and hands, other locations on the body provide large areas of skin that could be utilized to relay large-area haptic sensations. Researchers have thus developed wearable devices that use distributed vibrotactile actuators and distributed pneumatic force displays, but these methods have limitations. In prior work, we presented a novel actuation technique involving stacking pneumatic pouches and evaluated the actuator output. In this work, we developed a wearable haptic device using this actuation technique and evaluated how the actuator output is perceived. We conducted a user study with 20 participants to evaluate users&#39; perception thresholds, ability to localize, and ability to detect differences in contact area and compare their perception using the stacked pneumatic pouch actuation to traditional single-layer pouch actuation. We also used our device with stacked pneumatic actuation in a demonstration of a haptic hug that replicates the dynamics, pressure profile, and mapping to the human back, showcasing how this actuation technique can be used to create novel haptic stimuli.},
  archive   = {C_IROS},
  author    = {Cara M. Nunez and Brian H. Do and Andrew K. Low and Laura H. Blumenschein and Katsu Yamane and Allison M. Okamura},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981919},
  pages     = {591-598},
  title     = {A large-area wearable soft haptic device using stacked pneumatic pouch actuation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A soft robotic haptic feedback glove for colonoscopy
procedures. <em>IROS</em>, 583–590. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a proof-of-concept soft robotic glove that provides haptic feedback to the surgeon&#39;s hand during interventional endoscopy procedures, specifically colonoscopy. The glove is connected to a force sensing soft robotic sleeve that is mounted onto a colonoscope. The glove consists of pneumatic actuators that inflate in proportion to the incident forces on the soft robotic sleeve. Thus, the glove is capable of alerting the surgeon of potentially dangerous forces exerted on the colon wall by the colonoscope during the navigation. The proposed glove is adaptable to a variety of hand sizes. It features modular actuators that facilitate convenient and rapid assembly and attachment before the procedure and removal afterward. The glove is calibrated to respond to incident forces on the soft robotic sleeve ranging from 0–3 N. The glove&#39;s actuators are able to reach an internal pressure of 53 kPa and exert forces up to 20 N, thereby relaying and amplifying the force exerted by the colonoscope on the colon to the surgeon&#39;s hand.},
  archive   = {C_IROS},
  author    = {Arincheyan Gerald and Rukaiya Batliwala and Jonathan Ye and Patra Hsu and Hiroyuki Aihara and Sheila Russo},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981652},
  pages     = {583-590},
  title     = {A soft robotic haptic feedback glove for colonoscopy procedures},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CVFNet: Real-time 3D object detection by learning cross view
features. <em>IROS</em>, 568–574. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years 3D object detection from LiDAR point clouds has made great progress thanks to the development of deep learning technologies. Although voxel or point based methods are popular in 3D object detection, they usually involve time-consuming operations such as 3D convolutions on voxels or ball query among points, making the resulting network inappropriate for time critical applications. On the other hand, 2D view-based methods feature high computing efficiency while usually obtaining inferior performance than the voxel or point based methods. In this work, we present a real-time view-based single stage 3D object detector, namely CVFNet to fulfill this task. To strengthen the cross-view feature learning under the condition of demanding efficiency, our framework extracts the features of different views and fuses them in an efficient progressive way. We first propose a novel Point-Range feature fusion module that deeply integrates point and range view features in multiple stages. Then, a special Slice Pillar is designed to well maintain the 3D geometry when transforming the obtained deep point-view features into bird&#39;s eye view. To better balance the ratio of samples, a sparse pillar detection head is presented to focus the detection on the nonempty grids. We conduct experiments on the popular KITTI and NuScenes benchmark, and state-of-the-art performances are achieved in terms of both accuracy and speed.},
  archive   = {C_IROS},
  author    = {Jiaqi Gu and Zhiyu Xiang and Pan Zhao and Tingming Bai and Lingxuan Wang and Xijun Zhao and Zhiyuan Zhang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981087},
  pages     = {568-574},
  title     = {CVFNet: Real-time 3D object detection by learning cross view features},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DeepFusion: A robust and modular 3D object detector for
lidars, cameras and radars. <em>IROS</em>, 560–567. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose DeepFusion, a modular multi-modal architecture to fuse lidars, cameras and radars in different combinations for 3D object detection. Specialized feature extractors take advantage of each modality and can be exchanged easily, making the approach simple and flexible. Extracted features are transformed into bird&#39;s-eye-view as a common representation for fusion. Spatial and semantic alignment is performed prior to fusing modalities in the feature space. Finally, a detection head exploits rich multi-modal features for improved 3D detection performance. Experimental results for lidar-camera, lidar-camera-radar and camera-radar fusion show the flexibility and effectiveness of our fusion approach. In the process, we study the largely unexplored task of faraway car detection up to 225 meters, showing the benefits of our lidar-camera fusion. Furthermore, we investigate the required density of lidar points for 3D object detection and illustrate implications at the example of robustness against adverse weather conditions. Moreover, ablation studies on our camera-radar fusion highlight the importance of accurate depth estimation.},
  archive   = {C_IROS},
  author    = {Florian Drews and Di Feng and Florian Faion and Lars Rosenbaum and Michael Ulrich and Claudius Gläser},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981778},
  pages     = {560-567},
  title     = {DeepFusion: A robust and modular 3D object detector for lidars, cameras and radars},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fully convolutional transformer with local-global attention.
<em>IROS</em>, 552–559. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In an attempt to imitate the success of transformers in the field of natural language processing into computer vision tasks, vision transformers (ViTs) have recently gained attention. Performance breakthroughs have been achieved in coarse-grained tasks like classification. However, dense prediction tasks, such as detection, segmentation, and depth estimation, require additional modifications and have been tackled only in an ad-hoc manner, by replacing the convolutional neural network encoder backbone of an existing architecture with a ViT. This study proposes a fully convolutional transformer that can perform both coarse and dense prediction tasks. The proposed architecture is, to the best of our knowledge, the first architecture composed of attention layers, even in the decoder part of the network. This is because our newly proposed local-global attention (LGA) can flexibly perform both downsampling and upsampling of spatial features, which are key operations required for dense prediction. Against existing ViTs on classification tasks, our architecture shows a reasonable trade-off between performance and efficiency. In the depth estimation task, our architecture achieves performance comparable to that of state-of-the-art transformer-based methods.},
  archive   = {C_IROS},
  author    = {Sihaeng Lee and Eojindl Yi and Janghyeon Lee and Jinsu Yoo and Honglak Lee and Seung Hwan Kim},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981339},
  pages     = {552-559},
  title     = {Fully convolutional transformer with local-global attention},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RPG: Learning recursive point cloud generation.
<em>IROS</em>, 544–551. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we propose a novel point cloud generator that is able to reconstruct and generate 3D point clouds composed of semantic parts. Given a latent representation of the target 3D model, the generation starts from a single point and gets expanded recursively to produce the high-resolution point cloud via a sequence of point expansion stages. During the recursive procedure of generation, we not only obtain the coarse-to-fine point clouds for the target 3D model from every expansion stage, but also unsupervisedly discover the semantic segmentation of the target model according to the hierarchical/parent-child relation between the points across expansion stages. Moreover, the expansion modules and other elements used in our recursive generator are mostly sharing weights thus making the overall framework light and efficient. Extensive experiments are conducted to show that our point cloud generator has comparable or even superior performance on both generation and reconstruction tasks in comparison to various baselines, and provides the consistent co-segmentation among instances of the same object class.},
  archive   = {C_IROS},
  author    = {Wei-Jan Kol and Chen-Yi Chiu and Yu-Liang Kuo and Wei-Chen Chiu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981471},
  pages     = {544-551},
  title     = {RPG: Learning recursive point cloud generation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast detection of moving traffic participants in LiDAR point
clouds by using particles augmented with free space information.
<em>IROS</em>, 538–543. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To navigate safely, it is essential for a robot to detect all kinds of moving objects that could possibly interfere with the own trajectory. For common object classes, like cars, regular pedestrians, and trucks, there are large scale datasets as well as corresponding machine learning techniques, which provide remarkable results in commonly available detection benchmarks. A big challenge that remains, are less frequent classes, which are not part of a dataset in a sufficient number and variation. Dynamic occupancy grids are a promising approach for detection of moving objects in point clouds since they impose only a few assumptions about the objects&#39; appearance and shape. Typically, they use particle filters to detect motion of occupancy in the grid. Existing approaches, however, often generate false positives at long obstacles because particles move along them. Therefore we propose a highly efficient approach, which performs the classification in a more structured and conservative way by making extensive use of available free space information. As a result, much less false positives are generated while the number of false negatives remains low. Our approach can be used to complement CNN-based object detections in order to detect both, frequent and uncommon object classes reliably. By using polar data structures that match the polar measurement principle, we are able to process even large point clouds of modern LiDARs with 128 lasers efficiently.},
  archive   = {C_IROS},
  author    = {Andreas Reich and Hans-Joachim Wuensche},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982239},
  pages     = {538-543},
  title     = {Fast detection of moving traffic participants in LiDAR point clouds by using particles augmented with free space information},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). E2Pose: Fully convolutional networks for end-to-end
multi-person pose estimation. <em>IROS</em>, 532–537. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Highly accurate multi-person pose estimation at a high framerate is a fundamental problem in autonomous driving. Solving the problem could aid in preventing pedestrian-car accidents. The present study tackles this problem by proposing a new model composed of a feature pyramid and an original head to a general backbone. The original head is built using lightweight CNNs and directly estimates multi-person pose coordinates. This configuration avoids the complex post-processing and two-stage estimation adopted by other models and allows for a lightweight model. Our model can be trained end-to-end and performed in real-time on a resource-limited platform (low-cost edge device) during inference. Experimental results using the COCO and CrowdPose datasets showed that our model can achieve a higher framerate (approx. 20 frames/sec with NVIDIA Jetson AGX Xavier) than other state-of-the-art models while maintaining sufficient accuracy for practical use.},
  archive   = {C_IROS},
  author    = {Masakazu Tobeta and Yoshihide Sawada and Ze Zheng and Sawa Takamuku and Naotake Natori},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981322},
  pages     = {532-537},
  title     = {E2Pose: Fully convolutional networks for end-to-end multi-person pose estimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DeepShapeKit: Accurate 4D shape reconstruction of swimming
fish. <em>IROS</em>, 526–531. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present methods for capturing 4D body shapes of swimming fish with affordable small training datasets and textureless 2D videos. Automated capture of spatiotemporal animal movements and postures is revolutionizing the study of collective animal behavior. 4D (including 3D space + time) shape data from animals like schooling fish contains a rich array of social and non-social information that can be used to shed light on the fundamental mechanisms underlying collective behavior. However, unlike the large datasets used for 4D shape reconstructions of the human body, there are no large amounts of labeled training datasets for reconstructing fish bodies in 4D, due to the difficulty of underwater data collection. We created a template mesh model using 3D scan data from a real fish, then extracted silhouettes (segmentation masks) and key-points of the fish body using Mask R-CNN and DeepLabCut, respectively. Next, using the Adam optimizer, we optimized the 3D template mesh model for each frame by minimizing the difference between the projected 3D model and the detected silhouettes as well as the key-points. Finally, using an LSTM-based smoother, we generated accurate 4D shapes of schooling fish based on the 3D shapes over each frame. Our results show that the method is effective for 4D shape reconstructions of swimming fish, with greater fidelity than other state-of-the-art algorithms.},
  archive   = {C_IROS},
  author    = {Ruiheng Wu and Oliver Deussen and Liang Li},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982097},
  pages     = {526-531},
  title     = {DeepShapeKit: Accurate 4D shape reconstruction of swimming fish},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FocusTR: Focusing on valuable feature by multiple
transformers for fusing feature pyramid on object detection.
<em>IROS</em>, 518–525. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The feature pyramid, which is a vital component of the convolutional neural networks, plays a significant role in several perception tasks, including object detection for autonomous driving. However, how to better fuse multi-level and multi-sensor feature pyramids is still a significant challenge, especially for object detection. This paper presents a FocusTR (Focusing on the valuable features by multiple Transformers), which is a simple yet effective architecture, to fuse feature pyramid for the single-stream 2D detector and two-stream 3D detector. Specifically, FocusTR encompasses several novel self-attention mechanisms, including the spatial-wise boxAlign attention (SB) for low-level spatial locations, context-wise affinity attention (CA) for high-level context information, and level-wise attention for the multi-level feature. To alleviate self-attention&#39;s computational complexity and slow training convergence, Fo-cusTR introduces a low and high-level fusion (LHF) to reduce the computational parameters, and the Pre- Ln [1]to accelerate the training convergence.},
  archive   = {C_IROS},
  author    = {Bangquan Xie and Liang Yang and Zongming Yang and Ailin Wei and Xiaoxiong Weng and Bing Li},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981047},
  pages     = {518-525},
  title     = {FocusTR: Focusing on valuable feature by multiple transformers for fusing feature pyramid on object detection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ensemble based anomaly detection for legged robots to
explore unknown environments. <em>IROS</em>, 511–517. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Exploring unknown environments, such as caves or planetary surfaces, requires a quick understanding of the surroundings. Beforehand, only aerial footage from satellites or images from previous missions might be available. The proposed ensemble based anomaly detection framework utilizes previously gained knowledge and incorporates it with insights gained during the mission. The modular system consists of different networks which are combined to determine anomalies in the current surroundings. By utilizing data from other missions, simulations or aerial photos, a precise anomaly detection can be achieved at the start of a mission. The system can further be improved by training new networks during the mission, which can be incorporated into the ensemble at runtime. This allows for synchronous execution of mission and training of models on a base station. The proposed system is tested and evaluated on an ANYmal C walking robot in different scenarios, however the approach is applicable for different kinds of mobile robots. The results show a clear improvement of ensembles compared to individual networks, while keeping a small memory footprint and low inference time on the mobile system.},
  archive   = {C_IROS},
  author    = {Lennart Puck and Maximilian Schik and Tristan Schnell and Timothee Buettner and Arne Roennau and Rüdiger Dillmann},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981446},
  pages     = {511-517},
  title     = {Ensemble based anomaly detection for legged robots to explore unknown environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toroidal origami monotrack: Mechanism to realize smooth
driving and bending for closed-skin-drive robots. <em>IROS</em>,
480–487. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel toroidal origami monotrack capable of smooth-skin driving and bending for closed-skin-drive robots. Monotracks are a promising solution for achieving high mobility in unstructured environments. Toroidal-drive mechanisms enable whole skin drive; however, conventional methods experience unexpected wrinkling and buckles that lead to a large resistance. In this study, we propose an origami bellows structure with multiple rollers that can maintain the skin tension and deal with the cause of large friction between the skin and the body. The origami structure design method is presented, and the bending angle range and required drive force were derived through a theoretical analysis. The validity of the effectiveness of the concept was verified through prototype testing.},
  archive   = {C_IROS},
  author    = {Masahiro Watanabe and Yuto Kemmotsu and Kenjiro Tadakuma and Kazuki Abe and Masashi Konyo and Satoshi Tadokoro},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982162},
  pages     = {480-487},
  title     = {Toroidal origami monotrack: Mechanism to realize smooth driving and bending for closed-skin-drive robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiple curvatures in a tendon-driven continuum robot using
a novel magnetic locking mechanism. <em>IROS</em>, 472–479. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tendon-driven continuum robots show promise for use in surgical applications as they can assume complex configurations to navigate along tortuous paths. However, to achieve these complex robot shapes, multiple segments are required as each robot segment can bend only with a single constant curvature. To actuate these additional robot segments, multiple tendons must typically be added on-board the robot, complicating their integration, robot control, and actuation. This work presents a method of achieving two curvatures in a single tendon-driven continuum robot segment through use of a novel magnetic locking mechanism. Thus, the need for additional robot segments and actuating tendons is eliminated. The resulting two curvatures in a single segment are demonstrated in two and three dimensions. Furthermore, the maximum magnetic field required to actuate the locking mechanism for different robot bending angles is experimentally measured to be 6.1 mT. Additionally, the locking mechanism resists unintentional unlocking unless the robot assumes a 0° bending angle and a magnetic field of 18.1 mT is applied, conditions which are not typically reached during routine use of the system. Finally, addressable actuation of two locking mechanisms is achieved, demonstrating the capability of producing multiple curvatures in a single robot segment.},
  archive   = {C_IROS},
  author    = {Chloe Pogue and Priyanka Rao and Quentin Peyron and Jongwoo Kim and Jessica Burgner-Kahrs and Eric Diller},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982193},
  pages     = {472-479},
  title     = {Multiple curvatures in a tendon-driven continuum robot using a novel magnetic locking mechanism},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Wirelessly magnetically actuated motor for tissue
regeneration robotic implant. <em>IROS</em>, 465–471. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In biomedical engineering, robotic implants provide new methods to restore and improve bodily function, and regenerate tissue. A significant challenge with the design of these devices is to safely actuate them for weeks or months, while they are residing in a patient&#39;s body. Magnetic, and other force-at-distance actuation methods, allow mechanisms to be controlled remotely and without contact or line of sight to the device. In this paper, we present a novel magnetic field driven wireless motor. The motor drives a robotic implant for the treatment of long gap esophageal atresia and short bowel syndrome. The motor is equipped with two oppositely oriented permanent magnets which experience forces in opposite directions when a magnetic field is applied tangential to the magnets&#39; directions. The implant can produce a force of 2 N. It is demonstrated with an ex vivo porcine esophagus.},
  archive   = {C_IROS},
  author    = {Cameron Duffield and Abigail F Smith and Daniela Rus and Dana Damian and Shuhei Miyashita},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981834},
  pages     = {465-471},
  title     = {Wirelessly magnetically actuated motor for tissue regeneration robotic implant},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A compact, lightweight and singularity-free wrist joint
mechanism for humanoid robots. <em>IROS</em>, 457–464. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Building humanoid robots with properties similar to those of humans in terms of strength and agility is a great and unsolved challenge. This work introduces a compact and lightweight wrist joint mechanism that is singularity-free and has large range of motion. The mechanism provides two degrees of freedom (DoF) and was developed for integration into a human scale humanoid robot arm. It is based on a parallel mechanism with rolling contact joint behaviour and remote actuation that facilitates a compact design with low mass and inertia. The mechanism&#39;s kinematics along with a solution of the inverse kinematics problem for the specific design, and the manipulability analysis are presented. The first prototype of the proposed mechanism shows the possible integration of actuation, sensing and electronics in small and narrow space. Experimental evaluations shows that the design feature unique performance regarding weight, speed, payload and accuracy.},
  archive   = {C_IROS},
  author    = {Cornelius Klas and Tamim Asfour},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981787},
  pages     = {457-464},
  title     = {A compact, lightweight and singularity-free wrist joint mechanism for humanoid robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Single-rod brachiation robot. <em>IROS</em>, 451–456. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a new brachiation robot, a single-rod brachiation robot. Brachiation is a method of locomotion that makes clever use of gravity and has been tried to apply to robots. Conventional brachiation robots are multiple-pendulum-like robots that mimic a gibbon. Although the multiple-pendulum-like robot can easily change the length of one brachiation step by joints, it has complex structures and generates aperiodic motions such as chaos. In contrast, the single-rod brachiation robot has the advantages of simple structure and the ability to suppress complex multiple-pendulum trajectories. The single-rod brachiation robot has a disadvantage because it is difficult to adjust the distance to the next bar. However, we can solve it by aerial brachiation, which includes an aerial phase before grasping the next bar. Using the actual robot, we showed that the swinging amplitude could be increased by appropriately moving its center of gravity like a trapeze motion. In addition, using this, we achieved continuous brachiation across three bars and brachiation including the aerial phase with a flight distance of 140 mm.},
  archive   = {C_IROS},
  author    = {Hijiri Akahane and Ikuo Mizuuchi},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981331},
  pages     = {451-456},
  title     = {Single-rod brachiation robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DRPD, dual reduction ratio planetary drive for articulated
robot actuators. <em>IROS</em>, 443–450. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a reduction mechanism for robot actuators that can switch between two types of reduction ratio. By fixing the carrier or ring gear of the proposed actuator which is based on the 3K compound planetary drive, the actuator can shift its reduction ratio. For compact design with reduced weight of the actuator, unique pawl brake mechanism interacting with cams and micro servos for switching mechanism is designed. The resulting prototype module has a reduction ratio of 6.91 and 44.93 for ‘low-reduction’ and ‘high-reduction’ ratios, respectively. Reduction ratios can be easily adjusted by modifying the pitch diameters of gears. Experimental results demonstrate that the proposed actuator could extend its operation region via two reduction modes that are interchangeable with gear shifting.},
  archive   = {C_IROS},
  author    = {Tae-Gyu Song and Young-Ha Shin and Seungwoo Hong and Hyungho Chris Choi and Joon-Ha Kim and Hae-Won Park},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981201},
  pages     = {443-450},
  title     = {DRPD, dual reduction ratio planetary drive for articulated robot actuators},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Light in the larynx: A miniaturized robotic optical fiber
for in-office laser surgery of the vocal folds. <em>IROS</em>, 427–434.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper reports the design, construction, and experimental validation of a novel hand-held robot for inoffice laser surgery of the vocal folds. In-office endoscopic laser surgery is an emerging trend in Laryngology: It promises to deliver the same patient outcomes of traditional surgical treatment (i.e., in the operating room), at a fraction of the cost. Unfortunately, office procedures can be challenging to perform; the optical fibers used for laser delivery can only emit light forward in a line-of-sight fashion, which severely limits anatomical access. The robot we present in this paper aims to overcome these challenges. The end effector of the robot is a steerable laser fiber, created through the combination of a thin optical fiber (Ø 0.225 mm) with a tendon-actuated Nickel- Titanium notched sheath that provides bending. This device can be seamlessly used with most commercially available endoscopes, as it is sufficiently small (0Ø 1.1 mm) to pass through a working channel. To control the fiber, we propose a compact actuation unit that can be mounted on top of the endoscope handle, so that, during a procedure, the operating physician can operate both the endoscope and the steerable fiber with a single hand. We report simulation and phantom experiments demonstrating that the proposed device substantially enhances surgical access compared to current clinical fibers.},
  archive   = {C_IROS},
  author    = {Alex J. Chiluisa and Nicholas E. Pacheco and Hoang S. Do and Ryan M. Tougas and Emily V. Minch and Rositsa Mihaleva and Yao Shen and Yuxiang Liu and Thomas L. Carroll and Loris Fichera},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981202},
  pages     = {427-434},
  title     = {Light in the larynx: A miniaturized robotic optical fiber for in-office laser surgery of the vocal folds},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design and evaluation of the infant cardiac robotic surgical
system (iCROSS). <em>IROS</em>, 413–418. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this study, the infant Cardiac Robotic Surgical System (iCROSS) is developed to assist a surgeon in performing the patent ductus arteriosus (PDA) closure and other infant cardiac surgeries. The iCROSS is a dual-arm robot allowing two surgical instruments to collaborate in a narrow space while keeping a sufficiently large workspace. Compared with the existing surgical robotic systems, the iCROSS meets the specific requirements of infant cardiac surgeries. Its feasibility has been validated through several teleoperated tasks performed in the experiment. In particular, the iCROSS is able to perform surgical ligation successfully within one minute.},
  archive   = {C_IROS},
  author    = {Po-Chih Chen and Pei-An Hsieh and Jing-Yuan Huang and Shu-Chien Huang and Cheng-Wei Chen},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981503},
  pages     = {413-418},
  title     = {Design and evaluation of the infant cardiac robotic surgical system (iCROSS)},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A pneumatic MR-conditional guidewire delivery mechanism with
decoupled actuations for endovascular intervention. <em>IROS</em>,
407–412. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Percutaneous coronary intervention (PCI) involves the delivery of a flexible submillimeter guidewire and existing x- ray based approaches impose significant ironing radiation. The use of magnetic resonance imaging (MRI) for intraoperative guidance has the advantages of not only being safe but also having high positioning accuracy and excellent tissue contrast. This paper develops a pneumatically driven MR-conditional delivery mechanism for the ease of manipulation of the guidewire in vivo. It incorporates newly developed rotary pneumatic step motors and a pneumatic slip ring for actuation and decoupling of translational and rotational motions. An effective clamping mechanism for the locking and releasing of the guidewire is also incorporated. The proposed pneumatic slip ring mechanism decouples six gas lines, where four are used to supply a pneumatic step motor for translational motion and two for the clamping mechanism. High friction sil sleeve is used to hold the guidewire firmly. The rotary pneumatic motor has excellent sealing and stability, providing an output torque of 15.75 Nm/MPa. Experiments show that the average error of translational motion is 0.37 mm. Real-time MRI-guided endovascular intervention is performed in a vascular phantom with pulsatile flows to validate its potential clinical use. The imaging artifact test under MRI shows no noticeable distortion and the loss of Signal-to-Noise Ratio (SNR) is less than 2\%.},
  archive   = {C_IROS},
  author    = {Shaoping Huang and Chuqian Lou and Lian Xuan and Hongyan Gao and Anzhu Gao and Guang–Zhong Yang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981596},
  pages     = {407-412},
  title     = {A pneumatic MR-conditional guidewire delivery mechanism with decoupled actuations for endovascular intervention},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Virtual reality simulator for fetoscopic spina bifida repair
surgery. <em>IROS</em>, 401–406. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Spina Bifida (SB) is a birth defect developed during the early stage of pregnancy in which there is incomplete closing of the spine around the spinal cord. The growing interest in fetoscopic Spina-Bifida repair, which is performed in fetuses who are still in the pregnant uterus, prompts the need for appropriate training. The learning curve for such procedures is steep and requires excellent procedural skills. Computer-based virtual reality (VR) simulation systems offer a safe, cost-effective, and configurable training environment free from ethical and patient safety issues. However, to the best of our knowledge, there are currently no commercial or experimental VR training simulation systems available for fetoscopic SB-repair procedures. In this paper, we propose a novel VR simulator for core manual skills training for SB-repair. An initial simulation realism validation study was carried out by obtaining subjective feedback (face and content validity) from 14 clinicians. The overall simulation realism was on average marked 4.07 on a 5-point Likert scale (1 - ‘very unrealistic’, 5 - ‘very realistic’). Its usefulness as a training tool for SB-repair as well as in learning fundamental laparoscopic skills was marked 4.63 and 4.80, respectively. These results indicate that VR simulation of fetoscopic procedures may contribute to surgical training without putting fetuses and their mothers at risk. It could also facilitate wider adaptation of fetoscopic procedures in place of much more invasive open fetal surgeries.},
  archive   = {C_IROS},
  author    = {Przemysław Korzeniowski and Szymon Płotka and Robert Brawura-Biskupski-Samaha and Arkadiusz Sitek},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981920},
  pages     = {401-406},
  title     = {Virtual reality simulator for fetoscopic spina bifida repair surgery},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic laser steering for middle ear surgery.
<em>IROS</em>, 394–400. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper deals with the control of a laser spot in the context of minimally invasive surgery of the middle ear, e.g., cholesteatoma removal. More precisely, our work is concerned with the exhaustive burring of residual infected cells after primary mechanical resection of the pathological tissues since the latter cannot guarantee the treatment of all the infected tissues, the remaining infected cells cause regeneration of the diseases in 20\%-25\% of cases, which require a second surgery 12–18 months later. To tackle such a complex surgery, we have developed a robotic platform that consists of the combination of a macro-scale system (7 degrees of freedoms (DoFs) robotic arm) and a micro-scale flexible system (2 DoFs) which operates inside the middle ear cavity. To be able to treat the residual cholesteatoma regions, we proposed a method to generate optimal laser automatically scanning trajectories inside the areas and between them. The trajectories are tacked using an image-based control scheme. The proposed method and materials were validated experimentally using the lab-made robotic platform. The obtained results in terms of accuracy and behaviour meet the laser surgery requirements perfectly.},
  archive   = {C_IROS},
  author    = {Jae-Hun So and Jérême Szewczyk and Brahim Tamadazte},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982176},
  pages     = {394-400},
  title     = {Automatic laser steering for middle ear surgery},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient concurrent design of the morphology of unmanned
aerial systems and their collective-search behavior. <em>IROS</em>,
388–393. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The collective operation of robots, such as unmanned aerial vehicles (UAVs) operating as a team or swarm, is affected by their individual capabilities, which in turn is dependent on their physical design, aka morphology. However, with the exception of a few (albeit ad hoc) evolutionary robotics methods, there has been very little work on understanding the interplay of morphology and collective behavior. There is especially a lack of computational frameworks to concurrently search for the robot morphology and the hyper-parameters of their behavior model that jointly optimize the collective (team) performance. To address this gap, this paper proposes a new co-design framework. Here the exploding computational cost of an otherwise nested morphology/behavior co-design is effectively alleviated through the novel concept of “talent” metrics; while also allowing significantly better solutions compared to the typically sub-optimal sequential morphology → behavior design approach. This framework comprises four major steps: talent metrics selection, talent Pareto exploration (a multi-objective morphology optimization process), behavior optimization, and morphology finalization. This co-design concept is demonstrated by applying it to design UAVs that operate as a team to localize signal sources, e.g., in victim search and hazard localization. Here, the collective behavior is driven by a recently reported batch Bayesian search algorithm called Bayes-Swarm. Our case studies show that the outcome of co-design provides significantly higher success rates in signal source localization compared to a baseline design, across a variety of signal environments and teams with 6 to 15 UAVs. Moreover, this co-design process provides two orders of magnitude reduction in computing time compared to a projected nested design approach.},
  archive   = {C_IROS},
  author    = {Chen Zeng and Prajit KrisshnaKumar and Jhoel Witter and Souma Chowdhury},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981312},
  pages     = {388-393},
  title     = {Efficient concurrent design of the morphology of unmanned aerial systems and their collective-search behavior},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Vision-based relative detection and tracking for teams of
micro aerial vehicles. <em>IROS</em>, 380–387. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we address the vision-based detection and tracking problems of multiple aerial vehicles using a single camera and Inertial Measurement Unit (IMU) as well as the corresponding perception consensus problem (i.e., uniqueness and identical IDs across all observing agents). We design several vision-based decentralized Bayesian multi-tracking filtering strategies to resolve the association between the incoming unsorted measurements obtained by a visual detector algorithm and the tracked agents. We compare their accuracy in different operating conditions as well as their scalability according to the number of agents in the team. This analysis provides useful insights about the most appropriate design choice for the given task. We further show that the proposed perception and inference pipeline which includes a Deep Neural Network (DNN) as visual target detector is lightweight and capable of concurrently running control and planning with Size, Weight, and Power (SWaP) constrained robots on-board. Experimental results show the effective tracking of multiple drones in various challenging scenarios such as heavy occlusions.},
  archive   = {C_IROS},
  author    = {Rundong Ge and Moonyoung Lee and Vivek Radhakrishnan and Yang Zhou and Guanrui Li and Giuseppe Loianno},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981115},
  pages     = {380-387},
  title     = {Vision-based relative detection and tracking for teams of micro aerial vehicles},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RAPTOR: Rapid aerial pickup and transport of objects by
robots. <em>IROS</em>, 349–355. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Rapid aerial grasping through robots can lead to many applications that utilize fast and dynamic picking and placing of objects. Rigid grippers traditionally used in aerial manipulators require high precision and specific object geometries for successful grasping. We propose RAPTOR, a quadcopter platform combined with a custom Fin Ray ® gripper to enable more flexible grasping of objects with different geometries, leveraging the properties of soft materials to increase the contact surface between the gripper and the objects. To reduce the communication latency, we present a new lightweight middleware solution based on Fast DDS (Data Distribution Service) as an alternative to ROS (Robot Operating System). We show that RAPTOR achieves an average of 83\% grasping efficacy in a real-world setting for four different object geometries while moving at an average velocity of 1 m/s during grasping. In a high-velocity setting, RAPTOR supports up to four times the payload compared to previous works. Our results highlight the potential of aerial drones in automated warehouses and other manipulation applications where speed, swiftness, and robustness are essential while operating in hard-to-reach places. 1 1 Code: https://github.com/raptor-ethz/raptor_setup},
  archive   = {C_IROS},
  author    = {Aurel X. Appius and Erik Bauer and Marc Blöchlinger and Aashi Kalra and Robin Oberson and Arman Raayatsanati and Pascal Strauch and Sarath Suresh and Marco von Salis and Robert K. Katzschmann},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981668},
  pages     = {349-355},
  title     = {RAPTOR: Rapid aerial pickup and transport of objects by robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GaSLAM: An algorithm for simultaneous gas source
localization and gas distribution mapping in 3D. <em>IROS</em>, 333–340.
(<a href="https://doi.org/10.1109/IROS47612.2022.9981976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Chemical gas dispersion poses considerable threat to humans, animals and the environment. The research areas of gas source localization and gas distribution mapping aim to localize the source of gas leaks and map the gas plume respectively, in order to help the coordination of swift rescue missions. Although very similar, these two areas are often treated separately in literature. In some cases, inferences on the gas distribution are made a posteriori from the source location, or vice-versa. In this paper, we introduce GaSLAM, a methodology that couples the estimation of the gas map and the source location using two state of the art algorithms with a novel navigation strategy based on informative quantities. The synergistic approach allows our algorithm to achieve a good estimation of both objectives and push the navigation strategies towards informative areas of the experimental volume. We validate the algorithm in simulation and with physical experiments in varying environmental conditions. We show that the algorithm improves on the source location estimate compared to a similar approach found in literature, and is able to deliver good quality maps of the gas distribution.},
  archive   = {C_IROS},
  author    = {Chiara Ercolani and Lixuan Tang and Alcherio Martinoli},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981976},
  pages     = {333-340},
  title     = {GaSLAM: An algorithm for simultaneous gas source localization and gas distribution mapping in 3D},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time hybrid mapping of populated indoor scenes using a
low-cost monocular UAV. <em>IROS</em>, 325–332. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unmanned aerial vehicles (UAVs) have been used for many applications in recent years, from urban search and rescue, to agricultural surveying, to autonomous underground mine exploration. However, deploying UAVs in tight, indoor spaces, especially close to humans, remains a challenge. One solution, when limited payload is required, is to use micro-UAVs, which pose less risk to humans and typically cost less to replace after a crash. However, micro-UAVs can only carry a limited sensor suite, e.g. a monocular camera instead of a stereo pair or LiDAR, complicating tasks like dense mapping and markerless multi-person 3D human pose estimation, which are needed to operate in tight environments around people. Monocular approaches to such tasks exist, and dense monocular mapping approaches have been successfully deployed for UAV applications. However, despite many recent works on both marker-based and markerless multi-UAV single-person motion capture, markerless single-camera multi-person 3D human pose estimation remains a much earlier-stage technology, and we are not aware of existing attempts to deploy it in an aerial context. In this paper, we present what is thus, to our knowledge, the first system to perform simultaneous mapping and multi-person 3D human pose estimation from a monocular camera mounted on a single UAV. In particular, we show how to loosely couple state-of-the-art monocular depth estimation and monocular 3D human pose estimation approaches to reconstruct a hybrid map of a populated indoor scene in real time. We validate our component-level design choices via extensive experiments on the large-scale ScanNet and GTA-IM datasets. To evaluate our system-level performance, we also construct a new Oxford Hybrid Mapping dataset of populated indoor scenes.},
  archive   = {C_IROS},
  author    = {Stuart Golodetz and Madhu Vankadari and Aluna Everitt and Sangyun Shin and Andrew Markham and Niki Trigoni},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982054},
  pages     = {325-332},
  title     = {Real-time hybrid mapping of populated indoor scenes using a low-cost monocular UAV},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Depth360: Self-supervised learning for monocular depth
estimation using learnable camera distortion model. <em>IROS</em>,
317–324. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-supervised monocular depth estimation has been widely investigated to estimate depth images and relative poses from RGB images. This framework is promising because the depth and pose networks can be trained from just time-sequence images without the need for the ground truth depth and poses. In this work, we estimate the depth around a robot (360° view) using time-sequence spherical camera images, from a camera whose parameters are unknown. We propose a learnable axisymmetric camera model which accepts distorted spherical camera images with two fisheye camera images as well as pinhole camera images. In addition, we trained our models with a photo-realistic simulator to generate ground truth depth images to provide supervision. Moreover, we introduced loss functions to provide floor constraints to reduce artifacts that can result from reflective floor surfaces. We demonstrate the efficacy of our method using the spherical camera images from the GO Stanford dataset and pinhole camera images from the KITTI dataset to compare our method&#39;s performance with that of baseline method in learning the camera parameters.},
  archive   = {C_IROS},
  author    = {Noriaki Hirose and Kosuke Tahara},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981751},
  pages     = {317-324},
  title     = {Depth360: Self-supervised learning for monocular depth estimation using learnable camera distortion model},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-source domain alignment for domain invariant
segmentation in unknown targets. <em>IROS</em>, 309–316. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Semantic segmentation provides scene understanding capability by performing pixel-wise classification of objects within an image. However, the sensitivity of such algorithms towards domain changes requires fine-tuning using an annotated dataset for each novel domain, which is expensive to construct and inefficient. We highlight that irrespective of the training dataset, structural properties of scenes remain the same hence domain sensitivity arises from training methodology. Thus, in this paper, we propose a domain alignment approach wherein multiple synthetic source domains are used to train an underlying segmentation network such that it performs consistently in unknown real target domains. Towards this end, we propose a pixel-wise supervised contrastive learning framework that enforces constraints in latent space resulting in features belonging to the same class being clustered closely and away from different classes. This approach allows for better capturing of global and local semantics while providing domain invariant properties. Our approach can be easily incorporated into prior semantic segmentation approaches without the significant computational overhead. We empirically demonstrate the efficacy of the proposed approach on GTAV → Cityscapes, GTAV+Synthia → Cityscapes, and GTAV+Synthia+Synscapes → Cityscapes scenarios and report state-of-the-art (SoTA) performance without requiring access to images from the target domain.},
  archive   = {C_IROS},
  author    = {Pranjay Shyam and Kuk-Jin Yoon and Kyung-Soo Kim},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981166},
  pages     = {309-316},
  title     = {Multi-source domain alignment for domain invariant segmentation in unknown targets},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards safety-aware pedestrian detection in autonomous
systems. <em>IROS</em>, 293–300. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a framework to assess the quality of a pedestrian detector in an autonomous driving scenario. To do this, we exploit performance metrics from the domain of computer vision on one side and so-called threat metrics from the motion planning domain on the other side. Based on a reachability analysis that accounts for the uncertainty in future motions of other traffic participants, we can determine the worst-case threat from the planning domain and relate it to the corresponding detection from the visual input. Our evaluation results for a RetinaNet on the Argoverse 1.1 [1] dataset show that already a rather simple threat metric such as time-to-collision (TTC) allows to select potentially dangerous interactions between the ego vehicle and a pedestrian when purely vision-based detections fail, even if they are passed to a subsequent object tracker. In addition, our results show that two different DNNs (Deep Neural Networks) with comparable performance differ significantly in the number of critical scenarios that we can identify with our method.},
  archive   = {C_IROS},
  author    = {Maria Lyssenko and Christoph Gladisch and Christian Heinzemann and Matthias Woehrle and Rudolph Triebel},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981309},
  pages     = {293-300},
  title     = {Towards safety-aware pedestrian detection in autonomous systems},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attention-based deep driving model for autonomous vehicles
with surround-view cameras. <em>IROS</em>, 286–292. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Experienced human drivers always make safe driving decisions by selectively observing the front, rear and side- view mirrors. Several end - to-end methods have been pro-posed to learn driving models with multi-view visual infor-mation. However, these benchmark methods lack semantic understanding of multi-view image contents, where human drivers usually reason these information for decision making with different visual region of interests. In this paper, we propose an attention-based deep learning method to learn a driving model with input of surround-view visual information and the route planner, in which a multi-view attention module is designed for obtaining region of interests from human drivers. We evaluate our model on the Drive360 dataset with comparison of benchmarking deep driving models. Results demonstrate that our model achieves a competitive accuracy in both steering angle and speed prediction than benchmarking methods. Code is available at https://githuh.com/jet-uestc/MVA-Net.},
  archive   = {C_IROS},
  author    = {Yang Zhao and Jie Li and Rui Huang and Boqi Li and Ao Luo and Yaochen Li and Hong Cheng},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982143},
  pages     = {286-292},
  title     = {Attention-based deep driving model for autonomous vehicles with surround-view cameras},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TransDARC: Transformer-based driver activity recognition
with latent space feature calibration. <em>IROS</em>, 278–285. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traditional video-based human activity recognition has experienced remarkable progress linked to the rise of deep learning, but this effect was slower as it comes to the downstream task of driver behavior understanding. Understanding the situation inside the vehicle cabin is essential for Advanced Driving Assistant System (ADAS) as it enables identifying distraction, predicting driver&#39;s intent and leads to more convenient human-vehicle interaction. At the same time, driver observation systems face substantial obstacles as they need to capture different granularities of driver states, while the complexity of such secondary activities grows with the rising automation and increased driver freedom. Furthermore, a model is rarely deployed under conditions identical to the ones in the training set, as sensor placements and types vary from vehicle to vehicle, constituting a substantial obstacle for real-life deployment of data-driven models. In this work, we present a novel vision-based framework for recognizing secondary driver behaviours based on visual transformers and an additional augmented feature distribution calibration module. This module operates in the latent feature-space enriching and diversifying the training set at feature-level in order to improve generalization to novel data appearances, (e.g., sensor changes) and general feature quality. Our framework consistently leads to better recognition rates, surpassing previous state-of-the-art results of the public Drive&amp;Act benchmark on all granularity levels. Our code will be made publicly available at https://github.com/KPeng9510/TransDARC.},
  archive   = {C_IROS},
  author    = {Kunyu Peng and Alina Roitberg and Kailun Yang and Jiaming Zhang and Rainer Stiefelhagen},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981445},
  pages     = {278-285},
  title     = {TransDARC: Transformer-based driver activity recognition with latent space feature calibration},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 3D single-object tracking with spatial-temporal data
association. <em>IROS</em>, 264–269. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a novel 3D single-object tracker to more stably, accurately, and faster track objects, even if they are temporarily missed. Our idea is to utilize spatial-temporal data association to achieve object tracking robustly, and it consists of two main parts. We firstly employ a temporal motion model cross frames to estimate the object&#39;s temporal information and update the region of interest(ROI). The advanced detector only focuses on ROI rather than the whole scene to generate the spatial position. Second, we introduce a new pairwise evaluation system to exploit spatial-temporal data association in point clouds. The proposed evaluation system considers detection confidence, orientation offset, and objects distance to more stably achieve object matching. Then, we update the predicted state based on the pairwise spatial-temporal data. Finally, we utilize the previous trajectory to enhance the accuracy of static tracking in the refinement scheme. Experiments on the KITTI and nuScenes tracking datasets demonstrate that our method outperforms other state-of-the-art methods by a large margin (a 10\% improvement and 280 FPS on a single NVIDIA 1080Ti GPU). Compared with multi-object tracking, our tracker also has superiority.},
  archive   = {C_IROS},
  author    = {Yongchang Zhang and Hanbing Niu and Yue Guo and Wenhao He},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981905},
  pages     = {264-269},
  title     = {3D single-object tracking with spatial-temporal data association},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-sensor data annotation using sequence-based active
learning. <em>IROS</em>, 258–263. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Neural Networks are the state-of-the-art technology for environmental perception in applications such as autonomous driving. However, they require a large amount of training data in order to perform well, making the selection and annotation of sensor data a time-consuming and expensive task. Active learning is a promising approach to reduce the required amount of training data by selecting samples for annotation that are expected to improve the neural network the most. In this work, we propose a sequence-based active learning approach that selects sequences of consecutive frames instead of individual images. This allows to evaluate tracking algorithms and to reduce the annotation effort by interpolating labels between frames. Our approach is compared to a random sampling strategy as baseline. Over 15 iterations, both approaches select 1000 additional images for training in each iteration. The performance of the neural network trained on the data selected by our sequence-based active learning approach is compared to the performance of the network trained on the data select by the baseline approach. The results show that sequence-based active learning can reduce the required amount of training data by up to 25\% while reaching a similar performance. Furthermore, sequence-based active learning can improve the neural network&#39;s overall performance by 2\% compared to a random sampling strategy. In this work, the proposed method was evaluated with a new dataset consisting of 15 scenes in railway environments. The dataset has 45,888 frames in total, 14,513 frames contain persons on railway stations or close to tracks.},
  archive   = {C_IROS},
  author    = {Patrick Denzler and Markus Ziegler and Arne Jacobs and Volker Eiselein and Philipp Neumaier and Martin Köppel},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981074},
  pages     = {258-263},
  title     = {Multi-sensor data annotation using sequence-based active learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to fold real garments with one arm: A case study in
cloud-based robotics research. <em>IROS</em>, 251–257. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous fabric manipulation is a longstanding challenge in robotics, but evaluating progress is difficult due to the cost and diversity of robot hardware. Using Reach, a cloud robotics platform that enables low-latency remote execution of control policies on physical robots, we present the first systematic benchmarking of fabric manipulation al-gorithms on physical hardware. We develop 4 novel learning-based algorithms that model expert actions, keypoints, reward functions, and dynamic motions, and we compare these against 4 learning-free and inverse dynamics algorithms on the task of folding a crumpled T-shirt with a single robot arm. The entire lifecycle of data collection, model training, and policy evaluation was performed remotely without physical access to the robot workcell. Results suggest a new algorithm combining imitation learning with analytic methods achieves human-level performance on the flattening task and 93\% of human-level performance on the folding task. See https://sites.google.com/berkeley.edu/ cloudfolding for all data, code, models, and supplemental material.},
  archive   = {C_IROS},
  author    = {Ryan Hoque and Kaushik Shivakumar and Shrey Aeron and Gabriel Deza and Aditya Ganapathi and Adrian Wong and Johnny Lee and Andy Zeng and Vincent Vanhoucke and Ken Goldberg},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981253},
  pages     = {251-257},
  title     = {Learning to fold real garments with one arm: A case study in cloud-based robotics research},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A solution to slosh-free robot trajectory optimization.
<em>IROS</em>, 223–230. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper is about fast slosh-free fluid transportation. Existing approaches are either computationally heavy or only suitable for specific robots and container shapes. We model the end effector as a point mass suspended by a spherical pendulum and study the requirements for slosh-free motion and the validity of the point mass model. In this approach, slosh-free trajectories are generated by controlling the pendulum&#39;s pivot and simulating the motion of the point mass. We cast the trajectory optimization problem as a quadratic program-this strategy can be used to obtain valid control inputs. Through simulations and experiments on a 7 DoF Franka Emika Panda robot we validate the effectiveness of the proposed approach.},
  archive   = {C_IROS},
  author    = {Rafael I. Cabral Muchacho and Riddhiman Laha and Luis F.C. Figueredo and Sami Haddadin},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981173},
  pages     = {223-230},
  title     = {A solution to slosh-free robot trajectory optimization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient spatial representation and routing of deformable
one-dimensional objects for manipulation. <em>IROS</em>, 211–216. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the field of rigid-body robotics having matured in the last fifty years, routing, planning, and manipulation of deformable objects have recently emerged as a more untouched research area in many fields ranging from surgical robotics to industrial assembly and construction. Routing approaches for deformable objects which rely on learned implicit spatial representations (e.g., Learning-from-Demonstration methods) make them vulnerable to changes in the environment and the specific setup. On the other hand, algorithms that entirely separate the spatial representation of the deformable object from the routing and manipulation, often using a representation approach independent of planning, result in slow planning in high dimensional space. This paper proposes a novel approach to routing deformable one-dimensional objects (e.g., wires, cables, ropes, sutures, threads). This approach utilizes a compact representation for the object, allowing efficient and fast online routing. The spatial representation is based on the geometrical decomposition of the space into convex subspaces, resulting in a discrete coding of the deformable object configuration as a sequence. With such a configuration, the routing problem can be solved using a fast dynamic programming sequence matching method that calculates the next routing move. The proposed method couples the routing and efficient configuration for improved planning time. Our simulation and real experiments show the method correctly computing the next manipulation action in sub-millisecond time and accomplishing various routing and manipulation tasks.},
  archive   = {C_IROS},
  author    = {Azarakhsh Keipour and Maryam Bandari and Stefan Schaal},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981939},
  pages     = {211-216},
  title     = {Efficient spatial representation and routing of deformable one-dimensional objects for manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quasistatic contact-rich manipulation via linear
complementarity quadratic programming. <em>IROS</em>, 203–210. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Contact-rich manipulation is challenging due to dynamically-changing physical constraints by the contact mode changes undergone during manipulation. This paper proposes a versatile local planning and control framework for contact-rich manipulation that determines the continuous control action under variable contact modes online. We model the physical characteristics of contact-rich manipulation by quasistatic dynamics and complementarity constraints. We then propose a linear complementarity quadratic program (LCQP) to efficiently determine the control action that implicitly includes the decisions on the contact modes under these constraints. In the LCQP, we relax the complementarity constraints to alleviate ill-conditioned problems that are typically caused by measure noises or model miss-matches. We conduct dynamical simulations on a 3D physical simulator and demonstrate that the proposed method can achieve various contact-rich manipulation tasks by determining the control action including the contact modes in real-time.},
  archive   = {C_IROS},
  author    = {Sotaro Katayarna and Tatsunori Taniai and Kazutoshi Tanaka},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981498},
  pages     = {203-210},
  title     = {Quasistatic contact-rich manipulation via linear complementarity quadratic programming},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Constraint-based task specification and trajectory
optimization for sequential manipulation. <em>IROS</em>, 197–202. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To economically deploy robotic manipulators the programming and execution of robot motions must be swift. To this end, we propose a novel, constraint-based method to intuitively specify sequential manipulation tasks and to compute time-optimal robot motions for such a task specification. Our approach follows the ideas of constraint-based task specification by aiming for a minimal and object-centric task description that is largely independent of the underlying robot kinematics. We transform this task description into a non-linear optimization problem. By solving this problem we obtain a (locally) time-optimal robot motion, not just for a single motion, but for an entire manipulation sequence. We demonstrate the capabilities of our approach in a series of experiments involving five distinct robot models, including a highly redundant mobile manipulator.},
  archive   = {C_IROS},
  author    = {Mun Seng Phoon and Philipp S. Schmitt and Georg V. Wichert},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981909},
  pages     = {197-202},
  title     = {Constraint-based task specification and trajectory optimization for sequential manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hierarchical framework for long horizon planning of
object-contact trajectories. <em>IROS</em>, 189–196. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Given an object, an environment, and a goal pose, how should a robot make contact to move it? Solving this problem requires reasoning about rigid-body dynamics, object and environment geometries, and hybrid contact mechanics. This paper proposes a hierarchical framework that solves this problem in 2D worlds, with polygonal objects and point fingers. To achieve this, we decouple the problem in three stages: 1) a high-level graph search over regions of free-space, 2) a medium-level randomized motion planner for the object motion, and 3) a low-level contact-trajectory optimization for the robot and environment contacts. In contrast to the state of the art, this approach does not rely on handcrafted primitives and can still be solved efficiently. This algorithm does not require seeding and can be applied to complex object shapes and environments. We validate this framework with extensive simulated experiments showcasing long-horizon and contact-rich interactions. We demonstrate how our algorithm can reliably solve complex planar manipulation problems in the order of seconds.},
  archive   = {C_IROS},
  author    = {Bernardo Aceituno and Alberto Rodriguez},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981862},
  pages     = {189-196},
  title     = {A hierarchical framework for long horizon planning of object-contact trajectories},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Testing service robots in the field: An experience report.
<em>IROS</em>, 165–172. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Service robots are mobile autonomous robots, often operating in uncertain and difficult environments. While being increasingly popular, engineering service robots is challenging. Especially, evolving them from prototype to deployable product requires effective validation and verification, assuring the robot&#39;s correct and safe operation in the target environment. While testing is the most common validation and verification technique used in practice, surprisingly little is known about the actual testing practices and technologies used in the service robotics domain. We present an experience report on field testing of an industrial-strength service robot, as it transitions from lab experiments to an operational environment. We report challenges and solutions, and reflect on their effectiveness. Our long-term goal is to establish empirically-validated testing techniques for service robots. This experience report constitutes a necessary, but self-contained first step, exploring field testing practices in detail. Our data sources are detailed test artifacts and developer interviews. We model the field testing process and describe test-case design practices. We discuss experiences from performing these field tests over a 10-month test campaign.},
  archive   = {C_IROS},
  author    = {Argentina Ortega and Nico Hochgeschwender and Thorsten Berger},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981789},
  pages     = {165-172},
  title     = {Testing service robots in the field: An experience report},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design of a reconfigurable robot with size-adaptive path
planner. <em>IROS</em>, 157–164. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Area coverage is demanded from the robots utilized in application domains such as floor cleaning. Even though many advanced coverage algorithms have been developed, the area coverage performance is limited due to the inaccessibility of narrow spaces caused by physical constraints. Reconfigurable robots have been introduced to overcome this limitation where reconfigurability could help in assessing narrow spaces. Nevertheless, the state-of-the-art reconfigurable robots are not capable of changing the morphology size and shape as a single entity. Therefore, this paper proposes a novel design of a reconfigurable robot with a size-adaptive coverage strategy. The reconfiguration mechanism is designed in such a way that the robot can independently expand or shrink its size along the principal planar axes, where the behavior allows the change of size and shape. The coverage strategy is based on boustrophedon motion and the A* algorithm modified for accessing narrow areas using the size adaptability. The design of the robot is detailed in the paper, including electro-mechanical aspects, design considerations, and the coverage path planning method. Experiments have been conducted using a prototype of the proposed design to analyze and evaluate the characteristics and the performance of the robot. The results show that the proposed robot design can improve the productivity of a floor cleaning robot in terms of area coverage and coverage time.},
  archive   = {C_IROS},
  author    = {S. M. Bhagya P. Samarakoon and M. A. Viraj J. Muthugala and Manivannan Kalimuthu and Sathis Kumar Chandrasekaran and Mohan Rajesh Elara},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981125},
  pages     = {157-164},
  title     = {Design of a reconfigurable robot with size-adaptive path planner},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pedestrian-robot interactions on autonomous crowd
navigation: Reactive control methods and evaluation metrics.
<em>IROS</em>, 149–156. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous navigation in highly populated areas remains a challenging task for robots because of the difficulty in guaranteeing safe interactions with pedestrians in unstructured situations. In this work, we present a crowd navigation control framework that delivers continuous obstacle avoidance and post-contact control evaluated on an autonomous personal mobility vehicle. We propose evaluation metrics for accounting efficiency, controller response and crowd interactions in natural crowds. We report the results of over 110 trials in different crowd types: sparse, flows, and mixed traffic, with low- (&lt; 0.15 ppsm), mid- (&lt; 0.65 ppsm), and high- (&lt; 1 ppsm) pedestrian densities. We present comparative results between two low-level obstacle avoidance methods and a baseline of shared control. Results show a 10\% drop in relative time to goal on the highest density tests, and no other efficiency metric decrease. Moreover, autonomous navigation showed to be comparable to shared-control navigation with a lower relative jerk and significantly higher fluency in commands indicating high compatibility with the crowd. We conclude that the reactive controller fulfils a necessary task of fast and continuous adaptation to crowd navigation, and it should be coupled with high-level planners for environmental and situational awareness.},
  archive   = {C_IROS},
  author    = {Diego Paez-Granados and Yujie He and David Gonon and Dan Jia and Bastian Leibe and Kenji Suzuki and Aude Billard},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981705},
  pages     = {149-156},
  title     = {Pedestrian-robot interactions on autonomous crowd navigation: Reactive control methods and evaluation metrics},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Shared autonomy for safety between a self-reconfigurable
robot and a teleoperator using multi-layer fuzzy logic. <em>IROS</em>,
141–148. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous vehicles are designed to elevate the efficiency of assigned tasks and ensure the safety of the environment in which they operate. This paper presents a research study focused on shared autonomy using a multi-layer fuzzy logic framework to build a relationship between an autonomous self-reconfigurable robot and a human user by switching control to the teleoperator to assist the robot when it faces challenging scenarios while keeping a good performance and maintaining a safe environment. A novel multi-layer fuzzy logic decision process with shared autonomy for a safety framework is proposed. It evaluates safety based on the robot&#39;s multi-sensor inputs, the teleoperator&#39;s attention level, and the configuration state of the self-reconfigurable robot and switches the operation mode, robot speed gain, and configuration state for performance and safety without compromises. The experimental outcome successfully demonstrates the self-reconfigurable robot&#39;s capability to navigate safely using shared autonomy in real-world pavement scenarios using the proposed algorithm during autonomous navigation.},
  archive   = {C_IROS},
  author    = {Raul F.G. Azcarate and S.C. Daniela and A.A. Hayat and Lim Yi and M. A. Viraj J. Muthugala and Q.R Tang and A.P. Povendhan and K.J.K. Leong and M.R. Elara},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981872},
  pages     = {141-148},
  title     = {Shared autonomy for safety between a self-reconfigurable robot and a teleoperator using multi-layer fuzzy logic},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Service robots in a bakery shop: A field study.
<em>IROS</em>, 134–140. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we report on a field study in which we employed two service robots in a bakery store as a sales promotion. Previous studies have explored public applications of service robots public such as shopping malls. However, more evidence is needed that service robots can contribute to sales in real stores. Moreover, the behaviors of customers and service robots in the context of sales promotions have not been examined well. Hence, the types of robot behavior that can be considered effective and the customers&#39; responses to these robots remain unclear. To address these issues, we installed two tele-operated service robots in a bakery store for nearly 2 weeks, one at the entrance as a greeter and the other one inside the store to recommend products. The results show a dramatic increase in sales during the days when the robots were applied. Furthermore, we annotated the video recordings of both the robots&#39; and customers&#39; behavior. We found that although the robot placed at the entrance successfully attracted the interest of the passersby, no apparent increase in the number of customers visiting the store was observed. However, we confirmed that the recommendations of the robot operating inside the store did have a positive impact. We discuss our findings in detail and provide both theoretical and practical recommendations for future research and applications.},
  archive   = {C_IROS},
  author    = {Sichao Song and Baba Jun and Junya Nakanishi and Yuichiro Yoshikawa and Hiroshi Ishiguro},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981439},
  pages     = {134-140},
  title     = {Service robots in a bakery shop: A field study},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Non-parametric modeling of spatio-temporal human activity
based on mobile robot observations. <em>IROS</em>, 126–133. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents a non-parametric spatiotemporal model for mapping human activity by mobile autonomous robots in a long-term context. Based on Variational Gaussian Process Regression, the model incorporates prior information of spatial and temporal-periodic dependencies to create a continuous representation of human occurrences. The inhomogeneous data distribution resulting from movements of the robot is included in the model via a heteroscedastic likelihood function and can be accounted for as predictive uncertainty. Using a sparse formulation, data sets over multiple weeks and several hundred square meters can be used for model creation. The experimental evaluation, based on multi-week data sets, demonstrates that the proposed approach outperforms the state of the art both in terms of predictive quality and subsequent path planning.},
  archive   = {C_IROS},
  author    = {Marvin Stuede and Moritz Schappler},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982067},
  pages     = {126-133},
  title     = {Non-parametric modeling of spatio-temporal human activity based on mobile robot observations},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An autonomous descending-stair cleaning robot with RGB-d
based detection, approaching, and area coverage process. <em>IROS</em>,
118–125. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cleaning robots are one of the market dominators in the commercialized robot space. So far, numerous robots have been introduced that can perform cleaning tasks in various settings, including floor, pavement, pool, lawn, windows, etc. However, none of the existing commercial cleaning robots targets the staircase, commonly found in multi-story buildings. Even though few works in the literature introduced robotic solutions for staircase cleaning, they primarily focused on cleaning the ascending staircase often, with a loose connection to access the descending staircase. In this paper, we propose a novel autonomous reconfigurable robotic platform called sTetro-D that can autonomously detect the descending staircase, approach the step, and perform area coverage in an unknown environment. The developed autonomy framework consists of two modes which are search mode and clean mode. In search mode, we implemented an RGB-D camera-based fusion technique wherein we combined the image bounding box from DCNN (Deep Convolution Neural Network) with the depth information to find the 3D first step pose that assists the robot in approaching it precisely. After the successful stair approach, the cleaning mode enables the staircase area coverage process. We described all these aspects and concluded with an experimental analysis of the proposed robotic system in a real-world scenario. The results demonstrate that the robot has a significant performance in detecting the descending staircase, staircase approach, and area coverage.},
  archive   = {C_IROS},
  author    = {Prabakaran Veerajagadheswar and Anh Vu Le and Phone Thiha Kyaw and Mohan Rajesh Elara and Aung Paing},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982007},
  pages     = {118-125},
  title     = {An autonomous descending-stair cleaning robot with RGB-D based detection, approaching, and area coverage process},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LaneSNNs: Spiking neural networks for lane detection on the
loihi neuromorphic processor. <em>IROS</em>, 79–86. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous Driving (AD) related features represent important elements for the next generation of mobile robots and autonomous vehicles focused on increasingly intelligent, autonomous, and interconnected systems. The applications involving the use of these features must provide, by definition, real-time decisions, and this property is key to avoid catastrophic accidents. Moreover, all the decision processes must require low power consumption, to increase the lifetime and autonomy of battery-driven systems. These challenges can be addressed through efficient implementations of Spiking Neural Networks (SNNs) on Neuromorphic Chips and the use of event-based cameras instead of traditional frame-based cameras. In this paper, we present a new SNN-based approach, called LaneSNN, for detecting the lanes marked on the streets using the event-based camera input. We develop four novel SNN models characterized by low complexity and fast response, and train them using an offline supervised learning rule. Afterward, we implement and map the learned SNNs models onto the Intel Loihi Neuromorphic Research Chip. For the loss function, we develop a novel method based on the linear composition of Weighted binary Cross Entropy (WCE) and Mean Squared Error (MSE) measures. Our experimental results show a maximum Intersection over Union (IoU) measure of about 0.62 and very low power consumption of about 1 W. The best IoU is achieved with an SNN implementation that occupies only 36 neurocores on the Loihi processor while providing a low latency of less than 8 ms to recognize an image, thereby enabling real-time performance. The IoU measures provided by our networks are comparable with the state-of-the-art, but at a much low power consumption of 1 W.},
  archive   = {C_IROS},
  author    = {Alberto Viale and Alberto Marchisio and Maurizio Martina and Guido Masera and Muhammad Shafique},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981034},
  pages     = {79-86},
  title     = {LaneSNNs: Spiking neural networks for lane detection on the loihi neuromorphic processor},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PoseIt: A visual-tactile dataset of holding poses for grasp
stability analysis. <em>IROS</em>, 71–78. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When humans grasp objects in the real world, we often move our arms to hold the object in a different pose where we can use it. In contrast, typical lab settings only study the stability of the grasp immediately after lifting, without any subsequent re-positioning of the arm. However, the grasp stability could vary widely based on the object&#39;s holding pose, as the gravitational torque and gripper contact forces could change completely. To facilitate the study of how holding poses affect grasp stability, we present PoseIt, a novel multi-modal dataset that contains visual and tactile data collected from a full cycle of grasping an object, re-positioning the arm to one of the sampled poses, and shaking the object. Using data from PoseIt, we can formulate and tackle the task of predicting whether a grasped object is stable in a particular held pose. We train an LSTM classifier that achieves 85\% accuracy on the proposed task. Our experimental results show that multi-modal models trained on PoseIt achieve higher accuracy than using solely vision or tactile data and that our classifiers can also generalize to unseen objects and poses. The PoseIt dataset is publicly released here: https://github.com/CMURoboTouch/PoseIt.},
  archive   = {C_IROS},
  author    = {Shubham Kanitkar and Helen Jiang and Wenzhen Yuan},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981562},
  pages     = {71-78},
  title     = {PoseIt: A visual-tactile dataset of holding poses for grasp stability analysis},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RECALL: Rehearsal-free continual learning for object
classification. <em>IROS</em>, 63–70. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Convolutional neural networks show remarkable results in classification but struggle with learning new things on the fly. We present a novel rehearsal-free approach, where a deep neural network is continually learning new unseen object categories without saving any data of prior sequences. Our approach is called RECALL, as the network recalls categories by calculating logits for old categories before training new ones. These are then used during training to avoid changing the old categories. For each new sequence, a new head is added to accommodate the new categories. To mitigate forgetting, we present a regularization strategy where we replace the classification with a regression. Moreover, for the known categories, we propose a Mahalanobis loss that includes the variances to account for the changing densities between known and unknown categories. Finally, we present a novel dataset for continual learning (HOWS-CL-25), especially suited for object recognition on a mobile robot, including 150,795 synthetic images of 25 household object categories. Our approach RECALL outperforms the current state of the art on CORe50 and iCIFAR-100 and reaches the best performance on HOWS-CL-25.},
  archive   = {C_IROS},
  author    = {Markus Knauer and Maximilian Denninger and Rudolph Triebel},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981968},
  pages     = {63-70},
  title     = {RECALL: Rehearsal-free continual learning for object classification},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bio-inspired reflex system for learning visual information
for resilient robotic manipulation. <em>IROS</em>, 56–62. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans have an incredible sense of self-preservation that is both instilled, and also learned through experience. One system which contributes to this is the pain and reflex system which both minimizes damage through involuntary reflex actions and also serves as a means of &#39;negative reinforcement’ to allow learning of poor actions or decision. Equipping robots with a reflex system and parallel learning architecture could help to prolong their useful life and allow for continued learning of safe actions. Focusing on a specific mock-up scenario of cubes on a &#39;stove’ like setup, we investigate the hardware and learning approaches for a robotic manipulator to learn the presence of &#39;hot’ objects and its contextual relationship to the environment. By creating a reflex arc using analog electronics that bypasses the &#39;brain’ of the system we show an increase in the speed of release by at least two-fold. In parallel we have a learning procedure which combines visual information of the scene with this &#39;pain signal’ to learn and predict when an object may be hot, utilizing an object detection neural network. Finally, we are able to extract the learned contextual information of the environment by introducing a method inspired by &#39;thought experiments&#39; to generate heatmaps that indicate the probability of the environment being hot.},
  archive   = {C_IROS},
  author    = {Kai Junge and Kevin Qiu and Josie Hughes},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981310},
  pages     = {56-62},
  title     = {Bio-inspired reflex system for learning visual information for resilient robotic manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design and modelling of a spring-like continuum joint with
variable pitch for endoluminal surgery. <em>IROS</em>, 41–47. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In endoluminal surgery, the miniature instruments shall be of high accuracy and flexibility for minimal invasive diagnosis and surgical intervention. To this end, continuum robots with flexible joints have been proposed as the mechanism of endoscopic instruments. The compliance and deformability of the continuum joints enable access into the curved lumen. However, the manufacturing tolerances are normally not considered in the design procedure, and led to inaccuracy in the robotic control. To improve the control accuracy and flexibility of endoluminal surgical robots, we propose a novel design of a metal printed continuum joint in this paper, which incorporates a variable pitch design into the spring-like structure. The design can reduce the position errors accumulated on the distal tip of the joint, especially at large bending angles. The specification of variable pitch is investigated and determined with a friction model. In addition, to eliminate the distortion of the joint induced during the metal printing process, an extensive experiment was conducted to access the effect of the variables in the design (pitch, thickness, width and number of coils), with the aim of determining optimal parameters for reducing discrepancy caused by manufacturing variations. The final results indicated that the bending error of a single joint can be reduced from 18.10\% to 4.63\%, and a multi-segment prototype was developed to verify its effectiveness for potential surgical applications.},
  archive   = {C_IROS},
  author    = {Wei Li and Dandan Zhang and Guang-Zhong Yang and Benny Lo},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982261},
  pages     = {41-47},
  title     = {Design and modelling of a spring-like continuum joint with variable pitch for endoluminal surgery},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RCare world: A human-centric simulation world for caregiving
robots. <em>IROS</em>, 33–40. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present RCareWorld, a human-centric simulation world for physical and social robotic caregiving designed with inputs from stakeholders. RCareWorld has realistic human models of care recipients with mobility limitations and caregivers, home environments with multiple levels of accessibility and assistive devices, and robots commonly used for caregiving. It interfaces with various physics engines to model diverse material types necessary for simulating caregiving scenarios, and provides the capability to plan, control, and learn both human and robot control policies by integrating with state-of-the-art external planning and learning libraries, and VR devices. We propose a set of realistic caregiving tasks in RCareWorld as a benchmark for physical robotic caregiving and provide baseline control policies for them. We illustrate the high-fidelity simulation capabilities of RCareWorld by demonstrating the execution of a policy learnt in simulation for one of these tasks on a real-world setup. Additionally, we perform a real-world social robotic caregiving experiment using behaviors modeled in RCareWorld. Robotic caregiving, though potentially impactful towards enhancing the quality of life of care recipients and caregivers, is a field with many barriers to entry due to its interdisciplinary facets. RCareWorld takes the first step towards building a realistic simulation world for robotic caregiving that would enable researchers worldwide to contribute to this impactful field. Demo videos and supplementary materials can be found at: https://emprise.cs.cornell.edu/rcareworld/.},
  archive   = {C_IROS},
  author    = {Ruolin Ye and Wenqiang Xu and Haoyuan Fu and Rajat Kumar Jenamani and Vy Nguyen and Cewu Lu and Katherine Dimitropoulou and Tapomayukh Bhattacharjee},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982244},
  pages     = {33-40},
  title     = {RCare world: A human-centric simulation world for caregiving robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adversarial motion priors make good substitutes for complex
reward functions. <em>IROS</em>, 25–32. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Training a high-dimensional simulated agent with an under-specified reward function often leads the agent to learn physically infeasible strategies that are ineffective when deployed in the real world. To mitigate these unnatural behaviors, reinforcement learning practitioners often utilize complex reward functions that encourage physically plausible behaviors. However, a tedious labor-intensive tuning process is often required to create hand-designed rewards which might not easily generalize across platforms and tasks. We propose substituting complex reward functions with “style rewards” learned from a dataset of motion capture demonstrations. A learned style reward can be combined with an arbitrary task reward to train policies that perform tasks using naturalistic strategies. These natural strategies can also facilitate transfer to the real world. We build upon Adversarial Motion Priors - an approach from the computer graphics domain that encodes a style reward from a dataset of reference motions - to demonstrate that an adversarial approach to training policies can produce behaviors that transfer to a real quadrupedal robot without requiring complex reward functions. We also demonstrate that an effective style reward can be learned from a few seconds of motion capture data gathered from a German Shepherd and leads to energy-efficient locomotion strategies with natural gait transitions.},
  archive   = {C_IROS},
  author    = {Alejandro Escontrela and Xue Bin Peng and Wenhao Yu and Tingnan Zhang and Atil Iscen and Ken Goldberg and Pieter Abbeel},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981973},
  pages     = {25-32},
  title     = {Adversarial motion priors make good substitutes for complex reward functions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning-based localizability estimation for robust LiDAR
localization. <em>IROS</em>, 17–24. (<a
href="https://doi.org/10.1109/IROS47612.2022.9982257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {LiDAR-based localization and mapping is one of the core components in many modern robotic systems due to the direct integration of range and geometry, allowing for precise motion estimation and generation of high quality maps in real-time. Yet, as a consequence of insufficient environmental constraints present in the scene, this dependence on geometry can result in localization failure, happening in self-symmetric surroundings such as tunnels. This work addresses precisely this issue by proposing a neural network-based estimation approach for detecting (non-)localizability during robot operation. Special attention is given to the localizability of scan-to-scan registration, as it is a crucial component in many LiDAR odometry estimation pipelines. In contrast to previous, mostly traditional detection approaches, the proposed method enables early detection of failure by estimating the localizability on raw sensor measurements without evaluating the underlying registration optimization. Moreover, previous approaches remain limited in their ability to generalize across environments and sensor types, as heuristic-tuning of degeneracy detection thresholds is required. The proposed approach avoids this problem by learning from a collection of different environments, allowing the network to function over various scenarios. Furthermore, the network is trained exclusively on simulated data, avoiding arduous data collection in challenging and degenerate, often hard-to-access, environments. The presented method is tested during field experiments conducted across challenging environments and on two different sensor types without any modifications. The observed detection performance is on par with state-of-the-art methods after environment-specific threshold tuning 1 1 Supplementary Video: https://youtu.be/fm08PFwMO0c.},
  archive   = {C_IROS},
  author    = {Julian Nubert and Etienne Walther and Shehryar Khattak and Marco Hutter},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9982257},
  pages     = {17-24},
  title     = {Learning-based localizability estimation for robust LiDAR localization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FAR planner: Fast, attemptable route planner using dynamic
visibility update. <em>IROS</em>, 9–16. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Path planning in unknown environments remains a challenging problem, as the environment is gradually observed during the navigation, the underlying planner has to update the environment representation and replan, promptly and constantly, to account for the new observations. In this paper, we present a visibility graph-based planning framework capable of dealing with navigation tasks in both known and unknown environments. The planner employs a polygonal representation of the environment and constructs the representation by extracting edge points around obstacles to form enclosed polygons. With that, the method dynamically updates a global visibility graph using a two-layered data structure, expanding the visibility edges along with the navigation, and removing edges that become occluded by newly observed obstacles. When navigating in unknown environments, the method is attemptable in discovering a way to the goal by picking up the environment layout on the fly, updating the visibility graph, and fast replanning corresponding to the newly observed environment. We evaluate the method in simulated and real-world settings. The method shows the capability to attempt and navigate through unknown environments, reducing travel time by up to 12-47\% from search-based methods: A*, D* Lite, and more than 24-35\% from sampling-based methods: RRT*, BIT*, and SPARS.},
  archive   = {C_IROS},
  author    = {Fan Yang and Chao Cao and Hongbiao Zhu and Jean Oh and Ji Zhang},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981574},
  pages     = {9-16},
  title     = {FAR planner: Fast, attemptable route planner using dynamic visibility update},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Decay-based error correction in collective robotic
construction. <em>IROS</em>, 01–07. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-robot systems have been shown to build large-scale, user-specified structures using distributed, environmentally-mediated coordination in simulation. Little attention, however, has been devoted to error propagation and mitigation. In this paper, we introduce a detailed simulation of TERMES, a prototypical construction system, in which robots have realistic error profiles. We use this simulator and 32 randomly generated 250-brick blueprints to show that action errors can have significant long-term effects. We study the spatio-temporal error distribution and introduce and characterize the efficacy of a simple decay-based error correction mechanism. Although inefficient, this type of error correction is promising because it can be performed by robots with the same limited sensory capabilities as those who place bricks. To limit the impact on the construction rate, we also examine decay mechanisms informed by spatial and temporal error distributions. The incorporation of decay in our building process increases the probability of successful completion by ~ 4, at the expense of ~1/4 decrease in construction rate.},
  archive   = {C_IROS},
  author    = {Jiahe Chen and Kirstin Petersen},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981721},
  pages     = {01-07},
  title     = {Decay-based error correction in collective robotic construction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). External load estimation of hydraulically driven
construction machinery from cylinder pressures and link accelerations.
<em>IROS</em>, 01–07. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Remotely controlled hydraulically driven robots are expected to play an important role in extreme environments such as disaster sites, and force feedback is effective for improving the fidelity of the remote environment and the work efficiency. However, it is not reasonable to attach a force sensor directly to the end-point of a hydraulically driven robot. In a previous study, the authors showed that the impact forces, which are important information to improve the fidelity of the remote environment and work efficiency, can be estimated by using the information of acceleration of each link in addition to the cylinder pressures. In this paper, we investigated how many accelerometers and where those accelerometers should be attached on each link by using an index called GDOP (Geometric Dilution Of Precision) to improve the accuracy of impact force estimation. The experimental results show that although the estimation accuracy could not be improved significantly by rearranging the accelerometers, the effect of reducing the noise of the estimated load due to the sensor noise was confirmed.},
  archive   = {C_IROS},
  author    = {Naotake Shimamura and Raita Katayama and Hikaru Nagano and Yuichi Tazaki and Yasuyoshi Yokokohji},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981529},
  pages     = {01-07},
  title     = {External load estimation of hydraulically driven construction machinery from cylinder pressures and link accelerations},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spectral measurement sparsification for pose-graph SLAM.
<em>IROS</em>, 01–08. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simultaneous localization and mapping (SLAM) is a critical capability in autonomous navigation, but in order to scale SLAM to the setting of “lifelong” SLAM, particularly under memory or computation constraints, a robot must be able to determine what information should be retained and what can safely be forgotten. In graph-based SLAM, the number of edges (measurements) in a pose graph determines both the memory requirements of storing a robot&#39;s observations and the computational expense of algorithms deployed for performing state estimation using those observations; both of which can grow unbounded during long-term navigation. To address this, we propose a spectral approach for pose graph sparsification which maximizes the algebraic connectivity of the sparsified measurement graphs, a key quantity which has been shown to control the estimation error of pose graph SLAM solutions. Our algorithm, MAC (for maximizing algebraic connectivity), which is based on convex relaxation, is simple and computationally inexpensive, and admits formal post hoc performance guarantees on the quality of the solutions it provides. In experiments on benchmark pose-graph SLAM datasets, we show that our approach quickly produces high-quality sparsification results which retain the connectivity of the graph and, in turn, the quality of corresponding SLAM solutions, as compared to a baseline approach which does not consider graph connectivity.},
  archive   = {C_IROS},
  author    = {Kevin J. Doherty and David M. Rosen and John J. Leonard},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981584},
  pages     = {01-08},
  title     = {Spectral measurement sparsification for pose-graph SLAM},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). From human walking to bipedal robot locomotion: Reflex
inspired compensation on planned and unplanned downsteps. <em>IROS</em>,
01–08. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans are able to negotiate downstep behaviors-both planned and unplanned-with remarkable agility and ease. The goal of this paper is to systematically study the translation of this human behavior to bipedal walking robots, even if the morphology is inherently different. Concretely, we begin with human data wherein planned and unplanned downsteps are taken. We analyze this data from the perspective of reduced-order modelling of the human, encoding the center of mass (CoM) kinematics and contact forces, which allows for the translation of these behaviors into the corresponding reduced-order model of a bipedal robot. We embed the resulting behaviors into the full-order dynamics of a bipedal robot via nonlinear optimization-based controllers. The end result is the demonstration of planned and unplanned downsteps in simulation on an underactuated walking robot.},
  archive   = {C_IROS},
  author    = {Joris Verhagen and Xiaobin Xiong and Aaron D. Ames and Ajay Seth},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981593},
  pages     = {01-08},
  title     = {From human walking to bipedal robot locomotion: Reflex inspired compensation on planned and unplanned downsteps},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RILI: Robustly influencing latent intent. <em>IROS</em>,
01–08. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When robots interact with human partners, often these partners change their behavior in response to the robot. On the one hand this is challenging because the robot must learn to coordinate with a dynamic partner. But on the other hand - if the robot understands these dynamics - it can harness its own behavior, influence the human, and guide the team towards effective collaboration. Prior research enables robots to learn to influence other robots or simulated agents. In this paper we extend these learning approaches to now influence humans. What makes humans especially hard to influence is that - not only do humans react to the robot - but the way a single user reacts to the robot may change over time, and different humans will respond to the same robot behavior in different ways. We therefore propose a robust approach that learns to influence changing partner dynamics. Our method first trains with a set of partners across repeated interactions, and learns to predict the current partner&#39;s behavior based on the previous states, actions, and rewards. Next, we rapidly adapt to new partners by sampling trajectories the robot learned with the original partners, and then leveraging those existing behaviors to influence the new partner dynamics. We compare our resulting algorithm to state-of-the-art baselines across simulated environments and a user study where the robot and participants collaborate to build towers. We find that our approach outperforms the alternatives, even when the partner follows new or unexpected dynamics. Videos of the user study are available here: https://youtu.be/1YsWM8An18g},
  archive   = {C_IROS},
  author    = {Sagar Parekh and Soheil Habibian and Dylan P. Losey},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981601},
  pages     = {01-08},
  title     = {RILI: Robustly influencing latent intent},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatiotemporally enhanced photometric loss for
self-supervised monocular depth estimation. <em>IROS</em>, 01–08. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recovering depth information from a single image is a long-standing challenge, and self-supervised depth estimation methods have gradually attracted attention due to not relying on high-cost ground truth. Constructing an accurate photometric loss based on photometric consistency is crucial for these self-supervised methods to obtain high-quality depth maps. However, the photometric loss in most studies treats all pixels indiscriminately, resulting in poor performance. In this paper, we propose two modules based on the spatial and temporal cues to refine the photometric loss. Delving into the geometric model of photometric consistency, we introduce a depth-aware pixel correspondence module (DPC) inside the monocular depth estimation pipeline. It reduces the uncertainty of photometric errors by applying the homography matrix to the projection of corresponding pixels in far regions instead of the fundamental matrix. Furthermore, we design an omnidirectional auto-masking module (OA) to boost the robustness of our model, which utilizes temporal sequences to generate disturbance poses and hypothetical views to distin-guish dynamic objects with different directions that violate the photometric consistency. Experiments on the KITTI and the Make3d datasets reveal that our framework achieves state-of-the-art performance.},
  archive   = {C_IROS},
  author    = {Tianyu Zhang and Dongchen Zhu and Guanghui Zhang and Wenjun Shi and Yanqing Liu and Xiaolin Zhang and Jiamao Li},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981595},
  pages     = {01-08},
  title     = {Spatiotemporally enhanced photometric loss for self-supervised monocular depth estimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FBG-based variable-length estimation for shape sensing of
extensible soft robotic manipulators. <em>IROS</em>, 01–08. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel variable-length estimation approach for shape sensing of extensible soft robots utilizing fiber Bragg gratings (FBGs). Shape reconstruction from FBG sensors has been increasingly developed for soft robots, while the narrow stretching range of FBG fiber makes it difficult to acquire accurate sensing results for extensible robots. Towards this limitation, we newly introduce an FBG-based length sensor by leveraging a rigid curved channel, through which FBGs are allowed to slide within the robot following its body extension/compression, hence we can search and match the FBGs with specific constant curvature in the fiber to determine the effective length. From the fusion with the above measurements, a model-free filtering technique is accordingly presented for simultaneous calibration of a variable-length model and temporally continuous length estimation of the robot, enabling its accurate shape sensing using solely FBGs. The performances of the proposed method have been experimentally evaluated on an extensible soft robot equipped with an FBG fiber in both free and unstructured environments. The results concerning dynamic accuracy and robustness of length estimation and shape sensing demonstrate the effectiveness of our approach.},
  archive   = {C_IROS},
  author    = {Yiang Lu and Wei Chen and Zhi Chen and Jianshu Zhou and Yun–hui Liu},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981501},
  pages     = {01-08},
  title     = {FBG-based variable-length estimation for shape sensing of extensible soft robotic manipulators},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SpeedFolding: Learning efficient bimanual folding of
garments. <em>IROS</em>, 1–8. (<a
href="https://doi.org/10.1109/IROS47612.2022.9981402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Folding garments reliably and efficiently is a long standing challenge in robotic manipulation due to the complex dynamics and high dimensional configuration space of garments. An intuitive approach is to initially manipulate the garment to a canonical smooth configuration before folding. In this work, we develop SpeedFolding, a reliable and efficient bimanual system, which given user-defined instructions as folding lines, manipulates an initially crumpled garment to (1) a smoothed and (2) a folded configuration. Our primary contribution is a novel neural network architecture that is able to predict pairs of gripper poses to parameterize a diverse set of bimanual action primitives. After learning from 4300 human- annotated and self-supervised actions, the robot is able to fold garments from a random initial configuration in under 120 s on average with a success rate of 93\%. Real-world experiments show that the system is able to generalize to unseen garments of different color, shape, and stiffness. While prior work achieved 3–6 Folds Per Hour (FPH), SpeedFolding achieves 30–40 FPH. See https://pantor.github.io/speedfolding for code, videos, and datasets.},
  archive   = {C_IROS},
  author    = {Yahav Avigal and Lars Berscheid and Tamim Asfour and Torsten Kröger and Ken Goldberg},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS47612.2022.9981402},
  pages     = {1-8},
  title     = {SpeedFolding: Learning efficient bimanual folding of garments},
  year      = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
